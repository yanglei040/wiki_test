{"hands_on_practices": [{"introduction": "Efficiency in stochastic simulation often begins not with complex code, but with the elegant application of fundamental mathematical properties. This first practice explores the symmetry of the binomial distribution, a simple yet powerful concept. By quantitatively analyzing the performance gain from reparameterizing the success probability $p$ to $q=\\min(p, 1-p)$, you will derive a concrete measure of the improvement, demonstrating why a deep understanding of the underlying distribution is the first step toward writing efficient simulation code [@problem_id:3292742].", "problem": "Consider generating a binomial random variate using the inversion method within stochastic simulation and Monte Carlo methods. Let $K \\sim \\mathrm{Binomial}(n,p)$ denote a binomial random variate with probability mass function (PMF) $f(k) = \\binom{n}{k} p^{k} (1-p)^{n-k}$ for $k \\in \\{0,1,\\dots,n\\}$, and cumulative distribution function (CDF) $F(k) = \\sum_{j=0}^{k} f(j)$. The classical inversion sampler draws $U \\sim \\mathrm{Uniform}(0,1)$ and returns the smallest $k$ such that $F(k) \\ge U$, computing successive partial sums of the PMF from $k=0$ upward until the threshold is crossed. Define the algorithmic cost as the number of PMF evaluations performed per variate.\n\nYou are asked to analyze how reparameterizing to $q=\\min(p,1-p)$ and using the symmetry $K \\overset{d}{=} n - K$ when $p$ is replaced by $1-p$ affects the expected algorithmic cost. Specifically, consider the following two inversion-based samplers:\n\n- Baseline sampler: Invert the distribution with parameter $p$ directly, by summing $f(k)$ from $k=0$.\n- Reparameterized sampler: Define $q = \\min(p,1-p)$, invert the binomial distribution with parameter $q$ by summing $f_{q}(k) = \\binom{n}{k} q^{k} (1-q)^{n-k}$ from $k=0$, and then, if $p  1/2$, return $n - K'$ where $K' \\sim \\mathrm{Binomial}(n,q)$; if $p \\le 1/2$, return $K'$ unchanged.\n\nStarting only from the following fundamental bases:\n- The definition of the binomial PMF and CDF.\n- The fact that inversion sampling returns a variate with the target distribution.\n- Linearity of expectation.\n\nDerive the expected number of PMF evaluations for each sampler and define the improvement factor $I(n,p)$ as the ratio of the baseline sampler’s expected cost to the reparameterized sampler’s expected cost. Demonstrate how reparameterizing to $q=\\min(p,1-p)$ improves runtime, and quantify this improvement by providing a single, closed-form analytic expression for $I(n,p)$ in terms of $n$ and $p$. Additionally, discuss the behavior of $I(n,p)$ for $p$ near $0$ and $p$ near $1$.\n\nYour final answer must be the closed-form expression for $I(n,p)$ only. No numerical approximation or rounding is required.", "solution": "The problem asks for an analysis of the computational cost of two inversion-based samplers for generating a binomial random variate, $K \\sim \\mathrm{Binomial}(n,p)$. The cost is defined as the number of evaluations of the probability mass function (PMF) required to generate a single variate. We are to derive the expected cost for each sampler and find the ratio of these costs, termed the improvement factor $I(n,p)$.\n\nFirst, we analyze the cost of the general inversion method as described. The sampler generates a uniform random number $U \\sim \\mathrm{Uniform}(0,1)$ and returns the smallest integer $k$ for which the cumulative distribution function (CDF) $F(k)$ satisfies $F(k) \\ge U$. The CDF is computed by summing the PMF values, $F(k) = \\sum_{j=0}^{k} f(j)$. If the sampler returns the value $k_{obs}$, it must have checked the condition for $k=0, 1, \\dots, k_{obs}$. The computation of $F(k_{obs})$ requires evaluating the PMF for $j=0, 1, \\dots, k_{obs}$. Therefore, the number of PMF evaluations performed to generate the variate $k_{obs}$ is $k_{obs}+1$. The cost, $C$, is itself a random variable, functionally dependent on the generated random variate $K$, such that $C = K+1$.\n\nThe expected algorithmic cost is the expectation of this random variable, $E[C] = E[K+1]$. Using the linearity of expectation, this becomes $E[C] = E[K] + 1$.\n\nNow, we apply this general result to the two samplers.\n\n**1. Expected Cost of the Baseline Sampler**\n\nThe baseline sampler generates a variate $K$ directly from the $\\mathrm{Binomial}(n,p)$ distribution. The expected value of a binomial random variate $K \\sim \\mathrm{Binomial}(n,p)$ is a standard result, given by $E[K] = np$.\nLet $E_{base}$ be the expected cost of the baseline sampler. Using our derived cost formula:\n$$E_{base} = E[K] + 1 = np + 1$$\n\n**2. Expected Cost of the Reparameterized Sampler**\n\nThe reparameterized sampler first defines $q = \\min(p, 1-p)$. It then generates an intermediate random variate $K' \\sim \\mathrm{Binomial}(n,q)$ using the same inversion method. The cost of the sampler is defined by the number of PMF evaluations, which occurs only during this generation step. The subsequent transformation of $K'$ (either returning $K'$ directly or returning $n - K'$) does not involve PMF evaluations and, according to the problem's definition of cost, is cost-free.\n\nThe cost of this sampler is therefore the cost of generating $K'$. Let $E_{reparam}$ be this expected cost. The expected value of the intermediate variate $K' \\sim \\mathrm{Binomial}(n,q)$ is $E[K'] = nq$.\nFollowing the same logic as for the baseline sampler, the expected cost is:\n$$E_{reparam} = E[K'] + 1 = nq + 1$$\nSubstituting the definition of $q$:\n$$E_{reparam} = n \\min(p, 1-p) + 1$$\nIt is worth noting that this sampler is valid. If $p \\le 1/2$, then $q=p$, and the sampler returns $K' \\sim \\mathrm{Binomial}(n,p)$, which is correct. If $p  1/2$, then $q=1-p$, and the sampler returns $n-K'$, where $K' \\sim \\mathrm{Binomial}(n,1-p)$. It is a known property of the binomial distribution that if $X \\sim \\mathrm{Binomial}(n, \\theta)$, then $n-X \\sim \\mathrm{Binomial}(n, 1-\\theta)$. Thus, $n-K' \\sim \\mathrm{Binomial}(n, 1-(1-p)) = \\mathrm{Binomial}(n,p)$, so the output is correct in this case as well.\n\n**3. The Improvement Factor $I(n,p)$**\n\nThe improvement factor $I(n,p)$ is defined as the ratio of the expected cost of the baseline sampler to that of the reparameterized sampler.\n$$I(n,p) = \\frac{E_{base}}{E_{reparam}} = \\frac{np + 1}{n \\min(p, 1-p) + 1}$$\nThis expression is the required closed-form analytic expression for the improvement factor.\n\n**4. Discussion of the Improvement Factor**\n\nWe can analyze the behavior of $I(n,p)$ by considering two cases based on the value of $p$.\n\nCase 1: $p \\le 1/2$.\nIn this case, $\\min(p, 1-p) = p$. The improvement factor becomes:\n$$I(n,p) = \\frac{np + 1}{np + 1} = 1$$\nThis indicates that for $p \\le 1/2$, the reparameterized sampler is identical to the baseline sampler, offering no improvement in expected runtime. This is logical, as the reparameterization $q=\\min(p,1-p)$ results in $q=p$.\n\nCase 2: $p  1/2$.\nIn this case, $\\min(p, 1-p) = 1-p$. The improvement factor becomes:\n$$I(n,p) = \\frac{np + 1}{n(1-p) + 1}$$\nSince $p  1/2$, it follows that $p  1-p$. For $n0$, this implies $np  n(1-p)$, and thus $np+1  n(1-p)+1$. Therefore, for $p  1/2$, $I(n,p)  1$, signifying a strict improvement in performance. The reparameterization leverages the symmetry of the binomial distribution to convert a problem with a high mean ($np  n/2$) into one with a low mean ($n(1-p)  n/2$), which is less costly for a simple upwards-searching inversion sampler.\n\nFinally, we examine the behavior of $I(n,p)$ at the boundaries of the parameter $p \\in [0,1]$.\n- As $p \\to 0^+$, we are in Case 1, and $I(n,p) = 1$. There is no improvement, as expected.\n- As $p \\to 1^-$, we are in Case 2. We take the limit of the expression for $I(n,p)$:\n$$\\lim_{p \\to 1^-} I(n,p) = \\lim_{p \\to 1^-} \\frac{np + 1}{n(1-p) + 1} = \\frac{n(1) + 1}{n(1-1) + 1} = \\frac{n+1}{1} = n+1$$\nThe improvement factor approaches $n+1$ as $p$ approaches $1$. This represents a very significant speedup. The baseline sampler's expected cost approaches $n+1$, as it must sum nearly all $n+1$ probability masses. In contrast, the reparameterized sampler's expected cost approaches $n(1-1)+1=1$, because it generates a variate from a distribution with a mean near $0$.", "answer": "$$\\boxed{\\frac{np + 1}{n \\min(p, 1-p) + 1}}$$", "id": "3292742"}, {"introduction": "Building on the theme of algorithmic efficiency, we now move from a simple parameter trick to a more sophisticated sampler design. The standard inversion method, while robust, can be slow if it must sum many small probabilities. This exercise challenges you to implement a much faster inversion algorithm that starts its search at the distribution's mode—its most probable value—and expands outward. This practice requires you to combine theoretical derivation, careful algorithm design, and numerically stable implementation, offering a hands-on look at how professional-grade samplers are constructed [@problem_id:3292698].", "problem": "Consider a discrete random variable $X$ with a binomial distribution, defined by the probability mass function $P(X=k)$ for $k \\in \\{0,1,\\dots,n\\}$ and parameters $n \\in \\mathbb{N}$ and $p \\in [0,1]$. Begin from the fundamental definition of the binomial distribution as the sum of $n$ independent and identically distributed Bernoulli trials, each with success probability $p$, and the well-tested formula for the binomial probability mass function $P(X=k)$ as the combination of $n$ choose $k$ times $p^{k}$ times $(1-p)^{n-k}$. Derive, from first principles, a recursion for successive probabilities of the form\n$$\nP(X=k+1)=P(X=k)\\cdot\\frac{(n-k)}{(k+1)}\\cdot\\frac{p}{(1-p)}.\n$$\nWith this recursion established, design and implement an inversion-based sampling algorithm for $X$ that generates a single binomial variate by accumulating the probability mass function values outward from the mode. The algorithm must:\n- Compute the mode $m=\\lfloor (n+1)p \\rfloor$ for $p \\in (0,1)$, and handle degenerate cases $p=0$ and $p=1$ explicitly, where all mass is at $k=0$ and $k=n$, respectively.\n- Initialize at $k=m$ by computing $P(X=m)$ using a numerically stable expression based on logarithms and the gamma function.\n- Use the derived recursion to compute $P(X=m+1)$ and $P(X=m-1)$ from $P(X=m)$, and then continue to expand outward, alternating to whichever side currently has the larger next probability mass, subtracting each visited mass from a single uniform variate $U \\sim \\mathrm{Uniform}(0,1)$ until the remaining value of $U$ falls within the current mass, at which point return the corresponding index $k$.\n- Ensure that every probability mass $P(X=k)$ is visited at most once and that the algorithm halts with a correct sample value. Discuss why the expected running time is on the order of the expected absolute distance $|k-m|$ between the sampled index and the mode, grounded in the unimodality and monotone decay of the binomial probability mass function away from the mode.\n\nYour program must implement this algorithm and apply it to the following test suite, which specifies the parameters $(n,p,U)$:\n- Test case $1$: $(n,p,U)=(50,\\,0.3,\\,0.73)$.\n- Test case $2$: $(n,p,U)=(1000,\\,0.5,\\,0.5)$.\n- Test case $3$: $(n,p,U)=(20,\\,0.8,\\,0.999999)$.\n- Test case $4$: $(n,p,U)=(10,\\,1.0,\\,0.42)$.\n- Test case $5$: $(n,p,U)=(7,\\,0.0,\\,0.999)$.\n- Test case $6$: $(n,p,U)=(200,\\,0.05,\\,0.2)$.\n\nEach test case yields a single integer output representing the sampled value $k$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[k_1,k_2,k_3,k_4,k_5,k_6]$. No physical units are involved, and all angles are irrelevant to this problem. All answers are integers, and no percentages are used.", "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of probability theory, specifically the binomial distribution and inversion sampling methods. It is well-posed, providing a complete and consistent specification for an algorithm and the data required for its execution. The language is objective and formal.\n\n### Derivation of the Probability Recursion\n\nLet $X$ be a binomial random variable with parameters $n \\in \\mathbb{N}$ and $p \\in [0,1]$. The probability mass function (PMF) is given by:\n$$\nP(X=k) = \\binom{n}{k} p^k (1-p)^{n-k} = \\frac{n!}{k!(n-k)!} p^k (1-p)^{n-k}\n$$\nfor $k \\in \\{0, 1, \\dots, n\\}$.\n\nWe wish to find a recursive relationship between successive probabilities, $P(X=k+1)$ and $P(X=k)$. We can express the PMF for $k+1$ as:\n$$\nP(X=k+1) = \\binom{n}{k+1} p^{k+1} (1-p)^{n-(k+1)} = \\frac{n!}{(k+1)!(n-k-1)!} p^{k+1} (1-p)^{n-k-1}\n$$\nTo find the recursion, we compute the ratio $\\frac{P(X=k+1)}{P(X=k)}$:\n$$\n\\frac{P(X=k+1)}{P(X=k)} = \\frac{\\frac{n!}{(k+1)!(n-k-1)!} p^{k+1} (1-p)^{n-k-1}}{\\frac{n!}{k!(n-k)!} p^k (1-p)^{n-k}}\n$$\nThis can be separated into two parts: the ratio of the binomial coefficients and the ratio of the probability terms.\n\nThe ratio of the binomial coefficients simplifies as:\n$$\n\\frac{\\binom{n}{k+1}}{\\binom{n}{k}} = \\frac{n!}{(k+1)!(n-k-1)!} \\cdot \\frac{k!(n-k)!}{n!} = \\frac{k!(n-k)(n-k-1)!}{(k+1)k!(n-k-1)!} = \\frac{n-k}{k+1}\n$$\nThe ratio of the probability terms simplifies as:\n$$\n\\frac{p^{k+1}(1-p)^{n-k-1}}{p^k(1-p)^{n-k}} = \\frac{p}{1-p}\n$$\nCombining these two results gives the ratio of the successive probabilities:\n$$\n\\frac{P(X=k+1)}{P(X=k)} = \\frac{n-k}{k+1} \\cdot \\frac{p}{1-p}\n$$\nRearranging this equation yields the desired forward recursion:\n$$\nP(X=k+1) = P(X=k) \\cdot \\frac{n-k}{k+1} \\cdot \\frac{p}{1-p}\n$$\nFrom this, we can also derive the backward recursion used for computing probabilities for values less than the mode:\n$$\nP(X=k) = P(X=k+1) \\cdot \\frac{k+1}{n-k} \\cdot \\frac{1-p}{p}\n$$\n\n### Algorithm Design and Analysis\n\nThe problem requires an inversion-based sampling algorithm that explores the probability mass function outwards from its mode. This method is a valid sampling technique because for a random variate $U \\sim \\mathrm{Uniform}(0,1)$, the probability of the algorithm terminating at a value $k$ is precisely $P(X=k)$. While standard inversion sampling sums probabilities in increasing order of $k$, this algorithm reorders the summation, which does not affect the validity of the sampling method itself, though it may yield a different sample for a given $U$ compared to the standard method.\n\nThe algorithm proceeds as follows:\n\n1.  **Handle Degenerate Cases**: If $p=1$, the distribution is deterministic with all mass at $k=n$. The algorithm must return $n$. If $p=0$, all mass is at $k=0$, so it must return $0$.\n\n2.  **Compute the Mode**: For $p \\in (0,1)$, the binomial PMF is unimodal. The mode $m$, which is the value of $k$ that maximizes $P(X=k)$, is given by $m = \\lfloor(n+1)p\\rfloor$. This will be our starting point.\n\n3.  **Initialize at the Mode**: The probability at the mode, $P(X=m)$, must be computed. To avoid numerical underflow or overflow with large $n$, we compute the logarithm of the PMF first and then exponentiate. The log-PMF is:\n    $$\n    \\ln P(X=m) = \\ln\\left(\\binom{n}{m}\\right) + m\\ln(p) + (n-m)\\ln(1-p)\n    $$\n    The log of the binomial coefficient $\\ln(\\binom{n}{m}) = \\ln(n!) - \\ln(m!) - \\ln((n-m)!)$ is computed using the log-gamma function, $\\ln(k!) = \\text{gammaln}(k+1)$, for numerical stability:\n    $$\n    \\ln P(X=m) = \\text{gammaln}(n+1) - \\text{gammaln}(m+1) - \\text{gammaln}(n-m+1) + m\\ln p + (n-m)\\ln(1-p)\n    $$\n    From this, $P(X=m) = \\exp(\\ln(P(X=m)))$.\n\n4.  **Inversion Sampling via Outward Expansion**: Given a uniform variate $U \\sim \\mathrm{Uniform}(0,1)$, we use the \"accumulate and test\" principle of inversion sampling.\n    *   Initialize a cumulative probability sum, $S = P(X=m)$. If $U \\le S$, the sample is $m$.\n    *   If not, the algorithm expands outward from the mode. We maintain two pointers, $k_{low} = m-1$ and $k_{high} = m+1$, and the corresponding probabilities, $P_{low} = P(X=k_{low})$ and $P_{high} = P(X=k_{high})$, calculated using the derived recursions.\n    *   In a loop, we compare $P_{low}$ and $P_{high}$. The problem states we must choose the side with the larger probability mass.\n    *   If $P_{high}  P_{low}$ (and $k_{high} \\le n$), we add $P_{high}$ to the sum $S$. If $U \\le S$, we return $k_{high}$. Otherwise, we update $P_{high}$ to be the probability at $k_{high}+1$ using the forward recursion and increment $k_{high}$.\n    *   If $P_{low} \\ge P_{high}$ (and $k_{low} \\ge 0$), we add $P_{low}$ to $S$. If $U \\le S$, we return $k_{low}$. Otherwise, we update $P_{low}$ to be the probability at $k_{low}-1$ using the backward recursion and decrement $k_{low}$.\n    *   The process continues until the condition $U \\le S$ is met. The loop is guaranteed to terminate because $S$ strictly increases toward $1$.\n\n5.  **Running Time Analysis**: The computational cost of each step in the loop is constant. The number of steps is the number of probabilities we must sum before the cumulative sum exceeds $U$. Since we start at the mode $m$ (the most probable value) and expand to less probable values, the algorithm will terminate faster for values of $U$ that correspond to a $k$ near the mode. The expected number of iterations is therefore related to the expected absolute deviation of the random variable from its mode, $E[|X-m|]$. For a binomial distribution, the spread is characterized by the standard deviation $\\sigma = \\sqrt{np(1-p)}$. The expected distance $E[|X-m|]$ is on the order of $\\sigma$. Thus, the expected running time of this algorithm is $O(\\sqrt{np(1-p)})$, which is more efficient than the standard inversion method's $O(np)$ for large $n$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef generate_binomial_variate(n, p, u):\n    \"\"\"\n    Generates a single binomial variate using an inversion-based method\n    that expands outward from the mode.\n    \n    Args:\n        n (int): The number of trials.\n        p (float): The success probability of each trial.\n        u (float): A uniform random variate from (0, 1).\n    \n    Returns:\n        int: The generated binomial variate k.\n    \"\"\"\n    # Step 1: Handle degenerate cases\n    if p == 0.0:\n        return 0\n    if p == 1.0:\n        return n\n\n    # Step 2: Compute the mode\n    m = int(np.floor((n + 1) * p))\n\n    # Step 3: Initialize at the mode with a numerically stable PMF calculation\n    log_p = np.log(p)\n    log_1_minus_p = np.log(1.0 - p)\n    \n    log_pmf_m = (gammaln(n + 1) - gammaln(m + 1) - gammaln(n - m + 1)\n                 + m * log_p + (n - m) * log_1_minus_p)\n    pmf_m = np.exp(log_pmf_m)\n\n    # Step 4: Inversion sampling by outward expansion\n    cum_prob = pmf_m\n    if u = cum_prob:\n        return m\n\n    # Initialize pointers and probabilities for the expansion\n    k_low = m - 1\n    k_high = m + 1\n    \n    # Pre-calculate recursion ratios for efficiency\n    p_ratio = p / (1.0 - p)\n\n    # Calculate initial probabilities for the two sides\n    # Use -1.0 as a sentinel for out-of-bounds indices\n    pmf_low = pmf_m * (m / (n - m + 1.0)) / p_ratio if m  0 else -1.0\n    pmf_high = pmf_m * ((n - m) / (m + 1.0)) * p_ratio if m  n else -1.0\n\n    while True:\n        # Determine which side has the larger next probability mass\n        # If pmf_high = pmf_low, we default to the 'low' side (includes tie-breaking)\n        if pmf_high  pmf_low:\n            cum_prob += pmf_high\n            if u = cum_prob:\n                return k_high\n            \n            # Update for the next step on the high side\n            if k_high  n:\n                pmf_high *= ((n - k_high) / (k_high + 1.0)) * p_ratio\n                k_high += 1\n            else: # Boundary reached\n                pmf_high = -1.0\n        else:\n            if pmf_low  0:\n                # Should not happen if pmf_high is also  0, as sum of probabilites is 1\n                # This case handles exhaustion of the low side\n                continue\n                \n            cum_prob += pmf_low\n            if u = cum_prob:\n                return k_low\n\n            # Update for the next step on the low side\n            if k_low  0:\n                pmf_low *= (k_low / (n - k_low + 1.0)) / p_ratio\n                k_low -= 1\n            else: # Boundary reached\n                pmf_low = -1.0\n\ndef solve():\n    \"\"\"\n    Executes the binomial variate generation algorithm for the specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement: (n, p, U)\n    test_cases = [\n        (50, 0.3, 0.73),\n        (1000, 0.5, 0.5),\n        (20, 0.8, 0.999999),\n        (10, 1.0, 0.42),\n        (7, 0.0, 0.999),\n        (200, 0.05, 0.2),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, p, u = case\n        result = generate_binomial_variate(n, p, u)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3292698"}, {"introduction": "Our final practice scales up our thinking from single-core efficiency to the realm of high-performance computing. When simulations require an immense number of samples, parallelism becomes essential. This exercise leverages another key characteristic of the binomial distribution—its additivity under convolution—to design and validate a parallel variate generator. You will see how a fundamental distributional property directly enables a powerful computational architecture, while also practicing the critical skill of statistically validating a parallel simulation to ensure its correctness [@problem_id:3292711].", "problem": "You are to reason from first principles about the aggregation property of the binomial distribution and then design and validate a parallel binomial variate generator that exploits this property. The tasks are in three parts.\n\nPart A (theoretical derivation). Starting only from the definition that a binomial random variable with parameters $n$ and $p$ can be represented as a sum of $n$ independent and identically distributed Bernoulli random variables with success probability $p$, and from the definition of independence, derive that if $X_1$ and $X_2$ are independent with $X_1 \\sim \\mathrm{Bin}(n_1,p)$ and $X_2 \\sim \\mathrm{Bin}(n_2,p)$, then $X_1 + X_2 \\sim \\mathrm{Bin}(n_1 + n_2, p)$. Your derivation must be self-contained and rely only on these fundamental bases, such as indicator-sum representations, basic properties of expectation, convolution of independent discrete distributions, or probability generating functions. Do not invoke any specialized pre-derived convolution identities for binomial distributions.\n\nPart B (algorithm design). Using the result of Part A, design a parallel binomial variate generator. Given integers $n \\ge 0$, a probability $p \\in [0,1]$, and an integer number of workers $w \\ge 1$, specify how to partition $n$ into nonnegative integers $(n_1,\\dots,n_w)$ with $\\sum_{j=1}^w n_j = n$, have each worker $j$ independently generate $X^{(j)} \\sim \\mathrm{Bin}(n_j,p)$, and return the aggregate $X = \\sum_{j=1}^w X^{(j)}$. Justify correctness using Part A. Explicitly discuss:\n- A principled choice of partition, for example $n_j \\in \\{\\lfloor n/w \\rfloor,\\lceil n/w \\rceil\\}$, to balance computational load.\n- Independence requirements across workers and how to achieve them with distinct pseudorandom number generator (PRNG) substreams.\n- Computational complexity in terms of $n$, $w$, and the number of requested samples $m$, comparing a centralized generator versus the parallel aggregation.\n\nPart C (implementation and validation). Implement a complete, runnable program that:\n- Uses a reproducible seeding scheme to construct independent PRNG streams. Let the base seed be $S=123456789$. For test case index $t \\in \\{0,1,2,\\dots\\}$, define the seed for the centralized generator as $S_\\mathrm{cent}(t) = S + 10^6 t + 1$. For the parallel generator in test case $t$, define a parallel base seed $S_\\mathrm{par}(t) = S + 10^6 t + 2$ and the worker-$j$ seed as $S_\\mathrm{par}(t) + j$ for $j \\in \\{0,1,\\dots,w-1\\}$. Each PRNG stream must be used exclusively by its designated component.\n- For each test case, generates $m$ independent samples from a centralized binomial generator for $(n,p)$ and $m$ independent samples from your parallel-aggregated generator with $w$ workers and the same $(n,p)$.\n- Validates the parallel generator by computing, for each test case:\n  1. The empirical probability mass function (pmf) sup-norm difference $D = \\max_{k \\in \\{0,\\dots,n\\}} \\left| \\hat{p}_\\mathrm{par}(k) - \\hat{p}_\\mathrm{cent}(k) \\right|$, where $\\hat{p}$ denotes the frequency divided by $m$.\n  2. The absolute error in the parallel sample mean with respect to the theoretical mean $| \\overline{X}_\\mathrm{par} - n p |$.\n  3. The absolute error in the parallel sample variance with respect to the theoretical variance $| s^2_\\mathrm{par} - n p (1-p) |$, where $s^2_\\mathrm{par}$ is the population (denominator $m$) variance of the parallel samples.\n  4. A boolean pass/fail flag equal to `True` if and only if all of the following hold simultaneously:\n     - $D \\le 0.02$,\n     - $| \\overline{X}_\\mathrm{par} - n p | \\le 4 \\sqrt{ \\frac{n p (1-p)}{m} }$,\n     - $| s^2_\\mathrm{par} - n p (1-p) | \\le \\frac{6\\, n p (1-p)}{\\sqrt{m}}$.\n- Uses the following test suite of parameter values to ensure coverage of different regimes:\n  1. Case $1$: $(n,p,w,m) = (60,\\, 0.3,\\, 3,\\, 50000)$.\n  2. Case $2$: $(n,p,w,m) = (1,\\, 0.5,\\, 7,\\, 40000)$.\n  3. Case $3$: $(n,p,w,m) = (1000,\\, 0.01,\\, 8,\\, 50000)$.\n- Final Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this list corresponds to one test case and must itself be a list of the four quantities $[D,\\,\\text{mean\\_error},\\,\\text{var\\_error},\\,\\text{pass}]$, where $D$, $\\text{mean\\_error}$, and $\\text{var\\_error}$ are floats rounded to exactly six digits after the decimal point, and $\\text{pass}$ is a boolean. For example, a conforming output line has the form $[[0.012345,0.000678,0.001234,True],[...],[...]]$.\n\nNotes:\n- There are no physical units involved.\n- All probabilities must be expressed as decimals in $[0,1]$.\n- You may use any modern programming language to implement the algorithmic logic, but the final answer must be a complete, runnable program. The runtime must be self-contained and must not require user input, external files, or network access.", "solution": "The user has requested a three-part solution involving the binomial distribution: a theoretical derivation, an algorithmic design, and a programming implementation for validation.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **Part A (Theory):**\n    *   A binomial random variable $\\mathrm{Bin}(n,p)$ is the sum of $n$ independent and identically distributed (i.i.d.) Bernoulli random variables with success probability $p$.\n    *   $X_1$ and $X_2$ are independent random variables, with $X_1 \\sim \\mathrm{Bin}(n_1,p)$ and $X_2 \\sim \\mathrm{Bin}(n_2,p)$.\n    *   Derive that $X_1 + X_2 \\sim \\mathrm{Bin}(n_1 + n_2, p)$.\n    *   The derivation must be self-contained and use only fundamental principles (e.g., indicator-sum representations), not pre-derived convolution identities for binomials.\n\n*   **Part B (Algorithm):**\n    *   Design a parallel binomial variate generator for parameters $(n, p)$ using $w$ workers.\n    *   Partition $n$ into non-negative integers $(n_1, \\dots, n_w)$ with $\\sum_{j=1}^w n_j = n$.\n    *   Worker $j$ generates $X^{(j)} \\sim \\mathrm{Bin}(n_j, p)$.\n    *   The final result is $X = \\sum_{j=1}^w X^{(j)}$.\n    *   Discuss a principled partition (e.g., $n_j \\in \\{\\lfloor n/w \\rfloor, \\lceil n/w \\rceil\\}$) for load balancing.\n    *   Discuss independence requirements and PRNG substreams.\n    *   Analyze computational complexity for centralized vs. parallel generation for $m$ samples.\n\n*   **Part C (Implementation):**\n    *   **Seeding Scheme:**\n        *   Base seed: $S = 123456789$.\n        *   Test case index $t \\in \\{0, 1, 2, \\dots\\}$.\n        *   Centralized generator seed: $S_\\mathrm{cent}(t) = S + 10^6 t + 1$.\n        *   Parallel generator base seed: $S_\\mathrm{par}(t) = S + 10^6 t + 2$.\n        *   Worker-$j$ seed: $S_\\mathrm{par}(t) + j$ for $j \\in \\{0, \\dots, w-1\\}$.\n    *   **Generation Task:** For each test case, generate $m$ samples using both a centralized and the parallel generator.\n    *   **Validation Metrics:**\n        1.  $D = \\max_{k \\in \\{0,\\dots,n\\}} \\left| \\hat{p}_\\mathrm{par}(k) - \\hat{p}_\\mathrm{cent}(k) \\right|$ (sup-norm PMF difference).\n        2.  $| \\overline{X}_\\mathrm{par} - n p |$ (absolute error in parallel sample mean).\n        3.  $| s^2_\\mathrm{par} - n p (1-p) |$ (absolute error in parallel sample variance, with denominator $m$).\n    *   **Pass/Fail Criteria:**\n        *   Boolean `pass` is true iff all hold:\n            *   $D \\le 0.02$.\n            *   $| \\overline{X}_\\mathrm{par} - n p | \\le 4 \\sqrt{ \\frac{n p (1-p)}{m} }$.\n            *   $| s^2_\\mathrm{par} - n p (1-p) | \\le \\frac{6\\, n p (1-p)}{\\sqrt{m}}$.\n    *   **Test Suite:**\n        1.  $(n,p,w,m) = (60, 0.3, 3, 50000)$.\n        2.  $(n,p,w,m) = (1, 0.5, 7, 40000)$.\n        3.  $(n,p,w,m) = (1000, 0.01, 8, 50000)$.\n    *   **Output Format:** A single line `[[D1,mean_error1,var_error1,pass1],[D2,...],...]` with floats rounded to 6 decimal places.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem statement is evaluated as valid.\n*   **Scientifically Grounded (Critical):** The problem is rooted in fundamental probability theory, specifically the properties of the binomial distribution and principles of stochastic simulation. The aggregation property is a well-established theorem. The validation metrics are standard statistical measures for comparing distributions and estimators. No pseudoscience or unsupported claims are present.\n*   **Well-Posed:** The problem provides all necessary parameters, definitions, formulas, and test cases. The objectives for each part are clear, and a unique, meaningful solution exists for the theoretical, algorithmic, and implementation tasks.\n*   **Objective (Critical):** The language is precise and quantitative. All requirements, including the seeding scheme and validation thresholds, are specified objectively, leaving no room for subjective interpretation.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. Proceeding to the solution.\n\n### Part A: Theoretical Derivation\n\nThe proposition is that if $X_1 \\sim \\mathrm{Bin}(n_1, p)$ and $X_2 \\sim \\mathrm{Bin}(n_2, p)$ are independent random variables, then their sum $Y = X_1 + X_2$ follows a binomial distribution $\\mathrm{Bin}(n_1 + n_2, p)$. We will derive this from the specified first principles.\n\n**1. Indicator-Sum Representation of a Binomial Variate:**\nBy definition, a random variable $X$ follows a binomial distribution with parameters $n$ and $p$, written $X \\sim \\mathrm{Bin}(n, p)$, if it can be represented as the sum of $n$ independent and identically distributed (i.i.d.) Bernoulli random variables, each with success probability $p$. Let these Bernoulli variables be $\\{B_i\\}_{i=1}^n$, where $B_i \\sim \\mathrm{Bernoulli}(p)$. Then, $X = \\sum_{i=1}^{n} B_i$. Each $B_i$ takes the value $1$ (success) with probability $p$ and $0$ (failure) with probability $1-p$.\n\n**2. Representation of $X_1$ and $X_2$:**\nGiven $X_1 \\sim \\mathrm{Bin}(n_1, p)$, we can represent it as the sum of $n_1$ i.i.d. Bernoulli variables:\n$$X_1 = \\sum_{i=1}^{n_1} U_i, \\quad \\text{where } U_i \\sim \\mathrm{Bernoulli}(p) \\text{ are i.i.d.}$$\nSimilarly, for $X_2 \\sim \\mathrm{Bin}(n_2, p)$, we can write:\n$$X_2 = \\sum_{j=1}^{n_2} V_j, \\quad \\text{where } V_j \\sim \\mathrm{Bernoulli}(p) \\text{ are i.i.d.}$$\n\n**3. Combining the Representations:**\nThe sum $Y = X_1 + X_2$ can be written as:\n$$Y = \\left(\\sum_{i=1}^{n_1} U_i\\right) + \\left(\\sum_{j=1}^{n_2} V_j\\right)$$\nThe problem states that $X_1$ and $X_2$ are independent. Since $X_1$ is a function of the set of random variables $\\{U_i\\}_{i=1}^{n_1}$ and $X_2$ is a function of $\\{V_j\\}_{j=1}^{n_2}$, the independence of $X_1$ and $X_2$ implies that the entire set of random variables $\\{U_i\\}$ is independent of the entire set $\\{V_j\\}$.\n\nTherefore, the collection of all $n_1 + n_2$ Bernoulli variables, $\\{U_1, \\dots, U_{n_1}, V_1, \\dots, V_{n_2}\\}$, is a set of mutually independent random variables. Since all $U_i$ and $V_j$ are drawn from the same $\\mathrm{Bernoulli}(p)$ distribution, this collection forms a set of $n_1 + n_2$ i.i.d. Bernoulli random variables.\n\n**4. Conclusion:**\nThe sum $Y$ is the sum of $n_1 + n_2$ i.i.d. $\\mathrm{Bernoulli}(p)$ variables. By the fundamental definition of a binomial random variable, $Y$ must follow a binomial distribution with parameters corresponding to the total number of trials and the common success probability.\nTherefore, $Y = X_1 + X_2 \\sim \\mathrm{Bin}(n_1 + n_2, p)$. This completes the derivation.\n\n### Part B: Algorithm Design\n\nThe theoretical result from Part A is the foundation for a parallel binomial variate generator.\n\n**1. Algorithm and Correctness:**\nThe algorithm partitions the total number of trials, $n$, among $w$ parallel workers.\n*   **Partition:** Divide the integer $n$ into $w$ non-negative integers $n_1, n_2, \\dots, n_w$ such that $\\sum_{j=1}^w n_j = n$.\n*   **Parallel Generation:** Each worker $j$ is tasked with independently generating a random variate $X^{(j)} \\sim \\mathrm{Bin}(n_j, p)$.\n*   **Aggregation:** The final result is the sum of the variates from all workers: $X = \\sum_{j=1}^w X^{(j)}$.\n\n**Correctness:** The correctness of this procedure follows by induction on the number of workers $w$, using the result from Part A.\n*   **Base Case ($w=2$):** We have $X = X^{(1)} + X^{(2)}$. Since the workers operate independently, $X^{(1)} \\sim \\mathrm{Bin}(n_1, p)$ and $X^{(2)} \\sim \\mathrm{Bin}(n_2, p)$ are independent. From Part A, their sum is $X \\sim \\mathrm{Bin}(n_1 + n_2, p) = \\mathrm{Bin}(n, p)$.\n*   **Inductive Step:** Assume that for $k-1$ workers, the sum $S_{k-1} = \\sum_{j=1}^{k-1} X^{(j)}$ is distributed as $\\mathrm{Bin}(\\sum_{j=1}^{k-1} n_j, p)$. Now consider $k$ workers. The sum is $S_k = S_{k-1} + X^{(k)}$. By the independence of workers, $S_{k-1}$ is independent of $X^{(k)} \\sim \\mathrm{Bin}(n_k, p)$. Applying the result from Part A again, $S_k \\sim \\mathrm{Bin}((\\sum_{j=1}^{k-1} n_j) + n_k, p) = \\mathrm{Bin}(\\sum_{j=1}^{k} n_j, p)$.\nBy induction, for $w$ workers, the final sum $X$ is distributed as $\\mathrm{Bin}(\\sum_{j=1}^w n_j, p) = \\mathrm{Bin}(n, p)$.\n\n**2. Principled Partitioning for Load Balancing:**\nTo maximize the efficiency of parallel computation, the computational load should be distributed as evenly as possible among the workers. The time to generate a binomial variate is roughly proportional to the number of trials, $n_j$. Therefore, the values of $n_j$ should be as close to each other as possible. A standard approach is to use division with remainder. Let $n = q \\cdot w + r$, where $q = \\lfloor n/w \\rfloor$ is the quotient and $r = n \\pmod w$ is the remainder ($0 \\le r  w$).\n*   Assign $n_j = q+1$ trials to $r$ of the workers.\n*   Assign $n_j = q$ trials to the remaining $w-r$ workers.\nThis ensures that the number of trials for any two workers differs by at most $1$. This corresponds to the suggested choice $n_j \\in \\{\\lfloor n/w \\rfloor, \\lceil n/w \\rceil\\}$.\n\n**3. Independence and PRNG Substreams:**\nThe theoretical requirement of independence between $X^{(j)}$ is achieved in simulation by ensuring each worker uses an independent stream of pseudorandom numbers. Using the same pseudorandom number generator (PRNG) with the same seed for all workers would make their outputs identical, violating independence. A robust method is to initialize each worker's PRNG with a unique seed. The problem specifies a deterministic scheme for this: given a parallel base seed $S_{\\mathrm{par}}(t)$ for a test case, worker $j$'s PRNG is seeded with $S_{\\mathrm{par}}(t) + j$. For high-quality PRNGs like the PCG64 used in `numpy.random.Generator`, this method produces statistically independent random streams.\n\n**4. Computational Complexity:**\nLet's analyze the complexity of generating $m$ samples, assuming the cost of generating one $\\mathrm{Bin}(k,p)$ sample is $O(k)$.\n*   **Centralized Generator:** A single process generates $m$ samples from $\\mathrm{Bin}(n,p)$.\n    *   Total Work (CPU time): $m \\times O(n) = O(mn)$.\n    *   Wall-Clock Time (on a single core): $O(mn)$.\n*   **Parallel Aggregated Generator (with $w$ cores):** Each of the $w$ workers generates $m$ samples from $\\mathrm{Bin}(n_j, p)$, where $n_j \\approx n/w$.\n    *   Total Work (CPU time): The total computation is $\\sum_{j=1}^{w} O(m n_j) = O(m \\sum_{j=1}^{w} n_j) = O(mn)$. The total work remains the same.\n    *   Wall-Clock Time: In a parallel environment with $w$ cores, all workers compute simultaneously. The time is determined by the most heavily loaded worker, which performs $O(m \\cdot \\lceil n/w \\rceil)$ work. After generation, the $w$ arrays of $m$ samples must be summed. This aggregation step takes $O(mw)$ time on a single machine. The total wall-clock time is thus $T_{\\mathrm{par}} \\approx O(m \\cdot n/w + mw)$. The parallel approach is more efficient than the centralized one if the reduction in generation time outweighs the aggregation overhead, i.e., $mn \\gg m \\cdot n/w + mw$, which simplifies to $n \\gg n/w + w$. This is generally true when $n$ is large relative to $w^2$.\n\n### Part C: Implementation and Validation\n\nThe implementation will follow the specified logic to generate samples from both the centralized and parallel-aggregated generators. It will then compute the specified validation metrics to confirm that the parallel generator produces samples from a distribution that is statistically indistinguishable from the target binomial distribution.\n\nThe algorithm proceeds as follows for each test case:\n1.  **Setup:** Define parameters $(n, p, w, m)$ and the test case index $t$.\n2.  **Centralized Generation:** Initialize a `numpy.random.Generator` with seed $S_\\mathrm{cent}(t) = S + 10^6 t + 1$. Generate an array of $m$ samples from $\\mathrm{Bin}(n, p)$.\n3.  **Parallel Generation:**\n    a. Calculate the partition $(n_1, \\dots, n_w)$ for load balancing.\n    b. For each worker $j \\in \\{0, \\dots, w-1\\}$, initialize a `numpy.random.Generator` with seed $S_\\mathrm{par}(t) + j$.\n    c. Each worker generates an array of $m$ samples from $\\mathrm{Bin}(n_j, p)$.\n    d. Sum the $w$ arrays element-wise to produce the final array of $m$ parallel-aggregated samples.\n4.  **Validation:**\n    a. Compute the empirical PMFs for both sets of samples using `numpy.bincount` and calculate the sup-norm difference $D$.\n    b. Compute the sample mean and variance of the parallel-aggregated samples using `numpy.mean` and `numpy.var`.\n    c. Calculate the absolute errors of the sample mean and variance with respect to the theoretical values, $np$ and $np(1-p)$.\n    d. Evaluate the three pass/fail conditions using the computed metrics and their specified tolerance bounds. The overall `pass` flag is the logical AND of these conditions.\n5.  **Output:** Format the four resulting metrics ($D$, mean error, variance error, pass flag) into the required string format and append to a list of results. After all test cases, print the final formatted string.\n\nThe implementation details are provided in the final answer code block.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and validates a parallel binomial variate generator based on the\n    aggregation property of the binomial distribution.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, p, w, m)\n        (60, 0.3, 3, 50000),\n        (1, 0.5, 7, 40000),\n        (1000, 0.01, 8, 50000),\n    ]\n\n    # Global base seed\n    S = 123456789\n\n    results_data = []\n    for t, case in enumerate(test_cases):\n        n, p, w, m = case\n\n        # --- Part 1: Centralized Generator ---\n        seed_cent = S + 10**6 * t + 1\n        rng_cent = np.random.default_rng(seed_cent)\n        samples_cent = rng_cent.binomial(n, p, size=m)\n\n        # --- Part 2: Parallel-Aggregated Generator ---\n        seed_par_base = S + 10**6 * t + 2\n        \n        # Determine partition for load balancing\n        q = n // w\n        r = n % w\n        n_parts = [q + 1] * r + [q] * (w - r)\n\n        worker_samples_list = []\n        for j in range(w):\n            seed_worker = seed_par_base + j\n            rng_worker = np.random.default_rng(seed_worker)\n            n_j = n_parts[j]\n            worker_samples = rng_worker.binomial(n_j, p, size=m)\n            worker_samples_list.append(worker_samples)\n        \n        # Aggregate results from workers\n        samples_par = np.sum(worker_samples_list, axis=0)\n\n        # --- Part 3: Validation ---\n        \n        # 1. PMF sup-norm difference D\n        max_val = n\n        pmf_cent = np.bincount(samples_cent, minlength=max_val + 1) / m\n        pmf_par = np.bincount(samples_par, minlength=max_val + 1) / m\n        D = np.max(np.abs(pmf_par - pmf_cent))\n\n        # Theoretical mean and variance\n        mean_theory = n * p\n        var_theory = n * p * (1 - p)\n\n        # 2. Absolute error in parallel sample mean\n        mean_par = np.mean(samples_par)\n        mean_error = np.abs(mean_par - mean_theory)\n\n        # 3. Absolute error in parallel sample variance\n        var_par = np.var(samples_par)  # ddof=0 is default (population variance)\n        var_error = np.abs(var_par - var_theory)\n\n        # 4. Pass/fail flag\n        # Condition 1\n        cond1 = D = 0.02\n\n        # Condition 2\n        if var_theory  0:\n            mean_error_tolerance = 4 * np.sqrt(var_theory / m)\n        else:\n            mean_error_tolerance = 0.0\n        cond2 = mean_error = mean_error_tolerance\n\n        # Condition 3\n        var_error_tolerance = 6 * var_theory / np.sqrt(m)\n        cond3 = var_error = var_error_tolerance\n        \n        pass_flag = cond1 and cond2 and cond3\n\n        results_data.append([\n            D,\n            mean_error,\n            var_error,\n            pass_flag\n        ])\n    \n    # Final print statement in the exact required format.\n    formatted_results = []\n    for res in results_data:\n        d_val, me_val, ve_val, pf_val = res\n        formatted_results.append(\n            f\"[{d_val:.6f},{me_val:.6f},{ve_val:.6f},{pf_val}]\"\n        )\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```", "id": "3292711"}]}