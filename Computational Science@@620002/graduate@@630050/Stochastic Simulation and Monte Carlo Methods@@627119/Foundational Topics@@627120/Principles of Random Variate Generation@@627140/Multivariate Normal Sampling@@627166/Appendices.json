{"hands_on_practices": [{"introduction": "Generating samples is only the first step; generating them efficiently is the hallmark of an expert. This exercise delves into one of the most elegant variance reduction techniques: antithetic variates. By systematically pairing each random draw with its opposite, we can exploit the symmetry of the Gaussian distribution to reduce the variance of our Monte Carlo estimators, sometimes dramatically. Working through this problem [@problem_id:3322640] will provide a rigorous understanding of why this method works and quantifies the significant efficiency gains it can offer, particularly for certain classes of functions.", "problem": "Consider a dimension $d \\in \\mathbb{N}$ and a random vector $Z \\in \\mathbb{R}^{d}$ distributed as a $d$-dimensional multivariate normal $\\mathcal{N}_{d}(0, I_{d})$, where $I_{d}$ is the $d \\times d$ identity matrix. Let $\\mu \\in \\mathbb{R}^{d}$ be fixed and let $A \\in \\mathbb{R}^{d \\times d}$ be a fixed full-rank matrix, so that the covariance matrix $\\Sigma := A A^{\\top}$ is positive definite. Define the linear transform $X := \\mu + A Z$, which is a standard way to sample from a multivariate normal distribution $\\mathcal{N}_{d}(\\mu, \\Sigma)$ in stochastic simulation.\n\nIn the antithetic variates (AV) technique in Monte Carlo (MC), one forms pairs by using $Z$ and its antithetic $-Z$, and correspondingly $X := \\mu + A Z$ and $X^{-} := \\mu + A(-Z) = \\mu - A Z$. Assume we wish to estimate $\\theta := \\mathbb{E}[f(X)]$ for a measurable function $f : \\mathbb{R}^{d} \\to \\mathbb{R}$ such that $\\operatorname{Var}(f(X))  \\infty$. Consider two estimators that use two function evaluations per replication:\n(i) the independent-sample average $\\hat{\\theta}_{\\mathrm{ind}} := \\frac{1}{2}\\big(f(X^{(1)}) + f(X^{(2)})\\big)$ with $X^{(1)}$ and $X^{(2)}$ independent and identically distributed as $\\mathcal{N}_{d}(\\mu, \\Sigma)$,\nand (ii) the antithetic-pair average $\\hat{\\theta}_{\\mathrm{ant}} := \\frac{1}{2}\\big(f(X) + f(X^{-})\\big)$ built from a single $Z \\sim \\mathcal{N}_{d}(0, I_{d})$ and its antithetic $-Z$.\n\nStarting only from the defining properties of multivariate normal distributions and the linear transformation of Gaussian vectors, as well as the definitions of variance, covariance, and correlation, do the following:\n1. Prove that the marginal sampling distribution of each element of an antithetic pair, $X$ and $X^{-}$, is unchanged and remains $\\mathcal{N}_{d}(\\mu, \\Sigma)$.\n2. Derive the exact expression for $\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}})$ in terms of $\\operatorname{Var}(f(X))$ and $\\operatorname{Cov}(f(X), f(X^{-}))$.\n3. Derive the variance ratio $R := \\frac{\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}})}{\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ind}})}$ and express it in terms of the correlation $\\rho := \\operatorname{Corr}(f(X), f(X^{-}))$.\n4. Suppose $f$ is odd about $\\mu$, meaning $f(\\mu + y) = - f(\\mu - y)$ for all $y \\in \\mathbb{R}^{d}$. Use the symmetry of the multivariate normal distribution and the linearity of $A$ to compute $\\rho$ in this case and deduce the value of $R$.\n\nProvide your final answer as a single closed-form analytic expression for $R$ in terms of $\\rho$, together with its value for odd $f$ as described above. No rounding is required, and no physical units are involved.", "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. We proceed with a step-by-step derivation.\n\n1. Proof of the marginal distribution of $X$ and $X^{-}$.\n\nWe are given that $Z \\in \\mathbb{R}^{d}$ is a random vector with a standard multivariate normal distribution, $Z \\sim \\mathcal{N}_{d}(0, I_{d})$, where $I_d$ is the $d \\times d$ identity matrix. The probability density function (PDF) of $Z$ is symmetric about the origin, meaning $p_{Z}(z) = p_{Z}(-z)$ for all $z \\in \\mathbb{R}^{d}$. This implies that the random vector $-Z$ has the same distribution as $Z$.\nTo show this formally, let's find the mean and covariance of $-Z$.\nThe mean is $\\mathbb{E}[-Z] = -\\mathbb{E}[Z] = -0 = 0$.\nThe covariance matrix is $\\operatorname{Cov}(-Z) = \\mathbb{E}[(-Z)(-Z)^{\\top}] - \\mathbb{E}[-Z]\\mathbb{E}[-Z]^{\\top} = \\mathbb{E}[ZZ^{\\top}] - 0 = \\operatorname{Cov}(Z) = I_{d}$.\nSince $-Z$ is a linear transformation of the multivariate normal vector $Z$, it is also multivariate normal. As it has the same mean ($0$) and covariance matrix ($I_d$) as $Z$, we conclude that $-Z \\sim \\mathcal{N}_{d}(0, I_{d})$.\n\nThe random vector $X$ is defined by the affine transformation $X := \\mu + A Z$. Since $Z$ is multivariate normal, $X$ is also multivariate normal. Its mean is:\n$$ \\mathbb{E}[X] = \\mathbb{E}[\\mu + A Z] = \\mu + A \\mathbb{E}[Z] = \\mu + A \\cdot 0 = \\mu $$\nIts covariance matrix is:\n$$ \\operatorname{Cov}(X) = \\operatorname{Cov}(\\mu + A Z) = \\operatorname{Cov}(A Z) = A \\operatorname{Cov}(Z) A^{\\top} = A I_{d} A^{\\top} = A A^{\\top} = \\Sigma $$\nThus, as stated in the problem, $X \\sim \\mathcal{N}_{d}(\\mu, \\Sigma)$.\n\nThe antithetic vector $X^{-}$ is defined as $X^{-} := \\mu - A Z = \\mu + A(-Z)$. Since we have established that $-Z$ has the same distribution as $Z$, namely $\\mathcal{N}_{d}(0, I_{d})$, the random vector $X^{-}$ must have the same distribution as $X$. Following the same derivation:\n$$ \\mathbb{E}[X^{-}] = \\mathbb{E}[\\mu + A(-Z)] = \\mu + A \\mathbb{E}[-Z] = \\mu + A \\cdot 0 = \\mu $$\n$$ \\operatorname{Cov}(X^{-}) = \\operatorname{Cov}(\\mu + A(-Z)) = \\operatorname{Cov}(A(-Z)) = A \\operatorname{Cov}(-Z) A^{\\top} = A I_{d} A^{\\top} = A A^{\\top} = \\Sigma $$\nTherefore, the marginal distribution of $X^{-}$ is also $\\mathcal{N}_{d}(\\mu, \\Sigma)$, identical to the distribution of $X$. This completes the proof for the first part.\n\n2. Derivation of $\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}})$.\n\nThe antithetic-pair estimator is $\\hat{\\theta}_{\\mathrm{ant}} = \\frac{1}{2}\\big(f(X) + f(X^{-})\\big)$. We use the general formula for the variance of a sum of two random variables, $\\operatorname{Var}(aY_1 + bY_2) = a^2\\operatorname{Var}(Y_1) + b^2\\operatorname{Var}(Y_2) + 2ab\\operatorname{Cov}(Y_1, Y_2)$.\nHere, $Y_1 = f(X)$, $Y_2 = f(X^{-})$, and $a = b = \\frac{1}{2}$.\n$$ \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}}) = \\operatorname{Var}\\left(\\frac{1}{2}f(X) + \\frac{1}{2}f(X^{-})\\right) = \\left(\\frac{1}{2}\\right)^2 \\operatorname{Var}(f(X)) + \\left(\\frac{1}{2}\\right)^2 \\operatorname{Var}(f(X^{-})) + 2\\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right)\\operatorname{Cov}(f(X), f(X^{-})) $$\n$$ \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}}) = \\frac{1}{4}\\operatorname{Var}(f(X)) + \\frac{1}{4}\\operatorname{Var}(f(X^{-})) + \\frac{1}{2}\\operatorname{Cov}(f(X), f(X^{-})) $$\nFrom part $1$, we know that $X$ and $X^{-}$ are identically distributed. This implies that the random variables $f(X)$ and $f(X^{-})$ are also identically distributed, and thus have the same variance: $\\operatorname{Var}(f(X)) = \\operatorname{Var}(f(X^{-}))$.\nSubstituting this into the equation, we get:\n$$ \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}}) = \\frac{1}{4}\\operatorname{Var}(f(X)) + \\frac{1}{4}\\operatorname{Var}(f(X)) + \\frac{1}{2}\\operatorname{Cov}(f(X), f(X^{-})) $$\n$$ \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}}) = \\frac{1}{2}\\operatorname{Var}(f(X)) + \\frac{1}{2}\\operatorname{Cov}(f(X), f(X^{-})) $$\nThis is the desired expression.\n\n3. Derivation of the variance ratio $R$.\n\nFirst, we compute the variance of the independent-sample estimator, $\\hat{\\theta}_{\\mathrm{ind}} = \\frac{1}{2}\\big(f(X^{(1)}) + f(X^{(2)})\\big)$. The vectors $X^{(1)}$ and $X^{(2)}$ are independent and identically distributed (i.i.d.) as $\\mathcal{N}_{d}(\\mu, \\Sigma)$. Consequently, the random variables $f(X^{(1)})$ and $f(X^{(2)})$ are i.i.d. Their independence means their covariance is zero: $\\operatorname{Cov}(f(X^{(1)}), f(X^{(2)})) = 0$.\nThe variance is:\n$$ \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ind}}) = \\operatorname{Var}\\left(\\frac{1}{2}f(X^{(1)}) + \\frac{1}{2}f(X^{(2)})\\right) = \\left(\\frac{1}{2}\\right)^2\\operatorname{Var}(f(X^{(1)})) + \\left(\\frac{1}{2}\\right)^2\\operatorname{Var}(f(X^{(2)})) + 0 $$\nSince they are identically distributed, $\\operatorname{Var}(f(X^{(1)})) = \\operatorname{Var}(f(X^{(2)})) = \\operatorname{Var}(f(X))$.\n$$ \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ind}}) = \\frac{1}{4}\\operatorname{Var}(f(X)) + \\frac{1}{4}\\operatorname{Var}(f(X)) = \\frac{1}{2}\\operatorname{Var}(f(X)) $$\nThe variance ratio $R$ is defined as $R := \\frac{\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}})}{\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ind}})}$. Substituting the expressions we derived:\n$$ R = \\frac{\\frac{1}{2}\\operatorname{Var}(f(X)) + \\frac{1}{2}\\operatorname{Cov}(f(X), f(X^{-}))}{\\frac{1}{2}\\operatorname{Var}(f(X))} = 1 + \\frac{\\operatorname{Cov}(f(X), f(X^{-}))}{\\operatorname{Var}(f(X))} $$\nThe correlation coefficient $\\rho$ is defined as $\\rho := \\operatorname{Corr}(f(X), f(X^{-})) = \\frac{\\operatorname{Cov}(f(X), f(X^{-}))}{\\sqrt{\\operatorname{Var}(f(X)) \\operatorname{Var}(f(X^{-}))}}$.\nSince $\\operatorname{Var}(f(X)) = \\operatorname{Var}(f(X^{-}))$, this simplifies to $\\rho = \\frac{\\operatorname{Cov}(f(X), f(X^{-}))}{\\operatorname{Var}(f(X))}$.\nSubstituting this into our expression for $R$, we obtain:\n$$ R = 1 + \\rho $$\n\n4. Calculation of $\\rho$ and $R$ for an odd function $f$.\n\nWe are given that $f$ is odd about $\\mu$, meaning $f(\\mu + y) = -f(\\mu - y)$ for all $y \\in \\mathbb{R}^{d}$.\nLet us express $f(X)$ and $f(X^{-})$ using this property. We have $X = \\mu + AZ$ and $X^{-} = \\mu - AZ$. Let $y = AZ$. Then, we can write:\n$f(X) = f(\\mu + AZ)$\n$f(X^{-}) = f(\\mu - AZ)$\nUsing the odd-function property with $y = AZ$, we have $f(\\mu + AZ) = -f(\\mu - AZ)$.\nThis implies a deterministic relationship between the random variables $f(X)$ and $f(X^{-})$:\n$$ f(X) = -f(X^{-}) $$\nThis relation holds for every realization of $Z$. Now we compute the correlation $\\rho = \\operatorname{Corr}(f(X), f(X^{-}))$.\nUsing the relation $f(X^{-}) = -f(X)$, we have:\n$$ \\rho = \\operatorname{Corr}(f(X), -f(X)) $$\nUsing the properties of covariance and variance, $\\operatorname{Cov}(U, -V) = -\\operatorname{Cov}(U, V)$ and $\\operatorname{Var}(-U) = \\operatorname{Var}(U)$:\n$$ \\rho = \\frac{\\operatorname{Cov}(f(X), -f(X))}{\\sqrt{\\operatorname{Var}(f(X))\\operatorname{Var}(-f(X))}} = \\frac{-\\operatorname{Cov}(f(X), f(X))}{\\sqrt{\\operatorname{Var}(f(X))\\operatorname{Var}(f(X))}} = \\frac{-\\operatorname{Var}(f(X))}{\\operatorname{Var}(f(X))} $$\nAssuming $\\operatorname{Var}(f(X))  0$ (if not, $f(X)$ is a constant, and the problem is trivial), we get:\n$$ \\rho = -1 $$\nA perfect negative correlation is achieved.\nFinally, we can deduce the value of the variance ratio $R$:\n$$ R = 1 + \\rho = 1 + (-1) = 0 $$\nThis result signifies that for a function odd about $\\mu$, the antithetic variates technique yields a zero-variance estimator, which is the maximum possible variance reduction. The estimator $\\hat{\\theta}_{\\mathrm{ant}} = \\frac{1}{2}(f(X)+f(X^-)) = \\frac{1}{2}(f(X)-f(X)) = 0$, so its variance is indeed $0$.\n\nThe final answer combines the expression for $R$ in terms of $\\rho$ and the value of $R$ for the case of an odd function $f$.", "answer": "$$ \\boxed{ \\begin{pmatrix} 1 + \\rho  0 \\end{pmatrix} } $$", "id": "3322640"}, {"introduction": "Theoretical efficiency gains are best appreciated when seen in action within a realistic modeling context. This comprehensive programming exercise [@problem_id:3322642] challenges you to implement and compare several sampling strategies within a high-dimensional Bayesian linear regression model. You will move beyond simple independent sampling to implement antithetic variates and a powerful quasi-Monte Carlo (QMC) method, observing firsthand how these advanced techniques can accelerate the convergence of key model estimates. This practice bridges the gap between the theory of sampling and its practical application in computational statistics.", "problem": "Construct a complete program that, for multiple high-dimensional Bayesian linear regression instances with Gaussian priors, compares three Monte Carlo strategies for approximating predictive moments under a multivariate normal posterior: independent and identically distributed sampling, antithetic pairing, and quasi-Monte Carlo sampling via Sobol sequences transformed to a multivariate normal. The goal is to demonstrate improved convergence of predictive moments under quasi-multivariate normal sampling and to quantify the variance reduction properties of antithetic pairing for linear functionals.\n\nYou must work from the following fundamental base:\n- The linear-Gaussian model is given by $y = X w + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ and prior $w \\sim \\mathcal{N}(0, \\tau^2 I_d)$.\n- For Gaussian likelihood and Gaussian prior, the posterior $p(w \\mid y, X)$ is Gaussian. Its precision matrix is the sum of prior precision and data precision. Avoid any explicit matrix inversion; rely only on linear algebra identities and factorizations (for example, Cholesky factorization of a positive definite precision matrix) and triangular solves.\n- If $A$ is a symmetric positive definite matrix and $A = R^\\top R$ is its Cholesky factorization with $R$ upper triangular, then sampling $z \\sim \\mathcal{N}(0, I_d)$ and setting $u = R^{-1} z$ yields $\\operatorname{Cov}(u) = A^{-1}$. For multivariate normal sampling with covariance $A^{-1}$ and mean $\\mu$, one can use $w = \\mu + u$.\n- Antithetic pairing uses paired reference draws $z$ and $-z$ for symmetric distributions to reduce estimator variance of odd integrands.\n- Quasi-Monte Carlo sampling maps a low-discrepancy sequence $u \\in (0,1)^d$ to a standard normal vector $z$ via the inverse cumulative distribution function, then to the target multivariate normal via a linear transform.\n\nProgram requirements:\n1) For each test case below, generate data as follows, using the specified random number generator seeds (all seeds are nonnegative integers, to be used with any languageâ€™s pseudo-random number generator). All vectors and matrices are to be real-valued.\n   - Draw design matrix $X \\in \\mathbb{R}^{n \\times d}$ with rows independently distributed as $\\mathcal{N}(0, \\Sigma_X)$, where $(\\Sigma_X)_{ij} = \\rho^{|i-j|}$ for all $i, j \\in \\{1,\\dots,d\\}$ and $|\\rho|  1$. Use a Cholesky factorization of $\\Sigma_X$ to generate these rows without relying on a generic multivariate normal routine. For the generation of $X$, use the seed $s_X$.\n   - Draw a true coefficient vector $w_{\\text{true}} \\sim \\mathcal{N}(0, \\tau^2 I_d)$ using seed $s_w$, and draw noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ using seed $s_\\varepsilon$. Then set $y = X w_{\\text{true}} + \\varepsilon$.\n   - Draw a prediction covariate $x_* \\sim \\mathcal{N}(0, \\Sigma_X)$ using the same $\\Sigma_X$, with seed $s_*$.\n2) Compute the posterior precision matrix $A$ and posterior mean $\\mu$ using linear algebraic identities for the Gaussian model. Factor $A$ via a Cholesky factorization $A = R^\\top R$ with $R$ upper triangular. Do not invert any matrix explicitly.\n3) Compute the exact predictive mean $\\mu_* = \\mathbb{E}[x_*^\\top w \\mid y, X]$ and exact predictive variance $v_* = \\operatorname{Var}(x_*^\\top w \\mid y, X) + \\sigma^2$ by expressing them in terms of $A$, $\\mu$, and $x_*$, again using linear solves without explicit matrix inversion.\n4) Implement three sampling estimators to approximate the predictive mean and variance of $y_*$:\n   - Independent and identically distributed (IID) sampler: draw $N$ independent $z \\sim \\mathcal{N}(0, I_d)$ and map to $w = \\mu + R^{-1} z$. For the IID sampler, use seed $s_{\\text{iid}}$.\n   - Antithetic sampler: draw $N/2$ independent $z \\sim \\mathcal{N}(0, I_d)$ and include both $z$ and $-z$, mapping each to $w = \\mu + R^{-1} z$. Use seed $s_{\\text{anti}}$. Assume $N$ is even for all sample sizes considered.\n   - Quasi-Monte Carlo (QMC) sampler: create a Sobol sequence in $[0,1)^d$ with Owen scrambling using seed $s_{\\text{qmc}}$, transform componentwise via the inverse standard normal cumulative distribution function to obtain $z \\in \\mathbb{R}^d$, and then map to $w = \\mu + R^{-1} z$. Use power-of-two sample sizes and the first $N$ Sobol points for each $N$.\n5) For each sampler and each $N$ in the sample size set, approximate:\n   - The predictive mean by the sample mean of $x_*^\\top w$.\n   - The predictive variance by the sample variance of $x_*^\\top w$ (using population normalization) plus $\\sigma^2$.\n6) For each sampler, accumulate the integrated absolute errors across the sample sizes $N \\in \\{128, 512, 2048, 8192\\}$:\n   - For predictive mean: $E_{\\text{mean}} = \\sum_{N} \\left| \\hat{\\mu}_*(N) - \\mu_* \\right|$.\n   - For predictive variance: $E_{\\text{var}} = \\sum_{N} \\left| \\hat{v}_*(N) - v_* \\right|$.\n7) For each test case, report three boolean outcomes:\n   - Whether antithetic pairing improves the predictive mean convergence relative to IID: $E_{\\text{mean}}^{\\text{anti}}  E_{\\text{mean}}^{\\text{iid}}$.\n   - Whether quasi-Monte Carlo improves the predictive mean convergence relative to IID: $E_{\\text{mean}}^{\\text{qmc}}  E_{\\text{mean}}^{\\text{iid}}$.\n   - Whether quasi-Monte Carlo improves the predictive variance convergence relative to IID: $E_{\\text{var}}^{\\text{qmc}}  E_{\\text{var}}^{\\text{iid}}$.\n\nTest suite:\n- Case A (high-dimensional, moderately ill-conditioned):\n  - $n = 60$, $d = 50$, $\\tau = 1.0$, $\\sigma = 0.5$, $\\rho = 0.7$,\n  - $s_X = 1729$, $s_w = 2718$, $s_\\varepsilon = 31415$, $s_* = 4242$,\n  - $s_{\\text{iid}} = 7771$, $s_{\\text{anti}} = 7772$, $s_{\\text{qmc}} = 12345$.\n- Case B (underdetermined, $d \\gg n$):\n  - $n = 30$, $d = 100$, $\\tau = 1.5$, $\\sigma = 1.0$, $\\rho = 0.5$,\n  - $s_X = 2021$, $s_w = 1618$, $s_\\varepsilon = 1414$, $s_* = 1732$,\n  - $s_{\\text{iid}} = 8881$, $s_{\\text{anti}} = 8882$, $s_{\\text{qmc}} = 23456$.\n- Case C (near-collinearity):\n  - $n = 80$, $d = 80$, $\\tau = 0.8$, $\\sigma = 0.3$, $\\rho = 0.95$,\n  - $s_X = 271828$, $s_w = 161803$, $s_\\varepsilon = 141421$, $s_* = 173205$,\n  - $s_{\\text{iid}} = 9991$, $s_{\\text{anti}} = 9992$, $s_{\\text{qmc}} = 34567$.\n\nImplementation notes:\n- All linear algebra must avoid explicit matrix inversion. Use only factorizations and triangular solves.\n- The Sobol-based quasi-Monte Carlo must use a scrambled Sobol generator with the provided seed and power-of-two sample sizes $N \\in \\{128, 512, 2048, 8192\\}$; use the first $N$ points for each $N$. Map to standard normal via the inverse cumulative distribution function componentwise.\n- Clip any uniform values used in the inverse cumulative distribution function mapping to a closed interval $[10^{-12}, 1-10^{-12}]$ to avoid infinities.\n- Angles do not appear; physical units do not apply.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the three cases as a comma-separated list of bracketed triplets with no spaces, e.g., \"[[True,True,True],[True,True,True],[True,True,True]]\". Each inner triplet corresponds to a case and is ordered as $[\\text{antithetic improves mean}, \\text{quasi improves mean}, \\text{quasi improves variance}]$.", "solution": "The problem requires a comparative analysis of three Monte Carlo methods for approximating predictive moments in a high-dimensional Bayesian linear regression model. The methods are independent and identically distributed (IID) sampling, antithetic sampling, and quasi-Monte Carlo (QMC) sampling. The solution involves deriving the posterior distribution, calculating the exact predictive moments, implementing the three samplers, and evaluating their performance based on integrated absolute error.\n\n### 1. Bayesian Linear Regression Model and Posterior Distribution\n\nThe specified model is a linear relationship between a design matrix $X \\in \\mathbb{R}^{n \\times d}$ and a response vector $y \\in \\mathbb{R}^n$, with Gaussian noise.\nThe likelihood of the data is given by $p(y \\mid w, \\sigma^2) = \\mathcal{N}(y \\mid Xw, \\sigma^2 I_n)$, where $w \\in \\mathbb{R}^d$ is the vector of regression coefficients and $\\sigma^2$ is the noise variance. The likelihood function is proportional to:\n$$\np(y \\mid w) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} (y - Xw)^\\top (y - Xw)\\right)\n$$\nA Gaussian prior is placed on the weights, $p(w \\mid \\tau^2) = \\mathcal{N}(w \\mid 0, \\tau^2 I_d)$, where $\\tau^2$ is the prior variance. The prior is proportional to:\n$$\np(w) \\propto \\exp\\left(-\\frac{1}{2\\tau^2} w^\\top w\\right)\n$$\nAccording to Bayes' theorem, the posterior distribution $p(w \\mid y, X)$ is proportional to the product of the likelihood and the prior, $p(w \\mid y, X) \\propto p(y \\mid w) p(w)$. The exponent of the posterior is the sum of the exponents of the likelihood and the prior:\n$$\n-\\frac{1}{2\\sigma^2} (y^\\top y - 2y^\\top Xw + w^\\top X^\\top X w) - \\frac{1}{2\\tau^2} w^\\top w\n$$\nGrouping terms with respect to $w$, we get:\n$$\n-\\frac{1}{2} \\left( w^\\top \\left(\\frac{1}{\\sigma^2} X^\\top X + \\frac{1}{\\tau^2} I_d\\right) w - 2 \\left(\\frac{1}{\\sigma^2} y^\\top X\\right) w \\right) + \\text{const}\n$$\nThis is the exponent of a multivariate Gaussian distribution, confirming that the posterior is Gaussian, $p(w \\mid y, X) = \\mathcal{N}(w \\mid \\mu, A^{-1})$. By \"completing the square\" or comparing with the general Gaussian form $\\exp(-\\frac{1}{2}(w-\\mu)^\\top A(w-\\mu))$, we identify the posterior precision matrix $A$ and the posterior mean $\\mu$:\nThe posterior precision matrix is:\n$$\nA = \\frac{1}{\\sigma^2} X^\\top X + \\frac{1}{\\tau^2} I_d\n$$\nThe posterior mean $\\mu$ is the solution to the linear system:\n$$\nA\\mu = \\frac{1}{\\sigma^2} X^\\top y\n$$\nTo compute $\\mu$ without explicit matrix inversion, we first compute the Cholesky factorization of the symmetric positive definite matrix $A$, such that $A = R^\\top R$ where $R$ is upper triangular. Then, we solve $R^\\top R \\mu = \\frac{1}{\\sigma^2} X^\\top y$ using two triangular solves: first solving $R^\\top v = \\frac{1}{\\sigma^2} X^\\top y$ for $v$ (forward substitution), and then solving $R\\mu = v$ for $\\mu$ (backward substitution).\n\n### 2. Exact Predictive Moments\n\nFor a new covariate vector $x_* \\in \\mathbb{R}^d$, the predicted response is $y_* = x_*^\\top w + \\varepsilon_*$, where $w$ is drawn from the posterior. The predictive quantity of interest is $x_*^\\top w$. Since $w$ is Gaussian, $x_*^\\top w$ is a univariate Gaussian.\n\nThe exact predictive mean of $x_*^\\top w$ is:\n$$\n\\mu_* = \\mathbb{E}[x_*^\\top w \\mid y, X] = x_*^\\top \\mathbb{E}[w \\mid y, X] = x_*^\\top \\mu\n$$\nThe exact predictive variance of $y_*$ includes both the uncertainty in $w$ and the observational noise $\\sigma^2$. The variance of the term $x_*^\\top w$ is given by:\n$$\n\\operatorname{Var}(x_*^\\top w \\mid y, X) = x_*^\\top \\operatorname{Cov}(w) x_* = x_*^\\top A^{-1} x_*\n$$\nThe total predictive variance $v_*$ is:\n$$\nv_* = \\operatorname{Var}(x_*^\\top w \\mid y, X) + \\sigma^2 = x_*^\\top A^{-1} x_* + \\sigma^2\n$$\nTo compute $x_*^\\top A^{-1} x_*$ without inverting $A$, we solve the linear system $Av = x_*$ for $v$. Then, the variance term is $x_*^\\top v$. This linear system is solved using the Cholesky factor $R$ as before: solve $R^\\top u = x_*$ for $u$ and then $Rv = u$ for $v$.\n\n### 3. Monte Carlo Estimation Strategies\n\nThe core of the problem is to approximate $\\mu_*$ and $v_*$ using samples from the posterior $p(w \\mid y, X) = \\mathcal{N}(\\mu, A^{-1})$. To draw a sample $w$ from this distribution, we first draw a standard normal vector $z \\sim \\mathcal{N}(0, I_d)$ and then apply an affine transformation:\n$$\nw = \\mu + R^{-1} z\n$$\nThis is valid because if $z \\sim \\mathcal{N}(0, I_d)$, then $u = R^{-1} z$ has covariance $\\operatorname{Cov}(u) = R^{-1} \\operatorname{Cov}(z) (R^{-1})^\\top = R^{-1} (R^\\top)^{-1} = (R^\\top R)^{-1} = A^{-1}$. The term $R^{-1}z$ is computed by solving $R u = z$ for $u$ via backward substitution. The three sampling methods differ in how they generate the sequence of $z$ vectors.\n\n**Independent and Identically Distributed (IID) Sampling:** This is the baseline method. A set of $N$ vectors $\\{z_i\\}_{i=1}^N$ is drawn independently from $\\mathcal{N}(0, I_d)$, leading to $N$ posterior samples $\\{w_i\\}_{i=1}^N$. The approximation error for Monte Carlo integration typically decreases at a rate of $O(N^{-1/2})$.\n\n**Antithetic Sampling:** This variance reduction technique exploits symmetries in the integrand. For a symmetric distribution like $\\mathcal{N}(0, I_d)$, if we draw a sample $z$, we also include its antithetic pair $-z$. For an integrand $f(z)$ that is an odd function, the average of the pair $[f(z) + f(-z)]/2 = 0$, perfectly canceling the variation. The target for the predictive mean is $x_*^\\top w = x_*^\\top \\mu + x_*^\\top R^{-1} z$. The term $x_*^\\top R^{-1} z$ is an odd function of $z$. Each antithetic pair average for this term is $0$, so the estimator for $\\mathbb{E}[x_*^\\top R^{-1} z]$ is exactly $0$, and the estimator for $\\mathbb{E}[x_*^\\top w]$ becomes exactly $x_*^\\top \\mu$. This provides a massive variance reduction for the mean estimate. For an even integrand, like the one for variance, antithetic sampling provides no benefit.\n\n**Quasi-Monte Carlo (QMC) Sampling:** QMC methods replace random samples with deterministic, low-discrepancy sequences, such as the Sobol sequence. These points fill the sample space more uniformly than pseudo-random points. A Sobol sequence generates points $u_i$ in the unit hypercube $[0,1)^d$. To obtain standard normal samples $z_i$, each component of $u_i$ is transformed using the inverse of the standard normal cumulative distribution function (CDF), also known as the probit function. For well-behaved integrands, QMC methods can achieve a faster convergence rate, approaching $O(N^{-1})$, significantly outperforming IID sampling. Scrambling the Sobol sequence further improves its properties by breaking up deterministic patterns while preserving low discrepancy.\n\n### 4. Algorithm and Implementation\n\nFor each test case, the program will execute the following steps:\n1.  **Data Generation:**\n    - Construct the $d \\times d$ covariance matrix $\\Sigma_X$ with elements $(\\Sigma_X)_{ij} = \\rho^{|i-j|}$.\n    - Compute the lower Cholesky factor $L_X$ of $\\Sigma_X$ such that $\\Sigma_X = L_X L_X^\\top$.\n    - Using the specified seed $s_X$, generate $n$ vectors $z_{X,i} \\sim \\mathcal{N}(0, I_d)$ and form the rows of $X$ as $x_i^\\top = (L_X z_{X,i})^\\top$.\n    - Generate $w_{\\text{true}}$, $\\varepsilon$, and $x_*$ similarly using their respective seeds and parameters.\n    - Compute the observed data $y = X w_{\\text{true}} + \\varepsilon$.\n\n2.  **Posterior and Exact Moments Calculation:**\n    - Calculate the posterior precision $A = (1/\\sigma^2) X^\\top X + (1/\\tau^2) I_d$.\n    - Compute the upper Cholesky factor $R$ of $A$ such that $A = R^\\top R$.\n    - Calculate the right-hand side $b = (1/\\sigma^2) X^\\top y$.\n    - Solve for the posterior mean $\\mu$ from $R^\\top R \\mu = b$ using two triangular solves.\n    - Calculate the exact predictive mean $\\mu_* = x_*^\\top \\mu$.\n    - Solve for $v$ from $R^\\top R v = x_*$ using two triangular solves.\n    - Calculate the exact predictive variance $v_* = x_*^\\top v + \\sigma^2$.\n\n3.  **Monte Carlo Estimation and Error Accumulation:**\n    - For each sampler (IID, Antithetic, QMC) and each sample size $N \\in \\{128, 512, 2048, 8192\\}$:\n        - Generate $N$ standard normal vectors $\\{z_i\\}$ according to the sampler's logic, using its specified seed. For QMC, this involves generating a scrambled Sobol sequence and applying the inverse normal CDF.\n        - For each $z_i$, solve $R u_i = z_i$ for $u_i$ and form the posterior sample $w_i = \\mu + u_i$.\n        - Compute the sample predictions $p_i = x_*^\\top w_i$.\n        - Estimate the mean $\\hat{\\mu}_*(N)$ as the sample mean of $\\{p_i\\}$.\n        - Estimate the variance $\\hat{v}_*(N)$ as the sample variance of $\\{p_i\\}$ (with $1/N$ normalization) plus $\\sigma^2$.\n        - Calculate the absolute errors $|\\hat{\\mu}_*(N) - \\mu_*|$ and $|\\hat{v}_*(N) - v_*|$.\n    - Sum these absolute errors over all $N$ to get the integrated errors $E_{\\text{mean}}$ and $E_{\\text{var}}$ for each sampler.\n\n4.  **Final Comparison:**\n    - For each test case, perform the three boolean comparisons specified in the problem statement and store the resulting triplet.\n\n5.  **Output:**\n    - Collect the triplets from all test cases and print them in the required format: `[[bool,bool,bool],[bool,bool,bool],[...]]`.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular\nfrom scipy.stats import norm\nfrom scipy.stats.qmc import Sobol\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    test_cases = [\n        # Case A: high-dimensional, moderately ill-conditioned\n        {'n': 60, 'd': 50, 'tau': 1.0, 'sigma': 0.5, 'rho': 0.7,\n         's_X': 1729, 's_w': 2718, 's_epsilon': 31415, 's_star': 4242,\n         's_iid': 7771, 's_anti': 7772, 's_qmc': 12345},\n        # Case B: underdetermined, d  n\n        {'n': 30, 'd': 100, 'tau': 1.5, 'sigma': 1.0, 'rho': 0.5,\n         's_X': 2021, 's_w': 1618, 's_epsilon': 1414, 's_star': 1732,\n         's_iid': 8881, 's_anti': 8882, 's_qmc': 23456},\n        # Case C: near-collinearity\n        {'n': 80, 'd': 80, 'tau': 0.8, 'sigma': 0.3, 'rho': 0.95,\n         's_X': 271828, 's_w': 161803, 's_epsilon': 141421, 's_star': 173205,\n         's_iid': 9991, 's_anti': 9992, 's_qmc': 34567},\n    ]\n\n    sample_sizes = [128, 512, 2048, 8192]\n    all_results = []\n    for case_params in test_cases:\n        results = run_case(case_params, sample_sizes)\n        all_results.append(results)\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef run_case(params, sample_sizes):\n    \"\"\"\n    Executes the entire simulation for a single test case.\n    \"\"\"\n    n, d, tau, sigma, rho = params['n'], params['d'], params['tau'], params['sigma'], params['rho']\n    s_X, s_w, s_epsilon, s_star = params['s_X'], params['s_w'], params['s_epsilon'], params['s_star']\n    s_iid, s_anti, s_qmc = params['s_iid'], params['s_anti'], params['s_qmc']\n\n    # 1. Data Generation\n    # Generate covariance matrix Sigma_X and its Cholesky factor\n    indices = np.arange(d)\n    Sigma_X = rho ** np.abs(indices[:, np.newaxis] - indices)\n    L_X = np.linalg.cholesky(Sigma_X)\n\n    # Generate X\n    rng_X = np.random.default_rng(s_X)\n    Z_X = rng_X.standard_normal((n, d))\n    X = Z_X @ L_X.T\n\n    # Generate w_true\n    rng_w = np.random.default_rng(s_w)\n    w_true = tau * rng_w.standard_normal(d)\n\n    # Generate epsilon\n    rng_eps = np.random.default_rng(s_epsilon)\n    epsilon = sigma * rng_eps.standard_normal(n)\n\n    # Generate y\n    y = X @ w_true + epsilon\n\n    # Generate x_star\n    rng_star = np.random.default_rng(s_star)\n    x_star = L_X @ rng_star.standard_normal(d)\n\n    # 2. Posterior and Exact Moments Calculation\n    # Posterior precision matrix A\n    A = (1 / sigma**2) * (X.T @ X) + (1 / tau**2) * np.identity(d)\n    \n    # Cholesky factorization of A (upper triangular)\n    R = cholesky(A, lower=False)\n\n    # Posterior mean mu\n    b = (1 / sigma**2) * (X.T @ y)\n    v = solve_triangular(R, b, trans='T', lower=False)\n    mu = solve_triangular(R, v, trans='N', lower=False)\n\n    # Exact predictive mean mu_star\n    mu_star = x_star @ mu\n\n    # Exact predictive variance v_star\n    v_solve = solve_triangular(R, x_star, trans='T', lower=False)\n    u_solve = solve_triangular(R, v_solve, trans='N', lower=False)\n    var_w_term = x_star @ u_solve\n    v_star = var_w_term + sigma**2\n\n    # 3. Monte Carlo Estimation\n    errors = {}\n    samplers = {\n        'iid': lambda N, seed: np.random.default_rng(seed).standard_normal((N, d)),\n        'anti': lambda N, seed: _generate_antithetic(N, d, seed),\n        'qmc': lambda N, seed: _generate_qmc(N, d, seed)\n    }\n    sampler_seeds = {'iid': s_iid, 'anti': s_anti, 'qmc': s_qmc}\n\n    for name, sampler_func in samplers.items():\n        E_mean, E_var = 0.0, 0.0\n        for N in sample_sizes:\n            z_samples = sampler_func(N, sampler_seeds[name])\n\n            # Transform standard normal samples to posterior samples\n            # u = R^-1 * z\n            u_samples = solve_triangular(R, z_samples.T, lower=False).T\n            w_samples = mu + u_samples\n            \n            # Predictions\n            p_samples = w_samples @ x_star\n\n            # Estimated moments\n            mu_hat = np.mean(p_samples)\n            v_hat = np.var(p_samples, ddof=0) + sigma**2\n\n            # Accumulate errors\n            E_mean += np.abs(mu_hat - mu_star)\n            E_var += np.abs(v_hat - v_star)\n        \n        errors[name] = {'mean': E_mean, 'var': E_var}\n\n    # 4. Final Comparison\n    anti_improves_mean = errors['anti']['mean']  errors['iid']['mean']\n    qmc_improves_mean = errors['qmc']['mean']  errors['iid']['mean']\n    qmc_improves_var = errors['qmc']['var']  errors['iid']['var']\n\n    return [anti_improves_mean, qmc_improves_mean, qmc_improves_var]\n\ndef _generate_antithetic(N, d, seed):\n    \"\"\"Generates N antithetic standard normal samples.\"\"\"\n    rng = np.random.default_rng(seed)\n    half_N = N // 2\n    z_half = rng.standard_normal((half_N, d))\n    return np.vstack((z_half, -z_half))\n\ndef _generate_qmc(N, d, seed):\n    \"\"\"Generates N QMC standard normal samples.\"\"\"\n    sobol_engine = Sobol(d=d, scramble=True, seed=seed)\n    u_samples = sobol_engine.random(n=N)\n    # Clip to avoid infinity from ppf\n    u_samples = np.clip(u_samples, 1e-12, 1 - 1e-12)\n    return norm.ppf(u_samples)\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3322642"}, {"introduction": "In the world of complex simulations, especially those run in parallel, it is not enough to assume your code is correct; you must be able to verify it. This practice [@problem_id:3322614] addresses a critical and subtle challenge: detecting unintended correlations in your samples that can arise from improper management of random number streams. You will design and implement a suite of statistical tests to diagnose these hidden dependencies, transforming the abstract problem of sample quality into a concrete, testable hypothesis. Mastering this skill is essential for producing reliable and reproducible scientific results from Monte Carlo simulations.", "problem": "You are given a task in the domain of stochastic simulation and Monte Carlo methods to diagnose unintended dependence between coordinates in samples from a multivariate normal distribution when parallel implementations reuse or share random number streams. The goal is to propose, justify, and implement a set of statistical tests that can detect dependence between coordinates that is not part of the intended target covariance structure. Your final program must implement the tests, apply them to a specified test suite of scenarios, and output a single line with the results.\n\nStart from the fundamental base of definitions and widely accepted facts:\n- A random vector $\\mathbf{X} \\in \\mathbb{R}^p$ is said to be multivariate normal with mean $\\boldsymbol{\\mu} \\in \\mathbb{R}^p$ and covariance matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{p \\times p}$, denoted $\\mathbf{X} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, if every linear combination of its components is univariate normal.\n- If $\\boldsymbol{\\Sigma}$ is positive definite, there exists a lower-triangular Cholesky factor $\\mathbf{L}$ such that $\\boldsymbol{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top$.\n- If $\\mathbf{Z} \\sim \\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$, then $\\boldsymbol{\\mu} + \\mathbf{L}\\mathbf{Z} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$.\n- If samples $\\{\\mathbf{X}_k\\}_{k=1}^n$ are generated from $\\mathcal{N}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}_0)$ for a known target $\\boldsymbol{\\Sigma}_0$, then the whitened samples $\\mathbf{Y}_k = \\mathbf{L}_0^{-1}(\\mathbf{X}_k - \\boldsymbol{\\mu})$, where $\\boldsymbol{\\Sigma}_0 = \\mathbf{L}_0\\mathbf{L}_0^\\top$, should be independent and identically distributed with $\\mathbf{Y}_k \\sim \\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$ if no unintended dependence exists beyond $\\boldsymbol{\\Sigma}_0$.\n- Under multivariate normality, the sample covariance matrix has a Wishart distribution, and its correlation structure can be assessed against sphericity (identity correlation) using established tests.\n\nDesign and justify a set of tests that, when applied to the whitened samples $\\{\\mathbf{Y}_k\\}$, diagnose unintended dependence between coordinates:\n1. A global test for sphericity of the correlation matrix of $\\{\\mathbf{Y}_k\\}$ under the null hypothesis that the true correlation is $\\mathbf{I}_p$. Specifically, use the Bartlett sphericity test that evaluates whether the sample correlation matrix deviates from $\\mathbf{I}_p$ more than expected under $\\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$.\n2. A family of pairwise tests for zero correlation among coordinate pairs of $\\{\\mathbf{Y}_k\\}$, aggregated with a family-wise error rate control through Bonferroni adjustment. Each pairwise test should evaluate whether the sample correlation coefficient is zero.\n\nYour program must:\n- Generate samples for each test case using specified samplers.\n- Whiten the samples using the known target covariance $\\boldsymbol{\\Sigma}_0$ to transform the problem to testing independence under $\\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$.\n- Compute the Bartlett sphericity test statistic and its approximate chi-square $p$-value with degrees of freedom $p(p-1)/2$.\n- Compute all pairwise correlation tests using the Student's $t$ distribution for the correlation coefficient under the null hypothesis of zero correlation, and apply a Bonferroni correction across all $p(p-1)/2$ pairs to control the family-wise error rate.\n- Declare unintended dependence detected if either the Bartlett sphericity test rejects at significance level $\\alpha$ or any Bonferroni-adjusted pairwise test rejects at significance level $\\alpha$.\n\nUse significance level $\\alpha = 0.01$.\n\nTest suite:\nFor reproducibility, use the specified seeds. Each test case is a tuple specifying the sampler type, the sample size $n$, the dimension $p$, the target covariance $\\boldsymbol{\\Sigma}_0$, the shared stream parameter $\\rho$ if applicable, and the random seed. The samplers are defined as follows:\n- \"correct\": For each sample, draw $\\mathbf{Z}_k \\sim \\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$ independently and set $\\mathbf{X}_k = \\boldsymbol{\\mu} + \\mathbf{L}_0 \\mathbf{Z}_k$.\n- \"shared\": For each sample, draw $U_k \\sim \\mathcal{N}(0,1)$ and set $\\mathbf{Z}_k = U_k \\mathbf{1}_p$, so all coordinates share the same underlying standard normal, and then $\\mathbf{X}_k = \\boldsymbol{\\mu} + \\mathbf{L}_0 \\mathbf{Z}_k$.\n- \"partial\": For each sample, draw $U_k \\sim \\mathcal{N}(0,1)$ and $\\mathbf{V}_k \\sim \\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$ independently, and set $\\mathbf{Z}_k = \\sqrt{\\rho}\\,U_k \\mathbf{1}_p + \\sqrt{1-\\rho}\\,\\mathbf{V}_k$, then $\\mathbf{X}_k = \\boldsymbol{\\mu} + \\mathbf{L}_0 \\mathbf{Z}_k$.\n\nLet $\\boldsymbol{\\mu} = \\mathbf{0}$ for all cases. Define the target covariances $\\boldsymbol{\\Sigma}_0$ as:\n- Case $1$ (\"correct\"): $\\boldsymbol{\\Sigma}_0$ is autoregressive of order $1$ with parameter $\\phi = 0.6$, i.e., $(\\boldsymbol{\\Sigma}_0)_{ij} = \\phi^{|i-j|}$, with $p = 6$, sample size $n = 1000$, seed $12345$.\n- Case $2$ (\"shared\"): $\\boldsymbol{\\Sigma}_0$ is the same autoregressive covariance with $\\phi = 0.6$, with $p = 6$, sample size $n = 300$, seed $54321$.\n- Case $3$ (\"partial\"): $\\boldsymbol{\\Sigma}_0$ is diagonal with entries $(1.0, 1.5, 0.5, 2.0, 1.2, 0.8, 1.7, 1.1)$, with $p = 8$, shared stream parameter $\\rho = 0.3$, sample size $n = 100$, seed $2024$.\n\nAnswer specification:\n- For each test case, your program must output a boolean indicating whether unintended dependence was detected ($\\text{True}$ if detected, $\\text{False}$ otherwise), using the decision rule described above at significance level $\\alpha = 0.01$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases: Case $1$, Case $2$, Case $3$. For example, an output could be of the form $[\\text{False},\\text{True},\\text{True}]$.\n\nNo physical units or angle units are involved in this problem. The significance level should be expressed as a decimal number (e.g., $0.01$). Implement the program in a way that is fully deterministic based on the provided seeds and that requires no user input.", "solution": "We begin with the foundational definitions of the multivariate normal distribution and its properties. A random vector $\\mathbf{X} \\in \\mathbb{R}^p$ is multivariate normal with mean $\\boldsymbol{\\mu}$ and covariance $\\boldsymbol{\\Sigma}$, denoted $\\mathbf{X} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, if every linear combination of its components is univariate normal. If $\\boldsymbol{\\Sigma}$ is positive definite, we can write $\\boldsymbol{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top$ where $\\mathbf{L}$ is a lower-triangular Cholesky factor. If $\\mathbf{Z} \\sim \\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$, then $\\boldsymbol{\\mu} + \\mathbf{L}\\mathbf{Z}$ has distribution $\\mathcal{N}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$.\n\nWhen sampling from $\\mathcal{N}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}_0)$, if we apply the whitening transformation $\\mathbf{Y}_k = \\mathbf{L}_0^{-1}(\\mathbf{X}_k - \\boldsymbol{\\mu})$ with $\\boldsymbol{\\Sigma}_0 = \\mathbf{L}_0\\mathbf{L}_0^\\top$, then under correct implementation, we should have $\\mathbf{Y}_k \\sim \\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$ i.i.d. In parallel Monte Carlo implementations, unintended sharing or reuse of random number streams across coordinates can yield $\\mathbf{Z}_k$ vectors whose components are dependent beyond the intended structure implied by $\\boldsymbol{\\Sigma}_0$. After whitening, such unintended dependence manifests as departures from independence among the coordinates of $\\mathbf{Y}_k$.\n\nWe now design and justify tests to detect such departures, grounded in the normal theory and covariance properties.\n\nStep $1$: Whitening transformation.\nGiven samples $\\{\\mathbf{X}_k\\}_{k=1}^n$, known target covariance $\\boldsymbol{\\Sigma}_0$, and mean $\\boldsymbol{\\mu}$, compute the Cholesky factor $\\mathbf{L}_0$ such that $\\boldsymbol{\\Sigma}_0 = \\mathbf{L}_0\\mathbf{L}_0^\\top$, and set $\\mathbf{Y}_k = \\mathbf{L}_0^{-1}(\\mathbf{X}_k - \\boldsymbol{\\mu})$. Under the null hypothesis of correct sampling, $\\mathbf{Y}_k \\sim \\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$ i.i.d.\n\nStep $2$: Global sphericity test (Bartlett).\nUnder $\\mathbf{Y}_k \\sim \\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$, the true correlation matrix is $\\mathbf{I}_p$. Let $\\hat{\\mathbf{R}}$ denote the sample correlation matrix of $\\{\\mathbf{Y}_k\\}$. Bartlett's test of sphericity assesses\n$$\nH_0: \\ \\mathbf{R} = \\mathbf{I}_p \\quad \\text{versus} \\quad H_1: \\ \\mathbf{R} \\neq \\mathbf{I}_p.\n$$\nThe test statistic is\n$$\n\\chi^2_{\\text{Bart}} = -\\left(n - 1 - \\frac{2p + 5}{6}\\right)\\ln\\det(\\hat{\\mathbf{R}}),\n$$\nwhich, under $H_0$ and multivariate normality, has an approximate chi-square distribution with degrees of freedom $p(p-1)/2$. Large values of $\\chi^2_{\\text{Bart}}$ indicate departures from sphericity. If $\\det(\\hat{\\mathbf{R}}) \\leq 0$, which can occur with near-singular $\\hat{\\mathbf{R}}$, we interpret $\\chi^2_{\\text{Bart}}$ as $+\\infty$ and reject $H_0$ with $p$-value equal to $0$.\n\nJustification: Under $\\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$, the sample correlation matrix is, in expectation, $\\mathbf{I}_p$. Sharing random streams across coordinates induces linear dependence in $\\mathbf{Y}_k$, which reduces $\\det(\\hat{\\mathbf{R}})$ and inflates the $\\chi^2_{\\text{Bart}}$ statistic, providing sensitivity to global departures from independence.\n\nStep $3$: Pairwise correlation tests with Bonferroni control.\nFor each pair $(i,j)$ with $1 \\leq i  j \\leq p$, compute the sample correlation coefficient $\\hat{r}_{ij}$. Under $H_0$ of independence, the statistic\n$$\nt_{ij} = \\hat{r}_{ij}\\sqrt{\\frac{n - 2}{1 - \\hat{r}_{ij}^2}}\n$$\nhas a Student's $t$ distribution with $n - 2$ degrees of freedom. Compute the two-sided $p$-value $p_{ij} = 2 \\cdot \\Pr\\left(T_{n-2} \\geq |t_{ij}|\\right)$. There are $m = p(p-1)/2$ pairs; apply the Bonferroni correction to control family-wise error rate by testing each pair at level $\\alpha/m$, or equivalently by comparing the adjusted $p$-values $p_{ij}^{\\text{adj}} = \\min(1, m \\cdot p_{ij})$ to $\\alpha$. Declare that pair $(i,j)$ is dependent if $p_{ij}^{\\text{adj}}  \\alpha$. Declare unintended dependence detected if any pair rejects.\n\nJustification: Sharing a random stream typically induces linear correlation among coordinates. Pairwise correlation tests capture such linear dependence. Bonferroni correction provides a conservative, yet simple, control of the probability of at least one false rejection.\n\nDecision rule:\nAt significance level $\\alpha = 0.01$, declare unintended dependence detected if either the Bartlett sphericity test $p$-value is less than $\\alpha$ or the minimum Bonferroni-adjusted pairwise $p$-value across all pairs is less than $\\alpha$.\n\nTest suite and samplers:\nWe consider three cases with specified seeds to ensure reproducibility.\n\nCase $1$ (\"correct\"): $p = 6$, $n = 1000$, $\\boldsymbol{\\mu} = \\mathbf{0}$, $\\boldsymbol{\\Sigma}_0$ is autoregressive of order $1$ with parameter $\\phi = 0.6$, i.e., $(\\boldsymbol{\\Sigma}_0)_{ij} = \\phi^{|i - j|}$, seed $12345$. The sampler generates independent $\\mathbf{Z}_k \\sim \\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$, so after whitening, $\\mathbf{Y}_k \\sim \\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$, and the tests should not detect dependence, yielding $\\text{False}$.\n\nCase $2$ (\"shared\"): $p = 6$, $n = 300$, $\\boldsymbol{\\mu} = \\mathbf{0}$, $\\boldsymbol{\\Sigma}_0$ is the same autoregressive covariance with $\\phi = 0.6$, seed $54321$. The sampler uses $\\mathbf{Z}_k = U_k \\mathbf{1}_p$ with $U_k \\sim \\mathcal{N}(0,1)$, hence all coordinates share the same standard normal. After whitening, $\\mathbf{Y}_k = \\mathbf{Z}_k$, which has perfect correlation among coordinates. Both the Bartlett test and pairwise tests should detect dependence, yielding $\\text{True}$.\n\nCase $3$ (\"partial\"): $p = 8$, $n = 100$, $\\boldsymbol{\\mu} = \\mathbf{0}$, $\\boldsymbol{\\Sigma}_0 = \\operatorname{diag}(1.0, 1.5, 0.5, 2.0, 1.2, 0.8, 1.7, 1.1)$, seed $2024$, shared stream parameter $\\rho = 0.3$. The sampler uses $\\mathbf{Z}_k = \\sqrt{\\rho}\\, U_k \\mathbf{1}_p + \\sqrt{1 - \\rho}\\,\\mathbf{V}_k$ with independent $U_k \\sim \\mathcal{N}(0,1)$ and $\\mathbf{V}_k \\sim \\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$; the shared component induces equicorrelation of approximately $\\rho$ among the coordinates of $\\mathbf{Y}_k$. The Bartlett sphericity test, being sensitive to global correlation structure, should detect dependence at $\\alpha = 0.01$, yielding $\\text{True}$.\n\nAlgorithmic design:\n- Implement functions to construct $\\boldsymbol{\\Sigma}_0$ for autoregressive order $1$ and diagonal cases.\n- Implement samplers for \"correct\", \"shared\", and \"partial\" scenarios.\n- Implement whitening by solving $\\mathbf{L}_0 \\mathbf{y} = \\mathbf{x} - \\boldsymbol{\\mu}$ for each sample, equivalently $\\mathbf{y} = \\mathbf{L}_0^{-1}(\\mathbf{x} - \\boldsymbol{\\mu})$.\n- Compute the sample correlation matrix, the Bartlett sphericity statistic and $p$-value, and the family of pairwise correlations with Bonferroni-adjusted $p$-values.\n- Apply the decision rule to produce a boolean outcome per test case.\n\nFinal output format:\nProduce a single line of output containing the three boolean results in order, as a comma-separated list enclosed in square brackets, for example $[\\text{False},\\text{True},\\text{True}]$.\n\nThis approach integrates multivariate normal theory, covariance structure, whitening transformations, and classical hypothesis testing to diagnose unintended dependence arising from shared random streams in parallel implementations.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef make_cov_ar1(p: int, phi: float) - np.ndarray:\n    \"\"\"Construct an AR(1) covariance matrix with parameter phi.\"\"\"\n    idx = np.arange(p)\n    return phi ** np.abs(idx[:, None] - idx[None, :])\n\ndef sample_correct(n: int, mu: np.ndarray, Sigma: np.ndarray, rng: np.random.Generator) - np.ndarray:\n    \"\"\"Generate n samples from N(mu, Sigma) using independent standard normals.\"\"\"\n    p = Sigma.shape[0]\n    L = np.linalg.cholesky(Sigma)\n    Z = rng.standard_normal(size=(n, p))\n    return mu + Z @ L.T\n\ndef sample_shared_stream(n: int, mu: np.ndarray, Sigma: np.ndarray, rng: np.random.Generator) - np.ndarray:\n    \"\"\"Generate n samples where all coordinates share the same standard normal per sample.\"\"\"\n    p = Sigma.shape[0]\n    L = np.linalg.cholesky(Sigma)\n    U = rng.standard_normal(size=(n, 1))\n    Z = np.repeat(U, p, axis=1)  # each row has identical entries\n    return mu + Z @ L.T\n\ndef sample_partial_shared(n: int, mu: np.ndarray, Sigma: np.ndarray, rho: float, rng: np.random.Generator) - np.ndarray:\n    \"\"\"Generate n samples with a shared component sqrt(rho)*U*1_p plus independent component.\"\"\"\n    p = Sigma.shape[0]\n    L = np.linalg.cholesky(Sigma)\n    U = rng.standard_normal(size=(n, 1))\n    V = rng.standard_normal(size=(n, p))\n    Z = np.sqrt(rho) * np.repeat(U, p, axis=1) + np.sqrt(1.0 - rho) * V\n    return mu + Z @ L.T\n\ndef whiten_samples(X: np.ndarray, mu: np.ndarray, Sigma0: np.ndarray) - np.ndarray:\n    \"\"\"Whiten samples with respect to the known target covariance Sigma0.\"\"\"\n    L0 = np.linalg.cholesky(Sigma0)\n    # Solve L0 Y^T = (X - mu)^T - Y = L0^{-1} (X - mu)\n    centered = X - mu\n    # Use solve for stability\n    YT = np.linalg.solve(L0, centered.T)\n    Y = YT.T\n    return Y\n\ndef bartlett_sphericity_test(Y: np.ndarray) - float:\n    \"\"\"Compute Bartlett's sphericity test p-value for the correlation matrix of Y.\"\"\"\n    n, p = Y.shape\n    R = np.corrcoef(Y, rowvar=False)\n    sign, logdet = np.linalg.slogdet(R)\n    if sign = 0:\n        # Singular or non-positive definite correlation matrix: infinite statistic, p-value 0\n        return 0.0\n    # Bartlett's test statistic\n    c = n - 1 - (2 * p + 5) / 6.0\n    stat = -c * logdet\n    df = p * (p - 1) // 2\n    pval = 1.0 - stats.chi2.cdf(stat, df)\n    return float(pval)\n\ndef pairwise_bonferroni_test(Y: np.ndarray, alpha: float) - (bool, float):\n    \"\"\"Perform all pairwise correlation tests with Bonferroni adjustment. Return (reject_any, min_adj_p).\"\"\"\n    n, p = Y.shape\n    m = p * (p - 1) // 2\n    # Compute correlation matrix\n    R = np.corrcoef(Y, rowvar=False)\n    df = n - 2\n    min_adj_p = 1.0\n    reject_any = False\n    for i in range(p):\n        for j in range(i + 1, p):\n            r = R[i, j]\n            # Guard against edge cases r ~ +/-1 leading to division by zero; clamp slightly\n            r = np.clip(r, -0.999999, 0.999999)\n            t_stat = r * np.sqrt(df / (1.0 - r ** 2))\n            pval = 2.0 * stats.t.sf(np.abs(t_stat), df)\n            adj_p = min(1.0, m * pval)\n            if adj_p  min_adj_p:\n                min_adj_p = adj_p\n            if adj_p  alpha:\n                reject_any = True\n    return reject_any, float(min_adj_p)\n\ndef detect_unintended_dependence(X: np.ndarray, mu: np.ndarray, Sigma0: np.ndarray, alpha: float) - bool:\n    \"\"\"Apply whitening, then Bartlett sphericity and pairwise Bonferroni tests; return boolean detection.\"\"\"\n    Y = whiten_samples(X, mu, Sigma0)\n    pval_bartlett = bartlett_sphericity_test(Y)\n    reject_bartlett = pval_bartlett  alpha\n    reject_pairs, _ = pairwise_bonferroni_test(Y, alpha)\n    return bool(reject_bartlett or reject_pairs)\n\ndef solve():\n    # Define significance level\n    alpha = 0.01\n\n    # Define the test cases from the problem statement.\n    # Each case: dict with keys: sampler, n, p, Sigma0, rho (optional), seed\n    # Case 1: Correct sampler, AR(1) covariance\n    p1 = 6\n    Sigma1 = make_cov_ar1(p1, phi=0.6)\n    case1 = {\n        \"sampler\": \"correct\",\n        \"n\": 1000,\n        \"p\": p1,\n        \"Sigma0\": Sigma1,\n        \"seed\": 12345\n    }\n\n    # Case 2: Shared stream sampler, same AR(1) covariance\n    p2 = 6\n    Sigma2 = make_cov_ar1(p2, phi=0.6)\n    case2 = {\n        \"sampler\": \"shared\",\n        \"n\": 300,\n        \"p\": p2,\n        \"Sigma0\": Sigma2,\n        \"seed\": 54321\n    }\n\n    # Case 3: Partial shared stream sampler, diagonal covariance\n    Sigma3 = np.diag([1.0, 1.5, 0.5, 2.0, 1.2, 0.8, 1.7, 1.1])\n    case3 = {\n        \"sampler\": \"partial\",\n        \"n\": 100,\n        \"p\": 8,\n        \"Sigma0\": Sigma3,\n        \"rho\": 0.3,\n        \"seed\": 2024\n    }\n\n    test_cases = [case1, case2, case3]\n\n    results = []\n    for case in test_cases:\n        sampler = case[\"sampler\"]\n        n = case[\"n\"]\n        Sigma0 = case[\"Sigma0\"]\n        p = case[\"p\"]\n        mu = np.zeros(p)\n        rng = np.random.default_rng(case[\"seed\"])\n\n        if sampler == \"correct\":\n            X = sample_correct(n, mu, Sigma0, rng)\n        elif sampler == \"shared\":\n            X = sample_shared_stream(n, mu, Sigma0, rng)\n        elif sampler == \"partial\":\n            rho = case[\"rho\"]\n            X = sample_partial_shared(n, mu, Sigma0, rho, rng)\n        else:\n            # Unknown sampler type; treat as failure to detect\n            X = sample_correct(n, mu, Sigma0, rng)\n\n        detected = detect_unintended_dependence(X, mu, Sigma0, alpha)\n        results.append(detected)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3322614"}]}