{"hands_on_practices": [{"introduction": "To truly understand the alias method, the first step is to move beyond the abstract description and perform the construction process by hand. This exercise guides you through the foundational Walker algorithm for a specific, illustrative probability distribution. By manually partitioning probabilities and creating the alias and probability tables, you will gain a concrete intuition for how the method elegantly reallocates probability mass to achieve constant-time sampling [@problem_id:3350550].", "problem": "Consider a discrete distribution on $n$ categories with probabilities $p_{1}, p_{2}, \\dots, p_{n}$, where $\\sum_{i=1}^{n} p_{i} = 1$. The alias method of Walker constructs two arrays for constant-time sampling: a probability array $q$ with entries in $[0,1]$ and an alias array $a$ consisting of integer indices in $\\{1,2,\\dots,n\\}$. The construction is based on partitioning categories via scaled probabilities $s_{i} = n p_{i}$ into “small” (those with $s_{i}  1$) and “large” (those with $s_{i} \\ge 1$), and then pairing small with large to conserve mass until all categories are resolved. The resulting arrays $(q,a)$ must, when used in the standard alias sampling scheme that selects a column uniformly from $\\{1,\\dots,n\\}$ and then chooses either the column’s primary index with probability $q$ or its alias otherwise, reproduce the original distribution $p$.\n\nFor the explicit case $n=5$ and $p=(0.01, 0.01, 0.01, 0.01, 0.96)$, carry out the Walker partition and pairing steps from first principles, beginning with the scaled probabilities $s_{i} = n p_{i}$. Determine the final probability array $q$ and alias array $a$ that exactly encode this distribution under the alias method and briefly justify that the constructed $(q,a)$ reproduces the original probabilities. Report the final arrays by listing their entries in the natural index order $1$ through $5$. The final answer must be a single mathematical object; present both arrays in one row in the order $(q_{1}, q_{2}, q_{3}, q_{4}, q_{5}, a_{1}, a_{2}, a_{3}, a_{4}, a_{5})$. No rounding is required.", "solution": "The user wants me to solve a problem that involves building lookup tables for the alias method. I have validated the problem and it is sound. Now I will solve it.\n\nThe problem requires the construction of the probability array $q$ and the alias array $a$ for a discrete distribution on $n=5$ outcomes with probabilities $p = (0.01, 0.01, 0.01, 0.01, 0.96)$. The construction must follow the principles of the Walker alias method.\n\nFirst, we define the scaled probabilities $s_i = n p_i$ for $i \\in \\{1, 2, 3, 4, 5\\}$. With $n=5$, we have:\n$s_1 = 5 \\times 0.01 = 0.05$\n$s_2 = 5 \\times 0.01 = 0.05$\n$s_3 = 5 \\times 0.01 = 0.05$\n$s_4 = 5 \\times 0.01 = 0.05$\n$s_5 = 5 \\times 0.96 = 4.80$\n\nThe sum of these scaled probabilities is $\\sum_{i=1}^5 s_i = 4 \\times 0.05 + 4.80 = 0.20 + 4.80 = 5 = n$, as expected.\n\nThe core of the algorithm involves partitioning the categories into two sets: “small” categories with $s_i  1$ and “large” categories with $s_i \\ge 1$. We maintain two worklists, $S$ for small and $L$ for large.\nInitially:\n$S = \\{1, 2, 3, 4\\}$ since $s_{1,2,3,4} = 0.05  1$.\n$L = \\{5\\}$ since $s_5 = 4.80 \\ge 1$.\n\nThe algorithm iteratively processes pairs of small and large categories. In each step, we pick one small category $i \\in S$ and one large category $j \\in L$. The probability slot for column $i$ is partially filled by its own scaled probability $s_i$. The remaining space, $1 - s_i$, is filled by \"donating\" probability mass from the large category $j$.\n\nLet's trace the iterative construction of the arrays $q$ and $a$. We will use the scaled probabilities $s_i$ as a work array that tracks remaining probability mass.\n\n**Initial State:**\nWorklists: $S = \\{1, 2, 3, 4\\}$, $L = \\{5\\}$\nScaled probabilities: $s = (0.05, 0.05, 0.05, 0.05, 4.80)$\nArrays to be built: $q, a$\n\n**Iteration 1:**\n1. Select a small category $i=1$ from $S$ and a large category $j=5$ from $L$.\n2. The probability for the primary outcome in column $1$ is set to its scaled probability: $q_1 = s_1 = 0.05$.\n3. The alias for column $1$ is the large category that donates mass: $a_1 = 5$.\n4. The mass required to fill column $1$ is $1 - s_1 = 1 - 0.05 = 0.95$. This mass is subtracted from the large category's mass: $s_5 \\leftarrow s_5 - (1-s_1) = 4.80 - 0.95 = 3.85$.\n5. Category $1$ is now fully processed. We remove it from $S$.\nState after Iteration 1:\nWorklists: $S = \\{2, 3, 4\\}$, $L = \\{5\\}$ (since $s_5 = 3.85 \\ge 1$)\nWorking probabilities: $s = (\\text{resolved}, 0.05, 0.05, 0.05, 3.85)$\nArrays: $q_1 = 0.05$, $a_1 = 5$.\n\n**Iteration 2:**\n1. Select small category $i=2$ from $S$ and large category $j=5$ from $L$.\n2. Set $q_2 = s_2 = 0.05$.\n3. Set $a_2 = 5$.\n4. Update $s_5 \\leftarrow s_5 - (1-s_2) = 3.85 - 0.95 = 2.90$.\n5. Remove $i=2$ from $S$.\nState after Iteration 2:\nWorklists: $S = \\{3, 4\\}$, $L = \\{5\\}$ (since $s_5 = 2.90 \\ge 1$)\nWorking probabilities: $s = (\\dots, \\text{resolved}, 0.05, 0.05, 2.90)$\nArrays: $q_1 = 0.05, q_2 = 0.05$, $a_1 = 5, a_2 = 5$.\n\n**Iteration 3:**\n1. Select small category $i=3$ from $S$ and large category $j=5$ from $L$.\n2. Set $q_3 = s_3 = 0.05$.\n3. Set $a_3 = 5$.\n4. Update $s_5 \\leftarrow s_5 - (1-s_3) = 2.90 - 0.95 = 1.95$.\n5. Remove $i=3$ from $S$.\nState after Iteration 3:\nWorklists: $S = \\{4\\}$, $L = \\{5\\}$ (since $s_5 = 1.95 \\ge 1$)\nWorking probabilities: $s = (\\dots, \\text{resolved}, 0.05, 1.95)$\nArrays: $q_{1,2,3} = 0.05$, $a_{1,2,3} = 5$.\n\n**Iteration 4:**\n1. Select small category $i=4$ from $S$ and large category $j=5$ from $L$.\n2. Set $q_4 = s_4 = 0.05$.\n3. Set $a_4 = 5$.\n4. Update $s_5 \\leftarrow s_5 - (1-s_4) = 1.95 - 0.95 = 1.00$.\n5. Remove $i=4$ from $S$.\nState after Iteration 4:\nWorklists: $S = \\emptyset$, $L = \\{5\\}$\nWorking probabilities: $s = (\\dots, \\text{resolved}, 1.00)$\nArrays: $q_{1,2,3,4} = 0.05$, $a_{1,2,3,4} = 5$.\n\n**Termination:**\nThe loop terminates as the small-category worklist $S$ is empty. At this point, any remaining categories (which must all be in $L$) are guaranteed to have a remaining scaled probability of exactly $1$. In our case, category $5$ remains with $s_5 = 1.00$. For such a category $k$, we set its probability slot to be filled entirely by itself, so $q_k = 1$. The alias $a_k$ will never be used, so it is conventionally set to $k$.\n\nFor $k=5$:\nSet $q_5 = 1.0$.\nSet $a_5 = 5$.\n\n**Final Arrays:**\nThe constructed arrays are:\nProbability array: $q = (0.05, 0.05, 0.05, 0.05, 1.0)$\nAlias array: $a = (5, 5, 5, 5, 5)$\n\n**Justification:**\nTo verify that these arrays reproduce the original probabilities $p_i$, we analyze the sampling process. When sampling, we first select a column $i \\in \\{1, \\dots, n\\}$ uniformly at random (with probability $1/n$), then sample outcome $i$ with probability $q_i$ or outcome $a_i$ with probability $1-q_i$.\n\nThe total probability $p'_k$ of sampling outcome $k$ is the sum of probabilities of all events leading to this outcome. An outcome $k$ can be generated from column $k$ (as the primary outcome) or from any column $i$ where $a_i=k$ (as the alias outcome).\nThe probability of getting $k$ from column $k$ is $\\frac{1}{n}q_k$.\nThe probability of getting $k$ from another column $i \\neq k$ (where $a_i=k$) is $\\frac{1}{n}(1-q_i)$.\nThe reconstructed probability for outcome $k$, $p'_k$, is:\n$$p'_k = \\frac{q_k}{n} + \\sum_{i: a_i=k, i \\neq k} \\frac{1-q_i}{n}$$\n\nLet's check this for our results ($n=5$):\n\nFor categories $k \\in \\{1, 2, 3, 4\\}$:\nIn our construction, none of these categories appear in the alias array $a$. The summation term is therefore zero.\n$p'_k = \\frac{q_k}{5} = \\frac{0.05}{5} = 0.01$. This matches the original $p_k$.\n\nFor category $k=5$:\nThe alias for columns $i=1, 2, 3, 4$ is $a_i=5$. Also, $a_5=5$. In this case, our formula is applied as:\n$p'_5 = \\frac{q_5}{5} + \\sum_{i \\in \\{1,2,3,4\\}} \\frac{1-q_i}{5}$\n$p'_5 = \\frac{1.0}{5} + \\frac{(1-0.05) + (1-0.05) + (1-0.05) + (1-0.05)}{5}$\n$p'_5 = \\frac{1.0}{5} + \\frac{4 \\times 0.95}{5} = \\frac{1.0 + 3.8}{5} = \\frac{4.8}{5} = 0.96$\nThis matches the original $p_5$.\n\nThe justification is complete. The constructed arrays $(q,a)$ correctly reproduce the original probability distribution. The final result is the combined list of elements from $q$ and $a$.", "answer": "$$ \\boxed{ \\begin{pmatrix} 0.05  0.05  0.05  0.05  1.0  5  5  5  5  5 \\end{pmatrix} } $$", "id": "3350550"}, {"introduction": "After mastering the mechanics of the alias method, it is crucial to understand its operational requirements and the consequences of violating them. This practice explores a common implementation pitfall: applying the algorithm to unnormalized weights, a situation that can arise from upstream numerical errors. By analyzing this hypothetical scenario, you will see how it can lead to a catastrophic failure in the sampling distribution and learn to quantify the resulting bias using the total variation distance [@problem_id:3350552].", "problem": "Consider a categorical random variable with three outcomes indexed by $i \\in \\{1,2,3\\}$. Due to floating-point accumulation error in an upstream computation, the input \"probabilities\" arrive as nonnegative weights $w_{i}$ that do not sum to $1$. Specifically, suppose the weights are\n$$\nw_{1} = \\frac{1}{3} + \\epsilon,\\quad w_{2} = \\frac{1}{3},\\quad w_{3} = \\frac{1}{3},\n$$\nwith $\\epsilon  0$ and the intended distribution is the normalized categorical distribution $p_{i} = w_{i} / \\sum_{j=1}^{3} w_{j}$. You aim to sample from $p$ using the alias method, which represents a categorical distribution by partitioning a unit-area rectangle into $3$ columns of equal width; each column is split into a primary index and an alias index so that the total area assigned to each outcome equals its probability.\n\nAssume an implementer erroneously constructs the alias table by skipping the normalization step and uses the unnormalized weights $w_{i}$ directly. Under this mistake, and using the standard column-area interpretation of the alias method, demonstrate why the resulting sampler becomes uniform over $\\{1,2,3\\}$ in this specific scenario, and then quantify the resulting bias by computing the total variation distance between the intended normalized distribution $p$ and the produced uniform distribution. The total variation distance between two discrete distributions $P$ and $Q$ on the same finite set is defined as\n$$\n\\operatorname{TV}(P,Q) = \\frac{1}{2}\\sum_{i} \\left| P(i) - Q(i) \\right|.\n$$\nProvide the exact closed-form expression for $\\operatorname{TV}(p, U)$ as a function of $\\epsilon$, where $U$ denotes the uniform distribution on $\\{1,2,3\\}$. Your final answer must be a single closed-form expression; do not approximate and do not round.", "solution": "The problem presents a scenario where the alias method for sampling from a discrete distribution is implemented incorrectly. Specifically, unnormalized weights are used instead of probabilities that sum to $1$. We must first demonstrate why this error leads to sampling from a uniform distribution in the given case, and second, quantify the resulting bias using the total variation distance.\n\nLet the number of outcomes be $n=3$. The unnormalized, non-negative weights are given as\n$$\nw_{1} = \\frac{1}{3} + \\epsilon, \\quad w_{2} = \\frac{1}{3}, \\quad w_{3} = \\frac{1}{3}\n$$\nwhere $\\epsilon  0$.\n\nThe standard alias method algorithm begins by scaling the input probabilities $p_i$ by the number of outcomes $n$. In this erroneous implementation, the weights $w_i$ are used in place of probabilities. The scaled weights, which we denote by $w'_i$, are:\n$$\nw'_{i} = n \\cdot w_{i} = 3 \\cdot w_{i}\n$$\nSubstituting the given values, we find:\n$$\nw'_{1} = 3 \\left(\\frac{1}{3} + \\epsilon\\right) = 1 + 3\\epsilon\n$$\n$$\nw'_{2} = 3 \\left(\\frac{1}{3}\\right) = 1\n$$\n$$\nw'_{3} = 3 \\left(\\frac{1}{3}\\right) = 1\n$$\nThe next step in the algorithm (e.g., Vose's algorithm) is to partition the indices $\\{1, 2, 3\\}$ into two worklists: a `Small` list for indices $i$ where the scaled value is less than $1$, and a `Large` list for indices where the scaled value is greater than or equal to $1$. Based on our calculated $w'_i$:\n- Since $w'_{1} = 1 + 3\\epsilon  1$ (as $\\epsilon  0$), index $1$ goes into the `Large` list.\n- Since $w'_{2} = 1$, index $2$ goes into the `Large` list.\n- Since $w'_{3} = 1$, index $3$ goes into the `Large` list.\n\nThe `Small` worklist is empty, and the `Large` worklist contains all indices, i.e., $\\{1, 2, 3\\}$.\n\nThe main loop of the alias table construction algorithm proceeds by taking one item from the `Small` list and one from the `Large` list to balance them out. Since the `Small` list is empty, this main loop is never executed. The algorithm then finalizes the construction. A standard implementation handles any remaining items in the `Large` list by setting their corresponding probability table entries to $1$. Thus, the `Prob` table is populated as:\n$$\n\\text{Prob}[1] = 1, \\quad \\text{Prob}[2] = 1, \\quad \\text{Prob}[3] = 1\n$$\nThe `Alias` table is never written to, so its contents are irrelevant.\n\nNow, we analyze the sampling procedure with this erroneously constructed table. To generate a sample, the alias method performs two steps:\n1. Select a column index $i \\in \\{1, 2, 3\\}$ uniformly at random. The probability of choosing any specific $i$ is $\\frac{1}{n} = \\frac{1}{3}$.\n2. Generate a uniform random number $U \\sim \\text{Uniform}(0, 1)$. If $U  \\text{Prob}[i]$, the sampled outcome is $i$. Otherwise, the outcome is $\\text{Alias}[i]$.\n\nIn our case, $\\text{Prob}[i] = 1$ for all $i \\in \\{1, 2, 3\\}$. Since a random variable $U$ from a continuous distribution on $[0,1]$ satisfies $U  1$ with probability $1$, the condition $U  \\text{Prob}[i]$ is always met. Consequently, the sampler will always return the index $i$ that was chosen in the first step. Because the index $i$ is chosen uniformly at random from $\\{1, 2, 3\\}$, the resulting sampled outcomes follow a uniform distribution on $\\{1, 2, 3\\}$. We denote this distribution by $U$, where $U(i) = \\frac{1}{3}$ for $i \\in \\{1, 2, 3\\}$.\n\nNext, we quantify the bias by computing the total variation distance between the intended distribution $p$ and the produced uniform distribution $U$. The intended distribution $p$ is obtained by normalizing the weights $w_i$. The sum of the weights is:\n$$\n\\sum_{j=1}^{3} w_{j} = \\left(\\frac{1}{3} + \\epsilon\\right) + \\frac{1}{3} + \\frac{1}{3} = 1 + \\epsilon\n$$\nThe probabilities of the intended distribution $p$ are therefore:\n$$\np_{1} = \\frac{w_{1}}{\\sum w_{j}} = \\frac{\\frac{1}{3} + \\epsilon}{1 + \\epsilon}\n$$\n$$\np_{2} = \\frac{w_{2}}{\\sum w_{j}} = \\frac{\\frac{1}{3}}{1 + \\epsilon}\n$$\n$$\np_{3} = \\frac{w_{3}}{\\sum w_{j}} = \\frac{\\frac{1}{3}}{1 + \\epsilon}\n$$\nThe uniform distribution $U$ has probabilities $U(i) = \\frac{1}{3}$ for each outcome $i \\in \\{1, 2, 3\\}$. The total variation distance is defined as:\n$$\n\\operatorname{TV}(p, U) = \\frac{1}{2}\\sum_{i=1}^{3} |p_{i} - U(i)|\n$$\nWe compute the differences $p_i - U(i)$:\n$$\np_{1} - U(1) = \\frac{\\frac{1}{3} + \\epsilon}{1 + \\epsilon} - \\frac{1}{3} = \\frac{3(\\frac{1}{3} + \\epsilon) - (1 + \\epsilon)}{3(1 + \\epsilon)} = \\frac{1 + 3\\epsilon - 1 - \\epsilon}{3(1 + \\epsilon)} = \\frac{2\\epsilon}{3(1+\\epsilon)}\n$$\n$$\np_{2} - U(2) = \\frac{\\frac{1}{3}}{1 + \\epsilon} - \\frac{1}{3} = \\frac{1 - (1 + \\epsilon)}{3(1 + \\epsilon)} = \\frac{-\\epsilon}{3(1+\\epsilon)}\n$$\n$$\np_{3} - U(3) = \\frac{\\frac{1}{3}}{1 + \\epsilon} - \\frac{1}{3} = \\frac{-\\epsilon}{3(1+\\epsilon)}\n$$\nSince $\\epsilon  0$, the first difference is positive, and the other two are negative. Now we sum the absolute values of these differences:\n$$\n\\sum_{i=1}^{3} |p_{i} - U(i)| = \\left|\\frac{2\\epsilon}{3(1+\\epsilon)}\\right| + \\left|\\frac{-\\epsilon}{3(1+\\epsilon)}\\right| + \\left|\\frac{-\\epsilon}{3(1+\\epsilon)}\\right|\n$$\n$$\n= \\frac{2\\epsilon}{3(1+\\epsilon)} + \\frac{\\epsilon}{3(1+\\epsilon)} + \\frac{\\epsilon}{3(1+\\epsilon)} = \\frac{4\\epsilon}{3(1+\\epsilon)}\n$$\nFinally, we substitute this sum into the formula for total variation distance:\n$$\n\\operatorname{TV}(p, U) = \\frac{1}{2} \\left( \\frac{4\\epsilon}{3(1+\\epsilon)} \\right) = \\frac{2\\epsilon}{3(1+\\epsilon)}\n$$\nThis is the exact closed-form expression for the total variation distance between the intended distribution and the uniform distribution produced by the faulty sampler.", "answer": "$$\n\\boxed{\\frac{2\\epsilon}{3(1+\\epsilon)}}\n$$", "id": "3350552"}, {"introduction": "Translating an algorithm from theory to robust, real-world code requires confronting the challenges of finite-precision arithmetic. This advanced practice guides you through implementing a numerically stable alias sampler and validating its correctness against distributions specifically designed to stress floating-point precision. By building a statistical validation protocol based on concentration inequalities, you will develop practical skills in ensuring the reliability and accuracy of scientific computing software [@problem_id:3350533].", "problem": "You are tasked with designing and validating a numerically robust implementation of the alias method for sampling from a discrete probability mass function. The alias method is a constant-time sampler that represents a discrete distribution over $n$ categories using a pair of tables and draws a sample via two independent uniform random variates. Your objective is to construct distributions that are maximally challenging for numerical alias construction and then verify, via a concentration-based protocol, that empirical frequencies from the alias sampler match the target probabilities within a prescribed tolerance after $M$ samples.\n\nStart from the following fundamental base:\n- A discrete distribution over $n$ categories is specified by a probability mass function $\\{p_i\\}_{i=1}^n$ satisfying $p_i \\ge 0$ and $\\sum_{i=1}^n p_i = 1$.\n- Independent and identically distributed (i.i.d.) sampling from a discrete distribution produces empirical frequencies $\\{\\hat{p}_i\\}_{i=1}^n$ that converge to $\\{p_i\\}_{i=1}^n$ by the Law of Large Numbers (LLN).\n- A tail bound such as Hoeffding's inequality implies that, for each $i$, the deviation $\\lvert \\hat{p}_i - p_i \\rvert$ is small with high probability as $M$ grows, and a union bound provides a joint guarantee over all $i \\in \\{1,\\dots,n\\}$.\n\nConstruct two worst-case families of discrete distributions intended to be numerically error-prone for alias-table construction due to floating-point precision limits:\n1. Near-uniform alternating perturbations: for a given $n$ and a small $\\delta  0$, define\n$$\np_i = \\frac{1}{n} + \\frac{(-1)^i \\,\\delta}{n} \\quad \\text{for } i = 1,2,\\dots,n-1,\n$$\nand set $p_n = 1 - \\sum_{i=1}^{n-1} p_i$ so that $\\sum_{i=1}^n p_i = 1$. Choose $\\delta$ sufficiently small so that all $p_i$ remain nonnegative.\n2. One-dominant extreme: for a given $n$ and a tiny $\\varepsilon  0$, define\n$$\np_1 = 1 - (n-1)\\varepsilon, \\quad p_i = \\varepsilon \\quad \\text{for } i = 2,\\dots,n,\n$$\nwhich creates one category with probability near $1$ and $n-1$ categories with vanishingly small probability.\n\nImplement a numerically robust alias construction that prevents pathological floating-point behavior (for example, negative probabilities due to subtractive cancellation or probabilities exceeding one due to accumulation), while preserving correctness of the sampling distribution.\n\nValidation protocol. For each constructed distribution and sample size $M$, compute a tolerance $\\epsilon^\\star$ based on a concentration inequality and a union bound over all $n$ categories. Verify that the empirical frequencies $\\hat{p}_i$ obtained from $M$ independent alias-method samples satisfy\n$$\n\\max_{1 \\le i \\le n} \\lvert \\hat{p}_i - p_i \\rvert \\le \\epsilon^\\star\n$$\nfor the chosen risk level. The validation should return a boolean that is `true` if the above inequality holds and `false` otherwise.\n\nTest suite. Your program must implement the above for the following parameter sets, each with a fixed pseudo-random number generator seed to make the outcome reproducible:\n- Case A (near-uniform alternating): $n = 128$, $\\delta = 10^{-15}$, $M = 200000$, risk level $\\alpha = 0.01$, seed $12345$.\n- Case B (one dominant): $n = 64$, $\\varepsilon = 10^{-16}$, $M = 400000$, risk level $\\alpha = 0.01$, seed $54321$.\n- Case C (near-uniform alternating, odd $n$): $n = 1001$, $\\delta = 10^{-15}$, $M = 200000$, risk level $\\alpha = 0.02$, seed $314159$.\n- Case D (uniform baseline): $n = 5$, $M = 100000$, risk level $\\alpha = 0.01$, seed $42$; here $p_i = 1/n$ for all $i$.\n- Case E (binary extreme): $n = 2$, $\\varepsilon = 10^{-16}$, $M = 200000$, risk level $\\alpha = 0.01$, seed $2024$.\n\nOutput specification. Your program should produce a single line of output containing the validation results for the five cases as a comma-separated list enclosed in square brackets, in the order A through E, where each element is a boolean value indicating whether the validation protocol passed for that case (for example, \"[True,False,True,True,False]\"). No other text should be printed.", "solution": "We first recall the discrete probability mass function structure and independent sampling: a discrete distribution over $n$ categories is specified by $\\{p_i\\}_{i=1}^n$ with $p_i \\ge 0$ and $\\sum_{i=1}^n p_i = 1$. When we perform $M$ i.i.d. samples from this distribution, the empirical frequency $\\hat{p}_i$ for category $i$ is the count $C_i$ of occurrences of $i$ divided by $M$, that is $\\hat{p}_i = C_i / M$. The Law of Large Numbers (LLN) implies that $\\hat{p}_i \\to p_i$ as $M \\to \\infty$, and concentration inequalities quantify the deviation for finite $M$.\n\nThe alias method constructs a constant-time sampler that uses two arrays for the $n$ categories: a table of acceptance probabilities and a table of alias indices. Sampling proceeds by drawing one discrete index uniformly from $\\{1,\\dots,n\\}$ and then, via a second uniform draw, deciding whether to accept that index or switch to its alias. The construction is derived from the requirement that, for each column $j$, the combination of its acceptance probability and the redirected probability to its alias must preserve the overall distribution. The fundamental invariant is that each column contributes exactly $\\frac{1}{n}$ of probability mass, divided between its own index and its alias index.\n\nTo construct the alias tables from $\\{p_i\\}$, define scaled probabilities $q_i = n p_i$. Partition the indices into two sets: those with $q_i  1$ (small) and those with $q_i \\ge 1$ (large). Iteratively pair a small index $s$ with a large index $l$, assign the acceptance probability for $s$ as $q_s$, set the alias of $s$ as $l$, and then reduce $q_l$ by $(1 - q_s)$ to account for the mass transferred to $s$. Depending on whether the updated $q_l$ is below or above $1$, return $l$ to the appropriate set. Continue until at most one set has elements left. Assign acceptance probabilities of $1$ for any remaining indices and their aliases to themselves, which means they always accept. This algorithm is derived from the invariant that each column must allocate a unit of scaled mass across its own index and a paired alias without violating nonnegativity and normalization.\n\nNumerical stability concerns arise from floating-point arithmetic in the construction loop:\n- When $q_s$ is extremely close to $1$, the subtraction $(1 - q_s)$ suffers from subtractive cancellation, potentially producing values smaller than machine epsilon, which can accumulate rounding errors in the update $q_l \\leftarrow q_l - (1 - q_s)$.\n- When there is one dominant $p_1 \\approx 1$, repeated subtraction of tiny $(1 - q_s)$ values from a large $q_l$ can cause $q_l$ to drift slightly below $1$ or below $0$ due to rounding.\n- When probabilities are near the threshold $1/n$ (i.e., $q_i \\approx 1$), branches that decide whether $q_i$ is in small or large can flip due to tiny rounding differences, destabilizing the partition process.\n\nTo mitigate these issues, the implementation should:\n- Use double precision (that is, 64-bit floating-point numbers) throughout and avoid unnecessary conversions.\n- Introduce a tolerance $t$ on comparisons to $1$, classifying $q_i$ as small if $q_i  1 - t$ and large if $q_i  1 + t$, and allowing either classification in the narrow band $[1 - t, 1 + t]$ based on the current algorithmic needs.\n- Clip acceptance probabilities to $[0,1]$ at the end to counteract tiny negative values or slight overshoots from floating-point updates.\n- For any residual elements after the loop, set acceptance probability to $1$ and alias to self, which preserves the invariant of unit mass per column.\n\nWorst-case distributions for numerical alias construction. The two families specified stress the algorithm's arithmetic:\n1. Near-uniform alternating perturbations:\n$$\np_i = \\frac{1}{n} + \\frac{(-1)^i \\,\\delta}{n} \\quad \\text{for } i = 1,2,\\dots,n-1, \\qquad p_n = 1 - \\sum_{i=1}^{n-1} p_i.\n$$\nThis choice forces most $q_i = n p_i$ to be extremely close to $1$, triggering subtractive cancellation and unstable classification between small and large. The alternating signs cause frequent near-threshold transitions during the pairing steps, increasing sensitivity to floating-point comparisons.\n\n2. One-dominant extreme:\n$$\np_1 = 1 - (n-1)\\varepsilon, \\quad p_i = \\varepsilon \\quad \\text{for } i = 2,\\dots,n.\n$$\nHere, $q_1 \\approx n$ and $q_i \\approx n \\varepsilon \\ll 1$ for $i \\ge 2$. The construction repeatedly subtracts $(1 - q_s) \\approx 1$ from $q_1$, accumulating rounding errors and risking that $q_1$ drifts slightly below $1$ or becomes negative due to finite precision if not handled carefully.\n\nValidation protocol derived from concentration inequalities. For each category $i$, the count $C_i$ from $M$ i.i.d. samples is a binomial random variable with parameters $M$ and $p_i$. Hoeffding's inequality states that, for any $\\epsilon  0$,\n$$\n\\Pr\\left(\\lvert \\hat{p}_i - p_i \\rvert \\ge \\epsilon \\right) \\le 2 \\exp(-2 M \\epsilon^2).\n$$\nApplying the union bound over all $n$ categories gives\n$$\n\\Pr\\left( \\max_{1 \\le i \\le n} \\lvert \\hat{p}_i - p_i \\rvert \\ge \\epsilon \\right) \\le 2 n \\exp(-2 M \\epsilon^2).\n$$\nTo ensure this probability is at most a chosen risk level $\\alpha$, set $\\epsilon^\\star$ as the smallest nonnegative value satisfying $2 n \\exp(-2 M (\\epsilon^\\star)^2) \\le \\alpha$. Solving for $\\epsilon^\\star$ yields\n$$\n\\epsilon^\\star = \\sqrt{\\frac{\\ln \\left(\\frac{2 n}{\\alpha}\\right)}{2 M}}.\n$$\nWith this $\\epsilon^\\star$, we accept the alias sampler's output if\n$$\n\\max_{1 \\le i \\le n} \\lvert \\hat{p}_i - p_i \\rvert \\le \\epsilon^\\star.\n$$\nThis validation rule is independent of the alias method's internal mechanics and depends only on the sample size $M$, the number of categories $n$, and the desired risk level $\\alpha$, making it widely applicable.\n\nTest suite and expected behavior. The specified cases cover:\n- A near-uniform alternating scenario with $n = 128$ and $\\delta = 10^{-15}$, stressing the threshold logic.\n- A one-dominant scenario with $n = 64$ and $\\varepsilon = 10^{-16}$, stressing accumulated subtraction on a very large $q_1$.\n- A near-uniform alternating scenario with odd $n = 1001$ and $\\delta = 10^{-15}$, which requires careful adjustment of $p_n$ to preserve nonnegativity and the sum to $1$.\n- A uniform baseline with $n = 5$, where all $q_i = 1$ exactly in infinite precision, serving as a control case.\n- A binary extreme with $n = 2$ and $\\varepsilon = 10^{-16}$, directly probing the edge case of minimal dimension.\n\nThe program will:\n- Construct the specified distributions.\n- Build numerically robust alias tables.\n- Draw $M$ samples via the alias sampler with the given seed.\n- Compute $\\epsilon^\\star$ using the formula above and test whether the maximum absolute deviation across categories is within $\\epsilon^\\star$.\n- Output a list of five boolean values in the specified single-line format.\n\nGiven the sample sizes $M$ and the chosen $\\alpha$ across the test suite, Hoeffding's bound yields tolerances $\\epsilon^\\star$ that are much larger than typical sampling fluctuations, so the validation is expected to pass in all cases with the fixed seeds. Nevertheless, the program computes the outcome empirically and reports it per case.", "answer": "```python\nimport numpy as np\n\ndef build_alias(probabilities):\n    \"\"\"\n    Construct alias tables (prob and alias) for a given discrete distribution.\n    Uses numerically robust handling with tolerance thresholds and clipping.\n    \"\"\"\n    p = np.array(probabilities, dtype=np.float64)\n    n = p.size\n\n    # Scale probabilities\n    q = p * n\n\n    # Tolerance for comparisons around 1.0\n    tiny = np.finfo(np.float64).eps\n    tol = 10 * tiny  # Conservative band around 1\n\n    small = []\n    large = []\n    for i, qi in enumerate(q):\n        if qi  1.0 - tol:\n            small.append(i)\n        elif qi  1.0 + tol:\n            large.append(i)\n        else:\n            # Near 1: put in large to minimize aliasing; either choice is acceptable\n            large.append(i)\n\n    prob = np.zeros(n, dtype=np.float64)\n    alias = np.zeros(n, dtype=np.int64)\n\n    # Process pairs\n    while small and large:\n        s = small.pop()\n        l = large.pop()\n\n        # Acceptance probability for s\n        prob[s] = q[s]\n        alias[s] = l\n\n        # Update q[l] by the leftover mass from s\n        ql_new = q[l] - (1.0 - q[s])\n\n        # Numerical safety: avoid tiny negative or overshoot\n        # We do not clip here to keep dynamics, but use tolerance classification.\n        q[l] = ql_new\n\n        if q[l]  1.0 - tol:\n            small.append(l)\n        elif q[l]  1.0 + tol:\n            large.append(l)\n        else:\n            # Close enough to 1: finalize later\n            large.append(l)\n\n    # Remaining entries: set acceptance to 1 and alias to self\n    for idx in (large + small):\n        prob[idx] = 1.0\n        alias[idx] = idx\n\n    # Final safety: clip acceptance probabilities to [0,1]\n    prob = np.clip(prob, 0.0, 1.0)\n\n    # If any alias is unset (shouldn't happen), set to self\n    for i in range(n):\n        if alias[i]  0 or alias[i] = n:\n            alias[i] = i\n\n    return prob, alias\n\n\ndef sample_alias(prob, alias, M, rng):\n    \"\"\"\n    Draw M samples using the alias method defined by prob and alias tables.\n    \"\"\"\n    n = prob.size\n    # Draw columns uniformly at random\n    j = rng.integers(0, n, size=M, endpoint=False)\n    # Draw uniform for acceptance\n    u = rng.random(size=M, dtype=np.float64)\n    # Accept if u  prob[j], else take alias[j]\n    accept = u  prob[j]\n    samples = np.where(accept, j, alias[j])\n    return samples\n\n\ndef worst_case_alternating(n, delta):\n    \"\"\"\n    Construct near-uniform alternating perturbation distribution.\n    p_i = 1/n + (-1)^i * delta / n for i=1..n-1, and p_n adjusted to sum to 1.\n    Ensures nonnegativity by clipping minimal negatives to tiny positive and renormalizing.\n    \"\"\"\n    base = 1.0 / n\n    p = np.empty(n, dtype=np.float64)\n    # i from 0 to n-2 corresponds to indices 1..n-1 in math\n    for i in range(n - 1):\n        sign = -1 if ((i + 1) % 2 == 0) else 1  # (-1)^(i+1)\n        p[i] = base + sign * (delta / n)\n    p[n - 1] = 1.0 - np.sum(p[:n - 1])\n\n    # Guard against tiny negative due to floating error\n    tiny = np.finfo(np.float64).eps\n    if np.any(p  0):\n        p = np.maximum(p, tiny)\n        p /= np.sum(p)\n\n    return p\n\n\ndef worst_case_one_dominant(n, eps):\n    \"\"\"\n    Construct one-dominant extreme distribution:\n    p_1 = 1 - (n-1)*eps, p_i = eps for i=2.\n    \"\"\"\n    p = np.full(n, eps, dtype=np.float64)\n    p[0] = 1.0 - (n - 1) * eps\n    # Numerical guard: if rounding creates slight negative, clip and renormalize\n    tiny = np.finfo(np.float64).eps\n    p = np.maximum(p, tiny)\n    p /= np.sum(p)\n    return p\n\n\ndef uniform_distribution(n):\n    return np.full(n, 1.0 / n, dtype=np.float64)\n\n\ndef validation_tolerance(n, M, alpha):\n    \"\"\"\n    Compute epsilon* via union-bound Hoeffding:\n    epsilon* = sqrt( ln(2n/alpha) / (2M) ).\n    \"\"\"\n    return float(np.sqrt(np.log((2.0 * n) / alpha) / (2.0 * M)))\n\n\ndef validate_case(p, M, alpha, seed):\n    \"\"\"\n    Build alias tables, draw M samples, compute empirical frequencies,\n    and check max deviation against epsilon*.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    prob, alias = build_alias(p)\n    samples = sample_alias(prob, alias, M, rng)\n    n = p.size\n    counts = np.bincount(samples, minlength=n)\n    freqs = counts.astype(np.float64) / float(M)\n\n    eps_star = validation_tolerance(n, M, alpha)\n    max_dev = np.max(np.abs(freqs - p))\n    return max_dev = eps_star\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: near-uniform alternating\n        (\"alternating\", {\"n\": 128, \"delta\": 1e-15, \"M\": 200000, \"alpha\": 0.01, \"seed\": 12345}),\n        # Case B: one dominant\n        (\"one_dominant\", {\"n\": 64, \"eps\": 1e-16, \"M\": 400000, \"alpha\": 0.01, \"seed\": 54321}),\n        # Case C: near-uniform alternating, odd n\n        (\"alternating\", {\"n\": 1001, \"delta\": 1e-15, \"M\": 200000, \"alpha\": 0.02, \"seed\": 314159}),\n        # Case D: uniform baseline\n        (\"uniform\", {\"n\": 5, \"M\": 100000, \"alpha\": 0.01, \"seed\": 42}),\n        # Case E: binary extreme\n        (\"one_dominant\", {\"n\": 2, \"eps\": 1e-16, \"M\": 200000, \"alpha\": 0.01, \"seed\": 2024}),\n    ]\n\n    results = []\n    for kind, params in test_cases:\n        n = params[\"n\"]\n        if kind == \"alternating\":\n            p = worst_case_alternating(n, params[\"delta\"])\n        elif kind == \"one_dominant\":\n            p = worst_case_one_dominant(n, params[\"eps\"])\n        elif kind == \"uniform\":\n            p = uniform_distribution(n)\n        else:\n            raise ValueError(\"Unknown distribution kind\")\n\n        passed = validate_case(p, params[\"M\"], params[\"alpha\"], params[\"seed\"])\n        results.append(passed)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3350533"}]}