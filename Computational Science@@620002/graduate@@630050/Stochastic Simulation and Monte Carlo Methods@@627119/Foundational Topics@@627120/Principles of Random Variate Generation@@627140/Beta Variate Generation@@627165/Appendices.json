{"hands_on_practices": [{"introduction": "The ability to generate random variates from a target distribution is a cornerstone of stochastic simulation. This exercise guides you through implementing one of the most elegant methods for generating Beta variates: the ratio-of-gammas technique. More importantly, it emphasizes that implementation must be paired with rigorous validation, tasking you with designing a comprehensive test suite to verify the generator's statistical fidelity across diverse and challenging parameter regimes [@problem_id:3292091].", "problem": "You are tasked with constructing a robust generator for random variates from the Beta distribution and designing unit tests that stress this generator across challenging parameter regimes. The work must be grounded in first principles and well-tested facts and must not rely on any black-box Beta sampler. Your program must be a complete, runnable implementation that produces the final results for a specified test suite in a single line of output.\n\nBegin from the following fundamental base:\n- The Beta distribution with shape parameters $agt;0$ and $bgt;0$ has probability density function $f(x)=\\dfrac{x^{a-1}(1-x)^{b-1}}{B(a,b)}$ for $x\\in(0,1)$, where $B(a,b)$ is the Beta function.\n- The Beta function is related to the Gamma function by $B(a,b)=\\dfrac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$, and the Gamma function is defined by $\\Gamma(s)=\\int_{0}^{\\infty}x^{s-1}e^{-x}\\,dx$ for $sgt;0$.\n- The Gamma distribution with shape $kgt;0$ and unit scale has density $g(x)=\\dfrac{x^{k-1}e^{-x}}{\\Gamma(k)}$ for $xgt;0$.\n- Independence and transformation of random variables, together with Jacobian-based change-of-variables, are valid tools to derive distributions of transformed variables.\n\nUsing only the above, derive a mathematically correct algorithm to generate a random variate from $\\operatorname{Beta}(a,b)$ and implement it. Next, devise unit tests that verify statistical fidelity of your generator under the following regimes:\n1. Small-shape regime: $a,b\\ll 1$.\n2. Large-shape regime: $a,b\\gg 1$.\n3. Highly asymmetric shapes: one of $a$ or $b$ $\\ll 1$, the other $\\gg 1$.\n\nYour tests must be quantitative, reproducible, and scientifically sound. They must verify distributional fidelity through:\n- Sample mean accuracy against the theoretical mean $E[X]=\\dfrac{a}{a+b}$, using a tolerance derived from the sampling standard error $\\sqrt{\\operatorname{Var}(X)/n}$, where $\\operatorname{Var}(X)=\\dfrac{ab}{(a+b)^2(a+b+1)}$ and $n$ is the sample size.\n- Where specified, a Kolmogorov–Smirnov (K-S) goodness-of-fit test against the exact $\\operatorname{Beta}(a,b)$ cumulative distribution function, with an explicit numerical threshold on the K-S statistic.\n- Tail-probability checks in asymmetric regimes: compare the empirical tail frequency with the exact cumulative distribution function at a specified threshold and accept if the absolute discrepancy is within a binomial sampling standard error tolerance.\n\nDesign a test suite that includes the following cases, each with scientifically justified tolerances:\n- Case A (small-shape): $(a,b)=(0.1,0.1)$, sample size $n=80000$, accept if the K-S statistic $D$ satisfies $D\\leq 0.02$.\n- Case B (large-shape): $(a,b)=(100,100)$, sample size $n=120000$, accept if the absolute error of the sample mean is at most $5$ times the sampling standard error and the relative error of the sample variance is at most $0.15$.\n- Case C (asymmetric, left-heavy): $(a,b)=(0.2,5)$, sample size $n=100000$, accept both of the following simultaneously: the sample mean error is at most $5$ times the sampling standard error, and the discrepancy between empirical frequency of $X\\leq 0.01$ and the exact cumulative distribution value at $x=0.01$ is at most $4\\sqrt{p(1-p)/n}$, where $p$ is the exact cumulative probability at $x=0.01$.\n- Case D (asymmetric, right-heavy): $(a,b)=(5,0.2)$, sample size $n=100000$, accept both of the following simultaneously: the sample mean error is at most $5$ times the sampling standard error, and the discrepancy between empirical frequency of $X\\geq 0.99$ and the exact upper-tail probability at $x=0.99$ is at most $4\\sqrt{p(1-p)/n}$, where $p$ is the exact upper-tail probability at $x=0.99$.\n- Case E (extremely small-shape stress): $(a,b)=(0.01,0.01)$, sample size $n=120000$, accept if the K-S statistic $D$ satisfies $D\\leq 0.03$.\n\nAll numerical thresholds ($0.02$, $0.03$, $5$, $0.15$, $4$) must be implemented exactly as stated.\n\nFinal output format requirement:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"), where each result is a boolean for the corresponding case in the order A, B, C, D, E indicating whether the test passed (True) or failed (False). No additional text may be printed.", "solution": "The problem requires the derivation and implementation of a random variate generator for the Beta distribution, $\\operatorname{Beta}(a,b)$, from first principles, without using a pre-existing Beta sampler. The solution's fidelity must then be verified against a series of rigorous, quantitative unit tests for specified parameter regimes.\n\nThe fundamental principle for generating Beta variates is its relationship with the Gamma distribution. This relationship can be formally derived using the change of variables method in probability theory.\n\nLet $Y_1$ and $Y_2$ be two independent random variables following Gamma distributions with unit scale. Specifically, let $Y_1 \\sim \\operatorname{Gamma}(a, 1)$ and $Y_2 \\sim \\operatorname{Gamma}(b, 1)$, where $a  0$ and $b  0$ are the shape parameters. According to the problem statement, the probability density function (PDF) for a $\\operatorname{Gamma}(k, 1)$ variable $Y$ is $g(y) = \\frac{y^{k-1}e^{-y}}{\\Gamma(k)}$ for $y  0$.\n\nThe respective PDFs for $Y_1$ and $Y_2$ are:\n$$g_1(y_1) = \\frac{y_1^{a-1}e^{-y_1}}{\\Gamma(a)}, \\quad y_1  0$$\n$$g_2(y_2) = \\frac{y_2^{b-1}e^{-y_2}}{\\Gamma(b)}, \\quad y_2  0$$\nDue to their independence, their joint PDF is the product of their individual PDFs:\n$$f_{Y_1, Y_2}(y_1, y_2) = \\frac{y_1^{a-1} y_2^{b-1} e^{-(y_1+y_2)}}{\\Gamma(a)\\Gamma(b)}, \\quad y_1  0, y_2  0$$\n\nWe define a new random variable $X$ as the transformation $X = \\frac{Y_1}{Y_1 + Y_2}$. To find the distribution of $X$, we introduce an auxiliary variable $S = Y_1 + Y_2$. We then find the joint distribution of $(X, S)$ and marginalize over $S$.\n\nThe inverse transformation from $(x, s)$ to $(y_1, y_2)$ is:\n$$Y_1 = XS$$\n$$Y_2 = S - Y_1 = S - XS = S(1 - X)$$\nThe domain of the new variables is determined by $y_1  0$ and $y_2  0$. This implies $s  0$ and $0  x  1$.\n\nThe Jacobian of this transformation is the determinant of the matrix of partial derivatives:\n$$J = \\det \\begin{pmatrix} \\frac{\\partial y_1}{\\partial x}  \\frac{\\partial y_1}{\\partial s} \\\\ \\frac{\\partial y_2}{\\partial x}  \\frac{\\partial y_2}{\\partial s} \\end{pmatrix} = \\det \\begin{pmatrix} s  x \\\\ -s  1-x \\end{pmatrix} = s(1-x) - x(-s) = s$$\nThe absolute value of the Jacobian is $|J| = s$, since $s  0$.\n\nThe joint PDF of $(X, S)$ is given by $f_{X,S}(x, s) = f_{Y_1, Y_2}(xs, s(1-x))|J|$. Substituting the expressions for $y_1$, $y_2$, and $|J|$:\n$$f_{X,S}(x, s) = \\frac{(xs)^{a-1} (s(1-x))^{b-1} e^{-(xs + s(1-x))}}{\\Gamma(a)\\Gamma(b)} \\cdot s$$\n$$f_{X,S}(x, s) = \\frac{x^{a-1} s^{a-1} s^{b-1} (1-x)^{b-1} e^{-s}}{\\Gamma(a)\\Gamma(b)} \\cdot s$$\n$$f_{X,S}(x, s) = \\frac{x^{a-1}(1-x)^{b-1}}{\\Gamma(a)\\Gamma(b)} s^{a+b-1} e^{-s}$$\nThis joint PDF is valid for $0  x  1$ and $s  0$.\n\nTo find the marginal PDF of $X$, $f_X(x)$, we integrate $f_{X,S}(x, s)$ with respect to $s$ over its entire domain $(0, \\infty)$:\n$$f_X(x) = \\int_{0}^{\\infty} \\frac{x^{a-1}(1-x)^{b-1}}{\\Gamma(a)\\Gamma(b)} s^{a+b-1} e^{-s} \\,ds$$\nThe term involving $x$ is constant with respect to $s$, so we factor it out:\n$$f_X(x) = \\frac{x^{a-1}(1-x)^{b-1}}{\\Gamma(a)\\Gamma(b)} \\int_{0}^{\\infty} s^{(a+b)-1} e^{-s} \\,ds$$\nThe integral is the definition of the Gamma function $\\Gamma(a+b)$.\n$$\\int_{0}^{\\infty} s^{(a+b)-1} e^{-s} \\,ds = \\Gamma(a+b)$$\nSubstituting this back, we get:\n$$f_X(x) = \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)} x^{a-1}(1-x)^{b-1}$$\nUsing the definition of the Beta function, $B(a,b) = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$, the PDF becomes:\n$$f_X(x) = \\frac{x^{a-1}(1-x)^{b-1}}{B(a,b)}, \\quad 0  x  1$$\nThis is precisely the PDF of the Beta distribution with shape parameters $a$ and $b$, i.e., $X \\sim \\operatorname{Beta}(a,b)$.\n\nThis derivation provides the following algorithm to generate a $\\operatorname{Beta}(a,b)$ random variate:\n$1$. Generate an independent random variate $Y_1 \\sim \\operatorname{Gamma}(a, 1)$.\n$2$. Generate an independent random variate $Y_2 \\sim \\operatorname{Gamma}(b, 1)$.\n$3$. Compute $X = \\frac{Y_1}{Y_1 + Y_2}$. The result $X$ is a $\\operatorname{Beta}(a,b)$ variate.\n\nThis algorithm will be implemented using a high-quality Gamma generator, such as `numpy.random.gamma`, and then subjected to the specified test suite.\n\nThe test suite validates the generator's statistical properties across five distinct cases:\n- Case A: $(a,b)=(0.1,0.1)$, $n=80000$. A Kolmogorov-Smirnov (K-S) test compares the empirical cumulative distribution function (CDF) of the generated samples against the theoretical Beta CDF. The test passes if the K-S statistic $D$ is at most $0.02$.\n- Case B: $(a,b)=(100,100)$, $n=120000$. This tests large, symmetric shapes. The test passes if two conditions are met: the absolute error of the sample mean relative to the theoretical mean $E[X]=\\frac{a}{a+b}$ is within $5$ times the standard error of the mean, $\\sqrt{\\operatorname{Var}(X)/n}$, and the relative error of the sample variance is at most $0.15$. The theoretical variance is $\\operatorname{Var}(X)=\\frac{ab}{(a+b)^2(a+b+1)}$.\n- Case C: $(a,b)=(0.2,5)$, $n=100000$. This tests a highly asymmetric, left-heavy distribution. It combines the same mean accuracy test as Case B with a tail probability check. The empirical frequency of samples $X \\leq 0.01$ is compared to the exact probability $p = F(0.01; a, b)$. The test passes if this discrepancy is at most $4$ times the standard error of a binomial proportion, $4\\sqrt{p(1-p)/n}$.\n- Case D: $(a,b)=(5,0.2)$, $n=100000$. This is a symmetric case to C, testing a right-heavy distribution. It uses the mean accuracy test and a similar tail check for the upper tail: the empirical frequency of $X \\geq 0.99$ must be close to the exact probability $p = 1 - F(0.99; a, b)$, with the same tolerance factor of $4$.\n- Case E: $(a,b)=(0.01,0.01)$, $n=120000$. This case stresses the generator with extremely small shape parameters, which results in a U-shaped distribution heavily concentrated near $0$ and $1$. As in Case A, a K-S test is used, passing if the statistic $D$ is at most $0.03$.\n\nThe implementation will procedurally execute these test cases and aggregate the boolean results (True for pass, False for fail).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Derives, implements, and tests a Beta random variate generator.\n    \"\"\"\n    \n    # Use a fixed seed for reproducibility of the random number generation.\n    rng = np.random.default_rng(42)\n\n    def generate_beta(a, b, n, local_rng):\n        \"\"\"\n        Generates n random variates from a Beta(a,b) distribution.\n        \n        This function implements the algorithm based on Gamma variates:\n        If Y1 ~ Gamma(a, 1) and Y2 ~ Gamma(b, 1) are independent,\n        then X = Y1 / (Y1 + Y2) ~ Beta(a, b).\n        \n        Args:\n            a (float): The first shape parameter (alpha).\n            b (float): The second shape parameter (beta).\n            n (int): The number of variates to generate.\n            local_rng (numpy.random.Generator): The random number generator instance.\n        \n        Returns:\n            numpy.ndarray: An array of n Beta(a,b) random variates.\n        \"\"\"\n        # Generate Gamma(a, 1) variates\n        y1 = local_rng.gamma(shape=a, scale=1.0, size=n)\n        # Generate Gamma(b, 1) variates\n        y2 = local_rng.gamma(shape=b, scale=1.0, size=n)\n        \n        # The sum y1 + y2 should be non-zero as Gamma variates for a,b  0 are positive.\n        # This prevents division by zero.\n        return y1 / (y1 + y2)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'id': 'A', 'type': 'ks', 'params': (0.1, 0.1), 'n': 80000, 'ks_threshold': 0.02},\n        {'id': 'B', 'type': 'mean_var', 'params': (100, 100), 'n': 120000, 'mean_tol_factor': 5.0, 'rel_var_tol': 0.15},\n        {'id': 'C', 'type': 'mean_tail', 'params': (0.2, 5), 'n': 100000, 'mean_tol_factor': 5.0, 'tail_type': 'left', 'tail_x': 0.01, 'tail_tol_factor': 4.0},\n        {'id': 'D', 'type': 'mean_tail', 'params': (5, 0.2), 'n': 100000, 'mean_tol_factor': 5.0, 'tail_type': 'right', 'tail_x': 0.99, 'tail_tol_factor': 4.0},\n        {'id': 'E', 'type': 'ks', 'params': (0.01, 0.01), 'n': 120000, 'ks_threshold': 0.03}\n    ]\n\n    results = []\n    for case in test_cases:\n        a, b = case['params']\n        n = case['n']\n        samples = generate_beta(a, b, n, rng)\n        \n        test_passed = False\n        if case['type'] == 'ks':\n            ks_stat, _ = stats.kstest(samples, 'beta', args=(a, b))\n            test_passed = ks_stat = case['ks_threshold']\n        \n        elif case['type'] == 'mean_var':\n            theo_mean = a / (a + b)\n            theo_var = (a * b) / ((a + b)**2 * (a + b + 1))\n            \n            sample_mean = np.mean(samples)\n            sample_var = np.var(samples, ddof=0) # Population variance of sample\n            \n            mean_se = np.sqrt(theo_var / n)\n            mean_test = abs(sample_mean - theo_mean) = case['mean_tol_factor'] * mean_se\n            \n            rel_var_error = abs((sample_var - theo_var) / theo_var)\n            var_test = rel_var_error = case['rel_var_tol']\n            \n            test_passed = mean_test and var_test\n\n        elif case['type'] == 'mean_tail':\n            theo_mean = a / (a + b)\n            theo_var = (a * b) / ((a + b)**2 * (a + b + 1))\n            sample_mean = np.mean(samples)\n\n            mean_se = np.sqrt(theo_var / n)\n            mean_test = abs(sample_mean - theo_mean) = case['mean_tol_factor'] * mean_se\n            \n            x_thresh = case['tail_x']\n            if case['tail_type'] == 'left':\n                p_exact = stats.beta.cdf(x_thresh, a, b)\n                p_empirical = np.sum(samples = x_thresh) / n\n            else: # right\n                p_exact = stats.beta.sf(x_thresh, a, b)\n                p_empirical = np.sum(samples = x_thresh) / n\n            \n            # The standard error of the binomial proportion estimate\n            # Check for p_exact being 0 or 1 to avoid NaN from sqrt of a negative number due to precision.\n            if 0.0  p_exact  1.0:\n                tail_tolerance = case['tail_tol_factor'] * np.sqrt(p_exact * (1.0 - p_exact) / n)\n            else:\n                tail_tolerance = 0.0\n            \n            tail_test = abs(p_empirical - p_exact) = tail_tolerance\n            \n            test_passed = mean_test and tail_test\n        \n        results.append(test_passed)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3292091"}, {"introduction": "A theoretically correct algorithm may fail in practice due to the limitations of floating-point arithmetic. This practice addresses the critical issue of numerical stability, particularly when generating Beta variates with extreme shape parameters where direct computation can lead to overflow or underflow. You will learn to fortify the ratio-of-gammas generator by performing calculations in the logarithmic domain, using the robust log-sum-exp identity to ensure accuracy and reliability [@problem_id:3292073].", "problem": "You are asked to design, analyze, and implement a robust random variate generator for the Beta distribution based on the ratio of independent Gamma variates. The target is a generator for a random variable $X$ with distribution $X \\sim \\mathrm{Beta}(\\alpha,\\beta)$, built from two independent Gamma variates $G_\\alpha \\sim \\Gamma(\\alpha,1)$ and $G_\\beta \\sim \\Gamma(\\beta,1)$ via the transformation $X = \\dfrac{G_\\alpha}{G_\\alpha + G_\\beta}$. The implementation must be numerically stable for extreme shape parameters, specifically when $\\alpha$ or $\\beta$ are very large or very small. Your program must implement the stabilized computation of $X$ using the log-sum-exp identity to avoid catastrophic cancellation:\n$$\n\\log\\left(e^{u} + e^{v}\\right) = \\mathrm{LSE}(u,v) \\equiv \\max(u,v) + \\log\\!\\left( 1 + e^{-|u-v|} \\right),\n$$\nso that, when $u = \\log G_\\alpha$ and $v = \\log G_\\beta$, the ratio can be computed as\n$$\nX = \\exp\\!\\left( \\log G_\\alpha - \\mathrm{LSE}(\\log G_\\alpha, \\log G_\\beta) \\right).\n$$\n\nStart from the fundamental definitions of the Gamma distribution with shape-scale parameterization and the Beta distribution. Assume that $G_\\alpha$ and $G_\\beta$ are independent, and recall that numerical stability must be considered a first-class design constraint. You must reason from first principles to justify the correctness of the ratio-of-Gammas construction and the stability of the log-sum-exp technique.\n\nYour program must:\n- Implement a function that, for given real shapes $\\alpha0$, $\\beta0$ and integer sample size $n \\ge 1$, returns $n$ independent samples from $\\mathrm{Beta}(\\alpha,\\beta)$ using the ratio $X=\\dfrac{G_\\alpha}{G_\\alpha+G_\\beta}$ with $G_\\alpha \\sim \\Gamma(\\alpha,1)$, $G_\\beta \\sim \\Gamma(\\beta,1)$, and compute the ratio via the log-sum-exp identity as described above.\n- Use a fixed pseudorandom number generator seed to ensure reproducibility.\n- For validation, compute Monte Carlo diagnostics against known analytic properties of the Beta distribution. Specifically, if $\\mu=\\dfrac{\\alpha}{\\alpha+\\beta}$ and $\\sigma^2=\\dfrac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$ denote the exact mean and variance, then the Monte Carlo standard error of the sample mean $\\bar{X}_n$ is $\\mathrm{SE}=\\sqrt{\\sigma^2/n}$. For selected test cases, declare a pass if $|\\bar{X}_n-\\mu| \\le c \\cdot \\mathrm{SE}$ for a specified constant $c$.\n\nTest suite and required outputs:\n- Use the following five test cases, each specified by $(\\alpha,\\beta,n,c)$:\n  - Case A (happy path): $(\\alpha,\\beta,n,c)=(2.5,5.0,100000,5)$.\n  - Case B (both shapes less than one): $(\\alpha,\\beta,n,c)=(0.3,0.7,200000,5)$.\n  - Case C (very large, balanced shapes): $(\\alpha,\\beta,n,c)=(10^6,10^6,20000,10)$.\n  - Case D (highly imbalanced shapes): $(\\alpha,\\beta,n,c)=(10^{-6},10^{6},200000,6)$.\n  - Case E (quantile accuracy in the interior): $(\\alpha,\\beta,n)=(15.2,9.7,150000)$. For this case, assess the empirical quantiles at probabilities $\\tau \\in \\{0.1,0.5,0.9\\}$ against the theoretical quantiles $q_\\tau$ of $\\mathrm{Beta}(\\alpha,\\beta)$. Using the asymptotic variance formula for the sample quantile $\\hat{q}_\\tau$, \n    $$\n    \\mathrm{Var}(\\hat{q}_\\tau) \\approx \\frac{\\tau(1-\\tau)}{n\\,f(q_\\tau)^2},\n    $$\n    where $f$ is the $\\mathrm{Beta}(\\alpha,\\beta)$ probability density function, declare a pass if \n    $$\n    \\max_{\\tau \\in \\{0.1,0.5,0.9\\}} \\left|\\hat{q}_\\tau - q_\\tau\\right| \\le 4 \\cdot \\sqrt{\\frac{\\tau(1-\\tau)}{n\\,f(q_\\tau)^2}}.\n    $$\n    For this case, report a single boolean indicating whether all three quantiles meet the bound simultaneously.\n- Your program must produce a single line of output containing the results for Cases A–E as a comma-separated list enclosed in square brackets, for example, $[\\text{True},\\text{False},\\text{True},\\text{True},\\text{True}]$.\n\nAll numerical answers must be dimensionless real numbers. Angles are not involved. Do not express any answer as a percentage; if any proportion appears, it must be a real number in $[0,1]$.\n\nScientific realism and coverage:\n- The Gamma and Beta distributions must be used exactly as defined in probability theory, with shapes $\\alpha,\\beta0$ and unit scale for the Gamma variates.\n- The numeric stabilization via the log-sum-exp identity must be explicitly used in computing $X$ to guard against underflow or overflow when $\\alpha$ or $\\beta$ are extreme.\n- The test suite covers typical, small-shape, large-shape, imbalanced-shape, and quantile-accuracy scenarios.", "solution": "The problem of generating random variates from a Beta distribution is a fundamental task in stochastic simulation. The posed problem requires the design and implementation of a generator for $X \\sim \\mathrm{Beta}(\\alpha,\\beta)$ based on the ratio of independent Gamma variates, with a strict requirement for numerical stability, particularly for extreme values of the shape parameters $\\alpha$ and $\\beta$.\n\n**1. Theoretical Foundation: The Ratio-of-Gammas Method**\n\nThe foundation of the method lies in a well-established theorem in probability theory that connects the Beta and Gamma distributions.\n\nFirst, let us define the relevant distributions. A random variable $Y$ follows a Gamma distribution with shape parameter $k0$ and unit scale parameter ($\\theta=1$), denoted $Y \\sim \\Gamma(k,1)$, if its probability density function (PDF) is given by:\n$$f_Y(y; k, 1) = \\frac{1}{\\Gamma(k)} y^{k-1} e^{-y} \\quad \\text{for } y  0$$\nwhere $\\Gamma(k) = \\int_0^\\infty t^{k-1} e^{-t} dt$ is the Gamma function.\n\nA random variable $X$ follows a Beta distribution with shape parameters $\\alpha0$ and $\\beta0$, denoted $X \\sim \\mathrm{Beta}(\\alpha, \\beta)$, if its PDF is:\n$$f_X(x; \\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)} x^{\\alpha-1} (1-x)^{\\beta-1} \\quad \\text{for } x \\in (0, 1)$$\nwhere $B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$ is the Beta function.\n\nThe theorem states that if $G_\\alpha \\sim \\Gamma(\\alpha, 1)$ and $G_\\beta \\sim \\Gamma(\\beta, 1)$ are two independent random variables, then the variable $X$ defined by the ratio\n$$X = \\frac{G_\\alpha}{G_\\alpha + G_\\beta}$$\nis distributed as $\\mathrm{Beta}(\\alpha, \\beta)$.\n\nWe can prove this result via a change of variables. Let the joint PDF of the independent variables $(G_\\alpha, G_\\beta)$ be:\n$$f(g_\\alpha, g_\\beta) = f_{G_\\alpha}(g_\\alpha) f_{G_\\beta}(g_\\beta) = \\left(\\frac{1}{\\Gamma(\\alpha)} g_\\alpha^{\\alpha-1} e^{-g_\\alpha}\\right) \\left(\\frac{1}{\\Gamma(\\beta)} g_\\beta^{\\beta-1} e^{-g_\\beta}\\right)$$\nfor $g_\\alpha, g_\\beta  0$. We introduce the transformation:\n$$X = \\frac{G_\\alpha}{G_\\alpha + G_\\beta} \\quad \\text{and} \\quad Y = G_\\alpha + G_\\beta$$\nThe inverse transformation is $G_\\alpha = XY$ and $G_\\beta = Y(1-X)$. The domain $(g_\\alpha, g_\\beta) \\in (0, \\infty) \\times (0, \\infty)$ maps to $(x, y) \\in (0, 1) \\times (0, \\infty)$. The Jacobian of this inverse transformation is:\n$$J = \\det \\begin{pmatrix} \\frac{\\partial g_\\alpha}{\\partial x}  \\frac{\\partial g_\\alpha}{\\partial y} \\\\ \\frac{\\partial g_\\beta}{\\partial x}  \\frac{\\partial g_\\beta}{\\partial y} \\end{pmatrix} = \\det \\begin{pmatrix} y  x \\\\ -y  1-x \\end{pmatrix} = y(1-x) - (-y)x = y$$\nThe joint PDF of $(X, Y)$ is $f_{X,Y}(x, y) = f(xy, y(1-x))|J|$:\n$$f_{X,Y}(x, y) = \\frac{1}{\\Gamma(\\alpha)\\Gamma(\\beta)} (xy)^{\\alpha-1} e^{-xy} (y(1-x))^{\\beta-1} e^{-y(1-x)} y$$\n$$f_{X,Y}(x, y) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{\\Gamma(\\alpha)\\Gamma(\\beta)} y^{\\alpha-1} y^{\\beta-1} y \\, e^{-xy - y(1-x)}$$\n$$f_{X,Y}(x, y) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{\\Gamma(\\alpha)\\Gamma(\\beta)} y^{\\alpha+\\beta-1} e^{-y}$$\nTo find the marginal PDF of $X$, we integrate over all possible values of $Y$:\n$$f_X(x) = \\int_0^\\infty f_{X,Y}(x, y) dy = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\int_0^\\infty y^{\\alpha+\\beta-1} e^{-y} dy$$\nThe integral is the definition of $\\Gamma(\\alpha+\\beta)$. Therefore:\n$$f_X(x) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\alpha-1}(1-x)^{\\beta-1} = \\frac{1}{B(\\alpha, \\beta)} x^{\\alpha-1}(1-x)^{\\beta-1}$$\nThis is precisely the PDF of a $\\mathrm{Beta}(\\alpha, \\beta)$ distribution, which validates the method.\n\n**2. Numerical Stability and the Log-Sum-Exp Technique**\n\nDirectly computing the ratio $X = G_\\alpha / (G_\\alpha + G_\\beta)$ is numerically unstable when the shape parameters are extreme.\n- If $\\alpha$ and $\\beta$ are very large (e.g., $10^6$), the Gamma variates $G_\\alpha$ and $G_\\beta$ will likely be very large, since $\\mathbb{E}[\\Gamma(k,1)] = k$. Their sum $G_\\alpha + G_\\beta$ could overflow standard floating-point representations.\n- If $\\alpha$ is very small and $\\beta$ is very large (e.g., $\\alpha=10^{-6}, \\beta=10^6$), then $G_\\alpha$ will be extremely close to zero and may underflow to $0.0$. This would incorrectly yield $X=0$. Similarly, if $\\alpha$ is large and $\\beta$ is small, $G_\\beta$ may underflow, yielding $X=1$.\n\nTo circumvent these issues, we perform the computation in the logarithmic domain. Let $u = \\log G_\\alpha$ and $v = \\log G_\\beta$. Then $X$ can be written as:\n$$X = \\frac{e^u}{e^u + e^v}$$\nTaking the logarithm of $X$:\n$$\\log X = \\log(e^u) - \\log(e^u + e^v) = u - \\log(e^u + e^v)$$\nThe term $\\log(e^u + e^v)$, known as the log-sum-exp (LSE) function, can still cause overflow if $u$ or $v$ is large. We use the following stable identity for its computation:\n$$\\mathrm{LSE}(u, v) = \\log(e^u + e^v) = \\max(u, v) + \\log(1 + e^{-|u - v|})$$\nThis form is stable because the argument of the exponential, $-|u-v|$, is always non-positive, preventing overflow. The value of $e^{-|u-v|}$ is always in $[0, 1]$, so its sum with $1$ and the subsequent logarithm are well-behaved.\n\nThe stable algorithm for computing $X$ is therefore:\n$$X = \\exp\\left( \\log G_\\alpha - \\mathrm{LSE}(\\log G_\\alpha, \\log G_\\beta) \\right)$$\n\n**3. Algorithmic Implementation Steps**\n\nTo generate a vector of $n$ samples from $\\mathrm{Beta}(\\alpha, \\beta)$:\n1. Generate a vector $\\mathbf{g}_\\alpha$ of $n$ samples from $\\Gamma(\\alpha, 1)$.\n2. Generate a vector $\\mathbf{g}_\\beta$ of $n$ samples from $\\Gamma(\\beta, 1)$.\n3. Compute the element-wise natural logarithms: $\\mathbf{u} = \\log \\mathbf{g}_\\alpha$ and $\\mathbf{v} = \\log \\mathbf{g}_\\beta$.\n4. Compute the element-wise maximum: $\\mathbf{m} = \\max(\\mathbf{u}, \\mathbf{v})$.\n5. Compute the LSE term: $\\mathbf{lse} = \\mathbf{m} + \\log(1 + \\exp(-|\\mathbf{u} - \\mathbf{v}|))$.\n6. Compute the final samples: $\\mathbf{x} = \\exp(\\mathbf{u} - \\mathbf{lse})$. The vector $\\mathbf{x}$ contains the desired Beta variates.\n\n**4. Validation Methodology**\n\nThe generated samples are validated against known theoretical properties of the Beta distribution.\n\n**Mean Validation (Cases A-D):** The Central Limit Theorem states that for a large sample size $n$, the sample mean $\\bar{X}_n$ is approximately normally distributed with mean $\\mu$ and variance $\\sigma^2/n$, where $\\mu$ and $\\sigma^2$ are the true mean and variance of the distribution.\n- True Mean: $\\mu = \\frac{\\alpha}{\\alpha+\\beta}$\n- True Variance: $\\sigma^2 = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$\nThe Monte Carlo standard error of the sample mean is $\\mathrm{SE} = \\sqrt{\\sigma^2/n}$. We declare a pass if the sample mean is within $c$ standard errors of the true mean: $|\\bar{X}_n - \\mu| \\le c \\cdot \\mathrm{SE}$.\n\n**Quantile Validation (Case E):** For a stronger test of the distribution's shape, we compare empirical quantiles with theoretical quantiles. The asymptotic theory of order statistics provides the approximate variance of the sample quantile $\\hat{q}_\\tau$ for a probability level $\\tau$:\n$$\\mathrm{Var}(\\hat{q}_\\tau) \\approx \\frac{\\tau(1-\\tau)}{n f(q_\\tau)^2}$$\nwhere $q_\\tau$ is the true quantile and $f$ is the PDF of the distribution. We declare a pass if the maximum absolute deviation over a set of quantiles is bounded by $4$ times the asymptotic standard error:\n$$\\max_{\\tau \\in \\{0.1, 0.5, 0.9\\}} |\\hat{q}_\\tau - q_\\tau| \\le 4 \\cdot \\sqrt{\\frac{\\tau(1-\\tau)}{n f(q_\\tau)^2}}$$\nThis comprehensive validation framework ensures that the implemented generator is not only numerically stable but also statistically accurate.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the Beta variate generator.\n    \"\"\"\n    \n    # Use a fixed seed for the pseudorandom number generator for reproducibility.\n    SEED = 42\n    RNG = np.random.default_rng(SEED)\n\n    def generate_beta_stable(alpha, beta, n, rng_instance):\n        \"\"\"\n        Generates n samples from Beta(alpha, beta) using the stable\n        ratio-of-gammas method with the log-sum-exp trick.\n\n        Args:\n            alpha (float): Shape parameter  0.\n            beta (float): Shape parameter  0.\n            n (int): Number of samples to generate = 1.\n            rng_instance (numpy.random.Generator): A numpy random generator instance.\n\n        Returns:\n            numpy.ndarray: An array of n samples from the Beta distribution.\n        \"\"\"\n        # Generate n samples from two independent Gamma distributions.\n        # Scale parameter is 1 by default in numpy.random.gamma.\n        g_alpha = rng_instance.gamma(alpha, size=n)\n        g_beta = rng_instance.gamma(beta, size=n)\n\n        # Compute in the log domain to prevent overflow/underflow.\n        log_g_alpha = np.log(g_alpha)\n        log_g_beta = np.log(g_beta)\n\n        # Compute log(g_alpha + g_beta) robustly using log-sum-exp.\n        u = log_g_alpha\n        v = log_g_beta\n        m = np.maximum(u, v)\n        # LSE(u,v) = log(e^u + e^v)\n        log_sum_exp = m + np.log(1 + np.exp(-np.abs(u - v)))\n\n        # Compute log(X) = log(g_alpha / (g_alpha + g_beta))\n        # log(X) = log(g_alpha) - log(g_alpha + g_beta)\n        log_x = log_g_alpha - log_sum_exp\n        \n        # Exponentiate to get the final samples.\n        x = np.exp(log_x)\n        \n        return x\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (happy path)\n        {'type': 'mean', 'params': (2.5, 5.0, 100000, 5)},\n        # Case B (both shapes less than one)\n        {'type': 'mean', 'params': (0.3, 0.7, 200000, 5)},\n        # Case C (very large, balanced shapes)\n        {'type': 'mean', 'params': (10**6, 10**6, 20000, 10)},\n        # Case D (highly imbalanced shapes)\n        {'type': 'mean', 'params': (10**-6, 10**6, 200000, 6)},\n        # Case E (quantile accuracy)\n        {'type': 'quantile', 'params': (15.2, 9.7, 150000, 4)},\n    ]\n\n    results = []\n    for case in test_cases:\n        if case['type'] == 'mean':\n            alpha, beta, n, c = case['params']\n            \n            samples = generate_beta_stable(alpha, beta, n, RNG)\n            \n            sample_mean = np.mean(samples)\n            \n            # Theoretical properties\n            # Using np.longdouble for precision with extreme parameters\n            a_ld, b_ld = np.longdouble(alpha), np.longdouble(beta)\n            \n            mu = a_ld / (a_ld + b_ld)\n            \n            # Handle potential overflow in (alpha+beta+1) for large alpha/beta\n            if (a_ld + b_ld + 1) == np.inf:\n                 var = 0.0\n            else:\n                 var = (a_ld * b_ld) / ((a_ld + b_ld)**2 * (a_ld + b_ld + 1))\n            \n            se = np.sqrt(var / n)\n            \n            # Check pass condition\n            passed = np.abs(sample_mean - mu) = c * se\n            results.append(bool(passed))\n\n        elif case['type'] == 'quantile':\n            alpha, beta, n, c_q = case['params']\n            \n            samples = generate_beta_stable(alpha, beta, n, RNG)\n\n            taus = np.array([0.1, 0.5, 0.9])\n            \n            # Empirical quantiles\n            q_hats = np.quantile(samples, taus)\n            \n            # Theoretical properties\n            q_s = stats.beta.ppf(taus, a=alpha, b=beta)\n            f_q_s = stats.beta.pdf(q_s, a=alpha, b=beta)\n            \n            # Asymptotic standard error of the sample quantile\n            asymptotic_var = (taus * (1 - taus)) / (n * f_q_s**2)\n            bound = c_q * np.sqrt(asymptotic_var)\n\n            # Check if all quantiles pass the test simultaneously.\n            passed = np.all(np.abs(q_hats - q_s) = bound)\n            results.append(bool(passed))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3292073"}, {"introduction": "Beyond correctness and robustness, performance is a key consideration in simulation. This exercise shifts the focus from implementation to analysis, asking you to build a performance model that predicts the computational cost of different Beta generation algorithms. By applying principles of probability, you will learn to estimate the expected CPU cycles for each method, enabling a principled comparison and selection of the most efficient algorithm for a given scenario without exhaustive empirical testing [@problem_id:3292114].", "problem": "You are asked to construct a principled performance model for generating a Beta random variate by combining algorithmic operation counts, acceptance rates, and hardware-specific costs for primitive operations. The objective is to predict the expected Central Processing Unit (CPU) cycles per Beta variate for multiple algorithmic families under different hardware and acceptance regimes. The model must be based on first principles of probability and expectation rather than heuristics.\n\nBegin from the following foundational base:\n- Consider an algorithm that attempts to produce a sample through a sequence of independent trials, where each trial succeeds with probability $p \\in (0,1]$. The number of trials required until the first success is a geometrically distributed random variable. The expected number of trials equals the reciprocal of the success probability.\n- The expected total cost of a procedure equals the expectation of the sum of its constituent costs, and, under independence and fixed costs per trial, equals the expected number of trials multiplied by the cost per trial.\n- When an algorithm composes independent subprocedures, the expected overall cost is the sum of the expected costs of the components by linearity of expectation.\n\nYou will build a computational model around three families of Beta variate generation algorithms:\n1. A rejection-based power-transform method using uniform and power primitives with logarithm and exponential evaluations (denoted Algorithm J).\n2. A normal-based acceptance method using one normal proposal, one logarithm, and one uniform per trial (denoted Algorithm C).\n3. A ratio-of-Gamma method using two independent Gamma variates produced by a normal-based acceptance method (denoted Algorithm G). The Beta variate is obtained from the ratio of the two Gamma variates and requires no additional primitives beyond those used to obtain the two Gamma variates.\n\nFor each algorithm, a trial uses a fixed count of primitive operations. There is no additional post-acceptance primitive cost other than the primitives enumerated per trial. The primitive operations and their hardware-specific costs (in cycles) are:\n- Uniform random draw: hardware cost $C_{\\mathrm{u}}$ cycles.\n- Normal random draw: hardware cost $C_{\\mathrm{n}}$ cycles.\n- Natural logarithm: hardware cost $C_{\\log}$ cycles.\n- Exponential: hardware cost $C_{\\exp}$ cycles.\n\nAlgorithmic per-trial primitive counts are:\n- Algorithm J: $2$ uniforms, $2$ logarithms, $2$ exponentials, $0$ normals per trial. The acceptance probability per trial is $p_{\\mathrm{J}}$.\n- Algorithm C: $1$ normal, $1$ logarithm, $1$ uniform, $0$ exponentials per trial. The acceptance probability per trial is $p_{\\mathrm{C}}$.\n- Algorithm G: Two independent Gamma variates are generated. Each Gamma variate requires per trial $1$ normal, $1$ logarithm, $1$ uniform, $0$ exponentials, with acceptance probability $p_{\\mathrm{G}}(\\cdot)$ depending on the shape parameter used in that Gamma variate. Let $p_{\\mathrm{G}}(a)$ and $p_{\\mathrm{G}}(b)$ denote the acceptance probabilities for the two Gamma variates. The Beta variate is formed by the ratio of these two Gamma samples, and this ratio incurs no additional primitive operations.\n\nYour task is to write a program that, given the hardware primitive costs and acceptance probabilities for each algorithm and subcomponent, computes the expected CPU cycles per Beta variate for each algorithm and returns, for each test case, the minimum expected cycles among the three algorithms. All outputs must be expressed in cycles.\n\nUse the following test suite. Each test case provides the hardware costs $\\{C_{\\mathrm{u}}, C_{\\mathrm{n}}, C_{\\log}, C_{\\exp}\\}$, the Beta shape parameters $(a,b)$, and acceptance probabilities $\\{p_{\\mathrm{J}}, p_{\\mathrm{C}}, p_{\\mathrm{G}}(a), p_{\\mathrm{G}}(b)\\}$:\n\n- Test Case $1$ (general happy path): $C_{\\mathrm{u}}=40$, $C_{\\mathrm{n}}=200$, $C_{\\log}=80$, $C_{\\exp}=90$, $(a,b)=(2.0,5.0)$, $p_{\\mathrm{J}}=0.35$, $p_{\\mathrm{C}}=0.80$, $p_{\\mathrm{G}}(a)=0.95$, $p_{\\mathrm{G}}(b)=0.90$.\n- Test Case $2$ (near-unity acceptance): $C_{\\mathrm{u}}=40$, $C_{\\mathrm{n}}=200$, $C_{\\log}=80$, $C_{\\exp}=90$, $(a,b)=(1.5,3.0)$, $p_{\\mathrm{J}}=0.95$, $p_{\\mathrm{C}}=0.99$, $p_{\\mathrm{G}}(a)=0.99$, $p_{\\mathrm{G}}(b)=0.99$.\n- Test Case $3$ (expensive transcendental hardware, very low acceptance in one method): $C_{\\mathrm{u}}=40$, $C_{\\mathrm{n}}=150$, $C_{\\log}=500$, $C_{\\exp}=700$, $(a,b)=(0.5,0.5)$, $p_{\\mathrm{J}}=0.02$, $p_{\\mathrm{C}}=0.50$, $p_{\\mathrm{G}}(a)=0.60$, $p_{\\mathrm{G}}(b)=0.60$.\n- Test Case $4$ (expensive normal hardware): $C_{\\mathrm{u}}=40$, $C_{\\mathrm{n}}=1000$, $C_{\\log}=80$, $C_{\\exp}=90$, $(a,b)=(3.0,3.0)$, $p_{\\mathrm{J}}=0.40$, $p_{\\mathrm{C}}=0.60$, $p_{\\mathrm{G}}(a)=0.85$, $p_{\\mathrm{G}}(b)=0.85$.\n\nYour program must:\n- For each test case, compute the expected CPU cycles per Beta variate for Algorithms J, C, and G using only the operation counts, acceptance probabilities, and primitive costs provided.\n- For Algorithm J, base the expected cycles on repeated independent trials with success probability $p_{\\mathrm{J}}$.\n- For Algorithm C, base the expected cycles on repeated independent trials with success probability $p_{\\mathrm{C}}$.\n- For Algorithm G, compute the expected cycles as the sum of the expected cycles for generating the two Gamma variates with acceptance probabilities $p_{\\mathrm{G}}(a)$ and $p_{\\mathrm{G}}(b)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the minimum expected CPU cycles across the three algorithms for each test case, in order, as a comma-separated list enclosed in square brackets, for example $[x_{1},x_{2},x_{3},x_{4}]$. All values must be floats measured in cycles. No units should be printed, only the numeric values.\n\nEnsure scientific realism and internal consistency. Do not perform any random simulation; compute using the principled model described. The program must be self-contained and require no user input or external files.", "solution": "The problem statement is evaluated to be scientifically grounded, well-posed, objective, and self-contained. It presents a standard performance modeling task based on fundamental principles of probability theory and algorithm analysis. All necessary data are provided, and the task is to compute expected values based on a clearly defined model. Therefore, I will proceed with the solution.\n\nThe core of this problem rests on two fundamental principles of probability theory: the expected number of trials for a geometrically distributed process and the linearity of expectation for the total cost.\n\nLet a random process consist of a sequence of independent Bernoulli trials, each succeeding with probability $p \\in (0,1]$. Let $N$ be the random variable representing the number of trials required to achieve the first success. $N$ follows a geometric distribution on $\\{1, 2, 3, \\ldots\\}$ with probability mass function $P(N=k) = (1-p)^{k-1}p$. The expected value of $N$ is given by:\n$$\nE[N] = \\frac{1}{p}\n$$\nThis represents the expected number of trials required for any of the rejection-based algorithms.\n\nIf each trial incurs a fixed cost, say $C_{\\text{trial}}$, then the total cost of the procedure is the random variable $C_{\\text{total}} = N \\times C_{\\text{trial}}$. By the properties of expectation, the expected total cost is:\n$$\nE[C_{\\text{total}}] = E[N \\times C_{\\text{trial}}] = C_{\\text{trial}} \\times E[N] = \\frac{C_{\\text{trial}}}{p}\n$$\nFurthermore, if an algorithm's total cost is the sum of costs from independent subprocedures, the linearity of expectation states that the expected total cost is the sum of the expected costs of the subprocedures.\n\nWe will now apply these principles to derive the expected CPU cycle cost for each of the three specified algorithms. Let the hardware costs for primitive operations be denoted by $C_{\\mathrm{u}}$ (uniform), $C_{\\mathrm{n}}$ (normal), $C_{\\log}$ (logarithm), and $C_{\\exp}$ (exponential).\n\n**1. Algorithm J (Rejection-based power-transform method)**\n\nThis algorithm performs independent trials until one is accepted.\nThe primitives used per trial are specified as $2$ uniforms, $2$ logarithms, and $2$ exponentials.\nThe cost per trial, $C_{\\text{trial,J}}$, is the sum of the costs of these primitives:\n$$\nC_{\\text{trial,J}} = 2 C_{\\mathrm{u}} + 2 C_{\\log} + 2 C_{\\exp}\n$$\nThe probability of a single trial being successful (acceptance) is given as $p_{\\mathrm{J}}$.\nApplying the formula for expected total cost, the expected number of CPU cycles per generated Beta variate, $E_{\\mathrm{J}}$, is:\n$$\nE_{\\mathrm{J}} = \\frac{C_{\\text{trial,J}}}{p_{\\mathrm{J}}} = \\frac{2 C_{\\mathrm{u}} + 2 C_{\\log} + 2 C_{\\exp}}{p_{\\mathrm{J}}}\n$$\n\n**2. Algorithm C (Normal-based acceptance method)**\n\nThis algorithm also performs independent trials until acceptance.\nThe primitives used per trial are $1$ normal, $1$ logarithm, and $1$ uniform.\nThe cost per trial, $C_{\\text{trial,C}}$, is:\n$$\nC_{\\text{trial,C}} = C_{\\mathrm{n}} + C_{\\log} + C_{\\mathrm{u}}\n$$\nThe acceptance probability per trial is given as $p_{\\mathrm{C}}$.\nThe expected number of CPU cycles per generated Beta variate, $E_{\\mathrm{C}}$, is:\n$$\nE_{\\mathrm{C}} = \\frac{C_{\\text{trial,C}}}{p_{\\mathrm{C}}} = \\frac{C_{\\mathrm{n}} + C_{\\log} + C_{\\mathrm{u}}}{p_{\\mathrm{C}}}\n$$\n\n**3. Algorithm G (Ratio-of-Gamma method)**\n\nThis algorithm generates a Beta variate by first generating two independent Gamma variates and then taking their ratio. The problem states that the total expected cost is the sum of the expected costs for generating the two Gamma variates, which is a direct application of the linearity of expectation.\n\nLet the two Gamma variates correspond to the Beta shape parameters $a$ and $b$. Let their respective generation procedures be $G_a$ and $G_b$.\nThe total expected cost is $E_{\\mathrm{G}} = E[G_a] + E[G_b]$.\n\nFor each Gamma variate, the generation method is a normal-based acceptance method. The primitives per trial for generating one Gamma variate are $1$ normal, $1$ logarithm, and $1$ uniform.\nThe cost per trial for generating a single Gamma variate, $C_{\\text{trial,G}}$, is therefore:\n$$\nC_{\\text{trial,G}} = C_{\\mathrm{n}} + C_{\\log} + C_{\\mathrm{u}}\n$$\nNote that this is identical to $C_{\\text{trial,C}}$.\n\nFor the first Gamma variate (associated with parameter $a$), the acceptance probability is given as $p_{\\mathrm{G}}(a)$. The expected cost, $E[G_a]$, is:\n$$\nE[G_a] = \\frac{C_{\\text{trial,G}}}{p_{\\mathrm{G}}(a)} = \\frac{C_{\\mathrm{n}} + C_{\\log} + C_{\\mathrm{u}}}{p_{\\mathrm{G}}(a)}\n$$\nFor the second Gamma variate (associated with parameter $b$), the acceptance probability is given as $p_{\\mathrm{G}}(b)$. The expected cost, $E[G_b]$, is:\n$$\nE[G_b] = \\frac{C_{\\text{trial,G}}}{p_{\\mathrm{G}}(b)} = \\frac{C_{\\mathrm{n}} + C_{\\log} + C_{\\mathrm{u}}}{p_{\\mathrm{G}}(b)}\n$$\nThe total expected cost for Algorithm G is the sum of these two costs:\n$$\nE_{\\mathrm{G}} = E[G_a] + E[G_b] = \\frac{C_{\\mathrm{n}} + C_{\\log} + C_{\\mathrm{u}}}{p_{\\mathrm{G}}(a)} + \\frac{C_{\\mathrm{n}} + C_{\\log} + C_{\\mathrm{u}}}{p_{\\mathrm{G}}(b)}\n$$\nThis can be factored as:\n$$\nE_{\\mathrm{G}} = (C_{\\mathrm{n}} + C_{\\log} + C_{\\mathrm{u}}) \\left( \\frac{1}{p_{\\mathrm{G}}(a)} + \\frac{1}{p_{\\mathrm{G}}(b)} \\right)\n$$\n\nThe final task is to compute $E_{\\mathrm{J}}$, $E_{\\mathrm{C}}$, and $E_{\\mathrm{G}}$ for each test case using the provided parameters and determine the minimum value, $\\min(E_{\\mathrm{J}}, E_{\\mathrm{C}}, E_{\\mathrm{G}})$. This will be implemented programmatically.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the minimum expected CPU cycles for generating a Beta variate\n    across three different algorithmic families based on a provided performance model.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a dictionary containing hardware costs and acceptance probabilities.\n    test_cases = [\n        {\n            \"costs\": {\"Cu\": 40, \"Cn\": 200, \"Clog\": 80, \"Cexp\": 90},\n            \"params\": {\"a\": 2.0, \"b\": 5.0},\n            \"probs\": {\"pJ\": 0.35, \"pC\": 0.80, \"pG_a\": 0.95, \"pG_b\": 0.90}\n        },\n        {\n            \"costs\": {\"Cu\": 40, \"Cn\": 200, \"Clog\": 80, \"Cexp\": 90},\n            \"params\": {\"a\": 1.5, \"b\": 3.0},\n            \"probs\": {\"pJ\": 0.95, \"pC\": 0.99, \"pG_a\": 0.99, \"pG_b\": 0.99}\n        },\n        {\n            \"costs\": {\"Cu\": 40, \"Cn\": 150, \"Clog\": 500, \"Cexp\": 700},\n            \"params\": {\"a\": 0.5, \"b\": 0.5},\n            \"probs\": {\"pJ\": 0.02, \"pC\": 0.50, \"pG_a\": 0.60, \"pG_b\": 0.60}\n        },\n        {\n            \"costs\": {\"Cu\": 40, \"Cn\": 1000, \"Clog\": 80, \"Cexp\": 90},\n            \"params\": {\"a\": 3.0, \"b\": 3.0},\n            \"probs\": {\"pJ\": 0.40, \"pC\": 0.60, \"pG_a\": 0.85, \"pG_b\": 0.85}\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        costs = case[\"costs\"]\n        probs = case[\"probs\"]\n        \n        c_u, c_n, c_log, c_exp = costs[\"Cu\"], costs[\"Cn\"], costs[\"Clog\"], costs[\"Cexp\"]\n        p_j, p_c, p_g_a, p_g_b = probs[\"pJ\"], probs[\"pC\"], probs[\"pG_a\"], probs[\"pG_b\"]\n\n        # 1. Calculate Expected Cost for Algorithm J\n        # Cost per trial = 2*Cu + 2*Clog + 2*Cexp\n        # Expected cost = Cost per trial / pJ\n        cost_per_trial_j = 2 * c_u + 2 * c_log + 2 * c_exp\n        expected_cost_j = cost_per_trial_j / p_j\n\n        # 2. Calculate Expected Cost for Algorithm C\n        # Cost per trial = Cn + Clog + Cu\n        # Expected cost = Cost per trial / pC\n        cost_per_trial_c = c_n + c_log + c_u\n        expected_cost_c = cost_per_trial_c / p_c\n\n        # 3. Calculate Expected Cost for Algorithm G\n        # Cost is the sum of costs for two independent Gamma variates.\n        # Cost per trial for one Gamma = Cn + Clog + Cu\n        # Total Expected Cost = (Cost per trial / pG_a) + (Cost per trial / pG_b)\n        cost_per_trial_g = c_n + c_log + c_u\n        expected_cost_g = cost_per_trial_g * (1 / p_g_a + 1 / p_g_b)\n\n        # Find the minimum expected cost among the three algorithms\n        min_cost = np.min([expected_cost_j, expected_cost_c, expected_cost_g])\n        results.append(min_cost)\n\n    # Format the output as a comma-separated list of floating-point numbers\n    # enclosed in square brackets.\n    # The map to str is important to handle the floating point representation.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3292114"}]}