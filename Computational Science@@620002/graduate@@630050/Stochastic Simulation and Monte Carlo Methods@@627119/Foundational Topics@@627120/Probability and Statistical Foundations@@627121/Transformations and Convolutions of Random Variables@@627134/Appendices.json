{"hands_on_practices": [{"introduction": "The Cauchy distribution is a famous case study in probability theory, known for its \"heavy tails\" and undefined mean, which challenge many standard statistical assumptions. However, it possesses an elegant stability property: the sum of independent Cauchy random variables is itself a Cauchy variable. This exercise ([@problem_id:3357841]) provides a comprehensive practice by first asking you to prove this property using characteristic functions, and then to design and implement a Monte Carlo simulation to verify your theoretical result, connecting analytical derivation with computational experiment.", "problem": "Consider independent random variables $X$ and $Y$ with $X \\sim \\mathrm{Cauchy}(0,\\gamma_1)$ and $Y \\sim \\mathrm{Cauchy}(0,\\gamma_2)$, where the notation $\\mathrm{Cauchy}(\\mu,\\gamma)$ denotes the Cauchy distribution with location $\\mu$ and scale $\\gamma$. In stochastic simulation and Monte Carlo (MC) methods, transformations and convolutions of random variables are studied using foundational tools such as characteristic functions and quantile-based estimators. The characteristic function of a real-valued random variable $W$ is defined as $\\varphi_W(t) = \\mathbb{E}[e^{itW}]$, and for independent $X$ and $Y$ the characteristic function of their sum satisfies $\\varphi_{X+Y}(t) = \\varphi_X(t)\\,\\varphi_Y(t)$. \n\nYour tasks are:\n- Derive, from first principles using characteristic functions and independence, the distributional form of $Z = X+Y$ when $X$ and $Y$ are independent Cauchy random variables with zero location and positive scales $\\gamma_1$ and $\\gamma_2$. Explicitly show what the distribution of $Z$ is and identify its scale parameter in terms of $\\gamma_1$ and $\\gamma_2$.\n- Design a Monte Carlo estimator for the scale parameter of a zero-location Cauchy random variable based on order statistics. Justify your choice starting only from the definition of quantiles and properties of the Cauchy distribution, without assuming any closed-form estimator for the scale.\n- Implement a complete, runnable program that, for each test case below, performs the following steps:\n  1. Generates $N$ independent samples of $X$ and $Y$ using a reproducible pseudorandom number generator, with $X \\sim \\mathrm{Cauchy}(0,\\gamma_1)$ and $Y \\sim \\mathrm{Cauchy}(0,\\gamma_2)$, and forms $Z = X + Y$.\n  2. Estimates the scale of $Z$ using your quantile-based estimator derived above.\n  3. Computes the absolute relative error between the estimated scale and the theoretical scale predicted by your derivation for $Z$. The absolute relative error must be computed as $|\\widehat{\\gamma} - \\gamma_{\\text{theory}}|/\\gamma_{\\text{theory}}$.\n- The output must be a single line containing a comma-separated list enclosed in square brackets, where each entry is the absolute relative error for the corresponding test case, expressed as a floating-point number. No units are involved in this problem.\n\nUse the following test suite, each specified by $(\\gamma_1,\\gamma_2,N,\\text{seed})$, where $\\gamma_1$ and $\\gamma_2$ are strictly positive reals, $N$ is a positive integer sample size, and $\\text{seed}$ is a nonnegative integer to seed the random number generator:\n- Test case 1: $(\\gamma_1,\\gamma_2,N,\\text{seed}) = (0.5, 1.5, 400000, 12345)$\n- Test case 2: $(\\gamma_1,\\gamma_2,N,\\text{seed}) = (10^{-6}, 3\\cdot 10^{-6}, 400000, 54321)$\n- Test case 3: $(\\gamma_1,\\gamma_2,N,\\text{seed}) = (100, 250, 400000, 11111)$\n- Test case 4: $(\\gamma_1,\\gamma_2,N,\\text{seed}) = (0.1, 0.1, 400000, 22222)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4]$), where $r_k$ is the absolute relative error for test case $k$ computed from the simulation described above.", "solution": "The problem is assessed to be valid as it is scientifically grounded in probability theory, well-posed with a clear objective and sufficient data, and free from any factual errors, subjective claims, or ambiguities. It presents a standard, verifiable problem in stochastic simulation.\n\nThe solution is structured in two parts as required: first, the theoretical derivation of the distribution for the sum of two independent Cauchy variables; second, the design of a quantile-based Monte Carlo estimator for the scale parameter.\n\n### Part 1: Distribution of the Sum of Independent Cauchy Random Variables\n\nLet $X$ and $Y$ be independent random variables with $X \\sim \\mathrm{Cauchy}(0, \\gamma_1)$ and $Y \\sim \\mathrm{Cauchy}(0, \\gamma_2)$, where $\\gamma_1 > 0$ and $\\gamma_2 > 0$ are the scale parameters. We want to find the distribution of their sum, $Z = X+Y$.\n\nThe characteristic function of a random variable $W$ following a Cauchy distribution with location parameter $\\mu$ and scale parameter $\\gamma$, denoted $W \\sim \\mathrm{Cauchy}(\\mu, \\gamma)$, is given by:\n$$ \\varphi_W(t) = \\mathbb{E}[e^{itW}] = e^{i\\mu t - \\gamma|t|} $$\nFor the given random variable $X \\sim \\mathrm{Cauchy}(0, \\gamma_1)$, its location parameter is $\\mu_X = 0$ and scale is $\\gamma_1$. Its characteristic function is:\n$$ \\varphi_X(t) = e^{i(0)t - \\gamma_1|t|} = e^{-\\gamma_1|t|} $$\nSimilarly, for the random variable $Y \\sim \\mathrm{Cauchy}(0, \\gamma_2)$, its location parameter is $\\mu_Y = 0$ and scale is $\\gamma_2$. Its characteristic function is:\n$$ \\varphi_Y(t) = e^{i(0)t - \\gamma_2|t|} = e^{-\\gamma_2|t|} $$\nAs stated in the problem, for independent random variables $X$ and $Y$, the characteristic function of their sum $Z = X+Y$ is the product of their individual characteristic functions:\n$$ \\varphi_Z(t) = \\varphi_{X+Y}(t) = \\varphi_X(t) \\varphi_Y(t) $$\nSubstituting the expressions for $\\varphi_X(t)$ and $\\varphi_Y(t)$, we get:\n$$ \\varphi_Z(t) = (e^{-\\gamma_1|t|}) (e^{-\\gamma_2|t|}) = e^{-\\gamma_1|t| - \\gamma_2|t|} = e^{-(\\gamma_1 + \\gamma_2)|t|} $$\nThis resulting characteristic function, $\\varphi_Z(t) = e^{-(\\gamma_1 + \\gamma_2)|t|}$, is of the form $e^{i\\mu_Z t - \\gamma_Z|t|}$ where the location parameter is $\\mu_Z = 0$ and the scale parameter is $\\gamma_Z = \\gamma_1 + \\gamma_2$.\n\nBy the uniqueness property of characteristic functions, which states that a probability distribution is uniquely determined by its characteristic function, we can conclude that the random variable $Z$ also follows a Cauchy distribution. Specifically, $Z$ follows a Cauchy distribution with location $0$ and a scale parameter that is the sum of the individual scale parameters.\n\nTherefore, the distribution of $Z$ is:\n$$ Z \\sim \\mathrm{Cauchy}(0, \\gamma_1 + \\gamma_2) $$\nThe theoretical scale parameter of $Z$, which we denote as $\\gamma_{\\text{theory}}$, is $\\gamma_{\\text{theory}} = \\gamma_1 + \\gamma_2$. This property demonstrates that the Cauchy distribution is a stable distribution.\n\n### Part 2: Design of a Quantile-Based Estimator for the Scale Parameter\n\nThe task is to design a Monte Carlo estimator for the scale parameter $\\gamma$ of a zero-location Cauchy random variable, $W \\sim \\mathrm{Cauchy}(0, \\gamma)$, based on order statistics. We start from the definition of quantiles.\n\nThe cumulative distribution function (CDF) of a random variable $W \\sim \\mathrm{Cauchy}(\\mu, \\gamma)$ is:\n$$ F_W(w) = P(W \\le w) = \\frac{1}{2} + \\frac{1}{\\pi}\\arctan\\left(\\frac{w-\\mu}{\\gamma}\\right) $$\nThe quantile function, $Q_W(p) = F_W^{-1}(p)$, gives the value $w$ such that $F_W(w)=p$ for a probability $p \\in (0, 1)$. We can derive this function by inverting the CDF:\n$$ p = \\frac{1}{2} + \\frac{1}{\\pi}\\arctan\\left(\\frac{Q_W(p)-\\mu}{\\gamma}\\right) $$\n$$ \\pi\\left(p - \\frac{1}{2}\\right) = \\arctan\\left(\\frac{Q_W(p)-\\mu}{\\gamma}\\right) $$\n$$ \\tan\\left(\\pi\\left(p - \\frac{1}{2}\\right)\\right) = \\frac{Q_W(p)-\\mu}{\\gamma} $$\n$$ Q_W(p) = \\mu + \\gamma \\tan\\left(\\pi\\left(p - \\frac{1}{2}\\right)\\right) $$\nFor the specified case of a zero-location Cauchy distribution, we set $\\mu=0$:\n$$ Q_W(p) = \\gamma \\tan\\left(\\pi\\left(p - \\frac{1}{2}\\right)\\right) $$\nTo construct an estimator for $\\gamma$, we can use the relationship between two distinct quantiles. A robust and common choice is to use the first and third quartiles, which correspond to $p_1 = 0.25$ and $p_2 = 0.75$.\nLet $Q_1 = Q_W(0.25)$ and $Q_3 = Q_W(0.75)$.\nFor the third quartile ($p=0.75$):\n$$ Q_3 = \\gamma \\tan\\left(\\pi\\left(0.75 - 0.5\\right)\\right) = \\gamma \\tan\\left(\\frac{\\pi}{4}\\right) = \\gamma \\cdot 1 = \\gamma $$\nFor the first quartile ($p=0.25$):\n$$ Q_1 = \\gamma \\tan\\left(\\pi\\left(0.25 - 0.5\\right)\\right) = \\gamma \\tan\\left(-\\frac{\\pi}{4}\\right) = \\gamma \\cdot (-1) = -\\gamma $$\nThe difference between these two quantiles is the interquartile range (IQR):\n$$ \\mathrm{IQR} = Q_3 - Q_1 = \\gamma - (-\\gamma) = 2\\gamma $$\nFrom this relationship, we can express the scale parameter $\\gamma$ as half of the interquartile range:\n$$ \\gamma = \\frac{Q_3 - Q_1}{2} = \\frac{\\mathrm{IQR}}{2} $$\nThis provides a direct method for estimating $\\gamma$. Given a set of $N$ samples $\\{z_1, z_2, \\dots, z_N\\}$ drawn from the distribution of $Z$, we can compute the sample quartiles. The sample quantiles, derived from the order statistics (sorted samples), are estimators for the true quantiles. Let $\\widehat{Q}_Z(p)$ be the sample $p$-quantile of the data. Our estimator for $\\gamma_Z$, which we denote $\\widehat{\\gamma}$, is:\n$$ \\widehat{\\gamma} = \\frac{\\widehat{Q}_Z(0.75) - \\widehat{Q}_Z(0.25)}{2} $$\nThis estimator is based on order statistics, as requested, and is known to be a robust estimator for the scale parameter of a Cauchy distribution. The Monte Carlo simulation will use this formula to estimate the scale parameter of the generated samples of $Z$.\nThe program will implement this estimator to find $\\widehat{\\gamma}$, calculate the theoretical scale $\\gamma_{\\text{theory}} = \\gamma_1 + \\gamma_2$, and then compute the absolute relative error $|\\widehat{\\gamma} - \\gamma_{\\text{theory}}|/\\gamma_{\\text{theory}}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs Monte Carlo simulations to estimate the scale parameter of the sum\n    of two independent Cauchy random variables and calculates the relative error.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (gamma1, gamma2, N, seed)\n    test_cases = [\n        (0.5, 1.5, 400000, 12345),\n        (1e-6, 3e-6, 400000, 54321),\n        (100.0, 250.0, 400000, 11111),\n        (0.1, 0.1, 400000, 22222),\n    ]\n\n    results = []\n    for case in test_cases:\n        gamma1, gamma2, N, seed = case\n\n        # Step 1: Generate N independent samples of X and Y and form Z = X + Y.\n        \n        # Initialize a reproducible pseudorandom number generator.\n        rng = np.random.default_rng(seed)\n\n        # Generate standard Cauchy(0, 1) samples.\n        # Note: If S ~ Cauchy(0, 1), then mu + gamma * S ~ Cauchy(mu, gamma).\n        # Since mu=0, we only need to scale the standard samples.\n        x_samples = gamma1 * rng.standard_cauchy(size=N)\n        y_samples = gamma2 * rng.standard_cauchy(size=N)\n\n        # The sum of independent samples is a sample of the sum Z = X + Y.\n        z_samples = x_samples + y_samples\n\n        # Step 2: Estimate the scale of Z using the quantile-based estimator.\n        # The estimator is based on the interquartile range (IQR).\n        # gamma_hat = (Q3 - Q1) / 2\n        q75 = np.quantile(z_samples, 0.75)\n        q25 = np.quantile(z_samples, 0.25)\n        \n        gamma_hat = (q75 - q25) / 2.0\n\n        # Step 3: Compute the absolute relative error.\n        \n        # The theoretical scale of Z = X + Y is gamma_theory = gamma1 + gamma2.\n        gamma_theory = gamma1 + gamma2\n        \n        # Calculate the absolute relative error.\n        # error = |estimated - theoretical| / theoretical\n        absolute_relative_error = np.abs(gamma_hat - gamma_theory) / gamma_theory\n        \n        results.append(absolute_relative_error)\n\n    # Final print statement in the exact required format.\n    # The output is a single line with a comma-separated list of errors.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```", "id": "3357841"}, {"introduction": "In theory, the probability density of a sum of independent random variables is found through continuous linear convolution. In practice, we often use the highly efficient Fast Fourier Transform (FFT), but this algorithm natively computes a *circular* convolution on a discrete grid. This practice ([@problem_id:3357868]) tackles the critical task of bridging this gap, guiding you to develop a zero-padding strategy that allows the FFT to correctly approximate linear convolution and to quantify the \"wrap-around\" error that arises from an improper setup.", "problem": "You are given two independent random variables with probability densities on the real line, denoted $X$ with density $f_X$ and $Y$ with density $f_Y$. The density of the sum $Z = X + Y$ is the linear convolution on $\\mathbb{R}$ of $f_X$ and $f_Y$, defined by the integral $f_Z(z) = \\int_{\\mathbb{R}} f_X(x) f_Y(z - x)\\,dx$. In computation, it is common to approximate $f_X$ and $f_Y$ by samples on a uniform discrete grid, and then to compute an approximation of the convolution using the Discrete Fourier Transform, which natively implements circular convolution on a discrete grid. There is a conceptual and practical difference between circular convolution on a finite, periodic discrete grid and linear convolution on $\\mathbb{R}$. Your task is to precisely articulate this difference from first principles, design a zero-padding strategy to use circular convolution to approximate linear convolution, and to quantify the wrap-around error introduced when the padding is insufficient.\n\nStarting from the core definitions only (no specialized convolution theorems may be assumed), proceed as follows:\n\n1. Define the linear convolution of two integrable functions on $\\mathbb{R}$ and explain how sampling on a uniform grid with spacing $h > 0$ and finite support leads to a discrete linear convolution of two finite sequences. Define the discrete circular convolution of two finite sequences of common length $N$ as the convolution performed modulo $N$ (i.e., on a discrete torus). Explain why circular convolution corresponds to the convolution of the $N$-periodic extensions of the sequences.\n\n2. Propose a zero-padding strategy that uses circular convolution of padded sequences to exactly recover the discrete linear convolution of the original finite sequences. Your derivation must state a necessary and sufficient condition on the padding length that guarantees no wrap-around distortion occurs in the computed samples.\n\n3. For a general finite-support, nonnegative, summable pair of sequences $(x_k)_{k=0}^{n-1}$ and $(y_k)_{k=0}^{m-1}$ that represent discretized probability mass functions (i.e., $\\sum_k x_k = 1$ and $\\sum_k y_k = 1$), let $c^{\\mathrm{lin}}$ denote their discrete linear convolution of length $L = n + m - 1$, and let $c^{\\mathrm{circ}}_N$ denote their discrete circular convolution computed at length $N \\in \\mathbb{N}$. Define the wrap-around approximation of the linear convolution by tiling $c^{\\mathrm{circ}}_N$ periodically and truncating to the first $L$ samples. Define the wrap-around error as the $\\ell_1$-distance between this approximation and $c^{\\mathrm{lin}}$. Derive an expression for this error in terms of the $N$-periodization of $c^{\\mathrm{lin}}$, and obtain a bound that depends only on the tail mass of $c^{\\mathrm{lin}}$ beyond index $N - 1$.\n\n4. Implement a program that:\n   - Constructs the specified test sequences below.\n   - Computes the discrete linear convolution $c^{\\mathrm{lin}}$ directly.\n   - Computes the discrete circular convolution $c^{\\mathrm{circ}}_N$ using the Fast Fourier Transform (FFT).\n   - Forms the wrap-around approximation to length $L$ by periodic tiling of $c^{\\mathrm{circ}}_N$ and truncation.\n   - Outputs the wrap-around error as the $\\ell_1$-distance between the wrap-around approximation and $c^{\\mathrm{lin}}$.\n\nUse only purely mathematical quantities; no physical units are involved. All angles, if any, are not applicable here. All answers must be numeric floats.\n\nTest Suite (construct these exact cases; each case requires computing one float equal to the wrap-around error):\n- Case 1a: $x$ uniform on $\\{0,\\dots,9\\}$, i.e., $x_k = 1/10$ for $k \\in \\{0,\\dots,9\\}$; $y$ uniform on $\\{0,\\dots,4\\}$, i.e., $y_k = 1/5$ for $k \\in \\{0,\\dots,4\\}$; circular length $N = 10$.\n- Case 1b: same $x$ and $y$ as Case 1a; circular length $N = 14$.\n- Case 2a: $x$ is a Kronecker delta on $\\{0,\\dots,7\\}$ with $x_7 = 1$ and $x_k = 0$ for $k \\neq 7$; $y$ is a symmetric triangular sequence on $\\{0,\\dots,7\\}$ proportional to $\\{1,2,3,4,4,3,2,1\\}$ and normalized to sum to $1$; circular length $N = 8$.\n- Case 2b: same $x$ and $y$ as Case 2a; circular length $N = 15$.\n- Case 3a: $x$ geometric on $\\{0,\\dots,63\\}$ with parameter $p_X = 0.2$, i.e., $x_k \\propto p_X (1 - p_X)^k$ and then renormalized so that $\\sum_k x_k = 1$ on $\\{0,\\dots,63\\}$; $y$ geometric on $\\{0,\\dots,63\\}$ with parameter $p_Y = 0.35$ and similarly renormalized; circular length $N = 64$.\n- Case 3b: same $x$ and $y$ as Case 3a; circular length $N = 127$.\n- Case 3c: same $x$ and $y$ as Case 3a; circular length $N = 128$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[r1,r2,...,r7]\"), where each entry corresponds to the wrap-around error for Cases 1a, 1b, 2a, 2b, 3a, 3b, and 3c, in that order.", "solution": "The problem statement is a valid exercise in computational mathematics and digital signal processing, specifically concerning the relationship between linear and circular convolutions. It is scientifically grounded, well-posed, objective, and contains all necessary information to proceed.\n\nThe solution will be presented in three parts, corresponding to the theoretical exposition requested in the problem, followed by the implementation details which will inform the final program.\n\n**1. Definitions: Linear and Circular Convolution**\n\nLet $f, g: \\mathbb{R} \\to \\mathbb{R}$ be two integrable functions. Their **linear convolution**, denoted $(f * g)$, is a function on $\\mathbb{R}$ defined by the integral:\n$$ (f * g)(z) = \\int_{-\\infty}^{\\infty} f(x) g(z-x) \\,dx $$\nIn the context of probability, if $X$ and $Y$ are independent random variables with probability density functions (PDFs) $f_X$ and $f_Y$, the PDF of their sum $Z = X+Y$ is given by the linear convolution $f_Z = f_X * f_Y$.\n\nTo approximate this computationally, we sample the functions on a uniform grid with spacing $h > 0$. Let $x_k = f(kh)$ and $y_k = g(kh)$. Assuming the functions have finite support, we obtain two finite sequences, $(x_k)_{k=0}^{n-1}$ and $(y_k)_{k=0}^{m-1}$. The integral transforms into a sum, defining the **discrete linear convolution** of the sequences, denoted $c^{\\mathrm{lin}} = x * y$:\n$$ c^{\\mathrm{lin}}_k = (x * y)_k = \\sum_{j=-\\infty}^{\\infty} x_j y_{k-j} $$\nGiven that $x_j=0$ for $j \\notin \\{0, \\dots, n-1\\}$, and $y_j=0$ for $j \\notin \\{0, \\dots, m-1\\}$, the sum simplifies to $c^{\\mathrm{lin}}_k = \\sum_{j=0}^{n-1} x_j y_{k-j}$. The resulting sequence $c^{\\mathrm{lin}}$ is non-zero only for indices $k$ from $0 + 0 = 0$ to $(n-1) + (m-1) = n+m-2$. Thus, the discrete linear convolution produces a sequence of length $L = n+m-1$.\n\nThe **discrete circular convolution** is defined for two sequences of the same length, say $(a_k)_{k=0}^{N-1}$ and $(b_k)_{k=0}^{N-1}$. Their circular convolution, denoted $c^{\\mathrm{circ}}_N = a \\circledast_N b$, is a sequence of length $N$ defined as:\n$$ c^{\\mathrm{circ}}_{N,k} = (a \\circledast_N b)_k = \\sum_{j=0}^{N-1} a_j b_{(k-j) \\pmod N} $$\nwhere $(k-j) \\pmod N$ is the remainder of the division of $k-j$ by $N$. This operation is mathematically a convolution on the cyclic group of integers modulo $N$, $\\mathbb{Z}_N$.\n\nThis is equivalent to the convolution of the $N$-periodic extensions of the sequences. Let $\\tilde{a}$ and $\\tilde{b}$ be the $N$-periodic extensions, i.e., $\\tilde{a}_k = a_{k \\pmod N}$ and $\\tilde{b}_k = b_{k \\pmod N}$ for all $k \\in \\mathbb{Z}$. Then, one period of the linear convolution of these periodic sequences yields the circular convolution:\n$$ c^{\\mathrm{circ}}_{N,k} = \\sum_{j=0}^{N-1} \\tilde{a}_j \\tilde{b}_{k-j} \\quad \\text{for } k \\in \\{0, \\dots, N-1\\} $$\nThe Discrete Fourier Transform (DFT) naturally computes circular convolution. The convolution theorem for the DFT states that $\\text{DFT}(a \\circledast_N b) = \\text{DFT}(a) \\cdot \\text{DFT}(b)$, where the product is element-wise. An efficient implementation uses the Fast Fourier Transform (FFT) algorithm: $a \\circledast_N b = \\text{IDFT}(\\text{FFT}(a) \\cdot \\text{FFT}(b))$.\n\n**2. Zero-Padding Strategy for Linear Convolution**\n\nTo compute the discrete linear convolution $c^{\\mathrm{lin}}$ of two sequences $(x_k)_{k=0}^{n-1}$ and $(y_k)_{k=0}^{m-1}$ using circular convolution, we must prevent the \"wrap-around\" or \"aliasing\" effect inherent in the circular operation. The result of the linear convolution, $c^{\\mathrm{lin}}$, has length $L = n+m-1$.\n\nThe aliasing occurs when the result of the linear convolution is longer than the period $N$ of the circular convolution. Specifically, the circular convolution result $c^{\\mathrm{circ}}_N$ is related to the linear convolution $c^{\\mathrm{lin}}$ by the aliasing formula:\n$$ c^{\\mathrm{circ}}_{N,k} = \\sum_{j=-\\infty}^{\\infty} c^{\\mathrm{lin}}_{k+jN} $$\nFor the circular convolution to exactly match the linear convolution, we need $c^{\\mathrm{circ}}_{N,k} = c^{\\mathrm{lin}}_k$ for all relevant indices $k$. Since $c^{\\mathrm{lin}}_k$ is non-zero only for $k \\in \\{0, \\dots, L-1\\}$, the sum simplifies to $c^{\\mathrm{circ}}_{N,k} = c^{\\mathrm{lin}}_k + c^{\\mathrm{lin}}_{k+N} + c^{\\mathrm{lin}}_{k+2N} + \\dots$. For equality to hold, all terms $c^{\\mathrm{lin}}_{k+jN}$ for $j \\ge 1$ must be zero.\n\nThis requires that the support of $c^{\\mathrm{lin}}$, which is $[0, L-1]$, does not \"wrap around\" the period of length $N$. This is guaranteed if and only if the period $N$ is at least as long as the support of the linear convolution, $L$.\n\nThe strategy is as follows:\n1.  Choose a convolution length $N$.\n2.  Pad the sequence $x$ with zeros to create a new sequence $x'$ of length $N$.\n3.  Pad the sequence $y$ with zeros to create a new sequence $y'$ of length $N$.\n4.  Compute the circular convolution $c' = x' \\circledast_N y'$.\n\nThe result $c'$ will be identical to the linear convolution $c^{\\mathrm{lin}}$ (padded with zeros to length $N$) if and only if no aliasing occurs. Aliasing is avoided if and only if the length $N$ is greater than or equal to the length of the linear convolution result.\n\nThe necessary and sufficient condition on the padding length $N$ to exactly recover the discrete linear convolution of sequences of length $n$ and $m$ is:\n$$ N \\ge n+m-1 $$\n\nIf this condition is met, the first $L=n+m-1$ samples of the circular convolution of the padded sequences will be identical to the samples of the discrete linear convolution of the original sequences. If $N < n+m-1$, wrap-around distortion occurs.\n\n**3. Wrap-Around Error Quantification**\n\nLet $(x_k)_{k=0}^{n-1}$ and $(y_k)_{k=0}^{m-1}$ be non-negative sequences. Their discrete linear convolution is $c^{\\mathrm{lin}}$, a sequence of length $L=n+m-1$. We compute $c^{\\mathrm{circ}}_N$ as the circular convolution of $x$ and $y$ padded to length $N$. The \"wrap-around approximation,\" $\\hat{c}$, of length $L$ is formed by periodically repeating $c^{\\mathrm{circ}}_N$:\n$$ \\hat{c}_k = c^{\\mathrm{circ}}_{N, k \\pmod N} \\quad \\text{for } k \\in \\{0, \\dots, L-1\\} $$\nThe wrap-around error is the $\\ell_1$-distance between the approximation and the true linear convolution:\n$$ E = ||\\hat{c} - c^{\\mathrm{lin}}||_1 = \\sum_{k=0}^{L-1} |\\hat{c}_k - c^{\\mathrm{lin}}_k| $$\nUsing the aliasing formula, $c^{\\mathrm{circ}}_{N, i} = \\sum_{j=0}^{\\infty} c^{\\mathrm{lin}}_{i+jN}$ for $i \\in \\{0, \\dots, N-1\\}$. Substituting this into the definition for $\\hat{c}_k$, the error expression becomes:\n$$ E = \\sum_{k=0}^{L-1} \\left| \\left( \\sum_{j=0}^{\\infty} c^{\\mathrm{lin}}_{(k \\pmod N) + jN} \\right) - c^{\\mathrm{lin}}_k \\right| $$\nSince all sequences are non-negative, $c^{\\mathrm{lin}}_k \\ge 0$ for all $k$.\nWe can split the sum for $E$ into two parts: indices below $N$ and indices from $N$ to $L-1$.\n\nFor $k \\in \\{0, \\dots, N-1\\}$ (assuming $L>N$):\n$ k \\pmod N = k$. The error term is $|\\hat{c}_k - c^{\\mathrm{lin}}_k| = |(\\sum_{j=0}^{\\infty} c^{\\mathrm{lin}}_{k+jN}) - c^{\\mathrm{lin}}_k| = |\\sum_{j=1}^{\\infty} c^{\\mathrm{lin}}_{k+jN}| = \\sum_{j=1}^{\\infty} c^{\\mathrm{lin}}_{k+jN}$, since $c^{\\mathrm{lin}} \\ge 0$.\nThe sum of these contributions is $\\sum_{k=0}^{N-1} \\sum_{j=1}^{\\infty} c^{\\mathrm{lin}}_{k+jN}$. This sum covers all elements of $c^{\\mathrm{lin}}$ with indices greater than or equal to $N$. Let $S_{\\mathrm{tail}} = \\sum_{m=N}^{L-1} c^{\\mathrm{lin}}_m$ be the \"tail mass\" of the linear convolution. The error contribution from the first $N$ elements is exactly $S_{\\mathrm{tail}}$.\n\nFor $k \\in \\{N, \\dots, L-1\\}$:\nThe error contribution is $\\sum_{k=N}^{L-1} |\\hat{c}_k - c^{\\mathrm{lin}}_k|$. These terms represent the error from replacing the true tail values $c^{\\mathrm{lin}}_k$ with periodically repeated aliased values $\\hat{c}_k$.\n\nThe total error is the sum of contributions from all indices $k \\in \\{0, \\dots, L-1\\}$. While a closed-form simplification of the full expression is complicated, a simple bound can be obtained. The total mass that is misplaced is $S_{\\mathrm{tail}}$. This mass is removed from the tail (indices $k \\ge N$) and added to the head (indices $k < N$). The $\\ell_1$ error is the sum of the absolute values of all such changes. The sum of mass added to the head is $S_{\\mathrm{tail}}$. The sum of mass removed from the tail is also $S_{\\mathrm{tail}}$. This suggests a total change of $2S_{\\mathrm{tail}}$. A more rigorous argument shows that $E \\le 2S_{\\mathrm{tail}}$. This provides a useful bound: the wrap-around error is controlled by the mass of the linear convolution's tail that falls outside the circular convolution window of length $N$. In the implementation below, we compute the exact error $E$ as defined by the summation, not the bound.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom numpy.fft import fft, ifft\n\ndef solve():\n    \"\"\"\n    Computes the wrap-around error for several test cases of discrete convolution.\n    \"\"\"\n\n    test_cases = []\n\n    # Case 1a: Uniform distributions, N < L\n    x1 = np.full(10, 1/10)\n    y1 = np.full(5, 1/5)\n    test_cases.append({'x': x1, 'y': y1, 'N': 10, 'name': '1a'})\n\n    # Case 1b: Uniform distributions, N = L\n    test_cases.append({'x': x1, 'y': y1, 'N': 14, 'name': '1b'})\n\n    # Case 2a: Delta and Triangular, N < L\n    x2 = np.zeros(8)\n    x2[7] = 1.0\n    y2_unnormalized = np.array([1, 2, 3, 4, 4, 3, 2, 1])\n    y2 = y2_unnormalized / np.sum(y2_unnormalized)\n    test_cases.append({'x': x2, 'y': y2, 'N': 8, 'name': '2a'})\n\n    # Case 2b: Delta and Triangular, N = L\n    test_cases.append({'x': x2, 'y': y2, 'N': 15, 'name': '2b'})\n\n    # Case 3: Geometric distributions\n    n_geom = 64\n    p_x = 0.2\n    k = np.arange(n_geom)\n    x3_unnormalized = p_x * (1 - p_x)**k\n    x3 = x3_unnormalized / np.sum(x3_unnormalized)\n    \n    p_y = 0.35\n    y3_unnormalized = p_y * (1 - p_y)**k\n    y3 = y3_unnormalized / np.sum(y3_unnormalized)\n\n    # Case 3a: Geometric, N < L\n    test_cases.append({'x': x3, 'y': y3, 'N': 64, 'name': '3a'})\n\n    # Case 3b: Geometric, N = L\n    test_cases.append({'x': x3, 'y': y3, 'N': 127, 'name': '3b'})\n    \n    # Case 3c: Geometric, N > L\n    test_cases.append({'x': x3, 'y': y3, 'N': 128, 'name': '3c'})\n\n    results = []\n    for case in test_cases:\n        x, y, N = case['x'], case['y'], case['N']\n        n = len(x)\n        m = len(y)\n        L = n + m - 1\n\n        # 1. Compute discrete linear convolution (ground truth)\n        c_lin = np.convolve(x, y)\n\n        # 2. Compute discrete circular convolution using FFT\n        # Pad original sequences to length N for circular convolution\n        x_pad = np.zeros(N)\n        y_pad = np.zeros(N)\n        x_pad[:n] = x\n        y_pad[:m] = y\n        \n        # Use FFT to compute circular convolution\n        c_circ_N = np.real(ifft(fft(x_pad) * fft(y_pad)))\n\n        # 3. Form the wrap-around approximation of length L\n        # This is done by tiling c_circ_N periodically\n        indices = np.arange(L) % N\n        c_approx = c_circ_N[indices]\n\n        # 4. Compute the wrap-around error (L1 distance)\n        error = np.sum(np.abs(c_approx - c_lin))\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3357868"}, {"introduction": "The change-of-variables formula is a cornerstone of probability theory, but its application becomes challenging when dealing with distributions on constrained domains where the support is not the entire space $\\mathbb{R}^n$. A common pitfall is incorrectly handling how the boundaries of the support are transformed. This advanced exercise ([@problem_id:3357917]) provides a rigorous workout for this skill by analyzing a transformation from a triangular simplex, forcing a careful treatment of how the Jacobian interacts with the indicator functions that define the domain.", "problem": "Let $(X_{1}, X_{2})$ be uniformly distributed over the closed two-dimensional simplex (a right triangle with corners) \n$$\n\\mathcal{T} \\equiv \\{(x_{1}, x_{2}) \\in \\mathbb{R}^{2} : x_{1} \\ge 0,\\ x_{2} \\ge 0,\\ x_{1} + x_{2} \\le 1\\}.\n$$\nDefine the transformation that reparametrizes $X_{1}$ and $X_{2}$ in terms of their ratio to the remaining mass to the hypotenuse:\n$$\nU \\equiv \\frac{X_{1}}{1 - X_{1} - X_{2}}, \\qquad V \\equiv \\frac{X_{2}}{1 - X_{1} - X_{2}}.\n$$\nThis mapping sends the interior of the simplex $\\mathcal{T}$ to the nonnegative quadrant $\\{(u,v) \\in \\mathbb{R}^{2}: u \\ge 0, v \\ge 0\\}$, thus producing a nontrivial interaction between the Jacobian and the indicator functions that enforce the support constraints. Let $R \\equiv U + V$.\n\nStarting from first principles, namely the definition of the uniform distribution on $\\mathcal{T}$ and the change-of-variables formula with Jacobians and indicator functions, derive the probability density function of $R$ by explicitly tracking how the boundary of $\\mathcal{T}$ (including its corners and the hypotenuse) is mapped into the $(U,V)$-plane, and how the support is represented through indicator functions. Use this to compute the expectation\n$$\n\\mathbb{E}\\!\\left[\\frac{1}{1 + R}\\right].\n$$\nExpress your final answer as an exact value. No rounding is required.", "solution": "The problem is well-posed and scientifically grounded\nin the field of multivariate probability theory. All definitions are precise, and a unique solution exists. We shall proceed with the derivation as specified.\n\nThe random vector $(X_{1}, X_{2})$ is uniformly distributed over the simplex\n$$\n\\mathcal{T} \\equiv \\{(x_{1}, x_{2}) \\in \\mathbb{R}^{2} : x_{1} \\ge 0,\\ x_{2} \\ge 0,\\ x_{1} + x_{2} \\le 1\\}.\n$$\nThe area of this simplex, a right triangle with vertices at $(0,0)$, $(1,0)$, and $(0,1)$, is $\\frac{1}{2}(1)(1) = \\frac{1}{2}$. The joint probability density function (PDF) of $(X_{1}, X_{2})$ is therefore constant over $\\mathcal{T}$:\n$$\nf_{X_{1}, X_{2}}(x_{1}, x_{2}) = \\frac{1}{\\text{Area}(\\mathcal{T})} = 2, \\quad \\text{for } (x_{1}, x_{2}) \\in \\mathcal{T}.\n$$\nWe can express this using an indicator function $\\mathbb{I}_{\\mathcal{T}}(x_1, x_2)$, which is $1$ if $(x_{1}, x_{2}) \\in \\mathcal{T}$ and $0$ otherwise:\n$$\nf_{X_{1}, X_{2}}(x_{1}, x_{2}) = 2 \\, \\mathbb{I}_{\\mathcal{T}}(x_{1}, x_{2}).\n$$\nWe are asked to find the PDF of $R \\equiv U + V$ and then compute $\\mathbb{E}[\\frac{1}{1+R}]$, where $U$ and $V$ are defined by the transformation:\n$$\nU = \\frac{X_{1}}{1 - X_{1} - X_{2}}, \\qquad V = \\frac{X_{2}}{1 - X_{1} - X_{2}}.\n$$\nThe solution will proceed in three stages as required by the problem statement:\n1. Derive the joint PDF of $(U,V)$ using the change of variables formula.\n2. Derive the PDF of $R = U+V$.\n3. Compute the required expectation using the PDF of $R$.\n\nStage 1: Derivation of the joint PDF of $(U,V)$.\nThe change of variables formula states that $f_{U,V}(u,v) = f_{X_{1},X_{2}}(x_{1}(u,v), x_{2}(u,v)) |J|$, where $J$ is the Jacobian determinant of the inverse transformation.\n\nFirst, we find the inverse transformation from $(u,v)$ back to $(x_{1},x_{2})$. We have $R = U+V = \\frac{X_{1}+X_{2}}{1-X_{1}-X_{2}}$. We can write this as $R(1 - (X_{1}+X_{2})) = X_{1}+X_{2}$, which gives $R = (X_{1}+X_{2})(1+R)$, so $X_{1}+X_{2} = \\frac{R}{1+R}$.\nConsequently, $1-X_{1}-X_{2} = 1 - \\frac{R}{1+R} = \\frac{1}{1+R}$.\nSubstituting this back into the definitions of $U$ and $V$, and replacing $R$ with $u+v$:\n$$\nx_{1} = u (1-x_{1}-x_{2}) = u \\left(\\frac{1}{1+u+v}\\right) = \\frac{u}{1+u+v}\n$$\n$$\nx_{2} = v (1-x_{1}-x_{2}) = v \\left(\\frac{1}{1+u+v}\\right) = \\frac{v}{1+u+v}\n$$\nThis defines the inverse transformation $(x_{1}(u,v), x_{2}(u,v))$.\n\nNext, we compute the Jacobian determinant $J = \\det\\left(\\frac{\\partial(x_{1},x_{2})}{\\partial(u,v)}\\right)$:\n$$\n\\frac{\\partial x_{1}}{\\partial u} = \\frac{1(1+u+v) - u(1)}{(1+u+v)^{2}} = \\frac{1+v}{(1+u+v)^{2}}\n$$\n$$\n\\frac{\\partial x_{1}}{\\partial v} = \\frac{0 - u(1)}{(1+u+v)^{2}} = \\frac{-u}{(1+u+v)^{2}}\n$$\n$$\n\\frac{\\partial x_{2}}{\\partial u} = \\frac{0 - v(1)}{(1+u+v)^{2}} = \\frac{-v}{(1+u+v)^{2}}\n$$\n$$\n\\frac{\\partial x_{2}}{\\partial v} = \\frac{1(1+u+v) - v(1)}{(1+u+v)^{2}} = \\frac{1+u}{(1+u+v)^{2}}\n$$\nThe Jacobian determinant is:\n$$\nJ = \\left(\\frac{1+v}{(1+u+v)^{2}}\\right)\\left(\\frac{1+u}{(1+u+v)^{2}}\\right) - \\left(\\frac{-u}{(1+u+v)^{2}}\\right)\\left(\\frac{-v}{(1+u+v)^{2}}\\right)\n$$\n$$\nJ = \\frac{(1+v)(1+u) - uv}{(1+u+v)^{4}} = \\frac{1+u+v+uv-uv}{(1+u+v)^{4}} = \\frac{1+u+v}{(1+u+v)^{4}} = \\frac{1}{(1+u+v)^{3}}\n$$\nThe support of $(U,V)$ is determined by the mapping of the support of $(X_{1},X_{2})$. The interior of $\\mathcal{T}$ is defined by $x_{1}>0$, $x_{2}>0$, and $x_{1}+x_{2}<1$.\n- $x_{1} > 0 \\implies \\frac{u}{1+u+v} > 0 \\implies u > 0$.\n- $x_{2} > 0 \\implies \\frac{v}{1+u+v} > 0 \\implies v > 0$.\n- $x_{1}+x_{2} < 1 \\implies \\frac{u+v}{1+u+v} < 1$, which is always true for $u,v \\ge 0$.\nThe boundary $x_1=0$ maps to $u=0$. The boundary $x_2=0$ maps to $v=0$. The hypotenuse $x_1+x_2=1$ maps to infinity in the $(u,v)$ plane. Thus, the simplex $\\mathcal{T}$ is mapped to the non-negative quadrant $\\{(u,v) \\in \\mathbb{R}^2 : u \\ge 0, v \\ge 0\\}$. For any $(u,v)$ in this quadrant, the corresponding $(x_1, x_2)$ values satisfy $x_1 \\ge 0$, $x_2 \\ge 0$, and $x_1+x_2 = \\frac{u+v}{1+u+v} \\le 1$. Therefore, the indicator function $\\mathbb{I}_{\\mathcal{T}}(x_{1}(u,v), x_{2}(u,v))$ becomes $\\mathbb{I}_{u \\ge 0, v \\ge 0}$.\nThe absolute value of the Jacobian is $|J| = \\frac{1}{(1+u+v)^{3}}$ for $u,v \\ge 0$.\n\nThe joint PDF of $(U,V)$ is:\n$$\nf_{U,V}(u,v) = f_{X_{1}, X_{2}}(x_{1}(u,v), x_{2}(u,v)) |J| = 2 \\cdot \\mathbb{I}_{u \\ge 0, v \\ge 0} \\cdot \\frac{1}{(1+u+v)^{3}}\n$$\nSo, $f_{U,V}(u,v) = \\frac{2}{(1+u+v)^{3}}$ for $u \\ge 0, v \\ge 0$, and $0$ otherwise.\n\nStage 2: Derivation of the PDF of $R = U+V$.\nTo find the marginal PDF of $R$, we introduce an auxiliary variable, for instance $S=U$. We perform a change of variables from $(U,V)$ to $(R,S)$. The inverse transformation is $U=S$ and $V=R-S$. The Jacobian of this linear transformation is:\n$$\nJ' = \\det\\begin{pmatrix} \\frac{\\partial U}{\\partial R} & \\frac{\\partial U}{\\partial S} \\\\ \\frac{\\partial V}{\\partial R} & \\frac{\\partial V}{\\partial S} \\end{pmatrix} = \\det\\begin{pmatrix} 0 & 1 \\\\ 1 & -1 \\end{pmatrix} = -1.\n$$\nThe absolute value is $|J'| = 1$. The joint PDF of $(R,S)$ is $f_{R,S}(r,s) = f_{U,V}(u(r,s), v(r,s)) |J'|$.\n$$\nf_{R,S}(r,s) = f_{U,V}(s, r-s) \\cdot 1 = \\frac{2}{(1+s+(r-s))^{3}} = \\frac{2}{(1+r)^{3}}.\n$$\nThe support for $(R,S)$ is determined by the support of $(U,V)$:\n- $u \\ge 0 \\implies s \\ge 0$.\n- $v \\ge 0 \\implies r-s \\ge 0 \\implies s \\le r$.\n- Since $u,v \\ge 0$, we have $r = u+v \\ge 0$.\nThe support is $\\{(r,s) \\in \\mathbb{R}^2 : r \\ge 0, 0 \\le s \\le r\\}$.\nThe marginal PDF of $R$, denoted $f_{R}(r)$, is found by integrating $f_{R,S}(r,s)$ over all possible values of $s$:\n$$\nf_{R}(r) = \\int_{0}^{r} f_{R,S}(r,s) \\, ds = \\int_{0}^{r} \\frac{2}{(1+r)^{3}} \\, ds.\n$$\nSince the integrand is constant with respect to $s$:\n$$\nf_{R}(r) = \\frac{2}{(1+r)^{3}} [s]_{0}^{r} = \\frac{2r}{(1+r)^{3}}, \\quad \\text{for } r \\ge 0.\n$$\nThis is the required PDF for the random variable $R$.\n\nStage 3: Computation of the expectation.\nWe are asked to compute $\\mathbb{E}\\!\\left[\\frac{1}{1 + R}\\right]$. Using the PDF $f_{R}(r)$ derived above:\n$$\n\\mathbb{E}\\!\\left[\\frac{1}{1 + R}\\right] = \\int_{0}^{\\infty} \\frac{1}{1+r} f_{R}(r) \\, dr = \\int_{0}^{\\infty} \\frac{1}{1+r} \\frac{2r}{(1+r)^{3}} \\, dr = \\int_{0}^{\\infty} \\frac{2r}{(1+r)^{4}} \\, dr.\n$$\nTo evaluate this integral, we use the substitution $z = 1+r$. This means $r = z-1$ and $dr=dz$. The limits of integration change from $[0, \\infty)$ for $r$ to $[1, \\infty)$ for $z$.\n$$\n\\mathbb{E}\\!\\left[\\frac{1}{1 + R}\\right] = \\int_{1}^{\\infty} \\frac{2(z-1)}{z^{4}} \\, dz = 2 \\int_{1}^{\\infty} \\left(\\frac{z}{z^{4}} - \\frac{1}{z^{4}}\\right) \\, dz = 2 \\int_{1}^{\\infty} (z^{-3} - z^{-4}) \\, dz.\n$$\nPerforming the integration:\n$$\n2 \\left[ \\frac{z^{-2}}{-2} - \\frac{z^{-3}}{-3} \\right]_{1}^{\\infty} = 2 \\left[ -\\frac{1}{2z^{2}} + \\frac{1}{3z^{3}} \\right]_{1}^{\\infty}.\n$$\nEvaluating at the limits:\n$$\n= 2 \\left( \\lim_{z\\to\\infty} \\left(-\\frac{1}{2z^{2}} + \\frac{1}{3z^{3}}\\right) - \\left(-\\frac{1}{2(1)^{2}} + \\frac{1}{3(1)^{3}}\\right) \\right)\n$$\n$$\n= 2 \\left( (0 + 0) - \\left(-\\frac{1}{2} + \\frac{1}{3}\\right) \\right) = 2 \\left( - \\left(-\\frac{1}{6}\\right) \\right) = 2 \\left(\\frac{1}{6}\\right) = \\frac{1}{3}.\n$$\nThe expected value is an exact rational number.\nAs a verification (not part of the required procedure), we note that $\\frac{1}{1+R} = \\frac{1}{1 + \\frac{X_1+X_2}{1-X_1-X_2}} = 1-X_1-X_2$. By linearity of expectation, $\\mathbb{E}[1-X_1-X_2] = 1 - \\mathbb{E}[X_1] - \\mathbb{E}[X_2]$. The marginal expectations for this symmetric distribution are $\\mathbb{E}[X_1]=\\mathbb{E}[X_2]=\\frac{1}{3}$. This gives $1 - \\frac{1}{3} - \\frac{1}{3} = \\frac{1}{3}$, confirming our result obtained through the formally requested procedure.", "answer": "$$\n\\boxed{\\frac{1}{3}}\n$$", "id": "3357917"}]}