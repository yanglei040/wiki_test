{"hands_on_practices": [{"introduction": "This first practice is a cornerstone of Bayesian analysis, demonstrating how beliefs are formally updated in light of new evidence. We will explore the elegant concept of conjugate priors, where the posterior distribution belongs to the same family as the prior, which dramatically simplifies the mathematical challenge of Bayesian inference. By deriving the posterior for a Poisson rate parameter with a Gamma prior, you will gain fundamental, hands-on experience with the core mechanics of applying Bayes' theorem [@problem_id:3290545].", "problem": "Consider a count observation $x \\in \\{0,1,2,\\dots\\}$ generated conditionally on a rate parameter $\\lambda>0$ according to a Poisson likelihood with exposure $n>0$, that is, $x \\mid \\lambda \\sim \\mathrm{Poisson}(n \\lambda)$. Assume a Gamma prior for $\\lambda$ with shape $a>0$ and rate $b>0$, written in the shape-rate parameterization, that is, the prior density for $\\lambda$ is $p(\\lambda) = \\frac{b^{a}}{\\Gamma(a)} \\lambda^{a-1} \\exp(-b \\lambda)$ for $\\lambda>0$, where $\\Gamma(\\cdot)$ denotes the Gamma function.\n\nUsing Bayes' theorem for continuous parameters and the standard form of the Poisson likelihood $p(x \\mid \\lambda) = \\exp(-n \\lambda) \\frac{(n \\lambda)^{x}}{x!}$, derive the posterior density $p(\\lambda \\mid x)$, identify its distributional family, and determine the updated hyperparameters explicitly in terms of $a$, $b$, $n$, and $x$.\n\nThen, compute the posterior mean $\\mathbb{E}[\\lambda \\mid x]$ and the posterior variance $\\mathrm{Var}[\\lambda \\mid x]$ in closed form. Express your final answer as analytic expressions in terms of $a$, $b$, $n$, and $x$. No rounding is required.", "solution": "The problem is valid as it is a well-posed, scientifically grounded, and standard exercise in Bayesian statistics. It is self-contained, with all necessary definitions and conditions provided, and free from contradictions or ambiguities.\n\nThe objective is to derive the posterior distribution of a rate parameter $\\lambda$ and compute its mean and variance. We are given a Poisson likelihood for an observed count $x$ and a Gamma prior for $\\lambda$.\n\nThe given components are:\n1.  The likelihood function, derived from $x \\mid \\lambda \\sim \\mathrm{Poisson}(n\\lambda)$:\n    $$p(x \\mid \\lambda) = \\frac{(n\\lambda)^x \\exp(-n\\lambda)}{x!}$$\n    where $x \\in \\{0, 1, 2, \\dots\\}$, $n>0$, and $\\lambda>0$.\n2.  The prior distribution for $\\lambda$, which is a Gamma distribution with shape $a>0$ and rate $b>0$: $\\lambda \\sim \\mathrm{Gamma}(a, b)$.\n    The prior probability density function is:\n    $$p(\\lambda) = \\frac{b^a}{\\Gamma(a)} \\lambda^{a-1} \\exp(-b\\lambda)$$\n\nAccording to Bayes' theorem for continuous parameters, the posterior density $p(\\lambda \\mid x)$ is proportional to the product of the likelihood $p(x \\mid \\lambda)$ and the prior $p(\\lambda)$:\n$$p(\\lambda \\mid x) \\propto p(x \\mid \\lambda) p(\\lambda)$$\n\nSubstituting the given expressions for the likelihood and the prior:\n$$p(\\lambda \\mid x) \\propto \\left( \\frac{(n\\lambda)^x \\exp(-n\\lambda)}{x!} \\right) \\left( \\frac{b^a}{\\Gamma(a)} \\lambda^{a-1} \\exp(-b\\lambda) \\right)$$\n\nTo find the functional form of the posterior density with respect to $\\lambda$, we can collect all terms that are constant with respect to $\\lambda$ into the proportionality constant. These terms are $\\frac{n^x}{x!}$ and $\\frac{b^a}{\\Gamma(a)}$.\n$$p(\\lambda \\mid x) \\propto (\\lambda^x \\exp(-n\\lambda)) (\\lambda^{a-1} \\exp(-b\\lambda))$$\n\nNow, we combine the powers of $\\lambda$ and the arguments of the exponential functions:\n$$p(\\lambda \\mid x) \\propto \\lambda^{x} \\lambda^{a-1} \\exp(-n\\lambda) \\exp(-b\\lambda)$$\n$$p(\\lambda \\mid x) \\propto \\lambda^{x+a-1} \\exp(-(n+b)\\lambda)$$\n\nThis expression is the kernel of a probability density function for $\\lambda$. We recognize this as the kernel of a Gamma distribution. A Gamma distribution with shape parameter $\\alpha'$ and rate parameter $\\beta'$ has a probability density function proportional to $\\lambda^{\\alpha'-1} \\exp(-\\beta'\\lambda)$.\n\nBy comparing our result, $p(\\lambda \\mid x) \\propto \\lambda^{(a+x)-1} \\exp(-(b+n)\\lambda)$, with the general form, we can identify the parameters of the posterior distribution.\nThe posterior distribution of $\\lambda$ given $x$ is a Gamma distribution, which we can write as:\n$$\\lambda \\mid x \\sim \\mathrm{Gamma}(\\alpha', \\beta')$$\nThe updated hyperparameters (the posterior parameters) are:\n-   Posterior shape: $\\alpha' = a + x$\n-   Posterior rate: $\\beta' = b + n$\n\nThis demonstrates that the Gamma distribution is a conjugate prior for the rate parameter of a Poisson likelihood. The posterior density function is therefore:\n$$p(\\lambda \\mid x) = \\frac{(b+n)^{a+x}}{\\Gamma(a+x)} \\lambda^{(a+x)-1} \\exp(-(b+n)\\lambda)$$\n\nNext, we are asked to find the posterior mean $\\mathbb{E}[\\lambda \\mid x]$ and the posterior variance $\\mathrm{Var}[\\lambda \\mid x]$. For a random variable $Y$ that follows a Gamma distribution with shape $\\alpha$ and rate $\\beta$, i.e., $Y \\sim \\mathrm{Gamma}(\\alpha, \\beta)$, the mean and variance are given by the standard formulas:\n$$\\mathbb{E}[Y] = \\frac{\\alpha}{\\beta}$$\n$$\\mathrm{Var}[Y] = \\frac{\\alpha}{\\beta^2}$$\n\nApplying these formulas to our posterior distribution $\\lambda \\mid x \\sim \\mathrm{Gamma}(a+x, b+n)$, we substitute $\\alpha' = a+x$ for the shape and $\\beta' = b+n$ for the rate.\n\nThe posterior mean of $\\lambda$ is:\n$$\\mathbb{E}[\\lambda \\mid x] = \\frac{\\alpha'}{\\beta'} = \\frac{a+x}{b+n}$$\n\nThe posterior variance of $\\lambda$ is:\n$$\\mathrm{Var}[\\lambda \\mid x] = \\frac{\\alpha'}{(\\beta')^2} = \\frac{a+x}{(b+n)^2}$$\n\nThese are the closed-form expressions for the posterior mean and variance in terms of the given parameters $a, b, n,$ and the observation $x$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{a+x}{b+n} & \\frac{a+x}{(b+n)^2}\n\\end{pmatrix}\n}\n$$", "id": "3290545"}, {"introduction": "Building on the fundamentals, this exercise introduces the powerful and intuitive concept of hierarchical modeling. We will investigate how structuring priors in a hierarchy allows a Bayesian model to automatically share statistical strength across different groups, a phenomenon known as partial pooling. By deriving the posterior mean for a group-specific parameter, you will see how it manifests as a \"shrinkage\" estimator, which wisely balances information from the individual group with information pooled from all groups [@problem_id:3290513].", "problem": "Consider the hierarchical normal model frequently used in stochastic simulation and hierarchical Monte Carlo analyses. There are $m$ groups indexed by $i \\in \\{1,\\dots,m\\}$, and within each group $i$ there are $n_i$ observations $x_{ij}$, $j \\in \\{1,\\dots,n_i\\}$. The data model and priors are\n$$\nx_{ij} \\mid \\theta_i \\sim \\mathcal{N}(\\theta_i,\\sigma^2), \\quad \\theta_i \\mid \\mu \\sim \\mathcal{N}(\\mu,\\tau^2), \\quad \\mu \\sim \\mathcal{N}(m_0,s_0^2),\n$$\nwith $\\sigma^2>0$, $\\tau^2>0$, $s_0^2>0$ and $m_0 \\in \\mathbb{R}$ known. Let $\\bar{x}_i = \\frac{1}{n_i}\\sum_{j=1}^{n_i} x_{ij}$ denote the sample mean within group $i$, and let $x$ denote the full collection $\\{x_{ij}\\}_{i,j}$. Starting only from Bayes’ theorem and standard properties of the normal distribution, derive the closed-form expression for the posterior mean $E[\\theta_i \\mid x]$ under this model, and express it as a shrinkage estimator that linearly combines the within-group mean $\\bar{x}_i$ and a data-informed overall location. Your final answer must be a single closed-form analytic expression for $E[\\theta_i \\mid x]$ in terms of $\\bar{x}_i$, $n_i$, $\\sigma^2$, $\\tau^2$, $m_0$, $s_0^2$, and the collection $\\{\\bar{x}_k,n_k\\}_{k=1}^m$. No numerical approximation is required, and no units apply.", "solution": "The problem statement is valid. It describes a standard hierarchical normal model, a fundamental topic in Bayesian statistics and stochastic simulation, and asks for a rigorous derivation of a posterior mean. All parameters and distributions are clearly defined, and the task is well-posed. We proceed with the solution.\n\nThe problem is to derive the posterior mean $E[\\theta_i \\mid x]$ for the hierarchical model specified by:\n$1$. Data model: $x_{ij} \\mid \\theta_i \\sim \\mathcal{N}(\\theta_i, \\sigma^2)$ for $i \\in \\{1,\\dots,m\\}$ and $j \\in \\{1,\\dots,n_i\\}$.\n$2$. First-level prior: $\\theta_i \\mid \\mu \\sim \\mathcal{N}(\\mu, \\tau^2)$.\n$3$. Hyperprior: $\\mu \\sim \\mathcal{N}(m_0, s_0^2)$.\nThe parameters $\\sigma^2$, $\\tau^2$, $s_0^2$, and $m_0$ are known constants. The full data is denoted by $x = \\{x_{ij}\\}_{i,j}$.\n\nThe derivation proceeds in three main stages. First, we determine the conditional posterior expectation of $\\theta_i$ given both the data $x$ and the hyperparameter $\\mu$. Second, we determine the posterior distribution of $\\mu$ given the data $x$. Finally, we combine these results using the law of total expectation to find the marginal posterior expectation $E[\\theta_i \\mid x]$.\n\n**Step 1: Conditional Posterior Expectation $E[\\theta_i \\mid x, \\mu]$**\n\nWe begin by finding the posterior distribution of $\\theta_i$ conditional on $\\mu$ and the data $x$. Due to the model structure, the observations $x_{ij}$ for a given group $i$ are conditionally independent of $\\mu$ and all other data $x_{kj}$ ($k \\neq i$) once $\\theta_i$ is known. Therefore, $p(\\theta_i \\mid x, \\mu) = p(\\theta_i \\mid \\{x_{ij}\\}_{j=1}^{n_i}, \\mu)$.\nThe sufficient statistic for $\\theta_i$ from the data $\\{x_{ij}\\}_{j=1}^{n_i}$ is the sample mean $\\bar{x}_i = \\frac{1}{n_i}\\sum_{j=1}^{n_i} x_{ij}$. The sampling distribution of this statistic is $\\bar{x}_i \\mid \\theta_i \\sim \\mathcal{N}(\\theta_i, \\sigma^2/n_i)$.\n\nWe apply Bayes' theorem to find the posterior for $\\theta_i$:\n$$ p(\\theta_i \\mid \\bar{x}_i, \\mu) \\propto p(\\bar{x}_i \\mid \\theta_i) p(\\theta_i \\mid \\mu) $$\nThe likelihood is $p(\\bar{x}_i \\mid \\theta_i)$, corresponding to $\\mathcal{N}(\\theta_i, \\sigma^2/n_i)$, and the prior is $p(\\theta_i \\mid \\mu)$, corresponding to $\\mathcal{N}(\\mu, \\tau^2)$. The product of two normal probability density functions is proportional to the exponential of a quadratic function of $\\theta_i$, which implies the posterior is also normal. For a normal-normal conjugate pair, the posterior mean is a precision-weighted average of the prior mean and the likelihood mean.\n\nLet the precision of the likelihood be $\\pi_{\\text{data}, i} = \\frac{1}{\\sigma^2/n_i} = \\frac{n_i}{\\sigma^2}$ and the precision of the prior be $\\pi_{\\text{prior}} = \\frac{1}{\\tau^2}$.\nThe posterior mean is:\n$$ E[\\theta_i \\mid x, \\mu] = E[\\theta_i \\mid \\bar{x}_i, \\mu] = \\frac{\\pi_{\\text{data}, i} \\cdot \\bar{x}_i + \\pi_{\\text{prior}} \\cdot \\mu}{\\pi_{\\text{data}, i} + \\pi_{\\text{prior}}} = \\frac{\\frac{n_i}{\\sigma^2}\\bar{x}_i + \\frac{1}{\\tau^2}\\mu}{\\frac{n_i}{\\sigma^2} + \\frac{1}{\\tau^2}} $$\nMultiplying the numerator and denominator by $\\sigma^2\\tau^2$ gives:\n$$ E[\\theta_i \\mid x, \\mu] = \\frac{n_i\\tau^2 \\bar{x}_i + \\sigma^2 \\mu}{n_i\\tau^2 + \\sigma^2} $$\nThis expression can be written as a weighted average, or a \"shrinkage\" estimator:\n$$ E[\\theta_i \\mid x, \\mu] = \\left(\\frac{n_i\\tau^2}{n_i\\tau^2 + \\sigma^2}\\right)\\bar{x}_i + \\left(\\frac{\\sigma^2}{n_i\\tau^2 + \\sigma^2}\\right)\\mu $$\nLet $B_i = \\frac{n_i\\tau^2}{n_i\\tau^2 + \\sigma^2}$. Then $1-B_i = \\frac{\\sigma^2}{n_i\\tau^2 + \\sigma^2}$, and we have $E[\\theta_i \\mid x, \\mu] = B_i \\bar{x}_i + (1-B_i)\\mu$.\n\n**Step 2: Posterior Distribution of $\\mu$ given $x$**\n\nNext, we derive the posterior distribution $p(\\mu \\mid x)$. By Bayes' theorem:\n$$ p(\\mu \\mid x) \\propto p(x \\mid \\mu) p(\\mu) $$\nThe prior is given as $p(\\mu) \\sim \\mathcal{N}(m_0, s_0^2)$.\nThe likelihood $p(x \\mid \\mu)$ can be expressed in terms of the sufficient statistics $\\{\\bar{x}_k\\}_{k=1}^m$:\n$$ p(x \\mid \\mu) = p(\\{\\bar{x}_k\\}_{k=1}^m \\mid \\mu) = \\prod_{k=1}^m p(\\bar{x}_k \\mid \\mu) $$\nwhere the conditional independence of the $\\bar{x}_k$ given $\\mu$ is used.\nTo find $p(\\bar{x}_k \\mid \\mu)$, we integrate out the intermediate parameter $\\theta_k$:\n$$ p(\\bar{x}_k \\mid \\mu) = \\int p(\\bar{x}_k \\mid \\theta_k) p(\\theta_k \\mid \\mu) d\\theta_k $$\nThis is a convolution of two normal distributions: $\\bar{x}_k \\mid \\theta_k \\sim \\mathcal{N}(\\theta_k, \\sigma^2/n_k)$ and $\\theta_k \\mid \\mu \\sim \\mathcal{N}(\\mu, \\tau^2)$. The resulting distribution for $\\bar{x}_k$ is also normal, with mean equal to the sum of the means and variance equal to the sum of the variances:\n$$ \\bar{x}_k \\mid \\mu \\sim \\mathcal{N}(\\mu, \\tau^2 + \\sigma^2/n_k) $$\nNow we have a standard problem of finding the posterior for the mean $\\mu$ of a normal distribution with a normal prior, given $m$ independent observations $\\{\\bar{x}_k\\}$ with different known variances.\nThe posterior $p(\\mu \\mid x)$ is normal. The posterior mean $E[\\mu \\mid x]$ is a precision-weighted average of the prior mean $m_0$ and the observations $\\bar{x}_k$.\nThe precision of the prior for $\\mu$ is $\\pi_{\\mu,0} = \\frac{1}{s_0^2}$.\nThe precision of the $k$-th observation $\\bar{x}_k$ is $\\pi_{\\mu,k} = \\frac{1}{\\tau^2 + \\sigma^2/n_k}$.\nThe posterior mean $E[\\mu \\mid x]$ is:\n$$ E[\\mu \\mid x] = \\frac{\\pi_{\\mu,0} m_0 + \\sum_{k=1}^m \\pi_{\\mu,k} \\bar{x}_k}{\\pi_{\\mu,0} + \\sum_{k=1}^m \\pi_{\\mu,k}} = \\frac{\\frac{1}{s_0^2} m_0 + \\sum_{k=1}^m \\frac{1}{\\tau^2 + \\sigma^2/n_k} \\bar{x}_k}{\\frac{1}{s_0^2} + \\sum_{k=1}^m \\frac{1}{\\tau^2 + \\sigma^2/n_k}} $$\nFor algebraic clarity, we can write $\\frac{1}{\\tau^2 + \\sigma^2/n_k} = \\frac{n_k}{n_k\\tau^2 + \\sigma^2}$. Let's denote $\\hat{\\mu} = E[\\mu \\mid x]$. Then\n$$ \\hat{\\mu} = \\frac{\\frac{m_0}{s_0^2} + \\sum_{k=1}^m \\frac{n_k \\bar{x}_k}{n_k\\tau^2 + \\sigma^2}}{\\frac{1}{s_0^2} + \\sum_{k=1}^m \\frac{n_k}{n_k\\tau^2 + \\sigma^2}} $$\n\n**Step 3: Final Unconditional Posterior Expectation $E[\\theta_i \\mid x]$**\n\nFinally, we use the law of total expectation (also known as the tower property or smoothing property):\n$$ E[\\theta_i \\mid x] = E_{\\mu \\mid x}\\left[ E[\\theta_i \\mid x, \\mu] \\right] $$\nSubstituting the expression for $E[\\theta_i \\mid x, \\mu]$ from Step 1:\n$$ E[\\theta_i \\mid x] = E_{\\mu \\mid x}\\left[ B_i \\bar{x}_i + (1-B_i)\\mu \\right] $$\nSince $B_i$ and $\\bar{x}_i$ are treated as constants in this expectation (they do not depend on the random variable $\\mu$ in the posterior $p(\\mu \\mid x)$), we can write:\n$$ E[\\theta_i \\mid x] = B_i \\bar{x}_i + (1-B_i) E_{\\mu \\mid x}[\\mu] = B_i \\bar{x}_i + (1-B_i)\\hat{\\mu} $$\nSubstituting the expressions for $B_i$ and $\\hat{\\mu}$:\n$$ E[\\theta_i \\mid x] = \\left(\\frac{n_i\\tau^2}{n_i\\tau^2 + \\sigma^2}\\right)\\bar{x}_i + \\left(\\frac{\\sigma^2}{n_i\\tau^2 + \\sigma^2}\\right) \\hat{\\mu} $$\nwhere $\\hat{\\mu}$ is the data-informed overall location derived in Step 2.\nThe full expression is therefore:\n$$ E[\\theta_i \\mid x] = \\left(\\frac{n_i\\tau^2}{n_i\\tau^2 + \\sigma^2}\\right)\\bar{x}_i + \\left(\\frac{\\sigma^2}{n_i\\tau^2 + \\sigma^2}\\right) \\left( \\frac{\\frac{m_0}{s_0^2} + \\sum_{k=1}^m \\frac{n_k \\bar{x}_k}{n_k\\tau^2 + \\sigma^2}}{\\frac{1}{s_0^2} + \\sum_{k=1}^m \\frac{n_k}{n_k\\tau^2 + \\sigma^2}} \\right) $$\nThis expression represents the posterior mean for $\\theta_i$ as a shrinkage estimator. It is a linear combination of the group-specific mean $\\bar{x}_i$ and an overall mean estimate $\\hat{\\mu}$ that pools information from all groups and the hyperprior. The weighting factor $B_i$ controls the degree of shrinkage towards the overall mean, which depends on the relative precision of the group-level data versus the between-group variation.", "answer": "$$\n\\boxed{\\left(\\frac{n_i \\tau^2}{n_i \\tau^2 + \\sigma^2}\\right) \\bar{x}_i + \\left(\\frac{\\sigma^2}{n_i \\tau^2 + \\sigma^2}\\right) \\left( \\frac{\\frac{m_0}{s_0^2} + \\sum_{k=1}^m \\frac{n_k \\bar{x}_k}{n_k \\tau^2 + \\sigma^2}}{\\frac{1}{s_0^2} + \\sum_{k=1}^m \\frac{n_k}{n_k \\tau^2 + \\sigma^2}} \\right)}\n$$", "id": "3290513"}, {"introduction": "Our final practice ventures into the modern frontier of simulation-based inference, addressing the common challenge where the likelihood function $p(x | \\theta)$ is intractable but simulating from the model is feasible. This coding exercise integrates machine learning with Bayesian principles, using Noise-Contrastive Estimation (NCE) to train a classifier that serves as a proxy for the log-likelihood. You will implement a full pipeline to construct an approximate posterior, diagnose its potential distortions, and apply calibration techniques to improve its accuracy, offering a practical glimpse into advanced computational methods [@problem_id:3290514].", "problem": "You are given a generative model and a simulation-based inference task that must be solved by combining Noise-Contrastive Estimation (NCE) and Bayes' theorem. The purpose is to investigate how a classifier score can proxy the log-likelihood and how calibration modifies the resulting posterior. The underlying generative model is as follows: for a scalar parameter $\\theta \\in \\mathbb{R}$, and known noise variance $\\sigma^{2} > 0$, the observation model is $x \\mid \\theta \\sim \\mathcal{N}(\\theta, \\sigma^{2})$. The prior is $\\theta \\sim \\mathcal{N}(0, \\tau^{2})$ with known variance $\\tau^{2} > 0$. A fixed user-chosen auxiliary noise distribution $q(x)$ is used for NCE, with $x \\sim \\mathcal{N}(0, s_{q}^{2})$ and known variance $s_{q}^{2} > 0$. All quantities are one-dimensional. You will implement NCE via binary logistic regression with a restricted linear logit as a deliberate model misspecification, embed the resulting classifier score into Bayes' theorem, study posterior distortion, and then correct by calibration using temperature scaling.\n\nThe fundamental base to use includes the following: Bayes' theorem for two-class classification for deriving the optimal classifier under class-conditional densities, the definition of the logistic regression model, the definition of the logistic function, the law of the unconscious statistician, and the definition of Kullback–Leibler divergence.\n\nYour program must implement the following pipeline for each test case, using only simulation and deterministic optimization, with a fixed random seed for reproducibility:\n\n1. Construct a discrete grid of parameter values $\\{\\theta_{j}\\}_{j=1}^{J}$ with $J$ fixed across test cases. Use a symmetric grid $\\theta_{j} \\in [-T, T]$ with uniform spacing $h$, where $T = 4.0$ and $h = 0.2$, so $J = 41$ points. This grid serves as the discrete support for posterior computations.\n\n2. For each grid point $\\theta_{j}$, generate two labeled datasets:\n   - A positive class dataset of size $n_{\\text{train}}$ i.i.d. from $p_{\\theta_{j}}(x) = \\mathcal{N}(\\theta_{j}, \\sigma^{2})$ with label $y = 1$.\n   - A negative class dataset of size $n_{\\text{train}}$ i.i.d. from $q(x) = \\mathcal{N}(0, s_{q}^{2})$ with label $y = 0$.\n   Use a fixed seed across the entire program for all randomness, and no stochasticity in optimization.\n\n3. Fit a binary logistic regression classifier with a linear logit $r_{\\theta_{j}}(x) = w_{0}^{(j)} + w_{1}^{(j)} x$ by minimizing the cross-entropy over the training data for that $\\theta_{j}$ using exact Newton updates with a vanishingly small $\\ell_{2}$ regularization on the slope only (no penalty on the intercept) to ensure numerical stability. Use equal class sampling in training. This produces a raw score function $r_{\\theta_{j}}(x)$ for each $\\theta_{j}$.\n\n4. For each $\\theta_{j}$, independently generate a validation dataset consisting of $n_{\\text{val}}$ i.i.d. draws from $p_{\\theta_{j}}(x)$ with label $y = 1$ and $n_{\\text{val}}$ i.i.d. draws from $q(x)$ with label $y = 0$. Perform temperature scaling calibration by finding a scalar $\\alpha^{(j)} > 0$ that minimizes the validation cross-entropy when the calibrated class probability is $\\sigma(\\alpha^{(j)} r_{\\theta_{j}}(x))$, where $\\sigma(\\cdot)$ is the logistic sigmoid. Implement a one-dimensional Newton method for the convex objective in $\\alpha^{(j)}$, initialized at $\\alpha^{(j)} = 1$, with a positivity constraint enforced by projection.\n\n5. Compute three discrete posterior distributions over $\\{\\theta_{j}\\}$ for a single observed value $x_{\\text{obs}}$:\n   - The exact discrete posterior weights are proportional to $p(\\theta_{j}) p(x_{\\text{obs}} \\mid \\theta_{j})$, where $p(\\theta)$ is the $\\mathcal{N}(0, \\tau^{2})$ density and $p(x \\mid \\theta)$ is $\\mathcal{N}(\\theta, \\sigma^{2})$. Normalize to a proper discrete distribution by dividing by the sum over $j$.\n   - The uncalibrated NCE-based approximate discrete posterior weights are proportional to $p(\\theta_{j}) \\exp(r_{\\theta_{j}}(x_{\\text{obs}}))$. Normalize across $j$ to obtain a proper discrete distribution.\n   - The calibrated NCE-based approximate discrete posterior weights are proportional to $p(\\theta_{j}) \\exp(\\alpha^{(j)} r_{\\theta_{j}}(x_{\\text{obs}}))$. Normalize across $j$.\n\n6. For each test case, compute two scalar diagnostics that quantify posterior distortion:\n   - The Kullback–Leibler divergence from the exact discrete posterior to the uncalibrated approximation, defined as $D_{\\mathrm{KL}}(P \\Vert Q) = \\sum_{j=1}^{J} P_{j} \\log\\left(\\frac{P_{j}}{Q_{j}}\\right)$ with natural logarithm, where $P$ is the exact posterior and $Q$ is the approximate posterior.\n   - The Kullback–Leibler divergence from the exact discrete posterior to the calibrated approximation, computed identically but using the calibrated posterior.\n\nYour implementation details must satisfy:\n- Use a fixed random seed $s_{0} = 12345$ for all random number generation.\n- Use numerically stable computations for logistic functions and for normalization of posteriors (employ log-sum-exp stabilization).\n- Use only a linear feature $x$ in the classifier to intentionally induce misspecification when $s_{q}^{2} \\neq \\sigma^{2}$, thereby enabling nontrivial calibration effects.\n- No external data sources or user input are allowed.\n\nTest Suite:\nEvaluate the program on the following four test cases, each specified by $(\\sigma^{2}, \\tau^{2}, s_{q}^{2}, x_{\\text{obs}}, n_{\\text{train}}, n_{\\text{val}})$:\n- Case A (moderate samples, deliberately misspecified noise): $(1.0, 4.0, 4.0, 1.0, 200, 100)$.\n- Case B (small samples, deliberately misspecified noise): $(1.0, 4.0, 4.0, 1.0, 30, 30)$.\n- Case C (well-specified noise and tighter prior): $(1.0, 1.0, 1.0, 0.0, 200, 100)$.\n- Case D (moderate samples, mild misspecification, extreme observation): $(1.0, 4.0, 2.0, 3.0, 200, 100)$.\n\nRequired Final Output Format:\nYour program should produce a single line of output containing the eight results corresponding to the four test cases, in order A, B, C, D, where each case contributes two floats: first the uncalibrated Kullback–Leibler divergence and then the calibrated Kullback–Leibler divergence. The format must be a comma-separated list enclosed in square brackets and containing the eight numbers in this sequence, for example, $[r_{1}, r_{2}, r_{3}, r_{4}, r_{5}, r_{6}, r_{7}, r_{8}]$. No other text should be printed.", "solution": "The problem requires the implementation of a simulation-based inference pipeline to approximate a Bayesian posterior distribution. This is achieved by leveraging the principles of Noise-Contrastive Estimation (NCE), where a binary classifier is trained to distinguish between data from a generative model $p(x|\\theta)$ and an auxiliary noise distribution $q(x)$. The learned classifier's output can serve as a proxy for the log-likelihood ratio, which is then integrated into Bayes' theorem. The analysis focuses on the distortion of the resulting posterior due to deliberate model misspecification and the subsequent correction using temperature scaling.\n\nFirst, we establish the theoretical connection. For a two-class classification problem with feature vector $x$ and class label $y \\in \\{0, 1\\}$, Bayes' theorem for classifiers states that the posterior probability of class $1$ is $P(y=1|x) = \\frac{p(x|y=1)P(y=1)}{p(x|y=1)P(y=1) + p(x|y=0)P(y=0)}$. The logit, or log-odds, is $\\text{logit}(P(y=1|x)) = \\log\\left(\\frac{p(x|y=1)}{p(x|y=0)}\\right) + \\log\\left(\\frac{P(y=1)}{P(y=0)}\\right)$. In our NCE setup, for a fixed parameter $\\theta_j$, we identify $p(x|y=1)$ with the data-generating distribution $p_{\\theta_j}(x) = \\mathcal{N}(x; \\theta_j, \\sigma^2)$ and $p(x|y=0)$ with the noise distribution $q(x) = \\mathcal{N}(x; 0, s_q^2)$. By using equal sample sizes for training ($n_{\\text{train}}$ from each class), we effectively set the empirical class priors to be equal, i.e., $P(y=1) = P(y=0) = 0.5$. In this case, the optimal classifier's logit is exactly the log-likelihood ratio: $\\log\\left(\\frac{p_{\\theta_j}(x)}{q(x)}\\right)$. This log-ratio is given by:\n$$ \\log\\left(\\frac{p_{\\theta_j}(x)}{q(x)}\\right) = \\log\\left(\\frac{\\mathcal{N}(x; \\theta_j, \\sigma^2)}{\\mathcal{N}(x; 0, s_q^2)}\\right) = \\left(\\frac{1}{2s_q^2} - \\frac{1}{2\\sigma^2}\\right)x^2 + \\left(\\frac{\\theta_j}{\\sigma^2}\\right)x - \\frac{\\theta_j^2}{2\\sigma^2} + \\frac{1}{2}\\log\\left(\\frac{s_q^2}{\\sigma^2}\\right) $$\nThis expression is a quadratic function of $x$ unless $\\sigma^2 = s_q^2$. The problem deliberately uses a logistic regression model with a linear logit, $r_{\\theta_j}(x) = w_0^{(j)} + w_1^{(j)}x$, inducing a model misspecification when $\\sigma^2 \\neq s_q^2$. This trained logit $r_{\\theta_j}(x)$ becomes our approximation for the true log-ratio.\n\nThe Bayesian posterior over a discrete set of parameters $\\{\\theta_j\\}_{j=1}^J$ for an observation $x_{\\text{obs}}$ is given by $P(\\theta_j|x_{\\text{obs}}) \\propto p(x_{\\text{obs}}|\\theta_j)p(\\theta_j)$. The uncalibrated approximate posterior is constructed by replacing the log-likelihood $\\log p(x|\\theta)$ with the learned logit $r_{\\theta_j}(x)$, yielding $Q_{\\text{uncal}}(\\theta_j|x_{\\text{obs}}) \\propto \\exp(r_{\\theta_j}(x_{\\text{obs}})) p(\\theta_j)$. This substitution is justified because the term $q(x_{\\text{obs}})$ from the true log-ratio is constant with respect to $\\theta_j$ and is absorbed into the normalization constant. The calibrated posterior introduces a temperature parameter $\\alpha^{(j)}$ for each $\\theta_j$, giving $Q_{\\text{cal}}(\\theta_j|x_{\\text{obs}}) \\propto \\exp(\\alpha^{(j)} r_{\\theta_j}(x_{\\text{obs}})) p(\\theta_j)$.\n\nThe implementation proceeds as follows:\n1.  A discrete grid of $J=41$ parameter values $\\{\\theta_j\\}$ is defined on $[-4.0, 4.0]$ with spacing $h=0.2$.\n2.  For each $\\theta_j$ on this grid:\n    a. A training dataset is generated with $n_{\\text{train}}$ samples from $p_{\\theta_j}(x)$ (label $y=1$) and $n_{\\text{train}}$ from $q(x)$ (label $y=0$).\n    b. A logistic regression model with weights $w^{(j)} = (w_0^{(j)}, w_1^{(j)})$ is fitted by minimizing the cross-entropy loss with a minor $\\ell_2$ penalty $10^{-8}(w_1^{(j)})^2$. The optimization is performed using a fixed number of Newton-Raphson (also known as Iteratively Reweighted Least Squares, IRLS) updates. For a loss $L(w)$, the update is $w_{k+1} = w_k - H_L^{-1} \\nabla L$, where $\\nabla L$ is the gradient and $H_L$ is the Hessian matrix of the loss.\n    c. A separate validation set of size $2n_{\\text{val}}$ is generated. The raw scores $r_{\\theta_j}(x)$ from the fitted classifier are computed on this set.\n    d. The temperature parameter $\\alpha^{(j)}$ is optimized by minimizing the cross-entropy loss on the validation data for the calibrated probabilities $\\sigma(\\alpha^{(j)} r_{\\theta_j}(x))$. This is a one-dimensional convex optimization problem, solved using Newton's method for $\\alpha > 0$. The initial value is $\\alpha=1$, and positivity is enforced by projecting any negative update result to a small positive value, $10^{-6}$.\n3.  After obtaining $(w_0^{(j)}, w_1^{(j)})$ and $\\alpha^{(j)}$ for all $j=1, \\dots, J$, we compute the three discrete posterior distributions over $\\{\\theta_j\\}$ for a given observation $x_{\\text{obs}}$.\n    a. **Exact Posterior:** The weights are $P_j \\propto \\mathcal{N}(x_{\\text{obs}}|\\theta_j, \\sigma^2)\\mathcal{N}(\\theta_j|0, \\tau^2)$.\n    b. **Uncalibrated Posterior:** The weights are $Q_{\\text{uncal},j} \\propto \\exp(w_0^{(j)} + w_1^{(j)} x_{\\text{obs}}) \\mathcal{N}(\\theta_j|0, \\tau^2)$.\n    c. **Calibrated Posterior:** The weights are $Q_{\\text{cal},j} \\propto \\exp(\\alpha^{(j)}(w_0^{(j)} + w_1^{(j)} x_{\\text{obs}})) \\mathcal{N}(\\theta_j|0, \\tau^2)$.\n    All distributions are normalized to sum to $1$. To ensure numerical stability, computations are performed in log-space, using the log-sum-exp trick for normalization.\n4.  Finally, the distortion is quantified by computing the Kullback–Leibler (KL) divergence from the exact posterior $P$ to the approximate posteriors $Q_{\\text{uncal}}$ and $Q_{\\text{cal}}$, defined as $D_{\\mathrm{KL}}(P \\Vert Q) = \\sum_j P_j \\log(P_j/Q_j)$.\n\nThis entire procedure is executed for each of the four test cases, and the resulting KL divergences are reported. A fixed random seed ensures reproducibility.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import expit, logsumexp\n\ndef solve():\n    \"\"\"\n    Implements the full pipeline for NCE-based posterior approximation and calibration.\n    \"\"\"\n    # Use a fixed random seed for reproducibility\n    s0 = 12345\n    rng = np.random.default_rng(s0)\n    \n    # Grid definition for theta\n    T = 4.0\n    h = 0.2\n    thetas = np.arange(-T, T + h / 2, h)\n    J = len(thetas)\n\n    # Optimization parameters\n    newton_iters_w = 15\n    newton_iters_alpha = 15\n    reg_lambda = 1e-8\n    alpha_min_proj = 1e-6\n    \n    # Test Suite\n    test_cases = [\n        # (sigma^2, tau^2, s_q^2, x_obs, n_train, n_val)\n        (1.0, 4.0, 4.0, 1.0, 200, 100),  # Case A\n        (1.0, 4.0, 4.0, 1.0, 30, 30),    # Case B\n        (1.0, 1.0, 1.0, 0.0, 200, 100),  # Case C\n        (1.0, 4.0, 2.0, 3.0, 200, 100),  # Case D\n    ]\n\n    results = []\n\n    for case in test_cases:\n        sigma_sq, tau_sq, s_q_sq, x_obs, n_train, n_val = case\n        sigma = np.sqrt(sigma_sq)\n        tau = np.sqrt(tau_sq)\n        s_q = np.sqrt(s_q_sq)\n\n        w_params = np.zeros((J, 2))  # Stores (w0, w1) for each theta_j\n        alphas = np.zeros(J)         # Stores alpha for each theta_j\n\n        for j, theta_j in enumerate(thetas):\n            # 1. Generate Training Data\n            x_pos_train = rng.normal(loc=theta_j, scale=sigma, size=n_train)\n            x_neg_train = rng.normal(loc=0.0, scale=s_q, size=n_train)\n            \n            X_data_train = np.concatenate([x_pos_train, x_neg_train])\n            X_design_train = np.vstack([np.ones_like(X_data_train), X_data_train]).T\n            y_train = np.concatenate([np.ones(n_train), np.zeros(n_train)])\n\n            # 2. Fit Logistic Regression via Newton's Method (IRLS)\n            w = np.zeros(2) # Initial weights (w0, w1)\n            reg_hess_penalty = np.diag([0, 2 * reg_lambda])\n            for _ in range(newton_iters_w):\n                z = X_design_train @ w\n                p = expit(z)\n                grad = X_design_train.T @ (p - y_train) + np.array([0, 2 * reg_lambda * w[1]])\n                W = np.diag(p * (1 - p))\n                H = X_design_train.T @ W @ X_design_train + reg_hess_penalty\n                w = w - np.linalg.inv(H) @ grad\n            w_params[j, :] = w\n\n            # 3. Generate Validation Data\n            x_pos_val = rng.normal(loc=theta_j, scale=sigma, size=n_val)\n            x_neg_val = rng.normal(loc=0.0, scale=s_q, size=n_val)\n            \n            X_data_val = np.concatenate([x_pos_val, x_neg_val])\n            y_val = np.concatenate([np.ones(n_val), np.zeros(n_val)])\n            \n            # 4. Fit Temperature Scale via Newton's Method\n            raw_scores = w[0] + w[1] * X_data_val\n            alpha = 1.0 # Initial temperature\n            for _ in range(newton_iters_alpha):\n                p_alpha = expit(alpha * raw_scores)\n                grad_alpha = np.sum(raw_scores * (p_alpha - y_val))\n                hess_alpha = np.sum((raw_scores**2) * p_alpha * (1 - p_alpha))\n                if hess_alpha > 1e-12: # Avoid division by zero\n                    alpha = alpha - grad_alpha / hess_alpha\n                alpha = max(alpha, alpha_min_proj) # Enforce positivity\n            alphas[j] = alpha\n            \n        # 5. Compute Posteriors for x_obs\n        log_prior = norm.logpdf(thetas, loc=0.0, scale=tau)\n        \n        # Exact Posterior\n        log_likelihood_exact = norm.logpdf(x_obs, loc=thetas, scale=sigma)\n        log_posterior_exact_un = log_prior + log_likelihood_exact\n        log_norm_const_exact = logsumexp(log_posterior_exact_un)\n        log_posterior_exact = log_posterior_exact_un - log_norm_const_exact\n        posterior_exact = np.exp(log_posterior_exact)\n\n        # Uncalibrated NCE Posterior\n        logits_uncal = w_params[:, 0] + w_params[:, 1] * x_obs\n        log_posterior_uncal_un = log_prior + logits_uncal\n        log_norm_const_uncal = logsumexp(log_posterior_uncal_un)\n        log_posterior_uncal = log_posterior_uncal_un - log_norm_const_uncal\n        posterior_uncal = np.exp(log_posterior_uncal)\n\n        # Calibrated NCE Posterior\n        logits_cal = alphas * (w_params[:, 0] + w_params[:, 1] * x_obs)\n        log_posterior_cal_un = log_prior + logits_cal\n        log_norm_const_cal = logsumexp(log_posterior_cal_un)\n        log_posterior_cal = log_posterior_cal_un - log_norm_const_cal\n        posterior_cal = np.exp(log_posterior_cal)\n\n        # 6. Compute KL Divergences\n        # Add a small constant to prevent log(0) if any posterior probability is exactly zero\n        eps = 1e-99\n        kl_uncal = np.sum(posterior_exact * (log_posterior_exact - np.log(posterior_uncal + eps)))\n        kl_cal = np.sum(posterior_exact * (log_posterior_exact - np.log(posterior_cal + eps)))\n        results.extend([kl_uncal, kl_cal])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3290514"}]}