{"hands_on_practices": [{"introduction": "Chebyshev's inequality provides a universal probability bound, applicable to any distribution with a finite variance. However, this generality comes at the cost of being conservative, or 'loose,' for many common distributions. This first exercise provides a concrete, quantitative exploration of this trade-off by comparing the Chebyshev bound directly against the true tail probability for a Gaussian random variable, a cornerstone of statistical theory and practice [@problem_id:3294065]. By working through this calculation, you will gain a deeper appreciation for the contexts where the inequality is powerful and where more specific bounds might be necessary.", "problem": "Consider a real-valued random variable $X$ distributed as a Gaussian (normal) law with mean $\\mu$ and variance $\\sigma^{2}$, denoted $X \\sim \\mathcal{N}(\\mu,\\sigma^{2})$. In stochastic simulation and Monte Carlo (MC) methods, tail bounds control the probability of large deviations of estimators. A foundational inequality for variance-bounded random variables is Chebyshev's inequality, which states that for any $k>0$,\n$$\n\\mathbb{P}\\!\\left(|X-\\mu|\\geq k\\,\\sigma\\right)\\leq \\frac{1}{k^{2}}.\n$$\nWork from first principles to compute the actual two-sided tail probability for the Gaussian, and then compare it against the Chebyshev bound. Specifically:\n- Starting from the probability density function of the normal distribution,\n$$\nf_{X}(x)=\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right),\n$$\nand the definition of the cumulative distribution function (CDF), derive the exact expression for $\\mathbb{P}(|X-\\mu|\\geq k\\,\\sigma)$ for a general $k>0$.\n- Evaluate this exact tail probability for the set of thresholds $\\{k=1,\\,k=2,\\,k=3\\}$.\n- Quantify the conservatism of Chebyshev's inequality by defining the geometric-mean conservatism index over the set $\\{1,2,3\\}$ as\n$$\nC:=\\left(\\prod_{k\\in\\{1,2,3\\}}\\frac{\\frac{1}{k^{2}}}{\\mathbb{P}\\!\\left(|X-\\mu|\\geq k\\,\\sigma\\right)}\\right)^{\\frac{1}{3}}.\n$$\nCompute $C$ exactly from your derived tail probabilities.\n\nExplain why this index captures the typical multiplicative overestimation of the Chebyshev bound for sub-Gaussian tails in MC error analysis, and relate the calculation to the structure of Gaussian tails without invoking any shortcut formulas beyond core definitions and well-tested facts about Gaussian integrals.\n\nRound your final numerical value of $C$ to four significant figures. No units are required for the answer.", "solution": "The problem is valid as it is scientifically grounded in probability theory, well-posed with a clear objective and sufficient data, and free of any subjective or ambiguous statements.\n\nThe problem asks for a derivation of the exact tail probability for a Gaussian random variable, a comparison of this probability with the bound provided by Chebyshev's inequality, and the calculation of a \"conservatism index\" that quantifies this difference.\n\nLet $X$ be a random variable following a Gaussian (normal) distribution with mean $\\mu$ and variance $\\sigma^2$, denoted as $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$. The probability density function (PDF) is given by\n$$f_{X}(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$$\n\nFirst, we derive the exact expression for the two-sided tail probability $\\mathbb{P}(|X-\\mu| \\geq k\\sigma)$ for some constant $k > 0$. The inequality $|X - \\mu| \\geq k\\sigma$ is equivalent to the union of two disjoint events: $(X - \\mu) \\geq k\\sigma$ or $(X - \\mu) \\leq -k\\sigma$.\nTherefore, the probability is the sum of the probabilities of these two events:\n$$\\mathbb{P}(|X - \\mu| \\geq k\\sigma) = \\mathbb{P}(X \\geq \\mu + k\\sigma) + \\mathbb{P}(X \\leq \\mu - k\\sigma)$$\nTo simplify the calculation, we standardize the random variable $X$. Let $Z = \\frac{X - \\mu}{\\sigma}$. The random variable $Z$ follows the standard normal distribution, $Z \\sim \\mathcal{N}(0, 1)$, with PDF $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$. The inequality $|X - \\mu| \\geq k\\sigma$ can be rewritten in terms of $Z$:\n$$\\left|\\frac{X - \\mu}{\\sigma}\\right| \\geq k \\implies |Z| \\geq k$$\nThe probability becomes\n$$\\mathbb{P}(|Z| \\geq k) = \\mathbb{P}(Z \\geq k) + \\mathbb{P}(Z \\leq -k)$$\nDue to the symmetry of the standard normal distribution about $z=0$, we have $\\mathbb{P}(Z \\leq -k) = \\mathbb{P}(Z \\geq k)$. Thus,\n$$\\mathbb{P}(|Z| \\geq k) = 2 \\, \\mathbb{P}(Z \\geq k)$$\nThis probability can be expressed as an integral of the standard normal PDF:\n$$\\mathbb{P}(|Z| \\geq k) = 2 \\int_{k}^{\\infty} \\phi(z) \\, dz = 2 \\int_{k}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) dz$$\nThis integral defines the tail probability of the standard normal distribution. It is commonly expressed in terms of the complementary error function, $\\text{erfc}(x)$, which is defined as\n$$\\text{erfc}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_{x}^{\\infty} \\exp(-t^2) dt$$\nTo relate our integral to this definition, we perform a change of variables in the integral. Let $t = z/\\sqrt{2}$. Then $z = \\sqrt{2}t$ and $dz = \\sqrt{2}dt$. The lower limit of integration $z=k$ becomes $t=k/\\sqrt{2}$. The upper limit remains infinity.\n$$\\begin{align*} \\mathbb{P}(|X-\\mu| \\geq k\\sigma) &= 2 \\int_{k}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) dz \\\\ &= \\frac{2}{\\sqrt{2\\pi}} \\int_{k/\\sqrt{2}}^{\\infty} \\exp(-t^2) (\\sqrt{2}dt) \\\\ &= \\frac{2\\sqrt{2}}{\\sqrt{2\\pi}} \\int_{k/\\sqrt{2}}^{\\infty} \\exp(-t^2) dt \\\\ &= \\frac{2}{\\sqrt{\\pi}} \\int_{k/\\sqrt{2}}^{\\infty} \\exp(-t^2) dt \\end{align*}$$\nBy the definition of the complementary error function, this is exactly $\\text{erfc}(k/\\sqrt{2})$.\nThus, the exact expression for the tail probability is\n$$\\mathbb{P}(|X-\\mu| \\geq k\\sigma) = \\text{erfc}\\left(\\frac{k}{\\sqrt{2}}\\right)$$\n\nNext, we evaluate this exact probability for $k \\in \\{1, 2, 3\\}$. Let's denote these probabilities as $P_k = \\mathbb{P}(|X-\\mu| \\geq k\\sigma)$.\nFor $k=1$:\n$$P_1 = \\text{erfc}\\left(\\frac{1}{\\sqrt{2}}\\right) \\approx 0.3173105$$\nFor $k=2$:\n$$P_2 = \\text{erfc}\\left(\\frac{2}{\\sqrt{2}}\\right) = \\text{erfc}(\\sqrt{2}) \\approx 0.0455003$$\nFor $k=3$:\n$$P_3 = \\text{erfc}\\left(\\frac{3}{\\sqrt{2}}\\right) \\approx 0.0026998$$\nThe corresponding Chebyshev bounds are $1/k^2$: for $k=1$, the bound is $1$; for $k=2$, it is $1/4 = 0.25$; for $k=3$, it is $1/9 \\approx 0.1111$.\n\nNow, we compute the geometric-mean conservatism index, $C$.\n$$C = \\left(\\prod_{k\\in\\{1,2,3\\}} \\frac{1/k^2}{P_k}\\right)^{1/3} = \\left( \\frac{1/1^2}{P_1} \\cdot \\frac{1/2^2}{P_2} \\cdot \\frac{1/3^2}{P_3} \\right)^{1/3}$$\nLet's calculate the individual ratios, which represent the factor by which Chebyshev's inequality overestimates the true probability:\nRatio for $k=1$: $R_1 = \\frac{1}{P_1} = \\frac{1}{0.3173105} \\approx 3.15147$\nRatio for $k=2$: $R_2 = \\frac{1/4}{P_2} = \\frac{0.25}{0.0455003} \\approx 5.49448$\nRatio for $k=3$: $R_3 = \\frac{1/9}{P_3} = \\frac{1/9}{0.0026998} \\approx 41.1565$\nThe product of these ratios is\n$$R_1 \\cdot R_2 \\cdot R_3 \\approx 3.15147 \\times 5.49448 \\times 41.1565 \\approx 712.56$$\nThe index $C$ is the cube root of this product:\n$$C = (712.56)^{1/3} \\approx 8.93208$$\nRounding to four significant figures, we get $C \\approx 8.932$.\n\nThe conservatism index $C$ quantifies the degree to which Chebyshev's inequality provides a loose bound for a Gaussian distribution. Each term in the product, $\\frac{1/k^2}{P_k}$, is the multiplicative factor by which the Chebyshev bound exceeds the actual tail probability. The geometric mean is an appropriate average for these factors, as it gives a sense of the typical overestimation across the chosen scales $k \\in \\{1, 2, 3\\}$. A value of $C \\approx 8.932$ indicates that, on average for these small $k$ values, the Chebyshev bound is almost an order of magnitude larger than the true Gaussian tail probability.\n\nThis significant discrepancy arises because Chebyshev's inequality is universal. It applies to any random variable with a finite variance, making no further assumptions about the shape of its distribution. The bound is \"tight\" only for \"worst-case\" distributions that concentrate probability mass far from the mean, right at the $\\pm k\\sigma$ boundaries. The Gaussian distribution is fundamentally different; it is a \"sub-Gaussian\" distribution, meaning its tails decay exponentially fast (specifically as $\\exp(-z^2/2)$). This is a much faster decay than the algebraic $1/k^2$ decay of the Chebyshev bound. As $k$ increases, the overestimation factor grows rapidly, as seen by the jump from $R_1 \\approx 3.15$ to $R_3 \\approx 41.16$. In the context of Monte Carlo (MC) error analysis, estimators are often sums of many random variables and, by the Central Limit Theorem, their distributions are approximately Gaussian. Using Chebyshev's inequality to bound the probability of large estimation errors would be extremely pessimistic. This calculation demonstrates that employing knowledge of the (approximate) distribution shape allows for much tighter and more realistic error bounds, which is crucial for assessing the reliability and efficiency of MC simulations.", "answer": "$$\n\\boxed{8.932}\n$$", "id": "3294065"}, {"introduction": "One of the most practical applications of probability inequalities is in experiment design, particularly for determining the required number of samples in a Monte Carlo simulation. This practice tackles a fundamental design question: given a desired level of precision $\\epsilon$ and confidence $1-\\delta$, how many samples must we collect? This exercise guides you through deriving a stopping rule using Chebyshev's inequality, based on a known prior upper bound $B$ for the variance, providing a robust sample size that guarantees the desired statistical performance [@problem_id:3294130].", "problem": "Consider a Monte Carlo (MC) estimation scenario where independent and identically distributed random variables $\\{X_{i}\\}_{i \\geq 1}$ are sampled from an unknown distribution with finite mean $\\mu$ and finite variance $\\sigma^{2}$. Suppose an a priori upper bound $B$ satisfying $\\sigma^{2} \\leq B$ is available. Let the MC estimator of $\\mu$ after $n$ samples be the sample mean $\\bar{X}_{n} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i}$. You are tasked with designing a conservative sequential stopping rule that guarantees the coverage requirement $\\mathbb{P}(|\\bar{X}_{T} - \\mu| \\leq \\epsilon) \\geq 1 - \\delta$ for prespecified tolerances $\\epsilon > 0$ and $\\delta \\in (0,1)$, where $T$ is the (possibly random) stopping time determined by your rule.\n\nStarting only from fundamental facts about expectation and variance of sums of independent random variables, and general tail bounds derived from them, derive a stopping rule that is expressed solely in terms of $B$, $\\epsilon$, and $\\delta$ and does not depend on unknown distributional features beyond the bound $B$. Your rule must be sequential in the sense that it can be applied as samples arrive, but it may reduce to a deterministic threshold.\n\nFormally, specify the rule in the form “stop at the first $n$ such that $n \\geq N^{\\ast}(B,\\epsilon,\\delta)$,” and determine the exact analytic expression for the smallest integer threshold $N^{\\ast}(B,\\epsilon,\\delta)$ that guarantees the stated coverage requirement for all distributions with variance bounded by $B$. Provide the final answer as a single closed-form expression for $N^{\\ast}(B,\\epsilon,\\delta)$.", "solution": "The problem requires the derivation of a stopping rule for a Monte Carlo estimation of a mean $\\mu$. The rule must guarantee a specified coverage probability for the estimate. This will be accomplished by determining the minimum number of samples, $N^{\\ast}$, required to satisfy the condition $\\mathbb{P}(|\\bar{X}_{T} - \\mu| \\leq \\epsilon) \\geq 1 - \\delta$ for given tolerances $\\epsilon > 0$ and $\\delta \\in (0,1)$, where the variance $\\sigma^2$ of the underlying distribution is unknown but bounded by $\\sigma^2 \\leq B$. The rule must be derived from first principles concerning expectation and variance, and a general probability inequality.\n\nLet $\\{X_{i}\\}_{i \\geq 1}$ be a sequence of independent and identically distributed (i.i.d.) random variables with finite mean $\\mathbb{E}[X_i] = \\mu$ and finite variance $\\text{Var}(X_i) = \\sigma^2$. The Monte Carlo estimator for $\\mu$ based on $n$ samples is the sample mean, $\\bar{X}_{n} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i}$.\n\nFirst, we determine the expectation and variance of the estimator $\\bar{X}_n$.\nThe expectation of $\\bar{X}_n$ is:\n$$\n\\mathbb{E}[\\bar{X}_{n}] = \\mathbb{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n} X_{i}\\right] = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}[X_{i}] = \\frac{1}{n} \\sum_{i=1}^{n} \\mu = \\frac{n\\mu}{n} = \\mu\n$$\nThis shows that $\\bar{X}_n$ is an unbiased estimator of $\\mu$.\n\nThe variance of $\\bar{X}_n$ is calculated using the property that the variance of a sum of independent random variables is the sum of their variances:\n$$\n\\text{Var}(\\bar{X}_{n}) = \\text{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i}\\right) = \\frac{1}{n^2} \\text{Var}\\left(\\sum_{i=1}^{n} X_{i}\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\text{Var}(X_i) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\sigma^2 = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n}\n$$\n\nThe problem specifies a coverage requirement on the stopping time $T=n$:\n$$\n\\mathbb{P}(|\\bar{X}_{n} - \\mu| \\leq \\epsilon) \\geq 1 - \\delta\n$$\nThis is equivalent to bounding the probability of the complementary event, which is that the estimation error exceeds $\\epsilon$:\n$$\n\\mathbb{P}(|\\bar{X}_{n} - \\mu| > \\epsilon) \\leq \\delta\n$$\n\nSince the underlying distribution of the $X_i$ is unknown, we cannot make strong distributional assumptions (like normality, which would lead to the Central Limit Theorem). The problem directs us to use general tail bounds derived from expectation and variance. The appropriate tool for this is Chebyshev's inequality. For any random variable $Y$ with finite mean $\\mathbb{E}[Y]$ and finite variance $\\text{Var}(Y)$, Chebyshev's inequality states that for any constant $k > 0$:\n$$\n\\mathbb{P}(|Y - \\mathbb{E}[Y]| \\geq k) \\leq \\frac{\\text{Var}(Y)}{k^2}\n$$\nWe apply this inequality to our estimator $Y = \\bar{X}_n$, with $\\mathbb{E}[Y] = \\mu$, $\\text{Var}(Y) = \\sigma^2/n$, and the threshold $k = \\epsilon$ (since $\\epsilon > 0$). This yields:\n$$\n\\mathbb{P}(|\\bar{X}_{n} - \\mu| \\geq \\epsilon) \\leq \\frac{\\text{Var}(\\bar{X}_{n})}{\\epsilon^2}\n$$\nThe events $|\\bar{X}_{n} - \\mu| > \\epsilon$ and $|\\bar{X}_{n} - \\mu| \\geq \\epsilon$ are equivalent for continuous random variables. For discrete variables, the inequality still holds: $\\mathbb{P}(|\\bar{X}_{n} - \\mu| > \\epsilon) \\leq \\mathbb{P}(|\\bar{X}_{n} - \\mu| \\geq \\epsilon)$. Thus, we have:\n$$\n\\mathbb{P}(|\\bar{X}_{n} - \\mu| > \\epsilon) \\leq \\frac{\\sigma^2/n}{\\epsilon^2} = \\frac{\\sigma^2}{n\\epsilon^2}\n$$\n\nTo satisfy the required condition $\\mathbb{P}(|\\bar{X}_{n} - \\mu| > \\epsilon) \\leq \\delta$, it is sufficient to enforce that the upper bound from Chebyshev's inequality is less than or equal to $\\delta$:\n$$\n\\frac{\\sigma^2}{n\\epsilon^2} \\leq \\delta\n$$\nThis provides a conservative guarantee. Now, we must solve for the number of samples, $n$:\n$$\nn \\geq \\frac{\\sigma^2}{\\delta \\epsilon^2}\n$$\nThe true variance $\\sigma^2$ is unknown. However, we are given an a priori upper bound $\\sigma^2 \\leq B$. To ensure the inequality holds for any possible value of $\\sigma^2$ under this constraint, we must use the worst-case value for $\\sigma^2$, which is $B$. By substituting $B$ for $\\sigma^2$, we obtain a condition on $n$ that is independent of any unknown distributional parameters:\n$$\nn \\geq \\frac{B}{\\delta \\epsilon^2}\n$$\nAny integer $n$ satisfying this inequality will guarantee the coverage requirement. The problem asks for the smallest integer threshold, $N^{\\ast}(B,\\epsilon,\\delta)$, that satisfies this condition. This corresponds to the smallest integer $n$ that is greater than or equal to the quantity $\\frac{B}{\\delta \\epsilon^2}$. By definition, this is the ceiling of that quantity.\n$$\nN^{\\ast}(B,\\epsilon,\\delta) = \\left\\lceil \\frac{B}{\\delta \\epsilon^2} \\right\\rceil\n$$\nThe stopping rule is to stop sampling at the first integer $n$ such that $n \\geq N^{\\ast}$. This simplifies to a deterministic rule: set the number of samples to $n = T = N^{\\ast}$. This is the smallest sample size that provides the desired guarantee for all distributions with variance bounded by $B$, based on the information provided and the use of Chebyshev's inequality.", "answer": "$$\n\\boxed{\\left\\lceil \\frac{B}{\\delta \\epsilon^2} \\right\\rceil}\n$$", "id": "3294130"}, {"introduction": "Building on the principle of experiment design, we now address a more common and powerful scenario where an explicit variance bound is unavailable. Instead, we often have knowledge about the physical or logical constraints of a system, such as the fact that a random variable is confined to a known range $[a,b]$. This practice demonstrates how to derive a worst-case variance bound from the support of a distribution and then use it to determine a sufficient sample size for a Monte Carlo estimate [@problem_id:3294111]. This technique is essential for making robust error guarantees in a wide range of non-parametric settings.", "problem": "Consider a Monte Carlo estimator of the mean of an unknown real-valued random variable $X$ with independent and identically distributed (i.i.d.) draws. You are given only that the support of $X$ lies in a known closed interval $[a,b]$, with $a \\le b$, and you are not allowed to use any information about the variance beyond what can be deduced from this support constraint. Let $\\mu = \\mathbb{E}[X]$ denote the true mean, and define the sample mean $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i$ from $n$ i.i.d. draws.\n\nYour task is to design, from first principles, a conservative sample-size bound ensuring the following deviation control for the Monte Carlo estimator: for given tolerance $\\varepsilon > 0$ and risk level $\\delta \\in (0,1)$ expressed as a decimal (not a percentage), the probability that the sample mean deviates from the true mean by more than $\\varepsilon$ is at most $\\delta$, i.e., $\\mathbb{P}\\left(|\\bar{X}_n - \\mu| \\ge \\varepsilon\\right) \\le \\delta$. You must derive your bound using only:\n- the definition of expectation $\\mathbb{E}[\\,\\cdot\\,]$ and variance $\\mathrm{Var}(\\cdot)$,\n- independence of i.i.d. draws,\n- non-negativity of squares,\n- and the Markov inequality for non-negative random variables, stated as follows: for any non-negative random variable $Y$ and any threshold $t > 0$, $\\mathbb{P}(Y \\ge t) \\le \\frac{\\mathbb{E}[Y]}{t}$.\n\nNo other concentration inequalities or distribution-specific information may be used. In particular, do not assume knowledge of the variance $\\mathrm{Var}(X)$ beyond what can be deduced from $X \\in [a,b]$ almost surely.\n\nFrom these principles, derive a universal upper bound on $\\mathrm{Var}(X)$ that depends only on $a$ and $b$, and then use it to construct a conservative sufficient condition on the integer sample size $n$ that guarantees $\\mathbb{P}\\left(|\\bar{X}_n - \\mu| \\ge \\varepsilon\\right) \\le \\delta$. In the degenerate case $a = b$, specify the minimal integer $n$ consistent with the deviation control requirement.\n\nImplement this bound as a function that, given real numbers $a$, $b$, $\\varepsilon$, and $\\delta$, returns the minimal integer sample size $n$ that satisfies the conservative sufficient condition you derived.\n\nTest Suite:\nCompute the minimal integer $n$ for each of the following parameter sets, with all numeric values interpreted in the unitless mathematical sense:\n- Case $1$: $a = 0$, $b = 1$, $\\varepsilon = 0.05$, $\\delta = 0.1$.\n- Case $2$: $a = -10$, $b = 10$, $\\varepsilon = 0.5$, $\\delta = 0.05$.\n- Case $3$: $a = 2$, $b = 2$, $\\varepsilon = 0.1$, $\\delta = 0.01$.\n- Case $4$: $a = 0$, $b = 1$, $\\varepsilon = 0.01$, $\\delta = 0.001$.\n- Case $5$: $a = -1$, $b = 3$, $\\varepsilon = 0.02$, $\\delta = 0.05$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases listed above. For example, an output line must look like $[n_1,n_2,n_3,n_4,n_5]$ where each $n_i$ is an integer.", "solution": "The problem requires the derivation of a conservative sample size bound for a Monte Carlo estimator from first principles. The derivation must adhere to a specific set of allowed tools: the definitions of expectation and variance, the properties of i.i.d. random variables, the non-negativity of squares, and the Markov inequality.\n\nThe goal is to find the minimal integer sample size $n$ that guarantees $\\mathbb{P}\\left(|\\bar{X}_n - \\mu| \\ge \\varepsilon\\right) \\le \\delta$ for given constants $\\varepsilon > 0$ and $\\delta \\in (0,1)$, where $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i$ is the sample mean of a random variable $X$ with true mean $\\mu = \\mathbb{E}[X]$ and support in a known interval $[a,b]$.\n\nThe derivation proceeds in several steps. First, we use the Markov inequality to establish a general relationship between the probability of deviation and the variance of the estimator. This step essentially re-derives the one-sided Chebyshev inequality.\n\nLet the random variable of interest be the absolute deviation of the sample mean from the true mean, $|\\bar{X}_n - \\mu|$. The Markov inequality applies to non-negative random variables. While $|\\bar{X}_n - \\mu|$ is non-negative, its expectation is not straightforward to compute. A more tractable approach is to consider the squared deviation, $(\\bar{X}_n - \\mu)^2$. Because squares of real numbers are non-negative and the function $x \\mapsto x^2$ is monotonically increasing for non-negative arguments, the event $|\\bar{X}_n - \\mu| \\ge \\varepsilon$ is equivalent to the event $(\\bar{X}_n - \\mu)^2 \\ge \\varepsilon^2$, given that $\\varepsilon > 0$.\n\nLet $Y = (\\bar{X}_n - \\mu)^2$. This is a non-negative random variable. Let the threshold be $t = \\varepsilon^2$. Since $\\varepsilon > 0$, we have $t > 0$. We can now apply the Markov inequality, $\\mathbb{P}(Y \\ge t) \\le \\frac{\\mathbb{E}[Y]}{t}$:\n$$\n\\mathbb{P}\\left((\\bar{X}_n - \\mu)^2 \\ge \\varepsilon^2\\right) \\le \\frac{\\mathbb{E}\\left[(\\bar{X}_n - \\mu)^2\\right]}{\\varepsilon^2}\n$$\nThe expression in the numerator, $\\mathbb{E}\\left[(\\bar{X}_n - \\mu)^2\\right]$, is the definition of the variance of the sample mean, $\\mathrm{Var}(\\bar{X}_n)$. Substituting this and the equivalence of the events gives:\n$$\n\\mathbb{P}\\left(|\\bar{X}_n - \\mu| \\ge \\varepsilon\\right) \\le \\frac{\\mathrm{Var}(\\bar{X}_n)}{\\varepsilon^2}\n$$\nThis is Chebyshev's inequality applied to the sample mean.\n\nNext, we relate $\\mathrm{Var}(\\bar{X}_n)$ to the variance of the underlying random variable, $\\mathrm{Var}(X)$. The draws $X_i$ are independent and identically distributed (i.i.d.).\n$$\n\\mathrm{Var}(\\bar{X}_n) = \\mathrm{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_i\\right)\n$$\nUsing the variance property $\\mathrm{Var}(cZ) = c^2\\mathrm{Var}(Z)$ with $c = 1/n$:\n$$\n\\mathrm{Var}(\\bar{X}_n) = \\frac{1}{n^2} \\mathrm{Var}\\left(\\sum_{i=1}^{n} X_i\\right)\n$$\nBecause the $X_i$ are independent, the variance of their sum is the sum of their variances:\n$$\n\\mathrm{Var}(\\bar{X}_n) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\mathrm{Var}(X_i)\n$$\nBecause the $X_i$ are identically distributed, they all have the same variance, $\\mathrm{Var}(X)$. Therefore:\n$$\n\\mathrm{Var}(\\bar{X}_n) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\mathrm{Var}(X) = \\frac{1}{n^2} (n \\cdot \\mathrm{Var}(X)) = \\frac{\\mathrm{Var}(X)}{n}\n$$\nSubstituting this result back into our inequality yields:\n$$\n\\mathbb{P}\\left(|\\bar{X}_n - \\mu| \\ge \\varepsilon\\right) \\le \\frac{\\mathrm{Var}(X)}{n \\varepsilon^2}\n$$\n\nThe problem states that we have no information about $\\mathrm{Var}(X)$ beyond what can be deduced from the support of $X$ being $[a,b]$. To create a conservative bound valid for any distribution on $[a,b]$, we must find an upper bound on $\\mathrm{Var}(X)$ that depends only on $a$ and $b$. This is known as Popoviciu's inequality on variances, which we now derive.\n\nFor any random variable $X$ with support in $[a,b]$, it holds almost surely that $(X-a)(X-b) \\le 0$. Expanding this gives $X^2 - (a+b)X + ab \\le 0$, which can be rearranged to $X^2 \\le (a+b)X - ab$.\nTaking the expectation of both sides (an operation that preserves inequalities):\n$$\n\\mathbb{E}[X^2] \\le \\mathbb{E}[(a+b)X - ab] = (a+b)\\mathbb{E}[X] - ab = (a+b)\\mu - ab\n$$\nThe variance is defined as $\\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = \\mathbb{E}[X^2] - \\mu^2$. Substituting the inequality for $\\mathbb{E}[X^2]$:\n$$\n\\mathrm{Var}(X) \\le ((a+b)\\mu - ab) - \\mu^2 = -\\mu^2 + (a+b)\\mu - ab\n$$\nThe right-hand side is a quadratic function of $\\mu$, opening downwards. Its maximum value occurs at the vertex, $\\mu = -\\frac{a+b}{2(-1)} = \\frac{a+b}{2}$. Since $X$ is supported on $[a,b]$, its mean $\\mu$ must lie in $[a,b]$, and the vertex $\\frac{a+b}{2}$ is within this range. The maximum value of this quadratic is:\n$$\n-\\left(\\frac{a+b}{2}\\right)^2 + (a+b)\\left(\\frac{a+b}{2}\\right) - ab = \\frac{(a+b)^2}{4} - ab = \\frac{a^2+2ab+b^2 - 4ab}{4} = \\frac{a^2-2ab+b^2}{4} = \\frac{(b-a)^2}{4}\n$$\nThus, for any random variable $X$ with support on $[a,b]$, its variance is bounded by $\\mathrm{Var}(X) \\le \\frac{(b-a)^2}{4}$. This maximum is achieved by a distribution with probability mass $1/2$ at $a$ and $1/2$ at $b$.\n\nTo guarantee our probability constraint for any possible distribution on $[a,b]$, we substitute this worst-case variance into our inequality:\n$$\n\\mathbb{P}\\left(|\\bar{X}_n - \\mu| \\ge \\varepsilon\\right) \\le \\frac{\\mathrm{Var}(X)}{n \\varepsilon^2} \\le \\frac{(b-a)^2 / 4}{n \\varepsilon^2} = \\frac{(b-a)^2}{4 n \\varepsilon^2}\n$$\nWe require this bound to be at most $\\delta$:\n$$\n\\frac{(b-a)^2}{4 n \\varepsilon^2} \\le \\delta\n$$\nSolving for $n$ gives the sufficient condition:\n$$\nn \\ge \\frac{(b-a)^2}{4 \\delta \\varepsilon^2}\n$$\nSince the sample size $n$ must be an integer, we take the ceiling of this expression to find the minimum integer $n$ that satisfies the condition for the general case where $a < b$:\n$$\nn = \\left\\lceil \\frac{(b-a)^2}{4 \\delta \\varepsilon^2} \\right\\rceil\n$$\n\nFinally, we consider the degenerate case where $a=b$. In this situation, the random variable $X$ is a constant, $X=a$ with probability $1$. Consequently, its mean is $\\mu = a$ and its variance is $\\mathrm{Var}(X) = 0$. Every draw $X_i$ will be equal to $a$, so the sample mean $\\bar{X}_n$ will also be exactly $a$, regardless of $n$. The deviation $|\\bar{X}_n - \\mu| = |a-a| = 0$. The probability statement becomes $\\mathbb{P}(0 \\ge \\varepsilon) \\le \\delta$. Since $\\varepsilon > 0$ is given, the event $0 \\ge \\varepsilon$ is impossible and has probability $0$. The inequality $0 \\le \\delta$ is always true because $\\delta \\in (0,1)$. This holds for any sample size $n \\ge 1$. The minimal integer sample size required is therefore $n=1$.\n\nIn summary, the minimal integer sample size $n$ is given by:\n- If $a=b$, then $n=1$.\n- If $a<b$, then $n = \\left\\lceil \\frac{(b-a)^2}{4 \\delta \\varepsilon^2} \\right\\rceil$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Calculates the minimal integer sample size n based on a derived conservative bound\n    using Chebyshev's inequality and a variance upper bound for a random variable\n    with support on a known interval.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: a = 0, b = 1, ε = 0.05, δ = 0.1\n        (0.0, 1.0, 0.05, 0.1),\n        # Case 2: a = -10, b = 10, ε = 0.5, δ = 0.05\n        (-10.0, 10.0, 0.5, 0.05),\n        # Case 3: a = 2, b = 2, ε = 0.1, δ = 0.01\n        (2.0, 2.0, 0.1, 0.01),\n        # Case 4: a = 0, b = 1, ε = 0.01, δ = 0.001\n        (0.0, 1.0, 0.01, 0.001),\n        # Case 5: a = -1, b = 3, ε = 0.02, δ = 0.05\n        (-1.0, 3.0, 0.02, 0.05),\n    ]\n\n    results = []\n    for case in test_cases:\n        a, b, epsilon, delta = case\n        \n        # Validate inputs, although problem statement guarantees ε>0, δ∈(0,1).\n        if a > b or epsilon = 0 or not (0  delta  1):\n             # This case should not be reached with the given test suite.\n             # Handle error gracefully if it were a general-purpose function.\n            raise ValueError(\"Invalid input parameters.\")\n\n        # Handle the degenerate case where the random variable is a constant.\n        # The deviation is always 0, so a single sample is sufficient.\n        if a == b:\n            n = 1\n        else:\n            # For the non-degenerate case, apply the derived formula:\n            # n >= (b-a)^2 / (4 * δ * ε^2)\n            # We must find the smallest integer n satisfying this.\n            numerator = (b - a)**2\n            denominator = 4 * delta * epsilon**2\n            \n            # Since n must be an integer, we take the ceiling.\n            # Using math.ceil from the standard library.\n            # np.ceil also works and is imported.\n            n_float = numerator / denominator\n            n = math.ceil(n_float)\n            \n        results.append(int(n))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3294111"}]}