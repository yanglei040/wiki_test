{"hands_on_practices": [{"introduction": "The characteristic function uniquely defines a probability distribution, but how do we recover the density in practice? This exercise [@problem_id:3293843] challenges you to implement the numerical inversion of a characteristic function using the Fast Fourier Transform (FFT). You will explore the practical consequences of discretization, such as aliasing and spectral leakage, and learn how to mitigate these effects using windowing functions, a vital skill in computational statistics and signal processing.", "problem": "You are tasked with designing a computational procedure for numerically inverting a characteristic function using the Fast Fourier Transform (FFT), analyzing aliasing due to periodic extension, and proposing and implementing windowing to reduce spectral leakage. Your program must be a complete, runnable implementation that, for a given set of test cases, constructs an FFT grid for inversion, computes a probability density function approximation from a characteristic function, and reports quantitative error metrics.\n\nThe fundamental base to be used is the following:\n- The characteristic function of a real-valued random variable is defined by $\\varphi_{X}(t) = \\mathbb{E}[e^{i t X}]$.\n- When a distribution admits a probability density function $f_{X}(x)$, the inversion relation (Fourier inversion) holds in the form $f_{X}(x) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} e^{-i t x} \\varphi_{X}(t) \\, dt$, with the understanding that the integral is taken in a suitable sense when necessary.\n- The Discrete Fourier Transform (DFT) paired with a regular grid obeys the sampling relation that links spatial spacing $\\Delta x$ and frequency spacing $\\Delta t$ via $\\Delta x \\, \\Delta t = \\frac{2\\pi}{N}$ for an $N$-point FFT when using a consistent discretization of the inversion integral.\n- Truncating the Fourier integral to a finite frequency band and sampling at a finite rate implies a periodic extension of the recovered density in the spatial domain, with period $L = N \\Delta x$, causing aliasing of out-of-interval mass back into the fundamental cell. Multiplying the sampled characteristic function by a smooth window that tapers at the ends reduces spectral leakage into the spatial domain but introduces smoothing bias.\n\nYour algorithmic design must:\n1. Construct an evenly spaced spatial grid $\\{x_{j}\\}_{j=0}^{N-1}$ of length $L = N \\Delta x$, centered so that $x_{0} = -L/2$ and $x_{j} = x_{0} + j \\Delta x$; and a dual frequency grid $\\{t_{k}\\}_{k=0}^{N-1}$ with spacing $\\Delta t = \\frac{2\\pi}{L}$ and $t_{k} = (k - N/2)\\Delta t$.\n2. Approximate the inversion integral by a Riemann sum matched to the DFT, so that you can use an $N$-point FFT to compute \n$$\nf_{X}(x_{j}) \\approx \\frac{\\Delta t}{2\\pi} \\sum_{k=0}^{N-1} \\varphi_{X}(t_{k}) \\, e^{-i t_{k} x_{j}}.\n$$\n3. Analyze and reflect the aliasing due to $L$-periodic extension induced by the discrete transform, i.e., the recovered $f_{X}$ is effectively convolved with a Dirichlet kernel and periodized with period $L$. Control aliasing by selecting $L$ sufficiently large for the target distribution and by applying a taper (window) $w_{k}$ in the frequency domain to reduce leakage from the finite band.\n4. Implement at least two windows in the frequency domain: a rectangular window $w_{k} \\equiv 1$ (no taper) and a smooth taper such as the Blackman window. The window multiplies the sampled characteristic function before the FFT.\n5. Compare the recovered density against the exact analytic density on a specified evaluation interval strictly inside the fundamental cell to quantify the maximum absolute error.\n\nDistributions and their characteristic functions and densities to be used for test cases:\n- Normal distribution with mean $\\mu$ and standard deviation $\\sigma$: $\\varphi(t) = \\exp\\!\\left(i \\mu t - \\frac{1}{2}\\sigma^{2} t^{2}\\right)$ and $f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}\\right)$.\n- Laplace (double exponential) distribution with location $\\mu$ and scale $b$: $\\varphi(t) = \\frac{e^{i \\mu t}}{1 + b^{2} t^{2}}$ and $f(x) = \\frac{1}{2b} \\exp\\!\\left(-\\frac{|x - \\mu|}{b}\\right)$.\n- Cauchy distribution with location $\\mu$ and scale $\\gamma$: $\\varphi(t) = \\exp\\!\\left(i \\mu t - \\gamma |t|\\right)$ and $f(x) = \\frac{1}{\\pi} \\frac{\\gamma}{\\gamma^{2} + (x - \\mu)^{2}}$.\n\nTest suite:\nFor each test case, build the FFT inversion with the given parameters, recover the approximate density on the grid, and report the maximum absolute error between the approximate density and the exact density over the evaluation set $\\{x \\in [-x_{\\mathrm{eval}}, x_{\\mathrm{eval}}]\\}$, where $x_{\\mathrm{eval}}$ is given per case.\n\nProvide results for each of the following parameter sets:\n- Case 1 (happy path, light tails, large grid): Normal with $\\mu = 0$, $\\sigma = 1$, $N = 4096$, $L = 50$, rectangular window, evaluate on $[-6, 6]$.\n- Case 2 (coarse grid, aliasing/leakage more visible): Normal with $\\mu = 0$, $\\sigma = 1$, $N = 512$, $L = 20$, rectangular window, evaluate on $[-6, 6]$.\n- Case 3 (same as Case 2 but with windowing): Normal with $\\mu = 0$, $\\sigma = 1$, $N = 512$, $L = 20$, Blackman window, evaluate on $[-6, 6]$.\n- Case 4 (heavier tails than normal, moderate grid): Laplace with $\\mu = 0$, $b = 1$, $N = 1024$, $L = 20$, rectangular window, evaluate on $[-6, 6]$.\n- Case 5 (Laplace with windowing): Laplace with $\\mu = 0$, $b = 1$, $N = 1024$, $L = 20$, Blackman window, evaluate on $[-6, 6]$.\n- Case 6 (heavy tails, aliasing challenge): Cauchy with $\\mu = 0$, $\\gamma = 1$, $N = 2048$, $L = 40$, rectangular window, evaluate on $[-6, 6]$.\n- Case 7 (Cauchy with windowing): Cauchy with $\\mu = 0$, $\\gamma = 1$, $N = 2048$, $L = 40$, Blackman window, evaluate on $[-6, 6]$.\n- Case 8 (boundary sensitivity check, very small domain): Normal with $\\mu = 0$, $\\sigma = 1$, $N = 256$, $L = 8$, rectangular window, evaluate on $[-3, 3]$.\n\nAll angles are in radians. There are no physical units involved. For each case, the program must compute and return a single real number: the maximum absolute error between the numerically recovered density and the exact density on the specified evaluation interval.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,...,result8]\"). Each result must be a floating-point number computed by your program for the corresponding case in the order given above. No additional text should be printed.", "solution": "The problem R-0391 is scientifically grounded, well-posed, and objective. All necessary parameters, formulas, and evaluation criteria are provided. The problem is a standard exercise in numerical Fourier analysis applied to probability theory and is free of any scientific or logical flaws. The task is to numerically invert a characteristic function using the Fast Fourier Transform (FFT), a well-established technique. The problem is therefore deemed **valid**.\n\nThe solution to this problem involves the numerical approximation of the Fourier inversion integral that recovers a probability density function (PDF), $f_X(x)$, from its characteristic function (CF), $\\varphi_X(t)$. The core principle is to discretize the integral and map the resulting sum to a computationally efficient Discrete Fourier Transform (DFT), implemented via the Fast Fourier Transform (FFT) algorithm.\n\nThe relationship between the PDF and the CF is given by the Fourier inversion theorem:\n$$\nf_X(x) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} e^{-i t x} \\varphi_X(t) \\, dt\n$$\nTo evaluate this numerically, we truncate the integral to a finite domain $[t_{\\min}, t_{\\max}]$ and approximate it with a Riemann sum over a discrete grid of frequencies. The problem specifies a frequency grid $\\{t_k\\}_{k=0}^{N-1}$ and a corresponding spatial grid $\\{x_j\\}_{j=0}^{N-1}$.\n\nThe grids are defined as follows:\n- The spatial domain has length $L$. The grid spacing is $\\Delta x = L/N$. The grid points are $x_j = -L/2 + j \\Delta x$ for $j \\in \\{0, 1, \\dots, N-1\\}$.\n- The frequency domain spacing is linked by the DFT sampling relation, $\\Delta t = 2\\pi / L = 2\\pi / (N \\Delta x)$. The grid points are $t_k = (k - N/2) \\Delta t$ for $k \\in \\{0, 1, \\dots, N-1\\}$.\n\nUsing these grids, the inversion integral is approximated by the sum:\n$$\nf_X(x_j) \\approx \\frac{\\Delta t}{2\\pi} \\sum_{k=0}^{N-1} \\varphi_X(t_k) \\, e^{-i t_k x_j}\n$$\nThe primary challenge is to compute this sum efficiently for all $j=0, \\dots, N-1$. A direct summation would have a complexity of $O(N^2)$, which is inefficient for large $N$. The structure of the sum, however, is amenable to the FFT algorithm, which has a complexity of $O(N \\log N)$.\n\nTo map the sum to the standard definition of a DFT, which is $G_j = \\sum_{k=0}^{N-1} g_k e^{-i 2\\pi jk/N}$, we must analyze the phase term $e^{-i t_k x_j}$. Substituting the grid definitions:\n$$\nt_k x_j = \\left(k - \\frac{N}{2}\\right)\\Delta t \\left(j \\Delta x - \\frac{L}{2}\\right)\n$$\nUsing $\\Delta x = L/N$ and $\\Delta t = 2\\pi/L$:\n$$\n\\begin{align*}\nt_k x_j = \\left(k - \\frac{N}{2}\\right)\\frac{2\\pi}{L} \\left(j \\frac{L}{N} - \\frac{L}{2}\\right) \\\\\n= \\left(k - \\frac{N}{2}\\right)\\frac{2\\pi}{N} \\left(j - \\frac{N}{2}\\right) \\\\\n= \\frac{2\\pi}{N} \\left(kj - \\frac{kN}{2} - \\frac{jN}{2} + \\frac{N^2}{4}\\right) \\\\\n= \\frac{2\\pi kj}{N} - k\\pi - j\\pi + \\frac{N\\pi}{2}\n\\end{align*}\n$$\nThe exponential of the phase term is therefore:\n$$\ne^{-i t_k x_j} = e^{-i (2\\pi kj/N - k\\pi - j\\pi + N\\pi/2)} = e^{-i 2\\pi kj/N} e^{ik\\pi} e^{ij\\pi} e^{-iN\\pi/2}\n$$\nSince $e^{i\\pi} = -1$, this simplifies to:\n$$\ne^{-i t_k x_j} = (-1)^k (-1)^j e^{-iN\\pi/2} e^{-i 2\\pi kj/N}\n$$\nThe test cases use values of $N$ that are powers of two and thus multiples of 4 (e.g., $256, 512, \\dots, 4096$). For such $N$, $N/2$ is an even integer, so $e^{-iN\\pi/2} = \\cos(-N\\pi/2) + i\\sin(-N\\pi/2) = 1$. The phase term simplifies further to $e^{-i t_k x_j} = (-1)^k (-1)^j e^{-i 2\\pi kj/N}$.\n\nSubstituting this back into the sum, we get:\n$$\nf_X(x_j) \\approx \\frac{\\Delta t}{2\\pi} \\sum_{k=0}^{N-1} \\varphi_X(t_k) \\, (-1)^k (-1)^j e^{-i 2\\pi kj/N}\n$$\nThe term $(-1)^j$ can be moved outside the summation. Defining a new sequence $Y_k = (-1)^k \\varphi_X(t_k)$, the sum becomes:\n$$\nf_X(x_j) \\approx \\frac{\\Delta t}{2\\pi} (-1)^j \\sum_{k=0}^{N-1} Y_k \\, e^{-i 2\\pi kj/N} = \\frac{1}{L} (-1)^j \\cdot \\text{FFT}(Y)_j\n$$\nThis provides an efficient algorithm:\n1. Construct the frequency grid $\\{t_k\\}$.\n2. Evaluate the characteristic function $\\varphi_X(t_k)$ on this grid.\n3. Apply a window function $w_k$ if required: $\\varphi_{X,w}(t_k) = w_k \\varphi_X(t_k)$.\n4. Create the sequence for the FFT: $Y_k = (-1)^k \\varphi_{X,w}(t_k)$.\n5. Compute the FFT of $Y_k$ to get $\\hat{Y}_j = \\text{FFT}(Y)_j$.\n6. Compute the final approximation $f_X(x_j) \\approx \\frac{1}{L} \\text{Re}[(-1)^j \\hat{Y}_j]$. We take the real part as the PDF is real-valued, and any residual imaginary component is due to numerical approximation errors.\n\nThe discretization process introduces two main types of errors. First, truncating the integral to a finite frequency domain $[-t_{\\max}, t_{\\max}]$ (where $t_{\\max} \\approx N/2 \\cdot \\Delta t$) is equivalent to convolving the true PDF with a sinc-like kernel, causing ringing artifacts (Gibbs phenomenon or spectral leakage). Applying a smooth window function, such as the Blackman window, tapers the integrand $\\varphi_X(t)$ smoothly to zero at the truncation boundaries. This significantly reduces leakage at the cost of slightly broadening the main features of the recovered PDF (a bias-smoothing trade-off).\n\nSecond, sampling the frequency domain at a finite rate $\\Delta t$ causes the recovered spatial function to be periodic with period $L=2\\pi/\\Delta t$. If the true PDF $f_X(x)$ has significant mass outside the interval $[-L/2, L/2]$, this out-of-interval mass \"aliases\" back into the interval, distorting the approximation. This aliasing error is controlled by choosing a sufficiently large domain length $L$.\n\nThe implementation will execute this algorithm for each test case, comparing the resulting approximate PDF on the specified evaluation interval against the known analytical PDF to compute the maximum absolute error.", "answer": "```python\nimport numpy as np\n\ndef char_func_normal(t, mu, sigma):\n    \"\"\"Characteristic function of a Normal distribution.\"\"\"\n    return np.exp(1j * mu * t - 0.5 * (sigma**2) * (t**2))\n\ndef pdf_normal(x, mu, sigma):\n    \"\"\"Probability density function of a Normal distribution.\"\"\"\n    return (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mu) / sigma)**2)\n\ndef char_func_laplace(t, mu, b):\n    \"\"\"Characteristic function of a Laplace distribution.\"\"\"\n    return np.exp(1j * mu * t) / (1 + (b**2) * (t**2))\n\ndef pdf_laplace(x, mu, b):\n    \"\"\"Probability density function of a Laplace distribution.\"\"\"\n    return (1 / (2 * b)) * np.exp(-np.abs(x - mu) / b)\n\ndef char_func_cauchy(t, mu, gamma):\n    \"\"\"Characteristic function of a Cauchy distribution.\"\"\"\n    return np.exp(1j * mu * t - gamma * np.abs(t))\n\ndef pdf_cauchy(x, mu, gamma):\n    \"\"\"Probability density function of a Cauchy distribution.\"\"\"\n    return (1 / np.pi) * (gamma / (gamma**2 + (x - mu)**2))\n\ndef compute_pdf_error(dist_name, dist_params, N, L, window_type, x_eval):\n    \"\"\"\n    Computes the max absolute error of the FFT-recovered PDF for a given case.\n    \"\"\"\n    # 1. Select the appropriate characteristic and density functions\n    if dist_name == 'normal':\n        char_func = char_func_normal\n        pdf_exact = pdf_normal\n    elif dist_name == 'laplace':\n        char_func = char_func_laplace\n        pdf_exact = pdf_laplace\n    elif dist_name == 'cauchy':\n        char_func = char_func_cauchy\n        pdf_exact = pdf_cauchy\n    else:\n        raise ValueError(f\"Unknown distribution: {dist_name}\")\n\n    # 2. Construct spatial and frequency grids\n    dx = L / N\n    x_grid = -L / 2 + np.arange(N) * dx\n    \n    dt = 2 * np.pi / L\n    t_grid = (np.arange(N) - N / 2) * dt\n\n    # 3. Sample the characteristic function\n    phi_t = char_func(t_grid, **dist_params)\n\n    # 4. Apply windowing in the frequency domain\n    if window_type == 'rectangular':\n        window = np.ones(N)\n    elif window_type == 'blackman':\n        # The Blackman window is naturally centered at N/2, which corresponds\n        # to t=0 in our `t_grid` arrangement.\n        window = np.blackman(N)\n    else:\n        raise ValueError(f\"Unknown window type: {window_type}\")\n\n    phi_windowed = phi_t * window\n    \n    # 5. Perform FFT-based inversion\n    # The derivation f(x_j) approx (1/L) * (-1)^j * FFT[(-1)^k * phi(t_k)]\n    # is implemented below.\n    arange_N = np.arange(N)\n    phase_shift = (-1)**arange_N\n\n    Y_k = phase_shift * phi_windowed\n    Y_hat_j = np.fft.fft(Y_k)\n    f_approx_complex = (1 / L) * phase_shift * Y_hat_j\n    \n    # The resulting PDF must be real; imaginary part is numerical error.\n    f_approx = np.real(f_approx_complex)\n\n    # 6. Calculate maximum absolute error on the evaluation interval\n    eval_mask = (x_grid = -x_eval)  (x_grid = x_eval)\n    x_compare = x_grid[eval_mask]\n    \n    if len(x_compare) == 0:\n        # This case would indicate the evaluation interval is outside the grid,\n        # which shouldn't happen with the given test cases.\n        return np.inf\n\n    f_approx_compare = f_approx[eval_mask]\n    f_exact_compare = pdf_exact(x_compare, **dist_params)\n    \n    max_abs_error = np.max(np.abs(f_approx_compare - f_exact_compare))\n    \n    return max_abs_error\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        ('normal', {'mu': 0, 'sigma': 1}, 4096, 50, 'rectangular', 6),\n        # Case 2\n        ('normal', {'mu': 0, 'sigma': 1}, 512, 20, 'rectangular', 6),\n        # Case 3\n        ('normal', {'mu': 0, 'sigma': 1}, 512, 20, 'blackman', 6),\n        # Case 4\n        ('laplace', {'mu': 0, 'b': 1}, 1024, 20, 'rectangular', 6),\n        # Case 5\n        ('laplace', {'mu': 0, 'b': 1}, 1024, 20, 'blackman', 6),\n        # Case 6\n        ('cauchy', {'mu': 0, 'gamma': 1}, 2048, 40, 'rectangular', 6),\n        # Case 7\n        ('cauchy', {'mu': 0, 'gamma': 1}, 2048, 40, 'blackman', 6),\n        # Case 8\n        ('normal', {'mu': 0, 'sigma': 1}, 256, 8, 'rectangular', 3),\n    ]\n\n    results = []\n    for case in test_cases:\n        dist_name, dist_params, N, L, window_type, x_eval = case\n        error = compute_pdf_error(dist_name, dist_params, N, L, window_type, x_eval)\n        results.append(error)\n\n    # Print results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3293843"}, {"introduction": "Beyond direct inversion, the characteristic function offers a powerful way to analyze and approximate distributions through its cumulants. This practice [@problem_id:3293821] guides you through constructing a Gram-Charlier series, an analytical approximation that corrects a standard normal baseline using the target distribution's skewness and kurtosis. By comparing this approximation to a high-accuracy result from numerical integration, you will gain insight into the power and limitations of moment-based expansions.", "problem": "Construct a complete, runnable program that evaluates the accuracy of a Gram–Charlier correction for a normal baseline using cumulants up to order four and compares it against a tail probability computed via a characteristic function (CF) inversion. The program must implement the following.\n\nConsider a family of standardized random variables defined as follows. Let $Z$ be a standard normal random variable with CF $\\varphi_Z(t) = \\exp(-t^2/2)$. Let $E_1$ be an exponential random variable with rate $1$ and CF $\\varphi_{E_1}(t) = \\frac{1}{1 - i t}$. Define $U = E_1 - 1$, so that $\\mathbb{E}[U] = 0$ and $\\varphi_U(t) = \\exp(-i t) \\varphi_{E_1}(t) = \\frac{\\exp(-i t)}{1 - i t}$. For a given real parameter $\\alpha \\ge 0$, define $Y = Z + \\alpha U$, and the standardized variable $X = Y / s$ where $s = \\sqrt{1 + \\alpha^2}$. This construction ensures $\\mathbb{E}[X] = 0$ and $\\mathrm{Var}(X) = 1$. \n\nThe CF of $X$ is given by $\\varphi_X(t) = \\varphi_Y(t/s)$, where $\\varphi_Y(u) = \\varphi_Z(u)\\,\\varphi_{\\alpha U}(u)$ and $\\varphi_{\\alpha U}(u) = \\varphi_U(\\alpha u)$. The cumulants of independent sums add, and cumulants scale with powers under scaling: if $W = c V$ then $\\kappa_r(W) = c^r \\kappa_r(V)$. The exponential distribution with rate $1$ has cumulants $\\kappa_r(E_1) = (r-1)!$ for all integers $r \\ge 1$; hence $U = E_1 - 1$ satisfies $\\kappa_1(U) = 0$ and $\\kappa_r(U) = (r-1)!$ for all integers $r \\ge 2$. Use only these foundational facts, together with the definition of a CF $\\varphi_X(t) = \\mathbb{E}[e^{i t X}]$ and the definition of cumulants via the logarithm of the CF, to derive the standardized cumulants of $X$ up to order four and use them to build a Gram–Charlier correction to a normal baseline.\n\nTasks to implement in the program:\n\n1. Using only the fundamental facts stated above, derive and implement the CF $\\varphi_X(t)$ for the given $X$. Then, compute the survival probability $\\mathbb{P}(X  x)$ using a valid inversion of the CF for the cumulative distribution function. The inversion must be implemented numerically by quadrature with sufficiently tight tolerances to resolve errors at the scale of $10^{-8}$ or smaller.\n\n2. Using the cumulants up to order four for $X$, construct the Gram–Charlier A series correction to a standard normal baseline, truncated to include terms that depend on the third and fourth cumulants only. From this truncated series, derive the corresponding approximation to the cumulative distribution function $F_{\\mathrm{GC}}(x)$, and from it compute the approximate survival probability $1 - F_{\\mathrm{GC}}(x)$.\n\n3. For each test case specified below, compute the absolute error between the Gram–Charlier survival probability and the CF-inversion survival probability, that is, compute $\\left| \\left(1 - F_{\\mathrm{GC}}(x)\\right) - \\mathbb{P}(X  x) \\right|$.\n\nTest suite to evaluate:\n- $(\\alpha, x) = (0.0, 1.0)$\n- $(\\alpha, x) = (0.5, 1.0)$\n- $(\\alpha, x) = (0.5, 2.0)$\n- $(\\alpha, x) = (1.0, 1.0)$\n- $(\\alpha, x) = (1.5, 0.0)$\n- $(\\alpha, x) = (1.5, 2.0)$\n\nAll computations are purely mathematical and unitless. Angles used in complex exponentials are in radians.\n\nYour program should produce a single line of output containing the absolute errors for the above test cases as a comma-separated list enclosed in square brackets, with each floating-point number formatted in scientific notation with eight digits after the decimal point. For example, the output format must be like \"[1.23456789e-04,2.34567891e-06,...]\" but with no spaces, i.e., \"[1.23456789e-04,2.34567891e-06,...]\".", "solution": "The user-provided problem is a well-defined exercise in computational probability and is deemed valid after a thorough review. All provided information is scientifically sound, self-contained, and mathematically consistent. The problem asks for the implementation and comparison of two methods for approximating a survival probability: one based on the numerical inversion of a characteristic function (CF) and another using a Gram-Charlier A series expansion.\n\nWe begin by systematically deriving the necessary mathematical formulas as outlined in the problem statement.\n\n### 1. Derivation of the Characteristic Function of $X$.\n\nThe random variable $X$ is constructed in several steps. We derive its characteristic function, $\\varphi_X(t) = \\mathbb{E}[e^{itX}]$, by following this construction.\n\n-   The variable $U$ is defined as $U = E_1 - 1$, where $E_1$ is an exponential random variable with rate $1$. Its characteristic function is $\\varphi_{E_1}(t) = \\frac{1}{1 - it}$. The CF of $U$ is:\n    $$ \\varphi_U(t) = \\mathbb{E}[e^{it(E_1-1)}] = e^{-it}\\mathbb{E}[e^{itE_1}] = e^{-it}\\varphi_{E_1}(t) = \\frac{e^{-it}}{1 - it} $$\n-   The variable $Y$ is the sum of a standard normal variable $Z$ and a scaled version of $U$, i.e., $Y = Z + \\alpha U$. As $Z$ and $U$ are constructed from independent sources (implied by the problem's setup), the CF of their sum is the product of their individual CFs. The CF of $\\alpha U$ is $\\varphi_{\\alpha U}(t) = \\varphi_U(\\alpha t)$.\n    $$ \\varphi_Y(t) = \\varphi_Z(t) \\cdot \\varphi_{\\alpha U}(t) = \\varphi_Z(t) \\cdot \\varphi_U(\\alpha t) $$\n    Substituting the known CFs:\n    $$ \\varphi_Y(t) = \\exp\\left(-\\frac{t^2}{2}\\right) \\cdot \\frac{e^{-i\\alpha t}}{1 - i\\alpha t} $$\n-   Finally, $X$ is the standardized version of $Y$, given by $X = Y/s$, where $s = \\sqrt{1 + \\alpha^2}$. The CF of a scaled variable $aW$ is $\\varphi_W(at)$. Thus, the CF of $X$ is:\n    $$ \\varphi_X(t) = \\varphi_Y(t/s) = \\exp\\left(-\\frac{(t/s)^2}{2}\\right) \\frac{\\exp(-i\\alpha (t/s))}{1 - i\\alpha (t/s)} $$\n    Substituting $s = \\sqrt{1+\\alpha^2}$, we obtain the final expression for the characteristic function of $X$:\n    $$ \\varphi_X(t) = \\exp\\left(-\\frac{t^2}{2(1+\\alpha^2)}\\right) \\frac{\\exp\\left(-\\frac{i\\alpha t}{\\sqrt{1+\\alpha^2}}\\right)}{1 - \\frac{i\\alpha t}{\\sqrt{1+\\alpha^2}}} $$\n\n### 2. Survival Probability via CF Inversion\n\nThe cumulative distribution function (CDF), $F_X(x) = \\mathbb{P}(X \\le x)$, can be recovered from its CF, $\\varphi_X(t)$, using the Gil-Pelaez inversion formula:\n$$ F_X(x) = \\frac{1}{2} - \\frac{1}{\\pi} \\int_0^\\infty \\text{Im}\\left( e^{-itx} \\varphi_X(t) \\right) \\frac{dt}{t} $$\nThe survival probability is $\\mathbb{P}(X  x) = 1 - F_X(x)$. Substituting the formula for $F_X(x)$:\n$$ \\mathbb{P}(X  x) = 1 - \\left( \\frac{1}{2} - \\frac{1}{\\pi} \\int_0^\\infty \\text{Im}\\left( e^{-itx} \\varphi_X(t) \\right) \\frac{dt}{t} \\right) = \\frac{1}{2} + \\frac{1}{\\pi} \\int_0^\\infty \\text{Im}\\left( e^{-itx} \\varphi_X(t) \\right) \\frac{dt}{t} $$\nThe integrand, let's call it $I(t,x,\\alpha)$, must be computed numerically. The expression inside the imaginary part operator is:\n$$ e^{-itx} \\varphi_X(t) = \\exp(-itx) \\exp\\left(-\\frac{t^2}{2s^2}\\right) \\frac{\\exp(-i \\alpha t/s)}{1 - i \\alpha t/s} = \\exp\\left(-\\frac{t^2}{2s^2} - it\\left(x + \\frac{\\alpha}{s}\\right)\\right) \\frac{1}{1 - i \\alpha t/s} $$\nThis expression is computed as a complex number for each $t$, its imaginary part is taken, divided by $t$, and then integrated from $t=0$ to $t=\\infty$. The integral will be evaluated using numerical quadrature, for which the `scipy.integrate.quad` function is suitable. The Gaussian decay term $\\exp(-t^2/(2s^2))$ ensures the integrand converges rapidly, making the numerical integration stable and efficient.\n\n### 3. Derivation of the Cumulants of $X$\n\nThe problem provides the necessary properties of cumulants, $\\kappa_r$.\n-   $\\kappa_r(E_1) = (r-1)!$ for $r \\ge 1$.\n-   For $U = E_1 - 1$, we have $\\kappa_1(U) = \\kappa_1(E_1) - 1 = (1-1)! - 1 = 0$, and for $r \\ge 2$, $\\kappa_r(U) = \\kappa_r(E_1) = (r-1)!$.\n-   For $Y = Z + \\alpha U$, using cumulant additivity for independent variables and the scaling property:\n    $$ \\kappa_r(Y) = \\kappa_r(Z) + \\kappa_r(\\alpha U) = \\kappa_r(Z) + \\alpha^r \\kappa_r(U) $$\n    The cumulants of a standard normal variable $Z$ are $\\kappa_1(Z) = 0$, $\\kappa_2(Z) = 1$, and $\\kappa_r(Z) = 0$ for $r \\ge 3$.\n    -   $\\kappa_1(Y) = 0 + \\alpha^1 \\cdot 0 = 0$.\n    -   $\\kappa_2(Y) = 1 + \\alpha^2 \\kappa_2(U) = 1 + \\alpha^2 (2-1)! = 1 + \\alpha^2$.\n    -   $\\kappa_3(Y) = 0 + \\alpha^3 \\kappa_3(U) = \\alpha^3 (3-1)! = 2\\alpha^3$.\n    -   $\\kappa_4(Y) = 0 + \\alpha^4 \\kappa_4(U) = \\alpha^4 (4-1)! = 6\\alpha^4$.\n-   For $X = Y/s$ with $s = \\sqrt{\\kappa_2(Y)} = \\sqrt{1+\\alpha^2}$, the cumulants are scaled by $s^{-r}$:\n    -   $\\kappa_1(X) = \\kappa_1(Y)/s = 0$.\n    -   $\\kappa_2(X) = \\kappa_2(Y)/s^2 = (1+\\alpha^2)/(1+\\alpha^2) = 1$.\n    -   $\\kappa_3(X) = \\frac{2\\alpha^3}{(1+\\alpha^2)^{3/2}}$. (Standardized skewness)\n    -   $\\kappa_4(X) = \\frac{6\\alpha^4}{(1+\\alpha^2)^2}$. (Standardized excess kurtosis)\n\n### 4. Gram-Charlier Survival Probability\n\nThe Gram-Charlier A series provides an approximation for the probability density function (PDF) of a random variable in terms of the standard normal PDF, $\\phi(x)$, and its derivatives (related to Hermite polynomials). The truncated series for the PDF $f_X(x)$ using cumulants up to order four is:\n$$ f_{\\mathrm{GC}}(x) = \\phi(x) \\left[ 1 + \\frac{\\kappa_3(X)}{3!} H_3(x) + \\frac{\\kappa_4(X)}{4!} H_4(x) \\right] $$\nwhere $H_k(x)$ are the probabilist's Hermite polynomials, e.g., $H_2(x) = x^2-1$, $H_3(x) = x^3 - 3x$.\n\nTo find the CDF approximation $F_{\\mathrm{GC}}(x)$, we integrate $f_{\\mathrm{GC}}(u)$ from $-\\infty$ to $x$. Using the property $\\int_{-\\infty}^x \\phi(u) H_k(u) du = -\\phi(x) H_{k-1}(x)$, we get:\n$$ F_{\\mathrm{GC}}(x) = \\Phi(x) - \\phi(x) \\left[ \\frac{\\kappa_3(X)}{6} H_2(x) + \\frac{\\kappa_4(X)}{24} H_3(x) \\right] $$\nwhere $\\Phi(x)$ is the standard normal CDF.\nThe approximate survival probability, $1 - F_{\\mathrm{GC}}(x)$, is therefore:\n$$ 1 - F_{\\mathrm{GC}}(x) = (1 - \\Phi(x)) + \\phi(x) \\left[ \\frac{\\kappa_3(X)}{6} (x^2 - 1) + \\frac{\\kappa_4(X)}{24} (x^3 - 3x) \\right] $$\nwhere $1 - \\Phi(x)$ is the standard normal survival function.\n\n### 5. Algorithmic Implementation\n\nThe program will implement these derived formulas for each test case $(\\alpha, x)$.\n1.  For a given $(\\alpha, x)$, the value of $s = \\sqrt{1+\\alpha^2}$ is computed.\n2.  An integrand function based on the CF inversion formula is defined. `scipy.integrate.quad` is used to compute the integral, yielding the \"true\" survival probability $\\mathbb{P}(X  x)$.\n3.  The cumulants $\\kappa_3(X)$ and $\\kappa_4(X)$ are calculated.\n4.  The Gram-Charlier survival probability approximation is computed using the formula from step 4, with $\\phi(x)$ and $1-\\Phi(x)$ provided by `scipy.stats.norm`.\n5.  The absolute error between the two probabilities is calculated and stored.\n6.  A special case is handled for $\\alpha=0$, where $X$ is exactly standard normal. In this case, the Gram-Charlier correction terms are zero, and both methods should yield the standard normal survival function. The error is thus theoretically zero.\n7.  The final list of errors is formatted and printed as specified.", "answer": "```python\nimport numpy as np\nfrom scipy import integrate\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the absolute error between Gram-Charlier and CF-inversion\n    approximations for the survival probability of a defined random variable.\n    \"\"\"\n    \n    test_cases = [\n        (0.0, 1.0),\n        (0.5, 1.0),\n        (0.5, 2.0),\n        (1.0, 1.0),\n        (1.5, 0.0),\n        (1.5, 2.0)\n    ]\n\n    results = []\n\n    # Set a tight tolerance for numerical integration to meet problem requirements.\n    QUAD_TOL = 1e-12\n\n    for alpha, x in test_cases:\n        # Case 1: alpha = 0. The variable X is standard normal.\n        if alpha == 0.0:\n            # k3 and k4 are 0, so GC approx is just norm.sf(x).\n            # CF inversion also yields norm.sf(x).\n            # The theoretical error is 0.\n            results.append(0.0)\n            continue\n            \n        # For alpha  0, proceed with the full calculation.\n        s = np.sqrt(1.0 + alpha**2)\n\n        # ---- Task 1: Survival probability from CF inversion ----\n        \n        def char_func_x(t, local_alpha, local_s):\n            \"\"\"Characteristic function of the standardized variable X.\"\"\"\n            if t == 0:\n                return complex(1.0, 0.0)\n            \n            term1 = np.exp(- (t**2) / (2.0 * local_s**2))\n            \n            common_arg = local_alpha * t / local_s\n            numerator = np.exp(-1j * common_arg)\n            denominator = 1.0 - 1j * common_arg\n            \n            term2 = numerator / denominator\n            \n            return term1 * term2\n\n        def inversion_integrand(t, local_alpha, local_s, local_x):\n            \"\"\"Integrand for the Gil-Pelaez inversion formula.\"\"\"\n            if t == 0:\n                # The limit of Im(exp(-itx)phi(t))/t as t-0 is finite.\n                # However, for numerical stability, we can return 0 here\n                # because quad handles singularities at the boundary.\n                return 0.0\n            \n            phi_t = char_func_x(t, local_alpha, local_s)\n            val = np.exp(-1j * t * local_x) * phi_t\n            return val.imag / t\n        \n        # Integrate from 0 to infinity\n        integral_val, _ = integrate.quad(\n            inversion_integrand, 0, np.inf, args=(alpha, s, x),\n            epsabs=QUAD_TOL, epsrel=QUAD_TOL\n        )\n        \n        prob_cf = 0.5 + integral_val / np.pi\n\n        # ---- Task 2: Survival probability from Gram-Charlier series ----\n\n        # Calculate standardized cumulants k3 and k4\n        s_cubed = s**3\n        s_fourth = s**4\n        alpha_cubed = alpha**3\n        alpha_fourth = alpha**4\n\n        k3 = (2.0 * alpha_cubed) / s_cubed\n        k4 = (6.0 * alpha_fourth) / s_fourth\n        \n        # Hermite polynomials related terms\n        h2_term = x**2 - 1.0\n        h3_term = x**3 - 3.0 * x\n\n        # Gram-Charlier correction term\n        gc_correction = norm.pdf(x) * (k3 / 6.0 * h2_term + k4 / 24.0 * h3_term)\n        \n        prob_gc = norm.sf(x) + gc_correction\n        \n        # ---- Task 3: Compute absolute error ----\n        \n        absolute_error = abs(prob_gc - prob_cf)\n        results.append(absolute_error)\n\n    # Format and print the final output as specified.\n    output_str = \",\".join([f\"{res:.8e}\" for res in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n\n```", "id": "3293821"}, {"introduction": "Characteristic functions are not just theoretical objects; their sample-based counterparts, empirical characteristic functions (ECFs), are powerful tools for non-parametric inference. In this advanced exercise [@problem_id:3293826], you will design and implement a statistical test for stationarity in a time series by comparing ECFs from different data blocks. This task will give you hands-on experience with the block bootstrap, a crucial resampling method for calibrating tests with dependent data, showcasing how foundational concepts can be extended to solve complex inferential problems.", "problem": "Consider a real-valued, time-homogeneous Markov chain $\\{X_n\\}_{n \\ge 0}$ with one-step transition rule on $\\mathbb{R}$. The characteristic function of a random variable $X$ is the mapping $\\varphi_X(t) = \\mathbb{E}[\\exp(i t X)]$, for $t \\in \\mathbb{R}$, where $i$ denotes the imaginary unit and the angle unit is radians. The empirical characteristic function over a finite index set $A \\subset \\{1,2,\\dots,N\\}$ is the average $\\hat{\\varphi}_A(t)$ obtained by replacing $\\mathbb{E}[\\cdot]$ by the sample mean over the indices in $A$. Assume that under the null hypothesis $H_0$, the marginal distribution of $X_n$ is stationary, so that $\\varphi_X(t)$ does not depend on $n$. \n\nA mixing diagnostic can be constructed by comparing empirical characteristic functions computed on two consecutive blocks of equal length. Let $k \\in \\mathbb{N}$ and consider the first block $B_1 = \\{1,2,\\dots,k\\}$ and the second block $B_2 = \\{k+1,k+2,\\dots,2k\\}$. Define the blockwise difference process across a grid $\\mathcal{T} \\subset \\mathbb{R}$ by the quantity $Z_k(t)$ computed from the empirical characteristic functions for $B_1$ and $B_2$. The overall test statistic $T_k$ is the supremum of the absolute magnitude of $Z_k(t)$ across $t \\in \\mathcal{T}$. Threshold calibration must be performed by a dependence-respecting bootstrap that draws blocks from the observed series to approximate the distribution of the test statistic under $H_0$.\n\nStarting from the fundamental definitions of characteristic functions and the stationarity of a time-homogeneous Markov chain’s marginal distribution under $H_0$, design and implement a complete, runnable program that:\n- Simulates specific Markov chains with given parameters to produce a sequence $(X_1,\\dots,X_{2k})$.\n- Computes the empirical characteristic function on each block $B_1$ and $B_2$ over a user-specified finite grid $\\mathcal{T}$ of $t$ values (in radians).\n- Forms the blockwise difference process across $\\mathcal{T}$, scales it appropriately in $k$, and aggregates into a single scalar test statistic $T_k$ capturing the maximal discrepancy across $\\mathcal{T}$.\n- Calibrates the decision threshold by a circular moving block bootstrap of the observed series of length $2k$. The bootstrap must generate $B$ replicates, each of which is a resampled series of length $2k$ formed by concatenating blocks of length $l$ with wrap-around, compute the bootstrap analogue of the test statistic $T_k^\\ast$, and estimate a $p$-value as the proportion of replicates with $T_k^\\ast$ greater than or equal to the observed $T_k$.\n- Returns a boolean decision for each test case indicating whether to reject $H_0$ at level $\\alpha$.\n\nYour program must implement the above using only the definitions of characteristic functions and stationarity as the conceptual base, and must not rely on any pre-derived asymptotic distribution shortcuts. All angles must be treated in radians. No physical units are involved.\n\nTest Suite:\nFor coverage, implement the following five test cases, each producing one boolean outcome. In all cases, use only the first $2k$ simulated values to form the blocks.\n1. Happy path (independent, identically distributed Gaussian): \n   - Length $N = 2000$, block size $k = 1000$, autoregressive parameter $\\rho = 0$, Gaussian innovation standard deviation $\\sigma = 1$, mean shift $\\delta = 0$, $t$-grid $\\mathcal{T} = \\{-2,-1,0.5,1.5,3\\}$, bootstrap block length $l = 20$, number of bootstrap replicates $B = 300$, significance level $\\alpha = 0.05$, random seed $s = 42$.\n2. Slow mixing but stationary autoregression:\n   - $N = 2000$, $k = 1000$, $\\rho = 0.95$, choose $\\sigma$ so that the stationary variance is $1$ (i.e., $\\sigma = \\sqrt{1 - \\rho^2}$), $\\delta = 0$, $\\mathcal{T} = \\{-2,-1,0.5,1.5,3\\}$, $l = 50$, $B = 300$, $\\alpha = 0.05$, $s = 123$.\n3. Nonstationary mean shift at mid-point:\n   - $N = 2000$, $k = 1000$, $\\rho = 0.7$, choose $\\sigma = \\sqrt{1 - \\rho^2}$, apply a deterministic mean shift $\\delta = 0.5$ added to each step from index $k+1$ to $2k$, $\\mathcal{T} = \\{-2,-1,0.5,1.5,3\\}$, $l = 30$, $B = 300$, $\\alpha = 0.05$, $s = 7$.\n4. Boundary grid (degenerate at zero frequency):\n   - $N = 2000$, $k = 1000$, $\\rho = 0.7$, $\\sigma = \\sqrt{1 - \\rho^2}$, $\\delta = 0$, $\\mathcal{T} = \\{0.0\\}$, $l = 30$, $B = 200$, $\\alpha = 0.05$, $s = 99$.\n5. Small sample stationary autoregression:\n   - $N = 200$, $k = 100$, $\\rho = 0.7$, $\\sigma = \\sqrt{1 - \\rho^2}$, $\\delta = 0$, $\\mathcal{T} = \\{-1,0.1,0.5,1.0,2.0\\}$, $l = 10$, $B = 200$, $\\alpha = 0.05$, $s = 2024$.\n\nSimulation details:\n- For an autoregressive model of order one, use the update $X_{n+1} = \\rho X_n + \\epsilon_{n+1} + \\delta \\cdot \\mathbf{1}\\{n+1  k\\}$, where $\\epsilon_{n+1} \\sim \\mathcal{N}(0,\\sigma^2)$ are independent innovations, and the indicator applies only in the mean-shift case (test case $3$). \n- Initialize $X_0$ from the stationary distribution $\\mathcal{N}\\!\\left(0,\\frac{\\sigma^2}{1-\\rho^2}\\right)$ when $|\\rho|  1$, and from $\\mathcal{N}(0,\\sigma^2)$ when $\\rho = 0$.\n- Use the first $2k$ values $(X_1,\\dots,X_{2k})$.\n\nOutput specification:\n- For each test case, compute the bootstrap $p$-value and output a boolean indicating whether $H_0$ is rejected at level $\\alpha$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_5]$), where each result is either $\\text{True}$ or $\\text{False}$.", "solution": "The problem requires the design and implementation of a statistical test to detect non-stationarity in a time series. The test is based on comparing the empirical characteristic functions (ECFs) of two consecutive blocks of the series. The significance of the observed test statistic is assessed using a circular moving block bootstrap procedure, which is appropriate for dependent data.\n\n**1. Theoretical Framework**\n\nThe foundation of this test is the characteristic function of a real-valued random variable $X$, defined as:\n$$\n\\varphi_X(t) = \\mathbb{E}[e^{itX}]\n$$\nwhere $t \\in \\mathbb{R}$ is a real-valued frequency parameter, $i = \\sqrt{-1}$ is the imaginary unit, and $\\mathbb{E}[\\cdot]$ denotes the expectation operator. The characteristic function uniquely determines the probability distribution of $X$.\n\nThe null hypothesis, $H_0$, posits that the observed time series $\\{X_n\\}_{n \\ge 0}$ is stationary. For a stationary process, the marginal distribution of $X_n$ is independent of the time index $n$. Consequently, under $H_0$, all $X_n$ share the same characteristic function, which we can denote by $\\varphi(t)$.\n\nThe alternative hypothesis, $H_1$, is that the process is not stationary. Specifically, we test for a change in the distribution between the first and second halves of the observed data.\n\n**2. Test Statistic Formulation**\n\nGiven a time series of length $2k$, we partition it into two contiguous blocks of equal length $k$:\n$$\nB_1 = \\{X_1, X_2, \\dots, X_k\\}\n$$\n$$\nB_2 = \\{X_{k+1}, X_{k+2}, \\dots, X_{2k}\\}\n$$\n\nFor each block, we compute the empirical characteristic function (ECF), which is the sample analogue of the true characteristic function. For a set of observations $\\{x_j\\}_{j \\in A}$, the ECF is:\n$$\n\\hat{\\varphi}_A(t) = \\frac{1}{|A|} \\sum_{j \\in A} e^{itx_j}\n$$\nUnder $H_0$, both $\\hat{\\varphi}_{B_1}(t)$ and $\\hat{\\varphi}_{B_2}(t)$ are estimators of the same true characteristic function $\\varphi(t)$ and should be close in value. Under $H_1$, where the distributions in the two blocks differ, their ECFs are expected to diverge.\n\nWe quantify this divergence using a scaled difference process. The difference process is defined as $\\hat{\\varphi}_{B_1}(t) - \\hat{\\varphi}_{B_2}(t)$. To properly standardize this difference, we apply a scaling factor derived from the theory of two-sample mean tests. With sample sizes $n_1=k$ and $n_2=k$, the appropriate scaling factor is $\\sqrt{\\frac{n_1 n_2}{n_1+n_2}} = \\sqrt{\\frac{k^2}{2k}} = \\sqrt{k/2}$. The scaled difference process is thus:\n$$\nZ_k(t) = \\sqrt{\\frac{k}{2}} \\left( \\hat{\\varphi}_{B_1}(t) - \\hat{\\varphi}_{B_2}(t) \\right)\n$$\nThe overall test statistic, $T_k$, aggregates the evidence against $H_0$ by taking the maximum magnitude of this difference process over a pre-specified finite grid of frequencies $\\mathcal{T}$:\n$$\nT_k = \\max_{t \\in \\mathcal{T}} |Z_k(t)|\n$$\n\n**3. Significance Testing via Circular Moving Block Bootstrap**\n\nThe analytical distribution of $T_k$ under $H_0$ is generally intractable, especially for dependent data. We therefore resort to a bootstrap procedure to approximate this null distribution. To preserve the temporal dependence structure present in the data (as modeled by the AR($1$) process), the circular moving block bootstrap is employed.\n\nThe procedure is as follows:\n1.  **Compute Observed Statistic**: Calculate $T_k$ from the original series $X = (X_1, \\dots, X_{2k})$.\n2.  **Bootstrap Resampling**: Generate $B$ bootstrap time series, $X^\\ast_1, \\dots, X^\\ast_B$. Each series $X^\\ast_b$ is constructed by:\n    a.  Defining $2k$ overlapping blocks of length $l$ from the original series $X$, with wrap-around (circularity). For $j=1, \\dots, 2k$, the $j$-th block is $(X_j, X_{j+1}, \\dots, X_{j+l-1})$, where indices are taken modulo $2k$.\n    b.  Drawing $\\lceil 2k/l \\rceil$ of these blocks at random with replacement.\n    c.  Concatenating these drawn blocks and truncating the resulting series to length $2k$. This forms the bootstrap series $X^\\ast_b$.\n3.  **Compute Bootstrap Statistics**: For each bootstrap series $X^\\ast_b$, compute the corresponding test statistic $T_{k,b}^\\ast$ using the same method as for the original series.\n4.  **Calculate p-value**: The collection $\\{T_{k,1}^\\ast, \\dots, T_{k,B}^\\ast\\}$ forms an empirical approximation of the distribution of $T_k$ under $H_0$. The bootstrap p-value is the proportion of bootstrap statistics that are greater than or equal to the originally observed statistic:\n    $$\n    \\hat{p} = \\frac{1}{B} \\sum_{b=1}^{B} \\mathbf{1}\\{T_{k,b}^\\ast \\ge T_k\\}\n    $$\n5.  **Decision**: The null hypothesis $H_0$ is rejected at a significance level $\\alpha$ if $\\hat{p} \\le \\alpha$.\n\n**4. Simulation Model**\n\nThe data are generated from an autoregressive process of order one (AR($1$)):\n$$\nX_{n+1} = \\rho X_n + \\epsilon_{n+1} + \\delta \\cdot \\mathbf{1}\\{n+1  k\\}\n$$\nwhere $\\epsilon_{n+1}$ are independent and identically distributed innovations from a normal distribution $\\mathcal{N}(0, \\sigma^2)$. The parameter $\\rho$ controls the degree of persistence. The term $\\delta \\cdot \\mathbf{1}\\{n+1  k\\}$ introduces a deterministic mean shift for the second block of the series, directly inducing non-stationarity when $\\delta \\neq 0$.\n\nTo ensure the process is stationary under the null hypothesis (i.e., when $\\delta=0$ and $|\\rho|1$), the initial value $X_0$ is drawn from the process's stationary distribution, $\\mathcal{N}\\left(0, \\frac{\\sigma^2}{1-\\rho^2}\\right)$. For the i.i.d. case ($\\rho=0$), this simplifies to $\\mathcal{N}(0, \\sigma^2)$. In some test cases, $\\sigma$ is chosen as $\\sigma = \\sqrt{1-\\rho^2}$ to normalize the stationary variance to $1$.\n\nThis complete methodology allows for a robust, data-driven test for stationarity without making strong parametric assumptions about the underlying distributions, beyond the existence of moments required for the characteristic function.", "answer": "```python\nimport numpy as np\n\ndef simulate_ar1(k, rho, sigma, delta, seed):\n    \"\"\"\n    Simulates an AR(1) process of length 2k.\n    X_{n+1} = rho * X_n + epsilon_{n+1} + delta_shift\n    \"\"\"\n    np.random.seed(seed)\n    n_total = 2 * k\n    series = np.zeros(n_total + 1)\n    \n    # Initialize X_0 from the stationary distribution\n    if rho == 0:\n        var_x0 = sigma**2\n    else:\n        var_x0 = sigma**2 / (1 - rho**2)\n    \n    series[0] = np.random.normal(0, np.sqrt(var_x0))\n    \n    innovations = np.random.normal(0, sigma, n_total)\n    \n    for n in range(n_total):\n        mean_shift = delta if n + 1  k else 0.0\n        series[n + 1] = rho * series[n] + innovations[n] + mean_shift\n        \n    return series[1:] # Return (X_1, ..., X_{2k})\n\ndef compute_test_statistic(series, k, t_grid):\n    \"\"\"\n    Computes the test statistic T_k from a series of length 2k.\n    \"\"\"\n    block1 = series[:k]\n    block2 = series[k:]\n    \n    t_grid_arr = np.array(t_grid)\n    \n    # Empirical Characteristic Functions (ECFs)\n    # np.exp(1j * t * x) is computed for each t in t_grid and each x in the block.\n    # The result is a matrix of shape (len(t_grid), k).\n    # We then take the mean along the second axis (axis=1).\n    phi_hat_1 = np.mean(np.exp(1j * t_grid_arr[:, np.newaxis] * block1), axis=1)\n    phi_hat_2 = np.mean(np.exp(1j * t_grid_arr[:, np.newaxis] * block2), axis=1)\n    \n    # Scaled difference process Z_k(t)\n    # The scaling factor sqrt(k/2) comes from two-sample test theory.\n    zk_t = np.sqrt(k / 2.0) * (phi_hat_1 - phi_hat_2)\n    \n    # Test statistic T_k = max |Z_k(t)| over t in T_grid\n    tk = np.max(np.abs(zk_t))\n    \n    return tk\n\ndef run_test_case(k, rho, sigma_def, delta, t_grid, l, B, alpha, seed):\n    \"\"\"\n    Runs a single test case: simulates data, computes statistic, and performs bootstrap test.\n    \"\"\"\n    # Set the seed for all random operations in this test case\n    np.random.seed(seed)\n\n    # --- 1. Simulation ---\n    if sigma_def == 'sqrt(1-rho^2)':\n        sigma = np.sqrt(1 - rho**2)\n    else:\n        sigma = sigma_def\n        \n    series = simulate_ar1(k, rho, sigma, delta, seed)\n\n    # --- 2. Compute Observed Test Statistic ---\n    tk_obs = compute_test_statistic(series, k, t_grid)\n\n    # --- 3. Circular Moving Block Bootstrap ---\n    n_total = 2 * k\n    tk_stars = np.zeros(B)\n    \n    num_blocks_to_draw = int(np.ceil(n_total / l))\n\n    for i in range(B):\n        # Generate one bootstrap series\n        start_indices = np.random.randint(0, n_total, size=num_blocks_to_draw)\n        \n        bootstrap_series_list = []\n        for start_idx in start_indices:\n            # Circular indexing\n            indices = np.arange(start_idx, start_idx + l) % n_total\n            bootstrap_series_list.append(series[indices])\n            \n        bootstrap_series = np.concatenate(bootstrap_series_list)[:n_total]\n        \n        # Compute test statistic for the bootstrap series\n        tk_stars[i] = compute_test_statistic(bootstrap_series, k, t_grid)\n\n    # --- 4. Calculate p-value and Make Decision ---\n    # p-value is the proportion of bootstrap stats = observed stat\n    p_value = np.sum(tk_stars = tk_obs) / B\n    \n    # Reject H0 if p-value is less than or equal to alpha\n    return p_value = alpha\n\ndef solve():\n    \"\"\"\n    Main function to define and run all test cases.\n    \"\"\"\n    test_cases = [\n        # 1. Happy path (i.i.d. Gaussian, H0 is true)\n        {'k': 1000, 'rho': 0, 'sigma_def': 1.0, 'delta': 0, 't_grid': [-2, -1, 0.5, 1.5, 3], 'l': 20, 'B': 300, 'alpha': 0.05, 'seed': 42},\n        # 2. Slow mixing but stationary (H0 is true)\n        {'k': 1000, 'rho': 0.95, 'sigma_def': 'sqrt(1-rho^2)', 'delta': 0, 't_grid': [-2, -1, 0.5, 1.5, 3], 'l': 50, 'B': 300, 'alpha': 0.05, 'seed': 123},\n        # 3. Nonstationary mean shift (H0 is false)\n        {'k': 1000, 'rho': 0.7, 'sigma_def': 'sqrt(1-rho^2)', 'delta': 0.5, 't_grid': [-2, -1, 0.5, 1.5, 3], 'l': 30, 'B': 300, 'alpha': 0.05, 'seed': 7},\n        # 4. Boundary grid (degenerate case, H0 is technically true)\n        {'k': 1000, 'rho': 0.7, 'sigma_def': 'sqrt(1-rho^2)', 'delta': 0, 't_grid': [0.0], 'l': 30, 'B': 200, 'alpha': 0.05, 'seed': 99},\n        # 5. Small sample stationary (H0 is true)\n        {'k': 100, 'rho': 0.7, 'sigma_def': 'sqrt(1-rho^2)', 'delta': 0, 't_grid': [-1, 0.1, 0.5, 1.0, 2.0], 'l': 10, 'B': 200, 'alpha': 0.05, 'seed': 2024}\n    ]\n    \n    results = []\n    for case in test_cases:\n        # The main 'run_test_case' function encapsulates all logic per case.\n        # Note: 'seed' in the case dictionary will control BOTH simulation and bootstrap randomness.\n        # A single seed is given for the entire test case in the prompt.\n        # Re-seeding happens inside run_test_case implicitly via simulate_ar1 and directly for bootstrap.\n        # A better design would separate data-gen and bootstrap seeds, but we follow the prompt's implied structure.\n        result = run_test_case(**case)\n        results.append(result)\n        \n    # Format the final output exactly as required\n    print(f\"[{','.join(map(lambda b: str(b).capitalize(), results))}]\")\n\nsolve()\n```", "id": "3293826"}]}