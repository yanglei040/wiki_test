{"hands_on_practices": [{"introduction": "A common and powerful strategy in stochastic modeling is to assume that complex dependencies can be simplified by conditioning on a latent, or hidden, variable. This exercise explores a crucial consequence of this approach: even if two variables are independent given the state of a hidden factor, they can become correlated when we average over that factor. By working through a Gaussian mixture model, you will apply the law of total covariance to quantify this induced dependence, a fundamental skill for anyone working with hierarchical models in Bayesian statistics or machine learning [@problem_id:3300840].", "problem": "In a hierarchical stochastic simulation setting relevant to Monte Carlo (MC) methods, consider a latent variable $Z$ taking values in $\\{1,2,3\\}$ with $\\mathbb{P}(Z=1)=\\frac{1}{3}$, $\\mathbb{P}(Z=2)=\\frac{1}{2}$, and $\\mathbb{P}(Z=3)=\\frac{1}{6}$. Conditional on $Z=k$, the random variables $X$ and $Y$ are independent and distributed as $X \\mid Z=k \\sim \\mathcal{N}(a_k,\\sigma_{X,k}^{2})$ and $Y \\mid Z=k \\sim \\mathcal{N}(b_k,\\sigma_{Y,k}^{2})$, where the means are $a_1=0$, $a_2=2$, $a_3=-1$ and $b_1=1$, $b_2=-3$, $b_3=4$. The conditional variances are $\\sigma_{X,1}^{2}=1$, $\\sigma_{X,2}^{2}=9$, $\\sigma_{X,3}^{2}=4$ and $\\sigma_{Y,1}^{2}=4$, $\\sigma_{Y,2}^{2}=1$, $\\sigma_{Y,3}^{2}=16$. The construction ensures conditional independence of $X$ and $Y$ given $Z$ while allowing marginal dependence through the mixture structure.\n\nUsing only the core definitions of expectation, covariance, and conditional independence, and conditioning on $Z$ where appropriate, derive a closed-form expression for $\\mathrm{Cov}(X,Y)$ under this model. Express your final answer as an exact value; no rounding is required.", "solution": "To compute the covariance between $X$ and $Y$, $\\mathrm{Cov}(X,Y)$, we utilize the law of total covariance (also known as Eve's Law), which decomposes the covariance based on the conditioning variable $Z$. The formula is:\n$$\n\\mathrm{Cov}(X,Y) = \\mathbb{E}[\\mathrm{Cov}(X,Y \\mid Z)] + \\mathrm{Cov}(\\mathbb{E}[X \\mid Z], \\mathbb{E}[Y \\mid Z])\n$$\nWe shall evaluate each of the two terms on the right-hand side separately.\n\nFirst, consider the term $\\mathbb{E}[\\mathrm{Cov}(X,Y \\mid Z)]$. The problem statement specifies that $X$ and $Y$ are conditionally independent given $Z=k$. The covariance of two independent random variables is zero. Therefore, for each possible value $k$ of $Z$, we have $\\mathrm{Cov}(X,Y \\mid Z=k) = 0$. Since the conditional covariance is zero for every possible outcome of $Z$, its expectation is also zero:\n$$\n\\mathbb{E}[\\mathrm{Cov}(X,Y \\mid Z)] = \\mathbb{E}[0] = 0\n$$\n\nNext, we evaluate the second term, $\\mathrm{Cov}(\\mathbb{E}[X \\mid Z], \\mathbb{E}[Y \\mid Z])$. Let us define two new random variables, $U = \\mathbb{E}[X \\mid Z]$ and $V = \\mathbb{E}[Y \\mid Z]$. We need to compute $\\mathrm{Cov}(U,V) = \\mathbb{E}[UV] - \\mathbb{E}[U]\\mathbb{E}[V]$. The random variable $U$ takes the value $a_k = \\mathbb{E}[X \\mid Z=k]$ when $Z=k$, and $V$ takes the value $b_k = \\mathbb{E}[Y \\mid Z=k]$ when $Z=k$.\n\nWe first compute $\\mathbb{E}[U]$ and $\\mathbb{E}[V]$ using the law of total expectation:\n$$\n\\mathbb{E}[U] = \\mathbb{E}[\\mathbb{E}[X \\mid Z]] = \\mathbb{E}[X] = \\sum_{k=1}^{3} a_k \\mathbb{P}(Z=k)\n$$\nSubstituting the given values:\n$$\n\\mathbb{E}[U] = (0)\\left(\\frac{1}{3}\\right) + (2)\\left(\\frac{1}{2}\\right) + (-1)\\left(\\frac{1}{6}\\right) = 0 + 1 - \\frac{1}{6} = \\frac{5}{6}\n$$\nSimilarly, for $V$:\n$$\n\\mathbb{E}[V] = \\mathbb{E}[Y] = \\sum_{k=1}^{3} b_k \\mathbb{P}(Z=k)\n$$\nSubstituting the given values:\n$$\n\\mathbb{E}[V] = (1)\\left(\\frac{1}{3}\\right) + (-3)\\left(\\frac{1}{2}\\right) + (4)\\left(\\frac{1}{6}\\right) = \\frac{1}{3} - \\frac{3}{2} + \\frac{4}{6} = \\frac{2}{6} - \\frac{9}{6} + \\frac{4}{6} = -\\frac{3}{6} = -\\frac{1}{2}\n$$\nNow, we compute $\\mathbb{E}[UV]$. The random variable $UV$ takes the value $a_k b_k$ when $Z=k$.\n$$\n\\mathbb{E}[UV] = \\sum_{k=1}^{3} (a_k b_k) \\mathbb{P}(Z=k)\n$$\nSubstituting the given values:\n$$\n\\mathbb{E}[UV] = (0 \\cdot 1)\\left(\\frac{1}{3}\\right) + (2 \\cdot -3)\\left(\\frac{1}{2}\\right) + (-1 \\cdot 4)\\left(\\frac{1}{6}\\right)\n$$\n$$\n\\mathbb{E}[UV] = 0 + (-6)\\left(\\frac{1}{2}\\right) + (-4)\\left(\\frac{1}{6}\\right) = -3 - \\frac{4}{6} = -3 - \\frac{2}{3} = -\\frac{11}{3}\n$$\nNow we can compute $\\mathrm{Cov}(U,V)$:\n$$\n\\mathrm{Cov}(U,V) = \\mathbb{E}[UV] - \\mathbb{E}[U]\\mathbb{E}[V] = -\\frac{11}{3} - \\left(\\frac{5}{6}\\right)\\left(-\\frac{1}{2}\\right)\n$$\n$$\n\\mathrm{Cov}(U,V) = -\\frac{11}{3} + \\frac{5}{12} = -\\frac{44}{12} + \\frac{5}{12} = -\\frac{39}{12} = -\\frac{13}{4}\n$$\nThis is the value of the second term, $\\mathrm{Cov}(\\mathbb{E}[X \\mid Z], \\mathbb{E}[Y \\mid Z])$.\n\nFinally, we combine the two terms:\n$$\n\\mathrm{Cov}(X,Y) = \\mathbb{E}[\\mathrm{Cov}(X,Y \\mid Z)] + \\mathrm{Cov}(\\mathbb{E}[X \\mid Z], \\mathbb{E}[Y \\mid Z]) = 0 + \\left(-\\frac{13}{4}\\right)\n$$\n$$\n\\mathrm{Cov}(X,Y) = -\\frac{13}{4}\n$$\nThe covariance is non-zero, indicating that $X$ and $Y$ are not independent, even though they are conditionally independent given $Z$. Their dependence is mediated entirely by the latent variable $Z$. The specified conditional variances are not needed for this calculation.", "answer": "$$\\boxed{-\\frac{13}{4}}$$", "id": "3300840"}, {"introduction": "Antithetic variates represent a classic variance reduction technique in Monte Carlo simulation, designed to induce negative correlation between estimator pairs. This practice moves beyond simple textbook cases by examining an integrand that is not purely monotone. You will derive how the correlation between an antithetic pair can flip from negative to positive as a non-monotone component becomes dominant, providing a deeper insight into the mechanics and limitations of this powerful method [@problem_id:3300839].", "problem": "Consider Monte Carlo (MC) estimation of the integral $I = \\mathbb{E}[g(U)]$ where $U \\sim \\mathrm{Uniform}(0,1)$ and $g$ is an integrand modeled as a monotone baseline plus a non-monotone perturbation. Use the antithetic variates technique, which forms pairs $(U,1-U)$ to construct correlated estimates of $g(U)$ and $g(1-U)$ designed for variance reduction. Define the antithetic pair\n$$\nX = g(U), \\qquad X' = g(1-U),\n$$\nwith the parametric integrand\n$$\ng(u) = u + a \\cos(2\\pi u),\n$$\nwhere $a \\in \\mathbb{R}$ controls the amplitude of a symmetric, non-monotone oscillation about the monotone baseline $u$.\n\nStarting from the fundamental definitions of covariance and correlation,\n$$\n\\mathrm{Cov}(X,Y) = \\mathbb{E}\\big[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])\\big], \\qquad \\mathrm{Corr}(X,Y) = \\frac{\\mathrm{Cov}(X,Y)}{\\sqrt{\\mathrm{Var}(X)\\,\\mathrm{Var}(Y)}},\n$$\nderive the correlation $\\mathrm{Corr}(X,X')$ as a function of $a$ and determine the precise condition under which the sign of $\\mathrm{Corr}(X,X')$ flips from negative to positive due to the non-monotonicity induced by the perturbation $\\cos(2\\pi u)$. Provide the closed-form analytic expression for the critical amplitude $a_{\\star}$ at which $\\mathrm{Corr}(X,X') = 0$.\n\nExpress your final answer as an exact symbolic expression. No rounding is required. No units are required.", "solution": "We begin with the antithetic construction: for $U \\sim \\mathrm{Uniform}(0,1)$, define\n$$\nX = g(U) = U + a \\cos(2\\pi U), \\qquad X' = g(1-U) = (1-U) + a \\cos\\big(2\\pi(1-U)\\big).\n$$\nUsing the trigonometric identity $\\cos(2\\pi(1-U)) = \\cos(2\\pi U)$, we have\n$$\nX' = (1-U) + a \\cos(2\\pi U).\n$$\nTo find the correlation, we first need to compute the covariance $\\mathrm{Cov}(X,X')$ and the variances $\\mathrm{Var}(X)$ and $\\mathrm{Var}(X')$. We use the bilinearity of covariance and the following expectations for $U \\sim \\mathrm{Uniform}(0,1)$:\n- $\\mathbb{E}[U] = \\frac{1}{2}$ and $\\mathrm{Var}(U) = \\frac{1}{12}$.\n- $\\mathbb{E}[\\cos(2\\pi U)] = \\int_{0}^{1} \\cos(2\\pi u)\\,\\mathrm{d}u = 0$.\n- $\\mathbb{E}[\\cos^{2}(2\\pi U)] = \\int_{0}^{1} \\cos^{2}(2\\pi u)\\,\\mathrm{d}u = \\frac{1}{2}$, so $\\mathrm{Var}(\\cos(2\\pi U)) = \\mathbb{E}[\\cos^{2}(2\\pi U)] - (\\mathbb{E}[\\cos(2\\pi U)])^2 = \\frac{1}{2}$.\n- $\\mathrm{Cov}(U,\\cos(2\\pi U)) = \\mathbb{E}[U \\cos(2\\pi U)] - \\mathbb{E}[U]\\mathbb{E}[\\cos(2\\pi U)] = \\int_{0}^{1} u \\cos(2\\pi u)\\,\\mathrm{d}u - 0$. Integration by parts shows this integral is zero, so $\\mathrm{Cov}(U,\\cos(2\\pi U)) = 0$.\n\nNow, we compute $\\mathrm{Cov}(X,X')$:\n$$\n\\begin{aligned}\n\\mathrm{Cov}(X,X') = \\mathrm{Cov}\\big(U + a \\cos(2\\pi U),\\, 1 - U + a \\cos(2\\pi U)\\big) \\\\\n= \\mathrm{Cov}(U, -U) + \\mathrm{Cov}(U, a \\cos(2\\pi U)) + \\mathrm{Cov}(a \\cos(2\\pi U), -U) + \\mathrm{Cov}(a \\cos(2\\pi U), a \\cos(2\\pi U)) \\\\\n= -\\mathrm{Var}(U) - 2a\\,\\mathrm{Cov}(U,\\cos(2\\pi U)) + a^{2}\\,\\mathrm{Var}(\\cos(2\\pi U)) \\\\\n= -\\frac{1}{12} - 2a(0) + a^{2}\\left(\\frac{1}{2}\\right) = \\frac{a^{2}}{2} - \\frac{1}{12}.\n\\end{aligned}\n$$\n\nNext, we compute $\\mathrm{Var}(X)$. By symmetry, $\\mathrm{Var}(X) = \\mathrm{Var}(X')$.\n$$\n\\begin{aligned}\n\\mathrm{Var}(X) = \\mathrm{Var}\\big(U + a \\cos(2\\pi U)\\big) \\\\\n= \\mathrm{Var}(U) + a^{2}\\mathrm{Var}(\\cos(2\\pi U)) + 2a\\,\\mathrm{Cov}(U,\\cos(2\\pi U)) \\\\\n= \\frac{1}{12} + a^{2}\\cdot\\frac{1}{2} + 2a(0) = \\frac{1}{12} + \\frac{a^2}{2}.\n\\end{aligned}\n$$\nSince $\\mathrm{Var}(X) = \\mathrm{Var}(X')$, the denominator of the correlation is just $\\mathrm{Var}(X)$.\n\nTherefore, the correlation is\n$$\n\\mathrm{Corr}(X,X') = \\frac{\\mathrm{Cov}(X,X')}{\\sqrt{\\mathrm{Var}(X)\\mathrm{Var}(X')}} = \\frac{\\mathrm{Cov}(X,X')}{\\mathrm{Var}(X)} = \\frac{\\frac{a^{2}}{2} - \\frac{1}{12}}{\\frac{a^{2}}{2} + \\frac{1}{12}}.\n$$\n\nThe denominator is strictly positive for all real $a$. The sign of $\\mathrm{Corr}(X,X')$ is thus determined by the numerator. The correlation flips sign exactly when the numerator is zero:\n$$\n\\frac{a^{2}}{2} - \\frac{1}{12} = 0 \\quad \\Longleftrightarrow \\quad a^{2} = \\frac{2}{12} = \\frac{1}{6}.\n$$\nThus, the critical amplitude is\n$$\na_{\\star} = \\sqrt{\\frac{1}{6}}.\n$$\nFor $|a|  a_{\\star}$, the negative correlation from the monotone baseline dominates ($\\mathrm{Corr}(X,X')  0$). For $|a| > a_{\\star}$, the positive correlation from the shared non-monotone oscillation dominates ($\\mathrm{Corr}(X,X') > 0$).", "answer": "$$\\boxed{\\sqrt{\\frac{1}{6}}}$$", "id": "3300839"}, {"introduction": "In the modern era of data science, statistical estimation is often constrained by the need for privacy. This exercise bridges the gap between probability theory and practical data protection by analyzing the impact of adding Gaussian noise to an estimatorâ€”a common technique for achieving differential privacy. By deriving the new covariance matrix and the resulting inflation of confidence intervals, you will quantify the fundamental trade-off between privacy and statistical utility, a critical consideration in responsible data analysis [@problem_id:3300764].", "problem": "A data analyst uses a Monte Carlo estimator to approximate the mean of a $d$-dimensional random vector. Let $X_{1},\\dots,X_{n}$ be independent and identically distributed $d$-dimensional random vectors with $\\mathbb{E}[X_{i}]=\\mu\\in\\mathbb{R}^{d}$ and $\\mathrm{Cov}(X_{i})=\\Sigma\\in\\mathbb{R}^{d\\times d}$ with finite second moments. The analyst reports the sample mean $\\hat{\\mu}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$, and to enforce differential privacy (DP), publishes $\\tilde{\\mu}=\\hat{\\mu}+\\epsilon$, where $\\epsilon\\sim\\mathcal{N}(0,\\sigma^{2}I_{d})$ is independent of $\\hat{\\mu}$ and $I_{d}$ is the $d\\times d$ identity matrix. Starting only from the definition of covariance, the linearity of expectation, and the independence of $\\hat{\\mu}$ and $\\epsilon$, derive an explicit expression for $\\mathrm{Cov}(\\tilde{\\mu})$ in terms of $\\Sigma$, $n$, and $\\sigma^{2}$. Then, using the Multivariate Central Limit Theorem and the Gaussian approximation to the marginal sampling distributions of coordinates, consider any fixed coordinate index $j\\in\\{1,\\dots,d\\}$ with variance $v_{j}$ given by the $j$th diagonal entry of $\\Sigma$. Define the usual two-sided $(1-\\alpha)$ confidence interval for $\\mu_{j}$ based on a normal approximation both before and after adding DP noise, and obtain the multiplicative inflation factor $\\kappa_{j}$ by which the marginal confidence interval half-width for coordinate $j$ increases due to the added DP noise.\n\nProvide your final answer as a single closed-form expression for $\\kappa_{j}$ in terms of $n$, $\\sigma^{2}$, and $v_{j}$. No numerical rounding is required and no units are involved.", "solution": "### Part 1: Derivation of $\\mathrm{Cov}(\\tilde{\\mu})$\n\nFirst, we derive the covariance of the privatized estimator $\\tilde{\\mu} = \\hat{\\mu} + \\epsilon$.\nWe begin by finding the expectation of $\\tilde{\\mu}$ using the linearity of expectation:\n$$\n\\mathbb{E}[\\tilde{\\mu}] = \\mathbb{E}[\\hat{\\mu} + \\epsilon] = \\mathbb{E}[\\hat{\\mu}] + \\mathbb{E}[\\epsilon]\n$$\nThe expectation of the sample mean $\\hat{\\mu}$ is $\\mathbb{E}[\\hat{\\mu}] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right] = \\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}[X_{i}] = \\mu$. The noise vector has mean zero, $\\mathbb{E}[\\epsilon] = 0$. Thus, $\\mathbb{E}[\\tilde{\\mu}] = \\mu$, making $\\tilde{\\mu}$ an unbiased estimator.\n\nSince $\\hat{\\mu}$ and $\\epsilon$ are independent, the covariance of their sum is the sum of their covariances:\n$$\n\\mathrm{Cov}(\\tilde{\\mu}) = \\mathrm{Cov}(\\hat{\\mu} + \\epsilon) = \\mathrm{Cov}(\\hat{\\mu}) + \\mathrm{Cov}(\\epsilon)\n$$\nThe covariance of the sample mean $\\hat{\\mu}$, for i.i.d. samples $X_i$, is:\n$$\n\\mathrm{Cov}(\\hat{\\mu}) = \\mathrm{Cov}\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right) = \\frac{1}{n^2}\\mathrm{Cov}\\left(\\sum_{i=1}^{n}X_{i}\\right) = \\frac{1}{n^2}\\sum_{i=1}^{n}\\mathrm{Cov}(X_i) = \\frac{1}{n^2}(n\\Sigma) = \\frac{1}{n}\\Sigma\n$$\nThe covariance of the noise is given by its distribution, $\\mathrm{Cov}(\\epsilon) = \\sigma^2 I_d$.\nSubstituting these results yields the covariance of the private estimator:\n$$\n\\mathrm{Cov}(\\tilde{\\mu}) = \\frac{1}{n}\\Sigma + \\sigma^2 I_d\n$$\n\n### Part 2: Derivation of the Inflation Factor $\\kappa_j$\n\nNext, we find the multiplicative inflation factor $\\kappa_j$ for the confidence interval half-width of the $j$-th coordinate, $\\mu_j$. The half-width of a $(1-\\alpha)$ confidence interval is proportional to the square root of the estimator's variance. We compare the half-widths derived from $\\hat{\\mu}_j$ (before noise) and $\\tilde{\\mu}_j$ (after noise).\n\n**Confidence Interval without DP noise:**\nThe estimator is $\\hat{\\mu}_j$. Its variance is the $j$-th diagonal element of $\\mathrm{Cov}(\\hat{\\mu})$:\n$$\n\\mathrm{Var}(\\hat{\\mu}_j) = \\left(\\mathrm{Cov}(\\hat{\\mu})\\right)_{jj} = \\left(\\frac{1}{n}\\Sigma\\right)_{jj} = \\frac{1}{n}\\Sigma_{jj} = \\frac{v_j}{n}\n$$\nThe half-width of the confidence interval is $HW_j = z_{1-\\alpha/2}\\sqrt{\\frac{v_j}{n}}$, where $z_{1-\\alpha/2}$ is the standard normal critical value.\n\n**Confidence Interval with DP noise:**\nThe estimator is $\\tilde{\\mu}_j$. Its variance is the $j$-th diagonal element of $\\mathrm{Cov}(\\tilde{\\mu})$:\n$$\n\\mathrm{Var}(\\tilde{\\mu}_j) = (\\mathrm{Cov}(\\tilde{\\mu}))_{jj} = \\left(\\frac{1}{n}\\Sigma + \\sigma^2 I_d\\right)_{jj} = \\frac{v_j}{n} + \\sigma^2\n$$\nThe half-width of the new confidence interval is $\\tilde{HW}_j = z_{1-\\alpha/2}\\sqrt{\\frac{v_j}{n} + \\sigma^2}$.\n\n**Inflation Factor:**\nThe multiplicative inflation factor $\\kappa_j$ is the ratio of the new half-width to the original one:\n$$\n\\kappa_j = \\frac{\\tilde{HW}_j}{HW_j} = \\frac{z_{1-\\alpha/2}\\sqrt{\\frac{v_j}{n} + \\sigma^2}}{z_{1-\\alpha/2}\\sqrt{\\frac{v_j}{n}}} = \\frac{\\sqrt{\\frac{v_j}{n} + \\sigma^2}}{\\sqrt{\\frac{v_j}{n}}}\n$$\nSimplifying the expression gives the final result:\n$$\n\\kappa_j = \\sqrt{\\frac{\\frac{v_j}{n} + \\sigma^2}{\\frac{v_j}{n}}} = \\sqrt{1 + \\frac{\\sigma^2}{v_j/n}} = \\sqrt{1 + \\frac{n\\sigma^2}{v_j}}\n$$\nThis expression quantifies how much wider the confidence interval becomes due to the addition of privacy-preserving noise.", "answer": "$$\\boxed{\\sqrt{1 + \\frac{n\\sigma^2}{v_j}}}$$", "id": "3300764"}]}