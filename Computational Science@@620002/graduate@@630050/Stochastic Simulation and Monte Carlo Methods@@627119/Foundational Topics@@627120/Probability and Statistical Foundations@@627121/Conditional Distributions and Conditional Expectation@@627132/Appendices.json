{"hands_on_practices": [{"introduction": "The multivariate normal distribution is a cornerstone of statistical modeling. In modern computational statistics, algorithms like the Gibbs sampler rely on sequentially drawing from the full conditional distribution of each variable given the others. This exercise [@problem_id:3297685] provides essential practice by guiding you through the first-principles derivation of the conditional distribution for a bivariate normal, reinforcing your understanding of both Gaussian properties and the mechanics of conditioning.", "problem": "Consider a jointly Gaussian pair $\\,(X,Y)\\,$ with mean vector $\\;(\\mu_{X},\\mu_{Y})\\,$ and covariance matrix\n$$\n\\Sigma \\;=\\; \\begin{pmatrix}\n\\sigma_{X}^{2} & \\rho\\,\\sigma_{X}\\sigma_{Y} \\\\\n\\rho\\,\\sigma_{X}\\sigma_{Y} & \\sigma_{Y}^{2}\n\\end{pmatrix},\n$$\nwhere $\\;\\sigma_{X}>0\\,$, $\\;\\sigma_{Y}>0\\,$, and $\\;|\\rho|<1\\,$. In the context of a Gibbs step within a Markov Chain Monte Carlo (MCMC) algorithm for simulating from a multivariate normal distribution, one needs the conditional distribution of one component given the other. Starting only from the definition of the joint Gaussian density and the definition of a conditional density via the ratio $\\;f_{X|Y}(x|y)\\;=\\;f_{X,Y}(x,y)\\big/f_{Y}(y)\\,$, and using standard linear algebra identities for inverting a covariance matrix and completing the square, derive the conditional density $\\;f_{X|Y}(x\\,|\\,y)\\,$ for fixed $\\,y\\,$. Then deduce the conditional expectation $\\;E[X\\,|\\,Y=y]\\,$.\n\nYour reasoning must start from core definitions and well-tested facts and proceed by explicit algebraic derivations, without invoking any shortcut formulas for conditional normals. The final answer must be a single closed-form symbolic expression for $\\;E[X\\,|\\,Y=y]\\,$. Do not provide any numerical approximations.", "solution": "The problem is valid as it is scientifically grounded in probability theory, well-posed with all necessary information provided, and objective in its formulation. The task is a standard derivation in multivariate statistics.\n\nLet the random vector $\\mathbf{Z} = \\begin{pmatrix} X \\\\ Y \\end{pmatrix}$ follow a bivariate normal distribution with mean vector $\\mathbf{\\mu} = \\begin{pmatrix} \\mu_X \\\\ \\mu_Y \\end{pmatrix}$ and covariance matrix $\\Sigma = \\begin{pmatrix} \\sigma_X^2 & \\rho \\sigma_X \\sigma_Y \\\\ \\rho \\sigma_X \\sigma_Y & \\sigma_Y^2 \\end{pmatrix}$. The joint probability density function (PDF) is given by\n$$ f_{\\mathbf{Z}}(\\mathbf{z}) = \\frac{1}{\\sqrt{(2\\pi)^2 \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2} (\\mathbf{z}-\\mathbf{\\mu})^T \\Sigma^{-1} (\\mathbf{z}-\\mathbf{\\mu})\\right) $$\nwhere $\\mathbf{z} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$.\n\nFirst, we compute the determinant and inverse of the covariance matrix $\\Sigma$. The determinant is\n$$ \\det(\\Sigma) = (\\sigma_X^2)(\\sigma_Y^2) - (\\rho \\sigma_X \\sigma_Y)^2 = \\sigma_X^2 \\sigma_Y^2 (1-\\rho^2) $$\nThe condition $|\\rho| < 1$ ensures that $\\det(\\Sigma) > 0$ and the matrix is invertible. The inverse is\n$$ \\Sigma^{-1} = \\frac{1}{\\det(\\Sigma)} \\begin{pmatrix} \\sigma_Y^2 & -\\rho \\sigma_X \\sigma_Y \\\\ -\\rho \\sigma_X \\sigma_Y & \\sigma_X^2 \\end{pmatrix} = \\frac{1}{\\sigma_X^2 \\sigma_Y^2 (1-\\rho^2)} \\begin{pmatrix} \\sigma_Y^2 & -\\rho \\sigma_X \\sigma_Y \\\\ -\\rho \\sigma_X \\sigma_Y & \\sigma_X^2 \\end{pmatrix} = \\frac{1}{1-\\rho^2} \\begin{pmatrix} 1/\\sigma_X^2 & -\\rho/(\\sigma_X \\sigma_Y) \\\\ -\\rho/(\\sigma_X \\sigma_Y) & 1/\\sigma_Y^2 \\end{pmatrix} $$\nThe quadratic form in the exponent is $Q(x,y) = (\\mathbf{z}-\\mathbf{\\mu})^T \\Sigma^{-1} (\\mathbf{z}-\\mathbf{\\mu})$, which expands to:\n$$ Q(x,y) = \\begin{pmatrix} x-\\mu_X & y-\\mu_Y \\end{pmatrix} \\frac{1}{1-\\rho^2} \\begin{pmatrix} 1/\\sigma_X^2 & -\\rho/(\\sigma_X \\sigma_Y) \\\\ -\\rho/(\\sigma_X \\sigma_Y) & 1/\\sigma_Y^2 \\end{pmatrix} \\begin{pmatrix} x-\\mu_X \\\\ y-\\mu_Y \\end{pmatrix} $$\n$$ Q(x,y) = \\frac{1}{1-\\rho^2} \\left[ \\frac{(x-\\mu_X)^2}{\\sigma_X^2} - \\frac{2\\rho(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X\\sigma_Y} + \\frac{(y-\\mu_Y)^2}{\\sigma_Y^2} \\right] $$\nSo, the joint PDF $f_{X,Y}(x,y)$ is:\n$$ f_{X,Y}(x,y) = \\frac{1}{2\\pi \\sigma_X \\sigma_Y \\sqrt{1-\\rho^2}} \\exp\\left(-\\frac{1}{2(1-\\rho^2)} \\left[ \\frac{(x-\\mu_X)^2}{\\sigma_X^2} - \\frac{2\\rho(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X\\sigma_Y} + \\frac{(y-\\mu_Y)^2}{\\sigma_Y^2} \\right]\\right) $$\nThe marginal distribution of $Y$ is a normal distribution with mean $\\mu_Y$ and variance $\\sigma_Y^2$. Its PDF is:\n$$ f_Y(y) = \\frac{1}{\\sqrt{2\\pi \\sigma_Y^2}} \\exp\\left(-\\frac{(y-\\mu_Y)^2}{2\\sigma_Y^2}\\right) = \\frac{1}{\\sigma_Y \\sqrt{2\\pi}} \\exp\\left(-\\frac{(y-\\mu_Y)^2}{2\\sigma_Y^2}\\right) $$\nBy definition, the conditional density $f_{X|Y}(x|y)$ is the ratio of the joint density to the marginal density:\n$$ f_{X|Y}(x|y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)} $$\nThe constant part of the conditional PDF is:\n$$ \\frac{1 / (2\\pi \\sigma_X \\sigma_Y \\sqrt{1-\\rho^2})}{1 / (\\sigma_Y \\sqrt{2\\pi})} = \\frac{\\sigma_Y \\sqrt{2\\pi}}{2\\pi \\sigma_X \\sigma_Y \\sqrt{1-\\rho^2}} = \\frac{1}{\\sigma_X \\sqrt{2\\pi} \\sqrt{1-\\rho^2}} $$\nThe exponent of the conditional PDF is the difference between the exponent of the joint PDF and the exponent of the marginal PDF:\n$$ \\text{Exp}_{X|Y} = -\\frac{1}{2(1-\\rho^2)} \\left[ \\frac{(x-\\mu_X)^2}{\\sigma_X^2} - \\frac{2\\rho(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X\\sigma_Y} + \\frac{(y-\\mu_Y)^2}{\\sigma_Y^2} \\right] - \\left(-\\frac{(y-\\mu_Y)^2}{2\\sigma_Y^2}\\right) $$\n$$ \\text{Exp}_{X|Y} = -\\frac{1}{2} \\left\\{ \\frac{1}{1-\\rho^2} \\left[ \\frac{(x-\\mu_X)^2}{\\sigma_X^2} - \\frac{2\\rho(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X\\sigma_Y} + \\frac{(y-\\mu_Y)^2}{\\sigma_Y^2} \\right] - \\frac{(y-\\mu_Y)^2}{\\sigma_Y^2} \\right\\} $$\nWe combine the terms involving $(y-\\mu_Y)^2$:\n$$ \\text{Exp}_{X|Y} = -\\frac{1}{2} \\left\\{ \\frac{1}{1-\\rho^2}\\left[ \\frac{(x-\\mu_X)^2}{\\sigma_X^2} - \\frac{2\\rho(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X\\sigma_Y} \\right] + \\left(\\frac{1}{1-\\rho^2} - 1\\right)\\frac{(y-\\mu_Y)^2}{\\sigma_Y^2} \\right\\} $$\nSince $\\frac{1}{1-\\rho^2} - 1 = \\frac{1 - (1-\\rho^2)}{1-\\rho^2} = \\frac{\\rho^2}{1-\\rho^2}$, the expression becomes:\n$$ \\text{Exp}_{X|Y} = -\\frac{1}{2} \\left\\{ \\frac{1}{1-\\rho^2}\\left[ \\frac{(x-\\mu_X)^2}{\\sigma_X^2} - \\frac{2\\rho(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X\\sigma_Y} + \\rho^2\\frac{(y-\\mu_Y)^2}{\\sigma_Y^2} \\right] \\right\\} $$\n$$ \\text{Exp}_{X|Y} = -\\frac{1}{2(1-\\rho^2)} \\left[ \\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)^2 - 2\\rho\\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)\\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right) + \\rho^2\\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right)^2 \\right] $$\nThe term in the square brackets is a perfect square. We complete the square for the variable $x$:\n$$ \\text{Exp}_{X|Y} = -\\frac{1}{2(1-\\rho^2)} \\left( \\frac{x-\\mu_X}{\\sigma_X} - \\rho\\frac{y-\\mu_Y}{\\sigma_Y} \\right)^2 $$\nTo identify the mean and variance, we rearrange this expression into the standard form $-\\frac{(x-\\mu)^2}{2\\sigma^2}$:\n$$ \\text{Exp}_{X|Y} = -\\frac{1}{2(1-\\rho^2)\\sigma_X^2} \\left( (x-\\mu_X) - \\rho\\frac{\\sigma_X}{\\sigma_Y}(y-\\mu_Y) \\right)^2 $$\n$$ \\text{Exp}_{X|Y} = -\\frac{\\left( x - \\left[\\mu_X + \\rho\\frac{\\sigma_X}{\\sigma_Y}(y-\\mu_Y)\\right] \\right)^2}{2\\sigma_X^2(1-\\rho^2)} $$\nThe conditional density $f_{X|Y}(x|y)$ is therefore:\n$$ f_{X|Y}(x|y) = \\frac{1}{\\sigma_X \\sqrt{2\\pi(1-\\rho^2)}} \\exp\\left( -\\frac{\\left( x - \\left[\\mu_X + \\rho\\frac{\\sigma_X}{\\sigma_Y}(y-\\mu_Y)\\right] \\right)^2}{2\\sigma_X^2(1-\\rho^2)} \\right) $$\nThis is the PDF of a normal distribution with respect to $x$. By comparing this to the general form of a normal PDF, $f(z) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(z-\\mu)^2}{2\\sigma^2}\\right)$, we can identify the conditional mean and conditional variance.\nThe conditional mean is the expectation of $X$ given $Y=y$:\n$$ E[X|Y=y] = \\mu_{X|Y} = \\mu_X + \\rho\\frac{\\sigma_X}{\\sigma_Y}(y-\\mu_Y) $$\nThe conditional variance is:\n$$ \\text{Var}(X|Y=y) = \\sigma_{X|Y}^2 = \\sigma_X^2(1-\\rho^2) $$\nThe problem asks specifically for the conditional expectation $E[X|Y=y]$. Based on the derivation, this is the mean of the conditional normal distribution derived above.", "answer": "$$ \\boxed{\\mu_X + \\rho \\frac{\\sigma_X}{\\sigma_Y}(y - \\mu_Y)} $$", "id": "3297685"}, {"introduction": "Beyond calculations with densities, conditional expectation is a powerful abstract operator with fundamental properties. It can be viewed as a projection that isolates information contained in a given sigma-algebra, effectively averaging out any independent components. This practice problem [@problem_id:3297665] provides a clear illustration of this principle, showing how conditioning on a random variable $Y$ removes the influence of an independent noise term, a concept central to filtering theory and variance reduction.", "problem": "Consider a pair of random variables $(X,Y)$ defined on a common probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$ as follows. Let $Y \\sim \\mathrm{Uniform}(0,2\\pi)$, let $\\varepsilon \\sim \\mathcal{N}(0,\\tau^{2})$ for a fixed constant $\\tau > 0$, and assume that $\\varepsilon$ is independent of $Y$. Define $X$ by $X = \\sin(Y) + \\varepsilon$. In the context of conditional Monte Carlo methods, one replaces $X$ by its conditional expectation given the sigma-algebra generated by $Y$, denoted $\\sigma(Y)$, to reduce variance while preserving the mean. Using only fundamental definitions of conditional expectation with respect to a sigma-algebra, the property of independence, and well-tested facts about conditional distributions, compute the conditional expectation $E[X \\mid \\sigma(Y)]$ and express your final answer as a single closed-form analytic expression. No rounding is required and no units are involved. Provide a derivation that justifies the measurability and the defining property of the conditional expectation you obtain.", "solution": "The objective is to compute the conditional expectation $E[X \\mid \\sigma(Y)]$. The conditional expectation is defined as the unique (up to almost sure equality) random variable, let us call it $Z$, that satisfies two fundamental properties:\n1.  **Measurability:** $Z$ must be measurable with respect to the sigma-algebra $\\sigma(Y)$. This implies that $Z$ can be expressed as a Borel-measurable function of $Y$.\n2.  **Defining Property:** For any set $A$ in the sigma-algebra $\\sigma(Y)$, the following integral equality must hold: $\\int_A X \\, d\\mathbb{P} = \\int_A Z \\, d\\mathbb{P}$.\n\nWe begin by substituting the definition of $X$ into the conditional expectation. Using the linearity property of conditional expectation, which is a direct consequence of the linearity of the integral in its defining property, we have:\n$$\nE[X \\mid \\sigma(Y)] = E[\\sin(Y) + \\varepsilon \\mid \\sigma(Y)] = E[\\sin(Y) \\mid \\sigma(Y)] + E[\\varepsilon \\mid \\sigma(Y)]\n$$\nWe will now analyze each of the two terms on the right-hand side separately.\n\n**Analysis of the first term: $E[\\sin(Y) \\mid \\sigma(Y)]$**\nLet $W = \\sin(Y)$. The random variable $Y$ is, by definition, measurable with respect to the sigma-algebra it generates, $\\sigma(Y)$. The sine function, $g(y) = \\sin(y)$, is a continuous function from $\\mathbb{R}$ to $\\mathbb{R}$ and is therefore a Borel-measurable function. A standard result from measure theory states that if a random variable $Y$ is $\\mathcal{G}$-measurable and $g$ is a Borel-measurable function, then the random variable $g(Y)$ is also $\\mathcal{G}$-measurable. In our case, this means that $W = \\sin(Y)$ is a $\\sigma(Y)$-measurable random variable.\n\nA fundamental property of conditional expectation states that if a random variable is measurable with respect to the conditioning sigma-algebra, then its conditional expectation is the random variable itself (almost surely). That is, if $W$ is $\\sigma(Y)$-measurable, then $E[W \\mid \\sigma(Y)] = W$.\n\nTo be fully rigorous as requested, we verify this property for $W = \\sin(Y)$ using the two defining conditions. We propose that $Z_1 = \\sin(Y)$ is the conditional expectation $E[\\sin(Y) \\mid \\sigma(Y)]$.\n1.  **Measurability:** As we have already argued, the random variable $\\sin(Y)$ is $\\sigma(Y)$-measurable.\n2.  **Defining Property:** For any set $A \\in \\sigma(Y)$, we must verify that $\\int_A \\sin(Y) \\, d\\mathbb{P} = \\int_A Z_1 \\, d\\mathbb{P}$. Substituting $Z_1 = \\sin(Y)$, this becomes $\\int_A \\sin(Y) \\, d\\mathbb{P} = \\int_A \\sin(Y) \\, d\\mathbb{P}$, which is trivially true.\nBoth conditions are satisfied, so we conclude:\n$$\nE[\\sin(Y) \\mid \\sigma(Y)] = \\sin(Y)\n$$\n\n**Analysis of the second term: $E[\\varepsilon \\mid \\sigma(Y)]$**\nWe are given that the random variable $\\varepsilon$ is independent of the random variable $Y$. In terms of sigma-algebras, this means that $\\sigma(\\varepsilon)$ is independent of $\\sigma(Y)$. Another fundamental property of conditional expectation is that if an integrable random variable $\\varepsilon$ is independent of a sigma-algebra $\\mathcal{G}$, then its conditional expectation with respect to $\\mathcal{G}$ is simply its unconditional expectation, i.e., $E[\\varepsilon \\mid \\mathcal{G}] = E[\\varepsilon]$.\n\nApplying this property with $\\mathcal{G} = \\sigma(Y)$, we get:\n$$\nE[\\varepsilon \\mid \\sigma(Y)] = E[\\varepsilon]\n$$\nThe problem states that $\\varepsilon \\sim \\mathcal{N}(0, \\tau^2)$. The expectation of a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ is $\\mu$. Therefore, $E[\\varepsilon] = 0$. This leads to the result $E[\\varepsilon \\mid \\sigma(Y)] = 0$.\n\nAgain, for full rigor, we verify this result using the fundamental definition. We propose that $Z_2 = 0$ is the conditional expectation $E[\\varepsilon \\mid \\sigma(Y)]$.\n1.  **Measurability:** The constant random variable $Z_2 = 0$ is measurable with respect to any sigma-algebra, and thus is $\\sigma(Y)$-measurable.\n2.  **Defining Property:** For any set $A \\in \\sigma(Y)$, we must check if $\\int_A \\varepsilon \\, d\\mathbb{P} = \\int_A Z_2 \\, d\\mathbb{P}$. The right-hand side is $\\int_A 0 \\, d\\mathbb{P} = 0$. The left-hand side can be written as the expectation of a product: $\\int_A \\varepsilon \\, d\\mathbb{P} = E[I_A \\cdot \\varepsilon]$, where $I_A$ is the indicator function of the set $A$. Since $A \\in \\sigma(Y)$, the indicator function $I_A$ is a $\\sigma(Y)$-measurable random variable. The independence of $\\varepsilon$ and $Y$ implies the independence of $\\varepsilon$ and any Borel-measurable function of $Y$, such as $I_A$. For independent random variables, the expectation of their product is the product of their expectations:\n$$\nE[I_A \\cdot \\varepsilon] = E[I_A] \\cdot E[\\varepsilon]\n$$\nSince $E[\\varepsilon] = 0$, we have $E[I_A \\cdot \\varepsilon] = E[I_A] \\cdot 0 = 0$. This matches the right-hand side, so the defining property is satisfied.\nBoth conditions hold, confirming that:\n$$\nE[\\varepsilon \\mid \\sigma(Y)] = 0\n$$\n\n**Final Combination**\nSubstituting the results for both terms back into the original expression:\n$$\nE[X \\mid \\sigma(Y)] = E[\\sin(Y) \\mid \\sigma(Y)] + E[\\varepsilon \\mid \\sigma(Y)] = \\sin(Y) + 0\n$$\nTherefore, the conditional expectation is:\n$$\nE[X \\mid \\sigma(Y)] = \\sin(Y)\n$$\nThis is a closed-form analytic expression, as required. The result illustrates the core idea of conditional Monte Carlo: conditioning on $Y$ removes the variability from the independent noise term $\\varepsilon$, effectively replacing $X = \\sin(Y) + \\varepsilon$ with its conditional mean $\\sin(Y)$, which has a smaller variance while preserving the overall mean $E[X] = E[E[X \\mid \\sigma(Y)]]$.", "answer": "$$\n\\boxed{\\sin(Y)}\n$$", "id": "3297665"}, {"introduction": "A valid Gibbs sampler requires that the specified full conditional distributions are \"coherent,\" meaning they arise from a single, well-defined joint distribution. However, it is possible to write down a set of conditionals that are mutually incompatible, leading to an invalid algorithm. This advanced computational practice [@problem_id:3297655] tasks you with implementing a powerful diagnostic to empirically test for coherence by checking if the stationary distribution is invariant to the Gibbs sampling scan order, bridging the gap between theoretical guarantees and practical algorithm verification.", "problem": "You are given a family of full conditional distributions for a bivariate random vector $X=(X\\_{1},X\\_{2})$ of the following linear-Gaussian form:\n$$\nX\\_{1}\\mid X\\_{2}=x\\_{2}\\sim \\mathcal{N}\\big(\\mu\\_{1} + a\\,x\\_{2},\\, s\\_{1}^{2}\\big),\\qquad\nX\\_{2}\\mid X\\_{1}=x\\_{1}\\sim \\mathcal{N}\\big(\\mu\\_{2} + b\\,x\\_{1},\\, s\\_{2}^{2}\\big),\n$$\nwhere $a,b\\in\\mathbb{R}$, $s\\_{1}^{2}>0$, $s\\_{2}^{2}>0$, and $\\mu\\_{1},\\mu\\_{2}\\in\\mathbb{R}$ are fixed parameters. The central question is whether these full conditionals are coherent, that is, whether there exists any joint distribution on $\\mathbb{R}^{2}$ whose conditionals coincide with the family above. Because exact analytical compatibility conditions can be delicate, you will empirically test a necessary manifestation of coherence based on invariance of iterated conditionals under different Gibbs-scan orders using Monte Carlo simulation.\n\nFoundational base and definitions to be used:\n- For any integrable function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$, the conditional expectation $E[f\\mid X\\_{i}]$ is the orthogonal projection of $f$ onto the $\\sigma$-field generated by $X\\_{i}$, $i\\in\\{1,2\\}$.\n- A one-scan two-step Gibbs update from state $(x\\_{1},x\\_{2})$ in order $1\\to 2$ consists of sampling $X\\_{1}'\\sim p(\\cdot\\mid X\\_{2}=x\\_{2})$ and then $X\\_{2}'\\sim p(\\cdot\\mid X\\_{1}=X\\_{1}')$. The corresponding Markov operator applied to $f$ is the iterated conditional expectation $K\\_{12}f(x\\_{1},x\\_{2})=E\\!\\left[E\\!\\left[f\\mid X\\_{1}\\right]\\bigm\\vert X\\_{2}\\right]$ evaluated at $(x\\_{1},x\\_{2})$. Similarly, for order $2\\to 1$, $K\\_{21}f(x\\_{1},x\\_{2})=E\\!\\left[E\\!\\left[f\\mid X\\_{2}\\right]\\bigm\\vert X\\_{1}\\right]$.\n- If the full conditionals are coherent (i.e., come from some joint distribution $\\Pi$ on $\\mathbb{R}^{2}$), then both scan orders define Markov chains that share the same stationary distribution $\\Pi$, and for any integrable $f$ and either scan order, the stationary expectation satisfies $E\\_{\\Pi}[f]=E\\_{\\Pi}[K\\_{12}f]=E\\_{\\Pi}[K\\_{21}f]$. Thus, under coherence, stationary expectations of a set of test functions are invariant to scan order.\n\nDesign a Monte Carlo coherence diagnostic that, for a given parameter set $(a,b,s\\_{1},s\\_{2},\\mu\\_{1},\\mu\\_{2})$, performs the following:\n- Construct the two Gibbs-scan orders $1\\to 2$ and $2\\to 1$ using the given full conditionals.\n- Simulate each chain for a burn-in of $N\\_{\\mathrm{burn}}$ steps, then collect $N$ post burn-in iterates.\n- For the five test functions\n$$\nf\\_{1}(x\\_{1},x\\_{2})=x\\_{1},\\quad\nf\\_{2}(x\\_{1},x\\_{2})=x\\_{2},\\quad\nf\\_{3}(x\\_{1},x\\_{2})=x\\_{1}x\\_{2},\\quad\nf\\_{4}(x\\_{1},x\\_{2})=x\\_{1}^{2},\\quad\nf\\_{5}(x\\_{1},x\\_{2})=x\\_{2}^{2},\n$$\ncompute Monte Carlo estimates of their stationary expectations under each scan order, together with Monte Carlo standard errors via non-overlapping batch means. Use a batch size $B$ and $M=\\lfloor N/B\\rfloor$ batches.\n- For each $f\\_{k}$, compute the standardized discrepancy\n$$\nZ\\_{k}=\\frac{\\left|\\widehat{E}\\_{12}[f\\_{k}]-\\widehat{E}\\_{21}[f\\_{k}]\\right|}{\\sqrt{\\widehat{\\mathrm{Var}}\\_{12}(\\bar{f}\\_{k})+\\widehat{\\mathrm{Var}}\\_{21}(\\bar{f}\\_{k})}},\n$$\nwhere $\\widehat{E}\\_{12}[f\\_{k}]$ and $\\widehat{E}\\_{21}[f\\_{k}]$ are the sample means from the $1\\to 2$ and $2\\to 1$ scans respectively, and $\\widehat{\\mathrm{Var}}(\\bar{f}\\_{k})$ are their batch-means variance estimators of the sample mean. Declare the parameter set “empirically coherent” if $\\max\\_{k\\in\\{1,\\dots,5\\}} Z\\_{k}\\le \\tau$ for a fixed threshold $\\tau$.\n\nUse fixed simulation settings:\n- Use $N\\_{\\mathrm{burn}}=20000$, $N=120000$, $B=600$, and $\\tau=5$.\n- Initialize both chains at $(0,0)$.\n- Use a fixed pseudorandom generator seed so that results are exactly reproducible.\n- Angles are not used; there are no physical units to report.\n\nTest suite:\nProvide results for the following four parameter sets $(a,b,s\\_{1},s\\_{2},\\mu\\_{1},\\mu\\_{2})$:\n- Case $1$ (coherent, derived from a genuine bivariate normal with mean $(m\\_{1},m\\_{2})=(0.5,-0.3)$, variances $(v\\_{1},v\\_{2})=(1.5,0.7)$, covariance $c=0.6$): \n$$\na=\\frac{0.6}{0.7},\\quad b=\\frac{0.6}{1.5},\\quad s\\_{1}=\\sqrt{1.5-\\frac{0.6^{2}}{0.7}},\\quad s\\_{2}=\\sqrt{0.7-\\frac{0.6^{2}}{1.5}},\n$$\n$$\n\\mu\\_{1}=0.5-a(-0.3),\\quad \\mu\\_{2}=-0.3-b(0.5).\n$$\n- Case $2$ (intentionally incoherent via slope/variance mismatch):\n$$\na=0.8,\\quad b=0.2,\\quad s\\_{1}=1.0,\\quad s\\_{2}=\\sqrt{0.9},\\quad \\mu\\_{1}=0.0,\\quad \\mu\\_{2}=0.0.\n$$\n- Case $3$ (intentionally incoherent via mean mismatch while slopes/variances are compatible with some zero-mean joint):\n$$\na=0.25,\\quad b=0.5,\\quad s\\_{1}=\\sqrt{0.875},\\quad s\\_{2}=\\sqrt{1.75},\\quad \\mu\\_{1}=0.6,\\quad \\mu\\_{2}=1.5.\n$$\n- Case $4$ (coherent, near the boundary $ab$ close to $1$):\n$$\na=b=\\sqrt{0.95},\\quad s\\_{1}=s\\_{2}=\\sqrt{1-0.95},\\quad \\mu\\_{1}=0.0,\\quad \\mu\\_{2}=0.0.\n$$\n\nYour program must implement the diagnostic above for all four cases and output a single line containing a list of four Boolean values, each indicating whether the corresponding parameter set is empirically coherent according to the criterion $\\max\\_{k} Z\\_{k}\\le \\tau$. The format must be exactly a comma-separated list enclosed in square brackets, for example $[{\\mathrm{True}}, {\\mathrm{False}}, {\\mathrm{False}}, {\\mathrm{True}}]$.", "solution": "The core of the problem is to devise an empirical diagnostic for the coherence of a set of full conditional distributions. A set of full conditionals $\\{p(x_i | x_{-i})\\}_{i=1}^d$ is coherent if there exists a joint probability distribution $p(x_1, \\dots, x_d)$ that has these conditionals. For the given linear-Gaussian case,\n$$\nX_1 \\mid X_2=x_2 \\sim \\mathcal{N}(\\mu_1 + a\\,x_2, s_1^2)\n$$\n$$\nX_2 \\mid X_1=x_1 \\sim \\mathcal{N}(\\mu_2 + b\\,x_1, s_2^2)\n$$\na joint bivariate normal distribution exists if and only if $ab \\ge 0$, $ab < 1$, and $as_2^2 = bs_1^2$. If these conditions hold, then a Gibbs sampler constructed from these conditionals will have the corresponding joint normal distribution as its unique stationary distribution, $\\Pi$. A critical property of such a sampler is that its stationary distribution is invariant to the scan order of the variables. That is, updating variables in the order $X_1 \\to X_2$ or $X_2 \\to X_1$ results in two distinct Markov chains that nevertheless share the same stationary distribution $\\Pi$.\n\nThis invariance provides a powerful diagnostic tool. If the conditionals are coherent, the long-run average of any integrable test function $f(X_1, X_2)$ must be the same regardless of scan order. Let $E_{12}[f]$ and $E_{21}[f]$ denote the stationary expectations of $f$ under the $1 \\to 2$ and $2 \\to 1$ scan orders, respectively. Coherence implies $E_{12}[f] = E_{21}[f] = E_\\Pi[f]$. The proposed diagnostic tests this implication using Monte Carlo simulation.\n\nThe algorithm proceeds as follows:\n\n1.  **Gibbs Sampler Simulation**: Two parallel Gibbs sampling chains are run, one for each scan order.\n    -   **Chain 1 (Order $1 \\to 2$)**: From a state $(x_1^{(t)}, x_2^{(t)})$, the next state $(x_1^{(t+1)}, x_2^{(t+1)})$ is generated by sampling $x_{1, \\text{new}} \\sim \\mathcal{N}(\\mu_1 + a\\,x_2^{(t)}, s_1^2)$ and then $x_2^{(t+1)} \\sim \\mathcal{N}(\\mu_2 + b\\,x_{1, \\text{new}}, s_2^2)$, setting $x_1^{(t+1)} = x_{1, \\text{new}}$.\n    -   **Chain 2 (Order $2 \\to 1$)**: The update involves first sampling $x_{2, \\text{new}} \\sim \\mathcal{N}(\\mu_2 + b\\,x_1^{(t)}, s_2^2)$ and then $x_1^{(t+1)} \\sim \\mathcal{N}(\\mu_1 + a\\,x_{2, \\text{new}}, s_1^2)$, setting $x_2^{(t+1)} = x_{2, \\text{new}}$.\n\n    Both chains are initialized at $(x_1^{(0)}, x_2^{(0)}) = (0,0)$. They are run for $N_{\\mathrm{burn}} = 20000$ iterations to discard initial transient samples (burn-in). Subsequently, $N = 120000$ samples are collected from each chain.\n\n2.  **Monte Carlo Estimation of Expectations**: For each of the five test functions $f_k$ and for each chain (denoted by scan order $j \\in \\{12, 21\\}$), the stationary expectation $E_j[f_k]$ is estimated using the sample mean of the function evaluated over the $N$ post-burn-in samples $\\{(x_{1,j}^{(i)}, x_{2,j}^{(i)})\\}_{i=1}^N$:\n    $$\n    \\widehat{E}_j[f_k] = \\frac{1}{N} \\sum_{i=1}^{N} f_k(x_{1,j}^{(i)}, x_{2,j}^{(i)})\n    $$\n\n3.  **Standard Error Estimation via Batch Means**: The samples from an MCMC chain are inherently correlated, invalidating the simple variance formula for an i.i.d. sample mean. To obtain a valid estimate of the variance of $\\widehat{E}_j[f_k]$, the method of non-overlapping batch means (NOBM) is employed. The $N$ post-burn-in samples of $f_k$ are partitioned into $M = \\lfloor N/B \\rfloor = \\lfloor 120000/600 \\rfloor = 200$ non-overlapping batches, each of size $B=600$. For each batch $m \\in \\{1, \\dots, M\\}$, the batch mean $\\bar{f}_{k,j}^{(m)}$ is computed. The variance of the overall sample mean $\\widehat{E}_j[f_k]$ is then estimated as:\n    $$\n    \\widehat{\\mathrm{Var}}_j(\\bar{f}_k) = \\frac{1}{M} \\cdot \\frac{1}{M-1} \\sum_{m=1}^{M} \\left( \\bar{f}_{k,j}^{(m)} - \\widehat{E}_j[f_k] \\right)^2\n    $$\n    This formula computes the sample variance of the batch means and scales it by $1/M$. For large enough batch size $B$, the batch means are approximately uncorrelated and, by the Central Limit Theorem, normally distributed.\n\n4.  **Standardized Discrepancy Test**: For each test function $f_k$, the standardized discrepancy $Z_k$ between the estimates from the two chains is calculated. This is analogous to a two-sample Z-test:\n    $$\n    Z_k = \\frac{\\left|\\widehat{E}_{12}[f_k] - \\widehat{E}_{21}[f_k]\\right|}{\\sqrt{\\widehat{\\mathrm{Var}}_{12}(\\bar{f}_k) + \\widehat{\\mathrm{Var}}_{21}(\\bar{f}_k)}}\n    $$\n    Under the null hypothesis of coherence, $\\widehat{E}_{12}[f_k]$ and $\\widehat{E}_{21}[f_k]$ are estimates of the same quantity. Their difference should be small, and the $Z_k$ statistics should be realizations of approximately standard normal random variables. A large value of $Z_k$ provides evidence against coherence. The parameter set is declared \"empirically coherent\" if the maximum observed discrepancy across all test functions is statistically insignificant, i.e., $\\max_{k\\in\\{1,\\dots,5\\}} Z_k \\le \\tau$, for a given threshold $\\tau=5$.\n\nThis entire procedure is implemented in Python using the `numpy` library for numerical computations and random number generation, with a fixed seed to ensure reproducibility. The diagnostic is applied to each of the four specified parameter sets to determine its empirical coherence status.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_gibbs_sampler(params, settings, scan_order, rng):\n    \"\"\"\n    Runs a Gibbs sampler for a given scan order and returns the collected samples.\n    \"\"\"\n    a, b, s1, s2, mu1, mu2 = params\n    N_burn, N = settings['N_burn'], settings['N']\n    x1, x2 = 0.0, 0.0\n\n    # Burn-in phase\n    for _ in range(N_burn):\n        if scan_order == '12':\n            x1 = rng.normal(loc=mu1 + a * x2, scale=s1)\n            x2 = rng.normal(loc=mu2 + b * x1, scale=s2)\n        elif scan_order == '21':\n            x2 = rng.normal(loc=mu2 + b * x1, scale=s2)\n            x1 = rng.normal(loc=mu1 + a * x2, scale=s1)\n\n    # Sample collection phase\n    samples = np.zeros((N, 2))\n    for i in range(N):\n        if scan_order == '12':\n            x1 = rng.normal(loc=mu1 + a * x2, scale=s1)\n            x2 = rng.normal(loc=mu2 + b * x1, scale=s2)\n        elif scan_order == '21':\n            x2 = rng.normal(loc=mu2 + b * x1, scale=s2)\n            x1 = rng.normal(loc=mu1 + a * x2, scale=s1)\n        samples[i, :] = [x1, x2]\n\n    return samples\n\ndef calculate_stats(samples, settings):\n    \"\"\"\n    Computes Monte Carlo estimates and their batch-means variances.\n    \"\"\"\n    N, B = settings['N'], settings['B']\n    M = N // B\n\n    x1_s = samples[:, 0]\n    x2_s = samples[:, 1]\n\n    # Evaluate the five test functions over the samples\n    f_values = [\n        x1_s,\n        x2_s,\n        x1_s * x2_s,\n        x1_s**2,\n        x2_s**2\n    ]\n\n    means = []\n    vars_of_mean = []\n\n    for f_vec in f_values:\n        # Overall sample mean\n        overall_mean = np.mean(f_vec)\n        means.append(overall_mean)\n\n        # Reshape for batching\n        reshaped_f = f_vec.reshape((M, B))\n        batch_means = np.mean(reshaped_f, axis=1)\n\n        # Variance of the sample mean using batch means\n        # This is Var(batch_means) / M\n        var_of_batch_means = np.var(batch_means, ddof=1)\n        var_of_mean = var_of_batch_means / M\n        vars_of_mean.append(var_of_mean)\n\n    return means, vars_of_mean\n\ndef run_coherence_diagnostic(params, settings):\n    \"\"\"\n    Performs the full coherence diagnostic for a single parameter set.\n    \"\"\"\n    rng = np.random.default_rng(settings['seed'])\n\n    # Run sampler for scan order 1->2\n    samples_12 = run_gibbs_sampler(params, settings, '12', rng)\n    means_12, vars_12 = calculate_stats(samples_12, settings)\n\n    # Run sampler for scan order 2->1\n    # Re-seed the rng to get a different path, which is a stronger test.\n    # A fair comparison uses independent runs.\n    rng = np.random.default_rng(settings['seed'])\n    samples_21 = run_gibbs_sampler(params, settings, '21', rng)\n    means_21, vars_21 = calculate_stats(samples_21, settings)\n\n    Z_scores = []\n    for k in range(5):  # For each of the five test functions\n        numerator = np.abs(means_12[k] - means_21[k])\n        denominator = np.sqrt(vars_12[k] + vars_21[k])\n        \n        if denominator == 0.0:\n            Z_k = 0.0 if numerator == 0.0 else np.inf\n        else:\n            Z_k = numerator / denominator\n        Z_scores.append(Z_k)\n    \n    max_Z = np.max(Z_scores)\n    return max_Z = settings['tau']\n\ndef solve():\n    # Define the fixed simulation settings\n    simulation_settings = {\n        'N_burn': 20000,\n        'N': 120000,\n        'B': 600,\n        'tau': 5.0,\n        'seed': 42 # Fixed seed for reproducibility\n    }\n\n    # Define the test cases from the problem statement.\n    # Case 1 (coherent)\n    c1_v1, c1_v2, c1_c = 1.5, 0.7, 0.6\n    c1_m1, c1_m2 = 0.5, -0.3\n    c1_a = c1_c / c1_v2\n    c1_b = c1_c / c1_v1\n    c1_s1 = np.sqrt(c1_v1 - c1_c**2 / c1_v2)\n    c1_s2 = np.sqrt(c1_v2 - c1_c**2 / c1_v1)\n    c1_mu1 = c1_m1 - c1_a * c1_m2\n    c1_mu2 = c1_m2 - c1_b * c1_m1\n    case1 = (c1_a, c1_b, c1_s1, c1_s2, c1_mu1, c1_mu2)\n\n    # Case 2 (incoherent)\n    case2 = (0.8, 0.2, 1.0, np.sqrt(0.9), 0.0, 0.0)\n\n    # Case 3 (incoherent)\n    case3 = (0.25, 0.5, np.sqrt(0.875), np.sqrt(1.75), 0.6, 1.5)\n\n    # Case 4 (coherent, near boundary)\n    a_b_4 = np.sqrt(0.95)\n    s_4 = np.sqrt(1.0 - 0.95)\n    case4 = (a_b_4, a_b_4, s_4, s_4, 0.0, 0.0)\n\n    test_cases = [case1, case2, case3, case4]\n\n    results = []\n    for case in test_cases:\n        is_coherent = run_coherence_diagnostic(case, simulation_settings)\n        results.append(is_coherent)\n    \n    # Python's str(True) is 'True', which matches the requested format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3297655"}]}