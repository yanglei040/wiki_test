{"hands_on_practices": [{"introduction": "In our exploration of joint distributions, it is tempting to assume that a joint probability density function always exists. This first exercise challenges that notion by presenting a case where a two-dimensional random vector's entire probability mass lies on a one-dimensional curve. This practice will guide you through a measure-theoretic argument to show why such a distribution is singular and lacks a joint density, even while its marginal distributions are perfectly well-defined and can be derived using the fundamental cumulative distribution function (CDF) method [@problem_id:3315509].", "problem": "Let $X$ be a real-valued random variable with a probability density function (PDF) $f_{X}(x)$ that is the normal density with mean $1$ and variance $1$, i.e., $f_{X}(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\big(-\\frac{(x-1)^{2}}{2}\\big)$. Define the measurable mapping $T:\\mathbb{R}\\to\\mathbb{R}^{2}$ by $T(x) = (x,x^{2})$, and let $(X,Y) := T(X) = (X, X^{2})$. Denote by $\\mu$ the joint law of $(X,Y)$ on $\\mathbb{R}^{2}$, i.e., $\\mu = \\mathbb{P}\\circ T^{-1}$. Let $\\lambda_{1}$ and $\\lambda_{2}$ denote one-dimensional and two-dimensional Lebesgue measure, respectively.\n\nTasks:\n- Using only foundational measure-theoretic definitions (pushforward measure, absolute continuity, and singularity), construct the joint distribution $\\mu$ of $(X,Y)$ and prove that $\\mu$ is supported on the set $\\{(x,y)\\in\\mathbb{R}^{2}: y = x^{2}\\}$, has no joint density with respect to $\\lambda_{2}$, and yet has well-defined marginal distributions with respect to $\\lambda_{1}$.\n- Starting from the definition of the cumulative distribution function (CDF) and the chain rule for differentiation, derive the PDF of $Y$ for $y>0$.\n- Evaluate the resulting marginal density of $Y$ at $y=1$ as a closed-form expression. Provide your final answer as a single analytic expression. Do not approximate; no rounding is required.", "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information to derive a unique and meaningful solution. The tasks require a rigorous application of measure theory, probability theory, and calculus.\n\nThe problem asks for three items: a measure-theoretic analysis of the joint distribution, the derivation of a marginal probability density function (PDF), and the evaluation of this PDF at a specific point. We will address these in sequence.\n\nLet $X$ be a random variable with a normal distribution of mean $\\mu_X = 1$ and variance $\\sigma_X^2 = 1$. Its PDF is given by $f_{X}(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-1)^{2}}{2}\\right)$. Let $P_X$ be the law of $X$, which is a measure on $(\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))$. For any Borel set $B \\subseteq \\mathbb{R}$, $P_X(B) = \\mathbb{P}(X \\in B) = \\int_B f_X(x) \\, d\\lambda_1(x)$, where $\\lambda_1$ is the one-dimensional Lebesgue measure. This shows that $P_X$ is absolutely continuous with respect to $\\lambda_1$.\n\nThe random vector $(X,Y)$ is defined by the measurable transformation $T:\\mathbb{R} \\to \\mathbb{R}^2$, $T(x) = (x, x^2)$, such that $(X,Y) = T(X)$. The joint distribution $\\mu$ of $(X,Y)$ is the pushforward measure of $P_X$ under $T$.\n\n_Task 1: Measure-Theoretic Analysis of the Joint Distribution $\\mu$_\n\n- **Construction of $\\mu$**:\nBy the definition of a pushforward measure, for any Borel set $A \\subseteq \\mathbb{R}^2$, the measure $\\mu(A)$ is given by:\n$$ \\mu(A) = P_X(T^{-1}(A)) $$\nwhere $T^{-1}(A) = \\{x \\in \\mathbb{R} \\mid T(x) \\in A\\} = \\{x \\in \\mathbb{R} \\mid (x, x^2) \\in A\\}$.\nSince $P_X$ has a density $f_X$ with respect to $\\lambda_1$, we can write $\\mu(A)$ as an integral:\n$$ \\mu(A) = \\int_{T^{-1}(A)} f_X(x) \\, d\\lambda_1(x) $$\nThis integral expression is the construction of the joint distribution $\\mu$.\n\n- **Support of $\\mu$**:\nThe support of a measure is the smallest closed set $S$ such that $\\mu(\\mathbb{R}^2 \\setminus S) = 0$. Let $C$ be the parabola defined by $C = \\{(x,y) \\in \\mathbb{R}^2 \\mid y=x^2\\}$. The set $C$ is a closed set in $\\mathbb{R}^2$. By definition, the random vector $(X,Y)=(X,X^2)$ always lies on this parabola. Therefore, the entire probability mass of $\\mu$ is concentrated on $C$. We can show this formally:\n$$ \\mu(C) = \\mathbb{P}((X,Y) \\in C) = \\mathbb{P}((X, X^2) \\in C) = \\mathbb{P}(\\text{true}) = 1 $$\nSince $\\mu(C)=1$, the support of $\\mu$ must be a subset of $C$. To show that the support is exactly $C$, we must show that for any point $p \\in C$ and any open neighborhood $U$ of $p$, we have $\\mu(U) > 0$. Let $p = (x_0, x_0^2) \\in C$ and let $U$ be an open set containing $p$. The function $T(x)=(x, x^2)$ is continuous, so the preimage $T^{-1}(U)$ is an open set in $\\mathbb{R}$. Since $(x_0, x_0^2) \\in U$, we have $x_0 \\in T^{-1}(U)$, so $T^{-1}(U)$ is non-empty. Any non-empty open set in $\\mathbb{R}$ has a positive Lebesgue measure, $\\lambda_1(T^{-1}(U)) > 0$. The density $f_X(x)$ is strictly positive for all $x \\in \\mathbb{R}$. Therefore,\n$$ \\mu(U) = \\int_{T^{-1}(U)} f_X(x) \\, d\\lambda_1(x) > 0 $$\nThis confirms that the support of $\\mu$ is precisely the parabola $C = \\{(x,y) \\in \\mathbb{R}^2 \\mid y=x^2\\}$.\n\n- **Absence of Joint Density with respect to $\\lambda_2$**:\nA measure $\\mu$ on $\\mathbb{R}^2$ has a density with respect to the two-dimensional Lebesgue measure $\\lambda_2$ if and only if $\\mu$ is absolutely continuous with respect to $\\lambda_2$ ($\\mu \\ll \\lambda_2$). This means that for any Borel set $A \\subseteq \\mathbb{R}^2$, if $\\lambda_2(A)=0$, then $\\mu(A)=0$.\nConsider the set $C$, the support of $\\mu$. As the graph of a continuous function, $C$ is a curve in $\\mathbb{R}^2$ and has a $\\lambda_2$-measure of zero, i.e., $\\lambda_2(C)=0$. However, as shown above, $\\mu(C)=1$.\nSince we have a set $C$ for which $\\lambda_2(C)=0$ but $\\mu(C)=1 \\neq 0$, the condition for absolute continuity is violated. Therefore, by the Radon-Nikodym theorem, the measure $\\mu$ does not have a density with respect to $\\lambda_2$. The distribution of $(X,Y)$ is singular with respect to the 2D Lebesgue measure.\n\n- **Well-Defined Marginal Distributions**:\nThe marginal distribution of $X$, denoted $\\mu_X$, is given by $\\mu_X(B) = \\mu(B \\times \\mathbb{R})$ for a Borel set $B \\subseteq \\mathbb{R}$.\n$$ \\mu_X(B) = \\mathbb{P}((X,Y) \\in B \\times \\mathbb{R}) = \\mathbb{P}(X \\in B) = P_X(B) $$\nAs given, $P_X$ has a density $f_X(x)$ with respect to $\\lambda_1$. Thus, the marginal distribution of $X$ is well-defined and absolutely continuous with respect to $\\lambda_1$.\n\nThe marginal distribution of $Y$, denoted $\\mu_Y$, is given by $\\mu_Y(A) = \\mu(\\mathbb{R} \\times A)$ for a Borel set $A \\subseteq \\mathbb{R}$.\n$$ \\mu_Y(A) = \\mathbb{P}((X,Y) \\in \\mathbb{R} \\times A) = \\mathbb{P}(Y \\in A) $$\nIn the next task, we will derive the PDF $f_Y(y)$ for $Y$. The existence of this PDF demonstrates that $\\mu_Y$ is also absolutely continuous with respect to $\\lambda_1$. Therefore, both marginal distributions are well-defined and possess densities with respect to $\\lambda_1$.\n\n_Task 2: Derivation of the PDF of $Y$ for $y > 0$_\n\nWe derive the PDF of $Y=X^2$ using the cumulative distribution function (CDF) method. Let $F_Y(y)$ be the CDF of $Y$.\n$$ F_Y(y) = \\mathbb{P}(Y \\le y) $$\nSince $Y=X^2$, $Y$ is non-negative, so $F_Y(y) = 0$ for $y < 0$. We are asked to find the PDF for $y>0$.\nFor $y > 0$:\n$$ F_Y(y) = \\mathbb{P}(X^2 \\le y) = \\mathbb{P}(-\\sqrt{y} \\le X \\le \\sqrt{y}) $$\nThis probability can be expressed as an integral of the PDF of $X$:\n$$ F_Y(y) = \\int_{-\\sqrt{y}}^{\\sqrt{y}} f_X(x) \\,dx $$\nThe PDF $f_Y(y)$ is the derivative of the CDF, $f_Y(y) = \\frac{d}{dy}F_Y(y)$. We use the Leibniz integral rule, which is an application of the Fundamental Theorem of Calculus and the chain rule as requested:\n$$ f_Y(y) = \\frac{d}{dy} \\int_{-\\sqrt{y}}^{\\sqrt{y}} f_X(x) \\,dx = f_X(\\sqrt{y}) \\cdot \\frac{d}{dy}(\\sqrt{y}) - f_X(-\\sqrt{y}) \\cdot \\frac{d}{dy}(-\\sqrt{y}) $$\nThe derivatives of the limits are $\\frac{d}{dy}(\\sqrt{y}) = \\frac{1}{2\\sqrt{y}}$ and $\\frac{d}{dy}(-\\sqrt{y}) = -\\frac{1}{2\\sqrt{y}}$.\nSubstituting these into the expression for $f_Y(y)$:\n$$ f_Y(y) = f_X(\\sqrt{y}) \\left(\\frac{1}{2\\sqrt{y}}\\right) - f_X(-\\sqrt{y}) \\left(-\\frac{1}{2\\sqrt{y}}\\right) = \\frac{1}{2\\sqrt{y}} \\left[ f_X(\\sqrt{y}) + f_X(-\\sqrt{y}) \\right] $$\nNow, we substitute the specific PDF for $X$, $f_X(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x-1)^2}{2}\\right)$:\n$$ f_X(\\sqrt{y}) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\sqrt{y}-1)^2}{2}\\right) $$\n$$ f_X(-\\sqrt{y}) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(-\\sqrt{y}-1)^2}{2}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\sqrt{y}+1)^2}{2}\\right) $$\nCombining these gives:\n$$ f_Y(y) = \\frac{1}{2\\sqrt{y}} \\frac{1}{\\sqrt{2\\pi}} \\left[ \\exp\\left(-\\frac{y-2\\sqrt{y}+1}{2}\\right) + \\exp\\left(-\\frac{y+2\\sqrt{y}+1}{2}\\right) \\right] $$\n$$ f_Y(y) = \\frac{1}{2\\sqrt{2\\pi y}} \\exp\\left(-\\frac{y+1}{2}\\right) \\left[ \\exp\\left(\\sqrt{y}\\right) + \\exp\\left(-\\sqrt{y}\\right) \\right] $$\nUsing the identity $\\cosh(z) = \\frac{\\exp(z)+\\exp(-z)}{2}$, we have $2\\cosh(\\sqrt{y}) = \\exp(\\sqrt{y}) + \\exp(-\\sqrt{y})$.\n$$ f_Y(y) = \\frac{1}{2\\sqrt{2\\pi y}} \\exp\\left(-\\frac{y+1}{2}\\right) [2\\cosh(\\sqrt{y})] $$\n$$ f_Y(y) = \\frac{1}{\\sqrt{2\\pi y}} \\exp\\left(-\\frac{y+1}{2}\\right) \\cosh(\\sqrt{y}) $$\nThis is the PDF of $Y$ for $y>0$. This corresponds to a non-central chi-squared distribution with one degree of freedom and non-centrality parameter $\\lambda=1$.\n\n_Task 3: Evaluation of the Marginal Density of $Y$ at $y=1$_\n\nWe need to evaluate $f_Y(y)$ at $y=1$.\n$$ f_Y(1) = \\frac{1}{\\sqrt{2\\pi(1)}} \\exp\\left(-\\frac{1+1}{2}\\right) \\cosh(\\sqrt{1}) $$\n$$ f_Y(1) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-1) \\cosh(1) $$\nTo obtain a closed-form expression in terms of elementary exponentials, we use the definition of the hyperbolic cosine function: $\\cosh(1) = \\frac{\\exp(1) + \\exp(-1)}{2}$.\n$$ f_Y(1) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-1) \\left( \\frac{\\exp(1) + \\exp(-1)}{2} \\right) $$\n$$ f_Y(1) = \\frac{1}{2\\sqrt{2\\pi}} \\left( \\exp(-1)\\exp(1) + \\exp(-1)\\exp(-1) \\right) $$\n$$ f_Y(1) = \\frac{1}{2\\sqrt{2\\pi}} \\left( \\exp(0) + \\exp(-2) \\right) $$\n$$ f_Y(1) = \\frac{1 + \\exp(-2)}{2\\sqrt{2\\pi}} $$\nThis is the final analytic expression.", "answer": "$$\\boxed{\\frac{1 + \\exp(-2)}{2\\sqrt{2\\pi}}}$$", "id": "3315509"}, {"introduction": "Having explored a singular case, we now turn to the more common scenario of a smooth, one-to-one transformation between random vectors, where the joint density can be directly calculated. This exercise provides essential practice with the multivariate change of variables formula, a cornerstone technique for manipulating continuous random variables. You will apply the Jacobian method to find the joint density of a transformed pair of variables and then compute a marginal density by integrating out the nuisance variable, honing a core computational skill for stochastic modeling [@problem_id:3315512].", "problem": "Let $(U,V)$ be a pair of random variables with joint probability density function $f_{U,V}(u,v)=2\\,\\mathbf{1}\\{0\\le v\\le u\\le 1\\}$. Consider the transformation to $(X,Y)$ defined by $X=U$ and $Y=U\\,V$. Using only the definitions of joint densities under smooth, bijective transformations and the definition of marginalization by integration, derive the joint density $f_{X,Y}(x,y)$, including its precise support. Then compute the marginal density $f_Y(y)$ by integrating out $x$. Provide your final answer as a single closed-form analytic expression for $f_Y(y)$, simplified. Do not provide intermediate results in the final answer. No rounding is required.", "solution": "The problem is first validated to ensure it is well-posed and scientifically sound.\n\nThe problem provides a joint probability density function (PDF) for two random variables $(U,V)$, given by $f_{U,V}(u,v) = 2\\,\\mathbf{1}\\{0\\le v\\le u\\le 1\\}$. The symbol $\\mathbf{1}\\{\\cdot\\}$ denotes the indicator function. The support of this distribution is the triangular region $\\mathcal{S}_{U,V} = \\{(u,v) \\in \\mathbb{R}^2 \\mid 0 \\le v \\le u \\le 1\\}$.\nTo be a valid PDF, the function must be non-negative and its integral over its support must equal $1$.\nThe function $f_{U,V}(u,v)$ is $2$ on its support and $0$ elsewhere, so it is non-negative.\nThe integral is:\n$$ \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f_{U,V}(u,v) \\,dv\\,du = \\int_0^1 \\int_0^u 2 \\,dv\\,du $$\n$$ = \\int_0^1 [2v]_{v=0}^{v=u} \\,du = \\int_0^1 2u \\,du = [u^2]_0^1 = 1^2 - 0^2 = 1 $$\nThe PDF is valid. The problem statement is self-contained, unambiguous, and poses a standard question in probability theory. Thus, the problem is valid, and we may proceed with the solution.\n\nThe task is to find the marginal density $f_Y(y)$ of the random variable $Y = UV$, where the transformation is from $(U,V)$ to $(X,Y)$ defined by $X=U$ and $Y=UV$. This will be accomplished in two steps: first, finding the joint PDF $f_{X,Y}(x,y)$ using the change of variables method, and second, finding the marginal PDF $f_Y(y)$ by integrating $f_{X,Y}(x,y)$ with respect to $x$.\n\n**Step 1: Derive the joint density $f_{X,Y}(x,y)$**\n\nThe transformation from $(U,V)$ to $(X,Y)$ is given by the functions:\n$x = g_1(u,v) = u$\n$y = g_2(u,v) = uv$\n\nTo use the change of variables formula, we first need to find the inverse transformation, which expresses $(u,v)$ in terms of $(x,y)$.\nFrom $x=u$, we immediately have $u=x$.\nSubstituting this into the second equation, $y = xv$, which gives $v = y/x$.\nSo, the inverse transformation is:\n$u = h_1(x,y) = x$\n$v = h_2(x,y) = y/x$\n\nNext, we compute the Jacobian of this inverse transformation. The Jacobian matrix is:\n$$ J = \\begin{pmatrix} \\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial}{\\partial x}(x) & \\frac{\\partial}{\\partial y}(x) \\\\ \\frac{\\partial}{\\partial x}(y/x) & \\frac{\\partial}{\\partial y}(y/x) \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ -y/x^2 & 1/x \\end{pmatrix} $$\nThe determinant of the Jacobian is:\n$$ \\det(J) = (1)\\left(\\frac{1}{x}\\right) - (0)\\left(-\\frac{y}{x^2}\\right) = \\frac{1}{x} $$\nThe absolute value of the Jacobian determinant is $|\\det(J)| = |1/x|$.\n\nNow, we determine the support of the new variables $(X,Y)$, denoted $\\mathcal{S}_{X,Y}$. This region is the image of the support $\\mathcal{S}_{U,V}$ under the transformation. The constraints for $\\mathcal{S}_{U,V}$ are $0 \\le v \\le u \\le 1$. We substitute the inverse transformation expressions for $u$ and $v$:\n$1$. $u \\le 1 \\implies x \\le 1$\n$2$. $v \\le u \\implies y/x \\le x \\implies y \\le x^2$ (assuming $x > 0$)\n$3$. $v \\ge 0 \\implies y/x \\ge 0 \\implies y \\ge 0$ (assuming $x > 0$)\nThe condition $u \\ge 0$ implies $x \\ge 0$. Combining with $x \\le 1$, we have $0 \\le x \\le 1$.\nSince $x=u$ and $u \\ge v \\ge 0$, $x$ must be non-negative. On the interior of the support, $u>0$, hence $x>0$, so the multiplications by $x$ are valid. The case $x=0$ corresponds to $u=0$, which implies $v=0$, and thus $y=0$. This is a single point $(0,0)$ which has zero probability mass. Thus, we can safely assume $x>0$ for the density calculation, making $|\\det(J)| = 1/x$.\n\nThe support for $(X,Y)$ is therefore defined by the inequalities: $0 \\le x \\le 1$ and $0 \\le y \\le x^2$. This can be written compactly as $\\mathcal{S}_{X,Y} = \\{(x,y) \\in \\mathbb{R}^2 \\mid 0 \\le y \\le x^2 \\le 1\\}$.\n\nThe change of variables formula for the joint PDF is:\n$$ f_{X,Y}(x,y) = f_{U,V}(u(x,y), v(x,y)) |\\det(J)| $$\nSubstituting the expressions for $u,v$, the value of $f_{U,V}$ (which is $2$ on its support), and $|\\det(J)|$:\n$$ f_{X,Y}(x,y) = 2 \\cdot \\frac{1}{x} = \\frac{2}{x} $$\nThis expression is valid for $(x,y) \\in \\mathcal{S}_{X,Y}$. So, the full joint PDF for $(X,Y)$ is:\n$$ f_{X,Y}(x,y) = \\frac{2}{x} \\mathbf{1}\\{0 \\le y \\le x^2 \\le 1\\} $$\n\n**Step 2: Derive the marginal density $f_Y(y)$**\n\nThe marginal PDF of $Y$ is found by integrating the joint PDF $f_{X,Y}(x,y)$ over all possible values of $x$:\n$$ f_Y(y) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y) \\,dx $$\nTo perform this integration, we need the limits for $x$ for a fixed value of $y$. From the support $\\mathcal{S}_{X,Y}$, the inequalities are $0 \\le y \\le x^2$ and $x \\le 1$.\nThe first inequality, $y \\le x^2$, implies $x \\ge \\sqrt{y}$ (since $x \\ge 0$).\nThe second inequality is $x \\le 1$.\nSo, for a fixed $y$, $x$ ranges from $\\sqrt{y}$ to $1$.\nThe possible range for $y$ is determined from the support as well. Since $y \\ge 0$ and $y \\le x^2 \\le 1^2=1$, we have $0 \\le y \\le 1$.\n\nFor any $y \\in [0,1]$, the integral is:\n$$ f_Y(y) = \\int_{\\sqrt{y}}^{1} \\frac{2}{x} \\,dx $$\nThis is a standard integral:\n$$ f_Y(y) = [2 \\ln|x|]_{\\sqrt{y}}^{1} $$\nSince $x$ is integrated over the interval $[\\sqrt{y}, 1]$ where $y \\in [0,1]$, $x$ is always positive, so $|x|=x$.\n$$ f_Y(y) = 2 (\\ln(1) - \\ln(\\sqrt{y})) $$\nUsing the logarithmic properties $\\ln(1)=0$ and $\\ln(\\sqrt{y}) = \\ln(y^{1/2}) = \\frac{1}{2}\\ln(y)$:\n$$ f_Y(y) = 2 \\left(0 - \\frac{1}{2}\\ln(y)\\right) = -\\ln(y) $$\nThis expression is valid for $y \\in (0,1]$. For $y \\le 0$ or $y > 1$, $f_Y(y)=0$. The problem asks for the analytic expression, which is $-\\ln(y)$.\nAs a check, we can verify that $\\int_0^1 f_Y(y) \\,dy = 1$:\n$$ \\int_0^1 -\\ln(y) \\,dy $$\nUsing integration by parts, with $u = -\\ln(y)$ and $dv = dy$, so $du = -1/y \\,dy$ and $v=y$:\n$$ \\int_0^1 -\\ln(y) \\,dy = [-y\\ln(y)]_0^1 - \\int_0^1 y\\left(-\\frac{1}{y}\\right) \\,dy $$\n$$ = [-y\\ln(y)]_0^1 + \\int_0^1 1 \\,dy $$\nThe limit $\\lim_{y \\to 0^+} y\\ln(y) = 0$. At $y=1$, $-1\\ln(1)=0$. So the first term evaluates to $0$. The second term is $[y]_0^1=1$. The total is $0+1=1$, which confirms the marginal density is correct.\n\nThe final simplified, closed-form analytic expression for $f_Y(y)$ is $-\\ln(y)$.", "answer": "$$\\boxed{-\\ln(y)}$$", "id": "3315512"}, {"introduction": "Real-world systems often involve interactions between continuous quantities and discrete states, requiring a probabilistic framework that accommodates both. This practice extends our understanding of joint and marginal distributions to the important case of mixed-type variables, using a latent variable model as a guiding example. By deriving the joint and marginal densities for what is known as a Gaussian Mixture Model (GMM), you will see how the fundamental principles of conditioning and marginalization build one of the most versatile tools in modern statistics and machine learning [@problem_id:3315548].", "problem": "Consider a mixture model used in stochastic simulation and Monte Carlo methods for latent-variable sampling. Let $X \\in \\mathbb{R}^{d}$ be a continuous random vector and $Z \\in \\{1,2,\\dots,K\\}$ be a categorical latent variable. Assume $P(Z=k)=\\pi_{k}$ with $\\pi_{k}>0$ and $\\sum_{k=1}^{K}\\pi_{k}=1$, and conditionally $X \\mid Z=k$ follows a multivariate normal distribution with mean $\\mu_{k} \\in \\mathbb{R}^{d}$ and covariance matrix $\\Sigma_{k} \\in \\mathbb{R}^{d \\times d}$ that is symmetric positive definite for each $k$. Formulate the joint distribution of $(X,Z)$ with respect to the product of the $d$-dimensional Lebesgue measure and the counting measure on $\\{1,\\dots,K\\}$, and derive both the joint probability density function (PDF) $f_{X,Z}(x,k)$ and the marginal PDF $f_{X}(x)$ of $X$. Begin from the formal definitions of joint and marginal distributions and the law of total probability, and be explicit about the dominating measure for mixed discrete-continuous variables. Your derivation should not assume any shortcut identities beyond foundational definitions and well-tested formulas. The final answer must be provided as closed-form analytic expressions. No numerical approximation or rounding is required.", "solution": "The problem statement is a well-posed and scientifically grounded request to derive the joint and marginal probability density functions for a Gaussian Mixture Model. All provided conditions are standard, complete, and consistent. The problem is therefore valid.\n\nThe problem considers a mixed random variable $(X, Z)$, where $X$ is a continuous random vector in $\\mathbb{R}^{d}$ and $Z$ is a discrete (categorical) random variable taking values in the set $\\{1, 2, \\dots, K\\}$.\n\nFirst, we must define the sample space for the joint variable $(X,Z)$, which is the product space $\\mathbb{R}^{d} \\times \\{1, 2, \\dots, K\\}$. To define a density function for this mixed-type variable, we must specify a dominating measure. As requested, we use the product measure $\\lambda^{d} \\otimes \\nu$, where $\\lambda^{d}$ is the $d$-dimensional Lebesgue measure on $\\mathbb{R}^{d}$ and $\\nu$ is the counting measure on the finite set $\\{1, 2, \\dots, K\\}$. The joint probability density function (PDF) $f_{X,Z}(x,k)$ is defined with respect to this product measure. For any measurable set $A \\subseteq \\mathbb{R}^{d}$ and any subset $B \\subseteq \\{1, 2, \\dots, K\\}$, the probability is given by:\n$$\nP(X \\in A, Z \\in B) = \\sum_{k \\in B} \\int_{A} f_{X,Z}(x,k) \\, d\\lambda^{d}(x)\n$$\n\nThe derivation of the joint PDF $f_{X,Z}(x,k)$ proceeds from the chain rule of probability densities, which states that the joint density is the product of the marginal density of one variable and the conditional density of the other.\n$$\nf_{X,Z}(x,k) = f_{X|Z}(x|k) p_{Z}(k)\n$$\nHere, $p_{Z}(k)$ is the probability mass function (PMF) of the discrete variable $Z$, and $f_{X|Z}(x|k)$ is the conditional PDF of $X$ given $Z=k$.\n\nFrom the problem statement, we are given the following:\n$1$. The PMF of $Z$ is $p_{Z}(k) = P(Z=k) = \\pi_{k}$, where $\\pi_{k} > 0$ for all $k \\in \\{1, 2, \\dots, K\\}$ and $\\sum_{k=1}^{K} \\pi_{k} = 1$.\n$2$. The conditional distribution of $X$ given $Z=k$ is a multivariate normal distribution, $X \\mid (Z=k) \\sim \\mathcal{N}(\\mu_{k}, \\Sigma_{k})$. The PDF for this distribution is:\n$$\nf_{X|Z}(x|k) = \\frac{1}{(2\\pi)^{d/2} \\sqrt{\\det(\\Sigma_{k})}} \\exp\\left(-\\frac{1}{2}(x-\\mu_{k})^{T}\\Sigma_{k}^{-1}(x-\\mu_{k})\\right)\n$$\nwhere $\\mu_{k} \\in \\mathbb{R}^{d}$ is the mean vector and $\\Sigma_{k} \\in \\mathbb{R}^{d \\times d}$ is the symmetric positive definite covariance matrix. The term $\\det(\\Sigma_{k})$ denotes the determinant of the covariance matrix. Since $\\Sigma_k$ is positive definite, its determinant is strictly positive.\n\nBy substituting these two components into the chain rule formula, we obtain the expression for the joint PDF $f_{X,Z}(x,k)$:\n$$\nf_{X,Z}(x,k) = \\pi_{k} \\frac{1}{(2\\pi)^{d/2} \\sqrt{\\det(\\Sigma_{k})}} \\exp\\left(-\\frac{1}{2}(x-\\mu_{k})^{T}\\Sigma_{k}^{-1}(x-\\mu_{k})\\right)\n$$\nThis function is the density of the joint distribution of $(X,Z)$ with respect to the product of the Lebesgue and counting measures.\n\nNext, we derive the marginal PDF of the continuous random vector $X$, denoted $f_{X}(x)$. According to the law of total probability, the marginal density is obtained by integrating (or summing, in the case of discrete variables) the joint density over all possible values of the other variable(s). For our mixed-type distribution, this involves summing $f_{X,Z}(x,k)$ over all possible values of $k$ from $1$ to $K$.\n$$\nf_{X}(x) = \\sum_{k=1}^{K} f_{X,Z}(x,k)\n$$\nThis operation is the correct way to marginalize out the discrete variable $Z$ when working with a density defined with respect to the product of a Lebesgue and a counting measure.\n\nSubstituting the expression for the joint PDF $f_{X,Z}(x,k)$ that we just derived into the summation, we get:\n$$\nf_{X}(x) = \\sum_{k=1}^{K} \\left( \\pi_{k} \\frac{1}{(2\\pi)^{d/2} \\sqrt{\\det(\\Sigma_{k})}} \\exp\\left(-\\frac{1}{2}(x-\\mu_{k})^{T}\\Sigma_{k}^{-1}(x-\\mu_{k})\\right) \\right)\n$$\nThis expression represents the marginal PDF of $X$. It is a weighted sum of $K$ different multivariate normal densities, where the weights are the prior probabilities $\\pi_{k}$ of the latent variable $Z$. This is the defining equation for a Gaussian Mixture Model (GMM). Each term in the sum corresponds to a component of the mixture.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nf_{X,Z}(x,k) = \\pi_{k} \\frac{1}{(2\\pi)^{d/2} \\sqrt{\\det(\\Sigma_{k})}} \\exp\\left(-\\frac{1}{2}(x-\\mu_{k})^{T}\\Sigma_{k}^{-1}(x-\\mu_{k})\\right) \\\\\nf_{X}(x) = \\sum_{k=1}^{K} \\pi_{k} \\frac{1}{(2\\pi)^{d/2} \\sqrt{\\det(\\Sigma_{k})}} \\exp\\left(-\\frac{1}{2}(x-\\mu_{k})^{T}\\Sigma_{k}^{-1}(x-\\mu_{k})\\right)\n\\end{pmatrix}\n}\n$$", "id": "3315548"}]}