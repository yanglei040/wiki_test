{"hands_on_practices": [{"introduction": "A cornerstone of stochastic simulation is the ability to generate random variates from any specified distribution. This exercise [@problem_id:3296921] explores the versatile inverse transform sampling method by applying it to a zero-truncated Poisson distribution. You will first derive the properties of this conditional distribution and then construct an elegant and efficient sampler by cleverly adapting the original Poisson Cumulative Distribution Function (CDF).", "problem": "Let $X$ be a $\\mathrm{Poisson}(\\lambda)$ random variable with parameter $\\lambda \\in (0,\\infty)$, and let $Y$ denote the conditional random variable obtained by truncating at zero, $Y := X \\mid X > 0$, which is known as the zero-truncated Poisson. Begin from the fundamental definition of the $\\mathrm{Poisson}(\\lambda)$ distribution’s Probability Mass Function (pmf), $P(X = k)$ for integer $k \\geq 0$, and from the definition of conditional probability for discrete random variables. Derive the pmf of $Y$ on the support $\\{1,2,3,\\dots\\}$ and then derive the mean $\\mathbb{E}[Y]$ as a closed-form function of $\\lambda$. Next, describe an inversion-based simulator using the inverse transform sampling (ITS) method that uses a single $\\mathrm{Uniform}$ on $(0,1)$ random variate and sequential accumulation of the original Poisson distribution’s Cumulative Distribution Function (CDF) to produce one sample from $Y$; explicitly state the truncated CDF used for inversion and the transformed threshold that is compared to the original Poisson CDF in order to implement the truncation, and justify why the construction is correct. Provide, as your final answer, the closed-form expression for $\\mathbb{E}[Y]$ in terms of $\\lambda$.", "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- $X$ is a $\\mathrm{Poisson}(\\lambda)$ random variable with parameter $\\lambda \\in (0,\\infty)$.\n- The Probability Mass Function (pmf) of $X$ is $P(X = k)$ for integer $k \\geq 0$.\n- $Y$ is the conditional random variable $Y := X \\mid X > 0$, known as the zero-truncated Poisson.\n- The support of $Y$ is $\\{1,2,3,\\dots\\}$.\n- The task is to derive the pmf of $Y$.\n- The task is to derive the mean $\\mathbb{E}[Y]$ as a closed-form function of $\\lambda$.\n- The task is to describe an inversion-based simulator for $Y$ using the inverse transform sampling (ITS) method.\n- The simulator must use a single $\\mathrm{Uniform}$ on $(0,1)$ random variate.\n- The simulator must use sequential accumulation of the original Poisson distribution's Cumulative Distribution Function (CDF).\n- The task requires explicitly stating the truncated CDF used for inversion.\n- The task requires explicitly stating the transformed threshold.\n- The task requires justifying the correctness of the simulation method.\n- The final answer is the closed-form expression for $\\mathbb{E}[Y]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, as it deals with standard concepts in probability theory, namely the Poisson distribution, conditional probability, expectation, and the inverse transform sampling method. It is well-posed, providing a clear and complete set of instructions for deriving well-defined mathematical objects (a pmf, an expectation, and an algorithm). The problem is objective and uses precise mathematical language. It is self-contained and free of contradictions. The requested derivations are non-trivial and test fundamental understanding of the topic. Thus, the problem is valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\n### Derivation of the PMF of Y\nLet $X$ be a random variable following a Poisson distribution with parameter $\\lambda > 0$. Its Probability Mass Function (pmf) is given by:\n$$ P(X=k) = \\frac{\\lambda^k \\exp(-\\lambda)}{k!} \\quad \\text{for } k \\in \\{0, 1, 2, \\dots\\} $$\nThe random variable $Y$ is defined as $Y := X \\mid X > 0$. The support of $Y$ is the set of positive integers $\\{1, 2, 3, \\dots\\}$. The pmf of $Y$, denoted $P(Y=k)$, can be derived using the definition of conditional probability. For any integer $k \\geq 1$:\n$$ P(Y=k) = P(X=k \\mid X > 0) = \\frac{P(X=k \\text{ and } X>0)}{P(X>0)} $$\nSince the support of $Y$ is $k \\geq 1$, the event $\\{X=k\\}$ is a subset of the event $\\{X>0\\}$. Therefore, the intersection of these events is simply $\\{X=k\\}$. The expression simplifies to:\n$$ P(Y=k) = \\frac{P(X=k)}{P(X>0)} $$\nThe denominator, $P(X>0)$, is calculated as $1 - P(X=0)$. For the Poisson distribution:\n$$ P(X=0) = \\frac{\\lambda^0 \\exp(-\\lambda)}{0!} = \\exp(-\\lambda) $$\nThus, $P(X>0) = 1 - \\exp(-\\lambda)$.\nSubstituting this into the expression for the pmf of $Y$, we obtain:\n$$ P(Y=k) = \\frac{\\frac{\\lambda^k \\exp(-\\lambda)}{k!}}{1 - \\exp(-\\lambda)} = \\frac{\\lambda^k \\exp(-\\lambda)}{k! (1 - \\exp(-\\lambda))} \\quad \\text{for } k \\in \\{1, 2, 3, \\dots\\} $$\nThis is the pmf of the zero-truncated Poisson distribution.\n\n### Derivation of the Mean of Y\nThe mean (or expectation) of $Y$, denoted $\\mathbb{E}[Y]$, is calculated by summing the product of each possible value $k$ and its corresponding probability $P(Y=k)$ over the support of $Y$:\n$$ \\mathbb{E}[Y] = \\sum_{k=1}^{\\infty} k \\cdot P(Y=k) = \\sum_{k=1}^{\\infty} k \\cdot \\frac{\\lambda^k \\exp(-\\lambda)}{k! (1 - \\exp(-\\lambda))} $$\nWe can factor the constant terms out of the summation:\n$$ \\mathbb{E}[Y] = \\frac{\\exp(-\\lambda)}{1 - \\exp(-\\lambda)} \\sum_{k=1}^{\\infty} k \\frac{\\lambda^k}{k!} $$\nFor $k \\geq 1$, we can simplify the term inside the summation: $k/k! = 1/(k-1)!$.\n$$ \\mathbb{E}[Y] = \\frac{\\exp(-\\lambda)}{1 - \\exp(-\\lambda)} \\sum_{k=1}^{\\infty} \\frac{\\lambda^k}{(k-1)!} $$\nWe can factor out a $\\lambda$ from the summation:\n$$ \\mathbb{E}[Y] = \\frac{\\exp(-\\lambda)}{1 - \\exp(-\\lambda)} \\lambda \\sum_{k=1}^{\\infty} \\frac{\\lambda^{k-1}}{(k-1)!} $$\nLet $j = k-1$. As $k$ goes from $1$ to $\\infty$, $j$ goes from $0$ to $\\infty$. The sum becomes:\n$$ \\sum_{j=0}^{\\infty} \\frac{\\lambda^j}{j!} $$\nThis is the Taylor series expansion for $\\exp(\\lambda)$. Therefore, the sum is equal to $\\exp(\\lambda)$.\nSubstituting this result back into the expression for $\\mathbb{E}[Y]$:\n$$ \\mathbb{E}[Y] = \\frac{\\exp(-\\lambda)}{1 - \\exp(-\\lambda)} \\cdot \\lambda \\exp(\\lambda) = \\frac{\\lambda \\exp(-\\lambda)\\exp(\\lambda)}{1 - \\exp(-\\lambda)} = \\frac{\\lambda}{1 - \\exp(-\\lambda)} $$\nThis is the closed-form expression for the mean of the zero-truncated Poisson distribution.\n\n### Inversion-Based Simulator for Y\nThe inverse transform sampling (ITS) method generates a sample from a random variable by applying the inverse of its cumulative distribution function (CDF) to a uniform random variate. Let $U \\sim \\mathrm{Uniform}(0,1)$. A sample $y$ from $Y$ is given by $y = F_Y^{-1}(U)$, where $F_Y$ is the CDF of $Y$ and $F_Y^{-1}(u) = \\min\\{k \\in \\{1,2,\\dots\\} : F_Y(k) \\geq u\\}$.\n\nFirst, we express the CDF of $Y$, $F_Y(k) = P(Y \\leq k)$, in terms of the CDF of $X$, $F_X(k) = P(X \\leq k)$. For $k \\in \\{1, 2, 3, \\dots\\}$:\n$$ F_Y(k) = P(X \\leq k \\mid X>0) = \\frac{P(1 \\leq X \\leq k)}{P(X>0)} $$\nThe numerator can be written as $P(X \\leq k) - P(X=0) = F_X(k) - F_X(0)$. The denominator is $1 - P(X=0) = 1 - F_X(0)$.\nThe **truncated CDF used for inversion** is therefore:\n$$ F_Y(k) = \\frac{F_X(k) - F_X(0)}{1 - F_X(0)} $$\nThe ITS algorithm requires finding the smallest integer $y \\geq 1$ such that $F_Y(y) \\geq U$. Substituting the expression for $F_Y(y)$:\n$$ \\frac{F_X(y) - F_X(0)}{1 - F_X(0)} \\geq U $$\nRearranging the inequality to solve for $F_X(y)$:\n$$ F_X(y) - F_X(0) \\geq U(1 - F_X(0)) $$\n$$ F_X(y) \\geq F_X(0) + U(1 - F_X(0)) $$\nLet's define a **transformed threshold** $U'$ as:\n$$ U' = F_X(0) + U(1 - F_X(0)) $$\nSince $F_X(0) = P(X=0) = \\exp(-\\lambda)$, we have $U' = \\exp(-\\lambda) + U(1 - \\exp(-\\lambda))$. As $U$ is uniformly distributed on $(0,1)$, $U'$ is uniformly distributed on the interval $(\\exp(-\\lambda), 1)$.\n\nThe simulation algorithm is as follows:\n1. Generate a random variate $U \\sim \\mathrm{Uniform}(0,1)$.\n2. Compute the transformed threshold $U' = \\exp(-\\lambda) + U(1 - \\exp(-\\lambda))$.\n3. Find the smallest integer $y \\in \\{1, 2, 3, \\dots\\}$ such that $F_X(y) \\geq U'$. This can be done by sequential accumulation of the original Poisson probabilities:\n    a. Initialize probability sum $S \\leftarrow P(X=0) = \\exp(-\\lambda)$ and counter $k \\leftarrow 0$.\n    b. While $S < U'$:\n        i. Increment the counter: $k \\leftarrow k+1$.\n        ii. Update the sum: $S \\leftarrow S + P(X=k) = S + \\frac{\\lambda^k \\exp(-\\lambda)}{k!}$. Note that $P(X=k)$ can be computed iteratively: $P(X=k) = P(X=k-1) \\cdot \\frac{\\lambda}{k}$.\n    c. The generated sample is $y=k$.\n\n**Justification:** The method is correct because the condition $F_Y(y) \\geq U$ is algebraically equivalent to $F_X(y) \\geq U'$. The algorithm implements a search for the smallest integer $y$ that satisfies this latter condition. The probability of the algorithm returning a specific integer $k \\geq 1$ is the probability that the generated $U'$ falls into the interval $(F_X(k-1), F_X(k)]$. The length of this interval is $F_X(k) - F_X(k-1) = P(X=k)$. The total length of the interval from which $U'$ is drawn is $1 - F_X(0) = P(X>0)$. Therefore, the probability that the algorithm outputs $k$ is:\n$$ P(\\text{sample is } k) = \\frac{\\text{length of interval for } k}{\\text{total length of sample space}} = \\frac{P(X=k)}{P(X>0)} = P(Y=k) $$\nThis confirms that the algorithm correctly samples from the distribution of $Y$. Since $U' > \\exp(-\\lambda) = F_X(0)$, the loop will always execute at least once, ensuring the output $y$ is at least $1$, which respects the support of $Y$.", "answer": "$$\n\\boxed{\\frac{\\lambda}{1 - \\exp(-\\lambda)}}\n$$", "id": "3296921"}, {"introduction": "While standard Monte Carlo methods provide unbiased estimates, their precision can be poor. This practice [@problem_id:3296919] introduces the control variates technique, a method for reducing variance by exploiting the correlation between our target quantity and another variable with a known expectation. By working through the estimation of a simple Bernoulli probability, you will derive the optimal control coefficient and see precisely how this technique enhances simulation efficiency.", "problem": "Consider a single draw $U$ from the $\\mathrm{Uniform}(0,1)$ distribution and a fixed parameter $p \\in (0,1)$. Define the Bernoulli random variable $X=\\mathbf{1}\\{U \\le p\\}$, and the control variate $g(U)=U$, whose expectation $\\mathbb{E}[g(U)]$ is known. Define the control variate estimator\n$$\nZ_{c} \\;=\\; X \\;-\\; c\\big(g(U)-\\mathbb{E}[g(U)]\\big).\n$$\nStarting from the definitions of expectation, variance, and covariance, and without using any pre-derived control variate formulas, derive the following in closed form as functions of $p$:\n- the optimal control coefficient $c^{\\star}$ that minimizes $\\mathrm{Var}(Z_{c})$;\n- the corresponding variance reduction factor, defined as the ratio $\\mathrm{Var}(Z_{c^{\\star}})/\\mathrm{Var}(X)$.\n\nExpress your final answer as two closed-form analytic expressions in terms of $p$. No numerical rounding is required.", "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It is a standard problem in the field of Monte Carlo methods that can be solved from first principles as requested.\n\nThe control variate estimator is given by\n$$\nZ_{c} \\;=\\; X \\;-\\; c\\big(g(U)-\\mathbb{E}[g(U)]\\big)\n$$\nwhere $X = \\mathbf{1}\\{U \\le p\\}$ with $U \\sim \\mathrm{Uniform}(0,1)$, $p \\in (0,1)$, and the control variate is $g(U)=U$. Our objective is to find the value of the coefficient $c$, denoted $c^{\\star}$, that minimizes the variance of $Z_{c}$, and then to find the corresponding variance reduction factor.\n\nFirst, we express the variance of $Z_{c}$ using the properties of the variance operator. Let $Y = g(U) = U$. The estimator is $Z_c = X - c(Y - \\mathbb{E}[Y])$.\n$$\n\\mathrm{Var}(Z_c) = \\mathrm{Var}\\left(X - c(Y - \\mathbb{E}[Y])\\right)\n$$\nUsing the identity $\\mathrm{Var}(A - B) = \\mathrm{Var}(A) + \\mathrm{Var}(B) - 2\\mathrm{Cov}(A, B)$, we have:\n$$\n\\mathrm{Var}(Z_c) = \\mathrm{Var}(X) + \\mathrm{Var}\\left(c(Y - \\mathbb{E}[Y])\\right) - 2\\mathrm{Cov}\\left(X, c(Y - \\mathbb{E}[Y])\\right)\n$$\nUsing the properties $\\mathrm{Var}(aZ) = a^2\\mathrm{Var}(Z)$ and $\\mathrm{Cov}(Z_1, aZ_2) = a\\mathrm{Cov}(Z_1, Z_2)$, this becomes:\n$$\n\\mathrm{Var}(Z_c) = \\mathrm{Var}(X) + c^2\\mathrm{Var}(Y - \\mathbb{E}[Y]) - 2c\\mathrm{Cov}(X, Y - \\mathbb{E}[Y])\n$$\nSince $\\mathrm{Var}(Y - k) = \\mathrm{Var}(Y)$ and $\\mathrm{Cov}(X, Y - k) = \\mathrm{Cov}(X,Y)$ for any constant $k$, the expression simplifies to:\n$$\n\\mathrm{Var}(Z_c) = \\mathrm{Var}(X) + c^2\\mathrm{Var}(Y) - 2c\\mathrm{Cov}(X, Y)\n$$\nThis is a quadratic function of $c$. To find the minimum, we take the derivative with respect to $c$ and set it to $0$:\n$$\n\\frac{d}{dc}\\mathrm{Var}(Z_c) = 2c\\mathrm{Var}(Y) - 2\\mathrm{Cov}(X, Y)\n$$\nSetting the derivative to $0$ for the optimal coefficient $c^{\\star}$:\n$$\n2c^{\\star}\\mathrm{Var}(Y) - 2\\mathrm{Cov}(X, Y) = 0 \\implies c^{\\star} = \\frac{\\mathrm{Cov}(X, Y)}{\\mathrm{Var}(Y)}\n$$\nTo find the explicit value of $c^{\\star}$, we must compute $\\mathrm{Var}(Y)$ and $\\mathrm{Cov}(X, Y)$.\n\n1.  **Moments of the control variate** $Y=U$:\n    Since $U \\sim \\mathrm{Uniform}(0,1)$, its probability density function is $f_U(u) = 1$ for $u \\in [0,1]$.\n    The expectation is $\\mathbb{E}[Y] = \\mathbb{E}[U] = \\int_0^1 u \\cdot 1 \\,du = \\frac{1}{2}$.\n    The variance is $\\mathrm{Var}(Y) = \\mathrm{Var}(U) = \\mathbb{E}[U^2] - (\\mathbb{E}[U])^2$.\n    $\\mathbb{E}[U^2] = \\int_0^1 u^2 \\cdot 1 \\,du = \\frac{1}{3}$.\n    So, $\\mathrm{Var}(Y) = \\frac{1}{3} - (\\frac{1}{2})^2 = \\frac{1}{3} - \\frac{1}{4} = \\frac{1}{12}$.\n\n2.  **Moments of the estimator** $X = \\mathbf{1}\\{U \\le p\\}$:\n    $X$ is a Bernoulli random variable, since it takes the value $1$ if the event $\\{U \\le p\\}$ occurs and $0$ otherwise.\n    The probability of success is $\\mathbb{P}(X=1) = \\mathbb{P}(U \\le p) = \\int_0^p 1 \\,du = p$.\n    Thus, $X \\sim \\mathrm{Bernoulli}(p)$.\n    The expectation is $\\mathbb{E}[X] = p$.\n    The variance is $\\mathrm{Var}(X) = p(1-p)$.\n\n3.  **Covariance between** $X$ **and** $Y$:\n    $\\mathrm{Cov}(X, Y) = \\mathrm{Cov}(X, U) = \\mathbb{E}[XU] - \\mathbb{E}[X]\\mathbb{E}[U]$.\n    We need to compute $\\mathbb{E}[XU] = \\mathbb{E}[\\mathbf{1}\\{U \\le p\\} \\cdot U]$.\n    Using the law of the unconscious statistician:\n    $$\n    \\mathbb{E}[XU] = \\int_0^1 (\\mathbf{1}\\{u \\le p\\} \\cdot u) f_U(u) \\,du = \\int_0^1 \\mathbf{1}\\{u \\le p\\} \\cdot u \\,du = \\int_0^p u \\,du\n    $$\n    $$\n    \\mathbb{E}[XU] = \\left[\\frac{u^2}{2}\\right]_0^p = \\frac{p^2}{2}\n    $$\n    Now, we can compute the covariance:\n    $$\n    \\mathrm{Cov}(X, Y) = \\frac{p^2}{2} - (p)\\left(\\frac{1}{2}\\right) = \\frac{p^2 - p}{2} = -\\frac{p(1-p)}{2}\n    $$\n\n4.  **Optimal coefficient** $c^{\\star}$:\n    Substituting the calculated variance and covariance into the expression for $c^{\\star}$:\n    $$\n    c^{\\star} = \\frac{\\mathrm{Cov}(X, Y)}{\\mathrm{Var}(Y)} = \\frac{-p(1-p)/2}{1/12} = -\\frac{p(1-p)}{2} \\cdot 12 = -6p(1-p)\n    $$\n    This is the first required expression.\n\nNext, we derive the variance reduction factor, $\\mathrm{Var}(Z_{c^{\\star}})/\\mathrm{Var}(X)$.\nFirst, we find the minimal variance $\\mathrm{Var}(Z_{c^{\\star}})$ by substituting $c^{\\star}$ back into the variance formula:\n$$\n\\mathrm{Var}(Z_{c^{\\star}}) = \\mathrm{Var}(X) + (c^{\\star})^2\\mathrm{Var}(Y) - 2c^{\\star}\\mathrm{Cov}(X, Y)\n$$\n$$\n\\mathrm{Var}(Z_{c^{\\star}}) = \\mathrm{Var}(X) + \\left(\\frac{\\mathrm{Cov}(X, Y)}{\\mathrm{Var}(Y)}\\right)^2\\mathrm{Var}(Y) - 2\\frac{\\mathrm{Cov}(X, Y)}{\\mathrm{Var}(Y)}\\mathrm{Cov}(X, Y)\n$$\n$$\n\\mathrm{Var}(Z_{c^{\\star}}) = \\mathrm{Var}(X) + \\frac{\\mathrm{Cov}(X, Y)^2}{\\mathrm{Var}(Y)} - 2\\frac{\\mathrm{Cov}(X, Y)^2}{\\mathrm{Var}(Y)} = \\mathrm{Var}(X) - \\frac{\\mathrm{Cov}(X, Y)^2}{\\mathrm{Var}(Y)}\n$$\nThis result can be expressed in terms of the squared correlation coefficient, $\\rho_{XY}^2 = \\frac{\\mathrm{Cov}(X,Y)^2}{\\mathrm{Var}(X)\\mathrm{Var}(Y)}$.\n$$\n\\mathrm{Var}(Z_{c^{\\star}}) = \\mathrm{Var}(X) - \\frac{\\rho_{XY}^2 \\mathrm{Var}(X)\\mathrm{Var}(Y)}{\\mathrm{Var}(Y)} = \\mathrm{Var}(X)(1 - \\rho_{XY}^2)\n$$\nThe variance reduction factor is therefore:\n$$\n\\frac{\\mathrm{Var}(Z_{c^{\\star}})}{\\mathrm{Var}(X)} = 1 - \\rho_{XY}^2\n$$\nNow we compute $\\rho_{XY}^2$ using our previously derived quantities:\n$$\n\\rho_{XY}^2 = \\frac{\\mathrm{Cov}(X, Y)^2}{\\mathrm{Var(X)}\\mathrm{Var}(Y)} = \\frac{\\left(-\\frac{p(1-p)}{2}\\right)^2}{p(1-p) \\cdot \\frac{1}{12}} = \\frac{\\frac{p^2(1-p)^2}{4}}{\\frac{p(1-p)}{12}}\n$$\nAssuming $p \\in (0,1)$, we have $p \\neq 0$ and $1-p \\neq 0$, so we can simplify:\n$$\n\\rho_{XY}^2 = \\frac{p^2(1-p)^2}{4} \\cdot \\frac{12}{p(1-p)} = 3p(1-p)\n$$\nFinally, the variance reduction factor is:\n$$\n\\frac{\\mathrm{Var}(Z_{c^{\\star}})}{\\mathrm{Var}(X)} = 1 - 3p(1-p)\n$$\nNote that since $p \\in (0,1)$, the term $p(1-p)$ has a maximum value of $1/4$ at $p=1/2$. Therefore, $0 < \\rho_{XY}^2 \\le 3/4$, which ensures that $1/4 \\le 1 - \\rho_{XY}^2 < 1$, confirming a strict reduction in variance.\n\nThe two closed-form expressions are $c^{\\star} = -6p(1-p)$ and the variance reduction factor $1 - 3p(1-p)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} -6p(1-p) & 1 - 3p(1-p) \\end{pmatrix}}\n$$", "id": "3296919"}, {"introduction": "Estimating the probability of rare events is a notoriously difficult task where naive simulation is computationally infeasible. This exercise [@problem_id:3296986] delves into importance sampling, a powerful variance reduction technique tailored for such problems, using the framework of exponential tilting. You will design an efficient estimator for a binomial tail probability by systematically changing the sampling distribution to make the rare event more frequent, and then correcting for this change with a likelihood ratio.", "problem": "Consider independent and identically distributed Bernoulli random variables $X_{1},\\dots,X_{n}$ with parameter $p\\in(0,1)$, so that $\\mathbb{P}(X_{i}=1)=p$ and $\\mathbb{P}(X_{i}=0)=1-p$. Let $S_{n}=\\sum_{i=1}^{n}X_{i}$ denote the binomial count with parameters $n$ and $p$. Fix an integer threshold $a\\in\\{0,1,\\dots,n\\}$ satisfying $a>np$ so that the upper tail probability $\\mathbb{P}(S_{n}\\ge a)$ is rare. The task is to construct an Importance Sampling (IS) scheme based on exponential tilting to estimate $\\mathbb{P}(S_{n}\\ge a)$ efficiently.\n\nStarting only from the following foundational elements:\n- the definition of Bernoulli and binomial distributions,\n- the definition of Monte Carlo (MC) estimation and Importance Sampling (IS) using the Radon–Nikodym derivative (likelihood ratio),\n- the moment generating function (MGF) and the cumulant generating function (CGF) for sums of independent random variables,\n\nderive an exponential tilting change of measure that replaces $p$ by a new parameter $p^{\\star}\\in(0,1)$, under which samples are drawn from an independent and identically distributed Bernoulli model with parameter $p^{\\star}$, and the IS estimator uses the corresponding likelihood ratio to remain unbiased for $\\mathbb{P}(S_{n}\\ge a)$. Explicitly:\n\n- Derive the likelihood ratio as a function of the realized count $s$ (where $s=S_{n}$ for a given sample), $n$, $p$, and $p^{\\star}$.\n- Using the cumulant generating function and the exponential tilting principle, derive the choice of $p^{\\star}$ that makes the tilted mean of $S_{n}$ equal to the threshold $a$, which asymptotically minimizes the variance growth rate for estimating $\\mathbb{P}(S_{n}\\ge a)$ in the class of independent and identically distributed Bernoulli tilts.\n\nYour final answer must be a single closed-form analytic expression. Express your final answer as the row matrix $\\begin{pmatrix}p^{\\star} & L(s)\\end{pmatrix}$, where $p^{\\star}$ is given in terms of $n$ and $a$, and $L(s)$ is the likelihood ratio expressed solely in terms of $n$, $p$, $a$, and $s$. No rounding is required, and no physical units are involved.", "solution": "The problem is valid, as it presents a standard and well-posed question in the field of rare event simulation. It requires the derivation of an importance sampling scheme based on exponential tilting, a foundational technique in Monte Carlo methods. The derivation can be performed from the first principles provided.\n\n### Derivation of the Likelihood Ratio\nLet $\\mathbb{P}_{p}$ denote the original probability measure, where $X_i$ are i.i.d. Bernoulli($p$). Let $\\mathbb{P}_{p^{\\star}}$ be the new (tilted) measure, where $X_i$ are i.i.d. Bernoulli($p^{\\star}$). For a sequence of outcomes $\\mathbf{x} = (x_1, \\dots, x_n)$ with a sum $s = \\sum_{i=1}^n x_i$, the probability mass function under each measure is:\n$$\n\\mathbb{P}_{p}(\\mathbf{x}) = p^s(1-p)^{n-s}\n$$\n$$\n\\mathbb{P}_{p^{\\star}}(\\mathbf{x}) = (p^{\\star})^s(1-p^{\\star})^{n-s}\n$$\nThe importance sampling estimator for $P_a = \\mathbb{E}_p[\\mathbf{1}_{\\{S_n \\ge a\\}}]$ is $\\mathbb{E}_{p^\\star}[ \\mathbf{1}_{\\{S_n \\ge a\\}} L(S_n) ]$, where $L(s)$ is the likelihood ratio (Radon-Nikodym derivative) $\\frac{d\\mathbb{P}_{p}}{d\\mathbb{P}_{p^{\\star}}}$.\nThe likelihood ratio $L(s)$ for a realized count $s$ is:\n$$\nL(s) = \\frac{\\mathbb{P}_{p}(\\mathbf{x})}{\\mathbb{P}_{p^{\\star}}(\\mathbf{x})} = \\frac{p^s(1-p)^{n-s}}{(p^{\\star})^s(1-p^{\\star})^{n-s}} = \\left(\\frac{p}{p^{\\star}}\\right)^s \\left(\\frac{1-p}{1-p^{\\star}}\\right)^{n-s}\n$$\nThis is the likelihood ratio as a function of $s, n, p,$ and $p^{\\star}$.\n\n### Derivation of the Optimal Tilting Parameter $p^{\\star}$\nThe principle of exponential tilting introduces a new measure via a parameter $\\theta$, where the new probability of an outcome is proportional to its original probability times an exponential factor. The moment generating function (MGF) of a single Bernoulli($p$) variable is $M_X(\\theta) = \\mathbb{E}_p[e^{\\theta X}] = (1-p) + p e^{\\theta}$. The cumulant generating function (CGF) is $K_X(\\theta) = \\ln M_X(\\theta)$.\nFor the sum $S_n = \\sum X_i$, the CGF is $K_{S_n}(\\theta) = n K_X(\\theta) = n \\ln(1-p + p e^{\\theta})$.\n\nThe new probability parameter $p^{\\star}$ under this tilted measure corresponds to the tilted mean of a single variable, which is given by the derivative of its CGF:\n$$\np^{\\star} = K'_X(\\theta) = \\frac{p e^{\\theta}}{1-p + p e^{\\theta}}\n$$\nThe mean of the sum $S_n$ under this new measure is $\\mathbb{E}_{p^{\\star}}[S_n] = K'_{S_n}(\\theta) = n K'_X(\\theta) = np^{\\star}$.\n\nTo optimize the importance sampling estimator, we choose $p^{\\star}$ such that the new mean is centered on the rare event threshold, $a$. This is a standard heuristic that asymptotically minimizes the variance.\n$$\n\\mathbb{E}_{p^{\\star}}[S_n] = a \\implies np^{\\star} = a\n$$\nSolving for $p^{\\star}$ gives the required parameter:\n$$\np^{\\star} = \\frac{a}{n}\n$$\n\n### Final Likelihood Ratio Expression\nWe now substitute this choice of $p^{\\star} = a/n$ into our expression for the likelihood ratio $L(s)$:\n$$\nL(s) = \\left(\\frac{p}{a/n}\\right)^{s} \\left(\\frac{1-p}{1-a/n}\\right)^{n-s} = \\left(\\frac{np}{a}\\right)^{s} \\left(\\frac{1-p}{(n-a)/n}\\right)^{n-s}\n$$\n$$\nL(s) = \\left(\\frac{np}{a}\\right)^{s} \\left(\\frac{n(1-p)}{n-a}\\right)^{n-s}\n$$\nThis is the final expression for the likelihood ratio, expressed solely in terms of $n, p, a$, and the realized count $s$. The problem asks for the answer as the row matrix $\\begin{pmatrix}p^{\\star} & L(s)\\end{pmatrix}$, where $L(s)$ is this final form.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{a}{n} & \\left(\\frac{np}{a}\\right)^{s} \\left(\\frac{n(1-p)}{n-a}\\right)^{n-s}\n\\end{pmatrix}\n}\n$$", "id": "3296986"}]}