{"hands_on_practices": [{"introduction": "The Central Limit Theorem is a cornerstone of Monte Carlo analysis, but we are often interested in the properties of a function of a sample mean, not just the mean itself. The Delta Method is an indispensable tool that extends the power of the CLT to transformed estimators by using a simple first-order Taylor approximation. This exercise ([@problem_id:3317830]) provides fundamental practice in applying the Delta Method to find the asymptotic variance of the logarithm of a sample mean, a common and important case study.", "problem": "Consider a Monte Carlo setting in which one simulates $n$ independent and identically distributed positive random variables $\\{X_{i}\\}_{i=1}^{n}$ with common mean $\\mu = \\mathbb{E}[X_{1}]$ satisfying $\\mu \\in (0,\\infty)$ and variance $\\sigma^{2} = \\operatorname{Var}(X_{1})$ satisfying $\\sigma^{2} \\in (0,\\infty)$. Let $T_{n} = \\bar{X}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$ and define the transformed estimator $Y_{n} = \\log(T_{n})$. Starting from the Weak Law of Large Numbers (WLLN) and the Central Limit Theorem (CLT), and using only first principles such as differentiability and Taylor expansion of a smooth function, apply the delta method to the function $g(x) = \\log x$ at the point $x = \\mu$ to derive the leading-order asymptotic variance of $Y_{n}$ as $n \\to \\infty$. Express your final answer in closed form as a function of $\\mu$, $\\sigma^{2}$, and $n$. No numerical evaluation is required, and no rounding is needed. The final answer must be a single analytic expression.", "solution": "The problem is first validated against the required criteria.\n\n### Step 1: Extract Givens\n- A set of $n$ independent and identically distributed (i.i.d.) positive random variables $\\{X_{i}\\}_{i=1}^{n}$.\n- The common mean is $\\mu = \\mathbb{E}[X_{1}]$, with $\\mu \\in (0,\\infty)$.\n- The common variance is $\\sigma^{2} = \\operatorname{Var}(X_{1})$, with $\\sigma^{2} \\in (0,\\infty)$.\n- The sample mean is $T_{n} = \\bar{X}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$.\n- A transformed estimator is defined as $Y_{n} = \\log(T_{n})$.\n- The task is to apply the delta method to the function $g(x) = \\log x$ at the point $x = \\mu$.\n- The derivation must start from the Weak Law of Large Numbers (WLLN) and the Central Limit Theorem (CLT) and use first principles like Taylor expansion.\n- The objective is to find the leading-order asymptotic variance of $Y_{n}$ as $n \\to \\infty$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It is a standard application of limit theorems in statistics.\n1.  **Scientific Soundness**: The problem rests on fundamental principles of probability theory (WLLN, CLT) and calculus (Taylor expansion). The assumptions ($\\mu > 0$, $\\sigma^2 > 0$, $X_i > 0$) are consistent and standard for this type of analysis. The function $g(x) = \\log(x)$ is well-defined and differentiable for the domain of interest, since $X_i > 0$ implies $T_n > 0$ and it is given that $\\mu > 0$.\n2.  **Well-Posedness**: The problem is clearly stated and has a unique, meaningful solution obtainable via the specified method (delta method). All necessary information is provided.\n3.  **Objectivity**: The problem is stated in precise mathematical language, free of any subjectivity or ambiguity.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Derivation\nThe problem asks for the leading-order asymptotic variance of $Y_n = \\log(T_n)$, where $T_n = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$ is the sample mean of $n$ i.i.d. positive random variables with mean $\\mu$ and variance $\\sigma^2$.\n\nThe foundation of our analysis rests on two key limit theorems for the sample mean $T_n$:\n1.  The Weak Law of Large Numbers (WLLN) states that the sample mean converges in probability to the true mean $\\mu$. We denote this as:\n    $$T_n \\xrightarrow{p} \\mu \\quad \\text{as } n \\to \\infty$$\n2.  The Central Limit Theorem (CLT) describes the distribution of the standardized sample mean. It states that:\n    $$\\sqrt{n}(T_n - \\mu) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2) \\quad \\text{as } n \\to \\infty$$\n    where `d` denotes convergence in distribution, and $\\mathcal{N}(0, \\sigma^2)$ is a normal distribution with mean $0$ and variance $\\sigma^2$.\n\nWe are interested in the behavior of the transformed random variable $Y_n = g(T_n)$, where the function is $g(x) = \\log(x)$. The problem directs us to use the delta method, which we will derive from a first-order Taylor expansion of $g(T_n)$ around the point $x=\\mu$.\n\nSince we are given $\\mu \\in (0, \\infty)$, and the WLLN tells us $T_n$ will be close to $\\mu$ for large $n$, we can expand $g(T_n)$ around $\\mu$. The function $g(x) = \\log(x)$ is continuously differentiable for $x > 0$. Its first derivative is $g'(x) = \\frac{1}{x}$.\n\nThe first-order Taylor expansion of $g(T_n)$ around $\\mu$ is:\n$$g(T_n) = g(\\mu) + g'(\\mu)(T_n - \\mu) + R_n$$\nwhere $R_n$ is the remainder term, which satisfies $\\frac{R_n}{T_n - \\mu} \\to 0$ as $T_n \\to \\mu$.\n\nSubstituting $g(x) = \\log(x)$ and its derivative $g'(x) = \\frac{1}{x}$:\n$$Y_n = \\log(T_n) = \\log(\\mu) + \\frac{1}{\\mu}(T_n - \\mu) + R_n$$\nNote that $g'(\\mu) = \\frac{1}{\\mu}$ is well-defined and finite since $\\mu > 0$.\n\nTo analyze the asymptotic distribution, we rearrange the equation and scale by $\\sqrt{n}$:\n$$\\sqrt{n}(Y_n - \\log(\\mu)) = \\sqrt{n}(\\log(T_n) - \\log(\\mu)) = \\frac{1}{\\mu} \\sqrt{n}(T_n - \\mu) + \\sqrt{n} R_n$$\n\nAccording to the properties of the Taylor remainder and the fact that $T_n \\xrightarrow{p} \\mu$, the term $\\sqrt{n}R_n$ converges in probability to $0$. Therefore, by Slutsky's theorem, the asymptotic distribution of the left-hand side is the same as the asymptotic distribution of the first term on the right-hand side.\n$$\\sqrt{n}(Y_n - \\log(\\mu)) \\stackrel{d}{\\longrightarrow} \\frac{1}{\\mu} \\left( \\sqrt{n}(T_n - \\mu) \\right)$$\n\nFrom the CLT, we know that the term $\\sqrt{n}(T_n - \\mu)$ converges in distribution to a random variable $Z \\sim \\mathcal{N}(0, \\sigma^2)$. Consequently, the term $\\frac{1}{\\mu}\\sqrt{n}(T_n - \\mu)$ converges in distribution to $\\frac{1}{\\mu}Z$.\n\nLet's find the distribution of $\\frac{1}{\\mu}Z$. Since $Z$ is a normal random variable, any linear transformation of it is also normal.\nThe mean is $\\mathbb{E}\\left[\\frac{1}{\\mu}Z\\right] = \\frac{1}{\\mu}\\mathbb{E}[Z] = \\frac{1}{\\mu} \\cdot 0 = 0$.\nThe variance is $\\operatorname{Var}\\left(\\frac{1}{\\mu}Z\\right) = \\left(\\frac{1}{\\mu}\\right)^2 \\operatorname{Var}(Z) = \\frac{1}{\\mu^2} \\sigma^2 = \\frac{\\sigma^2}{\\mu^2}$.\n\nThus, we have established the asymptotic distribution for the transformed estimator:\n$$\\sqrt{n}(Y_n - \\log(\\mu)) \\xrightarrow{d} \\mathcal{N}\\left(0, \\frac{\\sigma^2}{\\mu^2}\\right)$$\n\nThis result implies that for large $n$, the distribution of $Y_n = \\log(T_n)$ can be approximated by a normal distribution:\n$$Y_n \\approx \\mathcal{N}\\left(\\log(\\mu), \\frac{1}{n} \\frac{\\sigma^2}{\\mu^2}\\right)$$\n\nThe variance of this approximating normal distribution is what is referred to as the leading-order asymptotic variance of $Y_n$. It is an approximation to $\\operatorname{Var}(Y_n)$ for large $n$.\n$$\\operatorname{Var}(Y_n) \\approx \\frac{\\sigma^2}{n\\mu^2}$$\nThis expression is the required leading-order asymptotic variance of $Y_n$, expressed as a function of $\\mu$, $\\sigma^2$, and $n$.", "answer": "$$\\boxed{\\frac{\\sigma^2}{n\\mu^2}}$$", "id": "3317830"}, {"introduction": "Limit theorems are not only for analyzing the error of estimators; they are also crucial for designing more efficient simulation strategies. Stratified sampling is a powerful variance reduction technique that involves dividing a population into subgroups, but its effectiveness hinges on how we allocate our computational budget across these strata. This practice problem ([@problem_id:3317803]) guides you through the process of deriving the asymptotically optimal allocation of samples, a result known as Neyman allocation, by minimizing the asymptotic variance derived from a stratified Central Limit Theorem.", "problem": "Consider a stratified Monte Carlo estimator built from $H$ disjoint strata indexed by $h \\in \\{1,\\dots,H\\}$. In stratum $h$, let $\\{X_{h,i}\\}_{i \\geq 1}$ be an independent and identically distributed sequence with finite mean $E[X_{h,1}]=\\mu_h$, finite variance $\\operatorname{Var}(X_{h,1})=\\sigma_h^{2}$, and finite $(2+\\delta)$-th absolute moment $E[|X_{h,1}-\\mu_h|^{2+\\delta}]<\\infty$ for some fixed $\\delta>0$. Assume independence across strata. Let $w_h>0$ be fixed strata weights satisfying $\\sum_{h=1}^{H} w_h = 1$, and let $c_h>0$ be the per-sample cost in stratum $h$. For a total budget $C>0$, choose sample sizes $n_h(C)\\in\\mathbb{N}$ satisfying the exact budget constraint $\\sum_{h=1}^{H} c_h\\,n_h(C)=C$, and define the stratified estimator\n$$\n\\hat{\\mu}_{C} \\;=\\; \\sum_{h=1}^{H} w_h\\,\\bar{X}_{n_h(C),h},\n$$\nwhere $\\bar{X}_{n_h(C),h} = \\frac{1}{n_h(C)}\\sum_{i=1}^{n_h(C)} X_{h,i}$.\n\nStarting from core definitions of expectation, variance, independence, and the conditions of the Lindeberg–Feller Central Limit Theorem (CLT), derive the asymptotic normality of $\\sqrt{C}\\big(\\hat{\\mu}_{C}-\\sum_{h=1}^{H} w_h \\mu_h\\big)$ as $C\\to\\infty$ under any allocation sequence for which the budget fractions $b_h(C):=\\frac{c_h\\,n_h(C)}{C}$ converge to strictly positive limits $b_h\\in(0,1)$ with $\\sum_{h=1}^{H} b_h=1$. Express the limiting variance explicitly in terms of $\\{w_h,\\sigma_h^{2},c_h,b_h\\}_{h=1}^{H}$.\n\nThen, using first principles of constrained optimization and without invoking pre-packaged allocation rules, identify the limiting budget fractions $b_h$ that minimize the asymptotic variance of $\\sqrt{C}\\big(\\hat{\\mu}_{C}-\\sum_{h=1}^{H} w_h \\mu_h\\big)$ subject to $\\sum_{h=1}^{H} b_h=1$ and $b_h>0$, and give the resulting minimal asymptotic variance in closed form in terms of $\\{w_h,\\sigma_h,c_h\\}_{h=1}^{H}$.\n\nProvide only the final closed-form expression for this minimal asymptotic variance as your answer. No numerical evaluation is required; do not round. The final answer must be a single analytic expression.", "solution": "The problem is valid as it is a well-posed, scientifically grounded question in the field of stochastic simulation and Monte Carlo methods. All necessary conditions and definitions are provided, and there are no internal contradictions or ambiguities.\n\nWe are tasked with finding the asymptotic distribution of the stratified estimator $\\hat{\\mu}_{C}$ and then minimizing its asymptotic variance. Let $\\mu_{S} = E[\\hat{\\mu}_{C}]$. First, we compute the expectation of the estimator:\n$$\nE[\\hat{\\mu}_{C}] = E\\left[\\sum_{h=1}^{H} w_h \\bar{X}_{n_h(C),h}\\right] = \\sum_{h=1}^{H} w_h E[\\bar{X}_{n_h(C),h}] = \\sum_{h=1}^{H} w_h \\mu_h\n$$\nWe have identified the true mean of the estimator as $\\mu_{S} = \\sum_{h=1}^{H} w_h \\mu_h$. We are interested in the asymptotic behavior of $\\sqrt{C}(\\hat{\\mu}_{C} - \\mu_{S})$ as the total budget $C \\to \\infty$.\n\nLet us analyze the variance of $\\hat{\\mu}_{C}$. Due to the independence of samples across strata, the variance of the sum is the sum of the variances:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{C}) = \\operatorname{Var}\\left(\\sum_{h=1}^{H} w_h \\bar{X}_{n_h(C),h}\\right) = \\sum_{h=1}^{H} w_h^2 \\operatorname{Var}(\\bar{X}_{n_h(C),h})\n$$\nThe variance of the sample mean in stratum $h$ is $\\operatorname{Var}(\\bar{X}_{n_h(C),h}) = \\frac{\\sigma_h^2}{n_h(C)}$. Thus,\n$$\n\\operatorname{Var}(\\hat{\\mu}_{C}) = \\sum_{h=1}^{H} \\frac{w_h^2 \\sigma_h^2}{n_h(C)}\n$$\nWe are interested in the normalized random variable $\\sqrt{C}(\\hat{\\mu}_{C} - \\mu_{S})$. Its variance is:\n$$\n\\operatorname{Var}\\left(\\sqrt{C}(\\hat{\\mu}_{C} - \\mu_{S})\\right) = C \\operatorname{Var}(\\hat{\\mu}_{C}) = C \\sum_{h=1}^{H} \\frac{w_h^2 \\sigma_h^2}{n_h(C)}\n$$\nWe are given that the budget fractions $b_h(C) = \\frac{c_h n_h(C)}{C}$ converge to a limit $b_h \\in (0,1)$ as $C \\to \\infty$. This implies that $\\frac{n_h(C)}{C} \\to \\frac{b_h}{c_h}$.\nLet us now find the limit of the variance as $C \\to \\infty$:\n$$\n\\lim_{C\\to\\infty} \\operatorname{Var}\\left(\\sqrt{C}(\\hat{\\mu}_{C} - \\mu_{S})\\right) = \\lim_{C\\to\\infty} \\sum_{h=1}^{H} \\frac{w_h^2 \\sigma_h^2}{n_h(C)/C} = \\sum_{h=1}^{H} \\frac{w_h^2 \\sigma_h^2}{b_h/c_h} = \\sum_{h=1}^{H} \\frac{c_h w_h^2 \\sigma_h^2}{b_h}\n$$\nLet this asymptotic variance be denoted by $\\sigma_{\\text{asy}}^2(b_1, \\dots, b_H)$.\n\nTo establish asymptotic normality, we represent the variable of interest as a sum of a large number of independent random variables and verify the Lindeberg condition. Let $Z_{h,i} = X_{h,i} - \\mu_h$.\n$$\n\\sqrt{C}(\\hat{\\mu}_{C} - \\mu_{S}) = \\sqrt{C} \\sum_{h=1}^{H} w_h \\left( \\frac{1}{n_h(C)} \\sum_{i=1}^{n_h(C)} Z_{h,i} \\right) = \\sum_{h=1}^{H} \\sum_{i=1}^{n_h(C)} \\frac{\\sqrt{C} w_h}{n_h(C)} Z_{h,i}\n$$\nLet $Y_{C,h,i} = \\frac{\\sqrt{C} w_h}{n_h(C)} Z_{h,i}$. These are independent random variables with $E[Y_{C,h,i}]=0$. The total variance is $s_C^2 = \\sum_{h,i} \\operatorname{Var}(Y_{C,h,i}) = \\sum_{h=1}^{H} n_h(C) \\frac{C w_h^2}{n_h(C)^2} \\sigma_h^2 = C \\sum_{h=1}^{H} \\frac{w_h^2 \\sigma_h^2}{n_h(C)}$, which converges to $\\sigma_{\\text{asy}}^2$. The Lindeberg condition requires that for any $\\epsilon > 0$,\n$$\n\\frac{1}{s_C^2} \\sum_{h=1}^{H} \\sum_{i=1}^{n_h(C)} E[Y_{C,h,i}^2 \\mathbb{I}(|Y_{C,h,i}| > \\epsilon s_C) ] \\to 0\n$$\nas $C \\to \\infty$. The condition $|Y_{C,h,i}| > \\epsilon s_C$ is equivalent to $|Z_{h,i}| > \\epsilon s_C \\frac{n_h(C)}{\\sqrt{C}w_h}$. Since $s_C$ converges to a constant and $\\frac{n_h(C)}{\\sqrt{C}} \\sim \\frac{b_h \\sqrt{C}}{c_h} \\to \\infty$, the threshold for $|Z_{h,i}|$ grows to infinity. The provided condition $E[|X_{h,1}-\\mu_h|^{2+\\delta}] < \\infty$ for some $\\delta>0$ is a Lyapunov condition. This condition is stronger than the Lindeberg condition and is sufficient to guarantee it holds. Therefore, by the Lindeberg-Feller Central Limit Theorem,\n$$\n\\sqrt{C}(\\hat{\\mu}_{C} - \\mu_{S}) \\xrightarrow{d} \\mathcal{N}\\left(0, \\sum_{h=1}^{H} \\frac{c_h w_h^2 \\sigma_h^2}{b_h}\\right)\n$$\nNow, we proceed to the second part of the problem: minimizing the asymptotic variance subject to constraints on the limiting budget fractions $b_h$. We want to minimize the objective function $V(b_1, \\dots, b_H) = \\sum_{h=1}^{H} \\frac{c_h w_h^2 \\sigma_h^2}{b_h}$ subject to the constraints $\\sum_{h=1}^{H} b_h = 1$ and $b_h > 0$ for all $h \\in \\{1,\\dots,H\\}$. We use the method of Lagrange multipliers. The Lagrangian is:\n$$\n\\mathcal{L}(b_1, \\dots, b_H, \\lambda) = \\sum_{h=1}^{H} \\frac{c_h w_h^2 \\sigma_h^2}{b_h} + \\lambda \\left( \\left(\\sum_{h=1}^{H} b_h\\right) - 1 \\right)\n$$\nTaking the partial derivative with respect to $b_k$ for any $k \\in \\{1, \\dots, H\\}$ and setting it to zero gives:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial b_k} = -\\frac{c_k w_k^2 \\sigma_k^2}{b_k^2} + \\lambda = 0\n$$\nThis implies $\\lambda = \\frac{c_k w_k^2 \\sigma_k^2}{b_k^2}$. From this, we solve for $b_k$:\n$$\nb_k^2 = \\frac{c_k w_k^2 \\sigma_k^2}{\\lambda} \\implies b_k = \\frac{\\sqrt{c_k} w_k \\sigma_k}{\\sqrt{\\lambda}}\n$$\nWe choose the positive root since we require $b_k > 0$. We substitute this into the constraint $\\sum_{h=1}^{H} b_h = 1$:\n$$\n\\sum_{k=1}^{H} \\frac{\\sqrt{c_k} w_k \\sigma_k}{\\sqrt{\\lambda}} = 1\n$$\nSolving for $\\sqrt{\\lambda}$:\n$$\n\\frac{1}{\\sqrt{\\lambda}} \\sum_{k=1}^{H} \\sqrt{c_k} w_k \\sigma_k = 1 \\implies \\sqrt{\\lambda} = \\sum_{k=1}^{H} w_k \\sigma_k \\sqrt{c_k}\n$$\nThe optimal budget fractions $b_k^*$ are thus:\n$$\nb_k^* = \\frac{\\sqrt{c_k} w_k \\sigma_k}{\\sum_{j=1}^{H} w_j \\sigma_j \\sqrt{c_j}}\n$$\nFinally, we substitute these optimal fractions back into the expression for the asymptotic variance to find the minimum value:\n$$\nV_{\\min} = \\sum_{k=1}^{H} \\frac{c_k w_k^2 \\sigma_k^2}{b_k^*} = \\sum_{k=1}^{H} \\frac{c_k w_k^2 \\sigma_k^2}{\\frac{\\sqrt{c_k} w_k \\sigma_k}{\\sum_{j=1}^{H} w_j \\sigma_j \\sqrt{c_j}}}\n$$\nSimplifying the term inside the summation:\n$$\n\\frac{c_k w_k^2 \\sigma_k^2}{\\frac{\\sqrt{c_k} w_k \\sigma_k}{\\sum_{j=1}^{H} w_j \\sigma_j \\sqrt{c_j}}} = (c_k w_k^2 \\sigma_k^2) \\left(\\frac{1}{\\sqrt{c_k} w_k \\sigma_k}\\right) \\left(\\sum_{j=1}^{H} w_j \\sigma_j \\sqrt{c_j}\\right) = (\\sqrt{c_k} w_k \\sigma_k) \\left(\\sum_{j=1}^{H} w_j \\sigma_j \\sqrt{c_j}\\right)\n$$\nSubstituting this back into the sum for $V_{\\min}$:\n$$\nV_{\\min} = \\sum_{k=1}^{H} \\left( (\\sqrt{c_k} w_k \\sigma_k) \\left(\\sum_{j=1}^{H} w_j \\sigma_j \\sqrt{c_j}\\right) \\right)\n$$\nThe term $\\sum_{j=1}^{H} w_j \\sigma_j \\sqrt{c_j}$ is a constant with respect to the summation index $k$, so we can factor it out:\n$$\nV_{\\min} = \\left(\\sum_{j=1}^{H} w_j \\sigma_j \\sqrt{c_j}\\right) \\left(\\sum_{k=1}^{H} \\sqrt{c_k} w_k \\sigma_k\\right)\n$$\nThe two sums are identical, so we obtain the final expression for the minimal asymptotic variance:\n$$\nV_{\\min} = \\left(\\sum_{h=1}^{H} w_h \\sigma_h \\sqrt{c_h}\\right)^2\n$$\nThe convexity of the objective function $V(b_1, \\dots, b_H)$ on the domain where $b_h > 0$ for all $h$ ensures that this stationary point is indeed a global minimum.", "answer": "$$\n\\boxed{\\left(\\sum_{h=1}^{H} w_h \\sigma_h \\sqrt{c_h}\\right)^{2}}\n$$", "id": "3317803"}, {"introduction": "While the Central Limit Theorem provides a powerful asymptotic approximation, practical Monte Carlo simulations operate with a finite number of samples. This raises the critical question of how to ensure our results meet a desired level of precision without running an infinite simulation. This exercise ([@problem_id:3317791]) bridges the gap between asymptotic theory and finite-sample practice by using the Berry-Esseen theorem to derive a rigorous, sufficient sample size for a fixed-width confidence interval, quantifying the cost of a formal performance guarantee compared to more heuristic methods.", "problem": "Consider independent and identically distributed random variables $X_{1}, X_{2}, \\dots$ with finite mean $E[X_{1}] = \\mu$, finite variance $\\operatorname{Var}(X_{1}) = \\sigma^{2} \\in (0, \\infty)$, and finite third absolute centered moment $\\rho = E\\!\\left[|X_{1} - \\mu|^{3}\\right] < \\infty$. Let $\\bar{X}_{n} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i}$ be the Monte Carlo estimator of $\\mu$. Fixed-width Monte Carlo stopping rules halt when the half-width $h$ of a two-sided interval $[\\bar{X}_{n} - h, \\bar{X}_{n} + h]$ achieves an honesty constraint $P(|\\bar{X}_{n} - \\mu| \\le h) \\ge 1 - \\alpha$, where $(1 - \\alpha)$ is the target coverage. The Central Limit Theorem (CLT) motivates the normal-approximate rule using the standard normal quantile, while the Berry–Esseen theorem refines the CLT by quantifying the deviation of the standardized $\\bar{X}_{n}$ from the standard normal distribution under finite third moments.\n\nStarting from the CLT and the Berry–Esseen theorem as well-tested facts, and using only independence and the above moment conditions as fundamental base, derive a Berry–Esseen-based lower bound of the form $P(|\\bar{X}_{n} - \\mu| \\le h) \\ge \\text{(function of } n, h, \\sigma, \\rho\\text{)}$. Then, eliminate the implicit dependence on $n$ inside the bound by applying a first-order expansion of the inverse cumulative distribution function at the normal quantile corresponding to two-sided coverage $(1 - \\alpha)$, thereby obtaining a conservative, closed-form sufficient condition for $n$ ensuring $P(|\\bar{X}_{n} - \\mu| \\le h) \\ge 1 - \\alpha$. Use the absolute Berry–Esseen constant $C_{\\text{BE}} = 0.4748$.\n\nNow calibrate the sufficient sample size $n$ under the following scientifically plausible scenario: $\\sigma = 2$, $\\rho = 28$ (so the standardized third absolute moment is $\\gamma = \\rho / \\sigma^{3}$), $\\alpha = 0.05$, $h = 0.1$, and the empirical bootstrap calibration for the standardized mean yields a two-sided $(1 - \\alpha)$ quantile $q_{\\text{boot}} = 2.03$ for the absolute standardized mean. Under the bootstrap rule, the stopping threshold solves $P\\!\\left(|\\bar{X}_{n} - \\mu| \\le h\\right) \\approx 1 - \\alpha$ with $n_{\\text{boot}}$ satisfying $h \\sqrt{n_{\\text{boot}}} / \\sigma \\approx q_{\\text{boot}}$.\n\nCompute the ratio $n_{\\text{BE}} / n_{\\text{boot}}$ of the Berry–Esseen-based sufficient $n_{\\text{BE}}$ to the bootstrap-calibrated $n_{\\text{boot}}$. Round your final ratio to four significant figures. The final answer must be a single real number.", "solution": "The user wants to derive a sufficient sample size condition based on the Berry–Esseen theorem and compare it to a bootstrap-calibrated sample size.\n\nFirst, we validate the problem statement.\n**Step 1: Extract Givens**\n- Independent and identically distributed (i.i.d.) random variables $X_{1}, X_{2}, \\dots$.\n- Finite mean $E[X_{1}] = \\mu$.\n- Finite variance $\\operatorname{Var}(X_{1}) = \\sigma^{2} \\in (0, \\infty)$.\n- Finite third absolute centered moment $\\rho = E\\!\\left[|X_{1} - \\mu|^{3}\\right] < \\infty$.\n- Monte Carlo estimator: $\\bar{X}_{n} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i}$.\n- Honesty constraint: $P(|\\bar{X}_{n} - \\mu| \\le h) \\ge 1 - \\alpha$.\n- Berry–Esseen constant: $C_{\\text{BE}} = 0.4748$.\n- Scenario parameters: $\\sigma = 2$, $\\rho = 28$, $\\alpha = 0.05$, $h = 0.1$.\n- Bootstrap quantile: $q_{\\text{boot}} = 2.03$.\n- Bootstrap sample size rule: $h \\sqrt{n_{\\text{boot}}} / \\sigma \\approx q_{\\text{boot}}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, drawing upon fundamental theorems of probability theory (CLT, Berry–Esseen) and standard practices in Monte Carlo methods. The concepts of moments, confidence intervals, and stopping rules are all well-established. The problem is well-posed, providing a clear path from derivation to numerical calculation with all necessary data. The language is objective and formal. The given numerical values are plausible. There are no contradictions or ambiguities.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed with the solution.\n\n**Part 1: Derivation of the Berry–Esseen-based lower bound**\nLet $Z_{n} = \\frac{\\sqrt{n}(\\bar{X}_{n} - \\mu)}{\\sigma}$ be the standardized sample mean. The objective is to find a lower bound for $P(|\\bar{X}_{n} - \\mu| \\le h)$, which is equivalent to $P(|Z_{n}| \\le \\frac{h\\sqrt{n}}{\\sigma})$.\nLet $F_{Z_{n}}(z)$ be the cumulative distribution function (CDF) of $Z_{n}$, and let $\\Phi(z)$ be the CDF of the standard normal distribution. The Berry–Esseen theorem states:\n$$\n\\sup_{z \\in \\mathbb{R}} |F_{Z_{n}}(z) - \\Phi(z)| \\le \\frac{C_{\\text{BE}} \\rho}{\\sigma^3 \\sqrt{n}}\n$$\nLet's define the constant $C_{0} = \\frac{C_{\\text{BE}} \\rho}{\\sigma^3}$. The inequality is $|F_{Z_{n}}(z) - \\Phi(z)| \\le \\frac{C_0}{\\sqrt{n}}$.\nThis implies two inequalities:\n$F_{Z_{n}}(z) \\ge \\Phi(z) - \\frac{C_0}{\\sqrt{n}}$\n$F_{Z_{n}}(z) \\le \\Phi(z) + \\frac{C_0}{\\sqrt{n}}$\n\nWe want to bound $P\\left(|Z_{n}| \\le \\frac{h\\sqrt{n}}{\\sigma}\\right) = F_{Z_{n}}\\left(\\frac{h\\sqrt{n}}{\\sigma}\\right) - F_{Z_{n}}\\left(-\\frac{h\\sqrt{n}}{\\sigma}\\right)$.\nUsing the bounds above:\n$F_{Z_{n}}\\left(\\frac{h\\sqrt{n}}{\\sigma}\\right) \\ge \\Phi\\left(\\frac{h\\sqrt{n}}{\\sigma}\\right) - \\frac{C_0}{\\sqrt{n}}$\n$-F_{Z_{n}}\\left(-\\frac{h\\sqrt{n}}{\\sigma}\\right) \\ge -\\left(\\Phi\\left(-\\frac{h\\sqrt{n}}{\\sigma}\\right) + \\frac{C_0}{\\sqrt{n}}\\right) = -\\Phi\\left(-\\frac{h\\sqrt{n}}{\\sigma}\\right) - \\frac{C_0}{\\sqrt{n}}$\nAdding these two inequalities gives the lower bound:\n$$\nP\\left(|\\bar{X}_{n} - \\mu| \\le h\\right) \\ge \\Phi\\left(\\frac{h\\sqrt{n}}{\\sigma}\\right) - \\Phi\\left(-\\frac{h\\sqrt{n}}{\\sigma}\\right) - \\frac{2C_0}{\\sqrt{n}}\n$$\nUsing the symmetry property $\\Phi(-z) = 1 - \\Phi(z)$, the bound becomes:\n$$\nP\\left(|\\bar{X}_{n} - \\mu| \\le h\\right) \\ge 2\\Phi\\left(\\frac{h\\sqrt{n}}{\\sigma}\\right) - 1 - \\frac{2 C_{\\text{BE}} \\rho}{\\sigma^3 \\sqrt{n}}\n$$\n\n**Part 2: Derivation of the sufficient condition for $n$**\nWe require $P(|\\bar{X}_{n} - \\mu| \\le h) \\ge 1 - \\alpha$. A sufficient condition is to set the lower bound to be greater than or equal to $1-\\alpha$:\n$$\n2\\Phi\\left(\\frac{h\\sqrt{n}}{\\sigma}\\right) - 1 - \\frac{2 C_{\\text{BE}} \\rho}{\\sigma^3 \\sqrt{n}} \\ge 1 - \\alpha\n$$\nRearranging the terms, we get:\n$$\n\\Phi\\left(\\frac{h\\sqrt{n}}{\\sigma}\\right) \\ge 1 - \\frac{\\alpha}{2} + \\frac{C_{\\text{BE}} \\rho}{\\sigma^3 \\sqrt{n}}\n$$\nLet $z_{1-\\alpha/2} = \\Phi^{-1}(1-\\alpha/2)$ be the standard normal quantile. Applying $\\Phi^{-1}$ to both sides (which is a monotonically increasing function) yields:\n$$\n\\frac{h\\sqrt{n}}{\\sigma} \\ge \\Phi^{-1}\\left(1 - \\frac{\\alpha}{2} + \\frac{C_{\\text{BE}} \\rho}{\\sigma^3 \\sqrt{n}}\\right)\n$$\nTo obtain a closed-form expression for $n$, we use a first-order Taylor expansion for the right-hand side. Let $g(u) = \\Phi^{-1}(u)$, $u_0 = 1 - \\alpha/2$, and $\\delta = \\frac{C_{\\text{BE}} \\rho}{\\sigma^3 \\sqrt{n}}$. The expansion is $g(u_0 + \\delta) \\approx g(u_0) + \\delta g'(u_0)$.\nThe derivative is $g'(u) = \\frac{d}{du}\\Phi^{-1}(u) = \\frac{1}{\\phi(\\Phi^{-1}(u))}$, where $\\phi(z)$ is the standard normal PDF.\nAt $u_0 = 1 - \\alpha/2$, we have $\\Phi^{-1}(u_0) = z_{1-\\alpha/2}$, so $g'(u_0) = \\frac{1}{\\phi(z_{1-\\alpha/2})}$.\nThe inequality becomes:\n$$\n\\frac{h\\sqrt{n}}{\\sigma} \\ge z_{1-\\alpha/2} + \\frac{C_{\\text{BE}} \\rho}{\\sigma^3 \\sqrt{n} \\phi(z_{1-\\alpha/2})}\n$$\nMultiplying by $\\sqrt{n}$ and rearranging gives a quadratic inequality in terms of $x = \\sqrt{n}$:\n$$\n\\left(\\frac{h}{\\sigma}\\right)x^2 - (z_{1-\\alpha/2})x - \\frac{C_{\\text{BE}} \\rho}{\\sigma^3 \\phi(z_{1-\\alpha/2})} \\ge 0\n$$\nThis is of the form $Ax^2 - Bx - C \\ge 0$, where $A = h/\\sigma$, $B = z_{1-\\alpha/2}$, and $C = \\frac{C_{\\text{BE}} \\rho}{\\sigma^3 \\phi(z_{1-\\alpha/2})}$ are all positive. The positive root of $Ax^2 - Bx - C = 0$ is $x = \\frac{B + \\sqrt{B^2 + 4AC}}{2A}$. The inequality holds for $x$ greater than or equal to this root.\nThus, the sufficient sample size $n_{\\text{BE}}$ is given by:\n$$\nn_{\\text{BE}} = \\left( \\frac{z_{1-\\alpha/2} + \\sqrt{z_{1-\\alpha/2}^2 + 4\\left(\\frac{h}{\\sigma}\\right)\\left(\\frac{C_{\\text{BE}} \\rho}{\\sigma^3 \\phi(z_{1-\\alpha/2})}\\right)}}{2(h/\\sigma)} \\right)^2\n$$\n$$\nn_{\\text{BE}} = \\left( \\frac{\\sigma}{2h} \\left[ z_{1-\\alpha/2} + \\sqrt{z_{1-\\alpha/2}^2 + \\frac{4h C_{\\text{BE}} \\rho}{\\sigma^4 \\phi(z_{1-\\alpha/2})}} \\right] \\right)^2\n$$\n\n**Part 3: Numerical Calculation**\nWe are given:\n$\\sigma = 2$, $\\rho = 28$, $\\alpha = 0.05$, $h = 0.1$, $C_{\\text{BE}} = 0.4748$, $q_{\\text{boot}} = 2.03$.\n\nFirst, calculate $n_{\\text{boot}}$. From the given rule $h \\sqrt{n_{\\text{boot}}} / \\sigma \\approx q_{\\text{boot}}$, we solve for $n_{\\text{boot}}$:\n$$\nn_{\\text{boot}} = \\left(\\frac{\\sigma q_{\\text{boot}}}{h}\\right)^2 = \\left(\\frac{2 \\times 2.03}{0.1}\\right)^2 = (40.6)^2 = 1648.36\n$$\n\nNext, calculate $n_{\\text{BE}}$. We need the following values:\nFor $\\alpha = 0.05$, $1 - \\alpha/2 = 0.975$.\nThe quantile $z_{1-\\alpha/2} = z_{0.975} \\approx 1.959964$.\nThe PDF value $\\phi(z_{0.975}) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{z_{0.975}^2}{2}\\right) \\approx \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1.959964^2}{2}\\right) \\approx 0.058441$.\n\nNow, we evaluate the term under the square root in the expression for $n_{\\text{BE}}$:\n$$\nz_{1-\\alpha/2}^2 + \\frac{4h C_{\\text{BE}} \\rho}{\\sigma^4 \\phi(z_{1-\\alpha/2})} \\approx (1.959964)^2 + \\frac{4(0.1)(0.4748)(28)}{2^4 (0.058441)}\n$$\n$$\n\\approx 3.841459 + \\frac{5.31776}{16 \\times 0.058441} \\approx 3.841459 + \\frac{5.31776}{0.935056} \\approx 3.841459 + 5.68697\n$$\n$$\n\\approx 9.528429\n$$\nTaking the square root: $\\sqrt{9.528429} \\approx 3.086815$.\n\nNow we find $\\sqrt{n_{\\text{BE}}}$:\n$$\n\\sqrt{n_{\\text{BE}}} = \\frac{\\sigma}{2h} \\left[ z_{1-\\alpha/2} + \\sqrt{\\dots} \\right] \\approx \\frac{2}{2(0.1)} \\left[ 1.959964 + 3.086815 \\right]\n$$\n$$\n\\sqrt{n_{\\text{BE}}} \\approx 10 \\times [5.046779] = 50.46779\n$$\nThen, $n_{\\text{BE}}$ is:\n$$\nn_{\\text{BE}} = (50.46779)^2 \\approx 2547.00\n$$\n\nFinally, we compute the ratio $n_{\\text{BE}} / n_{\\text{boot}}$:\n$$\n\\frac{n_{\\text{BE}}}{n_{\\text{boot}}} \\approx \\frac{2547.00}{1648.36} \\approx 1.545163\n$$\nRounding to four significant figures, the ratio is $1.545$.", "answer": "$$\n\\boxed{1.545}\n$$", "id": "3317791"}]}