{"hands_on_practices": [{"introduction": "The efficiency of sum factorization hinges on its ability to decompose multi-dimensional operations into a sequence of one-dimensional ones. This first exercise guides you through the foundational derivation of a sum-factorized gradient operator. By expressing the multi-dimensional operator in terms of one-dimensional interpolation and differentiation matrices using the Kronecker product, you will solidify your understanding of this cornerstone concept [@problem_id:3422305].", "problem": "Consider a one-dimensional polynomial approximation space spanned by Lagrange basis functions $\\ell_{j}(x)$ of degree at most $p$, built from $N=p+1$ distinct nodal points $\\{x_{j}\\}_{j=0}^{p}$ on an interval with affine coordinate $x$. Let $\\{\\xi_{q}\\}_{q=1}^{Q}$ be a quadrature grid with $Q \\geq N$, distinct from the nodal grid (a non-collocated scheme). Define the interpolation operator from nodal coefficients to quadrature values as the linear map $I \\in \\mathbb{R}^{Q \\times N}$ that takes a coefficient vector $u \\in \\mathbb{R}^{N}$ to the vector of values $(Iu) \\in \\mathbb{R}^{Q}$, with $(Iu)_{q} = \\sum_{j=0}^{p} u_{j} \\, \\ell_{j}(\\xi_{q})$. In addition, define the one-dimensional quadrature-grid differentiation operator $\\mathcal{D} \\in \\mathbb{R}^{Q \\times Q}$ as the linear map on quadrature values $f \\in \\mathbb{R}^{Q}$ satisfying $\\mathcal{D}(Iu) = \\left(\\frac{d}{dx}\\sum_{j=0}^{p} u_{j}\\,\\ell_{j}\\right)\\big|_{\\{\\xi_{q}\\}}$ for all $u \\in \\mathbb{R}^{N}$, that is, it returns the exact derivative values at the quadrature points for any polynomial represented in the nodal basis.\n\nNow consider a $d$-dimensional reference element with separable tensor-product Lagrange basis $\\{\\ell_{j_{1}}(x_{1}) \\cdots \\ell_{j_{d}}(x_{d})\\}$ constructed from the same one-dimensional nodal set in each coordinate direction $\\{x_{j}\\}_{j=0}^{p}$, and a separable quadrature grid $\\{\\xi_{q_{1}},\\dots,\\xi_{q_{d}}\\}$ constructed from the same one-dimensional quadrature in each coordinate direction $\\{\\xi_{q}\\}_{q=1}^{Q}$. Let $u \\in \\mathbb{R}^{N^{d}}$ denote the tensor of nodal coefficients reshaped as a vector in lexicographic order. In spectral and Discontinuous Galerkin (DG) methods, a fundamental building block for high order operators is the sequence $u \\mapsto Iu \\mapsto \\mathcal{D}(Iu)$ applied along each coordinate direction. Using only the core definitions above and the linearity of tensor products, derive the explicit entrywise formula for the one-dimensional interpolation matrix $I$ and then, by first principles, express the $d$-dimensional composite operator that maps $u$ to the sum of directional derivatives at all quadrature points, written purely in terms of one-dimensional operators and Kronecker products. Your final answer must be the single closed-form analytic expression for this $d$-dimensional sum-factorized operator in terms of $I$, $\\mathcal{D}$, and the Kronecker product. No numerical evaluation is required, and no rounding is needed. Express the final operator solely as a symbolic expression.", "solution": "The problem statement is rigorously analyzed for validity.\n\n### Step 1: Extract Givens\n- A one-dimensional polynomial approximation space of degree at most $p$.\n- The space is spanned by Lagrange basis functions $\\ell_{j}(x)$.\n- The basis is built from $N=p+1$ distinct nodal points $\\{x_{j}\\}_{j=0}^{p}$ on an interval with affine coordinate $x$.\n- A quadrature grid with $Q$ distinct points $\\{\\xi_{q}\\}_{q=1}^{Q}$, where $Q \\geq N$.\n- The quadrature grid is non-collocated with the nodal grid.\n- The one-dimensional interpolation operator is a linear map $I \\in \\mathbb{R}^{Q \\times N}$.\n- The action of $I$ on a nodal coefficient vector $u \\in \\mathbb{R}^{N}$ is defined by $(Iu)_{q} = \\sum_{j=0}^{p} u_{j} \\, \\ell_{j}(\\xi_{q})$.\n- The one-dimensional quadrature-grid differentiation operator is a linear map $\\mathcal{D} \\in \\mathbb{R}^{Q \\times Q}$.\n- The action of $\\mathcal{D}$ is defined such that for any $u \\in \\mathbb{R}^{N}$, $\\mathcal{D}(Iu)$ yields the vector of exact derivative values of the polynomial $\\sum_{j=0}^{p} u_{j}\\,\\ell_{j}(x)$ at the quadrature points $\\{\\xi_{q}\\}$, i.e., $\\mathcal{D}(Iu) = \\left(\\frac{d}{dx}\\sum_{j=0}^{p} u_{j}\\,\\ell_{j}\\right)\\big|_{\\{\\xi_{q}\\}}$.\n- A $d$-dimensional reference element with a separable tensor-product Lagrange basis given by $\\{\\ell_{j_{1}}(x_{1}) \\cdots \\ell_{j_{d}}(x_{d})\\}$, using the same one-dimensional nodes in each coordinate direction.\n- A separable $d$-dimensional quadrature grid constructed from the tensor product of the one-dimensional grid $\\{\\xi_{q}\\}$.\n- The tensor of nodal coefficients $u \\in \\mathbb{R}^{N^{d}}$, reshaped into a vector in lexicographic order.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem is set within the well-established mathematical framework of numerical analysis, specifically concerning spectral and Discontinuous Galerkin (DG) methods. The concepts of Lagrange polynomials, quadrature, tensor products, and Kronecker products are standard and mathematically sound.\n- **Well-Posed**: The problem is well-posed. It provides clear and precise definitions for all operators and spaces and asks for the derivation of a specific composite operator based on these definitions. The objective is unambiguous, and the provided information is sufficient to derive a unique analytical expression.\n- **Objective**: The problem is stated in formal, objective mathematical language, free of any subjectivity, ambiguity, or non-scientific claims.\n\nThe problem exhibits none of the invalidating flaws. It is mathematically sound, self-contained, and directly pertinent to the specified topic of sum factorization for high-order operators.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete, reasoned solution will be provided.\n\n### Solution Derivation\nThe objective is to derive the expression for the linear operator that maps the $d$-dimensional nodal coefficient vector to the sum of all first-order partial derivatives evaluated at the $d$-dimensional quadrature grid.\n\nFirst, we determine the explicit entrywise formula for the one-dimensional interpolation matrix $I \\in \\mathbb{R}^{Q \\times N}$. The definition states that for a vector of nodal coefficients $u = (u_0, u_1, \\dots, u_p)^T \\in \\mathbb{R}^N$, the $q$-th component of the resulting vector of quadrature-point values $v = Iu$ is given by:\n$$v_q = (Iu)_q = \\sum_{j=0}^{p} u_{j} \\, \\ell_{j}(\\xi_{q})$$\nThe standard definition of a matrix-vector product is $(Iu)_q = \\sum_{j=0}^{p} I_{q,j+1} u_j$ (using $1$-based indexing for matrix entries and $0$-based for vector components $u_j$). Assuming lexicographical mapping, where the column index corresponds to the basis function index, we have $(Iu)_q = \\sum_{j=0}^{p} I_{qj} u_j$. By direct comparison, the entries of the matrix $I$ must be:\n$$I_{qj} = \\ell_{j}(\\xi_{q})$$\nfor $q \\in \\{1, \\dots, Q\\}$ and $j \\in \\{0, \\dots, p\\}$. Thus, $I$ is the matrix that evaluates the basis functions at the quadrature points.\n\nNext, we construct the $d$-dimensional operator. Let $U(\\mathbf{x}) = U(x_1, \\dots, x_d)$ be the polynomial function represented by the nodal coefficients $u \\in \\mathbb{R}^{N^d}$. Due to the tensor-product basis, this function is:\n$$U(x_1, \\dots, x_d) = \\sum_{j_1=0}^{p} \\cdots \\sum_{j_d=0}^{p} u_{j_1, \\dots, j_d} \\left( \\ell_{j_1}(x_1) \\cdots \\ell_{j_d}(x_d) \\right)$$\nwhere $u_{j_1, \\dots, j_d}$ are the entries of the coefficient tensor.\n\nLet's consider the operator that maps the nodal coefficients $u$ to the values of a single partial derivative, $\\frac{\\partial U}{\\partial x_k}$, at the quadrature points.\n$$\\frac{\\partial U}{\\partial x_k} = \\sum_{j_1=0}^{p} \\cdots \\sum_{j_d=0}^{p} u_{j_1, \\dots, j_d} \\left( \\ell_{j_1}(x_1) \\cdots \\frac{d\\ell_{j_k}(x_k)}{dx_k} \\cdots \\ell_{j_d}(x_d) \\right)$$\nEvaluating this at a $d$-dimensional quadrature point $(\\xi_{q_1}, \\dots, \\xi_{q_d})$ gives:\n$$\\left. \\frac{\\partial U}{\\partial x_k} \\right|_{(\\xi_{q_1}, \\dots, \\xi_{q_d})} = \\sum_{j_1=0}^{p} \\cdots \\sum_{j_d=0}^{p} u_{j_1, \\dots, j_d} \\left( \\ell_{j_1}(\\xi_{q_1}) \\cdots \\left(\\frac{d\\ell_{j_k}}{dx_k}\\right)(\\xi_{q_k}) \\cdots \\ell_{j_d}(\\xi_{q_d}) \\right)$$\nThis operation, which maps the entire tensor of coefficients $u_{j_1, \\dots, j_d}$ to the tensor of derivative values at quadrature points, can be represented as a linear operator acting on the vectorized coefficients $u$.\n\nThe operation along each dimension $i$ is either interpolation (if $i \\neq k$) or differentiation followed by evaluation (if $i=k$).\n1.  For any dimension $i \\neq k$, the operation is interpolation, which maps $N$ nodal coefficients to $Q$ quadrature values. This is represented by the matrix $I \\in \\mathbb{R}^{Q \\times N}$.\n2.  For dimension $k$, the operation maps $N$ nodal coefficients to the $Q$ values of the derivative at the quadrature points. As per the problem definition, this operator is the composition $\\mathcal{D}I \\in \\mathbb{R}^{Q \\times N}$.\n\nFor a tensor-product structure, the multidimensional linear operator is the Kronecker product of the one-dimensional operators. The operator $\\mathbf{L}_k$ that computes the partial derivative with respect to $x_k$ at all quadrature points is therefore:\n$$\\mathbf{L}_k = I \\otimes \\cdots \\otimes I \\otimes \\underbrace{(\\mathcal{D}I)}_{k\\text{-th position}} \\otimes I \\otimes \\cdots \\otimes I$$\nThis operator $\\mathbf{L}_k$ is a matrix of size $Q^d \\times N^d$.\n\nThe problem asks for the composite operator that yields the sum of all first-order directional derivatives. By linearity of differentiation and the Kronecker product, this global operator, which we denote $\\mathbf{L}$, is the sum of the individual directional derivative operators:\n$$\\mathbf{L} = \\sum_{k=1}^{d} \\mathbf{L}_k$$\nSubstituting the expression for $\\mathbf{L}_k$, we get:\n$$\\mathbf{L} = \\sum_{k=1}^{d} \\left( I \\otimes \\cdots \\otimes I \\otimes (\\mathcal{D}I) \\otimes I \\otimes \\cdots \\otimes I \\right)$$\nwhere the term $(\\mathcal{D}I)$ appears in the $k$-th position of the Kronecker product for the $k$-th term in the summation. This expression is purely in terms of the one-dimensional operators $I$ and $\\mathcal{D}$ and the Kronecker product, as required. It represents the \"sum-factorized\" form of the gradient operator commonly used in spectral element and DG methods for efficient computation. Denoting the Kronecker product with $\\otimes$, a more formal expression can be written as follows.\nLet $\\mathbf{O}_k$ be an operator in the tensor product. For $k=1$, $\\mathbf{O}_1 = (\\mathcal{D}I) \\otimes \\left(\\bigotimes_{i=2}^{d} I\\right)$. For $1<k<d$, $\\mathbf{O}_k = \\left(\\bigotimes_{i=1}^{k-1} I\\right) \\otimes (\\mathcal{D}I) \\otimes \\left(\\bigotimes_{i=k+1}^{d} I\\right)$. For $k=d$, $\\mathbf{O}_d = \\left(\\bigotimes_{i=1}^{d-1} I\\right) \\otimes (\\mathcal{D}I)$. The final operator is $\\sum_{k=1}^{d}\\mathbf{O}_k$. The expression in the final answer is an explicit representation of this sum.", "answer": "$$\\boxed{\\sum_{k=1}^{d} \\left( I \\otimes \\cdots \\otimes I \\otimes (\\mathcal{D}I) \\otimes I \\otimes \\cdots \\otimes I \\right) \\quad \\text{where } (\\mathcal{D}I) \\text{ is the } k\\text{-th term in the product}}$$", "id": "3422305"}, {"introduction": "A correct implementation of a high-order method requires not only an efficient operator but also accurate numerical integration. This practice explores the crucial link between the polynomial degree $p$ of your basis functions and the required precision of the quadrature rule. You will determine the minimal number of quadrature points needed to exactly integrate the mass and stiffness matrices, a vital step for preventing the loss of accuracy in a simulation [@problem_id:3422359].", "problem": "Consider the tensor-product polynomial space $Q_p$ on the reference hypercube $\\hat{K} = [-1,1]^d$ with $d \\ge 2$, defined as the space of polynomials that have degree at most $p$ in each coordinate. Let $\\{\\phi_i\\}$ be a basis of $Q_p$, and consider an affine mapping from $\\hat{K}$ to a physical element $K$ so that the Jacobian and metric terms are constant over $K$. Using the standard Galerkin formulation for the scalar Poisson problem, the element mass and stiffness operators are defined by\n$$\nM_{ij} = \\int_{K} \\phi_i \\, \\phi_j \\, \\mathrm{d}x, \n\\qquad\nA_{ij} = \\int_{K} \\nabla \\phi_i \\cdot \\nabla \\phi_j \\, \\mathrm{d}x.\n$$\nAssume that both $M_{ij}$ and $A_{ij}$ are evaluated via tensor-product Gaussian quadrature in each coordinate with the same number of points $q$ per direction. In one dimension, a Gaussian quadrature rule with $q$ points is known to integrate polynomials up to a certain degree exactly. In multiple dimensions, the tensor-product quadrature integrates exactly the tensor-product of univariate polynomials up to the corresponding per-coordinate degrees.\n\nStarting only from the definitions above and the standard exactness property of Gaussian quadrature in one dimension, and exploiting the tensor-product separability that underlies sum factorization, derive the minimal exactness condition on $q$ that guarantees exact evaluation of $M_{ij}$ and $A_{ij}$ for all $i,j$ when using $Q_p$ basis functions on affine elements in dimension $d \\ge 2$. State clearly how the maximal polynomial degrees appearing in the integrands of $M_{ij}$ and $A_{ij}$ determine the per-direction quadrature requirement, and use this to determine the minimal integer $q$ needed for each operator.\n\nProvide your final answer as a row matrix $\\big(q_{\\text{mass}} \\;\\; q_{\\text{stiff}}\\big)$, where $q_{\\text{mass}}$ and $q_{\\text{stiff}}$ are the minimal numbers of Gaussian points per coordinate direction required for exactness of the mass and stiffness operators, respectively. No rounding is required and no units should be included in the final answer.", "solution": "The problem requires the determination of the minimal number of Gaussian quadrature points, $q$, per coordinate direction necessary for the exact evaluation of the element mass matrix, $M_{ij}$, and stiffness matrix, $A_{ij}$, for a scalar Poisson problem using a $Q_p$ polynomial basis on an affine hypercube element. The analysis proceeds by transforming the defining integrals to a reference element and determining the maximal polynomial degree of the integrands in any single coordinate direction.\n\nLet the affine mapping from the reference hypercube $\\hat{K} = [-1,1]^d$ to the physical element $K$ be denoted by $\\mathbf{x} = F(\\hat{\\mathbf{x}})$. Since the mapping is affine, the Jacobian matrix, $J$, of the transformation is constant. Consequently, its determinant, $|\\det(J)|$, is also a constant. The basis functions $\\phi_i$ on the physical element $K$ are related to the basis functions $\\hat{\\phi}_i$ on the reference element $\\hat{K}$ by composition, i.e., $\\phi_i = \\hat{\\phi}_i \\circ F^{-1}$. The basis functions $\\{\\hat{\\phi}_i\\}$ are polynomials in the space $Q_p$, meaning they have a degree of at most $p$ in each coordinate $\\hat{x}_k$ for $k \\in \\{1, 2, \\dots, d\\}$.\n\nA one-dimensional Gaussian quadrature rule with $q$ points is exact for polynomials of degree up to $2q-1$. For a multi-dimensional integral on a tensor-product domain, a tensor-product Gaussian quadrature rule with $q$ points in each of the $d$ directions is exact if and only if the integrand is a polynomial of degree at most $2q-1$ in each coordinate variable separately. Therefore, to find the minimal required $q$, we must find the maximal polynomial degree, $D$, of the integrand in any single coordinate direction. The condition for exactness is then $2q - 1 \\ge D$, which implies the minimal integer $q$ is given by $q = \\lceil \\frac{D+1}{2} \\rceil$.\n\nFirst, we analyze the mass matrix, $M_{ij}$.\nThe integral for the mass matrix entry is:\n$$\nM_{ij} = \\int_{K} \\phi_i(\\mathbf{x}) \\, \\phi_j(\\mathbf{x}) \\, \\mathrm{d}\\mathbf{x}\n$$\nTransforming this integral to the reference element $\\hat{K}$ yields:\n$$\nM_{ij} = \\int_{\\hat{K}} \\hat{\\phi}_i(\\hat{\\mathbf{x}}) \\, \\hat{\\phi}_j(\\hat{\\mathbf{x}}) \\, |\\det(J)| \\, \\mathrm{d}\\hat{\\mathbf{x}}\n$$\nSince $|\\det(J)|$ is a constant, the polynomial part of the integrand is the product $\\hat{\\phi}_i(\\hat{\\mathbf{x}}) \\, \\hat{\\phi}_j(\\hat{\\mathbf{x}})$. Both $\\hat{\\phi}_i$ and $\\hat{\\phi}_j$ belong to the space $Q_p$, which means they are polynomials of degree at most $p$ in each coordinate $\\hat{x}_k$. The product of two such polynomials will result in a polynomial of degree at most $p+p=2p$ in each coordinate. Therefore, the maximal degree of the mass matrix integrand in any coordinate direction is $D_{\\text{mass}} = 2p$.\n\nTo ensure exact integration, the number of quadrature points per direction, $q_{\\text{mass}}$, must satisfy:\n$$\n2q_{\\text{mass}} - 1 \\ge D_{\\text{mass}} = 2p\n$$\n$$\n2q_{\\text{mass}} \\ge 2p + 1\n$$\n$$\nq_{\\text{mass}} \\ge p + \\frac{1}{2}\n$$\nSince $q_{\\text{mass}}$ must be an integer, the minimal required number of points is $q_{\\text{mass}} = p+1$.\n\nNext, we analyze the stiffness matrix, $A_{ij}$.\nThe integral for the stiffness matrix entry is:\n$$\nA_{ij} = \\int_{K} \\nabla_{\\mathbf{x}} \\phi_i(\\mathbf{x}) \\cdot \\nabla_{\\mathbf{x}} \\phi_j(\\mathbf{x}) \\, \\mathrm{d}\\mathbf{x}\n$$\nThe gradient operator transforms according to the chain rule, $\\nabla_{\\mathbf{x}} = (J^{-1})^T \\nabla_{\\hat{\\mathbf{x}}}$. The integral on the reference element becomes:\n$$\nA_{ij} = \\int_{\\hat{K}} \\left((J^{-1})^T \\nabla_{\\hat{\\mathbf{x}}} \\hat{\\phi}_i(\\hat{\\mathbf{x}})\\right) \\cdot \\left((J^{-1})^T \\nabla_{\\hat{\\mathbf{x}}} \\hat{\\phi}_j(\\hat{\\mathbf{x}})\\right) \\, |\\det(J)| \\, \\mathrm{d}\\hat{\\mathbf{x}}\n$$\nThis can be written as:\n$$\nA_{ij} = \\int_{\\hat{K}} (\\nabla_{\\hat{\\mathbf{x}}} \\hat{\\phi}_i)^T G (\\nabla_{\\hat{\\mathbf{x}}} \\hat{\\phi}_j) \\, |\\det(J)| \\, \\mathrm{d}\\hat{\\mathbf{x}}\n$$\nwhere $G = J^{-1} (J^{-1})^T$ is a constant matrix, as $J$ is constant for an affine map. The integrand is $I_{\\text{stiff}} = |\\det(J)| \\sum_{k=1}^d \\sum_{l=1}^d G_{kl} \\frac{\\partial \\hat{\\phi}_i}{\\partial \\hat{x}_k} \\frac{\\partial \\hat{\\phi}_j}{\\partial \\hat{x}_l}$.\nWe must determine the maximal polynomial degree of $I_{\\text{stiff}}$ in any single coordinate direction, say $\\hat{x}_m$. The degree of the sum is the maximum of the degrees of its terms. Let's analyze the degree in $\\hat{x}_m$ of a single term $\\frac{\\partial \\hat{\\phi}_i}{\\partial \\hat{x}_k} \\frac{\\partial \\hat{\\phi}_j}{\\partial \\hat{x}_l}$.\nThe function $\\hat{\\phi}_i \\in Q_p$ has degree at most $p$ in $\\hat{x}_m$. Its partial derivative, $\\frac{\\partial \\hat{\\phi}_i}{\\partial \\hat{x}_k}$, has a degree in $\\hat{x}_m$ that is at most $p-1$ if $k=m$ and at most $p$ if $k \\ne m$.\nThe degree of the product term in $\\hat{x}_m$ is the sum of the degrees of its factors. We consider all possible cases for $k$ and $l$ relative to $m$:\n1.  If $k=m$ and $l=m$: The degree in $\\hat{x}_m$ is at most $(p-1) + (p-1) = 2p-2$.\n2.  If $k=m$ and $l \\ne m$ (or vice versa): The degree in $\\hat{x}_m$ is at most $(p-1) + p = 2p-1$.\n3.  If $k \\ne m$ and $l \\ne m$: The degree in $\\hat{x}_m$ is at most $p + p = 2p$.\n\nThis third case, which yields the highest degree, is only possible because the dimension $d \\ge 2$, allowing for differentiation with respect to coordinates other than $\\hat{x}_m$. For instance, for $d=2$, the term $\\frac{\\partial \\hat{\\phi}_i}{\\partial \\hat{x}_2} \\frac{\\partial \\hat{\\phi}_j}{\\partial \\hat{x}_2}$ contributes to the integrand. The basis functions $\\hat{\\phi}_i$ and $\\hat{\\phi}_j$ can have degree up to $p$ in $\\hat{x}_1$. Differentiating with respect to $\\hat{x}_2$ does not change their dependence on $\\hat{x}_1$. Thus, the product can have degree up to $2p$ in $\\hat{x}_1$. This represents the highest possible degree in any coordinate direction for any term in the integrand sum. Cancellation of the highest-degree terms is not guaranteed to occur for arbitrary affine elements (i.e., for arbitrary constant matrices $G$), so the maximum degree of the entire integrand $I_{\\text{stiff}}$ in any coordinate direction is $D_{\\text{stiff}} = 2p$.\n\nTo ensure exact integration, the number of quadrature points per direction, $q_{\\text{stiff}}$, must satisfy:\n$$\n2q_{\\text{stiff}} - 1 \\ge D_{\\text{stiff}} = 2p\n$$\n$$\n2q_{\\text{stiff}} \\ge 2p + 1\n$$\n$$\nq_{\\text{stiff}} \\ge p + \\frac{1}{2}\n$$\nSince $q_{\\text{stiff}}$ must be an integer, the minimal required number of points is $q_{\\text{stiff}} = p+1$.\n\nIn summary, for dimension $d \\ge 2$ and an affine element mapping, the maximal per-coordinate polynomial degree for the mass matrix integrand is $2p$, and for the stiffness matrix integrand, it is also $2p$. Both require the same minimal number of Gaussian quadrature points per direction for exact evaluation.\nThe minimal number of points for the mass operator is $q_{\\text{mass}} = p+1$.\nThe minimal number of points for the stiffness operator is $q_{\\text{stiff}} = p+1$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\np+1 & p+1\n\\end{pmatrix}\n}\n$$", "id": "3422359"}, {"introduction": "Why is sum factorization the preferred approach for high-order methods? This final practice provides a quantitative answer by comparing the performance of a matrix-free sum factorization scheme against the traditional explicit matrix assembly approach. Using a simplified but powerful Roofline performance model, you will identify the crossover point where the matrix-free method becomes more efficient, highlighting its dramatic advantages for high polynomial degrees on modern computer architectures [@problem_id:3422374].", "problem": "Consider a three-dimensional Discontinuous Galerkin (DG) discretization on tensor-product hexahedral elements using a collocated Lagrange basis at Gauss-Lobatto-Legendre points. Let the polynomial degree be $p$, and define $m = p+1$ and $n = m^3$ as the number of one-dimensional points per axis and the number of degrees of freedom per element, respectively. The focus is the per-element application of the symmetric Poisson operator on the reference element. Two strategies are to be compared: (1) explicit assembly of the element stiffness matrix followed by a dense matrix-vector multiplication, and (2) Sum Factorization (SF) matrix-free application using sequences of one-dimensional transforms.\n\nFoundational bases are as follows:\n- Tensor-product structure: The basis is separable, so application of a differential operator in three dimensions can be decomposed into sequences of one-dimensional operations.\n- Sum Factorization: In $d = 3$ dimensions, factorized application of the Laplacian via $D^\\top W D$ per axis reduces arithmetic from $O(n^2)$ to $O(d\\,m^{d+1})$ for application, exploiting one-dimensional operations along lines.\n- Roofline performance model: Execution time is modeled by $T = \\max\\left(\\frac{F}{P}, \\frac{B}{\\beta}\\right)$, where $F$ is the number of floating-point operations, $P$ is peak floating-point throughput in floating-point operations per second (flops/s), $B$ is the number of bytes moved between main memory and the processor, and $\\beta$ is sustained memory bandwidth in bytes per second (B/s). For this problem, all arithmetic is double precision, so each scalar read or write accounts for $8$ bytes.\n\nYou are to model, for each element, the following using the roofline model:\n- Explicit assembly:\n  - Assembly flops $F_\\mathrm{asm} = 3 m^6$ and assembly bytes $B_\\mathrm{asm} = n^2 \\cdot 8 = m^6 \\cdot 8$.\n  - Dense matrix-vector multiply flops $F_\\mathrm{spmv} = 2 n^2 = 2 m^6$ and bytes $B_\\mathrm{spmv} = n^2 \\cdot 8 + n \\cdot 8 + n \\cdot 8 = m^6 \\cdot 8 + 2 m^3 \\cdot 8$.\n  - Per-element explicit path time $T_\\mathrm{explicit}(m) = T_\\mathrm{asm}(m) + T_\\mathrm{spmv}(m)$ with $T_\\mathrm{asm}(m) = \\max\\left(\\frac{F_\\mathrm{asm}}{P}, \\frac{B_\\mathrm{asm}}{\\beta}\\right)$ and $T_\\mathrm{spmv}(m) = \\max\\left(\\frac{F_\\mathrm{spmv}}{P}, \\frac{B_\\mathrm{spmv}}{\\beta}\\right)$.\n- Sum Factorization matrix-free apply:\n  - Flops $F_\\mathrm{mf} = 12 m^4$ based on two one-dimensional matrix-vector products per axis (forward derivative and adjoint with weighting), across $3$ axes.\n  - Bytes $B_\\mathrm{mf} = 2 n \\cdot 8 = 16 m^3$ for reading the input vector and writing the output vector.\n  - Per-element matrix-free time $T_\\mathrm{mf}(m) = \\max\\left(\\frac{F_\\mathrm{mf}}{P}, \\frac{B_\\mathrm{mf}}{\\beta}\\right)$.\n- Architecture-dependent one-time overheads per run:\n  - Central Processing Unit (CPU): No extra overhead beyond per-element times.\n  - Graphics Processing Unit (GPU): One-time host-to-device transfer of operator constants $D$ and $W$ with bytes $B_\\mathrm{transfer}(m) = \\left(3 m^2 + m\\right) \\cdot 8$ and time $T_\\mathrm{transfer}(m) = \\frac{B_\\mathrm{transfer}(m)}{\\beta}$.\n\nLet $E$ denote the number of elements in the problem. For each architecture, the total runtime models are:\n- CPU totals: $T_\\mathrm{explicit,total}(m, E) = E \\cdot T_\\mathrm{explicit}(m)$ and $T_\\mathrm{mf,total}(m, E) = E \\cdot T_\\mathrm{mf}(m)$.\n- GPU totals: $T_\\mathrm{explicit,total}(m, E) = E \\cdot T_\\mathrm{explicit}(m)$ and $T_\\mathrm{mf,total}(m, E) = T_\\mathrm{transfer}(m) + E \\cdot T_\\mathrm{mf}(m)$.\n\nDefine the architecture parameters as:\n- CPU: peak throughput $P_\\mathrm{CPU} = 100 \\times 10^{9}$ flops/s and bandwidth $\\beta_\\mathrm{CPU} = 50 \\times 10^{9}$ bytes/s.\n- GPU: peak throughput $P_\\mathrm{GPU} = 10 \\times 10^{12}$ flops/s and bandwidth $\\beta_\\mathrm{GPU} = 900 \\times 10^{9}$ bytes/s.\n\nTask:\n- For each architecture separately, determine the minimal polynomial degree $p_c \\in \\{1, 2, 3, 4, 5, 6\\}$ and minimal element count $E_c$ from the test suite such that $T_\\mathrm{mf,total}(m, E) \\le T_\\mathrm{explicit,total}(m, E)$ holds. If the inequality holds for multiple pairs $(p, E)$, choose the smallest $p$ and, for that $p$, the smallest $E$. If it never holds over the specified test suite, return $-1$ for both $p$ and $E$.\n\nUse the following test suite ranges:\n- Polynomial degrees $p \\in \\{1, 2, 3, 4, 5, 6\\}$, where $m = p + 1$.\n- Element counts $E \\in \\{1, 8, 64, 512\\}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the four integers $[p_\\mathrm{CPU}, E_\\mathrm{CPU}, p_\\mathrm{GPU}, E_\\mathrm{GPU}]$, where $p_\\mathrm{CPU}$ and $E_\\mathrm{CPU}$ are the crossover degree and element count for the Central Processing Unit, and $p_\\mathrm{GPU}$ and $E_\\mathrm{GPU}$ are the crossover degree and element count for the Graphics Processing Unit. If no crossover is found for an architecture, output $-1$ for both entries of that architecture.", "solution": "The problem has been validated and is determined to be well-posed, scientifically grounded, and internally consistent. It presents a standard performance modeling exercise comparing two common algorithmic strategies—explicit matrix assembly versus matrix-free sum factorization—for applying a high-order differential operator in the context of the Discontinuous Galerkin method. All necessary formulas, parameters, and evaluation criteria are provided.\n\nThe task is to determine, for a Central Processing Unit (CPU) and a Graphics Processing Unit (GPU), the minimal polynomial degree $p_c$ and minimal element count $E_c$ at which the sum factorization (matrix-free) method becomes computationally faster than the explicit assembly method. The analysis will be performed over a specified test suite of polynomial degrees $p \\in \\{1, 2, 3, 4, 5, 6\\}$ and element counts $E \\in \\{1, 8, 64, 512\\}$. The number of points per dimension is $m = p + 1$.\n\nThe performance of each operation is governed by the roofline model, where the execution time $T$ is the maximum of the time required for floating-point operations (compute-bound) and the time required for data movement (memory-bound):\n$$T = \\max\\left(\\frac{F}{P}, \\frac{B}{\\beta}\\right)$$\nHere, $F$ is the count of floating-point operations, $P$ is the peak floating-point throughput, $B$ is the number of bytes transferred, and $\\beta$ is the memory bandwidth. All scalars are double-precision, occupying $8$ bytes.\n\nThe parameters for the two architectures are:\n- CPU: $P_\\mathrm{CPU} = 100 \\times 10^{9}$ flops/s, $\\beta_\\mathrm{CPU} = 50 \\times 10^{9}$ bytes/s.\n- GPU: $P_\\mathrm{GPU} = 10 \\times 10^{12}$ flops/s, $\\beta_\\mathrm{GPU} = 900 \\times 10^{9}$ bytes/s.\n\nThe per-element time for the explicit method, $T_\\mathrm{explicit}(m)$, is the sum of the assembly time, $T_\\mathrm{asm}(m)$, and the matrix-vector product time, $T_\\mathrm{spmv}(m)$.\n- Assembly: $F_\\mathrm{asm} = 3 m^6$ flops and $B_\\mathrm{asm} = 8 m^6$ bytes.\n  $$T_\\mathrm{asm}(m) = \\max\\left(\\frac{3 m^6}{P}, \\frac{8 m^6}{\\beta}\\right)$$\n- Matrix-vector product: $F_\\mathrm{spmv} = 2 m^6$ flops and $B_\\mathrm{spmv} = 8 m^6 + 16 m^3$ bytes.\n  $$T_\\mathrm{spmv}(m) = \\max\\left(\\frac{2 m^6}{P}, \\frac{8 m^6 + 16 m^3}{\\beta}\\right)$$\n- Total per-element explicit time:\n  $$T_\\mathrm{explicit}(m) = T_\\mathrm{asm}(m) + T_\\mathrm{spmv}(m)$$\n\nThe per-element time for the matrix-free method, $T_\\mathrm{mf}(m)$, is given by:\n- Matrix-free apply: $F_\\mathrm{mf} = 12 m^4$ flops and $B_\\mathrm{mf} = 16 m^3$ bytes.\n  $$T_\\mathrm{mf}(m) = \\max\\left(\\frac{12 m^4}{P}, \\frac{16 m^3}{\\beta}\\right)$$\n\nFor the GPU, there is a one-time data transfer cost for the matrix-free method operators.\n- Transfer: $B_\\mathrm{transfer}(m) = (3 m^2 + m) \\cdot 8$ bytes.\n  $$T_\\mathrm{transfer}(m) = \\frac{B_\\mathrm{transfer}(m)}{\\beta_\\mathrm{GPU}}$$\n\nThe total runtimes for a problem with $E$ elements are:\n- CPU:\n  $T_\\mathrm{explicit,total}(m, E) = E \\cdot T_\\mathrm{explicit}(m)$\n  $T_\\mathrm{mf,total}(m, E) = E \\cdot T_\\mathrm{mf}(m)$\n- GPU:\n  $T_\\mathrm{explicit,total}(m, E) = E \\cdot T_\\mathrm{explicit}(m)$\n  $T_\\mathrm{mf,total}(m, E) = T_\\mathrm{transfer}(m) + E \\cdot T_\\mathrm{mf}(m)$\n\nWe seek the minimal pair $(p_c, E_c)$ from the test suites such that $T_\\mathrm{mf,total}(m, E) \\le T_\\mathrm{explicit,total}(m, E)$.\n\n**Analysis for CPU**\n\nFor the CPU, the condition for the matrix-free method to be advantageous is:\n$$E \\cdot T_\\mathrm{mf}(m) \\le E \\cdot T_\\mathrm{explicit}(m)$$\nThis simplifies to:\n$$T_\\mathrm{mf}(m) \\le T_\\mathrm{explicit}(m)$$\nThe condition is independent of the number of elements $E$. We must find the smallest $p \\in \\{1, ..., 6\\}$ that satisfies this inequality. The problem then asks for the smallest $E$ from the test suite, which is $E_c = 1$.\n\nLet's test for $p=1$, which gives $m=2$.\nUsing $P = P_\\mathrm{CPU}$ and $\\beta = \\beta_\\mathrm{CPU}$:\n$T_\\mathrm{asm}(2) = \\max\\left(\\frac{3 \\cdot 2^6}{100 \\cdot 10^9}, \\frac{8 \\cdot 2^6}{50 \\cdot 10^9}\\right) = \\max(1.92 \\cdot 10^{-9}, 10.24 \\cdot 10^{-9}) = 10.24 \\cdot 10^{-9}$ s.\n$T_\\mathrm{spmv}(2) = \\max\\left(\\frac{2 \\cdot 2^6}{100 \\cdot 10^9}, \\frac{8 \\cdot 2^6 + 16 \\cdot 2^3}{50 \\cdot 10^9}\\right) = \\max(1.28 \\cdot 10^{-9}, 12.8 \\cdot 10^{-9}) = 12.8 \\cdot 10^{-9}$ s.\n$T_\\mathrm{explicit}(2) = 10.24 \\cdot 10^{-9} + 12.8 \\cdot 10^{-9} = 23.04 \\cdot 10^{-9}$ s.\n\n$T_\\mathrm{mf}(2) = \\max\\left(\\frac{12 \\cdot 2^4}{100 \\cdot 10^9}, \\frac{16 \\cdot 2^3}{50 \\cdot 10^9}\\right) = \\max(1.92 \\cdot 10^{-9}, 2.56 \\cdot 10^{-9}) = 2.56 \\cdot 10^{-9}$ s.\n\nThe inequality $2.56 \\cdot 10^{-9} \\le 23.04 \\cdot 10^{-9}$ is true.\nSince the condition is met for the smallest polynomial degree $p=1$, the minimal crossover pair for the CPU is $(p_\\mathrm{CPU}, E_\\mathrm{CPU}) = (1, 1)$.\n\n**Analysis for GPU**\n\nFor the GPU, the condition is:\n$$T_\\mathrm{transfer}(m) + E \\cdot T_\\mathrm{mf}(m) \\le E \\cdot T_\\mathrm{explicit}(m)$$\nThis can be rearranged to solve for $E$:\n$$T_\\mathrm{transfer}(m) \\le E \\cdot (T_\\mathrm{explicit}(m) - T_\\mathrm{mf}(m))$$\nA solution exists only if $T_\\mathrm{explicit}(m) > T_\\mathrm{mf}(m)$. If this holds, the crossover occurs for any E satisfying:\n$$E \\ge \\frac{T_\\mathrm{transfer}(m)}{T_\\mathrm{explicit}(m) - T_\\mathrm{mf}(m)}$$\nWe search for the smallest $p$ for which $T_\\mathrm{explicit}(m) > T_\\mathrm{mf}(m)$, and then find the smallest $E$ from its test suite that satisfies the condition.\n\nLet's test for $p=1$, which gives $m=2$.\nUsing $P = P_\\mathrm{GPU}$ and $\\beta = \\beta_\\mathrm{GPU}$:\n$T_\\mathrm{asm}(2) = \\max\\left(\\frac{3 \\cdot 2^6}{10 \\cdot 10^{12}}, \\frac{8 \\cdot 2^6}{900 \\cdot 10^9}\\right) = \\max(1.92 \\cdot 10^{-14}, 0.569 \\cdot 10^{-9}) \\approx 0.569 \\cdot 10^{-9}$ s.\n$T_\\mathrm{spmv}(2) = \\max\\left(\\frac{2 \\cdot 2^6}{10 \\cdot 10^{12}}, \\frac{8 \\cdot 2^6 + 16 \\cdot 2^3}{900 \\cdot 10^9}\\right) = \\max(1.28 \\cdot 10^{-14}, 0.711 \\cdot 10^{-9}) \\approx 0.711 \\cdot 10^{-9}$ s.\n$T_\\mathrm{explicit}(2) \\approx 0.569 \\cdot 10^{-9} + 0.711 \\cdot 10^{-9} \\approx 1.280 \\cdot 10^{-9}$ s.\n\n$T_\\mathrm{mf}(2) = \\max\\left(\\frac{12 \\cdot 2^4}{10 \\cdot 10^{12}}, \\frac{16 \\cdot 2^3}{900 \\cdot 10^9}\\right) = \\max(1.92 \\cdot 10^{-14}, 0.142 \\cdot 10^{-9}) \\approx 0.142 \\cdot 10^{-9}$ s.\n\nThe condition $T_\\mathrm{explicit}(2) > T_\\mathrm{mf}(2)$ holds ($1.280 \\cdot 10^{-9} > 0.142 \\cdot 10^{-9}$). Now we find the threshold for $E$.\n$T_\\mathrm{transfer}(2) = \\frac{(3 \\cdot 2^2 + 2) \\cdot 8}{900 \\cdot 10^9} = \\frac{112}{900 \\cdot 10^9} \\approx 0.124 \\cdot 10^{-9}$ s.\n$E \\ge \\frac{0.124 \\cdot 10^{-9}}{1.280 \\cdot 10^{-9} - 0.142 \\cdot 10^{-9}} = \\frac{0.124}{1.138} \\approx 0.109$.\n\nThe smallest integer value for $E$ in the test suite $\\{1, 8, 64, 512\\}$ that is greater than or equal to $0.109$ is $E=1$.\nSince a crossover is found for the smallest polynomial degree $p=1$, the minimal crossover pair for the GPU is $(p_\\mathrm{GPU}, E_\\mathrm{GPU}) = (1, 1)$.\n\n**Conclusion**\nThe analysis shows that for both the CPU and GPU architectures, the sum factorization method is superior to the explicit assembly method starting from the lowest-order polynomial ($p=1$) and the smallest mesh size ($E=1$) in the test suite.\n- For CPU: $p_c=1, E_c=1$.\n- For GPU: $p_c=1, E_c=1$.\nThe final result is thus $[1, 1, 1, 1]$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the crossover point (minimal polynomial degree p and element count E)\n    where the sum factorization method becomes more efficient than the explicit\n    assembly method for DG operator application on CPU and GPU architectures.\n    \"\"\"\n\n    # --- Architecture Parameters ---\n    P_CPU = 100e9  # Peak flops/s\n    BETA_CPU = 50e9  # Memory bandwidth B/s\n    P_GPU = 10e12   # Peak flops/s\n    BETA_GPU = 900e9 # Memory bandwidth B/s\n\n    # --- Test Suite ---\n    p_values = [1, 2, 3, 4, 5, 6]\n    E_values = [1, 8, 64, 512]\n\n    def find_crossover(P, beta, is_gpu, p_vals, E_vals):\n        \"\"\"\n        Finds the minimal (p, E) pair for a given architecture.\n        \n        Args:\n            P (float): Peak throughput in flops/s.\n            beta (float): Memory bandwidth in B/s.\n            is_gpu (bool): Flag indicating if the architecture is a GPU.\n            p_vals (list): List of polynomial degrees to test.\n            E_vals (list): List of element counts to test.\n\n        Returns:\n            tuple: A tuple (p, E) representing the crossover point, or (-1, -1) if none found.\n        \"\"\"\n        for p in p_vals:\n            m = float(p + 1)\n\n            # --- Explicit Method Time Calculation ---\n            # Assembly\n            F_asm = 3.0 * m**6\n            B_asm = 8.0 * m**6\n            T_asm = max(F_asm / P, B_asm / beta)\n\n            # Dense Matrix-Vector Multiply\n            F_spmv = 2.0 * m**6\n            B_spmv = 8.0 * m**6 + 16.0 * m**3\n            T_spmv = max(F_spmv / P, B_spmv / beta)\n            \n            T_explicit_per_element = T_asm + T_spmv\n\n            # --- Sum Factorization (Matrix-Free) Method Time Calculation ---\n            F_mf = 12.0 * m**4\n            B_mf = 16.0 * m**3\n            T_mf_per_element = max(F_mf / P, B_mf / beta)\n\n            # --- One-time Transfer Cost for GPU ---\n            T_transfer = 0.0\n            if is_gpu:\n                B_transfer = (3.0 * m**2 + m) * 8.0\n                T_transfer = B_transfer / beta\n            \n            for E in E_vals:\n                # --- Total Time Calculation ---\n                T_explicit_total = E * T_explicit_per_element\n                T_mf_total = E * T_mf_per_element + T_transfer\n\n                # --- Crossover Check ---\n                if T_mf_total <= T_explicit_total:\n                    return p, E\n        \n        return -1, -1\n\n    # Find crossover points for both architectures\n    p_cpu, E_cpu = find_crossover(P_CPU, BETA_CPU, False, p_values, E_values)\n    p_gpu, E_gpu = find_crossover(P_GPU, BETA_GPU, True, p_values, E_values)\n\n    # Format the final output\n    results = [p_cpu, E_cpu, p_gpu, E_gpu]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3422374"}]}