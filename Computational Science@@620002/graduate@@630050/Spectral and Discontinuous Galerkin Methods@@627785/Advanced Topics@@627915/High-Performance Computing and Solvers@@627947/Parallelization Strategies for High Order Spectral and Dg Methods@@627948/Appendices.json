{"hands_on_practices": [{"introduction": "To optimize a scientific code, we must first diagnose what limits its performance. The Roofline model provides a powerful visual and analytical framework for this task by relating a kernel's computational intensity to the hardware's peak performance and memory bandwidth. This exercise puts this model into practice, guiding you to calculate the arithmetic intensity of a high-order Discontinuous Galerkin (DG) kernel from performance measurements and determine whether it is limited by the processor's computational speed (compute-bound) or the speed of data transfer from memory (memory-bound). Understanding this distinction is the first step toward effective, targeted optimization [@problem_id:3407889].", "problem": "A high-order Discontinuous Galerkin (DG) method of polynomial degree $p=8$ is implemented for a three-dimensional hexahedral mesh using element-local parallelization and fused volume–surface kernels. A performance study on a single accelerator reports measurements for the DG kernel in one representative invocation: the total number of floating-point operations is $F=6.72\\times 10^{12}$ and the total data volume transferred between device global memory and streaming multiprocessors is $B=1.02\\times 10^{12}$ bytes. The device has a manufacturer-specified peak double-precision floating-point throughput $P_{\\mathrm{peak}}=9.7\\times 10^{12}$ flop/s and a peak memory bandwidth $B_{\\mathrm{peak}}=1.6\\times 10^{12}$ byte/s.\n\nStarting from the definitions underpinning the Roofline Model, use these measurements and hardware limits to determine, for this DG kernel:\n- the arithmetic intensity, expressed in flop per byte;\n- whether the kernel is compute-bound or memory-bound on this hardware.\n\nRound the arithmetic intensity to four significant figures. Express the intensity in flop per byte. Provide only the arithmetic intensity as your final answer.", "solution": "The problem requires the calculation of the arithmetic intensity for a given Discontinuous Galerkin (DG) kernel and a determination of whether the kernel is compute-bound or memory-bound. This analysis is based on the principles of the Roofline Performance Model.\n\nFirst, we define the arithmetic intensity, denoted by $I$. The arithmetic intensity of a computational kernel is the ratio of the total number of floating-point operations ($F$) performed to the total volume of data ($B$) transferred between the main memory (device global memory in this case) and the processor (streaming multiprocessors). The formula for arithmetic intensity is:\n$$I = \\frac{F}{B}$$\nThe units of arithmetic intensity are typically flops per byte.\n\nThe problem provides the following measured values for the DG kernel:\n-   Total floating-point operations: $F = 6.72\\times 10^{12}$ flop\n-   Total data volume transferred: $B = 1.02\\times 10^{12}$ bytes\n\nUsing these values, we can calculate the arithmetic intensity of the kernel:\n$$I = \\frac{6.72\\times 10^{12} \\text{ flop}}{1.02\\times 10^{12} \\text{ byte}}$$\nThe factor of $10^{12}$ cancels out, simplifying the calculation:\n$$I = \\frac{6.72}{1.02} \\frac{\\text{flop}}{\\text{byte}} \\approx 6.588235... \\frac{\\text{flop}}{\\text{byte}}$$\nAs requested, rounding this to four significant figures gives the arithmetic intensity:\n$$I \\approx 6.588 \\frac{\\text{flop}}{\\text{byte}}$$\n\nNext, we must determine if the kernel is compute-bound or memory-bound. According to the Roofline Model, the attainable performance of a kernel, $P_{\\text{attainable}}$, is limited by both the peak floating-point throughput of the hardware, $P_{\\text{peak}}$, and the rate at which data can be supplied to the processor, which is the product of the arithmetic intensity $I$ and the peak memory bandwidth $B_{\\text{peak}}$. Mathematically, this is expressed as:\n$$P_{\\text{attainable}} \\le \\min(P_{\\text{peak}}, I \\times B_{\\text{peak}})$$\n\nThe hardware specifications are given as:\n-   Peak double-precision floating-point throughput: $P_{\\mathrm{peak}} = 9.7\\times 10^{12}$ flop/s\n-   Peak memory bandwidth: $B_{\\mathrm{peak}} = 1.6\\times 10^{12}$ byte/s\n\nA kernel is considered **memory-bound** if its performance is limited by the memory bandwidth, which occurs when $I \\times B_{\\text{peak}}  P_{\\text{peak}}$.\nA kernel is considered **compute-bound** if its performance is limited by the processor's computational capability, which occurs when $I \\times B_{\\text{peak}} > P_{\\text{peak}}$.\n\nThe transition between these two regimes occurs at the 'ridge point' of the Roofline plot. The arithmetic intensity at this point is known as the machine balance, $I_{\\text{machine}}$, and is calculated as the ratio of the peak performance to the peak bandwidth:\n$$I_{\\text{machine}} = \\frac{P_{\\text{peak}}}{B_{\\text{peak}}}$$\nWe can determine the performance bound of the kernel by comparing its arithmetic intensity $I$ to the machine balance $I_{\\text{machine}}$.\n\nLet's calculate the machine balance for the given hardware:\n$$I_{\\text{machine}} = \\frac{9.7\\times 10^{12} \\text{ flop/s}}{1.6\\times 10^{12} \\text{ byte/s}} = \\frac{9.7}{1.6} \\frac{\\text{flop}}{\\text{byte}} = 6.0625 \\frac{\\text{flop}}{\\text{byte}}$$\n\nNow, we compare the kernel's arithmetic intensity $I$ with the machine balance $I_{\\text{machine}}$:\n-   Kernel intensity: $I \\approx 6.588$ flop/byte\n-   Machine balance: $I_{\\text{machine}} = 6.0625$ flop/byte\n\nSince $I > I_{\\text{machine}}$ ($6.588 > 6.0625$), the performance of the DG kernel is limited by the floating-point throughput of the processor, not the memory bandwidth. Therefore, the kernel is **compute-bound**. The information that this is a high-order DG method with $p=8$ and fused kernels is consistent with this finding, as these techniques are specifically designed to increase the ratio of computation to memory access, thereby increasing arithmetic intensity and shifting kernels towards the compute-bound regime.\n\nThe problem asks for the arithmetic intensity and to determine if the kernel is compute-bound or memory-bound. We have found both. The final answer should be the value of the arithmetic intensity.", "answer": "$$\\boxed{6.588}$$", "id": "3407889"}, {"introduction": "Moving from a single kernel to a large-scale parallel simulation introduces a new critical performance factor: inter-process communication. The efficiency of a parallel algorithm hinges on the balance between the time spent on computation and the time spent communicating data. This practice guides you through the process of building a simple but insightful analytical performance model for a parallel DG solver [@problem_id:3407861]. By deriving an expression for the \"strong-scaling threshold\"—the point at which computation time equals communication time—you will learn how to predict the minimum workload per processor required to overcome communication overheads and achieve efficient parallel execution.", "problem": "Consider a two-dimensional Discontinuous Galerkin (DG) method for a scalar conservation law on a partitioned unstructured quadrilateral mesh executed with Message Passing Interface (MPI). Each parallel rank owns $E$ elements and has $F$ halo faces (faces on the subdomain boundary that require inter-rank data exchange each operator application). The DG discretization uses a nodal tensor-product basis of polynomial degree $p$ on each element. Assume the following modeling framework:\n\n- The number of nodal degrees of freedom per element scales as $(p+1)^{2}$, and the number of nodal degrees of freedom per face scales as $(p+1)$.\n- A halo exchange per operator application sends one double-precision scalar ($8$ bytes) per face degree of freedom for each halo face; the exchange is aggregated into a single logical message per rank per step.\n- The standard latency-bandwidth communication model applies: if $S$ is the total message size in bytes, the communication time per step is $T_{\\mathrm{comm}} = \\alpha + \\frac{S}{\\beta}$, where $\\alpha$ is the latency (in seconds) and $\\beta$ is the sustained bandwidth (in bytes per second).\n- The local DG operator application cost per element per step is proportional to its degrees of freedom, with a machine- and implementation-dependent constant $\\gamma$ (in seconds per degree of freedom per element), so that the total computation time per rank per step is $T_{\\mathrm{comp}} = \\gamma\\,E\\,(p+1)^{2}$.\n\nStarting from these assumptions, derive the total halo exchange size $S$ as a function of $p$ and $F$, insert it into the latency-bandwidth model to obtain $T_{\\mathrm{comm}}(p,F,\\alpha,\\beta)$, and then determine the strong-scaling threshold $E_{\\mathrm{th}}$ defined by the condition $T_{\\mathrm{comp}} = T_{\\mathrm{comm}}$. Express $E_{\\mathrm{th}}$ as a closed-form analytic expression in terms of $p$, $F$, $\\alpha$, $\\beta$, and $\\gamma$. Provide your final answer as the expression for $E_{\\mathrm{th}}$ only. No numerical evaluation is required.", "solution": "The goal is to derive an expression for the strong-scaling threshold, $E_{\\mathrm{th}}$, which is the number of elements per rank where the computation time equals the communication time. The derivation proceeds in three steps.\n\n**Step 1: Formulate the Communication Time $T_{\\mathrm{comm}}$**\nThe communication time is given by the latency-bandwidth model:\n$$T_{\\mathrm{comm}} = \\alpha + \\frac{S}{\\beta}$$\nThe total message size, $S$, depends on the amount of data exchanged in the halo update. Each of the $F$ halo faces has $(p+1)$ degrees of freedom. Each degree of freedom corresponds to a double-precision scalar of $8$ bytes. Therefore, the total size of the halo data in bytes is:\n$$S = 8 F (p+1)$$\nSubstituting this into the communication model gives:\n$$T_{\\mathrm{comm}}(p, F, \\alpha, \\beta) = \\alpha + \\frac{8 F (p+1)}{\\beta}$$\n\n**Step 2: Formulate the Computation Time $T_{\\mathrm{comp}}$**\nThe computation time per rank is given in the problem statement as a function of the number of elements $E$ and the polynomial degree $p$:\n$$T_{\\mathrm{comp}}(E, p, \\gamma) = \\gamma E (p+1)^2$$\n\n**Step 3: Determination of the Strong-Scaling Threshold $E_{\\mathrm{th}}$**\nThe strong-scaling threshold is found by setting $T_{\\mathrm{comp}} = T_{\\mathrm{comm}}$ and solving for the number of elements, which we denote $E_{\\mathrm{th}}$:\n$$\\gamma E_{\\mathrm{th}} (p+1)^2 = \\alpha + \\frac{8 F (p+1)}{\\beta}$$\nTo find the expression for $E_{\\mathrm{th}}$, we isolate it by dividing both sides by its coefficient, $\\gamma (p+1)^2$:\n$$E_{\\mathrm{th}} = \\frac{1}{\\gamma (p+1)^2} \\left(\\alpha + \\frac{8 F (p+1)}{\\beta}\\right)$$\nThis expression represents the minimum number of elements per processor required for the computational work to be at least as large as the communication overhead, a critical metric for assessing parallel efficiency.", "answer": "$$\\boxed{\\frac{1}{\\gamma (p+1)^{2}} \\left(\\alpha + \\frac{8 F (p+1)}{\\beta}\\right)}$$", "id": "3407861"}, {"introduction": "The performance of high-order methods is not just a function of parallelization strategy but is deeply tied to fundamental algorithmic and data structure choices. For DG methods, a critical decision is whether to explicitly assemble and store a large, sparse global matrix or to use a \"matrix-free\" approach that recomputes operator actions on-the-fly. This exercise explores the profound consequences of this choice by asking you to derive and compare the asymptotic memory and communication costs of both strategies [@problem_id:3407879]. This analysis reveals why matrix-free techniques are not merely an implementation detail but a necessary foundation for creating memory-efficient and scalable high-order solvers, especially as the polynomial degree $p$ increases.", "problem": "Consider a scalar linear operator discretized with the Discontinuous Galerkin (DG) method on a shape-regular mesh of tensor-product hexahedra in spatial dimension $d \\in \\{2,3\\}$, using a nodal tensor-product basis of degree $p$ per coordinate with Legendre–Gauss–Lobatto (LGL) nodes. Each element then has $n_{e}=(p+1)^{d}$ unknowns, and each $(d-1)$-dimensional face has $n_{f}=(p+1)^{d-1}$ trace unknowns. Assume interior elements have $c_{n}=2d$ face neighbors. You will compare two operator representations for a single scalar field: (i) an assembled global sparse matrix stored in Compressed Sparse Row (CSR) format, and (ii) a matrix-free representation based on sum-factorization that stores only per-element geometric factors and $1$-dimensional operators.\n\nUse the following foundational facts and definitions:\n- The DG discretization yields contributions from element-interior (volume) terms and face (surface) flux terms. Volume terms couple all unknowns within the same element. Face terms couple unknowns on a face to the volume unknowns of the adjacent elements through lifting operators.\n- For the assembled matrix, the element self-coupling block from volume terms is dense of size $n_{e} \\times n_{e}$. For each face neighbor, the off-diagonal coupling has nonzero columns only for the neighbor’s face trace unknowns (there are $n_{f}$ such columns), and these columns are dense in the $n_{e}$ rows of the receiving element due to the lifting.\n- In CSR, storing one nonzero requires a constant number of bytes for the value and column index; denote the combined constant by $\\gamma$ bytes per nonzero. For the purposes of asymptotic analysis in $p$, you may treat $\\gamma$ as a constant independent of $p$ and $d$.\n- In a matrix-free sum-factorization implementation with element-local geometric factors, the dominant per-element storage scales linearly with $n_{e}$ (for example, storing metric terms and Jacobian determinants at tensor-product quadrature points). Model this per-element storage as $\\kappa\\, n_{e}$ bytes with $\\kappa$ constant in $p$ and $d$.\n- Consider a domain decomposition across processes. Let $S$ be the number of shared faces between a given process and its neighbors. For a single operator application:\n  - In the assembled sparse matrix-vector multiplication (SpMV), the only remote vector entries needed are those appearing in nonzero columns of off-diagonal blocks, i.e., the neighbor’s face trace entries. Model the communication volume as proportional to $S\\, n_{f}$ scalars.\n  - In the matrix-free face flux evaluation, the remote data required are likewise the neighbor’s face trace entries on each shared face, also proportional to $S\\, n_{f}$ scalars.\n\nTasks:\n1. Derive the leading-order per-element memory footprint $M_{\\mathrm{asm}}(p,d)$ for the assembled matrix in terms of $p$ and $d$, ignoring constants and lower-order terms in $p$.\n2. Derive the leading-order per-element memory footprint $M_{\\mathrm{mf}}(p,d)$ for the matrix-free operator storage in terms of $p$ and $d$, ignoring constants and lower-order terms in $p$.\n3. Using the above, form the asymptotic ratio $R_{\\mathrm{mem}}(p,d)=M_{\\mathrm{asm}}(p,d)/M_{\\mathrm{mf}}(p,d)$, simplified to its leading dependence on $p$ and $d$.\n4. Derive the leading-order communication volumes per operator application, $C_{\\mathrm{asm}}(p,d,S)$ and $C_{\\mathrm{mf}}(p,d,S)$, and form the ratio $R_{\\mathrm{comm}}(p,d)=C_{\\mathrm{asm}}(p,d,S)/C_{\\mathrm{mf}}(p,d,S)$, simplified to its leading dependence on $p$ and $d$.\n5. Provide, as your final answer, the row vector $\\bigl(R_{\\mathrm{mem}}(p,d),\\,R_{\\mathrm{comm}}(p,d)\\bigr)$ in closed form.\n\nYour derivations must start from the definitions above and standard properties of Discontinuous Galerkin operators and tensor-product bases. Ignore multiplicative constants that do not depend on $p$ or $d$, and ignore terms that are lower order in $p$ compared to the leading term. The final answer must be the single row vector expression described in task $5$ (no units).", "solution": "This problem requires deriving and comparing the asymptotic memory and communication costs for assembled versus matrix-free representations of a DG operator. The analysis focuses on the scaling with respect to the polynomial degree $p$ in $d$ spatial dimensions. We use the definitions $n_{e} = (p+1)^{d}$ for unknowns per element and $n_{f} = (p+1)^{d-1}$ for unknowns per face.\n\n### Task 1  2: Per-Element Memory Footprints ($M_{\\mathrm{asm}}$ and $M_{\\mathrm{mf}}$)\n\n**Assembled Matrix ($M_{\\mathrm{asm}}$):** The memory footprint is proportional to the number of nonzero entries per element's rows in the global matrix.\n- **Volume term coupling:** All $n_e$ unknowns within an element are coupled, creating a dense block with $n_e^2$ nonzeros.\n- **Face term coupling:** Each of the $c_n = 2d$ face neighbors contributes $n_e \\times n_f$ nonzeros.\nThe total number of nonzeros per element is $nnz_{e} = n_{e}^{2} + c_{n} n_{e} n_{f}$. The memory footprint is therefore:\n$$M_{\\mathrm{asm}}(p,d) \\propto n_{e}^{2} + 2d n_{e} n_{f} = ((p+1)^{d})^{2} + 2d (p+1)^{d} (p+1)^{d-1} = (p+1)^{2d} + 2d (p+1)^{2d-1}$$\nFor large $p$, the dominant term is the one with the highest power of $p$. Thus, the leading-order memory footprint is:\n$$M_{\\mathrm{asm}}(p,d) \\propto (p+1)^{2d}$$\n\n**Matrix-Free ($M_{\\mathrm{mf}}$):** The storage is dominated by geometric factors, which scales linearly with the number of points per element, $n_e$.\n$$M_{\\mathrm{mf}}(p,d) \\propto n_e = (p+1)^{d}$$\n\n### Task 3: Asymptotic Memory Ratio ($R_{\\mathrm{mem}}$)\n\nThe ratio of the leading-order memory footprints is:\n$$R_{\\mathrm{mem}}(p,d) = \\frac{M_{\\mathrm{asm}}(p,d)}{M_{\\mathrm{mf}}(p,d)} \\propto \\frac{(p+1)^{2d}}{(p+1)^{d}} = (p+1)^{d}$$\n\n### Task 4: Asymptotic Communication Ratio ($R_{\\mathrm{comm}}$)\n\nThe communication volume is determined by the data required from neighboring processes to perform the operator application.\n- **Assembled SpMV ($C_{\\mathrm{asm}}$):** The required remote data are the vector entries corresponding to the face unknowns, so the volume is proportional to the number of shared faces $S$ times the unknowns per face $n_f$.\n$$C_{\\mathrm{asm}}(p,d,S) \\propto S \\cdot n_f = S(p+1)^{d-1}$$\n- **Matrix-Free Flux ($C_{\\mathrm{mf}}$):** The halo exchange for face flux computation likewise requires the neighbor's face trace values.\n$$C_{\\mathrm{mf}}(p,d,S) \\propto S \\cdot n_f = S(p+1)^{d-1}$$\n\nThe ratio of the communication volumes is therefore:\n$$R_{\\mathrm{comm}}(p,d) = \\frac{C_{\\mathrm{asm}}(p,d,S)}{C_{\\mathrm{mf}}(p,d,S)} \\propto \\frac{S(p+1)^{d-1}}{S(p+1)^{d-1}} = 1$$\n\n### Task 5: Final Answer Vector\n\nCombining the results for the memory and communication ratios, we obtain the final row vector:\n$$\\bigl(R_{\\mathrm{mem}}(p,d),\\,R_{\\mathrm{comm}}(p,d)\\bigr) = \\bigl((p+1)^{d}, \\, 1\\bigr)$$", "answer": "$$\\boxed{\\begin{pmatrix} (p+1)^{d}  1 \\end{pmatrix}}$$", "id": "3407879"}]}