{"hands_on_practices": [{"introduction": "The efficiency of sparse grid methods hinges on a clever selection of tensor-product building blocks, a process governed by the Smolyak index set. Before diving into complex applications, it is crucial to develop a concrete intuition for this set's structure. This first exercise [@problem_id:3415802] provides a foundational, hands-on opportunity to demystify the abstract definition by manually enumerating the multi-indices for a low-dimensional case, connecting the concept to fundamental combinatorial principles.", "problem": "Consider the isotropic sparse-grid Smolyak index set used to construct hierarchical tensor-product polynomial subspaces for high-dimensional approximation in the context of Spectral and Discontinuous Galerkin (DG) methods. The Smolyak truncated index set is defined by\n$$\n\\Lambda_{d,q}=\\left\\{\\boldsymbol{\\ell}\\in\\mathbb{N}^{d}:\\ |\\boldsymbol{\\ell}|_{1}\\le q,\\ \\ell_{i}\\ge 1\\ \\text{for all}\\ i\\in\\{1,\\dots,d\\}\\right\\},\n$$\nwhere $\\mathbb{N}$ denotes the set of positive integers, $|\\boldsymbol{\\ell}|_{1}=\\sum_{i=1}^{d}\\ell_{i}$ is the $\\ell^{1}$-norm, and $q$ is the total level parameter. This index set is central in defining the hierarchical surplus combination in the Smolyak operator and governs the number of anisotropic tensor-product building blocks used to approximate a function in $d$ dimensions.\n\nStarting from the combinatorial interpretation of compositions of integers into $d$ positive parts, and without invoking pre-derived shortcut formulas, determine the complete enumeration of $\\Lambda_{3,5}$ and compute the cardinality $\\#\\Lambda_{3,5}$. Your final answer must be the single value of $\\#\\Lambda_{3,5}$, with no rounding necessary.", "solution": "The problem requires the determination of the cardinality of the Smolyak index set $\\Lambda_{d,q}$ for specific parameters $d=3$ and $q=5$. The set is defined as:\n$$\n\\Lambda_{d,q}=\\left\\{\\boldsymbol{\\ell}\\in\\mathbb{N}^{d}:\\ |\\boldsymbol{\\ell}|_{1}\\le q,\\ \\ell_{i}\\ge 1\\ \\text{for all}\\ i\\in\\{1,\\dots,d\\}\\right\\}\n$$\nFor the given parameters $d=3$ and $q=5$, the set is:\n$$\n\\Lambda_{3,5}=\\left\\{\\boldsymbol{\\ell}=(\\ell_1, \\ell_2, \\ell_3) \\in\\mathbb{N}^{3}:\\ \\ell_1+\\ell_2+\\ell_3 \\le 5,\\ \\text{with}\\ \\ell_1, \\ell_2, \\ell_3 \\ge 1\\right\\}\n$$\nHere, $\\mathbb{N}$ represents the set of positive integers $\\{1, 2, 3, \\dots\\}$.\n\nTo determine the cardinality $\\#\\Lambda_{3,5}$, we must count the number of integer vectors $(\\ell_1, \\ell_2, \\ell_3)$ satisfying the given conditions. The condition $\\ell_1+\\ell_2+\\ell_3 \\le 5$ can be decomposed into a set of equalities. Let $k = \\ell_1+\\ell_2+\\ell_3$. Since each $\\ell_i \\ge 1$, the smallest possible value for the sum $k$ is $1+1+1=3$. The largest possible value for $k$ is given by the inequality, so $k \\le 5$. Therefore, we need to find the number of solutions for $k=3$, $k=4$, and $k=5$. The total cardinality will be the sum of the number of solutions for each case.\n\nLet $N_k$ be the number of solutions to the equation $\\ell_1+\\ell_2+\\ell_3 = k$ where $\\ell_1, \\ell_2, \\ell_3$ are positive integers. The total cardinality is $\\#\\Lambda_{3,5} = N_3 + N_4 + N_5$.\n\nWe are instructed to follow the combinatorial interpretation of compositions of integers. A composition of a positive integer $k$ into $d$ parts is a way of writing $k$ as an ordered sum of $d$ positive integers. The number of such compositions is given by $\\binom{k-1}{d-1}$. This can be visualized using the \"stars and bars\" method. To represent $k$ as a sum of $d$ positive integers, we can imagine $k$ objects (stars) in a row. There are $k-1$ spaces between them. To partition the $k$ stars into $d$ non-empty groups, we need to place $d-1$ dividers (bars) into these $k-1$ available spaces. The number of ways to choose $d-1$ spaces from $k-1$ is $\\binom{k-1}{d-1}$.\n\nFor our problem, $d=3$, so we need to find the number of compositions of $k$ into $3$ parts, which is given by $\\binom{k-1}{3-1} = \\binom{k-1}{2}$.\n\nCase 1: $k=3$\nWe need to find the number of solutions to $\\ell_1+\\ell_2+\\ell_3 = 3$ with $\\ell_i \\ge 1$.\nUsing the formula for compositions, the number of solutions is $N_3 = \\binom{3-1}{2} = \\binom{2}{2} = 1$.\nThe single solution is $(\\ell_1, \\ell_2, \\ell_3) = (1,1,1)$. Let's list the corresponding set of indices:\n$$ S_3 = \\{(1,1,1)\\} $$\n\nCase 2: $k=4$\nWe need to find the number of solutions to $\\ell_1+\\ell_2+\\ell_3 = 4$ with $\\ell_i \\ge 1$.\nThe number of solutions is $N_4 = \\binom{4-1}{2} = \\binom{3}{2} = \\frac{3!}{2!1!} = 3$.\nTo enumerate them, we can list the partitions of $4$ into $3$ parts and then consider their distinct permutations. The only partition of $4$ into $3$ positive integers is $2+1+1$. The distinct permutations of $(2,1,1)$ are $(2,1,1)$, $(1,2,1)$, and $(1,1,2)$.\nThe set of indices is:\n$$ S_4 = \\{(2,1,1), (1,2,1), (1,1,2)\\} $$\n\nCase 3: $k=5$\nWe need to find the number of solutions to $\\ell_1+\\ell_2+\\ell_3 = 5$ with $\\ell_i \\ge 1$.\nThe number of solutions is $N_5 = \\binom{5-1}{2} = \\binom{4}{2} = \\frac{4!}{2!2!} = \\frac{4 \\times 3}{2} = 6$.\nTo enumerate them, we find the partitions of $5$ into $3$ positive integers:\n- $3+1+1$: The distinct permutations of $(3,1,1)$ are $(3,1,1)$, $(1,3,1)$, and $(1,1,3)$.\n- $2+2+1$: The distinct permutations of $(2,2,1)$ are $(2,2,1)$, $(2,1,2)$, and $(1,2,2)$.\nThere are $3+3=6$ solutions in total. The set of indices is:\n$$ S_5 = \\{(3,1,1), (1,3,1), (1,1,3), (2,2,1), (2,1,2), (1,2,2)\\} $$\n\nThe complete enumeration of the set $\\Lambda_{3,5}$ is the union of the sets from each case:\n$\\Lambda_{3,5} = S_3 \\cup S_4 \\cup S_5$.\n$$ \\Lambda_{3,5} = \\{(1,1,1), (2,1,1), (1,2,1), (1,1,2), (3,1,1), (1,3,1), (1,1,3), (2,2,1), (2,1,2), (1,2,2)\\} $$\nThe cardinality of $\\Lambda_{3,5}$ is the sum of the cardinalities from each case:\n$$ \\#\\Lambda_{3,5} = N_3 + N_4 + N_5 = 1 + 3 + 6 = 10 $$\nThus, there are $10$ elements in the sparse-grid index set $\\Lambda_{3,5}$.", "answer": "$$\\boxed{10}$$", "id": "3415802"}, {"introduction": "With a firm grasp of the index set, we can now explore its primary function: guiding the construction of an efficient approximation. This practice [@problem_id:3415859] bridges the gap between the abstract combinatorial structure and its application in spectral methods. By computing the spectral coefficients of a function using a sparse, Smolyak-type selection of Chebyshev polynomial basis functions, you will see firsthand how this hierarchical selection prunes less important terms and why properties like function parity lead to a naturally sparse representation.", "problem": "Let $f(x,y) = \\cos(\\pi x)\\cos(\\pi y)$ on the tensor product domain $[-1,1]\\times[-1,1]$. Consider the spectral approximation in the tensor-product basis of Chebyshev polynomials of the first kind $\\{T_m(x)T_n(y)\\}_{m,n\\ge 0}$ with respect to the separable Chebyshev weight $w(x)w(y)$, where $w(x) = (1-x^2)^{-1/2}$. Define the spectral coefficients $\\{c_{m,n}\\}_{m,n\\ge 0}$ by the weighted $L^2$ inner product\n$$\nc_{m,n} = \\gamma_m \\gamma_n \\int_{-1}^{1}\\int_{-1}^{1} f(x,y)\\,T_m(x)\\,T_n(y)\\,w(x)\\,w(y)\\,dx\\,dy,\n$$\nwhere $\\gamma_0 = \\frac{1}{\\pi}$ and $\\gamma_k = \\frac{2}{\\pi}$ for $k\\ge 1$. The corresponding spectral expansion is formed in the standard Chebyshev sense (with the conventional half-weights on zero indices implied by the above normalization).\n\nAdopt a Smolyak-type sparse polynomial index set based on isotropic hierarchical levels identified with the polynomial degrees in each coordinate. Specifically, for a fixed level $L\\in\\mathbb{N}$, use the index set\n$$\n\\mathcal{S}_L = \\{(m,n)\\in \\mathbb{N}_0^2 : m+n \\le L\\}.\n$$\nThis index set encodes a hierarchical selection of modes that reduces the number of retained terms compared to the full tensor-product truncation $\\{0,\\dots,L\\}^2$.\n\nTasks:\n- Derive the orthogonality relations for $\\{T_k\\}_{k\\ge 0}$ with respect to the Chebyshev weight, and use them to justify the coefficient definition above from first principles of Galerkin projection.\n- For $L=4$, determine the cardinality of the sparse Smolyak index set $\\mathcal{S}_4$ and compare it with the full tensor-product set $\\{0,1,2,3,4\\}^2$, explaining how hierarchical selection reduces the number of terms.\n- Using separability and parity considerations, determine which coefficients $c_{m,n}$ in $\\mathcal{S}_4$ vanish for $f(x,y) = \\cos(\\pi x)\\cos(\\pi y)$.\n- Compute the specific sparse spectral coefficient $c_{2,2}$ exactly, expressing your final answer as a single closed-form analytic expression.\n\nYour final answer must be only the single expression for $c_{2,2}$. Do not include units. Do not provide a numerical approximation.", "solution": "The problem is well-posed, scientifically grounded, and provides a complete set of definitions and data required for its solution. It is a standard problem in the field of spectral methods and approximation theory. We proceed with the solution, addressing each task in the stated order.\n\nFirst, we address the justification of the spectral coefficient formula from the principles of Galerkin projection. A spectral Galerkin projection seeks an approximation $f_S(x,y) = \\sum_{(m,n) \\in \\mathcal{I}} c_{m,n} T_m(x)T_n(y)$ to a function $f(x,y)$ from a finite-dimensional space spanned by a basis set $\\{\\phi_{m,n}(x,y) = T_m(x)T_n(y)\\}_{(m,n) \\in \\mathcal{I}}$. The Galerkin condition requires the residual $f(x,y) - f_S(x,y)$ to be orthogonal to every basis function in the spanning set with respect to a chosen inner product. The inner product is the weighted $L^2$ inner product on the domain $\\Omega = [-1,1]^2$ with weight $W(x,y) = w(x)w(y) = (1-x^2)^{-1/2}(1-y^2)^{-1/2}$:\n$$\n\\langle g, h \\rangle_W = \\int_{-1}^{1}\\int_{-1}^{1} g(x,y)h(x,y)W(x,y)\\,dx\\,dy.\n$$\nThe Galerkin condition is $\\langle f - f_S, T_p(x)T_q(y) \\rangle_W = 0$ for all $(p,q) \\in \\mathcal{I}$. Expanding $f_S$, we get:\n$$\n\\langle f, T_p T_q \\rangle_W = \\left\\langle \\sum_{(m,n) \\in \\mathcal{I}} c_{m,n} T_m T_n, T_p T_q \\right\\rangle_W = \\sum_{(m,n) \\in \\mathcal{I}} c_{m,n} \\langle T_m T_n, T_p T_q \\rangle_W.\n$$\nThe Chebyshev polynomials of the first kind $\\{T_k(x)\\}_{k\\ge 0}$ are orthogonal on $[-1,1]$ with respect to the weight $w(x)=(1-x^2)^{-1/2}$. The orthogonality relation is:\n$$\n\\int_{-1}^{1} T_m(x)T_p(x)w(x)\\,dx = \\begin{cases} 0 & \\text{if } m \\neq p \\\\ \\pi & \\text{if } m=p=0 \\\\ \\frac{\\pi}{2} & \\text{if } m=p>0 \\end{cases}\n$$\nDue to separability, the inner product of the tensor-product basis functions is:\n$$\n\\langle T_m T_n, T_p T_q \\rangle_W = \\left(\\int_{-1}^{1} T_m(x)T_p(x)w(x)\\,dx\\right) \\left(\\int_{-1}^{1} T_n(y)T_q(y)w(y)\\,dy\\right) = C_m C_n \\delta_{mp}\\delta_{nq},\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta and $C_k = \\langle T_k, T_k \\rangle_w$ are the normalization constants, $C_0=\\pi$ and $C_k=\\pi/2$ for $k\\ge 1$. Substituting this into the Galerkin condition gives:\n$$\n\\langle f, T_p T_q \\rangle_W = \\sum_{(m,n) \\in \\mathcal{I}} c_{m,n} C_m C_n \\delta_{mp}\\delta_{nq} = c_{p,q} C_p C_q.\n$$\nSolving for the coefficient $c_{p,q}$ yields:\n$$\nc_{p,q} = \\frac{1}{C_p C_q} \\langle f, T_p T_q \\rangle_W = \\frac{1}{C_p C_q} \\int_{-1}^{1}\\int_{-1}^{1} f(x,y)T_p(x)T_q(y)w(x)w(y)\\,dx\\,dy.\n$$\nThe constants $\\gamma_k$ are defined as $1/C_k$. We have $\\gamma_0 = 1/C_0 = 1/\\pi$ and $\\gamma_k = 1/C_k = 2/\\pi$ for $k \\ge 1$. Thus, the provided formula for $c_{m,n}$ is precisely the one derived from the Galerkin projection principle onto an orthogonal basis.\n\nSecond, we determine the cardinality of the index sets for $L=4$. The sparse Smolyak-type index set is $\\mathcal{S}_4 = \\{(m,n) \\in \\mathbb{N}_0^2 : m+n \\le 4\\}$. The number of non-negative integer solutions to $m+n \\le L$ for dimension $d=2$ is given by the formula $\\binom{L+d}{d}$. For $L=4$ and $d=2$, the cardinality is:\n$$\n|\\mathcal{S}_4| = \\binom{4+2}{2} = \\binom{6}{2} = \\frac{6 \\times 5}{2} = 15.\n$$\nThe full tensor-product set is $\\mathcal{I}_4 = \\{(m,n) : 0 \\le m \\le 4, 0 \\le n \\le 4\\}$. Its cardinality is simply $(L+1) \\times (L+1)$:\n$$\n|\\mathcal{I}_4| = (4+1)^2 = 5^2 = 25.\n$$\nThe sparse grid approach retains $15$ basis functions, whereas the full tensor-product truncation uses $25$. The sparse grid prunes terms with high interaction order (i.e., where both $m$ and $n$ are large, such as $(m,n)=(3,3)$ or $(4,2)$), which are not in $\\mathcal{S}_4$ but are in a full tensor product grid of maximum degree $L=4$ in each dimension. This hierarchical selection is based on the premise that the function's regularity implies that modes with a high total polynomial degree $m+n$ contribute less to the overall approximation.\n\nThird, we determine which coefficients $c_{m,n}$ in $\\mathcal{S}_4$ vanish. The function is $f(x,y) = \\cos(\\pi x)\\cos(\\pi y)$. Due to the separability of the function, domain, and weight, the coefficient formula separates:\n$$\nc_{m,n} = \\left( \\gamma_m \\int_{-1}^{1} \\cos(\\pi x)T_m(x)w(x)\\,dx \\right) \\left( \\gamma_n \\int_{-1}^{1} \\cos(\\pi y)T_n(y)w(y)\\,dy \\right).\n$$\nLet's analyze the one-dimensional integral $I_k = \\int_{-1}^{1} \\cos(\\pi x)T_k(x)w(x)\\,dx$. The function $\\cos(\\pi x)$ is even, and the weight $w(x)=(1-x^2)^{-1/2}$ is also even. The Chebyshev polynomial $T_k(x)$ is an even function for even $k$ and an odd function for odd $k$. The parity of the integrand $\\cos(\\pi x)T_k(x)w(x)$ is therefore determined by the parity of $T_k(x)$. If $k$ is odd, $T_k(x)$ is odd, and the integrand is an odd function. The integral of an odd function over a symmetric interval $[-1,1]$ is zero. Therefore, $I_k=0$ for all odd $k$.\nThis implies that $c_{m,n}$ will be zero if either $m$ is odd or $n$ is odd. The indices in $\\mathcal{S}_4$ for which $c_{m,n}$ will vanish are those with at least one odd component:\n$(1,0)$, $(0,1)$, $(1,1)$, $(3,0)$, $(2,1)$, $(1,2)$, $(0,3)$, $(3,1)$, and $(1,3)$. The coefficients that are potentially non-zero are those where both $m$ and $n$ are even: $(0,0)$, $(2,0)$, $(0,2)$, $(4,0)$, $(0,4)$, and $(2,2)$.\n\nFourth, we compute the specific coefficient $c_{2,2}$. From the separated form:\n$$\nc_{2,2} = \\gamma_2 \\gamma_2 \\left( \\int_{-1}^{1} \\cos(\\pi x)T_2(x)w(x)\\,dx \\right) \\left( \\int_{-1}^{1} \\cos(\\pi y)T_2(y)w(y)\\,dy \\right) = \\gamma_2^2 \\left( I_2 \\right)^2.\n$$\nWith $\\gamma_2 = 2/\\pi$, we have $\\gamma_2^2 = 4/\\pi^2$. We must evaluate the integral $I_2$:\n$$\nI_2 = \\int_{-1}^{1} \\cos(\\pi x)T_2(x)(1-x^2)^{-1/2}\\,dx.\n$$\nWe perform the standard substitution $x=\\cos(\\theta)$, which implies $dx = -\\sin(\\theta)d\\theta$. The integration limits $x \\in [-1,1]$ correspond to $\\theta \\in [\\pi,0]$. Under this substitution, $T_k(x) = T_k(\\cos\\theta) = \\cos(k\\theta)$, so $T_2(x) = \\cos(2\\theta)$. The term $(1-x^2)^{-1/2}dx$ becomes $(1-\\cos^2\\theta)^{-1/2}(-\\sin\\theta d\\theta) = (\\sin^2\\theta)^{-1/2}(-\\sin\\theta d\\theta) = -d\\theta$ for $\\theta \\in (0,\\pi)$.\nThe integral transforms to:\n$$\nI_2 = \\int_{\\pi}^{0} \\cos(\\pi\\cos\\theta)\\cos(2\\theta)(-d\\theta) = \\int_{0}^{\\pi} \\cos(\\pi\\cos\\theta)\\cos(2\\theta)\\,d\\theta.\n$$\nThis integral is a known integral representation of the Bessel function of the first kind, $J_n(z)$. The relevant identity is:\n$$\n\\int_0^\\pi \\cos(z\\cos\\theta)\\cos(n\\theta)\\,d\\theta = \\pi(-1)^{n/2} J_n(z), \\quad \\text{for } n \\text{ even}.\n$$\nApplying this identity with $z=\\pi$ and $n=2$:\n$$\nI_2 = \\pi(-1)^{2/2} J_2(\\pi) = \\pi(-1)^{1} J_2(\\pi) = -\\pi J_2(\\pi).\n$$\nNow, we substitute this result back into the expression for $c_{2,2}$:\n$$\nc_{2,2} = \\gamma_2^2 (I_2)^2 = \\left(\\frac{2}{\\pi}\\right)^2 (-\\pi J_2(\\pi))^2 = \\frac{4}{\\pi^2} (\\pi^2 (J_2(\\pi))^2).\n$$\nSimplifying this expression, we arrive at the final closed-form analytic expression for the coefficient:\n$$\nc_{2,2} = 4(J_2(\\pi))^2.\n$$", "answer": "$$\\boxed{4(J_2(\\pi))^2}$$", "id": "3415859"}, {"introduction": "While isotropic sparse grids offer significant advantages over full tensor products, their true power is often unleashed through adaptivity, especially for functions where different dimensions have vastly different importance. This exercise [@problem_id:3415800] explores the logic of a dimension-adaptive algorithm, which intelligently allocates computational resources to the \"smoothest\" or most influential dimensions. By simulating a greedy refinement strategy based on an a priori profit model, you will gain insight into how these methods automatically tailor the sparse grid to the specific anisotropic features of the problem at hand.", "problem": "Consider anisotropic sparse grid interpolation of the function $f(x,y,z)=\\exp(10x)+\\exp(y)+\\exp(0.1 z)$ on the unit cube $[0,1]^{3}$ using a Smolyak construction with an admissible multi-index set $\\mathcal{I}\\subset\\mathbb{N}_{0}^{3}$, where $\\mathbb{N}_{0}=\\{0,1,2,\\dots\\}$. Assume a dimension-adaptive selection strategy that starts from the base index $\\boldsymbol{0}=(0,0,0)$ and adds indices $\\boldsymbol{k}\\in\\mathbb{N}_{0}^{3}$ satisfying the admissibility condition that if $k_{i}>0$ then $\\boldsymbol{k}-\\boldsymbol{e}_{i}\\in\\mathcal{I}$ for each $i\\in\\{1,2,3\\}$, where $\\boldsymbol{e}_{i}$ is the $i$-th unit vector. Assume uniform and comparable work per index.\n\nAssume further a standard anisotropic a priori profit model consistent with analytic extension, namely that the hierarchical surplus (profit) associated with a multi-index $\\boldsymbol{k}=(k_{1},k_{2},k_{3})$ is bounded by\n$$\np(\\boldsymbol{k})\\leq A\\,\\exp\\big(-\\gamma_{1}k_{1}-\\gamma_{2}k_{2}-\\gamma_{3}k_{3}\\big),\n$$\nwith $A=1$ and fixed anisotropy weights $\\boldsymbol{\\gamma}=(\\gamma_{1},\\gamma_{2},\\gamma_{3})=(10,1,0.1)$. The adaptive algorithm greedily selects, among admissible candidates, an index with largest $p(\\boldsymbol{k})$ and stops as soon as every admissible candidate satisfies $p(\\boldsymbol{k})<\\varepsilon$, where the target tolerance is $\\varepsilon=\\exp(-10)$.\n\nTasks:\n- Using only the admissibility condition, the monotonicity of $p(\\boldsymbol{k})$ in each $k_{i}$, and the given profit model, determine the structure of the final index set $\\mathcal{I}_{\\varepsilon}$ produced by this greedy strategy. Your derivation must identify a scalar threshold, expressed in terms of $\\varepsilon$ and $\\boldsymbol{\\gamma}$, that characterizes $\\mathcal{I}_{\\varepsilon}$.\n- Enumerate the first ten refinement indices visited by the strategy (in the order they can be selected by the greedy rule), starting from $\\boldsymbol{0}$, and explain the pattern you observe in terms of the anisotropy $\\boldsymbol{\\gamma}$.\n- Compute the exact cardinality $N:=|\\mathcal{I}_{\\varepsilon}|$ of the final index set determined above. Provide $N$ as your final answer. No rounding is required.", "solution": "The problem asks for an analysis of a dimension-adaptive sparse grid algorithm based on a greedy selection of multi-indices from an admissible set. The selection is driven by an a priori profit model, and the algorithm terminates based on a tolerance threshold.\n\n### Part 1: Structure of the Final Index Set $\\mathcal{I}_{\\varepsilon}$\n\nThe core of the problem lies in understanding the interplay between the greedy selection rule, the admissibility condition, and the stopping criterion. The algorithm constructs an index set $\\mathcal{I}$ which must remain \"admissible,\" meaning that for any index $\\boldsymbol{k}=(k_1, k_2, k_3) \\in \\mathcal{I}$, if a component $k_i > 0$, then the predecessor index $\\boldsymbol{k}-\\boldsymbol{e}_i$ (where $\\boldsymbol{e}_i$ is the $i$-th standard basis vector) must also be in $\\mathcal{I}$. Such sets are also known as downward-closed sets.\n\nThe algorithm starts with $\\mathcal{I}_0 = \\{\\boldsymbol{0}\\}$ and iteratively adds the admissible candidate that maximizes the profit $p(\\boldsymbol{k})$. An admissible candidate is an index $\\boldsymbol{j} \\notin \\mathcal{I}$ for which all its predecessors are in $\\mathcal{I}$. The process halts when the profit of *every* admissible candidate falls below the tolerance $\\varepsilon$.\n\nThe profit model is given by $p(\\boldsymbol{k}) = \\exp(-\\boldsymbol{\\gamma} \\cdot \\boldsymbol{k})$, where $\\boldsymbol{\\gamma} = (\\gamma_1, \\gamma_2, \\gamma_3)$ has positive components. This profit function is strictly positive and monotonically decreasing in each component of $\\boldsymbol{k}$.\n\nLet's define a set $\\mathcal{S}_{\\varepsilon} = \\{ \\boldsymbol{k} \\in \\mathbb{N}_0^3 \\mid p(\\boldsymbol{k}) \\geq \\varepsilon \\}$. We will demonstrate that the final index set generated by the algorithm, $\\mathcal{I}_{\\varepsilon}$, is precisely this set $\\mathcal{S}_{\\varepsilon}$.\n\nFirst, we prove that $\\mathcal{S}_{\\varepsilon}$ is an admissible set. Let $\\boldsymbol{k} \\in \\mathcal{S}_{\\varepsilon}$, so $p(\\boldsymbol{k}) \\geq \\varepsilon$. For any $i \\in \\{1,2,3\\}$ with $k_i>0$, consider the predecessor index $\\boldsymbol{j} = \\boldsymbol{k}-\\boldsymbol{e}_i$. Since all $\\gamma_l>0$, we have $\\boldsymbol{\\gamma}\\cdot\\boldsymbol{j} = \\sum_l \\gamma_l j_l \\leq \\sum_l \\gamma_l k_l = \\boldsymbol{\\gamma}\\cdot\\boldsymbol{k}$. Consequently, $-\\boldsymbol{\\gamma}\\cdot\\boldsymbol{j} \\geq -\\boldsymbol{\\gamma}\\cdot\\boldsymbol{k}$, which upon exponentiation gives $p(\\boldsymbol{j})=\\exp(-\\boldsymbol{\\gamma}\\cdot\\boldsymbol{j}) \\geq \\exp(-\\boldsymbol{\\gamma}\\cdot\\boldsymbol{k}) = p(\\boldsymbol{k}) \\geq \\varepsilon$. This shows that $\\boldsymbol{j} \\in \\mathcal{S}_{\\varepsilon}$, satisfying the admissibility condition.\n\nNext, we establish the equivalence $\\mathcal{I}_{\\varepsilon} = \\mathcal{S}_{\\varepsilon}$.\n1.  Any index $\\boldsymbol{k}$ added to the set by the greedy algorithm must have had a profit $p(\\boldsymbol{k}) \\geq \\varepsilon$. If $p(\\boldsymbol{k}) < \\varepsilon$, then at the moment it was selected, it was the candidate with maximum profit. This would imply that all admissible candidates had profit less than $\\varepsilon$, triggering the stopping criterion. Thus, $\\boldsymbol{k}$ would not have been added. Therefore, every index in $\\mathcal{I}_{\\varepsilon}$ must also be in $\\mathcal{S}_{\\varepsilon}$, i.e., $\\mathcal{I}_{\\varepsilon} \\subseteq \\mathcal{S}_{\\varepsilon}$.\n2.  Conversely, consider any index $\\boldsymbol{k} \\in \\mathcal{S}_{\\varepsilon}$. Since $\\mathcal{S}_{\\varepsilon}$ is admissible, all predecessors of $\\boldsymbol{k}$ are also in $\\mathcal{S}_{\\varepsilon}$. The algorithm starts with $\\boldsymbol{0} \\in \\mathcal{I}$ (since $p(\\boldsymbol{0})=1 \\geq \\varepsilon$). As the algorithm proceeds, it always selects the candidate with the largest profit. Since any index in $\\mathcal{S}_{\\varepsilon}$ has a profit no less than $\\varepsilon$, and the algorithm only stops when all candidates have profits less than $\\varepsilon$, it must be that any index in $\\mathcal{S}_{\\varepsilon}$ will eventually become an admissible candidate and be selected. Therefore, $\\mathcal{S}_{\\varepsilon} \\subseteq \\mathcal{I}_{\\varepsilon}$.\n\nCombining both inclusions, we have $\\mathcal{I}_{\\varepsilon} = \\mathcal{S}_{\\varepsilon}$. The structure of the final index set is thus defined by the inequality $p(\\boldsymbol{k}) \\geq \\varepsilon$, which provides the requested characterization.\n$$ \\exp(-\\boldsymbol{\\gamma} \\cdot \\boldsymbol{k}) \\geq \\varepsilon $$\nTaking the natural logarithm of both sides:\n$$ -\\boldsymbol{\\gamma} \\cdot \\boldsymbol{k} \\geq \\ln(\\varepsilon) $$\nMultiplying by $-1$ reverses the inequality:\n$$ \\boldsymbol{\\gamma} \\cdot \\boldsymbol{k} \\leq -\\ln(\\varepsilon) $$\nThis inequality, with the scalar threshold $T = -\\ln(\\varepsilon)$, defines the final index set.\n\n### Part 2: First Ten Refinement Indices and Observed Pattern\n\nThe greedy algorithm selects the admissible candidate $\\boldsymbol{k}$ that maximizes $p(\\boldsymbol{k}) = \\exp(-\\boldsymbol{\\gamma} \\cdot \\boldsymbol{k})$. This is equivalent to selecting the candidate that minimizes the weighted sum $S(\\boldsymbol{k}) = \\boldsymbol{\\gamma} \\cdot \\boldsymbol{k} = 10k_1 + k_2 + 0.1k_3$.\nWe start with $\\mathcal{I}=\\{\\boldsymbol{0}\\}$ and track the admissible candidates and their scores.\n- Initial candidates: $(1,0,0)$ with $S=10$; $(0,1,0)$ with $S=1$; $(0,0,1)$ with $S=0.1$.\n1.  Select $\\boldsymbol{k}=(0,0,1)$ (score $0.1$). $\\mathcal{I}$ becomes $\\{\\boldsymbol{0},(0,0,1)\\}$. New candidate: $(0,0,2)$ (score $0.2$).\n2.  Select $\\boldsymbol{k}=(0,0,2)$ (score $0.2$). New candidate: $(0,0,3)$ (score $0.3$).\n3.  Select $\\boldsymbol{k}=(0,0,3)$ (score $0.3$).\n... This pattern continues as long as refining in the third dimension is the \"cheapest\" option.\n4.  $\\boldsymbol{k}=(0,0,4)$ (score $0.4$).\n5.  $\\boldsymbol{k}=(0,0,5)$ (score $0.5$).\n6.  $\\boldsymbol{k}=(0,0,6)$ (score $0.6$).\n7.  $\\boldsymbol{k}=(0,0,7)$ (score $0.7$).\n8.  $\\boldsymbol{k}=(0,0,8)$ (score $0.8$).\n9.  Select $\\boldsymbol{k}=(0,0,9)$ (score $0.9$). The new candidate is $(0,0,10)$ with a score of $0.1 \\times 10 = 1.0$.\n10. The set of admissible candidates now includes $(1,0,0)$ (score $10$), $(0,1,0)$ (score $1$), and $(0,0,10)$ (score $1$). The minimum score is $1$, which is a tie between $(0,1,0)$ and $(0,0,10)$. The algorithm can select either one.\n\nThe first ten refinement indices (the indices added to $\\mathcal{I}$ after the initial $\\boldsymbol{0}$) are:\n$$ (0,0,1), (0,0,2), (0,0,3), (0,0,4), (0,0,5), (0,0,6), (0,0,7), (0,0,8), (0,0,9), \\text{and then either } (0,1,0) \\text{ or } (0,0,10). $$\nThe pattern is dictated by the anisotropy weights $\\boldsymbol{\\gamma}=(10, 1, 0.1)$. The algorithm preferentially refines in the direction with the smallest weight, $\\gamma_3=0.1$, as this corresponds to the slowest decay of profit (or the smallest increase in the score $S(\\boldsymbol{k})$). This is the hallmark of dimension-adaptivity: the computational effort is focused on the directions where the function is smoothest (which corresponds to smaller decay rates $\\gamma_i$ in this profit model).\n\n### Part 3: Cardinality of the Final Index Set\n\nThe final index set $\\mathcal{I}_{\\varepsilon}$ is given by $\\boldsymbol{\\gamma} \\cdot \\boldsymbol{k} \\leq -\\ln(\\varepsilon)$.\nWith $\\boldsymbol{\\gamma}=(10, 1, 0.1)$ and $\\varepsilon = \\exp(-10)$, the condition becomes $10k_1 + k_2 + 0.1k_3 \\leq 10$.\nWe need to find the number of non-negative integer solutions $N=|\\mathcal{I}_{\\varepsilon}|$ for $(k_1, k_2, k_3) \\in \\mathbb{N}_0^3$. To work with integers only, we multiply the inequality by $10$:\n$$ 100k_1 + 10k_2 + k_3 \\leq 100 $$\nWe can enumerate the solutions by considering the possible values for $k_1$. Since $k_1 \\geq 0$, the term $100k_1$ immediately constrains $k_1$ to be $0$ or $1$. If $k_1 \\geq 2$, the inequality cannot be satisfied for non-negative $k_2, k_3$.\n\n**Case 1: $k_1=0$**\nThe inequality reduces to $10k_2 + k_3 \\leq 100$.\nWe can iterate over the possible values of $k_2$. Since $k_2, k_3 \\geq 0$, $k_2$ can range from $0$ to $10$. For each value of $k_2$, the number of possible integer values for $k_3$ is determined by $0 \\leq k_3 \\leq 100 - 10k_2$. This gives $(100 - 10k_2) - 0 + 1 = 101 - 10k_2$ possible values for $k_3$.\nThe total number of solutions for this case, $N_0$, is the sum over $k_2=0, \\dots, 10$:\n$$ N_0 = \\sum_{k_2=0}^{10} (101 - 10k_2) $$\nThis is the sum of an arithmetic progression: $101+91+81+\\dots+11+1$. There are $11$ terms. The sum is given by $\\frac{\\text{number of terms}}{2} \\times (\\text{first term} + \\text{last term})$.\n$$ N_0 = \\frac{11}{2} (101+1) = \\frac{11}{2}(102) = 11 \\times 51 = 561 $$\n\n**Case 2: $k_1=1$**\nThe inequality becomes $100(1) + 10k_2 + k_3 \\leq 100$, which simplifies to $10k_2 + k_3 \\leq 0$.\nSince $k_2$ and $k_3$ are non-negative integers, the only way to satisfy this is if both terms are zero, i.e., $10k_2=0$ and $k_3=0$. This implies $k_2=0$ and $k_3=0$.\nThus, there is only one solution for this case: $(k_1, k_2, k_3)=(1,0,0)$. The number of solutions is $N_1=1$.\n\n**Total Cardinality**\nThe total cardinality $N$ is the sum of the solutions from all cases:\n$$ N = N_0 + N_1 = 561 + 1 = 562 $$\nThe final index set $\\mathcal{I}_{\\varepsilon}$ contains exactly $562$ multi-indices.", "answer": "$$\\boxed{562}$$", "id": "3415800"}]}