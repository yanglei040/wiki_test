{"hands_on_practices": [{"introduction": "A reliable a posteriori error bound for coercive problems requires a certified, parameter-dependent lower bound on the coercivity constant, $\\alpha(\\mu)$. The Successive Constraint Method (SCM) is a cornerstone technique for computing such bounds efficiently. This exercise provides direct, hands-on experience with the SCM algorithm by applying it to a parametrized anisotropic diffusion problem, allowing you to build and test this crucial component of certified reduced basis methods. [@problem_id:3361086]", "problem": "Consider a parametrized anisotropic diffusion problem on the unit square domain with homogeneous Dirichlet boundary conditions. Let the bilinear form be defined for functions $u$ and $v$ by\n$$\na(u,v;\\mu) = \\int_{\\Omega} \\nabla u(x)^{\\top} A(\\mu) \\nabla v(x) \\, dx,\n$$\nwhere $\\Omega = (0,1)\\times(0,1)$ and $A(\\mu)$ is a constant, symmetric, positive definite tensor associated with parameter vector $\\mu$. Assume that $A(\\mu)$ is diagonal with entries $a_x(\\mu)$ and $a_y(\\mu)$ so that\n$$\nA(\\mu) = \\begin{pmatrix} a_x(\\mu) & 0 \\\\ 0 & a_y(\\mu) \\end{pmatrix}, \\quad a_x(\\mu) > 0, \\quad a_y(\\mu) > 0.\n$$\nDefine a reference norm induced by the isotropic bilinear form with the identity tensor,\n$$\n\\|v\\|_V^2 = a_{\\mathrm{ref}}(v,v) = \\int_{\\Omega} \\left( \\left|\\frac{\\partial v}{\\partial x}\\right|^2 + \\left|\\frac{\\partial v}{\\partial y}\\right|^2 \\right) dx,\n$$\nand the coercivity constant\n$$\n\\alpha(\\mu) = \\inf_{v \\in V \\setminus \\{0\\}} \\frac{a(v,v;\\mu)}{\\|v\\|_V^2}.\n$$\nThe parameter-affine decomposition of the bilinear form is\n$$\na(u,v;\\mu) = \\Theta_1(\\mu) a_1(u,v) + \\Theta_2(\\mu) a_2(u,v),\n$$\nwith $\\Theta_1(\\mu) = a_x(\\mu)$, $\\Theta_2(\\mu) = a_y(\\mu)$, $a_1(u,v) = \\int_{\\Omega} \\frac{\\partial u}{\\partial x} \\frac{\\partial v}{\\partial x} \\, dx$, and $a_2(u,v) = \\int_{\\Omega} \\frac{\\partial u}{\\partial y} \\frac{\\partial v}{\\partial y} \\, dx$. For any nonzero $v$, define the component ratios\n$$\nr_q(v) = \\frac{a_q(v,v)}{\\|v\\|_V^2}, \\quad q \\in \\{1,2\\}.\n$$\nThen $r_1(v) \\ge 0$, $r_2(v) \\ge 0$, and $r_1(v) + r_2(v) = 1$.\n\nTo obtain a computable discrete approximation, use a spectral Galerkin subspace spanned by tensor-product sine functions that satisfy homogeneous Dirichlet boundary conditions,\n$$\n\\phi_{k_x,k_y}(x,y) = \\sin(k_x \\pi x)\\sin(k_y \\pi y),\n$$\nwith integer wave numbers $k_x \\in \\{1,\\dots,N_x\\}$ and $k_y \\in \\{1,\\dots,N_y\\}$. In this basis, for a pure mode $\\phi_{k_x,k_y}$, the Rayleigh quotient is\n$$\n\\mathcal{R}(k_x,k_y;\\mu) = \\frac{a(\\phi_{k_x,k_y},\\phi_{k_x,k_y};\\mu)}{\\|\\phi_{k_x,k_y}\\|_V^2} = \\frac{a_x(\\mu) k_x^2 + a_y(\\mu) k_y^2}{k_x^2 + k_y^2}.\n$$\nThe discrete \"true\" coercivity constant over the chosen spectral subspace is thus\n$$\n\\alpha_{\\mathrm{true}}(\\mu) = \\min_{1 \\le k_x \\le N_x,\\; 1 \\le k_y \\le N_y} \\mathcal{R}(k_x,k_y;\\mu).\n$$\n\nConstruct a Successive Constraint Method (SCM) lower bound for $\\alpha(\\mu)$ using a training set of parameters $\\{\\mu_i\\}$ with known discrete $\\alpha_{\\mathrm{true}}(\\mu_i)$. Define a vector $y = (y_1,y_2)$ approximating $(r_1(v),r_2(v))$ and form the linear program\n$$\n\\alpha_{\\mathrm{LB}}(\\mu) = \\min_{y \\in \\mathbb{R}^2} \\;\\Theta_1(\\mu) y_1 + \\Theta_2(\\mu) y_2\n$$\nsubject to the constraints\n$$\ny_1 \\ge 0, \\quad y_2 \\ge 0, \\quad y_1 + y_2 = 1, \\quad \\Theta_1(\\mu_i) y_1 + \\Theta_2(\\mu_i) y_2 \\ge \\alpha_{\\mathrm{true}}(\\mu_i) \\quad \\text{for all training parameters } \\mu_i.\n$$\nThis optimization produces a lower bound $\\alpha_{\\mathrm{LB}}(\\mu)$ satisfying $\\alpha_{\\mathrm{LB}}(\\mu) \\le \\alpha_{\\mathrm{true}}(\\mu)$.\n\nYour task is to implement the calculation of $\\alpha_{\\mathrm{true}}(\\mu)$ using the spectral Galerkin discrete formula and $\\alpha_{\\mathrm{LB}}(\\mu)$ using the above SCM linear program. Then, study the tightness of the SCM bound as the anisotropy ratio grows by computing the ratio\n$$\n\\rho(\\mu) = \\frac{\\alpha_{\\mathrm{LB}}(\\mu)}{\\alpha_{\\mathrm{true}}(\\mu)}.\n$$\n\nUse the following finite-dimensional spectral parameters, with $N_x = 8$ and $N_y = 8$, and define the training parameter set as\n$$\n\\{(a_x,a_y)\\} = \\{(1,1), (1,10), (10,1), (1,100), (100,1), (1,1000), (1000,1)\\}.\n$$\nEvaluate the tightness ratio for the test suite\n$$\n\\text{Test }1:\\; (a_x,a_y)=(1,1), \\quad \\text{Test }2:\\; (a_x,a_y)=(1,10), \\quad \\text{Test }3:\\; (a_x,a_y)=(1,100), \\quad \\text{Test }4:\\; (a_x,a_y)=(1,1000).\n$$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the four tests, with each entry equal to the float value of $\\rho(\\mu)$ for the corresponding test, computed to full machine precision without rounding. No physical units are involved. Angles are not used.\n\nThe problem requires you to:\n- Start from the fundamental variational definition of the coercivity constant and the parameter-affine decomposition.\n- Derive and implement the discrete spectral Galerkin formula for $\\alpha_{\\mathrm{true}}(\\mu)$ in terms of the wave numbers.\n- Formulate and solve the SCM linear program to obtain $\\alpha_{\\mathrm{LB}}(\\mu)$ using the specified constraints.\n- Aggregate the tightness ratios $\\rho(\\mu)$ for the provided test suite.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$).", "solution": "The user wants to compute the tightness ratio of a Successive Constraint Method (SCM) lower bound for the coercivity constant of a parametrized anisotropic diffusion problem.\n\n### Step 1: Problem Validation\n\nThe problem statement has been meticulously reviewed against all validation criteria.\n\n- **Givens**: All data, including the bilinear form, parameter-affine decomposition, spectral basis functions, discretization parameters ($N_x=8$, $N_y=8$), training parameter set, and test parameter set, are explicitly provided.\n- **Scientific Grounding**: The problem is well-grounded in the theory of numerical analysis for partial differential equations, specifically reduced basis methods and the a posteriori error estimation for coercivity constants. All mathematical formulas and concepts are standard in this field.\n- **Well-Posedness**: The problem is well-posed. The calculation of the discrete coercivity constant $\\alpha_{\\mathrm{true}}(\\mu)$ is a minimization over a finite set. The SCM lower bound $\\alpha_{\\mathrm{LB}}(\\mu)$ is determined by a 2D linear program with a non-empty feasible set, which guarantees a solution exists.\n- **Objectivity**: The problem is stated in precise, objective mathematical language, free from ambiguity or subjective claims.\n- **Completeness**: All necessary information is provided to solve the problem as stated.\n- **Consistency**: The givens are internally consistent. The derivation of the Rayleigh quotient is mathematically sound and verifiable.\n- **Special Case**: The test parameters are a subset of the training parameters. While this leads to a specific numerical result ($\\rho=1$), it is a valid specification, not a flaw. The problem tests the understanding of the SCM method's properties, particularly its behavior at the training points themselves.\n\nThe problem is deemed **valid**. We proceed to the solution.\n\n### Step 2: Principle-Based Solution Derivation\n\nThe task is to compute the tightness ratio $\\rho(\\mu) = \\alpha_{\\mathrm{LB}}(\\mu) / \\alpha_{\\mathrm{true}}(\\mu)$ for a given set of test parameters $\\mu = (a_x, a_y)$. This requires deriving and implementing expressions for both $\\alpha_{\\mathrm{true}}(\\mu)$ and $\\alpha_{\\mathrm{LB}}(\\mu)$.\n\n#### 2.1. Calculation of the True Discrete Coercivity Constant, $\\alpha_{\\mathrm{true}}(\\mu)$\n\nThe discrete coercivity constant is defined as the minimum of the Rayleigh quotient over all basis functions in the spectral Galerkin subspace:\n$$\n\\alpha_{\\mathrm{true}}(\\mu) = \\min_{1 \\le k_x \\le N_x,\\; 1 \\le k_y \\le N_y} \\mathcal{R}(k_x,k_y;\\mu) = \\min_{1 \\le k_x \\le N_x,\\; 1 \\le k_y \\le N_y} \\frac{a_x(\\mu) k_x^2 + a_y(\\mu) k_y^2}{k_x^2 + k_y^2}\n$$\nThe Rayleigh quotient can be rewritten as a weighted average of $a_x(\\mu)$ and $a_y(\\mu)$:\n$$\n\\mathcal{R}(k_x,k_y;\\mu) = a_x(\\mu) \\frac{k_x^2}{k_x^2+k_y^2} + a_y(\\mu) \\frac{k_y^2}{k_x^2+k_y^2}\n$$\nThe minimum value of this expression is always bounded by $\\min(a_x(\\mu), a_y(\\mu))$. To find the minimum, we must adjust the weights (by choosing $k_x$ and $k_y$) to favor the smaller of the two coefficients.\n\n- If $a_x(\\mu) < a_y(\\mu)$, we must maximize the weight on $a_x(\\mu)$, which is $\\frac{k_x^2}{k_x^2+k_y^2}$. This is achieved by maximizing $k_x$ and minimizing $k_y$, i.e., setting $k_x = N_x$ and $k_y = 1$.\n- If $a_y(\\mu) < a_x(\\mu)$, we must maximize the weight on $a_y(\\mu)$, which is $\\frac{k_y^2}{k_x^2+k_y^2}$. This is achieved by maximizing $k_y$ and minimizing $k_x$, i.e., setting $k_x = 1$ and $k_y = N_y$.\n- If $a_x(\\mu) = a_y(\\mu)$, the expression is constant for all $k_x, k_y$ and equals $a_x(\\mu)$.\n\nGiven $N_x=8$ and $N_y=8$, the analytical formula for $\\alpha_{\\mathrm{true}}(\\mu)$ is:\n$$\n\\alpha_{\\mathrm{true}}(a_x, a_y) = \\begin{cases}\n\\frac{a_x \\cdot 8^2 + a_y \\cdot 1^2}{8^2+1^2} = \\frac{64 a_x + a_y}{65} & \\text{if } a_x < a_y \\\\\n\\frac{a_x \\cdot 1^2 + a_y \\cdot 8^2}{1^2+8^2} = \\frac{a_x + 64 a_y}{65} & \\text{if } a_y < a_x \\\\\na_x & \\text{if } a_x = a_y\n\\end{cases}\n$$\n\n#### 2.2. Calculation of the SCM Lower Bound, $\\alpha_{\\mathrm{LB}}(\\mu)$\n\nThe SCM lower bound $\\alpha_{\\mathrm{LB}}(\\mu)$ is the solution to the linear program (LP):\n$$\n\\alpha_{\\mathrm{LB}}(\\mu) = \\min_{y_1, y_2} \\left( a_x(\\mu) y_1 + a_y(\\mu) y_2 \\right)\n$$\nsubject to:\n1. $y_1 \\ge 0, y_2 \\ge 0$\n2. $y_1 + y_2 = 1$\n3. $a_x(\\mu_i) y_1 + a_y(\\mu_i) y_2 \\ge \\alpha_{\\mathrm{true}}(\\mu_i)$ for all training parameters $\\mu_i$.\n\nWe can simplify this LP. Using $y_2 = 1 - y_1$, the problem becomes one-dimensional in $y_1$:\n$$\n\\min_{y_1} \\left( (a_x(\\mu) - a_y(\\mu)) y_1 + a_y(\\mu) \\right)\n$$\nsubject to a feasible interval for $y_1$. The constraints become:\n1. $y_1 \\ge 0$ and $1 - y_1 \\ge 0 \\implies 0 \\le y_1 \\le 1$.\n2. For each training parameter $\\mu_i$: $(a_x(\\mu_i) - a_y(\\mu_i)) y_1 \\ge \\alpha_{\\mathrm{true}}(\\mu_i) - a_y(\\mu_i)$.\n\nThis second set of constraints defines the feasible interval $[y_{1,\\min}, y_{1,\\max}]$. For a training parameter $\\mu_i$:\n- If $a_x(\\mu_i) > a_y(\\mu_i)$: $y_1 \\ge \\frac{\\alpha_{\\mathrm{true}}(\\mu_i) - a_y(\\mu_i)}{a_x(\\mu_i) - a_y(\\mu_i)} = \\frac{\\frac{a_x(\\mu_i) + 64 a_y(\\mu_i)}{65} - a_y(\\mu_i)}{a_x(\\mu_i) - a_y(\\mu_i)} = \\frac{a_x(\\mu_i) - a_y(\\mu_i)}{65(a_x(\\mu_i) - a_y(\\mu_i))} = \\frac{1}{65}$. This provides a lower bound on $y_1$.\n- If $a_x(\\mu_i) < a_y(\\mu_i)$: $y_1 \\le \\frac{\\alpha_{\\mathrm{true}}(\\mu_i) - a_y(\\mu_i)}{a_x(\\mu_i) - a_y(\\mu_i)} = \\frac{\\frac{64 a_x(\\mu_i) + a_y(\\mu_i)}{65} - a_y(\\mu_i)}{a_x(\\mu_i) - a_y(\\mu_i)} = \\frac{64(a_x(\\mu_i) - a_y(\\mu_i))}{65(a_x(\\mu_i) - a_y(\\mu_i))} = \\frac{64}{65}$. This provides an upper bound on $y_1$.\n\nThe training set contains parameters of both types. Combining all constraints, the feasible interval for $y_1$ is $[\\frac{1}{65}, \\frac{64}{65}]$.\nThe LP is to minimize a linear function of $y_1$ over this interval. The minimum will occur at one of the endpoints:\n- If the slope $(a_x(\\mu) - a_y(\\mu)) > 0$, the minimum is at $y_1 = \\frac{1}{65}$.\n- If the slope $(a_x(\\mu) - a_y(\\mu)) < 0$, the minimum is at $y_1 = \\frac{64}{65}$.\n- If the slope is $0$, the objective is constant.\n\n#### 2.3. Evaluating the Tightness Ratio $\\rho(\\mu)$ at Test Parameters\n\nThe test parameters are $\\mu_1=(1,1)$, $\\mu_2=(1,10)$, $\\mu_3=(1,100)$, $\\mu_4=(1,1000)$. All of these are also in the training set. For any parameter $\\mu_k$ in the training set, the SCM constraints include $a_x(\\mu_k) y_1 + a_y(\\mu_k) y_2 \\ge \\alpha_{\\mathrm{true}}(\\mu_k)$. The objective function for $\\alpha_{\\mathrm{LB}}(\\mu_k)$ is to minimize $a_x(\\mu_k) y_1 + a_y(\\mu_k) y_2$. By construction, the minimum value cannot be less than $\\alpha_{\\mathrm{true}}(\\mu_k)$. Thus, $\\alpha_{\\mathrm{LB}}(\\mu_k) \\ge \\alpha_{\\mathrm{true}}(\\mu_k)$. Since it is also a lower bound, we have $\\alpha_{\\mathrm{LB}}(\\mu_k) \\le \\alpha_{\\mathrm{true}}(\\mu_k)$. This forces an equality: $\\alpha_{\\mathrm{LB}}(\\mu_k) = \\alpha_{\\mathrm{true}}(\\mu_k)$.\nTherefore, for any test parameter that is also a training parameter, the tightness ratio $\\rho(\\mu)$ must be exactly $1$. The implementation will confirm this by direct computation.\nWe will now implement the derived analytical formulas for $\\alpha_{\\mathrm{true}}(\\mu)$ and $\\alpha_{\\mathrm{LB}}(\\mu)$ to compute the ratios for the test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Computes the SCM tightness ratio for a parametrized anisotropic diffusion problem.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 1.0),\n        (1.0, 10.0),\n        (1.0, 100.0),\n        (1.0, 1000.0),\n    ]\n\n    # Define the training parameter set for the SCM.\n    training_params = [\n        (1.0, 1.0), (1.0, 10.0), (10.0, 1.0), (1.0, 100.0), \n        (100.0, 1.0), (1.0, 1000.0), (1000.0, 1.0)\n    ]\n    \n    # Define the spectral Galerkin subspace parameters.\n    Nx = 8.0\n    Ny = 8.0\n\n    def calculate_alpha_true(mu, Nx_val, Ny_val):\n        \"\"\"\n        Calculates the discrete coercivity constant alpha_true based on the\n        analytical minimum of the Rayleigh quotient.\n        \n        Args:\n            mu (tuple): Parameter vector (ax, ay).\n            Nx_val (float): Number of basis functions in x-direction.\n            Ny_val (float): Number of basis functions in y-direction.\n            \n        Returns:\n            float: The value of alpha_true(mu).\n        \"\"\"\n        ax, ay = mu\n        if ax < ay:\n            # Minimum is at (kx, ky) = (Nx, 1) to maximize weight on smaller coefficient ax.\n            return (ax * Nx_val**2 + ay) / (Nx_val**2 + 1.0)\n        elif ay < ax:\n            # Minimum is at (kx, ky) = (1, Ny) to maximize weight on smaller coefficient ay.\n            return (ax + ay * Ny_val**2) / (1.0 + Ny_val**2)\n        else:  # ax == ay\n            # Rayleigh quotient is constant and equal to ax.\n            return ax\n\n    # Determine the feasible region for y1 from the training set.\n    # The linear program is to minimize (ax-ay)*y1 + ay subject to constraints.\n    # The constraints on y1 are derived from the training set.\n    y1_min_constraints = [0.0]\n    y1_max_constraints = [1.0]\n\n    for mu_i in training_params:\n        ax_i, ay_i = mu_i\n        alpha_true_i = calculate_alpha_true(mu_i, Nx, Ny)\n        \n        # Rewrite constraint a_x*y1 + a_y*y2 >= alpha_true as (a_x - a_y)*y1 >= alpha_true - a_y\n        diff = ax_i - ay_i\n        if diff > 0:\n            # y1 >= (alpha_true_i - ay_i) / (ax_i - ay_i)\n            lower_bound = (alpha_true_i - ay_i) / diff\n            y1_min_constraints.append(lower_bound)\n        elif diff < 0:\n            # y1 <= (alpha_true_i - ay_i) / (ax_i - ay_i) (inequality flips)\n            upper_bound = (alpha_true_i - ay_i) / diff\n            y1_max_constraints.append(upper_bound)\n        # If diff is 0, the constraint is 0 >= 0, which is always true and adds no information.\n\n    # The feasible interval for y1 is [y1_min_feasible, y1_max_feasible].\n    y1_min_feasible = max(y1_min_constraints)\n    y1_max_feasible = min(y1_max_constraints)\n\n    results = []\n    for case in test_cases:\n        ax_test, ay_test = case\n        \n        # 1. Calculate the \"true\" discrete coercivity constant for the test case.\n        alpha_t = calculate_alpha_true(case, Nx, Ny)\n        \n        # 2. Calculate the SCM lower bound alpha_LB.\n        # This involves minimizing the linear objective function f(y1) = (ax-ay)*y1 + ay\n        # over the feasible interval for y1.\n        \n        y1_opt = 0.0\n        slope = ax_test - ay_test\n        \n        if slope > 0: # Increasing function, minimum is at the left endpoint.\n            y1_opt = y1_min_feasible\n        elif slope < 0: # Decreasing function, minimum is at the right endpoint.\n            y1_opt = y1_max_feasible\n        else: # slope is 0, function is constant, any point is a minimum.\n            y1_opt = y1_min_feasible\n\n        alpha_lb = slope * y1_opt + ay_test\n        \n        # 3. Compute the tightness ratio.\n        # As reasoned in the solution, since all test cases are in the training set,\n        # alpha_lb should be equal to alpha_t, making the ratio 1.0.\n        rho = alpha_lb / alpha_t\n        results.append(rho)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3361086"}, {"introduction": "Moving from a single component to a complete system, we now apply error estimation to a more complex, nonlinear problem. This practice tackles a diffusion equation featuring an entropy-viscosity stabilization term, a scenario common in computational fluid dynamics for capturing sharp gradients or shocks. You will implement the full workflow, from the Discontinuous Galerkin discretization to the construction of a reduced basis model and the evaluation of its a posteriori error estimator, giving you insight into how these bounds perform in practical, nonlinear settings. [@problem_id:3361071]", "problem": "Consider the scalar stationary boundary value problem on the one-dimensional domain $[0,1]$ with Dirichlet boundary conditions,\n$$\n- \\frac{\\mathrm{d}}{\\mathrm{d}x} \\left( \\left( \\nu_0(\\mu) + \\nu_{\\mathrm{art}}(u; \\mu) \\right) \\frac{\\mathrm{d}u}{\\mathrm{d}x} \\right) = s(x;\\mu), \\quad x \\in (0,1), \\quad u(0)=1,\\quad u(1)=0,\n$$\nwhere $\\mu$ denotes a parameter vector, $\\nu_0(\\mu) \\gt 0$ is a constant physical viscosity, and $\\nu_{\\mathrm{art}}(u; \\mu) \\ge 0$ is an artificial viscosity chosen by an entropy-viscosity shock-capturing mechanism to stabilize steep gradients. The source is taken as\n$$\ns(x;\\mu) = A(\\mu)\\,\\sin\\left(5\\pi x\\right),\n$$\nwhere $A(\\mu)\\gt 0$ is a source amplitude. We discretize this problem with a symmetric interior penalty discontinuous Galerkin method (SIPG) on a uniform mesh with $K$ elements and polynomial degree $p=0$. Let $h = 1/K$ denote the element size, and let $u_h \\in V_h$ be the piecewise-constant discrete solution with one degree of freedom per element.\n\nWe define the entropy-viscosity coefficient elementwise using the quadratic entropy $\\eta(u) = \\tfrac{1}{2} u^2$. For any $u_h \\in V_h$, we define, for element index $e=0,1,\\dots,K-1$,\n- the elementwise indicator of gradient magnitude\n$$\nG_e(u_h) = \\frac{1}{2}\\left( \\frac{|u_e - u_{e-1}|}{h} + \\frac{|u_{e+1} - u_e|}{h} \\right),\n$$\nwith the conventions $u_{-1} := u(0)=1$ and $u_{K} := u(1)=0$,\n- the normalization\n$$\n\\Delta\\eta(u_h) = \\max_{0\\le j \\le K-1} \\left\\{ \\eta(u_j) \\right\\} - \\min_{0\\le j \\le K-1} \\left\\{ \\eta(u_j) \\right\\} + \\varepsilon_{\\eta},\n$$\nwith $\\varepsilon_{\\eta} \\gt 0$ a small safety parameter,\n- and the entropy viscosity\n$$\n\\nu_{\\mathrm{art},e}(u_h;\\mu) = C_{\\mathrm{ev}}\\, h^2 \\,\\frac{|u_e|^2\\, G_e(u_h)}{\\Delta\\eta(u_h)} \\quad \\text{clipped to } [0, \\nu_{\\max}],\n$$\nwith $C_{\\mathrm{ev}} \\gt 0$ a tunable coefficient and $\\nu_{\\max} \\gt 0$ a viscosity cap.\n\nThe SIPG bilinear form for diffusion with piecewise-constant viscosity is, for trial $u_h$ and test $v_h$,\n$$\na(u_h,v_h;\\mu) = \\sum_{\\text{faces }F}\\lambda_F(\\mu)\\,[u_h]_F\\,[v_h]_F + \\lambda_{\\mathrm{L}}(\\mu)\\,u_0 v_0 + \\lambda_{\\mathrm{R}}(\\mu)\\,u_{K-1} v_{K-1},\n$$\nwhere the jump of $w_h$ across an interior face $F$ between elements $e$ and $e+1$ is $[w_h]_F = w_e - w_{e+1}$, and the penalties are $\\lambda_F(\\mu) = C_{\\mathrm{pen}} \\, \\overline{\\nu}_F(\\mu) / h$ with $\\overline{\\nu}_F(\\mu) = \\tfrac{1}{2} \\left( \\nu_{e}(\\mu) + \\nu_{e+1}(\\mu) \\right)$ and $\\nu_e(\\mu) = \\nu_0(\\mu) + \\nu_{\\mathrm{art},e}(u_h;\\mu)$. The boundary penalties satisfy $\\lambda_{\\mathrm{L}}(\\mu) = C_{\\mathrm{pen}} \\, \\nu_0(\\mu)/h$ and $\\lambda_{\\mathrm{R}}(\\mu) = C_{\\mathrm{pen}} \\, \\nu_0(\\mu)/h$. The linear functional incorporates the source and weak imposition of Dirichlet boundary conditions,\n$$\n\\ell(v_h;\\mu) = \\sum_{e=0}^{K-1} \\left( \\int_{x_e}^{x_{e+1}} s(x;\\mu)\\,\\mathrm{d}x \\right) v_e + \\lambda_{\\mathrm{L}}(\\mu)\\, g_{\\mathrm{L}}\\, v_0 + \\lambda_{\\mathrm{R}}(\\mu)\\, g_{\\mathrm{R}}\\, v_{K-1},\n$$\nwith $g_{\\mathrm{L}}=1$ and $g_{\\mathrm{R}}=0$. The discrete problem is: find $u_h \\in V_h$ such that $a(u_h,v_h;\\mu) = \\ell(v_h;\\mu)$ for all $v_h \\in V_h$. The dependence of $\\nu_{\\mathrm{art}}$ on $u_h$ leads to a nonlinear fixed-point problem which we solve by fixed-point iteration.\n\nA reduced basis (RB) space $V_N(\\mu)$ of dimension $N$ is constructed from a set of high-fidelity snapshots by orthonormalization in the Euclidean inner product. The reduced solution $u_N(\\mu) \\in V_N(\\mu)$ is the Galerkin solution obtained by solving the nonlinear fixed-point problem projected onto $V_N(\\mu)$.\n\nWe consider a residual-based a posteriori error estimator for the energy norm induced by the symmetric positive definite (SPD) part of the operator. Specifically, define the SPD matrix $S(u_N;\\mu)$ associated with $a(\\cdot,\\cdot;\\mu)$ at the current reduced solution $u_N(\\mu)$, and the full-order residual vector\n$$\nr_N(\\mu) = \\ell(\\cdot;\\mu) - A(u_N(\\mu);\\mu) u_N(\\mu),\n$$\nwhere $A(u_N(\\mu);\\mu)$ is the full-order stiffness matrix assembled with $\\nu_e(\\mu) = \\nu_0(\\mu) + \\nu_{\\mathrm{art},e}(u_N;\\mu)$. Then the estimator is the dual norm of the residual in the $S(u_N;\\mu)$-inner product,\n$$\n\\eta_N(\\mu) = \\left\\| r_N(\\mu) \\right\\|_{S(u_N;\\mu)^{-1}} = \\sqrt{ r_N(\\mu)^{\\top} S(u_N;\\mu)^{-1} r_N(\\mu) }.\n$$\nFor a linear problem with coercive symmetric bilinear form, this quantity is an upper bound for the error in the energy norm induced by $S$, that is,\n$$\n\\| u_h(\\mu) - u_N(\\mu) \\|_{S(u_N;\\mu)} \\le \\eta_N(\\mu),\n$$\nwhere\n$$\n\\| v \\|_{S}^2 = v^{\\top} S v.\n$$\nThe artificial viscosity increases the symmetric part and hence the coercivity, thereby entering directly into the estimator through $S$.\n\nDefine the certified decay ratio as the sequence\n$$\nq_N(\\mu) = \\frac{\\eta_{N+1}(\\mu)}{\\eta_N(\\mu)}, \\quad N=1,2,\\dots,\n$$\nfor a fixed parameter $\\mu$. We will verify numerically, for a chosen test suite of parameters, the monotonicity property of the certified decay: the sequence $\\{ \\eta_N(\\mu) \\}$ is nonincreasing with $N$ and satisfies $0 \\le q_N(\\mu) \\le 1$.\n\nTasks:\n1. Discretize the problem using the described symmetric interior penalty discontinuous Galerkin method with piecewise-constant basis ($p=0$) on a uniform mesh with $K=80$ elements and penalty coefficient $C_{\\mathrm{pen}}=10$. Use the fixed-point iteration to compute the high-fidelity solution $u_h(\\mu)$, starting from a linear initial guess, with convergence declared when the Euclidean norm of successive iterates changes by less than $10^{-10}$ or after $50$ iterations. Use $C_{\\mathrm{ev}}=0.02$, $\\nu_{\\max}=0.05$, and $\\varepsilon_{\\eta}=10^{-12}$.\n2. Build a reduced basis space from $M=5$ training parameters by collecting high-fidelity snapshots and performing orthonormalization. Use the following training set:\n   - $\\mu_1^{\\mathrm{train}}$: $\\nu_0 = 0.015$, $A = 0.8$,\n   - $\\mu_2^{\\mathrm{train}}$: $\\nu_0 = 0.008$, $A = 1.2$,\n   - $\\mu_3^{\\mathrm{train}}$: $\\nu_0 = 0.012$, $A = 1.5$,\n   - $\\mu_4^{\\mathrm{train}}$: $\\nu_0 = 0.020$, $A = 0.6$,\n   - $\\mu_5^{\\mathrm{train}}$: $\\nu_0 = 0.010$, $A = 1.1$.\n3. For each of the following test parameters $\\mu$, compute the reduced solution $u_N(\\mu)$ for $N=1,2,3,4,5$ using fixed-point iteration in the RB space, assemble the full residual $r_N(\\mu)$ and the SPD matrix $S(u_N;\\mu)$, and evaluate the estimator $\\eta_N(\\mu)$. Then compute the decay ratios $q_N(\\mu)$ and verify the monotonicity conditions $\\eta_{N+1}(\\mu) \\le \\eta_{N}(\\mu)$ and $0 \\le q_N(\\mu) \\le 1$ for all $N=1,\\dots,4$. Report a boolean for each test parameter indicating whether both conditions hold for all $N$.\n   - Test case 1: $\\nu_0 = 0.010$, $A = 1.0$,\n   - Test case 2: $\\nu_0 = 0.007$, $A = 1.5$,\n   - Test case 3: $\\nu_0 = 0.020$, $A = 0.5$.\n\nAngle units do not appear in this problem, and no physical units are required beyond the nondimensional parameter settings provided.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where each result is a boolean corresponding to the three test cases in the order listed above.", "solution": "The problem is valid. It presents a well-defined task in the field of numerical analysis, specifically concerning model order reduction for a nonlinear partial differential equation. All necessary parameters, equations, and procedures are specified, and the problem is scientifically grounded in established methods like the Discontinuous Galerkin method, entropy viscosity stabilization, and reduced basis methods with a posteriori error estimation.\n\nOur approach consists of the following steps, implemented as a series of computational procedures:\n1.  **High-Fidelity (HF) Solver:** We first construct a solver for the full-order discrete problem.\n2.  **Reduced Basis (RB) Generation:** We use the HF solver to generate solution \"snapshots\" for a set of training parameters and form an orthonormal basis from these snapshots.\n3.  **Reduced Order Model (ROM) Solver:** We implement a solver for the problem projected onto the reduced basis space.\n4.  **A Posteriori Error Estimation and Verification:** For each test parameter, we compute the reduced solutions for increasing basis sizes, evaluate the residual-based error estimator, and verify its monotonicity properties.\n\n### 1. High-Fidelity Discretization and Solution\n\nThe problem is a one-dimensional, stationary, nonlinear diffusion equation. We discretize it on a uniform mesh of $K=80$ elements of size $h=1/K$. We use a Symmetric Interior Penalty Discontinuous Galerkin (SIPG) method with piecewise constant ($p=0$) basis functions, $\\{\\phi_e\\}_{e=0}^{K-1}$. The discrete solution $u_h$ is a vector in $\\mathbb{R}^K$, where the component $u_e$ represents the constant value of the solution on element $e$.\n\nThe discrete weak formulation is: find $u_h \\in V_h \\cong \\mathbb{R}^K$ such that\n$$A(u_h;\\mu) u_h = \\ell(\\mu)$$\nThis is a system of $K$ nonlinear algebraic equations. The matrix $A$ and vector $\\ell$ arise from the bilinear form $a(u_h, v_h; \\mu)$ and linear form $\\ell(v_h; \\mu)$.\n\n**Assembly of the Stiffness Matrix and Load Vector**\nThe stiffness matrix $A(u_h;\\mu)$ has entries $A_{ij} = a(\\phi_j, \\phi_i; \\mu)$. Due to the local support of the basis functions, the matrix is tridiagonal. For $v_h=\\phi_i$, the non-zero contributions to $a(u_h, v_h; \\mu)$ are:\n-   **Diagonal entries ($i=j$):**\n    -   $i \\in \\{1, \\dots, K-2\\}$: $A_{i,i} = \\lambda_{i-1,i}(\\mu) + \\lambda_{i,i+1}(\\mu)$\n    -   $i=0$: $A_{0,0} = \\lambda_L(\\mu) + \\lambda_{0,1}(\\mu)$\n    -   $i=K-1$: $A_{K-1,K-1} = \\lambda_R(\\mu) + \\lambda_{K-2,K-1}(\\mu)$\n-   **Off-diagonal entries ($j=i \\pm 1$):**\n    -   $A_{i,i+1} = A_{i+1,i} = -\\lambda_{i,i+1}(\\mu)$\n\nThe penalty parameters are defined as $\\lambda_F(\\mu) = C_{\\mathrm{pen}} \\overline{\\nu}_F(\\mu) / h$ for interior faces $F$ between elements $e$ and $e+1$, where $\\overline{\\nu}_F(\\mu) = \\frac{1}{2}(\\nu_e(\\mu) + \\nu_{e+1}(\\mu))$ and $\\nu_e(\\mu) = \\nu_0(\\mu) + \\nu_{\\mathrm{art},e}(u_h;\\mu)$. The boundary penalties are $\\lambda_L(\\mu) = C_{\\mathrm{pen}}\\nu_0(\\mu)/h$ and $\\lambda_R(\\mu) = C_{\\mathrm{pen}}\\nu_0(\\mu)/h$, where $C_{\\mathrm{pen}}=10$.\n\nThe load vector $\\ell(\\mu)$ has entries $\\ell_i = \\ell(\\phi_i; \\mu)$:\n$$ \\ell_i = \\int_{x_i}^{x_{i+1}} s(x;\\mu) dx + \\delta_{i0} \\lambda_L(\\mu) g_L + \\delta_{i,K-1} \\lambda_R(\\mu) g_R $$\nwhere $g_L=1$, $g_R=0$, and $\\delta_{ij}$ is the Kronecker delta. The integral is computed analytically:\n$$ \\int_{x_i}^{x_{i+1}} A(\\mu) \\sin(5\\pi x) dx = \\frac{A(\\mu)}{5\\pi} (\\cos(5\\pi x_i) - \\cos(5\\pi x_{i+1})) $$\n\n**Nonlinear Solver**\nThe dependence of the matrix $A$ on the solution $u_h$ (through the artificial viscosity $\\nu_{\\mathrm{art}}$) makes the system nonlinear. We solve it via fixed-point iteration. Given an iterate $u_h^{(k)}$, we compute the next iterate $u_h^{(k+1)}$ by solving the linear system:\n$$ A(u_h^{(k)}; \\mu) u_h^{(k+1)} = \\ell(\\mu) $$\nThe iteration starts with an initial guess $u_h^{(0)}$ derived from sampling the linear function $u(x)=1-x$ at element centers, i.e., $u_e^{(0)}=1-(e+0.5)h$. The iteration terminates when $\\|u_h^{(k+1)} - u_h^{(k)}\\|_2 < 10^{-10}$ or a maximum of $50$ iterations is reached.\n\nThe artificial viscosity $\\nu_{\\mathrm{art}, e}(u_h; \\mu)$ is updated at each iteration. This involves computing the gradient indicator $G_e(u_h)$, the entropy range $\\Delta\\eta(u_h)$, and then the viscosity value itself, which is clipped to $[0, \\nu_{\\max}]$. Specifically, for a solution vector $u_h = (u_0, \\dots, u_{K-1})$, we construct an extended vector $(u(0), u_0, \\dots, u_{K-1}, u(1)) = (1, u_0, \\dots, u_{K-1}, 0)$ to compute the jumps needed for $G_e(u_h)$.\n\n### 2. Reduced Basis Generation\n\nWe generate a reduced basis from $M=5$ snapshots. Each snapshot is a high-fidelity solution $u_h(\\mu_j^{\\mathrm{train}})$ for one of the $M$ training parameters. These $M$ vectors, each of size $K$, are collected as columns of a snapshot matrix $S_{\\text{snap}} \\in \\mathbb{R}^{K \\times M}$. We then perform a (reduced) QR decomposition of this matrix, $S_{\\text{snap}} = VR$. The columns of the matrix $V \\in \\mathbb{R}^{K \\times M}$ form an orthonormal basis (in the Euclidean inner product) for the snapshot space. The resulting basis $V$ is used for all subsequent test cases.\n\n### 3. Reduced Order Model Solution\n\nFor a given test parameter $\\mu$ and a basis size $N \\le M$, the RB space is $V_N = \\text{span}\\{v_1, \\dots, v_N\\}$, where $v_i$ are the first $N$ columns of $V$. The reduced solution is sought in the form $u_N(\\mu) = V_N \\tilde{u}_N(\\mu)$, where $\\tilde{u}_N \\in \\mathbb{R}^N$ is the vector of reduced coordinates.\n\nApplying a Galerkin projection to the weak formulation yields a nonlinear system of size $N \\times N$ for $\\tilde{u}_N$:\n$$ V_N^T A(V_N \\tilde{u}_N; \\mu) V_N \\tilde{u}_N = V_N^T \\ell(\\mu) $$\nThis smaller system is solved using the same fixed-point iteration strategy as the high-fidelity model. The full-order quantities $A(u_N; \\mu)$ and $\\ell(\\mu)$ are computed at each step (offline/online decomposition is not needed for this problem), and then projected onto the reduced space.\n\n### 4. A Posteriori Error Estimation\n\nAfter computing the converged reduced solution $u_N(\\mu) = V_N \\tilde{u}_N(\\mu)$ for a given $N$, we evaluate the a posteriori error estimator $\\eta_N(\\mu)$. This involves:\n1.  Assembling the full-order stiffness matrix $S(u_N;\\mu) = A(u_N(\\mu);\\mu)$ and load vector $\\ell(\\mu)$. As the bilinear form is symmetric and positive definite (SPD), this matrix is the one used for the error norm.\n2.  Computing the full-order residual vector $r_N(\\mu) = \\ell(\\mu) - S(u_N;\\mu) u_N(\\mu)$.\n3.  Calculating the estimator as the dual norm of the residual:\n    $$ \\eta_N(\\mu) = \\sqrt{ r_N(\\mu)^T S(u_N;\\mu)^{-1} r_N(\\mu) } $$\n    This is computed efficiently by first solving the linear system $S(u_N;\\mu) z = r_N(\\mu)$ for $z$, and then taking $\\eta_N(\\mu) = \\sqrt{r_N(\\mu)^T z}$.\n\nFor each test case, we compute the estimators $\\eta_N(\\mu)$ for $N=1, 2, 3, 4, 5$. We then compute the decay ratios $q_N(\\mu) = \\eta_{N+1}(\\mu) / \\eta_N(\\mu)$ for $N=1, 2, 3, 4$. Finally, we verify that the conditions $\\eta_{N+1}(\\mu) \\le \\eta_N(\\mu)$ and $0 \\le q_N(\\mu) \\le 1$ hold for all $N \\in \\{1, 2, 3, 4\\}$. The final output is a boolean value for each test case indicating whether these properties are satisfied.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    # --- Problem Constants ---\n    K = 80\n    P_DEG = 0 # Not used explicitly as p=0 logic is hard-coded\n    H = 1.0 / K\n    C_PEN = 10.0\n    HF_TOL = 1e-10\n    ROM_TOL = 1e-10\n    MAX_ITER = 50\n    C_EV = 0.02\n    NU_MAX = 0.05\n    EPS_ETA = 1e-12\n    U_L, U_R = 1.0, 0.0\n    M = 5\n\n    # --- Training and Test Cases ---\n    train_cases = [\n        (0.015, 0.8), # (nu0, A)\n        (0.008, 1.2),\n        (0.012, 1.5),\n        (0.020, 0.6),\n        (0.010, 1.1),\n    ]\n    test_cases = [\n        (0.010, 1.0),\n        (0.007, 1.5),\n        (0.020, 0.5),\n    ]\n\n    # Shared pre-computations\n    x_nodes = np.linspace(0, 1, K + 1)\n    x_e_left = x_nodes[:-1]\n    x_e_right = x_nodes[1:]\n    \n    def compute_nu_art(u, nu0):\n        if not isinstance(u, np.ndarray):\n            u = np.array(u)\n\n        u_ext = np.concatenate(([U_L], u, [U_R]))\n        \n        # Gradient indicator G_e\n        jumps_abs = np.abs(u_ext[1:] - u_ext[:-1])\n        g_e = 0.5 * (jumps_abs[:-1] + jumps_abs[1:]) / H\n        \n        # Entropy normalization Delta_eta\n        eta = 0.5 * u**2\n        delta_eta = np.max(eta) - np.min(eta) + EPS_ETA\n        \n        # Artificial viscosity\n        nu_art = C_EV * H**2 * (u**2 * g_e) / delta_eta\n        np.clip(nu_art, 0, NU_MAX, out=nu_art)\n        \n        return nu_art\n\n    def assemble_stiffness(nu_art, nu0):\n        nu_total = nu0 + nu_art\n        \n        # Penalties\n        lambda_L = C_PEN * nu0 / H\n        lambda_R = C_PEN * nu0 / H\n        \n        nu_face_avg = 0.5 * (nu_total[:-1] + nu_total[1:])\n        lambda_F = C_PEN * nu_face_avg / H\n        \n        # Matrix assembly\n        diag = np.zeros(K)\n        diag[0] = lambda_L + lambda_F[0]\n        diag[K-1] = lambda_R + lambda_F[K-2]\n        diag[1:K-1] = lambda_F[:-1] + lambda_F[1:]\n        \n        off_diag = -lambda_F\n        \n        A = np.diag(diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n        return A\n\n    def assemble_load(A_param, nu0):\n        # Source term integral\n        integral_s = (A_param / (5 * np.pi)) * (np.cos(5 * np.pi * x_e_left) - np.cos(5 * np.pi * x_e_right))\n        \n        l = np.copy(integral_s)\n        \n        # Boundary terms\n        lambda_L = C_PEN * nu0 / H\n        lambda_R = C_PEN * nu0 / H\n        l[0] += lambda_L * U_L\n        l[K-1] += lambda_R * U_R\n        \n        return l\n\n    def solve_nonlinear_system(mu, u_initial, is_rom=False, V_N=None):\n        nu0, A_param = mu\n        u = u_initial\n        \n        for _ in range(MAX_ITER):\n            u_old = u.copy()\n            \n            if is_rom:\n                u_full = V_N @ u\n                nu_art = compute_nu_art(u_full, nu0)\n                A_full = assemble_stiffness(nu_art, nu0)\n                l_full = assemble_load(A_param, nu0)\n                \n                A_rom = V_N.T @ A_full @ V_N\n                l_rom = V_N.T @ l_full\n                \n                u = np.linalg.solve(A_rom, l_rom)\n                if np.linalg.norm(u - u_old) < ROM_TOL:\n                    break\n            else: # HF\n                nu_art = compute_nu_art(u, nu0)\n                A = assemble_stiffness(nu_art, nu0)\n                l = assemble_load(A_param, nu0)\n                u = np.linalg.solve(A, l)\n                if np.linalg.norm(u - u_old) < HF_TOL:\n                    break\n        return u\n\n    # --- Step 1 & 2: Build Reduced Basis ---\n    snapshots = []\n    u_h_initial = 1.0 - (x_e_left + 0.5 * H)\n    for mu_train in train_cases:\n        u_h = solve_nonlinear_system(mu_train, u_h_initial)\n        snapshots.append(u_h)\n    \n    S_matrix = np.stack(snapshots, axis=1)\n    V, _ = np.linalg.qr(S_matrix, mode='reduced')\n\n    # --- Step 3: Evaluate Test Cases ---\n    final_results = []\n    for mu_test in test_cases:\n        nu0_test, A_test = mu_test\n        estimators = []\n        \n        for N in range(1, M + 1):\n            V_N = V[:, :N]\n            u_rom_initial = np.zeros(N)\n            \n            # Solve ROM\n            u_N_coeffs = solve_nonlinear_system(mu_test, u_rom_initial, is_rom=True, V_N=V_N)\n            u_N = V_N @ u_N_coeffs\n            \n            # Compute estimator\n            nu_art_N = compute_nu_art(u_N, nu0_test)\n            S_N = assemble_stiffness(nu_art_N, nu0_test)\n            l_N = assemble_load(A_test, nu0_test)\n            r_N = l_N - S_N @ u_N\n            \n            # eta_N^2 = r_N^T S_N^{-1} r_N\n            try:\n                z = np.linalg.solve(S_N, r_N)\n                eta_N_sq = r_N.T @ z\n                eta_N = np.sqrt(max(0, eta_N_sq))\n            except np.linalg.LinAlgError:\n                eta_N = np.inf # Should not happen with this SPD system\n\n            estimators.append(eta_N)\n\n        # Verify monotonicity\n        monotonic = True\n        for i in range(M - 1): # For N = 1, 2, 3, 4\n            eta_N = estimators[i]\n            eta_Np1 = estimators[i+1]\n            \n            if eta_Np1 > eta_N:\n                monotonic = False\n                break\n            \n            if eta_N > 0:\n                q_N = eta_Np1 / eta_N\n            else: # If eta_N is zero, eta_Np1 must also be zero\n                q_N = 0.0 if eta_Np1 == 0.0 else np.inf\n\n            if not (0.0 <= q_N <= 1.0):\n                monotonic = False\n                break\n        \n        final_results.append(monotonic)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, final_results))}]\".lower())\n\nsolve()\n```", "id": "3361071"}, {"introduction": "The standard greedy algorithm for building a reduced basis relies on a posteriori error estimators, but the quality of these estimators, measured by the effectivity index, can vary. This advanced practice explores a modern, data-driven approach to creating a more intelligent greedy algorithm. By constructing a Bayesian statistical model to predict the effectivity index, we can account for this uncertainty and develop a more robust termination criterion, illustrating how statistical learning can enhance the efficiency of numerical simulation workflows. [@problem_id:3361052]", "problem": "Consider a coercive, affinely parameterized elliptic model problem discretized by a high-order spectral Discontinuous Galerkin (DG) method whose high-fidelity truth solutions are used to build a Reduced Basis (RB) approximation. For each parameter $\\mu \\in [0,1]$, let the RB approximation error in the energy norm be denoted by $e_N(\\mu)$, the residual-based a posteriori error estimator by $\\Delta_N(\\mu)$, and the effectivity index by $\\eta_N(\\mu) = \\Delta_N(\\mu) / e_N(\\mu)$. In typical coercive settings, one expects $\\eta_N(\\mu) \\ge 1$. Assume that, for a fixed RB space size $N$, the decay of the estimator follows an exponential law consistent with spectral DG behavior: $\\Delta_N(\\mu) = A(\\mu) \\exp(-\\rho N)$, where $A(\\mu)$ is a smooth amplitude function and $\\rho > 0$ encodes the spectral decay rate.\n\nWe seek to design an algorithm that terminates the standard greedy RB enrichment when a predicted worst-case credible upper bound on the true error falls below a target tolerance $\\tau$. To this end, model the logarithm of the effectivity index by a linear Gaussian Bayesian model. Let $\\phi(\\mu) \\in \\mathbb{R}^d$ be a feature map, $\\theta \\in \\mathbb{R}^d$ an unknown parameter vector with Gaussian prior $\\theta \\sim \\mathcal{N}(\\theta_0, \\Sigma_0)$, and suppose that the observable $y(\\mu)$ satisfies $y(\\mu) = \\log(\\eta_N(\\mu)) = \\phi(\\mu)^\\top \\theta + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$, independent across samples. The posterior distribution for $\\theta$ and the posterior predictive distribution for $y(\\mu)$ must be computed from Bayes' rule for linear-Gaussian models. Use the posterior predictive distribution for $y(\\mu)$ to obtain, for each $\\mu$, a credible lower bound on $\\eta_N(\\mu)$ at credibility level $1-\\beta$, interpreted as a lower $(\\beta)$-quantile of $\\eta_N(\\mu)$ under the model. Then define a predicted credible upper bound on the true error as $\\widehat{B}_N(\\mu) = \\Delta_N(\\mu) / \\underline{\\eta}_{\\beta}(\\mu)$, where $\\underline{\\eta}_{\\beta}(\\mu)$ is the $(\\beta)$-quantile lower bound for $\\eta_N(\\mu)$. The greedy procedure at RB size $N$ computes $\\max_{\\mu \\in \\mathcal{P}_{\\mathrm{train}}} \\widehat{B}_N(\\mu)$ over a finite training grid $\\mathcal{P}_{\\mathrm{train}}$, and terminates if $\\max_{\\mu \\in \\mathcal{P}_{\\mathrm{train}}} \\widehat{B}_N(\\mu) \\le \\tau$.\n\nConstruct a program that implements this termination logic on a synthetic, yet scientifically consistent, dataset as follows.\n\n- Use a parameter domain $\\mathcal{P} = [0,1]$ discretized into a uniform grid $\\mathcal{P}_{\\mathrm{train}} = \\{\\mu_j\\}_{j=0}^{M-1}$ with $M = 21$ and $\\mu_j = j/(M-1)$.\n- Choose a feature map $\\phi(\\mu) = [1, \\mu, \\mu^2]^\\top$ so that $d = 3$.\n- Synthesize a ground-truth effectivity model with a fixed, unknown vector $\\theta^\\star \\in \\mathbb{R}^3$ and zero observation noise for offline oracle evaluations: $y^\\star(\\mu) = \\phi(\\mu)^\\top \\theta^\\star$, with $\\theta^\\star = [0.25, 0.35, 0.10]^\\top$. Then $\\eta^\\star(\\mu) = \\exp(y^\\star(\\mu))$ is strictly greater than $1$ for all $\\mu \\in [0,1]$.\n- Use a residual-based estimator model $\\Delta_N(\\mu) = A(\\mu) \\exp(-\\rho N)$ with $A(\\mu) = 0.5 + \\mu$. Interpret $\\rho$ as an abstracted spectral DG decay rate consistent with polynomial-degree-controlled exponential convergence.\n- Initialize the offline dataset with $S_0 = 3$ exact oracle observations at $\\mu \\in \\{0, 0.5, 1\\}$: observe $y(\\mu) = y^\\star(\\mu)$ at these points to seed the Bayesian model.\n- For the Bayesian predictor, use a Gaussian prior $\\theta \\sim \\mathcal{N}(\\theta_0, \\Sigma_0)$ with $\\theta_0 = [0, 0, 0]^\\top$ and $\\Sigma_0 = s_0^2 I_3$ for a specified $s_0 > 0$. Use a specified observation noise standard deviation $\\sigma > 0$ in the predictor, which reflects model uncertainty, not oracle noise.\n- At RB size $N$ equal to the number of offline samples in the dataset, perform the following loop:\n  - Compute the posterior for $\\theta$ and the posterior predictive distribution for $y(\\mu)$ at each $\\mu \\in \\mathcal{P}_{\\mathrm{train}}$.\n  - Using credibility parameter $\\beta \\in (0, 0.5)$, form the lower credible bound $\\underline{\\eta}_{\\beta}(\\mu)$ for $\\eta_N(\\mu)$ implied by the Gaussian predictive model for $y(\\mu)$. Then compute $\\widehat{B}_N(\\mu) = \\Delta_N(\\mu)/\\underline{\\eta}_{\\beta}(\\mu)$ and its maximum over $\\mathcal{P}_{\\mathrm{train}}$, denoted $\\widehat{B}^{\\max}_N$.\n  - If $\\widehat{B}^{\\max}_N \\le \\tau$, terminate and output the current RB size $N$.\n  - Otherwise, select $\\mu^\\star = \\arg\\max_{\\mu \\in \\mathcal{P}_{\\mathrm{train}}} \\widehat{B}_N(\\mu)$, augment the offline dataset with a new exact oracle observation $y(\\mu^\\star) = y^\\star(\\mu^\\star)$, increment $N \\leftarrow N+1$, and repeat. Do not select the same $\\mu$ more than once. If a pre-specified maximum number of iterations $N_{\\max}$ is reached, terminate and output the attained $N$.\n\nDerive the necessary Bayesian update and predictive formulas from the definitions of the Gaussian prior, the linear observation model, and Bayes' rule for Gaussian distributions. Then implement the algorithm and run it on the following test suite, where each test case is a tuple $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max})$:\n\n- Test $1$: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.08, 0.10, 0.80, 0.10, 0.50, 20)$.\n- Test $2$: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.15, 0.10, 0.60, 0.10, 0.50, 20)$.\n- Test $3$: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.03, 0.05, 0.80, 0.15, 0.70, 20)$.\n- Test $4$: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.02, 0.01, 0.60, 0.25, 1.00, 12)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is the final RB size $N$ at termination for the corresponding test case, in order. For example, an output with four integers should look like $[n_1,n_2,n_3,n_4]$ with no extra spaces or text. No physical units are involved in this problem. All angles, if any, must be in radians, but no angles are present here. All fractional quantities must be represented as decimals in the program output.", "solution": "The foundational elements are the residual-based a posteriori bound for coercive problems and the effectivity index, and the linear-Gaussian Bayesian model. For each parameter $\\mu$, the RB energy-norm error $e_N(\\mu)$ and the residual-based bound $\\Delta_N(\\mu)$ satisfy the inequality $e_N(\\mu) \\le \\Delta_N(\\mu)$. The effectivity index is $\\eta_N(\\mu) = \\Delta_N(\\mu)/e_N(\\mu)$, which is typically $\\ge 1$ for coercive problems due to the use of a safe lower bound on the coercivity constant. The objective is to construct a conservative credible upper bound for $e_N(\\mu)$ using a lower credible bound on $\\eta_N(\\mu)$ obtained from a Bayesian predictor trained on offline samples.\n\nWe adopt the following synthetic but scientifically consistent models reflecting spectral Discontinuous Galerkin behavior. The estimator decays exponentially in $N$ as $\\Delta_N(\\mu) = A(\\mu) \\exp(-\\rho N)$, capturing spectral convergence with rate parameter $\\rho > 0$, and a smooth amplitude $A(\\mu) = 0.5 + \\mu$. For the effectivity, we model $\\log(\\eta_N(\\mu))$ as a linear function in $\\phi(\\mu) \\in \\mathbb{R}^3$, where $\\phi(\\mu) = [1, \\mu, \\mu^2]^\\top$. We assume\n$$\ny(\\mu) = \\log(\\eta_N(\\mu)) = \\phi(\\mu)^\\top \\theta + \\varepsilon,\\quad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2),\n$$\nwith a Gaussian prior $\\theta \\sim \\mathcal{N}(\\theta_0,\\Sigma_0)$. The offline oracle provides noise-free observations from a ground truth $\\theta^\\star = [0.25, 0.35, 0.10]^\\top$, so $y^\\star(\\mu) = \\phi(\\mu)^\\top \\theta^\\star$ and $\\eta^\\star(\\mu) = \\exp(y^\\star(\\mu))$. Because $\\phi(\\mu)$ has nonnegative components and $\\theta^\\star$ has nonnegative entries, we have $y^\\star(\\mu) \\ge 0$ and consequently $\\eta^\\star(\\mu) \\ge 1$ for all $\\mu \\in [0,1]$.\n\nWe begin with $S_0 = 3$ offline observations at $\\mu \\in \\{0,0.5,1\\}$, set $N = S_0$, and iteratively enrich by greedy selection. The Bayesian posterior for $\\theta$ under the linear-Gaussian model is Gaussian. Let $\\Phi \\in \\mathbb{R}^{S \\times d}$ be the design matrix with rows $\\phi(\\mu_i)^\\top$ for the $S$ observed parameters $\\{\\mu_i\\}_{i=1}^S$, and let $y \\in \\mathbb{R}^S$ be the vector of observed $y(\\mu_i)$. The Gaussian prior is $\\theta \\sim \\mathcal{N}(\\theta_0,\\Sigma_0)$ with $\\theta_0 \\in \\mathbb{R}^d$ and $\\Sigma_0 \\in \\mathbb{R}^{d \\times d}$ positive definite. The likelihood is $y \\mid \\theta \\sim \\mathcal{N}(\\Phi \\theta, \\sigma^2 I_S)$. By Bayes' rule for Gaussian distributions, the posterior is\n$$\n\\theta \\mid y \\sim \\mathcal{N}(\\theta_{\\mathrm{post}}, \\Sigma_{\\mathrm{post}}),\n$$\nwhere\n$$\n\\Sigma_{\\mathrm{post}} = \\left(\\Sigma_0^{-1} + \\sigma^{-2} \\Phi^\\top \\Phi\\right)^{-1}, \\quad \\theta_{\\mathrm{post}} = \\Sigma_{\\mathrm{post}} \\left(\\Sigma_0^{-1}\\theta_0 + \\sigma^{-2} \\Phi^\\top y \\right).\n$$\nFor any new $\\mu$, the posterior predictive distribution for $y(\\mu) = \\phi(\\mu)^\\top \\theta + \\varepsilon$ is Gaussian with mean and variance\n$$\nm(\\mu) = \\phi(\\mu)^\\top \\theta_{\\mathrm{post}}, \\quad s^2(\\mu) = \\phi(\\mu)^\\top \\Sigma_{\\mathrm{post}} \\phi(\\mu) + \\sigma^2.\n$$\nSince $\\eta_N(\\mu) = \\exp(y(\\mu))$ and $y(\\mu) \\sim \\mathcal{N}(m(\\mu), s^2(\\mu))$, we obtain a log-normal predictive model for $\\eta_N(\\mu)$. For a credibility parameter $\\beta \\in (0,0.5)$, a $(1-\\beta)$ lower credible bound for $\\eta_N(\\mu)$ is its $\\beta$-quantile\n$$\n\\underline{\\eta}_{\\beta}(\\mu) = \\exp\\left(m(\\mu) + s(\\mu) z_{\\beta}\\right), \\quad z_{\\beta} = \\Phi^{-1}(\\beta),\n$$\nwhere $\\Phi^{-1}$ is the inverse cumulative distribution function of the standard normal distribution. The corresponding predicted credible upper bound on the true error is then\n$$\n\\widehat{B}_N(\\mu) = \\frac{\\Delta_N(\\mu)}{\\underline{\\eta}_{\\beta}(\\mu)} = \\frac{A(\\mu)\\exp(-\\rho N)}{\\exp\\left(m(\\mu) + s(\\mu) z_{\\beta}\\right)}.\n$$\nWe compute the worst-case predicted bound as $\\widehat{B}^{\\max}_N = \\max_{\\mu \\in \\mathcal{P}_{\\mathrm{train}}} \\widehat{B}_N(\\mu)$. If $\\widehat{B}^{\\max}_N \\le \\tau$, we terminate the greedy process; otherwise, we select $\\mu^\\star = \\arg\\max_{\\mu \\in \\mathcal{P}_{\\mathrm{train}}} \\widehat{B}_N(\\mu)$, query the oracle for $y^\\star(\\mu^\\star)$ (noise-free in this synthetic setup), augment the dataset, and set $N \\leftarrow N+1$. The exponential decay of $\\Delta_N(\\mu)$ is motivated by spectral DG convergence with polynomial degree and stable coercivity lower bounds; the Bayesian predictor adapts to the effectivity landscape learned from samples.\n\nAlgorithmically, we implement the following steps for each test case $(\\tau,\\beta,\\rho,\\sigma,s_0,N_{\\max})$:\n- Initialize $\\mathcal{P}_{\\mathrm{train}} = \\{\\mu_j\\}_{j=0}^{M-1}$ with $M = 21$ uniform points on $[0,1]$.\n- Set $\\theta_0 = [0,0,0]^\\top$ and $\\Sigma_0 = s_0^2 I_3$.\n- Initialize the dataset with $S_0 = 3$ samples at $\\{0,0.5,1\\}$ where $y(\\mu) = \\phi(\\mu)^\\top \\theta^\\star$ and set $N = S_0$.\n- Loop until termination: compute $(\\theta_{\\mathrm{post}},\\Sigma_{\\mathrm{post}})$, then for each $\\mu \\in \\mathcal{P}_{\\mathrm{train}}$ compute $m(\\mu)$ and $s(\\mu)$, then compute $\\underline{\\eta}_{\\beta}(\\mu)$ and $\\widehat{B}_N(\\mu)$, and determine $\\widehat{B}^{\\max}_N$ and its maximizer $\\mu^\\star$. If $\\widehat{B}^{\\max}_N \\le \\tau$ or $N \\ge N_{\\max}$, stop and return $N$. Otherwise, add $(\\mu^\\star, y^\\star(\\mu^\\star))$ to the dataset and increment $N$.\n- Ensure that no parameter is selected more than once.\n\nThe test suite uses:\n- Test $1$: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.08, 0.10, 0.80, 0.10, 0.50, 20)$.\n- Test $2$: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.15, 0.10, 0.60, 0.10, 0.50, 20)$.\n- Test $3$: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.03, 0.05, 0.80, 0.15, 0.70, 20)$.\n- Test $4$: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.02, 0.01, 0.60, 0.25, 1.00, 12)$.\n\nThe program computes the final RB size $N$ for each test and prints them as a single list $[n_1,n_2,n_3,n_4]$ with no additional text.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\n# Synthetic ground truth for log-effectivity: y*(mu) = phi(mu)^T theta_true\ntheta_true = np.array([0.25, 0.35, 0.10])\n\ndef phi(mu: float) -> np.ndarray:\n    \"\"\"Feature map phi(mu) = [1, mu, mu^2].\"\"\"\n    return np.array([1.0, mu, mu * mu])\n\ndef log_eta_true(mu: float) -> float:\n    \"\"\"Noise-free ground-truth log-effectivity.\"\"\"\n    return float(phi(mu).dot(theta_true))\n\ndef posterior_update(theta0: np.ndarray, Sigma0: np.ndarray,\n                     mus: np.ndarray, y: np.ndarray, sigma: float):\n    \"\"\"\n    Compute Gaussian posterior parameters for linear model:\n    y = Phi theta + eps, eps ~ N(0, sigma^2 I).\n    \"\"\"\n    Phi = np.vstack([phi(mu) for mu in mus])  # S x d\n    Sigma0_inv = np.linalg.inv(Sigma0)\n    precision = Sigma0_inv + (1.0 / (sigma ** 2)) * (Phi.T @ Phi)\n    Sigma_post = np.linalg.inv(precision)\n    b = Sigma0_inv @ theta0 + (1.0 / (sigma ** 2)) * (Phi.T @ y)\n    theta_post = Sigma_post @ b\n    return theta_post, Sigma_post\n\ndef predictive_log_stats(mu: float, theta_post: np.ndarray,\n                         Sigma_post: np.ndarray, sigma: float):\n    \"\"\"Posterior predictive mean and std for log-effectivity at mu.\"\"\"\n    ph = phi(mu)\n    m = float(ph.dot(theta_post))\n    s2 = float(ph.dot(Sigma_post @ ph) + sigma ** 2)\n    s = np.sqrt(max(s2, 0.0))\n    return m, s\n\ndef eta_lower_quantile(mu: float, theta_post: np.ndarray,\n                       Sigma_post: np.ndarray, sigma: float, beta: float) -> float:\n    \"\"\"\n    Lower credible bound (beta-quantile) for eta = exp(log-eta),\n    where log-eta ~ N(m, s^2). Quantile is exp(m + s * z_beta).\n    \"\"\"\n    m, s = predictive_log_stats(mu, theta_post, Sigma_post, sigma)\n    z_beta = norm.ppf(beta)\n    log_q = m + s * z_beta\n    q = float(np.exp(log_q))\n    # Guard: effectivity should be >= 1 typically; for conservatism clamp to min 1e-6\n    return max(q, 1e-12)\n\ndef amplitude(mu: float) -> float:\n    \"\"\"Amplitude A(mu) = 0.5 + mu.\"\"\"\n    return 0.5 + mu\n\ndef delta_N(mu: float, N: int, rho: float) -> float:\n    \"\"\"Residual-based estimator model: Delta_N(mu) = A(mu) * exp(-rho * N).\"\"\"\n    return amplitude(mu) * np.exp(-rho * N)\n\ndef greedy_terminate_once(tau: float, beta: float, rho: float,\n                          sigma: float, s0: float, N_max: int) -> int:\n    \"\"\"\n    Run the Bayesian-predicted greedy termination.\n    Return final basis size N at termination (either by criterion or reaching N_max).\n    \"\"\"\n    # Training grid\n    M = 21\n    mus_grid = np.linspace(0.0, 1.0, M).tolist()\n\n    # Prior\n    theta0 = np.zeros(3)\n    Sigma0 = (s0 ** 2) * np.eye(3)\n\n    # Initial offline samples (noise-free oracle)\n    init_mus = [0.0, 0.5, 1.0]\n    train_mus = list(init_mus)\n    y_vals = np.array([log_eta_true(mu) for mu in train_mus], dtype=float)\n    N = len(train_mus)\n\n    # Ensure we do not reselect already used mus\n    used = set(train_mus)\n\n    while True:\n        # Posterior given current data\n        theta_post, Sigma_post = posterior_update(theta0, Sigma0,\n                                                  np.array(train_mus, dtype=float),\n                                                  y_vals, sigma)\n\n        # Compute predicted credible upper bound on true error over grid\n        worst_B = -np.inf\n        worst_mu = None\n        for mu in mus_grid:\n            eta_low = eta_lower_quantile(mu, theta_post, Sigma_post, sigma, beta)\n            B = delta_N(mu, N, rho) / eta_low\n            if B > worst_B:\n                worst_B = B\n                worst_mu = mu\n\n        # Termination checks\n        if worst_B <= tau or N >= N_max:\n            return N\n\n        # Enrich at worst_mu if not already used; otherwise, pick next worst\n        # Build a sorted list by decreasing B to find next unused if needed\n        Bs = []\n        for mu in mus_grid:\n            eta_low = eta_lower_quantile(mu, theta_post, Sigma_post, sigma, beta)\n            B = delta_N(mu, N, rho) / eta_low\n            Bs.append((B, mu))\n        Bs.sort(reverse=True, key=lambda t: t[0])\n        selected_mu = None\n        for _, mu in Bs:\n            if mu not in used:\n                selected_mu = mu\n                break\n\n        if selected_mu is None:\n            # No new parameter available; terminate to avoid infinite loop\n            return N\n\n        # Add new sample (noise-free oracle)\n        train_mus.append(selected_mu)\n        used.add(selected_mu)\n        y_vals = np.append(y_vals, log_eta_true(selected_mu))\n        N += 1\n\ndef solve():\n    # Define the test cases: (tau, beta, rho, sigma, s0, N_max)\n    test_cases = [\n        (0.08, 0.10, 0.80, 0.10, 0.50, 20),  # Test 1\n        (0.15, 0.10, 0.60, 0.10, 0.50, 20),  # Test 2\n        (0.03, 0.05, 0.80, 0.15, 0.70, 20),  # Test 3\n        (0.02, 0.01, 0.60, 0.25, 1.00, 12),  # Test 4\n    ]\n\n    results = []\n    for tau, beta, rho, sigma, s0, N_max in test_cases:\n        N_final = greedy_terminate_once(tau, beta, rho, sigma, s0, N_max)\n        # Ensure integer output\n        results.append(int(N_final))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3361052"}]}