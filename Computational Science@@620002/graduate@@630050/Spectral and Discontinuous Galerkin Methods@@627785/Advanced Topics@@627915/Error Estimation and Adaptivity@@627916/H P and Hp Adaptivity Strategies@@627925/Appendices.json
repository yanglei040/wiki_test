{"hands_on_practices": [{"introduction": "The foundation of any adaptive method is a reliable way to estimate the local error. This exercise provides hands-on practice in computing the components of a residual-based a posteriori error estimator for the Discontinuous Galerkin method. By applying the canonical $h$ and $p$ scalings derived from polynomial inverse and trace inequalities, you will quantify the error contributions from the element interior and its faces, a fundamental skill for implementing or interpreting adaptive solvers [@problem_id:3389838].", "problem": "Consider the scalar Poisson problem on a two-dimensional domain discretized by the Symmetric Interior Penalty Discontinuous Galerkin method (SIPDG). Let a single interior quadrilateral element $T$ be a square of side length $h=10^{-1}$ with polynomial degree $p=3$. Denote the interior residual by $r_{T}=f+\\Delta u_{h}$, the jump of the normal component of the gradient across face $F$ by $j_{F}=[\\nabla u_{h}\\cdot \\mathbf{n}_{F}]$, and the solution jump across face $F$ by $s_{F}=[u_{h}]$. Assume the penalty parameter has the canonical SIPDG scaling $\\sigma=\\kappa p^{2}/h$, and set the shape-regular constants and $\\kappa$ equal to $1$.\n\nYou are given the following $L^{2}$-norms:\n- The interior residual norm on $T$: $\\|r_{T}\\|_{T}=2.0$.\n- The flux-jump norms on the four faces $F_{1},F_{2},F_{3},F_{4}$: $\\|j_{F_{1}}\\|_{F_{1}}=1.5$, $\\|j_{F_{2}}\\|_{F_{2}}=0.6$, $\\|j_{F_{3}}\\|_{F_{3}}=1.0$, $\\|j_{F_{4}}\\|_{F_{4}}=0.2$.\n- The solution-jump norms on the four faces: $\\|s_{F_{1}}\\|_{F_{1}}=2\\times 10^{-2}$, $\\|s_{F_{2}}\\|_{F_{2}}=8\\times 10^{-2}$, $\\|s_{F_{3}}\\|_{F_{3}}=4\\times 10^{-2}$, $\\|s_{F_{4}}\\|_{F_{4}}=10^{-1}$.\n\nStarting from the foundational inequalities for polynomials on shape-regular elements (inverse estimates and trace inequalities) and the SIPDG stability scaling, derive the canonical $h$–$p$ weighted form of the local residual-based a posteriori estimator contributions for element and faces. Use these derived scalings to compute:\n- The element residual contribution $\\eta_{T}$.\n- The face contributions $\\eta_{F_{i}}$ for $i\\in\\{1,2,3,4\\}$.\n\nFinally, determine which face $F_{i}$ has the largest face contribution (i.e., dominates) and report, as the final answer, the index $i\\in\\{1,2,3,4\\}$ of that dominating face. No rounding is required and the final answer must be the single integer index $i$ without any units.", "solution": "The Symmetric Interior Penalty Discontinuous Galerkin method (SIPDG) for the scalar Poisson equation builds coercivity by adding a penalty term that, for polynomial degree $p$ and characteristic element size $h$, scales as $\\sigma=\\kappa p^{2}/h$ with a shape-regular constant $\\kappa0$. For a posteriori residual-based estimators in this context, the canonical $h$–$p$ scalings of the element residual and the face jumps follow from the combination of inverse inequalities and trace inequalities for polynomial spaces.\n\nThe fundamental building blocks are:\n- Polynomial inverse inequality: for $v$ in a polynomial space of degree $p$ on an element $T$ of size $h$, one has\n$$\n\\|\\nabla v\\|_{T}\\leq C_{\\mathrm{inv}}\\frac{p^{2}}{h}\\|v\\|_{T},\n$$\nand more generally higher derivatives scale with appropriate powers of $p/h$.\n- Trace inequality: for $v$ on $T$ with face $F\\subset\\partial T$,\n$$\n\\|v\\|_{F}\\leq C_{\\mathrm{tr}}\\frac{p}{h^{1/2}}\\|v\\|_{T},\n$$\nand similarly for normal derivatives,\n$$\n\\|\\nabla v\\cdot \\mathbf{n}\\|_{F}\\leq C_{\\mathrm{tr}}'\\frac{p^{2}}{h^{1/2}}\\|v\\|_{T}.\n$$\n\nThese inequalities enter reliability and efficiency proofs for residual-based estimators, yielding canonical $h$–$p$ weightings for the contributions:\n- The interior residual contributes with weight proportional to $h/p$ (reflecting that the element bubble functions used in lifting the residual provide an $h$ factor, while inverse scaling in $p$ arises from polynomial approximation properties). Thus one takes\n$$\n\\eta_{T}=\\left(\\frac{h}{p}\\right)\\|r_{T}\\|_{T}^{2}.\n$$\n- The flux-jump on a face $F$ contributes with weight $h/p$, consistent with lifting the normal derivative jump into the element and controlling it via trace and inverse inequalities:\n$$\n\\eta_{F}^{(\\mathrm{flux})}=\\left(\\frac{h}{p}\\right)\\|j_{F}\\|_{F}^{2}.\n$$\n- The solution-jump on a face $F$ contributes with a weight proportional to the SIPDG penalty scaling, namely $p^{2}/h$, ensuring coercivity and consistent control of the jump term:\n$$\n\\eta_{F}^{(\\mathrm{sol})}=\\left(\\frac{p^{2}}{h}\\right)\\|s_{F}\\|_{F}^{2}.\n$$\n\nFor this exercise, the shape-regular constants are set to $1$ and the penalty constant $\\kappa$ is set to $1$, so the local face contribution is the sum\n$$\n\\eta_{F}=\\left(\\frac{h}{p}\\right)\\|j_{F}\\|_{F}^{2}+\\left(\\frac{p^{2}}{h}\\right)\\|s_{F}\\|_{F}^{2}.\n$$\n\nWe now compute the numerical values for $h=10^{-1}$ and $p=3$:\n$$\n\\frac{h}{p}=\\frac{10^{-1}}{3}=\\frac{1}{30},\\qquad \\frac{p^{2}}{h}=\\frac{9}{10^{-1}}=90.\n$$\n\nElement residual contribution:\n$$\n\\eta_{T}=\\left(\\frac{h}{p}\\right)\\|r_{T}\\|_{T}^{2}=\\frac{1}{30}(2.0)^{2}=\\frac{1}{30}\\cdot 4=\\frac{2}{15}\\approx 0.133333\\ldots\n$$\n\nFace contributions:\n\nFor $F_{1}$, with $\\|j_{F_{1}}\\|_{F_{1}}=1.5$ and $\\|s_{F_{1}}\\|_{F_{1}}=2\\times 10^{-2}$,\n$$\n\\eta_{F_{1}}=\\frac{1}{30}(1.5)^{2}+90(2\\times 10^{-2})^{2}=\\frac{1}{30}\\cdot 2.25+90\\cdot 4\\times 10^{-4}=\\frac{2.25}{30}+0.036=0.075+0.036=0.111.\n$$\n\nFor $F_{2}$, with $\\|j_{F_{2}}\\|_{F_{2}}=0.6$ and $\\|s_{F_{2}}\\|_{F_{2}}=8\\times 10^{-2}$,\n$$\n\\eta_{F_{2}}=\\frac{1}{30}(0.6)^{2}+90(8\\times 10^{-2})^{2}=\\frac{1}{30}\\cdot 0.36+90\\cdot 6.4\\times 10^{-3}=0.012+0.576=0.588.\n$$\n\nFor $F_{3}$, with $\\|j_{F_{3}}\\|_{F_{3}}=1.0$ and $\\|s_{F_{3}}\\|_{F_{3}}=4\\times 10^{-2}$,\n$$\n\\eta_{F_{3}}=\\frac{1}{30}(1.0)^{2}+90(4\\times 10^{-2})^{2}=\\frac{1}{30}\\cdot 1+90\\cdot 1.6\\times 10^{-3}=\\frac{1}{30}+0.144\\approx 0.177333\\ldots\n$$\n\nFor $F_{4}$, with $\\|j_{F_{4}}\\|_{F_{4}}=0.2$ and $\\|s_{F_{4}}\\|_{F_{4}}=10^{-1}$,\n$$\n\\eta_{F_{4}}=\\frac{1}{30}(0.2)^{2}+90(10^{-1})^{2}=\\frac{1}{30}\\cdot 0.04+90\\cdot 10^{-2}=\\frac{1}{750}+0.9\\approx 0.901333\\ldots\n$$\n\nTo determine the dominating face, we compare the face contributions:\n$$\n\\eta_{F_{1}}= 0.111,\\quad\n\\eta_{F_{2}}= 0.588,\\quad\n\\eta_{F_{3}}\\approx 0.177333\\ldots,\\quad\n\\eta_{F_{4}}\\approx 0.901333\\ldots\n$$\nThe largest value is $\\eta_{F_{4}}$, so the dominating face is $F_{4}$, whose index is $4$.", "answer": "$$\\boxed{4}$$", "id": "3389838"}, {"introduction": "An effective $hp$-adaptive strategy must not only identify elements with large error but also decide whether $h$-refinement or $p$-refinement is more efficient, often under a strict computational budget. This problem challenges you to implement a complete decision-making step of an advanced adaptive algorithm, where the choice between refining in $h$ or $p$ is based on maximizing the predicted error reduction per additional degree of freedom. You will use a knapsack optimization algorithm to select the best set of refinements that adheres to a global budget, providing a realistic simulation of how modern solvers manage complexity [@problem_id:3389819].", "problem": "Consider a mesh of elements used in a Spectral Element Method (SEM) with Discontinuous Galerkin (DG) formulation. Each element $K$ is characterized by its current mesh size $h_K$, local polynomial degree $p_K$, and a non-negative local error indicator $\\eta_K$. The domain dimension is $d \\in \\{1,2,3\\}$, and the basis functions per element are tensor-product polynomials. The number of scalar degrees of freedom (DOFs) per element is modeled as $(p_K+1)^d$. You will simulate a single global decision step of an $hp$-adaptivity algorithm under a fixed budget of additional DOFs, deciding for each element whether to perform an $h$-refinement (split the element) or a $p$-refinement (increase the local polynomial degree), and then selecting a subset of these candidate actions to fit the budget while maximizing predicted error decrease.\n\nUse the following foundational principles and well-tested facts as the base of your model:\n\n- In DG and spectral element methods, the number of local basis functions per element for tensor-product polynomials of degree $p_K$ in $d$ dimensions is $(p_K+1)^d$.\n- A uniform $h$-refinement in $d$ dimensions splits one parent element into $2^d$ child elements, thereby increasing the global DOFs. The additional DOF cost of performing this $h$-refinement on a single element $K$ with degree $p_K$ is $\\Delta\\mathrm{dof}_h(K) = \\big(2^d-1\\big)\\,(p_K+1)^d$.\n- A $p$-refinement increases the local polynomial degree by $1$, from $p_K$ to $p_K+1$, with additional DOF cost $\\Delta\\mathrm{dof}_p(K) = \\big((p_K+2)^d-(p_K+1)^d\\big)$, provided $p_K  p_{\\max}$.\n- For smooth solutions, the $h$-convergence rate of DG methods is algebraic of order $p_K+1$; under limited regularity, the effective rate is capped. Model the predicted error indicator after one $h$-refinement on element $K$ as $\\eta'_h(K) = \\eta_K\\,2^{-\\mu_K}$ with $\\mu_K = \\min(p_K+1, r_{\\max})$, where $r_{\\max}$ is a regularity cap supplied as a global constant.\n- For analytic/sufficiently smooth solutions, $p$-refinement typically exhibits exponential convergence. Model the predicted error indicator after one $p$-refinement on element $K$ as $\\eta'_p(K) = \\eta_K\\,e^{-b}$, where $b0$ is a global saturation constant.\n\nDefine the local benefit of an action on element $K$ as the predicted reduction in the local error indicator:\n- $B_h(K) = \\eta_K - \\eta'_h(K)$ for $h$-refinement,\n- $B_p(K) = \\eta_K - \\eta'_p(K)$ for $p$-refinement (only if $p_K  p_{\\max}$).\n\nDefine the local efficiency ratio of an action as benefit divided by additional DOF cost:\n- $R_h(K) = \\dfrac{B_h(K)}{\\Delta\\mathrm{dof}_h(K)}$,\n- $R_p(K) = \\dfrac{B_p(K)}{\\Delta\\mathrm{dof}_p(K)}$ (only if $p_K  p_{\\max}$).\n\nFor each element $K$, select a single proposed action as follows:\n- If $p_K  p_{\\max}$, compare $R_h(K)$ and $R_p(K)$; propose the action with larger ratio (if equal within numerical equality, choose $p$-refinement).\n- If $p_K \\ge p_{\\max}$, propose $h$-refinement.\nIf the proposed action’s additional DOF cost exceeds the budget, it may still be proposed but will only be taken if the global selection under the budget allows it.\n\nGiven a total additional DOF budget $B$, choose a subset of the proposed element actions that maximizes the sum of their benefits $\\sum B(\\cdot)$ subject to the sum of their costs $\\sum \\Delta\\mathrm{dof}(\\cdot) \\le B$. This is a $0$-$1$ knapsack selection.\n\nUpon selecting the subset, update the element states:\n- For a selected $h$-refinement on element $K$, set $h_K \\leftarrow h_K/2$ and keep $p_K$ unchanged.\n- For a selected $p$-refinement on element $K$, set $p_K \\leftarrow p_K+1$ and keep $h_K$ unchanged.\nElements not selected remain unchanged. Do not create or delete elements; the $h$-refinement is recorded by halving $h_K$ of the existing element.\n\nAll quantities are dimensionless in this problem. The program must implement the above decision and selection and return the updated states and summary metrics for each test case.\n\nTest Suite:\nUse the following four test cases. For each case, the inputs are $(d, p_{\\max}, r_{\\max}, b, B)$ and a list of elements with triples $(h_K, p_K, \\eta_K)$:\n\n- Case $1$ (general case):\n  - $d=2$, $p_{\\max}=6$, $r_{\\max}=3$, $b=0.7$, $B=60$.\n  - Elements:\n    - $K_1$: $(h_{K_1}, p_{K_1}, \\eta_{K_1}) = (0.25, 2, 0.08)$,\n    - $K_2$: $(h_{K_2}, p_{K_2}, \\eta_{K_2}) = (0.10, 3, 0.02)$,\n    - $K_3$: $(h_{K_3}, p_{K_3}, \\eta_{K_3}) = (0.20, 1, 0.15)$,\n    - $K_4$: $(h_{K_4}, p_{K_4}, \\eta_{K_4}) = (0.05, 4, 0.01)$,\n    - $K_5$: $(h_{K_5}, p_{K_5}, \\eta_{K_5}) = (0.30, 0, 0.25)$.\n- Case $2$ (zero budget boundary):\n  - $d=1$, $p_{\\max}=5$, $r_{\\max}=4$, $b=0.5$, $B=0$.\n  - Elements:\n    - $K_1$: $(h_{K_1}, p_{K_1}, \\eta_{K_1}) = (0.50, 2, 0.12)$,\n    - $K_2$: $(h_{K_2}, p_{K_2}, \\eta_{K_2}) = (0.30, 0, 0.06)$,\n    - $K_3$: $(h_{K_3}, p_{K_3}, \\eta_{K_3}) = (0.25, 4, 0.02)$.\n- Case $3$ (high dimension with $p$ capped):\n  - $d=3$, $p_{\\max}=2$, $r_{\\max}=2$, $b=1.0$, $B=100$.\n  - Elements:\n    - $K_1$: $(h_{K_1}, p_{K_1}, \\eta_{K_1}) = (0.20, 2, 0.20)$,\n    - $K_2$: $(h_{K_2}, p_{K_2}, \\eta_{K_2}) = (0.10, 2, 0.05)$,\n    - $K_3$: $(h_{K_3}, p_{K_3}, \\eta_{K_3}) = (0.15, 1, 0.10)$.\n- Case $4$ (mixed actions under tight budget):\n  - $d=2$, $p_{\\max}=5$, $r_{\\max}=2$, $b=0.3$, $B=20$.\n  - Elements:\n    - $K_1$: $(h_{K_1}, p_{K_1}, \\eta_{K_1}) = (0.40, 0, 0.50)$,\n    - $K_2$: $(h_{K_2}, p_{K_2}, \\eta_{K_2}) = (0.10, 5, 0.04)$,\n    - $K_3$: $(h_{K_3}, p_{K_3}, \\eta_{K_3}) = (0.20, 2, 0.12)$,\n    - $K_4$: $(h_{K_4}, p_{K_4}, \\eta_{K_4}) = (0.30, 1, 0.08)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a list with three entries:\n- the flattened list of updated states in the form $[h_{K_1}, p_{K_1}, h_{K_2}, p_{K_2}, \\dots]$,\n- the integer total of used additional DOFs,\n- the float total predicted error reduction (sum of selected benefits).\n\nTherefore, the overall output is a list of these per-case lists, e.g., $[[\\dots], \\mathrm{used\\_dofs}_1, \\mathrm{total\\_reduction}_1], [[\\dots], \\mathrm{used\\_dofs}_2, \\mathrm{total\\_reduction}_2], \\dots$ printed on one line. All numeric values should be represented as Python numbers in the output.", "solution": "We base the design on standard modeling for Discontinuous Galerkin (DG) spectral element methods, combining the degrees of freedom (DOFs) scaling with $h$- and $p$-convergence properties to formulate an $hp$-decision under a DOF budget.\n\nFundamental base:\n- The number of local basis functions (and thus local scalar DOFs) for tensor-product polynomials of degree $p_K$ in $d$ dimensions is $(p_K+1)^d$. This follows from counting one-dimensional basis functions per coordinate, each contributing $(p_K+1)$ modes, and forming the tensor product across $d$ coordinates, yielding $(p_K+1)^d$ total modes.\n- Uniform $h$-refinement of one element in $d$ dimensions creates $2^d$ children. If the parent had $(p_K+1)^d$ DOFs locally, the children collectively have $2^d\\,(p_K+1)^d$ DOFs when keeping the same $p_K$. The global increment in DOFs attributable to refining this one element is $\\Delta\\mathrm{dof}_h(K)=\\big(2^d-1\\big)\\,(p_K+1)^d$.\n- A $p$-refinement of one element increases its local degree from $p_K$ to $p_K+1$, so the element’s DOFs increase from $(p_K+1)^d$ to $(p_K+2)^d$. The incremental DOFs are $\\Delta\\mathrm{dof}_p(K)=\\big((p_K+2)^d-(p_K+1)^d\\big)$, provided $p_K  p_{\\max}$.\n\nError indicator models:\n- For $h$-refinement, the $L^2$ or energy norm error for DG methods in smooth regimes typically scales like $h^{p_K+1}$. In limited regularity, we cap the effective order by $r_{\\max}$. When halving $h_K$ (uniform refinement across the element), the indicator contracts multiplicatively by a factor $2^{-\\mu_K}$ with $\\mu_K=\\min(p_K+1,r_{\\max})$. Thus, the predicted post-refinement indicator is $\\eta'_h(K)=\\eta_K\\,2^{-\\mu_K}$, and the benefit is $B_h(K)=\\eta_K-\\eta'_h(K)=\\eta_K\\big(1-2^{-\\mu_K}\\big)$.\n- For $p$-refinement, in analytic regimes the error contracts exponentially with $p$. We adopt the saturation model $\\eta'_p(K)=\\eta_K\\,e^{-b}$ for a single increment $p\\mapsto p+1$, with $b0$ given globally. The benefit is $B_p(K)=\\eta_K-\\eta'_p(K)=\\eta_K\\big(1-e^{-b}\\big)$.\n\nEfficiency and local proposal:\n- Define the efficiency ratios $R_h(K)=\\dfrac{B_h(K)}{\\Delta\\mathrm{dof}_h(K)}$ and $R_p(K)=\\dfrac{B_p(K)}{\\Delta\\mathrm{dof}_p(K)}$ if $p_Kp_{\\max}$. For each element, propose the action (either $h$ or $p$) with larger ratio; if $p_K\\ge p_{\\max}$ then only $h$ is eligible. In ties, prefer $p$.\n\nGlobal selection under budget:\n- With a total budget $B$ of additional DOFs available, choose a subset of proposed actions that maximizes the total predicted benefit subject to the sum of additional DOF costs not exceeding $B$. This is the classical $0$-$1$ knapsack problem. Since costs $\\Delta\\mathrm{dof}$ are integers and the number of elements is moderate, a dynamic programming solution over capacity from $0$ to $B$ yields the optimal subset.\n\nState update rules:\n- If an element’s selected action is $h$-refinement, set $h_K\\leftarrow h_K/2$ and keep $p_K$ unchanged. If the action is $p$-refinement, set $p_K\\leftarrow p_K+1$ and keep $h_K$ unchanged. Elements without selected actions remain unchanged. We do not change the number of elements; the refinement decision is recorded in the updated $(h_K,p_K)$ pair.\n\nAlgorithmic steps:\n- For each test case, compute $(\\Delta\\mathrm{dof}_h(K), B_h(K), R_h(K))$ and, if eligible, $(\\Delta\\mathrm{dof}_p(K), B_p(K), R_p(K))$ for each element.\n- Choose one proposal per element according to the ratio rule, forming items with weights (costs) and values (benefits).\n- Solve a $0$-$1$ knapsack via dynamic programming to select items with maximum total benefit under capacity $B$.\n- Apply updates to $(h_K,p_K)$ according to selected items.\n- Report the flattened list $[h_{K_1}, p_{K_1}, h_{K_2}, p_{K_2}, \\dots]$, the integer total used DOFs, and the total predicted reduction as a float, for each test case.\n\nIllustrative expectations on provided cases (the program performs exact computations):\n- Case $1$: With $d=2$, $p_{\\max}=6$, $r_{\\max}=3$, $b=0.7$, $B=60$, the efficiency ratios favor $p$-refinement for all elements, and the total cost fits the budget; thus, all $p_K$ increase by $1$, $h_K$ unchanged. The used additional DOFs and summed benefits are computed precisely.\n- Case $2$: With $B=0$, no action can be selected; outputs equal inputs, used DOFs are $0$, total reduction is $0.0$.\n- Case $3$: With $d=3$, $p_{\\max}=2$, most $p$-refinements are disallowed; $h$-refinements for elements at $p_{\\max}$ have costs exceeding the budget, so only the element with $p_K=1$ is eligible and selected according to the ratio comparison under the budget.\n- Case $4$: Mixed $h$ and $p$ proposals arise; under the tight budget, the knapsack chooses the combination of actions that maximizes total benefit per budget.\n\nThe final program implements the above steps deterministically and prints a single-line list aggregating the per-case results as specified.", "answer": "```python\nimport numpy as np\n\ndef hp_decision_update(elements, d, p_max, r_max, b, budget):\n    \"\"\"\n    elements: list of dicts with keys 'h', 'p', 'eta'\n    d: dimension (1,2,3)\n    p_max: maximum polynomial degree allowed\n    r_max: regularity cap for h-contraction exponent\n    b: p-saturation constant (0)\n    budget: integer additional DOF capacity\n    Returns: updated_flat_list, used_dofs, total_benefit\n    \"\"\"\n    # Prepare proposals: one per element\n    proposals = []  # each item: dict with keys idx, cost, benefit, action ('h' or 'p')\n    for idx, el in enumerate(elements):\n        h = el['h']\n        p = el['p']\n        eta = el['eta']\n        # Costs\n        base = (p + 1) ** d\n        cost_h = (2 ** d - 1) * base\n        mu = min(p + 1, r_max)\n        eta_h_new = eta * (2.0 ** (-mu))\n        benefit_h = eta - eta_h_new\n        ratio_h = benefit_h / cost_h if cost_h  0 else 0.0\n\n        # p candidate if allowed\n        candidate_p_allowed = p  p_max\n        ratio_p = -np.inf\n        cost_p = None\n        benefit_p = None\n        if candidate_p_allowed:\n            cost_p = (p + 2) ** d - (p + 1) ** d\n            eta_p_new = eta * np.exp(-b)\n            benefit_p = eta - eta_p_new\n            ratio_p = benefit_p / cost_p if cost_p  0 else 0.0\n\n        # Choose proposal: higher ratio; tie - prefer p\n        if candidate_p_allowed and ratio_p = ratio_h:\n            proposals.append({\n                'idx': idx,\n                'cost': int(cost_p),\n                'benefit': float(benefit_p),\n                'action': 'p'\n            })\n        else:\n            proposals.append({\n                'idx': idx,\n                'cost': int(cost_h),\n                'benefit': float(benefit_h),\n                'action': 'h'\n            })\n\n    # 0-1 knapsack dynamic programming\n    capacity = int(budget)\n    n = len(proposals)\n    # dp[w] = max benefit achievable with capacity w\n    dp = np.zeros(capacity + 1, dtype=float)\n    # keep track selection: for reconstruction, use 2D boolean or predecessor\n    # We'll use a 2D table of choices: take[i][w] indicates if item i taken at capacity w\n    take = np.zeros((n, capacity + 1), dtype=bool)\n\n    for i in range(n):\n        cost = proposals[i]['cost']\n        value = proposals[i]['benefit']\n        # iterate w descending to avoid reuse of same item\n        for w in range(capacity, cost - 1, -1):\n            if dp[w - cost] + value  dp[w] + 1e-16:  # small epsilon to break ties consistently\n                dp[w] = dp[w - cost] + value\n                take[i, w] = True\n        # For capacities below cost, take[i,w] remains False\n\n    # reconstruct selection from dp by backtracking the take table\n    selected = set()\n    used_dofs = 0\n    total_benefit = 0.0\n    w = capacity\n    for i in range(n - 1, -1, -1):\n        if w = 0 and take[i, w]:\n            selected.add(i)\n            used_dofs += proposals[i]['cost']\n            total_benefit += proposals[i]['benefit']\n            w -= proposals[i]['cost']\n\n    # Apply updates\n    updated = []\n    for i, el in enumerate(elements):\n        h = el['h']\n        p = el['p']\n        if i in selected:\n            if proposals[i]['action'] == 'h':\n                h = h / 2.0\n            else:\n                p = p + 1\n        updated.extend([h, int(p)])\n\n    return updated, used_dofs, float(total_benefit)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            'd': 2, 'p_max': 6, 'r_max': 3, 'b': 0.7, 'budget': 60,\n            'elements': [\n                {'h': 0.25, 'p': 2, 'eta': 0.08},\n                {'h': 0.10, 'p': 3, 'eta': 0.02},\n                {'h': 0.20, 'p': 1, 'eta': 0.15},\n                {'h': 0.05, 'p': 4, 'eta': 0.01},\n                {'h': 0.30, 'p': 0, 'eta': 0.25},\n            ]\n        },\n        # Case 2\n        {\n            'd': 1, 'p_max': 5, 'r_max': 4, 'b': 0.5, 'budget': 0,\n            'elements': [\n                {'h': 0.50, 'p': 2, 'eta': 0.12},\n                {'h': 0.30, 'p': 0, 'eta': 0.06},\n                {'h': 0.25, 'p': 4, 'eta': 0.02},\n            ]\n        },\n        # Case 3\n        {\n            'd': 3, 'p_max': 2, 'r_max': 2, 'b': 1.0, 'budget': 100,\n            'elements': [\n                {'h': 0.20, 'p': 2, 'eta': 0.20},\n                {'h': 0.10, 'p': 2, 'eta': 0.05},\n                {'h': 0.15, 'p': 1, 'eta': 0.10},\n            ]\n        },\n        # Case 4\n        {\n            'd': 2, 'p_max': 5, 'r_max': 2, 'b': 0.3, 'budget': 20,\n            'elements': [\n                {'h': 0.40, 'p': 0, 'eta': 0.50},\n                {'h': 0.10, 'p': 5, 'eta': 0.04},\n                {'h': 0.20, 'p': 2, 'eta': 0.12},\n                {'h': 0.30, 'p': 1, 'eta': 0.08},\n            ]\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        updated, used_dofs, total_benefit = hp_decision_update(\n            case['elements'], case['d'], case['p_max'], case['r_max'], case['b'], case['budget']\n        )\n        results.append([updated, used_dofs, total_benefit])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3389819"}, {"introduction": "Applying $h$-refinement often leads to non-conforming meshes containing \"hanging nodes,\" which pose a significant challenge for ensuring a conservative numerical scheme. This problem focuses on the crucial implementation detail of handling such a T-junction interface between a coarse element and its refined neighbors. You will evaluate different strategies to correctly compute the numerical flux, leading you to the robust mortar method, which is essential for maintaining accuracy and discrete conservation in high-order DG methods on adaptively refined meshes [@problem_id:3389932].", "problem": "Consider a $2$D conservative system posed on a mesh formed by isoparametric quadrilateral elements. Let $K_c$ be a coarse quadrilateral with reference mapping $\\mathbf F_c : [-1,1]^2 \\to \\mathbb R^2$ and polynomial degree $p_c$, and let $K_{f,1}$ and $K_{f,2}$ be two refined quadrilateral neighbors with mappings $\\mathbf F_{f,1}, \\mathbf F_{f,2} : [-1,1]^2 \\to \\mathbb R^2$ and degrees $p_{f,1}$ and $p_{f,2}$, respectively. The three elements meet along a single geometric interface segment $\\Gamma$ that coincides with one entire face of $K_c$ and with one face of each $K_{f,i}$, such that $\\Gamma$ on $K_c$ is partitioned into two subsegments by a $T$-junction: one subsegment coincides with the face of $K_{f,1}$ and the other with the face of $K_{f,2}$. Assume the face geometry is consistent so that the union of the two refined faces matches the coarse face exactly in physical space.\n\nIn a Discontinuous Galerkin (DG) method, the inter-element coupling contributes interface integrals involving a numerical flux $\\widehat{\\mathbf F}$ evaluated from traces taken from adjacent elements and integrated along $\\Gamma$. The fundamental requirements are discrete conservation across $\\Gamma$, geometrically consistent integration, and handling of $h$-, $p$-, and $hp$-nonconformity.\n\nLet the coarse face be parameterized by a one-dimensional reference coordinate $\\xi \\in [-1,1]$ via the restriction of $\\mathbf F_c$ to that face, and let each refined face be parameterized by its own coordinate $\\eta \\in [-1,1]$ via the restriction of $\\mathbf F_{f,i}$. Denote by $\\mathbf n_c(\\xi)$ the outward unit normal of $K_c$ along $\\Gamma$ and by $\\mathbf n_{f,i}(\\eta)$ the outward unit normal of $K_{f,i}$ along its face on $\\Gamma$. Let $J_c(\\xi)$ and $J_{f,i}(\\eta)$ be the line Jacobians induced by the respective isoparametric mappings along the interface, defined by $J(\\cdot) = \\|\\partial(\\cdot) \\mathbf F\\|$ on the face curve. You may assume the numerical flux $\\widehat{\\mathbf F}$ depends (locally) and smoothly on the physical states and the normal direction at the evaluation point.\n\nWhich of the following constructions correctly sets up the face integration and quadrature mappings required to compute a conservative numerical flux along $\\Gamma$ between $K_c$ and its two refined neighbors, in the presence of $h$- and $p$-nonconformity?\n\nA. Use a single Gauss–Legendre quadrature rule on the coarse parameter interval $\\xi \\in [-1,1]$ with $p_c+1$ points. For the refined neighbors, map coarse points into each refined face by the affine map $\\eta = \\frac{1}{2}\\xi \\pm \\frac{1}{2}$ on the corresponding half of $\\Gamma$, evaluate the flux at these points using $\\mathbf n_c(\\xi)$, and assign half of each coarse weight to each refined neighbor, ignoring line Jacobians of the refined faces.\n\nB. Define a mortar parameter $\\xi \\in [-1,1]$ on the coarse face, partition it into two subintervals, $\\xi \\in [-1,0]$ and $\\xi \\in [0,1]$, and use Gauss–Legendre quadrature independently on each subinterval. Map each subinterval to the corresponding refined face by the affine relations\n$$\n\\eta_{(1)} = 2\\xi + 1 \\quad \\text{for} \\quad \\xi \\in [-1,0], \\qquad\n\\eta_{(2)} = 2\\xi - 1 \\quad \\text{for} \\quad \\xi \\in [0,1].\n$$\nEvaluate a single numerical flux $\\widehat{\\mathbf F}$ at common physical points using $\\mathbf n_c(\\xi)$, and accumulate contributions to all three elements using the coarse line Jacobian $J_c(\\xi)$ and quadrature weights of the mortar subintervals; do not perform any projection of traces to a common interface space even if $p_c \\neq p_{f,i}$.\n\nC. Compute two separate flux integrals: one on each refined face, using Gauss–Legendre quadrature on $\\eta \\in [-1,1]$ for $K_{f,1}$ and $K_{f,2}$, respectively. For the coarse element contribution, integrate independently on $\\xi \\in [-1,1]$ with its own quadrature. Evaluate $\\widehat{\\mathbf F}$ at the distinct quadrature nodes and sum the three integrals, relying on the fact that the union of refined faces equals the coarse face; do not enforce common quadrature nodes, weights, or normals across the interface.\n\nD. Construct a mortar interface space on $\\Gamma$ with parameter $\\xi \\in [-1,1]$ and polynomial degree $p_m = \\max\\{p_c, p_{f,1}, p_{f,2}\\}$. Partition the mortar into the two subsegments $\\xi \\in [-1,0]$ and $\\xi \\in [0,1]$, and on each subsegment select a Gauss–Legendre quadrature rule with enough points to integrate polynomials up to degree $p_m$ on that subsegment. Map the mortar nodes to each refined face via\n$$\n\\eta_{(1)} = 2\\xi + 1 \\quad \\text{for} \\quad \\xi \\in [-1,0], \\qquad\n\\eta_{(2)} = 2\\xi - 1 \\quad \\text{for} \\quad \\xi \\in [0,1],\n$$\nand evaluate a single numerical flux $\\widehat{\\mathbf F}$ at the common physical points with consistent normals, taking $\\mathbf n_{f,i}(\\eta(\\xi)) = -\\mathbf n_c(\\xi)$. Project both coarse and refined trace fields onto the mortar space on their respective subsegments before flux evaluation, so that all traces live in the same polynomial space on $\\Gamma$. Assemble the interface contribution once using the physical line measure $J(\\xi)$ and distribute equal-and-opposite contributions to $K_c$ and $\\{K_{f,1},K_{f,2}\\}$ to enforce discrete conservation.\n\nChoose the correct option.", "solution": "The problem statement is a valid and well-posed question in the field of numerical analysis, specifically concerning the implementation of Discontinuous Galerkin (DG) methods on meshes with $h$- and $p$-nonconformity. The setup describes a standard scenario with a hanging node (a T-junction) where a coarse element interfaces with two smaller, refined elements. The core task is to identify the correct procedure for computing the interface flux integrals in a way that is geometrically consistent, accurate, and, most importantly, discretely conservative.\n\nThe fundamental principles for handling such non-conforming interfaces in a DG framework are as follows:\n$1$. **Discrete Conservation**: To ensure that the numerical scheme is conservative, the flux exchange between neighboring elements must sum to zero. For a non-conforming interface, this is achieved by defining a single numerical integration scheme (i.e., a single set of quadrature points and weights) that is used for all elements sharing the interface. At each quadrature point, a single flux value is computed based on the states of the adjacent elements. This flux value is then added to the residual of one element and subtracted from the residual of its neighbor. Any method that uses different quadrature rules for adjacent elements will, in general, fail to be conservative.\n$2$. **Geometric and Functional Consistency**: The physical interface $\\Gamma$ is a single geometric entity. The integrals must be computed over this common geometry. The parameterizations for the coarse face, $\\xi \\in [-1,1]$, and the refined faces, $\\eta \\in [-1,1]$, must be correctly related. As the problem states, the coarse face is partitioned. This suggests that the integration scheme must be defined on the coarse face (the \"mortar\" face) and should respect the partition. A composite quadrature rule, with separate rules on the sub-intervals corresponding to each refined neighbor, is appropriate.\n$3$. **Handling Polynomial Non-conformity ($p$-refinement)**: When the polynomial degrees of the solutions in adjacent elements differ (e.g., $p_c \\neq p_{f,1}$), their traces on the interface belong to different polynomial spaces. A naive evaluation of these different polynomials at quadrature points can lead to a loss of accuracy and stability. The standard and most robust technique, known as a mortar method, is to introduce a common, intermediate polynomial space on the interface, the \"mortar space\", with a sufficiently high polynomial degree $p_m$. The traces from all adjacent elements are first projected (typically via an $L^2$-projection) into this mortar space before the numerical flux is computed. This ensures that the two states being compared by the flux function reside in the same functional space.\n\nBased on these principles, we can now evaluate each option.\n\n**Option A: Use a single Gauss–Legendre quadrature rule on the coarse parameter interval $\\xi \\in [-1,1]$ with $p_c+1$ points. For the refined neighbors, map coarse points into each refined face by the affine map $\\eta = \\frac{1}{2}\\xi \\pm \\frac{1}{2}$ on the corresponding half of $\\Gamma$, evaluate the flux at these points using $\\mathbf n_c(\\xi)$, and assign half of each coarse weight to each refined neighbor, ignoring line Jacobians of the refined faces.**\n\nThis option is flawed for several reasons:\n- A single Gauss-Legendre quadrature rule over the entire interval $\\xi \\in [-1,1]$ is inappropriate. The trace of the solution from the refined side is discontinuous at the T-junction (corresponding to $\\xi=0$), as it comes from two different elements, $K_{f,1}$ and $K_{f,2}$. Standard Gaussian quadrature is only optimal for smooth (specifically, polynomial) integrands and will yield inaccurate results for functions with discontinuities. A composite rule is required.\n- The proposed affine map $\\eta = \\frac{1}{2}\\xi \\pm \\frac{1}{2}$ is incorrect. This map transforms the entire coarse interval $\\xi \\in [-1,1]$ into a half-interval (e.g., $[-1,0]$ or $[0,1]$). The correct mapping should transform a half-interval of the coarse face into the full interval of the refined face, such as the mappings provided in options B and D.\n- The prescription to \"assign half of each coarse weight to each refined neighbor\" is an arbitrary and non-rigorous procedure that is not guaranteed to produce a conservative or accurate scheme.\n\n**Verdict:** Incorrect.\n\n**Option B: Define a mortar parameter $\\xi \\in [-1,1]$ on the coarse face, partition it into two subintervals, $\\xi \\in [-1,0]$ and $\\xi \\in [0,1]$, and use Gauss–Legendre quadrature independently on each subinterval. Map each subinterval to the corresponding refined face by the affine relations ... Evaluate a single numerical flux $\\widehat{\\mathbf F}$ at common physical points using $\\mathbf n_c(\\xi)$, and accumulate contributions to all three elements using the coarse line Jacobian $J_c(\\xi)$ and quadrature weights of the mortar subintervals; do not perform any projection of traces to a common interface space even if $p_c \\neq p_{f,i}$.**\n\nThis option correctly identifies several key aspects for handling the $h$-nonconformity:\n- It correctly uses the coarse face as the mortar and partitions the integration domain, using a composite quadrature rule.\n- The provided affine mappings, $\\eta_{(1)} = 2\\xi + 1$ for $\\xi \\in [-1,0]$ and $\\eta_{(2)} = 2\\xi - 1$ for $\\xi \\in [0,1]$, are the standard, correct mappings to relate the sub-intervals of the mortar face to the full reference faces of the refined elements.\n- It enforces conservation by using common quadrature points and evaluating a single numerical flux.\n\nHowever, the final clause, \"do not perform any projection of traces to a common interface space even if $p_c \\neq p_{f,i}$\", represents a significant deficiency. In the context of $hp$-adaptivity, where polynomial degrees can vary arbitrarily, comparing traces from different polynomial spaces without projection is not robust. It complicates the mathematical analysis and can lead to suboptimal convergence and stability issues. While such a \"collocation-type\" flux evaluation can be made conservative, it is not the standard or preferred method for high-order DG schemes, as it fails to properly reconcile the different function spaces. The robust and generally accepted \"correct\" procedure for $hp$-DG involves projection.\n\n**Verdict:** Incorrect. While it correctly handles conservation and $h$-nonconformity, its handling of $p$-nonconformity is suboptimal and deviates from the standard, robust mortar element methodology.\n\n**Option C: Compute two separate flux integrals: one on each refined face, using Gauss–Legendre quadrature on $\\eta \\in [-1,1]$ for $K_{f,1}$ and $K_{f,2}$, respectively. For the coarse element contribution, integrate independently on $\\xi \\in [-1,1]$ with its own quadrature. Evaluate $\\widehat{\\mathbf F}$ at the distinct quadrature nodes and sum the three integrals, relying on the fact that the union of refined faces equals the coarse face; do not enforce common quadrature nodes, weights, or normals across the interface.**\n\nThis approach is fundamentally flawed because it violates the principle of discrete conservation. By using three independent sets of quadrature points for the three elements involved ($K_c$, $K_{f,1}$, $K_{f,2}$), there is no guarantee that the numerically integrated flux leaving $K_c$ will equal the sum of the fluxes entering $K_{f,1}$ and $K_{f,2}$. The discrete flux balance is not enforced at the quadrature level. This will lead to a non-conservative scheme, which is unacceptable for methods designed to solve conservation laws.\n\n**Verdict:** Incorrect.\n\n**Option D: Construct a mortar interface space on $\\Gamma$ with parameter $\\xi \\in [-1,1]$ and polynomial degree $p_m = \\max\\{p_c, p_{f,1}, p_{f,2}\\}$. Partition the mortar into the two subsegments $\\xi \\in [-1,0]$ and $\\xi \\in [0,1]$, and on each subsegment select a Gauss–Legendre quadrature rule with enough points to integrate polynomials up to degree $p_m$ on that subsegment. Map the mortar nodes to each refined face via ... and evaluate a single numerical flux $\\widehat{\\mathbf F}$ at the common physical points with consistent normals... Project both coarse and refined trace fields onto the mortar space on their respective subsegments before flux evaluation... Assemble the interface contribution once using the physical line measure $J(\\xi)$ and distribute equal-and-opposite contributions...**\n\nThis option describes the state-of-the-art mortar method for handling $hp$-nonconforming interfaces in DG. It correctly incorporates all the necessary principles:\n- **Mortar approach:** It correctly identifies the coarse face as the mortar and uses a composite quadrature rule respecting the T-junction.\n- **Conservative assembly:** It uses a single set of quadrature points and a single flux evaluation at each point, with equal-and-opposite contributions to the neighboring elements, thus guaranteeing discrete conservation.\n- **Consistent geometry and normals:** It uses the correct affine mappings and explicitly states that normals should be consistent ($\\mathbf n_{f,i} = -\\mathbf n_c$).\n- **Robustness for $p$-nonconformity:** Crucially, it mandates the projection of all traces onto a common mortar space. The choice of $p_m = \\max\\{p_c, p_{f,1}, p_{f,2}\\}$ for the mortar space degree is standard practice to avoid loss of information from the higher-order element. This projection is the key to a stable and optimally convergent scheme in the presence of varying polynomial degrees.\nThe phrase \"integrate polynomials up to degree $p_m$\" can be slightly ambiguous regarding the exact number of quadrature points, but in the context of describing the overall procedure, it correctly conveys the intent of using a sufficiently accurate quadrature rule matched to the polynomial space. The rest of the procedure is described perfectly.\n\n**Verdict:** Correct.", "answer": "$$\\boxed{D}$$", "id": "3389932"}]}