{"hands_on_practices": [{"introduction": "Our first exercise is a thought experiment designed to probe the fundamental limits of polynomial approximation. We will analyze a specially constructed function with sharp, smooth boundary layers whose width shrinks in proportion to $p^{-2}$, where $p$ is the polynomial degree. This problem [@problem_id:3392308] challenges the intuition that high-degree polynomial interpolation on good nodes always converges, revealing a scenario where the $L^\\infty$ error remains stubbornly large, a consequence beautifully explained by the Markov inequality.", "problem": "Consider a single spectral element on the interval $[-1,1]$ as used in a spectral Discontinuous Galerkin (DG) method. For each polynomial degree $p \\in \\mathbb{N}$, define the $p$-dependent, infinitely differentiable function $f_p : [-1,1] \\to \\mathbb{R}$ by\n$$\nf_p(x) \\;=\\; W\\!\\left(\\frac{1-|x|}{\\delta_p}\\right), \\quad \\delta_p \\;=\\; \\alpha\\,p^{-2}, \\quad \\alpha \\in (0,1),\n$$\nwhere $W : \\mathbb{R} \\to [0,1]$ is the smooth step constructed from the standard $C^\\infty$ mollifier $\\eta(t) = e^{-1/t}$ for $t>0$ and $\\eta(t)=0$ for $t \\le 0$ via\n$$\nS(t) \\;=\\; \\frac{\\eta(t)}{\\eta(t)+\\eta(1-t)}, \\quad W(t) \\;=\\; S(1-t).\n$$\nBy construction, $W(t) = 1$ for $t \\le 0$, $W(t) = 0$ for $t \\ge 1$, and $W$ transitions smoothly on $(0,1)$. Hence $f_p$ has two boundary layers of width $O(p^{-2})$ adjacent to $x=\\pm 1$, is equal to $1$ at the endpoints, and decays smoothly to $0$ over a distance $\\delta_p$ into the interior.\n\nLet $\\mathbb{P}_p$ denote polynomials of degree at most $p$ on $[-1,1]$. Denote by $I_p f_p \\in \\mathbb{P}_p$ the degree-$p$ Lagrange interpolant of $f_p$ at the Legendre–Gauss–Lobatto (LGL) nodes, and by $\\Pi_p f_p \\in \\mathbb{P}_p$ the $L^2([-1,1])$-orthogonal projection of $f_p$ onto $\\mathbb{P}_p$ (as used in a modal DG formulation). Let $\\Lambda_p$ be the Lebesgue constant for Lagrange interpolation at LGL nodes, that is, the operator norm of $I_p : C([-1,1]) \\to C([-1,1])$ in $L^\\infty$, equivalently $\\Lambda_p = \\sup_{x \\in [-1,1]} \\sum_{j=0}^p |\\ell_j(x)|$, where $\\{\\ell_j\\}_{j=0}^p$ are the LGL Lagrange basis polynomials.\n\nStarting from the following foundational definitions and facts:\n- The Lebesgue constant $\\Lambda_p$ measures the amplification of data perturbations by the interpolation operator in the $L^\\infty$ norm, and for LGL nodes one has $\\Lambda_p = \\Theta(\\log p)$ as $p \\to \\infty$.\n- The mean-value theorem and the Markov inequality on $[-1,1]$, namely for any $q \\in \\mathbb{P}_p$, $\\|q'\\|_{L^\\infty(-1,1)} \\le p^2 \\|q\\|_{L^\\infty(-1,1)}$.\n- The $L^2$ projection $\\Pi_p$ minimizes the $L^2$ error over $\\mathbb{P}_p$: $\\|f_p - \\Pi_p f_p\\|_{L^2(-1,1)} = \\inf_{q \\in \\mathbb{P}_p} \\|f_p - q\\|_{L^2(-1,1)}$.\n\nAnalyze how the Lebesgue constant $\\Lambda_p$ influences the $L^\\infty$ interpolation error versus the $L^2$ projection error for the family $\\{f_p\\}$, and select all statements that are correct. Your reasoning must carefully relate the boundary-layer width $O(p^{-2})$, the Markov inequality, and the operator norms involved.\n\nOptions:\nA. There exist constants $C, C' > 0$ independent of $p$ such that\n$$\n\\|f_p - I_p f_p\\|_{L^\\infty(-1,1)} \\;\\le\\; C\\,(1+\\Lambda_p)\n\\quad\\text{and}\\quad\n\\|f_p - \\Pi_p f_p\\|_{L^2(-1,1)} \\;\\le\\; C'\\,p^{-1}.\n$$\nB. Because the LGL nodes cluster near the endpoints and include $x=\\pm 1$, one has $\\|f_p - I_p f_p\\|_{L^\\infty(-1,1)} \\to 0$ as $p \\to \\infty$, independently of the size of $\\Lambda_p$.\nC. If $\\delta_p = \\alpha p^{-2}$ with $\\alpha \\in (0,1)$, then for every $p$ and every $q \\in \\mathbb{P}_p$,\n$$\n\\|f_p - q\\|_{L^\\infty(-1,1)} \\;\\ge\\; \\frac{1-\\alpha}{2+\\alpha},\n$$\nand hence in particular $\\|f_p - I_p f_p\\|_{L^\\infty(-1,1)} \\ge \\dfrac{1-\\alpha}{2+\\alpha}$ for all $p$.\nD. The $L^2$ projection error cannot decay faster than $O(p^{-1/2})$ as $p \\to \\infty$ because the thin boundary layers impose a square-root barrier in $L^2$.\nE. The Lebesgue constant $\\Lambda_p$ multiplies the $L^2$ projection error bound as well, so that $\\|f_p - \\Pi_p f_p\\|_{L^2(-1,1)} = O(\\Lambda_p\\,p^{-1})$ for LGL nodes.", "solution": "The problem statement is a well-posed theoretical question in the field of numerical analysis, specifically concerning spectral and discontinuous Galerkin methods. All definitions and provided facts are standard within this discipline. The function family $\\{f_p\\}_{p \\in \\mathbb{N}}$ is constructed as a test case to probe the resolution limits of polynomial approximations, a common practice in the analysis of numerical methods. No scientific or mathematical flaws, ambiguities, or contradictions are present. The problem is valid.\n\nWe proceed with the analysis of the approximation errors.\n\nThe function $f_p: [-1,1] \\to \\mathbb{R}$ is defined as $f_p(x) = W\\left(\\frac{1-|x|}{\\delta_p}\\right)$, where $\\delta_p = \\alpha p^{-2}$ for some constant $\\alpha \\in (0,1)$. The function $W(t)$ is a smooth transition from $1$ to $0$ on the interval $[0,1]$.\nLet's analyze the behavior of $f_p(x)$ for $x \\in [0,1]$. Here, $f_p(x) = W\\left(\\frac{1-x}{\\delta_p}\\right)$.\n- If $x=1$, the argument is $0$, so $f_p(1) = W(0) = 1$.\n- If $x=1-\\delta_p$, the argument is $1$, so $f_p(1-\\delta_p) = W(1) = 0$.\n- If $x < 1-\\delta_p$, the argument $\\frac{1-x}{\\delta_p} > 1$, so $f_p(x) = W(\\dots) = 0$.\nDue to the symmetry $f_p(x) = f_p(-x)$, the function $f_p$ is non-zero only in two boundary layer regions: $[-1, -1+\\delta_p]$ and $[1-\\delta_p, 1]$. The width of these layers is $\\delta_p = \\alpha p^{-2}$. The function represents a signal that is mostly zero but has sharp, smooth spikes at the boundaries. The core of the problem is to determine how well polynomials of degree $p$ can approximate a function whose features have a characteristic width of $O(p^{-2})$.\n\n**Analysis of Option C**\n\nThis option claims that for any polynomial $q \\in \\mathbb{P}_p$, the uniform error $\\|f_p - q\\|_{L^\\infty(-1,1)}$ is bounded below by a positive constant that depends on $\\alpha$. This suggests that polynomial approximation in the $L^\\infty$ norm fails to converge for this family of functions.\n\nLet $q \\in \\mathbb{P}_p$ be an arbitrary polynomial of degree at most $p$. Let $\\epsilon = \\|f_p - q\\|_{L^\\infty(-1,1)}$. By definition, $|f_p(x) - q(x)| \\le \\epsilon$ for all $x \\in [-1,1]$.\nAt $x=1$, we have $f_p(1) = 1$. Thus, $|1 - q(1)| \\le \\epsilon$, which implies $q(1) \\ge 1-\\epsilon$.\nAt $x=1-\\delta_p$, we have $f_p(1-\\delta_p) = 0$. Thus, $|0 - q(1-\\delta_p)| \\le \\epsilon$, which implies $q(1-\\delta_p) \\le \\epsilon$.\n\nBy the Mean Value Theorem, there exists a point $\\xi \\in (1-\\delta_p, 1)$ such that\n$$\nq'(\\xi) = \\frac{q(1) - q(1-\\delta_p)}{1 - (1-\\delta_p)} = \\frac{q(1) - q(1-\\delta_p)}{\\delta_p}\n$$\nUsing the inequalities for $q(1)$ and $q(1-\\delta_p)$, we can establish a lower bound for $q'(\\xi)$:\n$$\nq'(\\xi) \\ge \\frac{(1-\\epsilon) - \\epsilon}{\\delta_p} = \\frac{1-2\\epsilon}{\\delta_p}\n$$\nThis implies that the maximum value of the derivative must satisfy $\\|q'\\|_{L^\\infty(-1,1)} \\ge \\frac{1-2\\epsilon}{\\delta_p}$, assuming $1-2\\epsilon \\ge 0$.\n\nNow we invoke the Markov inequality for polynomials on $[-1,1]$, which states $\\|q'\\|_{L^\\infty(-1,1)} \\le p^2 \\|q\\|_{L^\\infty(-1,1)}$.\nCombining these results, we get:\n$$\n\\frac{1-2\\epsilon}{\\delta_p} \\le \\|q'\\|_{L^\\infty(-1,1)} \\le p^2 \\|q\\|_{L^\\infty(-1,1)}\n$$\nSubstituting $\\delta_p = \\alpha p^{-2}$:\n$$\n\\frac{1-2\\epsilon}{\\alpha p^{-2}} \\le p^2 \\|q\\|_{L^\\infty(-1,1)} \\implies \\frac{1-2\\epsilon}{\\alpha} \\le \\|q\\|_{L^\\infty(-1,1)}\n$$\nWe can also bound $\\|q\\|_{L^\\infty(-1,1)}$ in terms of $\\epsilon$. By the triangle inequality:\n$\\|q\\|_{L^\\infty} = \\|q - f_p + f_p\\|_{L^\\infty} \\le \\|q - f_p\\|_{L^\\infty} + \\|f_p\\|_{L^\\infty} = \\epsilon + 1$, since $\\|f_p\\|_{L^\\infty}=1$.\n\nPlugging this back into our inequality:\n$$\n\\frac{1-2\\epsilon}{\\alpha} \\le 1+\\epsilon\n$$\n$$\n1-2\\epsilon \\le \\alpha(1+\\epsilon) = \\alpha + \\alpha\\epsilon\n$$\n$$\n1-\\alpha \\le 2\\epsilon + \\alpha\\epsilon = \\epsilon(2+\\alpha)\n$$\nSince $\\alpha \\in (0,1)$, $2+\\alpha > 0$. We can divide to find a lower bound for $\\epsilon$:\n$$\n\\epsilon \\ge \\frac{1-\\alpha}{2+\\alpha}\n$$\nSince this holds for any $q \\in \\mathbb{P}_p$, it holds for the best approximant. Specifically, for the Lagrange interpolant $I_p f_p \\in \\mathbb{P}_p$:\n$$\n\\|f_p - I_p f_p\\|_{L^\\infty(-1,1)} \\ge \\inf_{q \\in \\mathbb{P}_p} \\|f_p - q\\|_{L^\\infty(-1,1)} \\ge \\frac{1-\\alpha}{2+\\alpha}\n$$\nSince $\\alpha \\in (0,1)$, this lower bound is a positive constant. The statement in option C is therefore correct. This result fundamentally stems from the fact that a polynomial of degree $p$ cannot change arbitrarily fast; the rate of change is limited by the Markov inequality. The function $f_p$ is constructed with boundary layers of width $O(p^{-2})$, which is the critical scaling that pushes this limit.\n\n**Verdict for C: Correct.**\n\n**Analysis of Option B**\n\nThis option states that $\\|f_p - I_p f_p\\|_{L^\\infty(-1,1)} \\to 0$ as $p \\to \\infty$. Our analysis for Option C shows that this error is bounded below by the positive constant $\\frac{1-\\alpha}{2+\\alpha}$. Therefore, the error cannot converge to $0$. The reasoning provided in the option, concerning the clustering of LGL nodes, is insufficient to guarantee convergence for this type of function. While the interpolant $I_p f_p$ will match $f_p$ at the nodes, the function $f_p$ is too steep between the nodes for the polynomial to follow without large oscillations (a manifestation of the Gibbs phenomenon), preventing uniform convergence.\n\n**Verdict for B: Incorrect.**\n\n**Analysis of Option A**\n\nThis option presents two inequalities.\n1.  $\\|f_p - I_p f_p\\|_{L^\\infty(-1,1)} \\le C\\,(1+\\Lambda_p)$:\nThe standard bound for Lagrange interpolation error is\n$$\n\\|f - I_p f\\|_{L^\\infty} \\le (1+\\Lambda_p) \\inf_{q \\in \\mathbb{P}_p} \\|f - q\\|_{L^\\infty}\n$$\nFor our function $f_p$, we can find a simple bound for the best approximation error. Let $q(x)=0$ for all $x$. Then $q \\in \\mathbb{P}_p$, and\n$$\n\\inf_{q \\in \\mathbb{P}_p} \\|f_p - q\\|_{L^\\infty} \\le \\|f_p - 0\\|_{L^\\infty} = \\|f_p\\|_{L^\\infty} = 1\n$$\nTherefore, $\\|f_p - I_p f_p\\|_{L^\\infty} \\le (1+\\Lambda_p) \\cdot 1$. This inequality holds with $C=1$. So, the first part of the statement is a valid, albeit not necessarily sharp, upper bound.\n\n2.  $\\|f_p - \\Pi_p f_p\\|_{L^2(-1,1)} \\le C'\\,p^{-1}$:\nThe $L^2$ projection $\\Pi_p f_p$ is the best approximant to $f_p$ from $\\mathbb{P}_p$ in the $L^2$ norm. Thus, for any $q \\in \\mathbb{P}_p$:\n$$\n\\|f_p - \\Pi_p f_p\\|_{L^2} \\le \\|f_p - q\\|_{L^2}\n$$\nLet's again choose the trivial approximant $q(x)=0$. Then the error is bounded by the $L^2$ norm of $f_p$ itself:\n$$\n\\|f_p - \\Pi_p f_p\\|_{L^2} \\le \\|f_p\\|_{L^2} = \\left(\\int_{-1}^1 |f_p(x)|^2 dx\\right)^{1/2}\n$$\nThe support of $f_p(x)$ is the union of two intervals of length $\\delta_p$. Due to symmetry, and since $|f_p(x)| \\le 1$:\n$$\n\\|f_p\\|_{L^2}^2 = 2 \\int_{1-\\delta_p}^1 |f_p(x)|^2 dx \\le 2 \\int_{1-\\delta_p}^1 1^2 dx = 2 \\delta_p\n$$\nSubstituting $\\delta_p = \\alpha p^{-2}$:\n$$\n\\|f_p\\|_{L^2}^2 \\le 2\\alpha p^{-2} \\implies \\|f_p\\|_{L^2} \\le \\sqrt{2\\alpha}\\,p^{-1}\n$$\nTherefore, $\\|f_p - \\Pi_p f_p\\|_{L^2} \\le \\sqrt{2\\alpha}\\,p^{-1}$. This inequality shows that the $L^2$ projection error is indeed bounded by $C'p^{-1}$ with $C'=\\sqrt{2\\alpha}$.\n\nBoth inequalities in Option A are correct statements.\n\n**Verdict for A: Correct.**\n\n**Analysis of Option D**\n\nThis option claims the $L^2$ projection error cannot decay faster than $O(p^{-1/2})$. Our analysis for Option A showed that the error is bounded by $O(p^{-1})$. Since the exponent $-1 < -1/2$, a decay rate of $O(p^{-1})$ is faster than $O(p^{-1/2})$. Thus, the statement is false. The reference to a \"square-root barrier\" is a misapplication of a concept that applies to functions with fixed jump discontinuities. For such a function $g$, it is true that $\\|g - \\Pi_p g\\|_{L^2}$ decays as $O(p^{-1/2})$. However, our function $f_p$ is infinitely differentiable, and the region where it varies rapidly is shrinking with $p$. This shrinking support is key to the faster $O(p^{-1})$ convergence rate in the $L^2$ norm.\n\n**Verdict for D: Incorrect.**\n\n**Analysis of Option E**\n\nThis option claims that the Lebesgue constant $\\Lambda_p$ is a factor in the $L^2$ projection error bound. This is fundamentally incorrect. The Lebesgue constant $\\Lambda_p$ is the operator norm of the Lagrange interpolation operator $I_p$ in the $L^\\infty$ setting: $\\Lambda_p = \\|I_p\\|_{C \\to C}$. The $L^2$ projection operator $\\Pi_p$ is, by definition, an orthogonal projection in the Hilbert space $L^2([-1,1])$. As such, its operator norm is $\\|\\Pi_p\\|_{L^2 \\to L^2} = 1$, and it is the optimal linear projector in this norm. The error $\\|f - \\Pi_p f\\|_{L^2}$ is determined by the best approximation properties of the subspace $\\mathbb{P}_p$ in the $L^2$ norm, i.e., $\\|f - \\Pi_p f\\|_{L^2} = \\inf_{q \\in \\mathbb{P}_p} \\|f-q\\|_{L^2}$. The Lebesgue constant $\\Lambda_p$ plays no role in this definition or its standard bounds. The statement incorrectly conflates the stability properties of $L^\\infty$ interpolation with those of $L^2$ projection.\n\n**Verdict for E: Incorrect.**", "answer": "$$\\boxed{AC}$$", "id": "3392308"}, {"introduction": "Having explored the conceptual limits of interpolation, we now ground our understanding with a concrete calculation. This exercise [@problem_id:3392355] requires you to compute the leading-order error introduced by approximating the geometry of a curvilinear element, a common practice in spectral and Discontinuous Galerkin methods. By working through a low-degree example from first principles, you will calculate the Lebesgue constant $\\Lambda_{2}$ and see precisely how it amplifies the so-called 'geometric aliasing' error.", "problem": "Consider a one-dimensional curvilinear spectral element on the reference interval $[-1,1]$ with a smooth, orientation-preserving mapping $x(\\xi)$ given by $x(\\xi) = \\xi + \\alpha \\,\\xi^{4}$, where $\\alpha \\in \\mathbb{R}$ satisfies $|\\alpha| < \\tfrac{1}{4}$ to ensure a positive Jacobian. Let the Jacobian (metric term) be $J(\\xi) = \\dfrac{dx}{d\\xi} = 1 + 4\\alpha\\,\\xi^{3}$. Suppose a nodal polynomial approximation of degree $p=2$ is used on the Legendre–Gauss–Lobatto (LGL) nodes. Define the Lebesgue constant $\\Lambda_{p}$ as the $L^{\\infty}$ norm of the Lebesgue function associated with the LGL nodes of degree $p$, and define the geometric aliasing bound as $E_{\\mathrm{geo}} := \\|J - I_{p}J\\|_{L^{\\infty}([-1,1])}$, where $I_{p}$ denotes the degree-$p$ Lagrange interpolation operator on the LGL nodes.\n\nIn a Discontinuous Galerkin (DG) or nodal spectral collocation formulation that multiplies nodal fields by interpolated metric terms prior to differentiation, the leading-order geometry-induced perturbation of the nodal field can be upper-bounded by the product $\\Lambda_{p}\\,E_{\\mathrm{geo}}$. Working at degree $p=2$ on the LGL nodes and using only the definitions above together with elementary properties of polynomials on $[-1,1]$, compute the closed-form expression for\n$$\nC(\\alpha) \\;:=\\; \\Lambda_{2}\\,\\big\\|J - I_{2}J\\big\\|_{L^{\\infty}([-1,1])}\n$$\nas an explicit function of $\\alpha$.\n\nYour final answer must be a single simplified analytic expression for $C(\\alpha)$ with no inequality signs. No rounding is required. Express your final answer without units.", "solution": "The problem requires the computation of the quantity $C(\\alpha) = \\Lambda_{2} \\|J - I_{2}J\\|_{L^{\\infty}([-1,1])}$, where $J(\\xi) = 1 + 4\\alpha\\xi^3$ is the Jacobian of a coordinate transformation, $I_{2}$ is the degree-$2$ Lagrange interpolation operator on the Legendre–Gauss–Lobatto (LGL) nodes, and $\\Lambda_{2}$ is the corresponding Lebesgue constant.\n\nThe computation can be divided into two main parts: the determination of the Lebesgue constant $\\Lambda_{2}$, and the calculation of the interpolation error norm $\\|J - I_{2}J\\|_{L^{\\infty}([-1,1])}$.\n\nPart 1: Calculation of the Lebesgue Constant $\\Lambda_{2}$.\n\nThe LGL nodes of degree $p$ are the roots of the equation $(1-\\xi^2)P'_{p}(\\xi) = 0$, where $P_p(\\xi)$ is the Legendre polynomial of degree $p$. For $p=2$, the Legendre polynomial is $P_2(\\xi) = \\frac{1}{2}(3\\xi^2 - 1)$, and its derivative is $P'_2(\\xi) = 3\\xi$. The LGL nodes for $p=2$ are the roots of $(1-\\xi^2)(3\\xi) = 0$, which are $\\xi_0 = -1$, $\\xi_1 = 0$, and $\\xi_2 = 1$.\n\nThe Lagrange basis polynomials $\\ell_j(\\xi)$ for these nodes are:\n$\\ell_0(\\xi) = \\frac{(\\xi - \\xi_1)(\\xi - \\xi_2)}{(\\xi_0 - \\xi_1)(\\xi_0 - \\xi_2)} = \\frac{(\\xi - 0)(\\xi - 1)}{(-1 - 0)(-1 - 1)} = \\frac{\\xi(\\xi-1)}{2} = \\frac{1}{2}(\\xi^2 - \\xi)$.\n$\\ell_1(\\xi) = \\frac{(\\xi - \\xi_0)(\\xi - \\xi_2)}{(\\xi_1 - \\xi_0)(\\xi_1 - \\xi_2)} = \\frac{(\\xi - (-1))(\\xi - 1)}{(0 - (-1))(0 - 1)} = \\frac{(\\xi+1)(\\xi-1)}{-1} = 1 - \\xi^2$.\n$\\ell_2(\\xi) = \\frac{(\\xi - \\xi_0)(\\xi - \\xi_1)}{(\\xi_2 - \\xi_0)(\\xi_2 - \\xi_1)} = \\frac{(\\xi - (-1))(\\xi - 0)}{(1 - (-1))(1 - 0)} = \\frac{\\xi(\\xi+1)}{2} = \\frac{1}{2}(\\xi^2 + \\xi)$.\n\nThe Lebesgue function is defined as $L_2(\\xi) = \\sum_{j=0}^{2} |\\ell_j(\\xi)| = |\\frac{1}{2}(\\xi^2 - \\xi)| + |1 - \\xi^2| + |\\frac{1}{2}(\\xi^2 + \\xi)|$.\nThe Lebesgue constant is $\\Lambda_2 = \\max_{\\xi \\in [-1,1]} L_2(\\xi)$.\nOn the interval $[-1,1]$, the term $1-\\xi^2$ is always non-negative. We analyze the function by splitting the domain.\n\nFor $\\xi \\in [0,1]$:\n$\\xi^2 - \\xi = \\xi(\\xi-1) \\le 0$, so $|\\xi^2 - \\xi| = -(\\xi^2 - \\xi) = \\xi - \\xi^2$.\n$\\xi^2 + \\xi = \\xi(\\xi+1) \\ge 0$, so $|\\xi^2 + \\xi| = \\xi^2 + \\xi$.\n$L_2(\\xi) = \\frac{1}{2}(\\xi - \\xi^2) + (1 - \\xi^2) + \\frac{1}{2}(\\xi^2 + \\xi) = \\frac{1}{2}\\xi - \\frac{1}{2}\\xi^2 + 1 - \\xi^2 + \\frac{1}{2}\\xi^2 + \\frac{1}{2}\\xi = 1 + \\xi - \\xi^2$.\nTo find the maximum of $f(\\xi) = 1 + \\xi - \\xi^2$ on $[0,1]$, we compute its derivative: $f'(\\xi) = 1 - 2\\xi$. Setting $f'(\\xi) = 0$ yields $\\xi = 1/2$. The value at this critical point is $f(1/2) = 1 + 1/2 - (1/2)^2 = 3/2 - 1/4 = 5/4$. At the endpoints, $f(0) = 1$ and $f(1) = 1$. The maximum value on $[0,1]$ is $5/4$.\n\nFor $\\xi \\in [-1,0]$:\n$\\xi^2 - \\xi = \\xi(\\xi-1) \\ge 0$, so $|\\xi^2 - \\xi| = \\xi^2 - \\xi$.\n$\\xi^2 + \\xi = \\xi(\\xi+1) \\le 0$, so $|\\xi^2 + \\xi| = -(\\xi^2 + \\xi)$.\n$L_2(\\xi) = \\frac{1}{2}(\\xi^2 - \\xi) + (1 - \\xi^2) - \\frac{1}{2}(\\xi^2 + \\xi) = \\frac{1}{2}\\xi^2 - \\frac{1}{2}\\xi + 1 - \\xi^2 - \\frac{1}{2}\\xi^2 - \\frac{1}{2}\\xi = 1 - \\xi - \\xi^2$.\nTo find the maximum of $g(\\xi) = 1 - \\xi - \\xi^2$ on $[-1,0]$, we compute its derivative: $g'(\\xi) = -1 - 2\\xi$. Setting $g'(\\xi) = 0$ yields $\\xi = -1/2$. The value is $g(-1/2) = 1 - (-1/2) - (-1/2)^2 = 1 + 1/2 - 1/4 = 5/4$. At the endpoints, $g(-1) = 1$ and $g(0) = 1$. The maximum value on $[-1,0]$ is $5/4$.\n\nCombining the results from both intervals, the maximum value of $L_2(\\xi)$ on $[-1,1]$ is $\\Lambda_2 = 5/4$.\n\nPart 2: Calculation of the Interpolation Error Norm $\\|J - I_{2}J\\|_{L^{\\infty}([-1,1])}$.\n\nThe function to be interpolated is the Jacobian $J(\\xi) = 1 + 4\\alpha\\xi^3$. The operator $I_2$ projects $J(\\xi)$ onto the space of polynomials of degree at most $2$ by interpolating at the nodes $\\xi_0 = -1$, $\\xi_1 = 0$, $\\xi_2 = 1$.\nThe values of $J(\\xi)$ at these nodes are:\n$J(-1) = 1 + 4\\alpha(-1)^3 = 1 - 4\\alpha$.\n$J(0) = 1 + 4\\alpha(0)^3 = 1$.\n$J(1) = 1 + 4\\alpha(1)^3 = 1 + 4\\alpha$.\n\nThe interpolating polynomial $I_2J(\\xi)$ can be constructed using the Lagrange basis:\n$I_2J(\\xi) = J(-1)\\ell_0(\\xi) + J(0)\\ell_1(\\xi) + J(1)\\ell_2(\\xi)$\n$I_2J(\\xi) = (1 - 4\\alpha)\\frac{\\xi^2 - \\xi}{2} + (1)(1 - \\xi^2) + (1 + 4\\alpha)\\frac{\\xi^2 + \\xi}{2}$\n$I_2J(\\xi) = \\frac{1}{2}(\\xi^2 - \\xi - 4\\alpha\\xi^2 + 4\\alpha\\xi) + 1 - \\xi^2 + \\frac{1}{2}(\\xi^2 + \\xi + 4\\alpha\\xi^2 + 4\\alpha\\xi)$\n$I_2J(\\xi) = (\\frac{1}{2} - 1 + \\frac{1}{2})\\xi^2 + (-\\frac{1}{2} + \\frac{1}{2})\\xi + (\\frac{-4\\alpha}{2} + \\frac{4\\alpha}{2})\\xi^2 + (\\frac{4\\alpha}{2} + \\frac{4\\alpha}{2})\\xi + 1$\n$I_2J(\\xi) = 0 \\cdot \\xi^2 + 0 \\cdot \\xi + 0 \\cdot \\xi^2 + 4\\alpha\\xi + 1 = 1 + 4\\alpha\\xi$.\n\nThe interpolation error is $E(\\xi) = J(\\xi) - I_2J(\\xi) = (1 + 4\\alpha\\xi^3) - (1 + 4\\alpha\\xi) = 4\\alpha(\\xi^3 - \\xi)$.\nWe must find the $L^{\\infty}$ norm of this error on $[-1,1]$:\n$\\|J - I_{2}J\\|_{L^{\\infty}([-1,1])} = \\max_{\\xi \\in [-1,1]} |4\\alpha(\\xi^3 - \\xi)| = |4\\alpha| \\max_{\\xi \\in [-1,1]} |\\xi^3 - \\xi|$.\n\nLet $h(\\xi) = \\xi^3 - \\xi$. To find its maximum absolute value on $[-1,1]$, we find its critical points: $h'(\\xi) = 3\\xi^2 - 1$. Setting $h'(\\xi)=0$ gives $\\xi^2 = 1/3$, so $\\xi = \\pm 1/\\sqrt{3}$.\nWe evaluate $|h(\\xi)|$ at these points and at the endpoints $\\pm 1$:\n$|h(1)| = |1^3 - 1| = 0$.\n$|h(-1)| = |(-1)^3 - (-1)| = 0$.\n$|h(1/\\sqrt{3})| = |(1/\\sqrt{3})^3 - 1/\\sqrt{3}| = |\\frac{1}{3\\sqrt{3}} - \\frac{1}{\\sqrt{3}}| = |\\frac{1-3}{3\\sqrt{3}}| = \\frac{2}{3\\sqrt{3}}$.\n$|h(-1/\\sqrt{3})| = |(-1/\\sqrt{3})^3 - (-1/\\sqrt{3})| = |-\\frac{1}{3\\sqrt{3}} + \\frac{1}{\\sqrt{3}}| = |\\frac{-1+3}{3\\sqrt{3}}| = \\frac{2}{3\\sqrt{3}}$.\nThe maximum value of $|\\xi^3 - \\xi|$ on $[-1,1]$ is $2/(3\\sqrt{3})$.\n\nTherefore, the geometric aliasing bound is:\n$E_{\\mathrm{geo}} = \\|J - I_2J\\|_{L^{\\infty}([-1,1])} = |4\\alpha| \\frac{2}{3\\sqrt{3}} = \\frac{8|\\alpha|}{3\\sqrt{3}}$.\n\nPart 3: Final Computation of $C(\\alpha)$.\n\nWe now combine the results from Part 1 and Part 2:\n$C(\\alpha) = \\Lambda_2 \\cdot E_{\\mathrm{geo}} = \\frac{5}{4} \\cdot \\frac{8|\\alpha|}{3\\sqrt{3}} = \\frac{40|\\alpha|}{12\\sqrt{3}} = \\frac{10|\\alpha|}{3\\sqrt{3}}$.\nTo rationalize the denominator, we multiply the numerator and denominator by $\\sqrt{3}$:\n$C(\\alpha) = \\frac{10|\\alpha|}{3\\sqrt{3}} \\cdot \\frac{\\sqrt{3}}{\\sqrt{3}} = \\frac{10\\sqrt{3}|\\alpha|}{9}$.\n\nThis is the final closed-form expression for $C(\\alpha)$. The constraint $|\\alpha| < 1/4$ ensures the mapping is well-defined but does not alter the form of this expression.", "answer": "$$\n\\boxed{\\frac{10\\sqrt{3}}{9}|\\alpha|}\n$$", "id": "3392355"}, {"introduction": "In our final practice, we transition from analysis to synthesis by building a practical tool for adaptive simulations. This problem [@problem_id:3392318] asks you to implement an a posteriori error indicator that uses the local Lebesgue function as a 'stability monitor' to make intelligent, element-wise decisions. Your algorithm will decide whether it is more effective to increase the polynomial degree or to improve the nodal distribution, providing a direct application of the Lebesgue constant in driving efficient and accurate numerical methods.", "problem": "Consider a one-dimensional reference interval $[-1,1]$ and its affine images (elements) $K=[a,b]$ in the physical domain. For an element $K$ equipped with $N+1$ interpolation nodes $\\{x_i\\}_{i=0}^{N}$ and the associated Lagrange basis functions $\\{\\ell_i(x)\\}_{i=0}^{N}$, define the local Lebesgue monitor $\\mu_K(x)=\\sum_{i=0}^N \\lvert \\ell_i(x)\\rvert$ and the local Lebesgue constant $\\Lambda_K=\\sup_{x\\in K}\\mu_K(x)$. Let $I_N$ denote the nodal Lagrange interpolation operator of degree $N$ on the element $K$ for a target function $f$. In a spectral element or Discontinuous Galerkin setting, the interpolation operator $I_N$ acts locally on each element and the Lebesgue constant controls the operator norm of $I_N$ in the maximum norm.\n\nYou will implement the following a posteriori indicator for hp-adaptation that incorporates the local Lebesgue monitor. For an element $K$ that currently uses equidistant nodes of degree $N$:\n- Define the hierarchical surplus $s_K(x)=I_{N+1}^{\\mathrm{eq}}f(x)-I_{N}^{\\mathrm{eq}}f(x)$, the difference of two successive equidistant-node interpolants.\n- Define the weighted indicator $\\eta_K=\\sup_{x\\in K}\\mu_K^{\\mathrm{eq},N}(x)\\,\\lvert s_K(x)\\rvert$, where $\\mu_K^{\\mathrm{eq},N}$ is the Lebesgue monitor computed with the current equidistant nodes of degree $N$.\n- Define the node-sensitivity ratio $R_K=\\Lambda_K^{\\mathrm{eq},N}/\\Lambda_K^{\\mathrm{GLL},N}$, where $\\Lambda_K^{\\mathrm{eq},N}=\\sup_{x\\in K}\\mu_K^{\\mathrm{eq},N}(x)$ is the Lebesgue constant for equidistant nodes and $\\Lambda_K^{\\mathrm{GLL},N}=\\sup_{x\\in K}\\mu_K^{\\mathrm{GLL},N}(x)$ is the Lebesgue constant for Gauss–Lobatto–Legendre nodes of the same degree $N$.\n\nYou will adopt the following local adaptation decision rule. Given fixed thresholds $\\gamma&gt;1$ and $\\tau&gt;0$:\n- If $R_K\\ge \\gamma$ and $\\eta_K\\ge \\tau$, then recommend node redistribution (switch to Gauss–Lobatto–Legendre nodes) at the same degree $N$.\n- Else if $\\eta_K\\ge \\tau$, recommend $p$-refinement (increase degree from $N$ to $N+2$) while keeping equidistant nodes.\n- Else recommend no change.\n\nTo evaluate the ability of the Lebesgue monitor to predict regions needing node redistribution, define the following ground-truth classification for each element that currently uses equidistant nodes of degree $N$. Let $\\alpha&gt;1$ be a fixed factor. Compute the actual interpolation errors in the maximum norm:\n- $E_{\\mathrm{GLL},N}(K)=\\Vert f-I_{N}^{\\mathrm{GLL}}f\\Vert_{L^\\infty(K)}$,\n- $E_{\\mathrm{eq},N+2}(K)=\\Vert f-I_{N+2}^{\\mathrm{eq}}f\\Vert_{L^\\infty(K)}$.\nClassify the element as truly needing node redistribution if $E_{\\mathrm{GLL},N}(K)\\le E_{\\mathrm{eq},N+2}(K)/\\alpha$; otherwise classify it as not needing node redistribution (where $p$-refinement is at least as effective).\n\nYour program must:\n- Implement barycentric Lagrange interpolation to compute $I_N f$ for arbitrary nodes, and compute the Lebesgue monitor $\\mu_K(x)$ and constant $\\Lambda_K$ via dense sampling.\n- For each test case, apply the decision rule elementwise when the element uses equidistant nodes, and produce a predicted set of elements recommended for node redistribution.\n- Compute the ground-truth classification elementwise for equidistant-node elements using the error comparison stated above.\n- Return a boolean per test case indicating whether, for all elements in the case, the predicted recommendations for node redistribution match the ground-truth classification. For elements that already use Gauss–Lobatto–Legendre nodes, the prediction must be “no redistribution,” and these elements are excluded from ground-truth comparison.\n\nAdopt the following test suite with thresholds $\\gamma=1.6$, $\\tau=10^{-4}$, and $\\alpha=1.2$. All norms and evaluations are computed on $4001$ equispaced sample points per element:\n1. Single-element “Runge” case:\n   - Domain: $[-1,1]$,\n   - Elements: one element $K_1=[-1,1]$ using equidistant nodes, degree $N=12$,\n   - Function: $f(x)=\\dfrac{1}{1+25x^2}$.\n2. Two-element endpoint-layer case:\n   - Domain: $[-1,1]$,\n   - Elements: $K_1=[-1,-0.2]$ and $K_2=[-0.2,1]$, both using equidistant nodes, degree $N=10$,\n   - Function: $f(x)=\\sqrt{x+1.000001}$.\n3. Single-element already Gauss–Lobatto–Legendre case:\n   - Domain: $[-1,1]$,\n   - Elements: one element $K_1=[-1,1]$ using Gauss–Lobatto–Legendre nodes, degree $N=10$,\n   - Function: $f(x)=\\cos(20x)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., “[result1,result2,result3]”), where each result is a boolean corresponding to the success condition for each test case in the order listed above. No physical units or angles are involved.", "solution": "The problem asks for the implementation and evaluation of an a posteriori indicator for `hp`-adaptation in the context of spectral and Discontinuous Galerkin methods. The indicator is designed to decide between `p`-refinement (increasing the polynomial degree) and node redistribution (switching from equidistant to Gauss-Lobatto-Legendre nodes) on a per-element basis. This decision is then compared against a ground-truth classification based on the actual reduction in interpolation error.\n\nThe solution requires the implementation of several core concepts from numerical approximation theory, which will be detailed step-by-step.\n\n**1. Foundational Concepts and Numerical Tools**\n\nAt the heart of spectral and Discontinuous Galerkin methods lies polynomial interpolation on individual elements. The quality of this interpolation is paramount to the accuracy of the overall numerical solution.\n\n**1.1. Nodal Sets**\nThe choice of interpolation nodes $\\{x_i\\}_{i=0}^N$ on an element $K=[a,b]$ profoundly affects the stability and accuracy of the interpolation. The problem considers two types of nodes.\n\n- **Equidistant Nodes**: These are the most intuitive choice, defined on an element $[a,b]$ as:\n$$x_j = a + j \\frac{b-a}{N}, \\quad j = 0, 1, \\dots, N$$\nWhile simple, they are known to lead to disastrously large errors and oscillations near the boundaries for high polynomial degrees $N$, a phenomenon named after Runge.\n\n- **Gauss-Lobatto-Legendre (GLL) Nodes**: These nodes are derived from the theory of orthogonal polynomials and are designed to minimize interpolation error. On the reference interval $\\hat{K}=[-1,1]$, the $N+1$ GLL nodes $\\{\\hat{x}_j\\}_{j=0}^N$ are the roots of the polynomial $(1-\\xi^2)P'_N(\\xi)$, where $P_N(\\xi)$ is the Legendre polynomial of degree $N$. These nodes include the endpoints $\\pm 1$ and are clustered towards them, which mitigates the Runge phenomenon. These reference nodes are then mapped to a physical element $K=[a,b]$ via the affine transformation:\n$$x_j = \\frac{a+b}{2} + \\frac{b-a}{2}\\hat{x}_j$$\n\n**1.2. Barycentric Lagrange Interpolation**\nGiven a set of $N+1$ nodes $\\{x_j\\}$ and corresponding function values $\\{y_j=f(x_j)\\}$, the Lagrange interpolating polynomial $I_N f(x)$ can be evaluated efficiently and stably using the barycentric formula:\n$$I_N f(x) = \\frac{\\sum_{j=0}^{N} \\frac{w_j}{x-x_j} y_j}{\\sum_{j=0}^{N} \\frac{w_j}{x-x_j}}$$\nwhere the barycentric weights $w_j$ are pre-computed for the given set of nodes:\n$$w_j = \\frac{1}{\\prod_{k=0, k \\neq j}^{N} (x_j - x_k)}$$\nThis formula is valid for any $x$ that is not a node. If $x=x_k$ for some node $k$, the interpolated value is simply $y_k$.\n\n**1.3. The Lebesgue Monitor and Constant**\nThe stability of the interpolation process is quantified by the Lebesgue constant, $\\Lambda_K$. It is the operator norm of the interpolation operator $I_N$ in the maximum norm, mapping $C(K) \\to C(K)$. It is defined via the Lebesgue function, or monitor, $\\mu_K(x)$:\n$$\\mu_K(x) = \\sum_{i=0}^{N} |\\ell_i(x)|$$\nwhere $\\{\\ell_i(x)\\}$ are the Lagrange basis polynomials. The Lebesgue constant is the maximum of this function over the element:\n$$\\Lambda_K = \\sup_{x \\in K} \\mu_K(x)$$\nA small Lebesgue constant ensures that the interpolant $I_N f$ is not much larger than the best possible polynomial approximation of $f$. The interpolation error is bounded by:\n$$\\|f - I_N f\\|_{L^\\infty(K)} \\le (1 + \\Lambda_K) \\inf_{p \\in \\mathcal{P}_N} \\|f - p\\|_{L^\\infty(K)}$$\nwhere $\\mathcal{P}_N$ is the space of polynomials of degree at most $N$. For equidistant nodes, $\\Lambda_K^{\\mathrm{eq},N}$ grows exponentially with $N$ ($\\sim 2^N/N\\log N$), whereas for GLL nodes, $\\Lambda_K^{\\mathrm{GLL},N}$ grows only logarithmically ($\\sim \\log N$).\n\nThe Lebesgue monitor $\\mu_K(x)$ can be computed using the barycentric weights. The individual Lagrange basis function $\\ell_j(x)$ can be expressed as:\n$$\\ell_j(x) = \\frac{\\frac{w_j}{x-x_j}}{\\sum_{k=0}^{N} \\frac{w_k}{x-x_k}}$$\nSumming the absolute values of these expressions for $j=0, \\dots, N$ gives $\\mu_K(x)$.\n\n**2. A Posteriori Adaptation Indicator**\nThe problem defines a set of rules to decide whether an element $K$ using equidistant nodes would benefit more from node redistribution or from `p`-refinement. This decision is based on three quantities.\n\n- **Hierarchical Surplus, $s_K(x)$**: This quantity, $s_K(x) = I_{N+1}^{\\mathrm{eq}}f(x) - I_{N}^{\\mathrm{eq}}f(x)$, approximates the error of the current interpolant $I_{N}^{\\mathrm{eq}}f(x)$. It uses the difference between two successive interpolants in a hierarchy as a proxy for the true error, a common technique in a posteriori error estimation.\n\n- **Node-Sensitivity Ratio, $R_K$**: Defined as $R_K = \\Lambda_K^{\\mathrm{eq},N} / \\Lambda_K^{\\mathrm{GLL},N}$, this ratio directly measures the potential improvement in stability from switching to GLL nodes. A large $R_K$ indicates that the current equidistant nodes are poorly conditioned compared to GLL nodes.\n\n- **Weighted Indicator, $\\eta_K$**: This is defined as $\\eta_K = \\sup_{x \\in K} \\mu_K^{\\mathrm{eq},N}(x) |s_K(x)|$. It combines the estimated error (via $s_K$) with the local instability of the interpolation (via $\\mu_K^{\\mathrm{eq},N}$), providing a measure of the \"amplified\" error.\n\n**3. Decision Rule and Ground-Truth Classification**\nThe decision-making process is as follows, for given thresholds $\\gamma > 1$ and $\\tau > 0$:\n\n- **Prediction**:\n  1. If $R_K \\ge \\gamma$ and $\\eta_K \\ge \\tau$: Predict that **node redistribution** is the best strategy. The large sensitivity ratio $R_K$ suggests high potential gain from switching nodes, and the large indicator $\\eta_K$ suggests that some form of adaptation is necessary.\n  2. Else if $\\eta_K \\ge \\tau$: Predict `p`-refinement (increase degree to $N+2$).\n  3. Else: Predict no change.\n\n- **Ground Truth**: To validate the prediction, we compare the actual errors of the two competing strategies. For a factor $\\alpha > 1$:\n  1. Compute the error of `node redistribution`: $E_{\\mathrm{GLL},N}(K) = \\|f - I_{N}^{\\mathrm{GLL}}f\\|_{L^\\infty(K)}$.\n  2. Compute the error of `p`-refinement: $E_{\\mathrm{eq},N+2}(K) = \\|f - I_{N+2}^{\\mathrm{eq}}f\\|_{L^\\infty(K)}$.\n  3. Classify the element as truly needing **node redistribution** if the error reduction is significant enough, i.e., $E_{\\mathrm{GLL},N}(K) \\le E_{\\mathrm{eq},N+2}(K) / \\alpha$. Otherwise, it does not.\n\nThe final task is to verify, for each test case, whether the indicator's prediction for \"node redistribution\" matches the ground-truth classification for all elements that are initially using equidistant nodes. Elements already using GLL nodes are predicted to need \"no redistribution\" and are not part of the ground-truth comparison.\n\n**4. Implementation Strategy**\nThe solution is implemented by first constructing the required numerical tools in a vectorized manner for efficiency. The core functions are:\n- `get_equidistant_nodes` and `get_gll_nodes` for generating the nodal sets. GLL nodes are found using `scipy.special.legendre` to get the polynomial and its derivative's roots.\n- `barycentric_weights` for pre-computing the weights for a given nodal set.\n- `barycentric_interpolate` and `lebesgue_monitor` for evaluating the interpolant and Lebesgue monitor over a dense grid of points on each element.\nA main analysis function processes each element according to its initial node type. For equidistant elements, it computes the indicators $R_K$ and $\\eta_K$ to make a prediction, computes the ground-truth errors $E_{\\mathrm{GLL},N}$ and $E_{\\mathrm{eq},N+2}$ for classification, and compares the two. For GLL elements, the logic is trivial as per the problem statement. The results for each element in a test case are aggregated to produce a final boolean success flag for that case. All suprema (for $\\Lambda_K$, $\\eta_K$, and $L^\\infty$ errors) are approximated by taking the maximum over $4001$ equispaced points on the element.", "answer": "```python\nimport numpy as np\nfrom scipy.special import legendre\n\n# Define global constants from the problem description\nGAMMA = 1.6\nTAU = 1e-4\nALPHA = 1.2\nNUM_SAMPLES = 4001\n\ndef get_equidistant_nodes(N, a, b):\n    \"\"\"Generates N+1 equidistant nodes on the interval [a, b].\"\"\"\n    return np.linspace(a, b, N + 1)\n\ndef get_gll_nodes(N, a, b):\n    \"\"\"Generates N+1 Gauss-Lobatto-Legendre nodes on the interval [a, b].\"\"\"\n    if N == 0:\n        return np.array([(a+b)/2.0])\n    if N == 1:\n        return np.array([a, b])\n    \n    # Roots of the derivative of the N-th Legendre polynomial for interior nodes\n    p_N = legendre(N)\n    p_N_deriv = p_N.deriv(1)\n    interior_nodes_ref = p_N_deriv.roots\n    \n    # GLL nodes on the reference interval [-1, 1] include the endpoints\n    nodes_ref = np.concatenate(([-1.0], interior_nodes_ref, [1.0]))\n    nodes_ref.sort()\n    \n    # Affine mapping from [-1, 1] to [a, b]\n    return (b - a) / 2.0 * nodes_ref + (a + b) / 2.0\n\ndef barycentric_weights(nodes):\n    \"\"\"Computes barycentric weights for a given set of nodes.\"\"\"\n    N = len(nodes) - 1\n    weights = np.ones(N + 1)\n    for j in range(N + 1):\n        # Product over k != j of (xj - xk)\n        prod = np.prod([nodes[j] - nodes[k] for k in range(N + 1) if k != j])\n        weights[j] = 1.0 / prod\n    return weights\n\ndef barycentric_interpolate(nodes, f_values, weights, x_eval):\n    \"\"\"Evaluates the Lagrange interpolant using the barycentric formula.\"\"\"\n    y_interp = np.zeros_like(x_eval, dtype=float)\n    x_eval_b = x_eval[:, np.newaxis]\n    nodes_b = nodes[np.newaxis, :]\n    \n    # Find points in x_eval that are numerically close to a node\n    close_matrix = np.isclose(x_eval_b, nodes_b)\n    is_node_mask = np.any(close_matrix, axis=1)\n    \n    # Handle points that are not nodes via vectorized barycentric formula\n    if not np.all(is_node_mask):\n        non_node_x = x_eval[~is_node_mask]\n        diffs = non_node_x[:, np.newaxis] - nodes_b\n        terms = weights[np.newaxis, :] / diffs\n        numerator = np.sum(terms * f_values[np.newaxis, :], axis=1)\n        denominator = np.sum(terms, axis=1)\n        y_interp[~is_node_mask] = numerator / denominator\n\n    # Handle points that are nodes\n    if np.any(is_node_mask):\n        row_indices, col_indices = np.where(close_matrix)\n        y_interp[row_indices] = f_values[col_indices]\n\n    return y_interp\n\ndef lebesgue_monitor(nodes, weights, x_eval):\n    \"\"\"Computes the Lebesgue monitor function mu(x) = sum(|l_i(x)|).\"\"\"\n    mu_vals = np.zeros_like(x_eval, dtype=float)\n    x_eval_b = x_eval[:, np.newaxis]\n    nodes_b = nodes[np.newaxis, :]\n\n    close_matrix = np.isclose(x_eval_b, nodes_b)\n    is_node_mask = np.any(close_matrix, axis=1)\n\n    # Handle non-node points\n    if not np.all(is_node_mask):\n        non_node_x = x_eval[~is_node_mask]\n        diffs = non_node_x[:, np.newaxis] - nodes_b\n        terms = weights[np.newaxis, :] / diffs\n        denominator = np.sum(terms, axis=1, keepdims=True)\n        # individual li_vals for each non-node x\n        li_vals = terms / denominator\n        mu_vals[~is_node_mask] = np.sum(np.abs(li_vals), axis=1)\n\n    # At nodes, the Lebesgue function value is 1\n    mu_vals[is_node_mask] = 1.0\n    \n    return mu_vals\n\ndef analyze_element(element_params, f):\n    \"\"\"Analyzes a single element according to the problem rules.\"\"\"\n    a, b, N, node_type = element_params\n    \n    if node_type == 'GLL':\n        # Per problem, prediction is \"no redistribution\" and these are excluded\n        # from ground-truth comparison. This element passes by definition.\n        return True\n\n    x_eval = np.linspace(a, b, NUM_SAMPLES)\n    f_eval = f(x_eval)\n\n    # --- 1. Calculate indicators and make prediction ---\n    # Current setup (eq, N)\n    nodes_eq_N = get_equidistant_nodes(N, a, b)\n    weights_eq_N = barycentric_weights(nodes_eq_N)\n    \n    # Calculate mu_K^{eq, N}(x) and Lambda_K^{eq, N}\n    mu_K_eq_N_vals = lebesgue_monitor(nodes_eq_N, weights_eq_N, x_eval)\n    Lambda_K_eq_N = np.max(mu_K_eq_N_vals)\n\n    # Calculate s_K(x) = I_{N+1}^{eq}f - I_{N}^{eq}f\n    f_eq_N = f(nodes_eq_N)\n    I_N_eq_f = barycentric_interpolate(nodes_eq_N, f_eq_N, weights_eq_N, x_eval)\n    \n    nodes_eq_N_plus_1 = get_equidistant_nodes(N + 1, a, b)\n    weights_eq_N_plus_1 = barycentric_weights(nodes_eq_N_plus_1)\n    f_eq_N_plus_1 = f(nodes_eq_N_plus_1)\n    I_N_plus_1_eq_f = barycentric_interpolate(nodes_eq_N_plus_1, f_eq_N_plus_1, weights_eq_N_plus_1, x_eval)\n    s_K_vals = I_N_plus_1_eq_f - I_N_eq_f\n\n    # Calculate eta_K\n    eta_K = np.max(mu_K_eq_N_vals * np.abs(s_K_vals))\n\n    # Calculate R_K = Lambda_K^{eq,N} / Lambda_K^{GLL,N}\n    nodes_gll_N = get_gll_nodes(N, a, b)\n    weights_gll_N = barycentric_weights(nodes_gll_N)\n    mu_K_gll_N_vals = lebesgue_monitor(nodes_gll_N, weights_gll_N, x_eval)\n    Lambda_K_gll_N = np.max(mu_K_gll_N_vals)\n    R_K = Lambda_K_eq_N / Lambda_K_gll_N\n\n    # Apply decision rule\n    prediction_needs_redistribution = (R_K >= GAMMA and eta_K >= TAU)\n\n    # --- 2. Calculate ground truth classification ---\n    # E_GLL,N\n    f_gll_N = f(nodes_gll_N)\n    I_N_gll_f = barycentric_interpolate(nodes_gll_N, f_gll_N, weights_gll_N, x_eval)\n    E_GLL_N = np.max(np.abs(f_eval - I_N_gll_f))\n\n    # E_eq,N+2\n    nodes_eq_N_plus_2 = get_equidistant_nodes(N + 2, a, b)\n    weights_eq_N_plus_2 = barycentric_weights(nodes_eq_N_plus_2)\n    f_eq_N_plus_2 = f(nodes_eq_N_plus_2)\n    I_N_plus_2_eq_f = barycentric_interpolate(nodes_eq_N_plus_2, f_eq_N_plus_2, weights_eq_N_plus_2, x_eval)\n    E_eq_N_plus_2 = np.max(np.abs(f_eval - I_N_plus_2_eq_f))\n\n    # Ground truth classification\n    truth_needs_redistribution = (E_GLL_N <= E_eq_N_plus_2 / ALPHA)\n    \n    # --- 3. Compare prediction and truth ---\n    return prediction_needs_redistribution == truth_needs_redistribution\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    \n    test_cases = [\n        {\n            \"name\": \"Runge\",\n            \"function\": lambda x: 1.0 / (1.0 + 25.0 * x**2),\n            \"elements\": [\n                # K_1: [a, b], N, node_type\n                [-1.0, 1.0, 12, 'equidistant'],\n            ],\n        },\n        {\n            \"name\": \"Endpoint layer\",\n            \"function\": lambda x: np.sqrt(x + 1.000001),\n            \"elements\": [\n                [-1.0, -0.2, 10, 'equidistant'],\n                [-0.2, 1.0, 10, 'equidistant'],\n            ],\n        },\n        {\n            \"name\": \"Already GLL\",\n            \"function\": lambda x: np.cos(20.0 * x),\n            \"elements\": [\n                [-1.0, 1.0, 10, 'GLL'],\n            ],\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        case_match_success = True\n        for elem_params in case[\"elements\"]:\n            element_match = analyze_element(elem_params, case[\"function\"])\n            if not element_match:\n                case_match_success = False\n                break\n        results.append(case_match_success)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3392318"}]}