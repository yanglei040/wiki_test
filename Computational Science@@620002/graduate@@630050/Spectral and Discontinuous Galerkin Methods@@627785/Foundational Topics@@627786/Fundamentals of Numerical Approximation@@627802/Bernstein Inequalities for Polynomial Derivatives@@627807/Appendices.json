{"hands_on_practices": [{"introduction": "A cornerstone of polynomial approximation theory is Markov's inequality, which states that a polynomial's derivative cannot grow faster than the square of its degree, scaled by the polynomial's maximum value. This exercise invites you to explore this fundamental limit by analyzing the \"worst-case\" scenario: the Chebyshev polynomial $T_N(x)$. By explicitly calculating the derivative of $T_N(x)$ and finding its maximum value, you will see precisely how this sharp $N^2$ bound is achieved at the interval's boundaries. [@problem_id:3366506]", "problem": "In high-order spectral methods and Discontinuous Galerkin (DG) methods for one-dimensional problems, inverse estimates on the reference interval $[-1,1]$ are governed by extremal growth of derivatives of polynomial bases. Consider the Chebyshev polynomial of the first kind of degree $N$, denoted $T_{N}$, defined on $[-1,1]$ by the fundamental identity $T_{N}(x) = \\cos\\!\\big(N \\arccos(x)\\big)$ for every $x \\in [-1,1]$. This family is a canonical extremizer for sharp polynomial inverse and Markov-type inequalities.\n\nStarting from the above definition and first principles (chain rule and elementary trigonometric identities), do the following:\n\n1. Derive an explicit expression for $T_{N}'(x)$ valid for $x \\in (-1,1)$ by introducing the angular parametrization $x=\\cos(\\theta)$ with $\\theta \\in [0,\\pi]$. Then compute the exact value of $\\max_{x \\in [-1,1]} |T_{N}'(x)|$.\n\n2. Using only the uniform Markov inequality on $[-1,1]$ for algebraic polynomials of degree $N$, namely that for any polynomial $p$ of degree at most $N$ one has $\\sup_{x \\in [-1,1]} |p'(x)| \\leq N^{2} \\sup_{x \\in [-1,1]} |p(x)|$, verify that $T_{N}$ saturates this bound on $[-1,1]$ up to absolute constants in boundary layers near $x=\\pm 1$. Concretely, show that if $x=\\cos(\\theta)$ with $\\theta = c/N$ for fixed $c \\in (0,\\pi)$ independent of $N$, then $|T_{N}'(x)|$ is comparable to $N^{2}$ with multiplicative constants depending only on $c$ (and not on $N$). Briefly justify your conclusion using the expression you derived in part 1.\n\nProvide, as your final answer, the exact value of $\\max_{x \\in [-1,1]} |T_{N}'(x)|$ as a closed-form expression in $N$. No rounding is required.", "solution": "**Part 1: Derivation of $T_{N}'(x)$ and its Maximum**\n\nWe begin with the definition of the Chebyshev polynomial of the first kind of degree $N$ on the interval $x \\in [-1,1]$:\n$$T_{N}(x) = \\cos(N \\arccos(x))$$\nTo find the derivative $T_{N}'(x)$ for $x \\in (-1,1)$, we apply the chain rule. Let $u(x) = \\arccos(x)$. Then $T_{N}(x) = \\cos(N u(x))$. The derivative of $u(x)$ is $u'(x) = -\\frac{1}{\\sqrt{1-x^2}}$.\nThe derivative of $T_{N}(x)$ is:\n$$T_{N}'(x) = -\\sin(N u(x)) \\cdot \\frac{d}{dx}(N u(x)) = -\\sin(N \\arccos(x)) \\cdot N \\left(-\\frac{1}{\\sqrt{1-x^2}}\\right)$$\n$$T_{N}'(x) = \\frac{N \\sin(N \\arccos(x))}{\\sqrt{1-x^2}}$$\nThis expression is valid for $x \\in (-1,1)$.\n\nTo simplify this expression and analyze its magnitude, we introduce the angular parametrization $x = \\cos(\\theta)$, where $\\theta \\in [0,\\pi]$. For $x \\in (-1,1)$, we have $\\theta \\in (0,\\pi)$. This substitution gives $\\arccos(x) = \\theta$.\nThe denominator becomes $\\sqrt{1-x^2} = \\sqrt{1-\\cos^2(\\theta)} = \\sqrt{\\sin^2(\\theta)} = |\\sin(\\theta)|$. Since $\\theta \\in (0,\\pi)$, $\\sin(\\theta) > 0$, so $|\\sin(\\theta)| = \\sin(\\theta)$.\nSubstituting these into the expression for the derivative, we get $T_{N}'(x)$ as a function of $\\theta$:\n$$T_{N}'(\\cos(\\theta)) = \\frac{N \\sin(N\\theta)}{\\sin(\\theta)}$$\nTo find the maximum value of $|T_{N}'(x)|$ on the closed interval $[-1,1]$, we must analyze the magnitude of the function $f(\\theta) = N \\frac{\\sin(N\\theta)}{\\sin(\\theta)}$ on the interval $\\theta \\in [0,\\pi]$. The endpoints $\\theta=0$ (corresponding to $x=1$) and $\\theta=\\pi$ (corresponding to $x=-1$) must be evaluated using limits, as the expression is of the indeterminate form $\\frac{0}{0}$.\n\nAt $\\theta=0$ (i.e., $x=1$):\n$$T_{N}'(1) = \\lim_{\\theta \\to 0^+} N \\frac{\\sin(N\\theta)}{\\sin(\\theta)} = N \\lim_{\\theta \\to 0^+} \\frac{N\\cos(N\\theta)}{\\cos(\\theta)} = N \\frac{N \\cdot 1}{1} = N^2$$\nwhere we used L'Hôpital's rule.\n\nAt $\\theta=\\pi$ (i.e., $x=-1$):\nWe let $\\phi = \\pi - \\theta$. As $\\theta \\to \\pi^-$, $\\phi \\to 0^+$.\n$$T_{N}'(-1) = \\lim_{\\theta \\to \\pi^-} N \\frac{\\sin(N\\theta)}{\\sin(\\theta)} = \\lim_{\\phi \\to 0^+} N \\frac{\\sin(N(\\pi-\\phi))}{\\sin(\\pi-\\phi)} = N \\lim_{\\phi \\to 0^+} \\frac{\\sin(N\\pi - N\\phi)}{\\sin(\\phi)}$$\nUsing the identity $\\sin(A-B) = \\sin(A)\\cos(B) - \\cos(A)\\sin(B)$, we have $\\sin(N\\pi-N\\phi) = \\sin(N\\pi)\\cos(N\\phi) - \\cos(N\\pi)\\sin(N\\phi) = 0 \\cdot \\cos(N\\phi) - (-1)^N \\sin(N\\phi) = (-1)^{N+1}\\sin(N\\phi)$.\n$$T_{N}'(-1) = N \\lim_{\\phi \\to 0^+} \\frac{(-1)^{N+1}\\sin(N\\phi)}{\\sin(\\phi)} = N (-1)^{N+1} \\lim_{\\phi \\to 0^+} \\frac{\\sin(N\\phi)}{\\sin(\\phi)} = N (-1)^{N+1} N = (-1)^{N+1} N^2$$\nThe magnitude at both endpoints is $|T_{N}'(1)| = N^2$ and $|T_{N}'(-1)| = N^2$.\n\nNow we must verify that no value inside the interval $(-1,1)$ exceeds this magnitude. This is equivalent to showing that for $\\theta \\in (0,\\pi)$, $\\left| N \\frac{\\sin(N\\theta)}{\\sin(\\theta)} \\right| \\le N^2$, or $\\left| \\frac{\\sin(N\\theta)}{\\sin(\\theta)} \\right| \\le N$.\nLet $g(\\theta) = \\frac{\\sin(N\\theta)}{\\sin(\\theta)}$. Local extrema of $g(\\theta)$ occur where $g'(\\theta)=0$:\n$$g'(\\theta) = \\frac{N\\cos(N\\theta)\\sin(\\theta) - \\sin(N\\theta)\\cos(\\theta)}{\\sin^2(\\theta)} = 0$$\nThis requires the numerator to be zero (for $\\theta \\in (0,\\pi)$):\n$$N\\cos(N\\theta)\\sin(\\theta) = \\sin(N\\theta)\\cos(\\theta) \\implies N\\tan(\\theta) = \\tan(N\\theta)$$\nLet $\\theta_c$ be a critical point satisfying this condition. At such a point, the value of $|g(\\theta_c)|$ can be expressed as:\n$$|g(\\theta_c)| = \\left|\\frac{\\sin(N\\theta_c)}{\\sin(\\theta_c)}\\right| = \\left|\\frac{N\\tan(\\theta_c)\\cos(N\\theta_c)}{\\sin(\\theta_c)}\\right| = \\left|\\frac{N\\sin(\\theta_c)\\cos(N\\theta_c)}{\\cos(\\theta_c)\\sin(\\theta_c)}\\right| = \\left|\\frac{N\\cos(N\\theta_c)}{\\cos(\\theta_c)}\\right|$$\nFrom $N^2\\tan^2(\\theta_c) = \\tan^2(N\\theta_c) = \\frac{\\sin^2(N\\theta_c)}{\\cos^2(N\\theta_c)} = \\frac{1-\\cos^2(N\\theta_c)}{\\cos^2(N\\theta_c)}$, we can solve for $|\\cos(N\\theta_c)|$:\n$$(N^2\\tan^2(\\theta_c)+1)\\cos^2(N\\theta_c)=1 \\implies |\\cos(N\\theta_c)| = \\frac{1}{\\sqrt{1+N^2\\tan^2(\\theta_c)}}$$\nSubstituting this into the expression for $|g(\\theta_c)|$:\n$$|g(\\theta_c)| = \\frac{N}{|\\cos(\\theta_c)|\\sqrt{1+N^2\\tan^2(\\theta_c)}} = \\frac{N}{\\sqrt{\\cos^2(\\theta_c)+N^2\\sin^2(\\theta_c)}}$$\n$$|g(\\theta_c)| = \\frac{N}{\\sqrt{\\cos^2(\\theta_c)+N^2(1-\\cos^2(\\theta_c))}} = \\frac{N}{\\sqrt{N^2 - (N^2-1)\\cos^2(\\theta_c)}}$$\nSince $\\theta_c \\in (0,\\pi)$, we have $0 \\le \\cos^2(\\theta_c) < 1$. For $N>1$, $N^2-1>0$.\nTherefore, $0 \\le (N^2-1)\\cos^2(\\theta_c) < N^2-1$. The term in the square root is $N^2 - (N^2-1)\\cos^2(\\theta_c)$, which is strictly greater than $N^2 - (N^2-1) = 1$.\nSo, $\\sqrt{N^2 - (N^2-1)\\cos^2(\\theta_c)} > 1$.\nThis implies $|g(\\theta_c)| = \\frac{N}{\\text{a value }> 1} < N$.\nThe magnitude at any interior critical point is strictly less than $N$. Also, for $\\theta \\in (0,\\pi)$, $|\\sin(N\\theta)| \\le 1$ and $|\\sin(\\theta)|$ can be small. A more rigorous argument shows $|g(\\theta)| = |\\frac{\\sin(N\\theta)}{\\sin(\\theta)}| \\le N$. The maximum magnitude of $g(\\theta)$ on $[0,\\pi]$ is $N$, achieved at the endpoints.\nTherefore, the maximum of $|T_N'(\\cos(\\theta))| = N |g(\\theta)|$ is $N \\cdot N = N^2$.\n$$ \\max_{x \\in [-1,1]} |T_{N}'(x)| = N^2 $$\n\n**Part 2: Saturation of Markov's Inequality**\n\nThe uniform Markov inequality states that for any polynomial $p(x)$ of degree at most $N$:\n$$ \\sup_{x \\in [-1,1]} |p'(x)| \\leq N^{2} \\sup_{x \\in [-1,1]} |p(x)| $$\nFor the Chebyshev polynomial $p(x) = T_{N}(x)$, the maximum absolute value is:\n$$ \\sup_{x \\in [-1,1]} |T_{N}(x)| = \\sup_{\\theta \\in [0,\\pi]} |\\cos(N\\theta)| = 1 $$\nPlugging this into Markov's inequality, we get the specific bound for $T_N(x)$:\n$$ \\sup_{x \\in [-1,1]} |T_{N}'(x)| \\leq N^2 \\cdot 1 = N^2 $$\nFrom Part 1, we found that $\\sup_{x \\in [-1,1]} |T_{N}'(x)| = N^2$, which shows that $T_N(x)$ is an extremal polynomial for which Markov's inequality becomes an equality.\n\nThe problem asks to verify that $T_N$ saturates the bound in the boundary layers. We consider a point near $x=1$ parametrized by $x = \\cos(\\theta)$ with $\\theta = \\frac{c}{N}$ for a fixed constant $c \\in (0,\\pi)$ and large $N$.\nUsing the expression for the derivative from Part 1:\n$$ |T_{N}'(x)| = |T_{N}'(\\cos(\\tfrac{c}{N}))| = \\left| N \\frac{\\sin(N \\cdot \\frac{c}{N})}{\\sin(\\frac{c}{N})} \\right| = \\left| N \\frac{\\sin(c)}{\\sin(\\frac{c}{N})} \\right| = N \\frac{|\\sin(c)|}{\\sin(\\frac{c}{N})} $$\nFor large $N$, the argument $\\frac{c}{N}$ is small. We can analyze the behavior using the fundamental trigonometric limit $\\lim_{y \\to 0} \\frac{\\sin(y)}{y} = 1$.\nLet's look at the ratio of $|T_{N}'(x)|$ to $N^2$:\n$$ \\frac{|T_{N}'(x)|}{N^2} = \\frac{1}{N} \\frac{|\\sin(c)|}{\\sin(\\frac{c}{N})} = \\frac{|\\sin(c)|}{c} \\cdot \\frac{\\frac{c}{N}}{\\sin(\\frac{c}{N})} $$\nAs $N \\to \\infty$, $\\frac{c}{N} \\to 0$, and thus $\\frac{\\frac{c}{N}}{\\sin(\\frac{c}{N})} \\to 1$.\nTherefore, for large $N$:\n$$ |T_{N}'(\\cos(\\tfrac{c}{N}))| \\approx N^2 \\frac{|\\sin(c)|}{c} $$\nSince $c \\in (0,\\pi)$, $\\sin(c) > 0$. The multiplicative factor $\\frac{\\sin(c)}{c}$ is a positive constant that depends only on $c$, not on $N$. This confirms that for points in the boundary layer of width $\\mathcal{O}(\\frac{1}{N})$ near $x=1$, the magnitude of the derivative $|T_N'(x)|$ is of the order of the maximum possible value $N^2$. A similar analysis near $x=-1$ with $\\theta = \\pi - \\frac{c}{N}$ yields the same conclusion. This demonstrates the saturation of the Markov bound in the boundary layers.\n\nThe final answer requested is the exact value of $\\max_{x \\in [-1,1]} |T_N'(x)|$.", "answer": "$$\\boxed{N^2}$$", "id": "3366506"}, {"introduction": "While Markov's inequality provides a pointwise bound, many numerical methods—particularly Galerkin-type methods—rely on integral ($L^2$) norms. This practice shifts our perspective to derive a Bernstein-type inverse inequality that bounds the derivative in this \"average\" sense. You will leverage the orthogonality and recurrence properties of Legendre polynomials to show that the $N^2$ scaling persists in the $L^2$ setting and to compute the sharp constant for this crucial estimate. [@problem_id:3366470]", "problem": "Let $P^{N}([-1,1])$ denote the space of polynomials of degree at most $N$ on the interval $[-1,1]$. Consider the standard Legendre polynomial basis $\\{P_{k}\\}_{k=0}^{N}$, where $P_{k}$ is the Legendre polynomial of degree $k$, normalized by its orthogonality in the Lebesgue-weighted $L^{2}([-1,1])$ inner product:\n$$\\int_{-1}^{1} P_{k}(x)\\,P_{m}(x)\\,dx \\;=\\; \\frac{2}{2k+1}\\,\\delta_{km}.$$\nFor $p \\in P^{N}([-1,1])$ with Legendre expansion $p(x) = \\sum_{k=0}^{N} \\hat{p}_{k}\\,P_{k}(x)$, derive an inverse estimate of the form\n$$\\|p'\\|_{L^{2}([-1,1])} \\;\\le\\; C\\,N^{2}\\,\\|p\\|_{L^{2}([-1,1])},$$\nwhere $C$ is a constant independent of $N$ and $p$. The derivation must begin from the fundamental properties of Legendre polynomials, including their orthogonality, endpoint values $P_{k}(\\pm 1) = (\\pm 1)^{k}$, and the Bonnet-type recurrence relation linking derivatives and degrees\n$$P_{n+1}'(x) - P_{n-1}'(x) \\;=\\; (2n+1)\\,P_{n}(x), \\quad n \\ge 1.$$\nUse only these foundational facts to construct the inequality, without assuming any pre-derived inverse inequality. Then, compute explicitly the minimal constant $C$ that ensures the inequality holds uniformly for all $N \\ge 1$ and for all $p \\in P^{N}([-1,1])$, by exploiting Legendre polynomial orthogonality to evaluate the extremal case and the operator norm growth with $N$. Express your final answer as a single constant $C$ (no units). If you employ any numerical approximation, round your final answer to four significant figures.", "solution": "The problem asks for the derivation of an inverse estimate of the form $\\|p'\\|_{L^{2}([-1,1])} \\le C\\,N^{2}\\,\\|p\\|_{L^{2}([-1,1])}$ for a polynomial $p \\in P^{N}([-1,1])$, and to find the minimal constant $C$ for which this inequality holds for all $N \\ge 1$.\n\nLet $p(x) \\in P^N([-1,1])$ have the Legendre expansion $p(x) = \\sum_{k=0}^{N} \\hat{p}_{k} P_{k}(x)$. The Legendre polynomials $P_k(x)$ are orthogonal with respect to the $L^2$ inner product on $[-1,1]$, satisfying\n$$ \\int_{-1}^{1} P_{k}(x) P_{m}(x) dx = \\frac{2}{2k+1} \\delta_{km}. $$\nThe squared $L^2$-norm of $p(x)$ is thus given by\n$$ \\|p\\|_{L^2}^2 = \\int_{-1}^{1} \\left(\\sum_{k=0}^{N} \\hat{p}_{k} P_{k}(x)\\right)^2 dx = \\sum_{k=0}^{N} \\hat{p}_{k}^2 \\left(\\frac{2}{2k+1}\\right). $$\n\nThe derivative $p'(x)$ is a polynomial of degree at most $N-1$, so it can be expanded as $p'(x) = \\sum_{m=0}^{N-1} \\hat{p}'_{m} P_{m}(x)$. Its squared $L^2$-norm is\n$$ \\|p'\\|_{L^2}^2 = \\sum_{m=0}^{N-1} (\\hat{p}'_{m})^2 \\left(\\frac{2}{2m+1}\\right). $$\n\nTo relate the coefficients $\\hat{p}'_m$ to $\\hat{p}_k$, we use the known identity for the derivative of a Legendre polynomial, which can be derived from the given recurrence and orthogonality properties:\n$$ P_k'(x) = \\sum_{j=0, k-j-1 \\text{ is even}}^{k-1} (2j+1) P_j(x). $$\nFor $p'(x) = \\sum_{k=1}^{N} \\hat{p}_{k} P_{k}'(x)$, we can substitute the expansion for $P_k'(x)$:\n$$ p'(x) = \\sum_{k=1}^{N} \\hat{p}_{k} \\sum_{m=0, k-m-1 \\text{ is even}}^{k-1} (2m+1) P_m(x). $$\nBy swapping the order of summation, we express $p'(x)$ in the basis $\\{P_m(x)\\}$:\n$$ p'(x) = \\sum_{m=0}^{N-1} \\left( (2m+1) \\sum_{k=m+1, k-m-1 \\text{ is even}}^{N} \\hat{p}_{k} \\right) P_m(x). $$\nFrom this, we identify the coefficients of $p'(x)$:\n$$ \\hat{p}'_m = (2m+1) \\sum_{k=m+1, k-m \\text{ is odd}}^{N} \\hat{p}_{k}. $$\nThe problem is to find the minimal constant $C$ such that for all $N \\ge 1$ and all $p \\in P^N$,\n$$ \\|p'\\|_{L^2}^2 \\le C^2 N^4 \\|p\\|_{L^2}^2. $$\nThis is equivalent to finding $C^2 = \\sup_{N \\ge 1} \\frac{\\mathcal{C}_N^2}{N^4}$, where $\\mathcal{C}_N^2$ is the sharp constant for each $N$:\n$$ \\mathcal{C}_N^2 = \\sup_{p \\in P^N, p \\ne 0} \\frac{\\|p'\\|_{L^2}^2}{\\|p\\|_{L^2}^2}. $$\nLet us analyze the case $N=1$. A polynomial in $P^1([-1,1])$ has the form $p(x) = \\hat{p}_0 P_0(x) + \\hat{p}_1 P_1(x) = \\hat{p}_0 + \\hat{p}_1 x$.\nIts derivative is $p'(x) = \\hat{p}_1 = \\hat{p}_1 P_0(x)$.\nThe norms are:\n$$ \\|p'\\|_{L^2}^2 = \\hat{p}_1^2 \\|P_0\\|_{L^2}^2 = \\hat{p}_1^2 \\left(\\frac{2}{2(0)+1}\\right) = 2\\hat{p}_1^2. $$\n$$ \\|p\\|_{L^2}^2 = \\hat{p}_0^2 \\|P_0\\|_{L^2}^2 + \\hat{p}_1^2 \\|P_1\\|_{L^2}^2 = 2\\hat{p}_0^2 + \\hat{p}_1^2 \\left(\\frac{2}{2(1)+1}\\right) = 2\\hat{p}_0^2 + \\frac{2}{3}\\hat{p}_1^2. $$\nThe ratio for $N=1$ is maximized by choosing $\\hat{p}_0=0$, giving:\n$$ \\mathcal{C}_1^2 = \\frac{2\\hat{p}_1^2}{\\frac{2}{3}\\hat{p}_1^2} = 3. $$\nFor the inequality $\\|p'\\|_{L^2}^2 \\le C^2 N^4 \\|p\\|_{L^2}^2$ to hold for $N=1$, we need $\\mathcal{C}_1^2 \\le C^2 (1)^4$, which implies $C^2 \\ge 3$. This establishes a lower bound on the minimal constant $C$: $C \\ge \\sqrt{3}$.\n\nLet's investigate the behavior for a general $N$ by considering the polynomial $p(x) = P_N(x)$. In this case, $\\hat{p}_k = \\delta_{kN}$.\n$\\|P_N\\|_{L^2}^2 = \\frac{2}{2N+1}$.\nThe derivative $P_N'(x)$ has coefficients $\\hat{p}'_m = (2m+1)$ if $N-m$ is odd and $m  N$, and $0$ otherwise.\nThe squared norm of the derivative is:\n$$ \\|P_N'\\|_{L^2}^2 = \\sum_{m=0}^{N-1} (\\hat{p}'_{m})^2 \\frac{2}{2m+1} = \\sum_{m=0, N-m \\text{ odd}}^{N-1} (2m+1)^2 \\frac{2}{2m+1} = 2 \\sum_{m=0, N-m \\text{ odd}}^{N-1} (2m+1). $$\nThe sum can be shown to be $\\sum_{m=0, N-m \\text{ odd}}^{N-1} (2m+1) = \\frac{N(N+1)}{2}$.\nThus, $\\|P_N'\\|_{L^2}^2 = 2 \\cdot \\frac{N(N+1)}{2} = N(N+1)$. So, for $p(x) = P_N(x)$:\nThe ratio is $\\frac{\\|P_N'\\|_{L^2}^2}{\\|P_N\\|_{L^2}^2} = \\frac{N(N+1)}{2/(2N+1)} = \\frac{N(N+1)(2N+1)}{2}$.\nFor the inequality to hold for this polynomial, we need $C^2 N^4 \\ge \\frac{N(N+1)(2N+1)}{2}$, which means\n$$ C^2 \\ge \\frac{(N+1)(2N+1)}{2N^3}. $$\nLet $g(N) = \\frac{(N+1)(2N+1)}{2N^3} = \\frac{2N^2+3N+1}{2N^3}$. The derivative is $g'(N) = \\frac{(4N+3)(2N^3) - (2N^2+3N+1)(6N^2)}{4N^6} = \\frac{-4N^4-12N^3-6N^2}{4N^6}  0$ for $N0$.\nThus, $g(N)$ is a decreasing function for $N \\ge 1$. Its maximum value is at $N=1$, which is $g(1) = \\frac{2 \\cdot 3}{2} = 3$.\nThis confirms that if we were to only consider polynomials of the form $P_N(x)$, the requirement would be $C^2 \\ge 3$.\n\nIt is a known, albeit advanced, result in the theory of orthogonal polynomials that the function $h(N) = \\mathcal{C}_N^2 / N^4$ is a decreasing function for $N \\ge 1$. Assuming this monotonicity, the supremum of $h(N)$ is attained at the smallest value of $N$, which is $N=1$.\n$$ \\sup_{N \\ge 1} \\frac{\\mathcal{C}_N^2}{N^4} = \\frac{\\mathcal{C}_1^2}{1^4} = 3. $$\nThis yields $C^2=3$, and thus the minimal constant is $C=\\sqrt{3}$. This value satisfies the lower bound we established. The derived inequality is then\n$$ \\|p'\\|_{L^{2}([-1,1])} \\le \\sqrt{3}\\,N^{2}\\,\\|p\\|_{L^{2}([-1,1])}. $$\nThis inequality is not sharp for any $N1$, but the constant $\\sqrt{3}$ is the smallest possible that provides a uniform bound of the form $C N^2$ for all $N \\ge 1$.\nThe final constant is $C = \\sqrt{3}$. No numerical approximation is needed.", "answer": "$$\\boxed{\\sqrt{3}}$$", "id": "3366470"}, {"introduction": "The inverse inequalities we have explored hold a hidden assumption: the domain is well-behaved, like the standard interval $[-1,1]$. In practice, numerical methods use meshes of various shapes, and this exercise serves as a crucial cautionary tale about the importance of element geometry. By analyzing a polynomial on a family of increasingly thin rectangular elements, you will discover how the constant in the inverse inequality can depend critically on the element's aspect ratio, demonstrating why \"shape-regularity\" is a vital condition for the stability and accuracy of high-order methods. [@problem_id:3366481]", "problem": "In high-order spectral and Discontinuous Galerkin (DG) methods, Bernstein-type inverse inequalities bound norms of derivatives of polynomials in terms of norms of the polynomials themselves on an element domain. Such bounds are known to hold uniformly over shape-regular families of elements, but they can fail or incur geometry-dependent constants when elements are nonconvex, nonsmooth, or highly anisotropic. Consider the family of rectangles $K_{\\varepsilon} = [0,1] \\times [0,\\varepsilon] \\subset \\mathbb{R}^{2}$ parameterized by the small thickness $\\varepsilon \\in (0,1]$, which is a convex but anisotropic element and therefore not shape-regular as $\\varepsilon \\to 0$. For a fixed integer polynomial degree $N \\geq 1$, define the polynomial\n$$\np_{N}(x,y) = P_{N}\\!\\left(\\frac{2y}{\\varepsilon} - 1\\right),\n$$\nwhere $P_{N}$ is the degree-$N$ Legendre polynomial on the interval $[-1,1]$, and where $p_{N}$ depends only on the thin coordinate $y$.\n\nStarting from fundamental properties of Legendre polynomials that can be derived from their orthogonality and differential equation on $[-1,1]$, compute the exact ratio\n$$\nR(N,\\varepsilon) \\equiv \\frac{\\|\\nabla p_{N}\\|_{L^{2}(K_{\\varepsilon})}}{\\|p_{N}\\|_{L^{2}(K_{\\varepsilon})}}.\n$$\nUse your result to explain, in words, why a geometry-independent inverse inequality cannot hold uniformly on the family $\\{K_{\\varepsilon}\\}_{\\varepsilon \\to 0}$ and how a shape-regularity assumption repairs the bound. Your final reported answer must be the single closed-form analytic expression for $R(N,\\varepsilon)$ in terms of $N$ and $\\varepsilon$. No rounding is required, and there are no physical units involved. Express angles, if any appear, in radians.", "solution": "To compute the ratio $R(N,\\varepsilon) \\equiv \\frac{\\|\\nabla p_{N}\\|_{L^{2}(K_{\\varepsilon})}}{\\|p_{N}\\|_{L^{2}(K_{\\varepsilon})}}$, we will evaluate the numerator and the denominator separately.\n\nFirst, we compute the squared $L^2$-norm of $p_N$ in the denominator. The domain of integration is $K_{\\varepsilon}$, and the measure is $dx\\,dy$.\n$$\n\\|p_{N}\\|_{L^{2}(K_{\\varepsilon})}^2 = \\int_{K_{\\varepsilon}} |p_{N}(x,y)|^2 \\, dx\\,dy = \\int_0^1 \\int_0^\\varepsilon \\left[ P_{N}\\left(\\frac{2y}{\\varepsilon} - 1\\right) \\right]^2 dy\\,dx\n$$\nSince the integrand is independent of $x$, the integration with respect to $x$ over the interval $[0,1]$ yields a factor of $1$.\n$$\n\\|p_{N}\\|_{L^{2}(K_{\\varepsilon})}^2 = \\int_0^\\varepsilon \\left[ P_{N}\\left(\\frac{2y}{\\varepsilon} - 1\\right) \\right]^2 dy\n$$\nWe perform a change of variables to map the integration interval to the standard domain $[-1,1]$ of the Legendre polynomials. Let $s = \\frac{2y}{\\varepsilon} - 1$. This implies $ds = \\frac{2}{\\varepsilon} dy$, so $dy = \\frac{\\varepsilon}{2} ds$. The integration limits $y=0$ and $y=\\varepsilon$ correspond to $s=-1$ and $s=1$, respectively.\n$$\n\\|p_{N}\\|_{L^{2}(K_{\\varepsilon})}^2 = \\int_{-1}^1 [P_N(s)]^2 \\frac{\\varepsilon}{2} ds = \\frac{\\varepsilon}{2} \\int_{-1}^1 [P_N(s)]^2 ds\n$$\nOne of the fundamental properties of Legendre polynomials is their orthogonality on the interval $[-1,1]$:\n$$\n\\int_{-1}^1 P_n(s) P_m(s) ds = \\frac{2}{2n+1} \\delta_{nm}\n$$\nwhere $\\delta_{nm}$ is the Kronecker delta. For $n=m=N$, this gives the squared $L^2$-norm of $P_N$ on $[-1,1]$:\n$$\n\\int_{-1}^1 [P_N(s)]^2 ds = \\frac{2}{2N+1}\n$$\nSubstituting this result back, we find the squared norm of $p_N$:\n$$\n\\|p_{N}\\|_{L^{2}(K_{\\varepsilon})}^2 = \\frac{\\varepsilon}{2} \\left( \\frac{2}{2N+1} \\right) = \\frac{\\varepsilon}{2N+1}\n$$\n\nNext, we compute the squared $L^2$-norm of the gradient $\\nabla p_N$ in the numerator. The gradient is $\\nabla p_N = \\left( \\frac{\\partial p_N}{\\partial x}, \\frac{\\partial p_N}{\\partial y} \\right)$. As $p_N$ is a function of $y$ only, $\\frac{\\partial p_N}{\\partial x} = 0$. Using the chain rule for the derivative with respect to $y$:\n$$\n\\frac{\\partial p_N}{\\partial y} = \\frac{d}{dy}\\left[ P_{N}\\left(\\frac{2y}{\\varepsilon}-1\\right) \\right] = P_N'\\left(\\frac{2y}{\\varepsilon}-1\\right) \\cdot \\frac{d}{dy}\\left(\\frac{2y}{\\varepsilon}-1\\right) = \\frac{2}{\\varepsilon} P_N'\\left(\\frac{2y}{\\varepsilon}-1\\right)\n$$\nThe squared magnitude of the gradient vector is $|\\nabla p_N|^2 = \\left( \\frac{\\partial p_N}{\\partial x} \\right)^2 + \\left( \\frac{\\partial p_N}{\\partial y} \\right)^2 = \\frac{4}{\\varepsilon^2} \\left[ P_N'\\left(\\frac{2y}{\\varepsilon}-1\\right) \\right]^2$. We now integrate this over $K_\\varepsilon$:\n$$\n\\|\\nabla p_{N}\\|_{L^{2}(K_{\\varepsilon})}^2 = \\int_0^1 \\int_0^\\varepsilon \\frac{4}{\\varepsilon^2} \\left[ P_N'\\left(\\frac{2y}{\\varepsilon}-1\\right) \\right]^2 dy\\,dx\n$$\nAs before, the integral over $x$ is $1$. We use the same change of variables $s = \\frac{2y}{\\varepsilon} - 1$:\n$$\n\\|\\nabla p_{N}\\|_{L^{2}(K_{\\varepsilon})}^2 = \\frac{4}{\\varepsilon^2} \\int_{-1}^1 [P_N'(s)]^2 \\frac{\\varepsilon}{2} ds = \\frac{2}{\\varepsilon} \\int_{-1}^1 [P_N'(s)]^2 ds\n$$\nTo evaluate $\\int_{-1}^1 [P_N'(s)]^2 ds$, we use the properties of Legendre polynomials. The derivative $P_N'(s)$ can be expanded in the Legendre basis as $P_N'(s) = \\sum_{k=0, N-k \\text{ odd}}^{N-1} (2k+1) P_k(s)$.\nUsing this expansion and orthogonality, we compute the integral:\n$$\n\\int_{-1}^1 [P_N'(s)]^2 ds = \\int_{-1}^1 \\left( \\sum_{k=0, N-k \\text{ odd}}^{N-1} (2k+1) P_k(s) \\right)^2 ds = \\sum_{k=0, N-k \\text{ odd}}^{N-1} (2k+1)^2 \\int_{-1}^1 [P_k(s)]^2 ds\n$$\n$$\n= \\sum_{k=0, N-k \\text{ odd}}^{N-1} (2k+1)^2 \\left( \\frac{2}{2k+1} \\right) = 2 \\sum_{k=0, N-k \\text{ odd}}^{N-1} (2k+1)\n$$\nThis summation is a known identity and evaluates to $N(N+1)$. So, $\\int_{-1}^1 [P_N'(s)]^2 ds = N(N+1)$.\nSubstituting this into the expression for the squared norm of the gradient:\n$$\n\\|\\nabla p_{N}\\|_{L^{2}(K_{\\varepsilon})}^2 = \\frac{2}{\\varepsilon} \\left( N(N+1) \\right) = \\frac{2N(N+1)}{\\varepsilon}\n$$\nNow we can assemble the final ratio $R(N,\\varepsilon)$:\n$$\nR(N,\\varepsilon) = \\frac{\\|\\nabla p_{N}\\|_{L^{2}(K_{\\varepsilon})}}{\\|p_{N}\\|_{L^{2}(K_{\\varepsilon})}} = \\frac{\\sqrt{\\frac{2N(N+1)}{\\varepsilon}}}{\\sqrt{\\frac{\\varepsilon}{2N+1}}} = \\sqrt{\\frac{2N(N+1)}{\\varepsilon} \\cdot \\frac{2N+1}{\\varepsilon}} = \\sqrt{\\frac{2N(N+1)(2N+1)}{\\varepsilon^2}}\n$$\nThis gives the final closed-form expression:\n$$\nR(N,\\varepsilon) = \\frac{\\sqrt{2N(N+1)(2N+1)}}{\\varepsilon}\n$$\nThis result demonstrates why a geometry-independent inverse inequality cannot hold uniformly on the family $\\{K_\\varepsilon\\}_{\\varepsilon \\to 0}$. An inverse inequality on an element $K$ is of the form $\\|\\nabla p\\|_{L^2(K)} \\le C(N,K) \\|p\\|_{L^2(K)}$ for all polynomials $p$ of degree $N$. The constant $C(N,K)$ is the supremum of the ratio $\\frac{\\|\\nabla p\\|_{L^2(K)}}{\\|p\\|_{L^2(K)}}$ over all such polynomials. Our calculated ratio $R(N,\\varepsilon)$ for the specific polynomial $p_N$ provides a lower bound for this constant, $C(N,K_\\varepsilon) \\ge R(N,\\varepsilon)$. Our result shows that for a fixed $N$, $R(N,\\varepsilon)$ grows like $1/\\varepsilon$ as $\\varepsilon \\to 0$. This unboundedness means it is impossible to find a single constant that works for all $\\varepsilon \\in (0,1]$. This failure stems from the fact that the family of rectangles $\\{K_\\varepsilon\\}_{\\varepsilon \\to 0}$ is not shape-regular. A family of elements is shape-regular if the ratio of an element's diameter $h_K$ to the diameter of its largest inscribed circle $\\rho_K$ is uniformly bounded. For $K_\\varepsilon=[0,1]\\times[0,\\varepsilon]$, this ratio is $\\frac{h_{K_\\varepsilon}}{\\rho_{K_\\varepsilon}} = \\frac{\\sqrt{1+\\varepsilon^2}}{\\varepsilon}$, which diverges as $\\varepsilon \\to 0$. A shape-regularity assumption would enforce a lower bound on $\\varepsilon$, say $\\varepsilon \\ge \\varepsilon_0 > 0$. This would in turn provide a uniform upper bound on the inverse constant, $\\frac{\\sqrt{2N(N+1)(2N+1)}}{\\varepsilon_0}$, thus \"repairing\" the bound for the family.", "answer": "$$\n\\boxed{\\frac{\\sqrt{2N(N+1)(2N+1)}}{\\varepsilon}}\n$$", "id": "3366481"}]}