{"hands_on_practices": [{"introduction": "We begin with the canonical example of approximating $f(x)=|x|$, a function that is continuous but has a \"kink\" at the origin. This exercise demonstrates how a single point of non-smoothness dictates the global convergence rate for polynomial approximation. By analyzing the Chebyshev series coefficients, you will verify the optimal $O(N^{-1})$ convergence rate in the uniform norm, as predicted by Jackson's theorem for Lipschitz functions, and contrast it with the faster convergence in a weighted $L^2$ norm [@problem_id:3393562].", "problem": "Consider the polynomial approximation of the function $f(x)=|x|$ on the interval $[-1,1]$ within the setting of spectral and Discontinuous Galerkin (DG) methods. Let $\\{T_{k}(x)\\}_{k=0}^{\\infty}$ denote the Chebyshev polynomials of the first kind, which form an orthogonal system on $[-1,1]$ with respect to the weight $w(x)=(1-x^{2})^{-1/2}$, and satisfy the orthogonality relation $\\int_{-1}^{1} T_{j}(x)\\,T_{k}(x)\\,w(x)\\,dx=0$ for $j\\neq k$. The Chebyshev polynomial expansion of $f$ is $f(x)=\\frac{a_{0}}{2}+\\sum_{k=1}^{\\infty} a_{k}\\,T_{k}(x)$, where the Chebyshev coefficients $a_{k}$ are defined by the weighted projection formula consistent with the orthogonality of $\\{T_{k}\\}$.\n\nStarting from the orthogonality of Chebyshev polynomials, the change of variables $x=\\cos\\theta$, and fundamental Fourier-cosine integral identities, derive the large-$k$ asymptotics of the Chebyshev coefficients $a_{k}$ for $f(x)=|x|$, and use these asymptotics to:\n- justify the convergence rate of the truncated Chebyshev series in the weighted $L^{2}$ norm associated with $w(x)$;\n- explain, using the Nikolskii inequality (a standard result bounding the uniform norm of polynomials by a multiple of their weighted $L^{2}$ norm), why the uniform norm ($L^{\\infty}$) convergence is slower than the weighted $L^{2}$ convergence for this function, and connect this with the Jackson error estimate for Lipschitz functions.\n\nYour final answer must be the leading-order asymptotic expression for $a_{k}$ as a single closed-form analytic expression in $k$. No numerical rounding is required, and no units are involved.", "solution": "The problem is valid as it is scientifically grounded in the established theory of polynomial approximation, well-posed, objective, and self-contained.\n\nThe problem asks for an analysis of the Chebyshev polynomial approximation of the function $f(x)=|x|$ on the interval $[-1,1]$. This involves deriving the asymptotic form of the Chebyshev coefficients, and using them to analyze the convergence rates in the weighted $L^2$ and uniform norms.\n\nFirst, we derive the Chebyshev coefficients $a_k$ for $f(x)=|x|$. The function is expanded as $f(x) = \\frac{a_0}{2} + \\sum_{k=1}^{\\infty} a_k T_k(x)$. The coefficients are given by the projection formula, which, using the orthogonality of Chebyshev polynomials $\\{T_k(x)\\}$ with weight $w(x)=(1-x^2)^{-1/2}$, is:\n$$a_0 = \\frac{2}{\\pi} \\int_{-1}^{1} f(x) w(x) \\, dx$$\n$$a_k = \\frac{2}{\\pi} \\int_{-1}^{1} f(x) T_k(x) w(x) \\, dx \\quad \\text{for } k \\ge 1$$\n\nWe use the change of variables $x=\\cos\\theta$. For $x \\in [-1,1]$, $\\theta$ goes from $\\pi$ to $0$. So, $dx=-\\sin\\theta\\,d\\theta$. The weight function becomes $w(x)=(1-\\cos^2\\theta)^{-1/2} = 1/\\sin\\theta$ for $\\theta \\in (0,\\pi)$. The Chebyshev polynomials become $T_k(x) = T_k(\\cos\\theta) = \\cos(k\\theta)$. The function to be approximated becomes $g(\\theta) = f(\\cos\\theta) = |\\cos\\theta|$.\n\nThe integral for $a_k$ transforms to:\n$$a_k = \\frac{2}{\\pi} \\int_{\\pi}^{0} |\\cos\\theta| \\cos(k\\theta) \\frac{1}{\\sin\\theta} (-\\sin\\theta\\,d\\theta) = \\frac{2}{\\pi} \\int_{0}^{\\pi} |\\cos\\theta| \\cos(k\\theta) \\,d\\theta$$\n\nThe function $f(x)=|x|$ is an even function on $[-1,1]$. The Chebyshev polynomial $T_k(x)$ is an even function for even $k$ and an odd function for odd $k$. The product $f(x)T_k(x)$ is therefore an odd function for odd $k$. The integral of an odd function over the symmetric interval $[-1,1]$ is zero. Thus, $a_k=0$ for all odd $k$.\n\nWe only need to calculate $a_k$ for even $k$. Let $k=2m$ for an integer $m \\ge 0$. The integrand $|\\cos\\theta|\\cos(2m\\theta)$ is an even function about $\\theta=\\pi/2$. We can simplify the integral:\n$$a_{2m} = \\frac{2}{\\pi} \\int_{0}^{\\pi} |\\cos\\theta| \\cos(2m\\theta) \\,d\\theta = \\frac{4}{\\pi} \\int_{0}^{\\pi/2} |\\cos\\theta| \\cos(2m\\theta) \\,d\\theta$$\nOn the interval $[0, \\pi/2]$, $\\cos\\theta \\ge 0$, so $|\\cos\\theta|=\\cos\\theta$.\n$$a_{2m} = \\frac{4}{\\pi} \\int_{0}^{\\pi/2} \\cos\\theta \\cos(2m\\theta) \\,d\\theta$$\nUsing the product-to-sum identity $\\cos A \\cos B = \\frac{1}{2}[\\cos(A+B) + \\cos(A-B)]$:\n$$a_{2m} = \\frac{4}{\\pi} \\int_{0}^{\\pi/2} \\frac{1}{2}[\\cos((2m+1)\\theta) + \\cos((2m-1)\\theta)] \\,d\\theta$$\nFor $m=0$, we calculate $a_0$ separately:\n$$a_0 = \\frac{4}{\\pi} \\int_{0}^{\\pi/2} \\cos\\theta \\,d\\theta = \\frac{4}{\\pi} [\\sin\\theta]_0^{\\pi/2} = \\frac{4}{\\pi}$$\nFor $m \\ge 1$ (so $k \\ge 2$):\n$$a_{2m} = \\frac{2}{\\pi} \\left[ \\frac{\\sin((2m+1)\\theta)}{2m+1} + \\frac{\\sin((2m-1)\\theta)}{2m-1} \\right]_0^{\\pi/2}$$\n$$a_{2m} = \\frac{2}{\\pi} \\left( \\frac{\\sin((2m+1)\\pi/2)}{2m+1} + \\frac{\\sin((2m-1)\\pi/2)}{2m-1} \\right)$$\nWe use $\\sin((2m+1)\\pi/2) = \\sin(m\\pi+\\pi/2) = (-1)^m$ and $\\sin((2m-1)\\pi/2) = \\sin(m\\pi-\\pi/2) = -(-1)^m$.\n$$a_{2m} = \\frac{2}{\\pi} \\left( \\frac{(-1)^m}{2m+1} - \\frac{(-1)^m}{2m-1} \\right) = \\frac{2(-1)^m}{\\pi} \\left( \\frac{(2m-1) - (2m+1)}{(2m+1)(2m-1)} \\right)$$\n$$a_{2m} = \\frac{2(-1)^m}{\\pi} \\left( \\frac{-2}{4m^2-1} \\right) = \\frac{4(-1)^{m+1}}{\\pi(4m^2-1)}$$\nSubstituting $k=2m$, we get the exact expression for even $k \\ge 2$:\n$$a_k = \\frac{4(-1)^{k/2+1}}{\\pi(k^2-1)}$$\nFor large $k$, we have $k^2-1 \\sim k^2$. The leading-order asymptotic expression for even $k$ is therefore:\n$$a_k \\sim \\frac{4(-1)^{k/2+1}}{\\pi k^2}$$\nTo write a single expression valid for all large $k$, we note that for even $k$, $\\cos(k\\pi/2) = (-1)^{k/2}$, and for odd $k$, $\\cos(k\\pi/2)=0$. Also, $(-1)^{k/2+1} = -\\cos(k\\pi/2)$. Since $a_k=0$ for odd $k$, we can combine these results into a single asymptotic formula:\n$$a_k \\sim -\\frac{4\\cos(k\\pi/2)}{\\pi k^2}$$\nThis expression is the required leading-order asymptotic form for $a_k$.\n\nNow we analyze the convergence rates. Let $P_N f = \\frac{a_0}{2} + \\sum_{k=1}^N a_k T_k(x)$ be the truncated Chebyshev series (the degree-$N$ projection).\n\nThe error in the weighted $L^2$ norm, denoted $\\| \\cdot \\|_w$, is given by Parseval's identity:\n$$\\|f - P_N f\\|_w^2 = \\int_{-1}^{1} (f(x) - P_N f(x))^2 w(x) \\, dx = \\sum_{k=N+1}^{\\infty} a_k^2 \\int_{-1}^{1} T_k(x)^2 w(x) \\, dx$$\nThe normalization integrals are $\\int_{-1}^1 T_k^2 w dx = \\pi/2$ for $k \\ge 1$. Since $a_k=0$ for odd $k$, the sum contains only even indices. For large $N$, we assume $N$ is even, $N=2M$. The first non-zero term in the sum is for $k=N+2 = 2M+2$.\n$$\\|f - P_N f\\|_w^2 = \\frac{\\pi}{2} \\sum_{k=N+1, k \\text{ even}}^{\\infty} a_k^2 = \\frac{\\pi}{2} \\sum_{m=M+1}^{\\infty} a_{2m}^2$$\nUsing the asymptotic form $a_{2m} \\sim C/m^2$ (where $C$ is a constant), we get:\n$$\\|f - P_N f\\|_w^2 \\sim \\frac{\\pi}{2} \\sum_{m=M+1}^{\\infty} \\left(\\frac{4(-1)^{m+1}}{\\pi(4m^2-1)}\\right)^2 \\approx \\frac{8}{\\pi} \\sum_{m=M+1}^{\\infty} \\frac{1}{(4m^2)^2} = \\frac{1}{2\\pi} \\sum_{m=M+1}^{\\infty} \\frac{1}{m^4}$$\nThis sum can be bounded by an integral: $\\sum_{m=M+1}^{\\infty} m^{-4} \\approx \\int_{M+1}^{\\infty} x^{-4} dx = \\frac{1}{3(M+1)^3} = O(M^{-3}) = O(N^{-3})$.\nThus, $\\|f - P_N f\\|_w^2 = O(N^{-3})$, which implies a weighted $L^2$ convergence rate of:\n$$\\|f - P_N f\\|_w = O(N^{-3/2})$$\n\nNext, we explain the slower convergence in the uniform norm ($L^\\infty$).\nThe Nikolskii inequality for a polynomial $p_N$ of degree $N$ relates its uniform and weighted $L^2$ norms: $\\|p_N\\|_{L^\\infty} \\le C N \\|p_N\\|_{L_w^2}$. The key feature is the factor of $N$, which implies that high-degree polynomials can exhibit large peaks (high $L^\\infty$ norm) even if their weighted average size ($L_w^2$ norm) is small.\n\nThe error function $E_N(x) = f(x) - P_N f(x) = \\sum_{k=N+1}^{\\infty} a_k T_k(x)$ is not a polynomial, but it is composed of high-frequency polynomials. The principle of the Nikolskii inequality applies. The pointwise error tends to be largest near the singularity of the function, which for $f(x)=|x|$ occurs at $x=0$. At this point, the high-frequency Chebyshev polynomials $T_k(x)$ for even $k$ all contribute with the same sign pattern, leading to constructive interference and a slower decay of the error peak compared to the average error.\nOur calculation showed that $\\|f-P_N f\\|_w = O(N^{-3/2})$. If the $L^\\infty$ norm simply followed the $L_w^2$ norm, we would expect a similar rate. However, the Nikolskii inequality warns that converting an $L_w^2$ bound to an $L^\\infty$ bound for the high-degree polynomial components of the error introduces a penalty factor related to the degree. A detailed analysis shows the error at $x=0$ to be $\\sum_{k=N+1, k \\text{ even}}^\\infty |a_k T_k(0)| \\sim \\sum_{m \\sim N/2}^\\infty m^{-2} \\sim \\int_{N/2}^\\infty x^{-2}dx = O(N^{-1})$. Therefore, $\\|f - P_N f\\|_{L^\\infty} = O(N^{-1})$.\nThis rate of $O(N^{-1})$ is clearly slower than the weighted $L^2$ rate of $O(N^{-3/2})$.\n\nFinally, we connect this result to the Jackson error estimate. The function $f(x)=|x|$ is Lipschitz continuous on $[-1,1]$ (it belongs to the class $C^{0,1}$). Jackson's theorem for such functions states that the error of the *best* uniform approximation by a polynomial of degree $N$, denoted $E_N(f)$, satisfies $E_N(f) = \\inf_{p \\in \\mathcal{P}_N} \\|f-p\\|_{L^\\infty} = O(N^{-1})$. Our finding that the Chebyshev projection error converges as $\\|f-P_Nf\\|_{L^\\infty}=O(N^{-1})$ demonstrates that for this specific function, the projection method achieves the optimal rate of convergence predicted by Jackson's theorem. The slower $L^\\infty$ convergence is not an artifact of the method but is the best possible rate for any polynomial approximation due to the limited smoothness of $f(x)=|x|$.", "answer": "$$\\boxed{-\\frac{4\\cos(\\frac{k\\pi}{2})}{\\pi k^{2}}}$$", "id": "3393562"}, {"introduction": "We now shift our focus to functions with endpoint singularities, such as $f(x) = (1 - x)^{\\gamma}$, which are crucial models in many physical and engineering applications. This practice introduces a fundamental technique: mapping the problem from algebraic polynomials on $[-1,1]$ to trigonometric polynomials on the circle via the substitution $x=\\cos\\theta$. You will compute the classical modulus of continuity for the transformed function to determine how the singularity's strength, governed by $\\gamma$, controls the best approximation rate [@problem_id:3393500].", "problem": "Let $f(x) = (1 - x)^{\\gamma}$ on $[-1,1]$ with a fixed exponent $\\gamma > 0$. Define $g(\\theta) = f(\\cos \\theta)$ on $[0,\\pi]$ and extend $g$ to an even $2\\pi$-periodic function. Consider the uniform norm $\\|h\\|_{\\infty} := \\sup_{y} |h(y)|$ on the respective domains and the trigonometric modulus of continuity\n$$\n\\omega(g,t) := \\sup_{|h| \\le t} \\,\\|g(\\cdot + h) - g(\\cdot)\\|_{\\infty}, \\quad t > 0.\n$$\nWork from the core definitions of the modulus of continuity, the mapping $x = \\cos \\theta$ that identifies algebraic polynomials with trigonometric polynomials via the Chebyshev polynomials $T_k(\\cos \\theta) = \\cos(k\\theta)$, and the classical Jackson inequality for trigonometric approximation, which states that there is a constant $C$, independent of $n$ and $g$, such that the best trigonometric degree-$n$ approximation error satisfies $E_n^{\\mathrm{trig}}(g) \\le C\\,\\omega(g,1/n)$.\n\n(a) Compute the small-$t$ behavior of the trigonometric modulus $\\omega(g,t)$ by analyzing the endpoint singularity structure of $g$ at $\\theta = 0$ and $\\theta = \\pi$. Your derivation must explicitly use the local behavior of $1 - \\cos \\theta$ near $\\theta = 0$ and concavity or Lipschitz properties of $g$ as appropriate, and must be justified from first principles.\n\n(b) Using the Jackson inequality and the change of variables $x=\\cos \\theta$, pull back the resulting rate from the $\\theta$-domain to the $x$-domain and express the asymptotic order in $n$ of the best uniform approximation error on $[-1,1]$ by degree-$n$ algebraic polynomials:\n$$\nE_n(f;[-1,1]) := \\inf_{\\deg p \\le n} \\|f - p\\|_{\\infty,[-1,1]}.\n$$\nProvide your final answer as a single closed-form function of $n$ and $\\gamma$ that captures the asymptotic Jackson rate. Do not include unspecified constants. No rounding is required.", "solution": "The problem asks for the asymptotic rate of uniform polynomial approximation for the function $f(x) = (1 - x)^{\\gamma}$ on the interval $[-1,1]$, where $\\gamma > 0$ is a fixed exponent. The derivation is to be performed by mapping the problem to the trigonometric domain, analyzing the modulus of continuity of the resulting function, and applying the classical Jackson inequality.\n\nFirst, we validate the problem statement.\n**Step 1: Extract Givens**\n- Function: $f(x) = (1 - x)^{\\gamma}$ on $[-1,1]$.\n- Parameter: $\\gamma > 0$.\n- Transformed function: $g(\\theta) = f(\\cos \\theta)$ on $[0,\\pi]$, extended to an even $2\\pi$-periodic function.\n- Norm: $\\|h\\|_{\\infty} := \\sup_{y} |h(y)|$.\n- Modulus of continuity: $\\omega(g,t) := \\sup_{|h| \\le t} \\,\\|g(\\cdot + h) - g(\\cdot)\\|_{\\infty}$ for $t > 0$.\n- Mapping: $x = \\cos \\theta$.\n- Jackson inequality: $E_n^{\\mathrm{trig}}(g) \\le C\\,\\omega(g,1/n)$, where $E_n^{\\mathrm{trig}}(g)$ is the error of best trigonometric approximation of degree $n$.\n- Goal (a): Determine the small-$t$ behavior of $\\omega(g,t)$.\n- Goal (b): Determine the asymptotic order in $n$ of the best uniform algebraic polynomial approximation error $E_n(f;[-1,1])$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is mathematically and scientifically sound, falling into the standard framework of approximation theory. It is well-posed, providing all necessary definitions and constraints to arrive at a unique solution for the asymptotic rate as requested. The analysis of approximation rates for functions with singularities is a cornerstone of the theory and has direct applications in numerical analysis, including spectral methods. All terms are formally defined, and there are no contradictions or ambiguities. The problem is a valid and substantive exercise in mathematical analysis.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. We proceed with the solution.\n\n### Part (a): Analysis of the Trigonometric Modulus of Continuity\n\nWe define the function $g(\\theta)$ by substituting $x = \\cos\\theta$ into $f(x)$:\n$$\ng(\\theta) = f(\\cos \\theta) = (1 - \\cos \\theta)^{\\gamma}, \\quad \\theta \\in [0, \\pi].\n$$\nThis function is extended to be an even, $2\\pi$-periodic function on $\\mathbb{R}$. The smoothness of $g(\\theta)$ determines the behavior of its modulus of continuity $\\omega(g,t)$. The function $g(\\theta)$ is smooth for all $\\theta$ where $1-\\cos\\theta \\ne 0$. The potential non-smooth points occur when $\\cos\\theta = 1$, which corresponds to $\\theta = 2k\\pi$ for any integer $k$. Due to periodicity, we only need to analyze the singularity at $\\theta=0$.\n\nFor small $\\theta$, we use the Taylor expansion of $\\cos\\theta$ around $\\theta=0$:\n$$\n1 - \\cos\\theta = 1 - \\left(1 - \\frac{\\theta^2}{2!} + \\frac{\\theta^4}{4!} - \\dots\\right) = \\frac{\\theta^2}{2} - O(\\theta^4).\n$$\nTherefore, for $\\theta$ near $0$, the local behavior of $g(\\theta)$ is\n$$\ng(\\theta) = \\left(\\frac{\\theta^2}{2} - O(\\theta^4)\\right)^{\\gamma} = \\left(\\frac{1}{2}\\right)^{\\gamma} |\\theta|^{2\\gamma} \\left(1 - O(\\theta^2)\\right)^{\\gamma} = C|\\theta|^{2\\gamma} (1+O(\\theta^2)),\n$$\nwhere the absolute value arises from the even extension of $g$. The dominant behavior near the origin is $g(\\theta) \\sim |\\theta|^{2\\gamma}$. The regularity of $g(\\theta)$ at $\\theta=0$ depends critically on the value of the exponent $2\\gamma$. We analyze the modulus of continuity $\\omega(g,t)$ by cases.\n\n**Case 1: $0  \\gamma  1/2$**\nIn this case, the exponent $2\\gamma$ is in the interval $(0,1)$. The function $g(\\theta)$ behaves like $|\\theta|^{2\\gamma}$ near the origin. Such a function is known to be Hölder continuous with exponent $2\\gamma$, but not Lipschitz continuous. To show this from first principles, we analyze the difference $|g(\\theta+h)-g(\\theta)|$. The supremum is dominated by the behavior near the singularity. A lower bound on the modulus is established by considering the increment from the singular point $\\theta=0$:\n$$\n\\omega(g,t) \\ge |g(t) - g(0)| = (1-\\cos t)^{\\gamma} \\approx \\left(\\frac{t^2}{2}\\right)^{\\gamma} = C_1 t^{2\\gamma} \\quad \\text{for small } t > 0.\n$$\nAn upper bound can be derived by showing that $g$ satisfies a Hölder condition $|g(\\theta_1)-g(\\theta_2)| \\le C_2|\\theta_1-\\theta_2|^{2\\gamma}$. This is a standard result for functions with this type of singularity. The rate is determined by the local behavior, so we have $\\omega(g,t) = O(t^{2\\gamma})$.\n\n**Case 2: $\\gamma \\ge 1/2$**\nIn this case, the exponent $2\\gamma \\ge 1$. We investigate if the function is Lipschitz continuous by checking if its first derivative is bounded. The derivative of $g(\\theta)$ for $\\theta \\in (0,\\pi)$ is:\n$$\ng'(\\theta) = \\gamma (1-\\cos\\theta)^{\\gamma-1} \\sin\\theta.\n$$\nFor $\\theta$ near $0$, this behaves as:\n$$\n|g'(\\theta)| \\approx \\gamma \\left(\\frac{\\theta^2}{2}\\right)^{\\gamma-1} |\\theta| = C_3 |\\theta|^{2\\gamma-2} |\\theta| = C_3 |\\theta|^{2\\gamma-1}.\n$$\nSince $\\gamma \\ge 1/2$, the exponent $2\\gamma-1 \\ge 0$.\n- If $\\gamma = 1/2$, then $2\\gamma-1=0$, and $|g'(\\theta)|$ approaches a constant as $\\theta \\to 0^+$. The derivative has a jump discontinuity at $\\theta=0$ (due to the even extension), but it is bounded.\n- If $\\gamma > 1/2$, then $2\\gamma-1 > 0$, and $g'(\\theta) \\to 0$ as $\\theta \\to 0$. The derivative is continuous everywhere.\nIn both sub-cases, $|g'(\\theta)|$ is bounded on $[0, \\pi]$. Let $M = \\sup_{\\theta \\in [0,\\pi]} |g'(\\theta)|  \\infty$. By the Mean Value Theorem, for any $\\theta$ and $h$:\n$$\n|g(\\theta+h) - g(\\theta)| = |h| |g'(\\xi)| \\le M |h|,\n$$\nwhere $\\xi$ is between $\\theta$ and $\\theta+h$. Taking the supremum over $\\theta$ and then over $|h| \\le t$, we get:\n$$\n\\omega(g,t) = \\sup_{|h| \\le t} \\sup_{\\theta} |g(\\theta+h) - g(\\theta)| \\le M t.\n$$\nThus, $\\omega(g,t) = O(t)$. To show this is the correct order, we note that $\\lim_{t\\to 0} \\omega(g,t)/t$ exists and is equal to $\\|g'\\|_{\\infty}$ if $g'$ is continuous, or bounded below by a positive constant if $g'$ has jump discontinuities (as long as it's not identically zero). For any $\\gamma > 0$, $g'(\\theta)$ is not identically zero (e.g., $g'(\\pi/2) = \\gamma \\ne 0$). Therefore, the asymptotic behavior is precisely of order $t$.\n\nCombining the cases, the small-$t$ behavior of the modulus of continuity is:\n$$\n\\omega(g,t) \\sim \\begin{cases} t^{2\\gamma}  \\text{if } 0  \\gamma  1/2 \\\\ t  \\text{if } \\gamma \\ge 1/2 \\end{cases}\n$$\nThis can be written compactly as $O(t^{\\min(1, 2\\gamma)})$, noting that for $\\gamma=1/2$, $2\\gamma=1$.\n\n### Part (b): Asymptotic Approximation Rate\n\nThe connection between algebraic polynomial approximation on $[-1,1]$ and trigonometric polynomial approximation is established via the change of variables $x = \\cos\\theta$. An even trigonometric polynomial of degree $n$, $q_n(\\theta) = \\sum_{k=0}^n a_k \\cos(k\\theta)$, can be written as an algebraic polynomial of degree $n$ in $x$ by using the identity of Chebyshev polynomials $T_k(x) = \\cos(k\\arccos x)$. Specifically, $q_n(\\theta) = P_n(\\cos\\theta)$ where $P_n(x) = \\sum_{k=0}^n a_k T_k(x)$.\nThe best uniform approximation error by algebraic polynomials of degree $n$ on $[-1,1]$ is therefore equal to the best uniform approximation error of the transformed function $g(\\theta)$ by even trigonometric polynomials of degree $n$. Since $g$ is even, its best trigonometric approximant is also even. Thus,\n$$\nE_n(f;[-1,1]) = \\inf_{\\deg p \\le n} \\|f - p\\|_{\\infty,[-1,1]} = \\inf_{p_n} \\sup_{\\theta \\in [0,\\pi]} |g(\\theta) - p_n(\\cos\\theta)| = E_n^{\\mathrm{trig}}(g).\n$$\nWe now apply the classical Jackson inequality provided in the problem statement:\n$$\nE_n(f;[-1,1]) = E_n^{\\mathrm{trig}}(g) \\le C\\,\\omega(g, 1/n).\n$$\nSubstituting the asymptotic behavior of $\\omega(g,t)$ found in Part (a) with $t=1/n$:\n\n**Case 1: $0  \\gamma  1/2$**\nThe modulus of continuity has the behavior $\\omega(g, 1/n) \\sim (1/n)^{2\\gamma}$. The Jackson inequality thus gives an error bound:\n$$\nE_n(f;[-1,1]) \\le C \\cdot (1/n)^{2\\gamma} = O(n^{-2\\gamma}).\n$$\nFor functions that are not Lipschitz continuous, this rate given by the first Jackson theorem is known to be sharp.\n\n**Case 2: $\\gamma \\ge 1/2$**\nThe modulus of continuity has the behavior $\\omega(g, 1/n) \\sim 1/n$. The Jackson inequality gives an error bound:\n$$\nE_n(f;[-1,1]) \\le C \\cdot (1/n) = O(n^{-1}).\n$$\nThis rate is known to be sharp for functions that are Lipschitz but not $C^1$, which corresponds to the boundary case $\\gamma=1/2$. For $\\gamma > 1/2$, the function $g$ is smoother and the actual convergence rate is faster; however, the problem restricts us to use the provided inequality involving the first modulus of continuity, which is insensitive to higher-order smoothness and cannot provide a rate faster than $O(n^{-1})$.\n\nCombining both cases, the asymptotic rate derived from the specified tools is $O(n^{-2\\gamma})$ for $0  \\gamma  1/2$ and $O(n^{-1})$ for $\\gamma \\ge 1/2$. Noting that $2\\gamma=1$ when $\\gamma=1/2$, these two regimes connect at the boundary. The resulting rate can be expressed using a single formula. The problem asks for the closed-form function that captures this asymptotic rate. This is given by the expression $n^{-\\alpha}$ where $\\alpha$ is the convergence order. The order is $2\\gamma$ if $2\\gamma  1$, and $1$ if $2\\gamma \\ge 1$. This corresponds to an exponent of $\\min(1, 2\\gamma)$.\n\nThe asymptotic Jackson rate is therefore described by the function $n^{-\\min(1, 2\\gamma)}$.", "answer": "$$\n\\boxed{n^{-\\min(1, 2\\gamma)}}\n$$", "id": "3393500"}, {"introduction": "Building on the previous example, we revisit the function $f(x) = (1 - x)^{\\gamma}$ to explore a more advanced and precise method for characterizing approximation error. This exercise introduces the Ditzian-Totik (DT) modulus of smoothness, a powerful tool that uses a variable step size to better capture a function's regularity near endpoints. By applying the corresponding Jackson-type theorems, you will derive a sharp estimate for the approximation rate, revealing the deep connection between error decay and weighted smoothness properties [@problem_id:3393513].", "problem": "Consider algebraic polynomial approximation on the interval $[-1,1]$ in the uniform norm $L^{\\infty}$. Let $\\varphi(x) = \\sqrt{1 - x^{2}}$ and, for an integer $r \\geq 1$, define the $r$-th Ditzian–Totik (DT) modulus of smoothness in $L^{\\infty}$ by\n$$\n\\omega_{r}^{\\varphi}(f,t)_{L^{\\infty}} := \\sup_{0  h \\leq t} \\left\\| \\Delta_{h \\varphi(\\cdot)}^{r} f \\right\\|_{L^{\\infty}(-1,1)},\n$$\nwhere the $r$-th forward difference with variable step $\\delta(x) := h \\varphi(x)$ is\n$$\n\\Delta_{\\delta}^{r} f(x) := \\sum_{k=0}^{r} (-1)^{r-k} \\binom{r}{k} f\\big(x + k \\delta(x)\\big),\n$$\nwith the convention that the sum is taken over those $x$ for which all arguments $x + k \\delta(x)$ lie in $[-1,1]$. Consider the endpoint-singular function\n$$\nf(x) = (1 - x)^{\\gamma}, \\quad \\gamma \\in (0,1),\n$$\nand assume the approximation space consists of algebraic polynomials of degree at most $n$. Starting strictly from the above definitions and foundational Jackson-type direct and inverse principles for polynomial approximation in the Ditzian–Totik framework (without invoking any specialized shortcut formulas), determine:\n\n1. The leading-order asymptotic behavior in $t$ of the DT modulus $\\omega_{r}^{\\varphi}(f,t)_{L^{\\infty}}$ for a fixed integer $r$ satisfying $r  2 \\gamma$.\n\n2. The resulting leading-order asymptotic rate, in terms of $n$, of the best uniform-approximation error of $f$ by degree-$n$ polynomials,\n$$\nE_{n}(f)_{L^{\\infty}} := \\inf_{p \\in \\mathbb{P}_{n}} \\| f - p \\|_{L^{\\infty}(-1,1)},\n$$\nwhere $\\mathbb{P}_{n}$ denotes the set of algebraic polynomials of degree at most $n$.\n\nExpress your final answer as two closed-form analytic expressions, one in $t$ and one in $n$, respectively. No numerical rounding is required.", "solution": "The problem is valid as it is a well-posed question in the mathematical field of approximation theory, built upon standard definitions and concepts such as Ditzian-Totik moduli of smoothness and best polynomial approximation. All provided information is self-contained and scientifically sound.\n\n### Part 1: Asymptotic Behavior of the Ditzian–Totik Modulus\n\nThe first task is to determine the leading-order asymptotic behavior of the Ditzian–Totik (DT) modulus of smoothness $\\omega_{r}^{\\varphi}(f,t)_{L^{\\infty}}$ for the function $f(x) = (1 - x)^{\\gamma}$, where $\\gamma \\in (0,1)$, and the integer $r$ satisfies $r  2 \\gamma$.\n\nThe key to analyzing the DT modulus with the weight $\\varphi(x) = \\sqrt{1 - x^2}$ is to employ the change of variables $x = \\cos(\\theta)$, which maps the interval $x \\in [-1,1]$ to $\\theta \\in [0,\\pi]$. This transformation is fundamental in the theory of orthogonal polynomials and approximation on an interval, as it \"unfolds\" the endpoint singularities.\n\nLet $g(\\theta) = f(\\cos \\theta)$. The function $f(x)$ becomes:\n$$ g(\\theta) = (1 - \\cos \\theta)^{\\gamma} = \\left(2 \\sin^2\\left(\\frac{\\theta}{2}\\right)\\right)^{\\gamma} $$\nThe singularity of $f(x)$ at $x=1$ corresponds to a singularity of $g(\\theta)$ at $\\theta=0$. For small $\\theta  0$, we have $\\sin(\\theta/2) \\approx \\theta/2$, so the behavior of $g(\\theta)$ near $\\theta=0$ is:\n$$ g(\\theta) \\asymp \\theta^{2\\gamma} $$\nwhere $\\asymp$ denotes that the ratio of the two sides is bounded above and below by positive constants.\n\nThe variable step in the definition of the DT modulus is $\\delta(x) = h \\varphi(x) = h\\sqrt{1-x^2}$. Under the substitution $x=\\cos\\theta$, this step becomes $\\delta(\\cos\\theta) = h \\sin\\theta$.\nThe points $x_k = x + k \\delta(x)$ used in the forward difference operator become $x_k = \\cos\\theta + k h \\sin\\theta$. For small $h$, these points can be related to a uniform step in the $\\theta$ variable:\n$$ \\cos(\\theta - kh) = \\cos\\theta \\cos(kh) + \\sin\\theta \\sin(kh) \\approx \\cos\\theta (1) + \\sin\\theta (kh) = \\cos\\theta + kh \\sin\\theta = x_k $$\nThis implies that the point $x_k$ corresponds approximately to the angle $\\theta_k \\approx \\theta - kh$. Therefore, the forward difference of $f$ with a variable step in $x$ is approximately equivalent to a forward difference of $g$ with a constant step in $\\theta$:\n$$ \\Delta_{h\\varphi(x)}^{r} f(x) = \\sum_{k=0}^{r} (-1)^{r-k} \\binom{r}{k} f(x_k) \\approx \\sum_{k=0}^{r} (-1)^{r-k} \\binom{r}{k} g(\\theta - kh) = (-1)^r \\Delta_{-h}^r g(\\theta) $$\nwhere $\\Delta_{-h}^r$ is the standard $r$-th forward difference operator with step $-h$.\n\nThe norm is taken over $L^{\\infty}(-1,1)$, which corresponds to $L^{\\infty}[0,\\pi]$ for the variable $\\theta$. Thus, the DT modulus $\\omega_{r}^{\\varphi}(f,t)_{L^{\\infty}}$ is asymptotically equivalent to the classical modulus of smoothness $\\omega_r(g,t)_{L^{\\infty}[0,\\pi]}$:\n$$ \\omega_{r}^{\\varphi}(f,t)_{L^{\\infty}} \\asymp \\omega_r(g,t)_{L^{\\infty}[0,\\pi]} := \\sup_{0  h \\leq t} \\sup_{\\theta} |\\Delta_h^r g(\\theta)| $$\nThe behavior of $\\omega_r(g,t)$ is determined by the smoothness of $g(\\theta)$. The function $g(\\theta)$ is smooth everywhere on $[0,\\pi]$ except at $\\theta=0$, where $g(\\theta) \\asymp \\theta^{2\\gamma}$.\nFor a function behaving like $y^{\\alpha}$ near the origin, its $r$-th modulus of smoothness $\\omega_r(y^\\alpha, t)_{L^\\infty}$ has a well-known behavior. Given the condition $r  \\alpha$, we have:\n$$ \\omega_r(y^\\alpha, t)_{L^\\infty} \\asymp t^\\alpha $$\nIn our case, the exponent is $\\alpha = 2\\gamma$, and the problem states that $r  2\\gamma$. Therefore, the modulus of smoothness for $g(\\theta)$ behaves as:\n$$ \\omega_r(g,t)_{L^{\\infty}[0,\\pi]} \\asymp t^{2\\gamma} $$\nThis behavior stems from taking the difference operator across the singularity at $\\theta=0$. For any $\\theta_0  0$, $g$ is $C^\\infty$ on $[\\theta_0, \\pi]$, so $|\\Delta_h^r g(\\theta)| \\le C h^r$ for $\\theta \\ge \\theta_0$. Since $r  2\\gamma$, this term is dominated by the $t^{2\\gamma}$ behavior for small $t$.\n\nFrom the equivalence, we conclude that the leading-order asymptotic behavior of the DT modulus for $f(x)$ is:\n$$ \\omega_{r}^{\\varphi}(f,t)_{L^{\\infty}} \\asymp t^{2\\gamma} $$\n\n### Part 2: Asymptotic Rate of Best Approximation Error\n\nThe second task is to find the leading-order asymptotic rate of the best uniform approximation error $E_n(f)_{L^\\infty}$. This is achieved by using the foundational direct and inverse theorems of approximation theory in the Ditzian-Totik framework.\n\nA **direct theorem**, or Jackson-type theorem, provides an upper bound for the approximation error in terms of the modulus of smoothness. For any function $f \\in C([-1,1])$ and for $n \\ge r$, there is a constant $C$ such that:\n$$ E_n(f)_{L^\\infty} \\le C\\, \\omega_r^\\varphi\\left(f, \\frac{1}{n}\\right)_{L^\\infty} $$\nSubstituting the result from Part 1, where $\\omega_{r}^{\\varphi}(f,t)_{L^{\\infty}} \\asymp t^{2\\gamma}$:\n$$ E_n(f)_{L^\\infty} \\le C \\left(\\frac{1}{n}\\right)^{2\\gamma} = C n^{-2\\gamma} $$\nThis establishes an upper bound on the rate of convergence: $E_n(f)_{L^\\infty} = O(n^{-2\\gamma})$.\n\nAn **inverse theorem**, or Bernstein-Stechkin-type theorem, provides a lower bound on the modulus of smoothness in terms of the approximation errors. It states that for $t \\in (0, 1/r)$:\n$$ \\omega_r^\\varphi(f, t)_{L^\\infty} \\le C t^r \\sum_{k=0}^{\\lfloor 1/t \\rfloor} (k+1)^{r-1} E_k(f)_{L^\\infty} $$\nWe use this theorem to establish a lower bound on $E_n(f)_{L^\\infty}$. Let $t = 1/n$. Using the lower bound on the modulus from Part 1, $\\omega_{r}^{\\varphi}(f, 1/n)_{L^{\\infty}} \\ge C_1 n^{-2\\gamma}$, we get:\n$$ C_1 n^{-2\\gamma} \\le C n^{-r} \\sum_{k=0}^{n} (k+1)^{r-1} E_k(f)_{L^\\infty} $$\nThe sequence of best approximation errors $E_k(f)_{L^\\infty}$ is non-increasing. Thus, for $k \\le n$, we have $E_k(f)_{L^\\infty} \\ge E_n(f)_{L^\\infty}$. We can therefore write:\n$$ C_1 n^{-2\\gamma} \\le C n^{-r} \\sum_{k=0}^{n} (k+1)^{r-1} E_n(f)_{L^\\infty} = C E_n(f)_{L^\\infty} n^{-r} \\sum_{j=1}^{n+1} j^{r-1} $$\nThe sum can be bounded by an integral:\n$$ \\sum_{j=1}^{n+1} j^{r-1} \\asymp \\int_1^{n+1} x^{r-1} dx = \\frac{(n+1)^r - 1}{r} \\asymp n^r $$\nSubstituting this back into the inequality:\n$$ C_1 n^{-2\\gamma} \\le C E_n(f)_{L^\\infty} n^{-r} (C_2 n^r) = (C C_2) E_n(f)_{L^\\infty} $$\nThis implies a lower bound on the error:\n$$ E_n(f)_{L^\\infty} \\ge \\frac{C_1}{C C_2} n^{-2\\gamma} $$\nThis establishes that $E_n(f)_{L^\\infty} = \\Omega(n^{-2\\gamma})$.\n\nCombining the upper bound $E_n(f)_{L^\\infty} = O(n^{-2\\gamma})$ and the lower bound $E_n(f)_{L^\\infty} = \\Omega(n^{-2\\gamma})$, we conclude that the asymptotic rate of convergence is:\n$$ E_n(f)_{L^\\infty} \\asymp n^{-2\\gamma} $$\nThe leading-order asymptotic rate is $n^{-2\\gamma}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nt^{2\\gamma}  n^{-2\\gamma}\n\\end{pmatrix}\n}\n$$", "id": "3393513"}]}