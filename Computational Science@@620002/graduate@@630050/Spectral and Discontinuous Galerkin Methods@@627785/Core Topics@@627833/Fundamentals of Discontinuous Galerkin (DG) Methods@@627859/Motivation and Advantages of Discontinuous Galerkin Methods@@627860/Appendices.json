{"hands_on_practices": [{"introduction": "A primary motivation for using Discontinuous Galerkin (DG) methods is their outstanding computational efficiency, particularly for solving time-dependent partial differential equations. This efficiency stems from the structure of the global mass matrix, which is block-diagonal, allowing for element-local computations. This practice [@problem_id:3401227] takes you through a foundational calculation that reveals the ideal outcome of this structure: for an affine element mapping and an orthogonal basis, the local mass matrix becomes diagonal, making its inversion trivial and paving the way for highly parallelizable explicit time-stepping schemes.", "problem": "Consider a Discontinuous Galerkin (DG) method on a single quadrilateral element with nondimensional coordinates. Let the reference element be the square $\\widehat{K} = [-1,1] \\times [-1,1]$ with local coordinates $(r,s)$. The physical element $K$ has vertices ordered counterclockwise corresponding to the reference corners $(-1,-1)$, $(1,-1)$, $(1,1)$, $(-1,1)$, and is given by the points $(0,0)$, $(2,0)$, $(2,3)$, $(0,3)$ in the physical coordinate system $(x,y)$. Use the standard bilinear isoparametric mapping constructed from the four bilinear Lagrange shape functions on $\\widehat{K}$:\n$$\nN_{1}(r,s) = \\frac{1}{4}(1-r)(1-s),\\quad\nN_{2}(r,s) = \\frac{1}{4}(1+r)(1-s),\\quad\nN_{3}(r,s) = \\frac{1}{4}(1+r)(1+s),\\quad\nN_{4}(r,s) = \\frac{1}{4}(1-r)(1+s),\n$$\nand define the mapping to the physical element by\n$$\nx(r,s) = \\sum_{i=1}^{4} N_{i}(r,s)\\,x_{i},\\qquad\ny(r,s) = \\sum_{i=1}^{4} N_{i}(r,s)\\,y_{i},\n$$\nwhere $(x_{i},y_{i})$ are the physical vertex coordinates in the order above. Let the approximation space on $\\widehat{K}$ be the tensor-product $L^{2}$-orthonormal polynomial basis\n$$\n\\psi_{p,q}(r,s) = \\varphi_{p}(r)\\,\\varphi_{q}(s),\\qquad p,q \\in \\{0,1,\\dots,P\\},\n$$\nwith $\\varphi_{p}(r) = \\sqrt{\\frac{2p+1}{2}}\\,P_{p}(r)$, where $P_{p}$ is the Legendre polynomial of degree $p$ on $[-1,1]$, so that\n$$\n\\int_{-1}^{1}\\int_{-1}^{1} \\psi_{p,q}(r,s)\\,\\psi_{p',q'}(r,s)\\,dr\\,ds = \\delta_{p,p'}\\,\\delta_{q,q'}.\n$$\nThe local mass matrix entry for the DG method is defined by\n$$\nM_{(p,q),(p',q')} = \\int_{K} \\psi_{p,q}(x,y)\\,\\psi_{p',q'}(x,y)\\,dA,\n$$\nwhere $dA$ is the physical area element, and $\\delta_{i,j}$ denotes the Kronecker delta.\n\nStarting from these definitions:\n- Compute the explicit reference-to-physical mapping $(x(r,s),y(r,s))$ and the Jacobian determinant $J(r,s) = \\det\\left(\\frac{\\partial(x,y)}{\\partial(r,s)}\\right)$.\n- Use the change-of-variables formula to write $M_{(p,q),(p',q')}$ as an integral over $\\widehat{K}$ including the Jacobian determinant.\n- Evaluate this integral in closed form.\n\nExplain briefly, based on your derivation, how the resulting structure of the local mass matrix illustrates a core computational motivation and advantage of Discontinuous Galerkin methods (DG), focusing on locality and efficiency. Express your final answer as a single closed-form analytic expression for $M_{(p,q),(p',q')}$; no rounding is required.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n-   Reference element: The square $\\widehat{K} = [-1,1] \\times [-1,1]$ with local coordinates $(r,s)$.\n-   Physical element vertices: Ordered counterclockwise, corresponding to reference corners $(-1,-1)$, $(1,-1)$, $(1,1)$, $(-1,1)$, are $(x_1,y_1)=(0,0)$, $(x_2,y_2)=(2,0)$, $(x_3,y_3)=(2,3)$, $(x_4,y_4)=(0,3)$.\n-   Bilinear Lagrange shape functions:\n    $N_{1}(r,s) = \\frac{1}{4}(1-r)(1-s)$\n    $N_{2}(r,s) = \\frac{1}{4}(1+r)(1-s)$\n    $N_{3}(r,s) = \\frac{1}{4}(1+r)(1+s)$\n    $N_{4}(r,s) = \\frac{1}{4}(1-r)(1+s)$\n-   Isoparametric mapping: $x(r,s) = \\sum_{i=1}^{4} N_{i}(r,s)\\,x_{i}$, $y(r,s) = \\sum_{i=1}^{4} N_{i}(r,s)\\,y_{i}$.\n-   Approximation basis on $\\widehat{K}$: $\\psi_{p,q}(r,s) = \\varphi_{p}(r)\\,\\varphi_{q}(s)$ for $p,q \\in \\{0,1,\\dots,P\\}$.\n-   One-dimensional basis functions: $\\varphi_{p}(r) = \\sqrt{\\frac{2p+1}{2}}\\,P_{p}(r)$, where $P_{p}$ is the Legendre polynomial of degree $p$.\n-   Orthonormality condition: $\\int_{-1}^{1}\\int_{-1}^{1} \\psi_{p,q}(r,s)\\,\\psi_{p',q'}(r,s)\\,dr\\,ds = \\delta_{p,p'}\\,\\delta_{q,q'}$.\n-   Local mass matrix definition: $M_{(p,q),(p',q')} = \\int_{K} \\psi_{p,q}(x,y)\\,\\psi_{p',q'}(x,y)\\,dA$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard exercise in the computational theory of Discontinuous Galerkin (DG) methods. All provided definitions, including the shape functions, isoparametric mapping, and Legendre polynomial basis, are standard in the field of spectral and finite element methods. The problem is self-contained, with all necessary data and definitions provided. The vertices define a simple rectangle, which simplifies the Jacobian calculation, a common technique for illustrating fundamental principles. The question is objective, mathematically formalizable, and directly pertains to the specified topic. There are no scientific or factual unsoundness, contradictions, or ambiguities.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A complete solution will be provided.\n\n### Solution Derivation\n\nThe solution proceeds in three parts as requested: computation of the mapping and Jacobian, transformation of the mass matrix integral, and evaluation of the integral, followed by an explanation of the result's significance.\n\n**1. Reference-to-Physical Mapping and Jacobian Determinant**\n\nThe isoparametric mapping from the reference coordinates $(r,s) \\in \\widehat{K}$ to the physical coordinates $(x,y) \\in K$ is constructed using the given shape functions and vertex coordinates.\n\nThe $x$-coordinate mapping is:\n$$\nx(r,s) = N_{1}(r,s)x_{1} + N_{2}(r,s)x_{2} + N_{3}(r,s)x_{3} + N_{4}(r,s)x_{4}\n$$\nSubstituting the vertex coordinates $(x_1, x_2, x_3, x_4) = (0, 2, 2, 0)$:\n$$\nx(r,s) = \\frac{1}{4}(1-r)(1-s)(0) + \\frac{1}{4}(1+r)(1-s)(2) + \\frac{1}{4}(1+r)(1+s)(2) + \\frac{1}{4}(1-r)(1+s)(0)\n$$\n$$\nx(r,s) = \\frac{1}{2}(1+r)(1-s) + \\frac{1}{2}(1+r)(1+s) = \\frac{1}{2}(1+r)[(1-s)+(1+s)] = \\frac{1}{2}(1+r)(2) = 1+r\n$$\n\nThe $y$-coordinate mapping is:\n$$\ny(r,s) = N_{1}(r,s)y_{1} + N_{2}(r,s)y_{2} + N_{3}(r,s)y_{3} + N_{4}(r,s)y_{4}\n$$\nSubstituting the vertex coordinates $(y_1, y_2, y_3, y_4) = (0, 0, 3, 3)$:\n$$\ny(r,s) = \\frac{1}{4}(1-r)(1-s)(0) + \\frac{1}{4}(1+r)(1-s)(0) + \\frac{1}{4}(1+r)(1+s)(3) + \\frac{1}{4}(1-r)(1+s)(3)\n$$\n$$\ny(r,s) = \\frac{3}{4}(1+r)(1+s) + \\frac{3}{4}(1-r)(1+s) = \\frac{3}{4}(1+s)[(1+r)+(1-r)] = \\frac{3}{4}(1+s)(2) = \\frac{3}{2}(1+s)\n$$\nThe explicit mapping is $(x(r,s), y(r,s)) = \\left(1+r, \\frac{3}{2}(1+s)\\right)$. This is an affine mapping.\n\nNext, we compute the Jacobian matrix of the transformation, $\\frac{\\partial(x,y)}{\\partial(r,s)}$:\n$$\n\\mathbf{J} =\n\\begin{pmatrix}\n\\frac{\\partial x}{\\partial r} & \\frac{\\partial x}{\\partial s} \\\\\n\\frac{\\partial y}{\\partial r} & \\frac{\\partial y}{\\partial s}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{\\partial}{\\partial r}(1+r) & \\frac{\\partial}{\\partial s}(1+r) \\\\\n\\frac{\\partial}{\\partial r}\\left(\\frac{3}{2}(1+s)\\right) & \\frac{\\partial}{\\partial s}\\left(\\frac{3}{2}(1+s)\\right)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & \\frac{3}{2}\n\\end{pmatrix}\n$$\nThe Jacobian determinant, $J(r,s)$, is the determinant of this matrix:\n$$\nJ(r,s) = \\det(\\mathbf{J}) = (1)\\left(\\frac{3}{2}\\right) - (0)(0) = \\frac{3}{2}\n$$\nThe Jacobian determinant is constant because the mapping is affine (mapping a square to a rectangle). This constant represents the ratio of the physical element area to the reference element area: $\\text{Area}(K) / \\text{Area}(\\widehat{K}) = (2 \\times 3) / (2 \\times 2) = 6/4 = 3/2$.\n\n**2. Mass Matrix Integral Transformation**\n\nThe local mass matrix entry is defined as an integral over the physical element $K$:\n$$\nM_{(p,q),(p',q')} = \\int_{K} \\psi_{p,q}(x,y)\\,\\psi_{p',q'}(x,y)\\,dA\n$$\nThe basis functions $\\psi_{p,q}$ are defined on the reference element $\\widehat{K}$ in terms of $(r,s)$. We use the change of variables formula for integration, where the physical area element $dA$ is replaced by $J(r,s)\\,dr\\,ds$.\n$$\nM_{(p,q),(p',q')} = \\int_{\\widehat{K}} \\psi_{p,q}(r,s)\\,\\psi_{p',q'}(r,s)\\,J(r,s)\\,dr\\,ds\n$$\nSubstituting the calculated Jacobian determinant $J(r,s) = \\frac{3}{2}$:\n$$\nM_{(p,q),(p',q')} = \\int_{-1}^{1}\\int_{-1}^{1} \\psi_{p,q}(r,s)\\,\\psi_{p',q'}(r,s)\\,\\left(\\frac{3}{2}\\right)\\,dr\\,ds\n$$\n\n**3. Closed-Form Evaluation and Interpretation**\n\nSince the Jacobian determinant is a constant, it can be factored out of the integral:\n$$\nM_{(p,q),(p',q')} = \\frac{3}{2} \\int_{-1}^{1}\\int_{-1}^{1} \\psi_{p,q}(r,s)\\,\\psi_{p',q'}(r,s)\\,dr\\,ds\n$$\nThe problem statement provides the orthonormality property of the basis functions $\\psi_{p,q}$:\n$$\n\\int_{-1}^{1}\\int_{-1}^{1} \\psi_{p,q}(r,s)\\,\\psi_{p',q'}(r,s)\\,dr\\,ds = \\delta_{p,p'}\\,\\delta_{q,q'}\n$$\nwhere $\\delta_{i,j}$ is the Kronecker delta. Substituting this into the expression for the mass matrix entry yields the final closed-form result:\n$$\nM_{(p,q),(p',q')} = \\frac{3}{2}\\,\\delta_{p,p'}\\,\\delta_{q,q'}\n$$\nThis result indicates that the local mass matrix is a diagonal matrix. Specifically, it is $\\frac{3}{2}$ times the identity matrix.\n\n**Explanation of Computational Motivation and Advantage:**\n\nThis derivation illustrates a core computational advantage of the Discontinuous Galerkin method, particularly when using an orthogonal basis.\n\n1.  **Locality and Block-Diagonal Structure:** The mass matrix is computed on an element-by-element basis. In a global system assembled from many elements, the global mass matrix becomes block-diagonal, where each block is the local mass matrix of an element. There is no coupling between basis functions on different elements, hence the \"discontinuous\" nature of the approximation.\n\n2.  **Efficiency through Diagonalization:** The pivotal result here is that the local mass matrix is diagonal. For time-dependent problems, which are often solved with explicit time-stepping schemes (e.g., Runge-Kutta methods), a crucial step involves solving a linear system of the form $M\\dot{u} = F(u)$, which requires computing $M^{-1}F(u)$.\n    -   Because the global mass matrix $M$ is block-diagonal, its inverse $M^{-1}$ is also block-diagonal, where each block is the inverse of the corresponding local mass matrix. The inversion can be done locally on each element.\n    -   Crucially, since the local mass matrix itself is diagonal ($M_{\\text{local}} = cI$), its inverse is trivial to compute: $(M_{\\text{local}})^{-1} = (1/c)I$. In this specific problem, the inverse of any local mass matrix block is simply multiplication by $2/3$.\n    -   This avoids the need for a computationally expensive global linear solve at each time step, which would be necessary in continuous Galerkin methods where the global mass matrix is sparse but not block-diagonal. The DG method's mass matrix is thus \"explicit-friendly,\" leading to highly efficient and parallelizable algorithms.\n\nWhile the perfectly diagonal mass matrix occurs for affine elements and orthogonal bases, the block-diagonal global structure is a general feature of DG methods. For more complex, non-affine (curved) elements, the Jacobian is no longer constant, and the local mass matrix becomes dense. However, it is still a small matrix that can be inverted far more efficiently than a large global system matrix, preserving a significant computational advantage over continuous methods.", "answer": "$$\n\\boxed{\\frac{3}{2}\\delta_{p,p'}\\delta_{q,q'}}\n$$", "id": "3401227"}, {"introduction": "The \"discontinuous\" nature of the DG approximation space requires a carefully designed \"glue\" to connect neighboring elements and ensure the scheme is stable. This role is filled by the numerical flux, a term that dictates how information is exchanged across element interfaces. This hands-on practice [@problem_id:3401252] explores the critical design trade-offs involved in choosing a flux, contrasting an energy-conserving but unstable central flux with stable, dissipative alternatives. By analyzing the energy behavior, you will understand how the deliberate introduction of numerical dissipation is a cornerstone of creating robust and reliable DG schemes for hyperbolic problems.", "problem": "Consider the linear hyperbolic system $q_t + A q_x = 0$ on a one-dimensional periodic domain of length $L$, where $A \\in \\mathbb{R}^{m \\times m}$ is a fixed symmetric matrix, and $q(x,t) \\in \\mathbb{R}^m$. Use a uniform mesh with $N$ cells of size $h = L/N$ and the Discontinuous Galerkin (DG) method with piecewise-constant basis (polynomial degree zero). Let $q_i(t)$ denote the cellwise-constant approximation in cell $i$ and $F_{i+\\frac{1}{2}}$ denote the numerical flux at the interface between cells $i$ and $i+1$.\n\nDefine the following three numerical fluxes, all of which are standard and widely used in practice:\n\n- Central flux: $F^{\\mathrm{cen}}_{i+\\frac{1}{2}} = \\frac{1}{2} A q_i + \\frac{1}{2} A q_{i+1}$.\n- Upwind flux: $F^{\\mathrm{up}}_{i+\\frac{1}{2}} = A^+ q_i + A^- q_{i+1}$, where $A = Q \\Lambda Q^\\top$ is an orthogonal eigen-decomposition of $A$ with $\\Lambda = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_m)$ and $Q^\\top Q = I$, and $A^+ = Q \\mathrm{diag}(\\max(\\lambda_1,0),\\dots,\\max(\\lambda_m,0)) Q^\\top$, $A^- = Q \\mathrm{diag}(\\min(\\lambda_1,0),\\dots,\\min(\\lambda_m,0)) Q^\\top$.\n- Lax–Friedrichs (LF) flux: $F^{\\mathrm{LF}}_{i+\\frac{1}{2}} = \\frac{1}{2} A q_i + \\frac{1}{2} A q_{i+1} - \\frac{\\alpha}{2} (q_{i+1} - q_i)$, where $\\alpha \\ge 0$ is a scalar penalty parameter.\n\nLet the semi-discrete DG scheme be\n$$\n\\frac{d}{dt} q_i = -\\frac{1}{h}\\left(F_{i+\\frac{1}{2}} - F_{i-\\frac{1}{2}}\\right),\n$$\nwith periodic indexing, i.e., $q_{N} \\equiv q_0$.\n\nDefine the discrete energy\n$$\nE(t) = \\frac{1}{2} \\sum_{i=0}^{N-1} h\\, q_i(t)^\\top q_i(t).\n$$\n\nYour tasks:\n\n1. Starting from the above semi-discrete scheme and the definitions of the central, upwind, and Lax–Friedrichs (LF) fluxes, derive the exact algebraic expression for $\\frac{d}{dt} E(t)$ in terms of interface jumps $q_{i+1}-q_i$ and the chosen numerical flux, for each of the three fluxes. The derivation must use only the periodicity and the symmetry of $A$, and must clearly show the sign properties of the energy rate for each flux.\n\n2. Show how the semi-discrete operator $L$ of size $mN \\times mN$, defined by stacking all cell states into a single vector $Q \\in \\mathbb{R}^{mN}$ with ordering $Q = [q_0; q_1; \\dots; q_{N-1}]$ so that $\\frac{d}{dt} Q = L Q$, can be constructed for a given flux using constant $m \\times m$ matrices $K_L$ and $K_R$ such that $F_{i+\\frac{1}{2}} = K_L q_i + K_R q_{i+1}$. Explicitly write $L$ as a block-circulant matrix in terms of $K_L$, $K_R$, $h$, and $N$.\n\n3. Explain, from first principles, why the energy behavior identified in part 1 is a central motivation for Discontinuous Galerkin (DG) design: discuss the trade-offs between no dissipation (central), characteristic dissipation guided by the matrix $|A|$ (upwind), and tunable scalar dissipation (LF), and the implications for stability and accuracy.\n\n4. Implement a program that, for each test case listed below, constructs the semi-discrete operator $L$, computes the energy rate $\\frac{d}{dt} E$ for a specified state $q$, and computes the spectral radius of $L$ (the maximum modulus of its eigenvalues). Report the results in the exact order specified.\n\nTest suite:\n\n- Case 1 (happy path, scalar advection):\n  - $m = 1$, $L = 1$, $N = 8$, $h = L/N$.\n  - $A = [1]$.\n  - State $q_i = \\sin(2\\pi x_i)$ with cell centers $x_i = \\left(i+\\frac{1}{2}\\right) h$, angle in radians.\n  - LF parameter $\\alpha = 1.1$.\n\n- Case 2 (mixed characteristic speeds, two-component system):\n  - $m = 2$, $L = 1$, $N = 10$, $h = L/N$.\n  - $A = Q \\mathrm{diag}(2,-1) Q^\\top$ with $Q$ a rotation by angle $\\theta = 0.7$ radians, i.e., $Q = \\begin{pmatrix}\\cos \\theta & -\\sin \\theta\\\\ \\sin \\theta & \\cos \\theta\\end{pmatrix}$.\n  - State $q_i = \\begin{pmatrix}\\sin(2\\pi x_i)\\\\ \\cos(4\\pi x_i)\\end{pmatrix}$, angle in radians.\n  - LF parameter $\\alpha = 2.5$.\n\n- Case 3 (edge case, zero matrix):\n  - $m = 1$, $L = 1$, $N = 8$, $h = L/N$.\n  - $A = [0]$.\n  - State $q_i = \\sin(2\\pi x_i)$ with $x_i = \\left(i+\\frac{1}{2}\\right) h$, angle in radians.\n  - LF parameter $\\alpha = 0.5$.\n\nFor each case, compute and return six floating-point values in the following order:\n- Energy rate with central flux, $\\frac{d}{dt} E$ (central).\n- Energy rate with upwind flux, $\\frac{d}{dt} E$ (upwind).\n- Energy rate with LF flux, $\\frac{d}{dt} E$ (LF).\n- Spectral radius of $L$ with central flux.\n- Spectral radius of $L$ with upwind flux.\n- Spectral radius of $L$ with LF flux.\n\nFinal output format:\n\nYour program should produce a single line containing the concatenated results of all three cases as a comma-separated list enclosed in square brackets:\n$$\n[\\mathrm{ER}^{\\mathrm{cen}}_1,\\mathrm{ER}^{\\mathrm{up}}_1,\\mathrm{ER}^{\\mathrm{LF}}_1,\\rho^{\\mathrm{cen}}_1,\\rho^{\\mathrm{up}}_1,\\rho^{\\mathrm{LF}}_1,\\mathrm{ER}^{\\mathrm{cen}}_2,\\dots,\\rho^{\\mathrm{LF}}_3],\n$$\nwhere $\\mathrm{ER}$ denotes energy rate and $\\rho$ denotes spectral radius. All numerical values must be reported as plain floating-point numbers without any physical units.", "solution": "The problem is assessed to be valid as it is scientifically grounded in the theory of numerical methods for partial differential equations, is well-posed, objective, and internally consistent. All necessary information for a complete solution is provided.\n\n### Part 1: Derivation of the Energy Rate $\\frac{d}{dt}E(t)$\n\nThe discrete energy is defined as $E(t) = \\frac{1}{2} \\sum_{i=0}^{N-1} h\\, q_i(t)^\\top q_i(t)$. Its time derivative is:\n$$\n\\frac{dE}{dt} = \\frac{d}{dt} \\left( \\frac{1}{2} \\sum_{i=0}^{N-1} h\\, q_i^\\top q_i \\right) = \\frac{1}{2} \\sum_{i=0}^{N-1} h \\left( \\frac{dq_i^\\top}{dt} q_i + q_i^\\top \\frac{dq_i}{dt} \\right)\n$$\nSince $q_i^\\top \\frac{dq_i}{dt}$ is a scalar, it equals its own transpose, $(\\frac{dq_i}{dt})^\\top q_i$. Thus,\n$$\n\\frac{dE}{dt} = \\sum_{i=0}^{N-1} h\\, q_i^\\top \\frac{dq_i}{dt}\n$$\nSubstituting the semi-discrete scheme, $\\frac{dq_i}{dt} = -\\frac{1}{h}(F_{i+\\frac{1}{2}} - F_{i-\\frac{1}{2}})$, we obtain:\n$$\n\\frac{dE}{dt} = \\sum_{i=0}^{N-1} h\\, q_i^\\top \\left( -\\frac{1}{h} (F_{i+\\frac{1}{2}} - F_{i-\\frac{1}{2}}) \\right) = - \\sum_{i=0}^{N-1} q_i^\\top (F_{i+\\frac{1}{2}} - F_{i-\\frac{1}{2}})\n$$\nWe can split the sum and perform a summation by parts by re-indexing the second term.\n$$\n\\frac{dE}{dt} = - \\sum_{i=0}^{N-1} q_i^\\top F_{i+\\frac{1}{2}} + \\sum_{i=0}^{N-1} q_i^\\top F_{i-\\frac{1}{2}}\n$$\nLet $j = i-1$ in the second sum. This gives $\\sum_{j=-1}^{N-2} q_{j+1}^\\top F_{j+\\frac{1}{2}}$. Due to periodic boundary conditions, the interface at $j=-1+\\frac{1}{2}$ is identical to the interface at $N-1+\\frac{1}{2}$, and state $q_{j+1}$ for $j=-1$ is $q_0$. The sum over periodic indices from $j=0$ to $N-1$ is equivalent. Thus, $\\sum_{i=0}^{N-1} q_i^\\top F_{i-\\frac{1}{2}} = \\sum_{i=0}^{N-1} q_{i+1}^\\top F_{i+\\frac{1}{2}}$. Substituting this back:\n$$\n\\frac{dE}{dt} = - \\sum_{i=0}^{N-1} q_i^\\top F_{i+\\frac{1}{2}} + \\sum_{i=0}^{N-1} q_{i+1}^\\top F_{i+\\frac{1}{2}} = \\sum_{i=0}^{N-1} (q_{i+1} - q_i)^\\top F_{i+\\frac{1}{2}}\n$$\nLet $[q]_{i+\\frac{1}{2}} = q_{i+1} - q_i$ denote the jump at the interface. The general expression for the energy rate is:\n$$\n\\frac{dE}{dt} = \\sum_{i=0}^{N-1} [q]_{i+\\frac{1}{2}}^\\top F_{i+\\frac{1}{2}}\n$$\nWe now analyze this expression for each of the three given fluxes.\n\n**1. Central Flux:**\n$F^{\\mathrm{cen}}_{i+\\frac{1}{2}} = \\frac{1}{2} A (q_i + q_{i+1})$.\n$$\n\\frac{dE}{dt}_{\\mathrm{cen}} = \\sum_{i=0}^{N-1} (q_{i+1} - q_i)^\\top \\left( \\frac{1}{2} A (q_i + q_{i+1}) \\right) = \\frac{1}{2} \\sum_{i=0}^{N-1} (q_{i+1}^\\top A q_i + q_{i+1}^\\top A q_{i+1} - q_i^\\top A q_i - q_i^\\top A q_{i+1})\n$$\nThe sum $\\sum_{i=0}^{N-1} (q_{i+1}^\\top A q_{i+1} - q_i^\\top A q_i)$ is a telescoping sum over a periodic domain, which evaluates to zero. Using the symmetry of $A$ ($A=A^\\top$), we have $q_i^\\top A q_{i+1} = (q_{i+1}^\\top A^\\top q_i) = q_{i+1}^\\top A q_i$. Therefore, the remaining two terms also cancel: $q_{i+1}^\\top A q_i - q_i^\\top A q_{i+1} = 0$.\n$$\n\\frac{dE}{dt}_{\\mathrm{cen}} = 0\n$$\nThe central flux scheme is perfectly energy-conserving, mimicking the property of the continuous PDE.\n\n**2. Upwind Flux:**\nThe upwind flux can be expressed as a central flux plus a dissipation term. Let $|A| = A^+ - A^-$. Note that $A = A^+ + A^-$. Then, $A^+ = \\frac{1}{2}(A+|A|)$ and $A^- = \\frac{1}{2}(A-|A|)$.\n$F^{\\mathrm{up}}_{i+\\frac{1}{2}} = A^+ q_i + A^- q_{i+1} = \\frac{1}{2}(A+|A|)q_i + \\frac{1}{2}(A-|A|)q_{i+1} = \\frac{1}{2}A(q_i+q_{i+1}) - \\frac{1}{2}|A|(q_{i+1}-q_i)$.\nSubstituting this into the energy rate formula:\n$$\n\\frac{dE}{dt}_{\\mathrm{up}} = \\sum_{i=0}^{N-1} [q]_{i+\\frac{1}{2}}^\\top \\left( \\frac{1}{2}A(q_i+q_{i+1}) - \\frac{1}{2}|A|[q]_{i+\\frac{1}{2}} \\right)\n$$\nThe central flux part contributes zero, as shown above. We are left with:\n$$\n\\frac{dE}{dt}_{\\mathrm{up}} = - \\frac{1}{2} \\sum_{i=0}^{N-1} [q]_{i+\\frac{1}{2}}^\\top |A| [q]_{i+\\frac{1}{2}}\n$$\nSince $A=Q\\Lambda Q^\\top$ is symmetric, $|A|=Q|\\Lambda|Q^\\top$ is also symmetric. Its eigenvalues, $|\\lambda_j|$, are non-negative. Thus, $|A|$ is a positive semi-definite matrix. For any vector $v$, $v^\\top |A| v \\ge 0$. Consequently, each term in the sum is non-negative, and the total energy rate is non-positive.\n$$\n\\frac{dE}{dt}_{\\mathrm{up}} \\le 0\n$$\nThe upwind flux is energy-dissipative.\n\n**3. Lax–Friedrichs (LF) Flux:**\n$F^{\\mathrm{LF}}_{i+\\frac{1}{2}} = \\frac{1}{2} A (q_i + q_{i+1}) - \\frac{\\alpha}{2} (q_{i+1} - q_i)$. This flux has the same structure as the upwind flux formulation above, with the matrix dissipation $|A|$ replaced by a scalar dissipation $\\alpha I$.\n$$\n\\frac{dE}{dt}_{\\mathrm{LF}} = \\sum_{i=0}^{N-1} [q]_{i+\\frac{1}{2}}^\\top \\left( \\frac{1}{2}A(q_i+q_{i+1}) - \\frac{\\alpha}{2}[q]_{i+\\frac{1}{2}} \\right)\n$$\nThe central flux part again contributes zero. The remaining term is:\n$$\n\\frac{dE}{dt}_{\\mathrm{LF}} = - \\frac{\\alpha}{2} \\sum_{i=0}^{N-1} [q]_{i+\\frac{1}{2}}^\\top [q]_{i+\\frac{1}{2}} = - \\frac{\\alpha}{2} \\sum_{i=0}^{N-1} \\|q_{i+1}-q_i\\|_2^2\n$$\nGiven that the penalty parameter $\\alpha \\ge 0$ and the squared norm is always non-negative, the energy rate is non-positive.\n$$\n\\frac{dE}{dt}_{\\mathrm{LF}} \\le 0\n$$\nThe Lax–Friedrichs flux is also energy-dissipative.\n\n### Part 2: Construction of the Semi-Discrete Operator $L$\n\nThe semi-discrete scheme for cell $i$ is $\\frac{d}{dt} q_i = -\\frac{1}{h}(F_{i+\\frac{1}{2}} - F_{i-\\frac{1}{2}})$. Using the general two-point flux form $F_{j+\\frac{1}{2}} = K_L q_j + K_R q_{j+1}$, we substitute for both fluxes:\n$$\n\\frac{d}{dt} q_i = -\\frac{1}{h} \\left( (K_L q_i + K_R q_{i+1}) - (K_L q_{i-1} + K_R q_i) \\right)\n$$\nRearranging the terms based on their cell index gives:\n$$\n\\frac{d}{dt} q_i = \\frac{1}{h} K_L q_{i-1} + \\frac{1}{h}(K_R - K_L) q_i - \\frac{1}{h} K_R q_{i+1}\n$$\nLet the global state vector be $Q = [q_0^\\top, q_1^\\top, \\dots, q_{N-1}^\\top]^\\top \\in \\mathbb{R}^{mN}$. The system is $\\frac{d}{dt} Q = L Q$. The matrix $L$ is a block matrix of size $mN \\times mN$, where each block is an $m \\times m$ matrix. The update rule for $q_i$ depends on $q_{i-1}$, $q_i$, and $q_{i+1}$, which defines the block structure of $L$. Let us define the blocks:\n$P = \\frac{1}{h} K_L$ (pre-diagonal), $C = \\frac{1}{h} (K_R - K_L)$ (diagonal), and $D = -\\frac{1}{h} K_R$ (post-diagonal).\nThe system for $i \\in \\{1, \\dots, N-2\\}$ is $\\frac{d}{dt} q_i = P q_{i-1} + C q_i + D q_{i+1}$.\nFor the boundary cells, we use periodic indexing ($q_N = q_0, q_{-1} = q_{N-1}$):\nFor $i=0$: $\\frac{d}{dt} q_0 = P q_{N-1} + C q_0 + D q_1$.\nFor $i=N-1$: $\\frac{d}{dt} q_{N-1} = P q_{N-2} + C q_{N-1} + D q_0$.\nThis leads to a block-circulant matrix $L$:\n$$\nL =\n\\begin{pmatrix}\nC & D & 0 & \\dots & 0 & P \\\\\nP & C & D & \\dots & 0 & 0 \\\\\n0 & P & C & \\ddots & & \\vdots \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & \\ddots & 0 \\\\\n0 & & \\ddots & P & C & D \\\\\nD & 0 & \\dots & 0 & P & C\n\\end{pmatrix}\n$$\nThe matrices $K_L$ and $K_R$ for each flux are:\n- **Central:** $K_L^{\\mathrm{cen}} = \\frac{1}{2} A$, $K_R^{\\mathrm{cen}} = \\frac{1}{2} A$.\n- **Upwind:** $K_L^{\\mathrm{up}} = A^+$, $K_R^{\\mathrm{up}} = A^-$.\n- **Lax–Friedrichs:** $K_L^{\\mathrm{LF}} = \\frac{1}{2}(A + \\alpha I)$, $K_R^{\\mathrm{LF}} = \\frac{1}{2}(A - \\alpha I)$.\n\n### Part 3: Motivation for DG Design from Energy Behavior\n\nThe design of numerical fluxes in Discontinuous Galerkin (and Finite Volume) methods is fundamentally motivated by the need to ensure numerical stability while retaining accuracy. The behavior of the discrete energy, analyzed in Part 1, provides a direct path to understanding this trade-off.\n\nThe continuous linear hyperbolic system $q_t + A q_x = 0$ with a symmetric coefficient matrix $A$ conserves the total energy $\\frac{1}{2} \\int q^\\top q \\, dx$. A numerical scheme that perfectly mimics this conservation is not necessarily ideal.\n\n1.  **Central Flux (No Dissipation):** The central flux leads to $\\frac{dE}{dt} = 0$, exactly preserving energy. While this seems desirable as it matches the physics, it means the scheme has no mechanism to damp numerical errors or instabilities. When approximating solutions with sharp gradients or discontinuities, all centered schemes (including this one) are prone to producing spurious, high-frequency oscillations (Gibbs phenomenon) that can grow and destroy the solution. The eigenvalues of the resulting operator $L$ are purely imaginary, corresponding to non-dissipative wave propagation, which offers no damping for oscillations.\n\n2.  **Upwind Flux (Characteristic Dissipation):** The upwind flux results in $\\frac{dE}{dt} \\le 0$, making the scheme provably stable in the energy norm. The crucial insight is the form of the dissipation: $- \\frac{1}{2} \\sum [q]^\\top |A| [q]$. The dissipation acts only where there are jumps or sharp gradients ($[q] \\neq 0$), and its strength is modulated by $|A|$. The matrix $|A| = Q|\\Lambda|Q^\\top$ is derived from the characteristic decomposition of $A$, meaning it automatically scales the dissipation according to the wave speeds ($|\\lambda_j|$) of the system. This is a form of \"intelligent\" or \"physical\" dissipation: it is active where needed (at jumps), acts anisotropically respecting the characteristic structure of the PDE, and provides the necessary stability to handle shocks and discontinuities robustly. This is a central motivation for upwind-based DG schemes: they provide stability by adding a minimal, physically-motivated amount of numerical dissipation.\n\n3.  **Lax–Friedrichs Flux (Tunable Scalar Dissipation):** The LF flux also ensures stability via $\\frac{dE}{dt} \\le 0$. Its dissipation term, $- \\frac{\\alpha}{2} \\sum \\|[q]\\|_2^2$, is simpler. It is isotropic (scaled by $\\alpha I$ instead of $|A|$) and applies the same amount of damping to all solution components. The user must choose $\\alpha$ large enough to damp the fastest waves in the system (typically $\\alpha \\ge \\max_j |\\lambda_j|$). While this guarantees stability and is easier to implement (no eigendecomposition needed), it is often a \"brute force\" approach. It can be overly dissipative for multi-component systems with a wide range of wave speeds, smearing out features that the upwind flux would preserve.\n\nIn summary, the ability to design numerical fluxes that control the energy behavior is a cornerstone of the DG method. It allows for a spectrum of schemes ranging from non-dissipative (central) to physically-motivated dissipation (upwind) and simple, robust dissipation (LF). This deliberate introduction of numerical dissipation at element interfaces is the key mechanism that enables DG methods to be stable even for high-order approximations of solutions with discontinuities.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef solve():\n    \"\"\"\n    Main solver function to process test cases and generate results.\n    \"\"\"\n    test_cases = [\n        # Case 1: m=1, L=1, N=8, A=[1], q=sin(2*pi*x), alpha=1.1\n        {\n            \"m\": 1, \"L\": 1.0, \"N\": 8, \"A\": np.array([[1.0]]),\n            \"q_func\": lambda x: np.array([np.sin(2 * np.pi * x)]),\n            \"alpha\": 1.1\n        },\n        # Case 2: m=2, L=1, N=10, A=rotation, q=mixed sin/cos, alpha=2.5\n        {\n            \"m\": 2, \"L\": 1.0, \"N\": 10,\n            \"A_func\": lambda: (\n                lambda theta: np.array([\n                    [np.cos(theta), -np.sin(theta)],\n                    [np.sin(theta), np.cos(theta)]\n                ])\n            )(0.7) @ np.diag([2.0, -1.0]) @ (\n                lambda theta: np.array([\n                    [np.cos(theta), -np.sin(theta)],\n                    [np.sin(theta), np.cos(theta)]\n                ])\n            )(0.7).T,\n            \"q_func\": lambda x: np.array([np.sin(2 * np.pi * x), np.cos(4 * np.pi * x)]),\n            \"alpha\": 2.5\n        },\n        # Case 3: m=1, L=1, N=8, A=[0], q=sin(2*pi*x), alpha=0.5\n        {\n            \"m\": 1, \"L\": 1.0, \"N\": 8, \"A\": np.array([[0.0]]),\n            \"q_func\": lambda x: np.array([np.sin(2 * np.pi * x)]),\n            \"alpha\": 0.5\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        m, L, N = case[\"m\"], case[\"L\"], case[\"N\"]\n        h = L / N\n        alpha = case[\"alpha\"]\n\n        if \"A\" in case:\n            A = case[\"A\"]\n        else:\n            A = case[\"A_func\"]()\n\n        # Generate state vector q\n        x_centers = (np.arange(N) + 0.5) * h\n        q = np.array([case[\"q_func\"](x) for x in x_centers]) # Shape (N, m)\n\n        # Precompute jumps [q] = q_{i+1} - q_i\n        q_plus_1 = np.roll(q, -1, axis=0)\n        jumps = q_plus_1 - q\n\n        # --- Eigendecomposition for Upwind Flux ---\n        # Since A is symmetric, use eigh for stability/efficiency\n        eigvals, Q_mat = eigh(A)\n        Q_inv = Q_mat.T\n\n        # A+ and A-\n        A_plus = Q_mat @ np.diag(np.maximum(eigvals, 0)) @ Q_inv\n        A_minus = Q_mat @ np.diag(np.minimum(eigvals, 0)) @ Q_inv\n        \n        # |A|\n        abs_A = Q_mat @ np.diag(np.abs(eigvals)) @ Q_inv\n\n        # Flux definitions (K_L, K_R)\n        flux_defs = {\n            \"central\": (0.5 * A, 0.5 * A),\n            \"upwind\": (A_plus, A_minus),\n            \"lf\": (0.5 * (A + alpha * np.eye(m)), 0.5 * (A - alpha * np.eye(m)))\n        }\n\n        case_results = []\n\n        # --- Energy Rates ---\n        # Central\n        er_cen = 0.0\n        case_results.append(er_cen)\n\n        # Upwind\n        er_up = -0.5 * np.sum((jumps @ abs_A) * jumps)\n        case_results.append(er_up)\n\n        # Lax-Friedrichs\n        er_lf = -alpha / 2.0 * np.sum(jumps**2)\n        case_results.append(er_lf)\n\n        # --- Spectral Radii ---\n        for flux_type in [\"central\", \"upwind\", \"lf\"]:\n            K_L, K_R = flux_defs[flux_type]\n\n            # Construct system matrix L\n            L_mat = np.zeros((m * N, m * N))\n            P = (1 / h) * K_L\n            C = (1 / h) * (K_R - K_L)\n            D = -(1 / h) * K_R\n            \n            for i in range(N):\n                # Diagonal block\n                L_mat[i*m:(i+1)*m, i*m:(i+1)*m] = C\n                \n                # Pre-diagonal block (with periodicity)\n                j_prev = (i - 1 + N) % N\n                L_mat[i*m:(i+1)*m, j_prev*m:(j_prev+1)*m] = P\n                \n                # Post-diagonal block (with periodicity)\n                j_next = (i + 1) % N\n                L_mat[i*m:(i+1)*m, j_next*m:(j_next+1)*m] = D\n\n            if N > 1: # Re-arrange for block circulant structure (more standard representation)\n                L_mat_circ = np.zeros((m * N, m * N))\n                for i in range(N):\n                    L_mat_circ[i*m:(i+1)*m, i*m:(i+1)*m] = C\n                    j_prev = (i - 1 + N) % N\n                    L_mat_circ[i*m:(i+1)*m, j_prev*m:(j_prev+1)*m] = P if i != 0 else L_mat_circ[i*m:(i+1)*m, j_prev*m:(j_prev+1)*m]\n                    j_next = (i + 1) % N\n                    L_mat_circ[i*m:(i+1)*m, j_next*m:(j_next+1)*m] = D if i != N-1 else L_mat_circ[i*m:(i+1)*m, j_next*m:(j_next+1)*m]\n                #corner blocks\n                L_mat_circ[0:m, (N-1)*m:N*m] = P\n                L_mat_circ[(N-1)*m:N*m, 0:m] = D\n            else: # N=1 case, d/dt q0 = 0\n                L_mat_circ = C + P + D # which should be 0 from K_L+K_R-K_L-K_R\n            \n            # The manual construction used above is correct, a second thought on rearranging proved it is redundant.\n            # I will use the first construction which is more direct.\n\n            eigvals_L = np.linalg.eigvals(L_mat)\n            spectral_radius = np.max(np.abs(eigvals_L))\n            case_results.append(spectral_radius)\n        \n        all_results.extend(case_results)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```", "id": "3401252"}, {"introduction": "While high-order accuracy is a major advantage of DG methods for smooth solutions, it can lead to non-physical oscillations near sharp features like shock waves. A key strength of the DG framework is its flexibility to overcome this challenge through solution limiting. This exercise [@problem_id:3401228] provides a concrete example of applying a `minmod` limiter, a technique that detects and suppresses spurious oscillations. By working through this calculation, you will see how the method can locally adjust the solution to enforce physical properties like monotonicity, a crucial feature that makes DG a powerful tool for complex simulations in science and engineering.", "problem": "Consider the one-dimensional scalar conservation law $u_{t} + f(u)_{x} = 0$ on a uniform mesh with cells $I_{j} = [x_{j - \\frac{1}{2}}, x_{j + \\frac{1}{2}}]$ of width $h$, discretized by a Discontinuous Galerkin (DG) method with polynomial degree $p=1$. In each cell $I_{j}$, the approximate solution is represented in a modal Legendre basis mapped from the reference interval $[-1,1]$ via $x = x_{j} + \\frac{h}{2}\\,\\xi$, as\n$$\nu_{h}(x)\\big|_{I_{j}} = \\bar{u}_{j} + \\hat{u}_{j,1}\\,\\xi,\n$$\nwhere $\\bar{u}_{j}$ is the cell average, $\\hat{u}_{j,1}$ is the linear modal coefficient associated with the reference basis function $\\xi$, and $\\xi \\in [-1,1]$.\n\nTo enhance robustness in the presence of a discontinuity, a slope limiter based on the minmod function is applied elementwise to the physical slope $s_{j}$, defined for $p=1$ as the constant derivative within the cell,\n$$\ns_{j} := \\frac{\\mathrm{d}u_{h}}{\\mathrm{d}x}\\bigg|_{I_{j}}.\n$$\nThe limited slope is defined by\n$$\ns_{j}^{\\mathrm{lim}} = \\minmod\\!\\Big(s_{j}, s_{j}^{-}, s_{j}^{+}\\Big),\n$$\nwhere\n$$\ns_{j}^{-} := \\frac{\\bar{u}_{j} - \\bar{u}_{j-1}}{h}, \\qquad s_{j}^{+} := \\frac{\\bar{u}_{j+1} - \\bar{u}_{j}}{h},\n$$\nand the minmod function is\n$$\n\\minmod(a_{1},\\dots,a_{n}) = \\begin{cases}\n\\operatorname{sign}(a_{1})\\,\\min\\{|a_{1}|,\\dots,|a_{n}|\\}, & \\text{if } \\operatorname{sign}(a_{1}) = \\cdots = \\operatorname{sign}(a_{n}),\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\n\nYou are given a test configuration with a discontinuity to the right of cell $I_{j}$:\n- Mesh width $h = 0.5$,\n- Cell averages $\\bar{u}_{j-1} = 1.9$, $\\bar{u}_{j} = 2.0$, $\\bar{u}_{j+1} = -1.8$,\n- Modal coefficient $\\hat{u}_{j,1} = 0.15$.\n\nUsing only these data and the definitions above, compute the limited slope $s_{j}^{\\mathrm{lim}}$. Express your final answer exactly as a single real number. No rounding is required. Briefly explain the motivation for applying this limiter in the Discontinuous Galerkin framework in the presence of a discontinuity, making clear which properties of the method are being preserved by this operation, but ensure that your final reported value is only the requested limited slope.", "solution": "The problem is valid. It is a well-posed, scientifically grounded problem from the field of numerical analysis for partial differential equations, specifically concerning spectral and discontinuous Galerkin (DG) methods. It provides all necessary definitions and data to compute a unique solution.\n\nThe task is to compute the limited slope $s_{j}^{\\mathrm{lim}}$ for the given data and DG method setup. The limited slope is defined as\n$$\ns_{j}^{\\mathrm{lim}} = \\minmod\\!\\Big(s_{j}, s_{j}^{-}, s_{j}^{+}\\Big).\n$$\nTo compute this, we must first determine the values of the three arguments of the `minmod` function: the physical slope within cell $I_j$, $s_j$, and the slopes estimated from the neighboring cell averages, $s_j^-$ and $s_j^+$.\n\nFirst, we calculate the physical slope $s_j$ within cell $I_j$. The definition is given as the spatial derivative of the approximate solution $u_h(x)$ within the cell:\n$$\ns_{j} := \\frac{\\mathrm{d}u_{h}}{\\mathrm{d}x}\\bigg|_{I_{j}}.\n$$\nThe solution is represented in terms of the reference coordinate $\\xi \\in [-1, 1]$ as $u_{h}(x)\\big|_{I_{j}} = \\bar{u}_{j} + \\hat{u}_{j,1}\\,\\xi$. The mapping between the physical coordinate $x$ and the reference coordinate $\\xi$ is $x = x_{j} + \\frac{h}{2}\\,\\xi$. We use the chain rule to find the derivative:\n$$\n\\frac{\\mathrm{d}u_{h}}{\\mathrm{d}x} = \\frac{\\mathrm{d}u_{h}}{\\mathrm{d}\\xi} \\frac{\\mathrm{d}\\xi}{\\mathrm{d}x}.\n$$\nFrom the solution representation, the derivative with respect to $\\xi$ is:\n$$\n\\frac{\\mathrm{d}u_{h}}{\\mathrm{d}\\xi} = \\frac{\\mathrm{d}}{\\mathrm{d}\\xi} (\\bar{u}_{j} + \\hat{u}_{j,1}\\,\\xi) = \\hat{u}_{j,1}.\n$$\nFrom the coordinate mapping, we find $\\frac{\\mathrm{d}x}{\\mathrm{d}\\xi} = \\frac{h}{2}$, which implies $\\frac{\\mathrm{d}\\xi}{\\mathrm{d}x} = \\frac{2}{h}$.\nCombining these results, the physical slope $s_j$ is:\n$$\ns_{j} = \\hat{u}_{j,1} \\cdot \\frac{2}{h}.\n$$\nSubstituting the given values $h = 0.5$ and $\\hat{u}_{j,1} = 0.15$:\n$$\ns_{j} = 0.15 \\cdot \\frac{2}{0.5} = 0.15 \\cdot 4 = 0.6.\n$$\n\nNext, we calculate the slopes $s_j^-$ and $s_j^+$ based on the neighboring cell averages. The definitions are:\n$$\ns_{j}^{-} := \\frac{\\bar{u}_{j} - \\bar{u}_{j-1}}{h}, \\qquad s_{j}^{+} := \\frac{\\bar{u}_{j+1} - \\bar{u}_{j}}{h}.\n$$\nUsing the provided cell averages $\\bar{u}_{j-1} = 1.9$, $\\bar{u}_{j} = 2.0$, $\\bar{u}_{j+1} = -1.8$, and mesh width $h = 0.5$:\n$$\ns_{j}^{-} = \\frac{2.0 - 1.9}{0.5} = \\frac{0.1}{0.5} = 0.2.\n$$\n$$\ns_{j}^{+} = \\frac{-1.8 - 2.0}{0.5} = \\frac{-3.8}{0.5} = -7.6.\n$$\n\nFinally, we compute the limited slope $s_{j}^{\\mathrm{lim}}$ using the `minmod` function with the three calculated slopes: $s_{j} = 0.6$, $s_{j}^{-} = 0.2$, and $s_{j}^{+} = -7.6$:\n$$\ns_{j}^{\\mathrm{lim}} = \\minmod(0.6, 0.2, -7.6).\n$$\nThe `minmod` function is defined as:\n$$\n\\minmod(a_{1},\\dots,a_{n}) = \\begin{cases}\n\\operatorname{sign}(a_{1})\\,\\min\\{|a_{1}|,\\dots,|a_{n}|\\}, & \\text{if } \\operatorname{sign}(a_{1}) = \\cdots = \\operatorname{sign}(a_{n}),\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nWe check the signs of the arguments: $\\operatorname{sign}(0.6) = +1$, $\\operatorname{sign}(0.2) = +1$, and $\\operatorname{sign}(-7.6) = -1$. Since the signs of the three arguments are not all identical, the second case of the `minmod` definition applies.\nTherefore, the limited slope is:\n$$\ns_{j}^{\\mathrm{lim}} = 0.\n$$\n\nThe motivation for applying such a limiter is to ensure numerical stability and physical realism when solving conservation laws with high-order methods like DG. High-order schemes can accurately capture smooth solutions but are prone to generating spurious, non-physical oscillations (Gibbs phenomenon) near discontinuities such as shocks. These oscillations can lead to non-physical states (e.g., negative density) and cause the numerical simulation to fail.\n\nThe limiter's function is to detect regions of incipient oscillations and locally modify the solution to suppress them. The key property being preserved or enforced by the limiter is **monotonicity**. A scheme is monotonicity-preserving if it does not create new local maxima or minima in the solution. By comparing the slope within a cell ($s_j$) to the finite difference slopes from its neighbors ($s_j^-$ and $s_j^+$), the limiter assesses whether the solution is monotonic. In this problem, the cell-average values $\\bar{u}_{j-1} = 1.9$, $\\bar{u}_{j} = 2.0$, and $\\bar{u}_{j+1} = -1.8$ indicate a local maximum at cell $I_j$. The mixture of positive and negative slope arguments for the `minmod` function is a direct consequence of this non-monotonic profile. By returning a value of $0$, the limiter effectively eliminates the slope in cell $I_j$, reducing the polynomial representation to a constant, $u_h(x)|_{I_j} = \\bar{u}_j$. This locally reduces the scheme's accuracy to first order, which is inherently non-oscillatory, thus preserving monotonicity at the cost of local accuracy. This process is crucial for making high-order DG methods robust for problems involving shocks.", "answer": "$$\\boxed{0}$$", "id": "3401228"}]}