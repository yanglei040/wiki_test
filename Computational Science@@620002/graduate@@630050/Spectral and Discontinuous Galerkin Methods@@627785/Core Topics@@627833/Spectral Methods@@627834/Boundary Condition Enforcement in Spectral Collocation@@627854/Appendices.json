{"hands_on_practices": [{"introduction": "The foundation of spectral collocation is the ability to accurately differentiate a function represented by its values at a set of grid points. This is accomplished using a spectral differentiation matrix, a dense matrix whose entries are derived from the properties of the underlying basis polynomials. This first practice is a fundamental exercise where you will derive the explicit formulas for the Chebyshev first- and second-derivative matrices, providing a crucial look 'under the hood' of spectral methods. [@problem_id:3367639]", "problem": "Consider the Spectral Collocation Method (SCM) on the interval $[-1,1]$ using Chebyshev-Gauss-Lobatto (CGL) nodes. Let $N \\in \\mathbb{N}$ and define the nodes by $x_{j} = \\cos\\!\\left(\\frac{\\pi j}{N}\\right)$ for $j = 0,1,\\dots,N$. Let $u:[-1,1] \\to \\mathbb{R}$ be sufficiently smooth and let $p_{N}(x)$ denote its unique degree-$N$ Lagrange interpolant at the CGL nodes, so that $p_{N}(x) = \\sum_{j=0}^{N} u(x_{j}) \\ell_{j}(x)$ with Lagrange basis functions $\\ell_{j}(x)$ satisfying $\\ell_{j}(x_{i}) = \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta. The pseudospectral first- and second-derivative matrices $D$ and $D^{(2)}$ are defined entrywise by $D_{ij} = \\ell_{j}'(x_{i})$ and $D_{ij}^{(2)} = \\ell_{j}''(x_{i})$, so that $\\left(p_{N}'(x_{i})\\right)_{i=0}^{N} = D \\left(u(x_{j})\\right)_{j=0}^{N}$ and $\\left(p_{N}''(x_{i})\\right)_{i=0}^{N} = D^{(2)} \\left(u(x_{j})\\right)_{j=0}^{N}$.\n\nStarting from the barycentric form of polynomial interpolation and the defining properties of the CGL nodes, derive closed-form analytic expressions for the entries of the first-derivative pseudospectral matrix $D$, explicitly distinguishing diagonal and off-diagonal formulas and making clear how the endpoint rows ($i=0$ and $i=N$) differ from interior rows ($i=1,\\dots,N-1$). Then express the second-derivative pseudospectral matrix $D^{(2)}$ in terms of $D$ and give explicit formulas for its diagonal and off-diagonal entries at the CGL nodes.\n\nYour final answer must be a single analytic expression containing the entrywise formulas for $D$ and $D^{(2)}$. No numerical evaluation is required. If you provide more than one expression, present them as a single row using the LaTeX $\\mathrm{pmatrix}$ environment as specified. No units are involved.", "solution": "The problem statement is a valid and well-posed request for the derivation of standard formulas in the theory of spectral methods. I will now proceed with the derivation.\n\nLet the Chebyshev-Gauss-Lobatto (CGL) nodes be denoted by $x_j = \\cos\\left(\\frac{\\pi j}{N}\\right)$ for $j = 0, 1, \\dots, N$. The unique degree-$N$ polynomial interpolant of a function $u(x)$ at these nodes is $p_N(x) = \\sum_{j=0}^{N} u(x_j) \\ell_j(x)$, where $\\ell_j(x)$ are the Lagrange basis polynomials. The first- and second-derivative pseudospectral matrices, $D$ and $D^{(2)}$, have entries $D_{ij} = \\ell_j'(x_i)$ and $D^{(2)}_{ij} = \\ell_j''(x_i)$, respectively.\n\n**Part 1: The First-Derivative Matrix $D$**\n\nThe Lagrange basis polynomial $\\ell_j(x)$ can be written in terms of the nodal polynomial $L(x) = \\prod_{k=0}^{N} (x-x_k)$ as $\\ell_j(x) = \\frac{L(x)}{(x-x_j) L'(x_j)}$. Differentiating with respect to $x$ yields:\n$$ \\ell_j'(x) = \\frac{L'(x)(x-x_j) - L(x)}{(x-x_j)^2 L'(x_j)} $$\nTo find the off-diagonal entries $D_{ij}$ for $i \\neq j$, we evaluate $\\ell_j'(x)$ at $x=x_i$. Since $x_i$ is a node, $L(x_i) = 0$.\n$$ D_{ij} = \\ell_j'(x_i) = \\frac{L'(x_i)(x_i-x_j)}{(x_i-x_j)^2 L'(x_j)} = \\frac{L'(x_i)}{L'(x_j)(x_i - x_j)}, \\quad i \\neq j $$\nThe terms $L'(x_j) = \\prod_{k \\neq j} (x_j - x_k)$ are related to the barycentric weights $w_j$. For CGL nodes, the barycentric weights are given by $w_j = (-1)^j \\bar{c}_j^{-1}$, where $\\bar{c}_0 = 2$, $\\bar{c}_N = 2$, and $\\bar{c}_j = 1$ for $j = 1, \\dots, N-1$. It follows that the off-diagonal entries are:\n$$ D_{ij} = \\frac{\\bar{c}_i}{\\bar{c}_j} \\frac{(-1)^{i+j}}{x_i - x_j}, \\quad i \\neq j $$\nFor the diagonal entries $D_{ii}$, we use the fact that the derivative of a constant function is zero, which implies that the sum of each row of $D$ must be zero.\n$$ D_{ii} = - \\sum_{j \\neq i} D_{ij} $$\nEvaluating this sum gives the following compact formulas for the diagonal entries:\n\\begin{itemize}\n    \\item For interior nodes, $i=1, \\dots, N-1$: $D_{ii} = -\\frac{x_i}{2(1-x_i^2)}$\n    \\item For the endpoints, $i=0$ and $i=N$: $D_{00} = \\frac{2N^2+1}{6}$ and $D_{NN} = -\\frac{2N^2+1}{6}$\n\\end{itemize}\n\n**Part 2: The Second-Derivative Matrix $D^{(2)}$**\n\nThe second derivative of the interpolating polynomial $p_N(x)$ at the nodes can be found by applying the differentiation operator twice. If $\\mathbf{u} = (u(x_0), \\dots, u(x_N))^T$ is the vector of function values at the nodes, the vector of first derivatives is $\\mathbf{p}_N' = D\\mathbf{u}$. The vector of second derivatives is then $\\mathbf{p}_N'' = D\\mathbf{p}_N' = D(D\\mathbf{u}) = D^2\\mathbf{u}$.\nThus, the second-derivative matrix is the square of the first-derivative matrix:\n$$ D^{(2)} = D^2 $$\nThe entrywise formula for $D^{(2)}$ is $D^{(2)}_{ij} = \\sum_{k=0}^{N} D_{ik} D_{kj}$. While it is possible to derive explicit formulas for each entry, they are algebraically complex. The most direct and computationally reliable method is to compute the matrix product $D^2$. For completeness, we state the definitional formulas derived from the matrix product.\n\nFor the off-diagonal entries ($i \\neq j$), a derivation leads to:\n$$ D^{(2)}_{ij} = 2D_{ij} \\left( D_{ii} - \\frac{1}{x_i-x_j} \\right), \\quad i \\neq j $$\n\nFor the diagonal entries $D^{(2)}_{ii}$, we compute:\n$$ D^{(2)}_{ii} = \\sum_{k=0}^{N} D_{ik} D_{ki} = D_{ii}^2 + \\sum_{k \\neq i} D_{ik}D_{ki} $$\nThe terms in the sum are:\n$$ D_{ik}D_{ki} = \\left(\\frac{\\bar{c}_i}{\\bar{c}_k} \\frac{(-1)^{i+k}}{x_i-x_k}\\right) \\left(\\frac{\\bar{c}_k}{\\bar{c}_i} \\frac{(-1)^{k+i}}{x_k-x_i}\\right) = -\\frac{1}{(x_i-x_k)^2} $$\nThis yields the definitional formula:\n$$ D^{(2)}_{ii} = D_{ii}^2 - \\sum_{k \\neq i} \\frac{1}{(x_i-x_k)^2} $$\nWhile compact closed-form expressions for this sum exist, they are algebraically complex. For practical computation, the matrix product $D^2$ is the most direct and reliable method.\n\n**Summary of Formulas**\n\nFirst-derivative matrix $D$:\n$$\nD_{ij} =\n\\begin{cases}\n\\frac{\\bar{c}_i}{\\bar{c}_j} \\frac{(-1)^{i+j}}{x_i - x_j}, & i \\neq j \\\\\n-\\frac{x_i}{2(1-x_i^2)}, & i=j, \\quad i \\in \\{1,\\dots,N-1\\} \\\\\n\\frac{2N^2+1}{6}, & i=j=0 \\\\\n-\\frac{2N^2+1}{6}, & i=j=N\n\\end{cases}\n$$\nwhere $x_j = \\cos(\\frac{\\pi j}{N})$ and $\\bar{c}_0 = \\bar{c}_N = 2$, $\\bar{c}_j = 1$ for $j \\in \\{1,\\dots,N-1\\}$.\n\nSecond-derivative matrix $D^{(2)} = D^2$:\n$$\nD^{(2)}_{ij} =\n\\begin{cases}\n2D_{ij} \\left( D_{ii} - \\frac{1}{x_i-x_j} \\right), & i \\neq j \\\\\nD_{ii}^2 - \\sum_{k \\neq i} \\frac{1}{(x_i-x_k)^2}, & i = j\n\\end{cases}\n$$\nThese formulas express the matrix entries analytically.", "answer": "$$ \\boxed{ \\begin{pmatrix} D_{ij} = \\begin{cases} \\frac{\\bar{c}_i}{\\bar{c}_j} \\frac{(-1)^{i+j}}{x_i - x_j}, & i \\neq j \\\\ -\\frac{x_i}{2(1-x_i^2)}, & i=j \\in \\{1,\\dots,N-1\\} \\\\ \\frac{2N^2+1}{6}, & i=j=0 \\\\ -\\frac{2N^2+1}{6}, & i=j=N \\end{cases} & D^{(2)}_{ij} = \\left(D^2\\right)_{ij} = \\begin{cases} 2D_{ij}\\left(D_{ii} - \\frac{1}{x_i-x_j}\\right), & i \\neq j \\\\ D_{ii}^2 - \\sum_{k=0, k \\neq i}^N \\frac{1}{(x_i-x_k)^2}, & i = j \\end{cases} \\end{pmatrix} } $$", "id": "3367639"}, {"introduction": "With differentiation matrices in hand, the most intuitive way to enforce a Dirichlet boundary condition like $u(1)=0$ is to simply replace the corresponding row in the linear system with an equation that sets the boundary value to zero. While straightforward, this 'strong imposition' or 'collocation' method can introduce severe numerical instabilities, a fact not immediately obvious from the setup. This numerical experiment will guide you to discover the degradation in the operator's condition number and explore how the non-normal spectrum relates to the clustering of Chebyshev points near the boundaries. [@problem_id:3367677]", "problem": "Consider the boundary value problem on the interval $\\left[-1,1\\right]$ governed by the linear differential operator $\\mathcal{L}u = -u'' + u$ with homogeneous Dirichlet boundary conditions $u(-1) = 0$ and $u(1) = 0$. The goal is to analyze, in the spectral collocation framework on Chebyshev–Gauss–Lobatto points, how enforcing Dirichlet boundary conditions by modifying the first and last rows of the discrete operator affects the spectrum and the two-norm condition number of the discrete operator as the polynomial degree increases. The analysis must be related to the clustering of Chebyshev points near the endpoints and its numerical impact.\n\nBegin from the following fundamental bases:\n- The definition of Chebyshev–Gauss–Lobatto points $x_j = \\cos\\left(\\frac{\\pi j}{N}\\right)$ for $j = 0,1,\\dots,N$, where $N$ is the polynomial degree and the number of collocation points is $N+1$.\n- The definition of the spectral first differentiation matrix $D$ at Chebyshev–Gauss–Lobatto points as the matrix that maps nodal values to approximations of their first derivatives, constructed from the barycentric-like weights prescribed by Chebyshev theory to satisfy exactness for polynomials up to degree $N$.\n- The discrete approximation of the second derivative via the squared first differentiation matrix, namely $D^{(2)} = D D$.\n- The discrete operator approximation of $\\mathcal{L}$ at collocation points as $A_{\\mathrm{full}} = -D^{(2)} + I$, where $I$ is the identity matrix of size $N+1$.\n- Enforcement of homogeneous Dirichlet boundary conditions by row modification: replace the first and last rows of $A_{\\mathrm{full}}$ with unit rows that enforce $u(-1)=0$ and $u(1)=0$, respectively, yielding $A_{\\mathrm{bc}}$.\n- Enforcement of homogeneous Dirichlet boundary conditions by interior reduction: extract the interior block of $A_{\\mathrm{full}}$ corresponding to indices $1,2,\\dots,N-1$ to form $A_{\\mathrm{int}}$, which acts on interior unknowns with boundary values pinned to zero.\n\nYou must:\n1. Construct the Chebyshev–Gauss–Lobatto grid for each prescribed $N$ and build the spectral first differentiation matrix $D$ using the canonical Chebyshev weights and the difference of node coordinates, with diagonal entries defined by the requirement that each row of $D$ sums to zero (a consequence of differentiating constants to zero). Do not use shortcut formulas not derived from these bases.\n2. Form the discrete second derivative operator $D^{(2)} = D D$.\n3. Assemble the discrete operator $A_{\\mathrm{full}} = -D^{(2)} + I$.\n4. Enforce homogeneous Dirichlet boundary conditions using two approaches:\n   - Row modification: construct $A_{\\mathrm{bc}}$ by replacing the first and last rows of $A_{\\mathrm{full}}$ by rows that enforce $u(-1)=0$ and $u(1)=0$, respectively, i.e., set the first row to $[1,0,\\dots,0]$ and the last row to $[0,\\dots,0,1]$.\n   - Interior reduction: construct $A_{\\mathrm{int}}$ as the $(N-1)\\times(N-1)$ matrix obtained by extracting the interior block of $A_{\\mathrm{full}}$ corresponding to rows and columns indexed by $1$ through $N-1$.\n5. For each matrix $A_{\\mathrm{bc}}$ and $A_{\\mathrm{int}}$, compute:\n   - The two-norm condition number $\\kappa_2$ defined as the ratio of the largest singular value to the smallest singular value.\n   - The spectrum (set of eigenvalues), from which you must extract the smallest and largest real parts of eigenvalues, and the largest absolute value of imaginary parts. These spectral characteristics quantify how row modification alters both the spread and nonnormality of the spectrum relative to the interior reduction.\n6. Explain how the growth of $\\kappa_2$ with $N$ relates to the clustering of Chebyshev collocation points near $x=-1$ and $x=1$, and how row modification influences high-frequency modes localized near the endpoints.\n\nYou must implement a program that returns numeric results for the following test suite of polynomial degrees:\n- $N=6$ (small size to verify baseline behavior),\n- $N=16$ (moderate size),\n- $N=32$ (larger size),\n- $N=64$ (high resolution).\n\nFor each $N$ in the test suite, produce a list with the following nine floating-point values in this order:\n- $\\kappa_2(A_{\\mathrm{bc}})$,\n- $\\kappa_2(A_{\\mathrm{int}})$,\n- $\\kappa_2(A_{\\mathrm{bc}})/\\kappa_2(A_{\\mathrm{int}})$,\n- $\\min\\left(\\Re\\lambda\\left(A_{\\mathrm{bc}}\\right)\\right)$,\n- $\\max\\left(\\Re\\lambda\\left(A_{\\mathrm{bc}}\\right)\\right)$,\n- $\\min\\left(\\Re\\lambda\\left(A_{\\mathrm{int}}\\right)\\right)$,\n- $\\max\\left(\\Re\\lambda\\left(A_{\\mathrm{int}}\\right)\\right)$,\n- $\\max\\left(\\left|\\Im\\lambda\\left(A_{\\mathrm{bc}}\\right)\\right|\\right)$,\n- $\\max\\left(\\left|\\Im\\lambda\\left(A_{\\mathrm{int}}\\right)\\right|\\right)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one $N$ and is itself a list of the nine numbers described above. For example, the output format must be like:\n[[n1_1,n1_2,n1_3,n1_4,n1_5,n1_6,n1_7,n1_8,n1_9],[n2_1,n2_2,n2_3,n2_4,n2_5,n2_6,n2_7,n2_8,n2_9],[n3_1,n3_2,n3_3,n3_4,n3_5,n3_6,n3_7,n3_8,n3_9],[n4_1,n4_2,n4_3,n4_4,n4_5,n4_6,n4_7,n4_8,n4_9]]\n\nAll quantities are dimensionless; no physical units are involved. The final printed output must be exactly one line in the format specified, with all values represented as plain decimal numbers.", "solution": "The user-provided problem is valid. It is a well-posed, scientifically grounded problem in the field of numerical analysis, specifically spectral methods. All necessary definitions and constraints are provided, and there are no contradictions or ambiguities. The problem asks for a quantitative and qualitative analysis of two standard methods for enforcing Dirichlet boundary conditions in a Chebyshev collocation scheme, which is a core topic in the discipline.\n\n### **1. Theoretical Framework**\n\nThe problem concerns the numerical solution of the one-dimensional linear boundary value problem (BVP) on the interval $x \\in [-1, 1]$:\n$$\n\\mathcal{L}u = -u''(x) + u(x) = f(x)\n$$\nwith homogeneous Dirichlet boundary conditions $u(-1) = 0$ and $u(1) = 0$. We will analyze the properties of the discrete operator obtained from a spectral collocation method.\n\n### **2. Spectral Collocation Discretization**\n\nThe foundation of the method is the approximation of the unknown function $u(x)$ by a single global polynomial $p(x)$ of degree $N$. The coefficients of this polynomial are determined by enforcing the differential equation at a set of $N+1$ specific points, known as collocation points.\n\n**2.1. Chebyshev–Gauss–Lobatto (CGL) Grid**\nThe chosen collocation points are the Chebyshev–Gauss–Lobatto (CGL) points, defined as the extrema of the $N$-th degree Chebyshev polynomial of the first kind:\n$$\nx_j = \\cos\\left(\\frac{\\pi j}{N}\\right) \\quad \\text{for } j = 0, 1, \\dots, N\n$$\nThese points include the boundaries $x_0 = 1$ and $x_N = -1$. A critical property of this grid is its non-uniform spacing: the points cluster near the boundaries, with the distance between adjacent points $\\Delta x_j$ scaling as $O(1/N^2)$ near $x=\\pm 1$ and as $O(1/N)$ in the interior of the domain. This clustering is essential for resolving boundary layers and mitigating the Runge phenomenon but has significant implications for the conditioning of numerical operators.\n\n**2.2. Spectral Differentiation Matrix**\nThe derivative of the interpolating polynomial $p(x)$ at the CGL points can be computed via a linear transformation of the function values $\\{u(x_j)\\}_{j=0}^N$. This transformation is represented by the $(N+1) \\times (N+1)$ spectral differentiation matrix, $D$. The entries of this matrix are given by:\n$$\nD_{jk} = \\begin{cases}\n    \\frac{c_j}{c_k} \\frac{(-1)^{j+k}}{x_j - x_k} & \\text{if } j \\neq k \\\\\n    -\\sum_{l=0, l \\neq j}^{N} D_{jl} & \\text{if } j = k\n\\end{cases}\n$$\nwhere the weights $c_j$ are defined as $c_0 = c_N = 2$ and $c_j = 1$ for $j=1, \\dots, N-1$. The diagonal entries are explicitly defined by the property that the derivative of a constant function (a polynomial of degree $0$) is zero, which implies that each row of $D$ must sum to zero.\n\nThe second derivative is approximated by applying the differentiation matrix twice, yielding the second-derivative matrix $D^{(2)} = D^2 = D \\cdot D$. The full discrete operator corresponding to $\\mathcal{L} = -d^2/dx^2 + 1$ is then assembled as:\n$$\nA_{\\mathrm{full}} = -D^2 + I\n$$\nwhere $I$ is the $(N+1) \\times (N+1)$ identity matrix.\n\n### **3. Enforcement of Boundary Conditions**\n\nThe matrix $A_{\\mathrm{full}}$ is singular because it does not yet incorporate the boundary conditions. We analyze two methods to enforce $u(1)=u(x_0)=0$ and $u(-1)=u(x_N)=0$.\n\n**3.1. Method 1: Interior Reduction**\nThis method eliminates the known boundary values from the system of equations. Since $u_0 = 0$ and $u_N = 0$, the equations for the interior points $j=1, \\dots, N-1$ depend only on the interior unknowns $u_1, \\dots, u_{N-1}$. The resulting system is governed by an $(N-1) \\times (N-1)$ matrix, $A_{\\mathrm{int}}$, which is formed by extracting the block of $A_{\\mathrm{full}}$ corresponding to rows and columns $1$ through $N-1$.\n$$\nA_{\\mathrm{int}} = (A_{\\mathrm{full}})_{1:N-1, 1:N-1}\n$$\nThis matrix provides a direct and stable representation of the BVP. Although $A_{\\mathrm{int}}$ is non-symmetric, its eigenvalues are real, positive, and accurately approximate the eigenvalues of the continuous operator $\\mathcal{L}$, which are $\\lambda_k = 1 + (k\\pi/2)^2$ for $k=1, 2, \\dots$. Consequently, its imaginary spectral component is zero (up to floating-point error), and its condition number, while growing with $N$ (typically as $O(N^4)$ for this operator), behaves predictably.\n\n**3.2. Method 2: Row Modification**\nThis approach retains the full $(N+1) \\times (N+1)$ system size. The boundary conditions are enforced by replacing the first and last rows of the matrix $A_{\\mathrm{full}}$ and the corresponding entries in the right-hand-side vector. To solve the homogeneous problem or analyze the operator spectrum, we consider the modified matrix $A_{\\mathrm{bc}}$:\n$$\nA_{\\mathrm{bc}} =\n\\begin{pmatrix}\n1 & 0 & \\dots & 0 & 0 \\\\\n(A_{\\mathrm{full}})_{1,0} & (A_{\\mathrm{full}})_{1,1} & \\dots & (A_{\\mathrm{full}})_{1,N-1} & (A_{\\mathrm{full}})_{1,N} \\\\\n\\vdots & & \\ddots & & \\vdots \\\\\n(A_{\\mathrm{full}})_{N-1,0} & (A_{\\mathrm{full}})_{N-1,1} & \\dots & (A_{\\mathrm{full}})_{N-1,N-1} & (A_{\\mathrm{full}})_{N-1,N} \\\\\n0 & 0 & \\dots & 0 & 1\n\\end{pmatrix}\n$$\nwhere $(A_{\\mathrm{full}})_{j,k}$ denotes the entry in row $j$, column $k$ of the original full operator.\n\n### **4. Analysis of Spectra and Conditioning**\n\nA careful analysis of the eigenvalue problem $A_{\\mathrm{bc}}v = \\lambda v$ reveals a deceptive relationship. The first and last rows of this equation imply $v_0 = \\lambda v_0$ and $v_N = \\lambda v_N$.\n- If $\\lambda \\neq 1$, then it must be that $v_0 = 0$ and $v_N = 0$. The remaining equations for the interior components $v_{1}, \\dots, v_{N-1}$ then reduce to precisely the eigenvalue problem for the interior matrix: $A_{\\mathrm{int}}v_{\\mathrm{int}} = \\lambda v_{\\mathrm{int}}$.\n- If $\\lambda = 1$, the conditions $v_0=v_0$ and $v_N=v_N$ are trivial. One can construct two linearly independent eigenvectors corresponding to $\\lambda=1$.\n\nThis proves that the spectrum of $A_{\\mathrm{bc}}$ is the union of the spectrum of $A_{\\mathrm{int}}$ and a double eigenvalue at $\\lambda=1$:\n$$\n\\sigma(A_{\\mathrm{bc}}) = \\sigma(A_{\\mathrm{int}}) \\cup \\{1, 1\\}\n$$\nThis implies that the range of eigenvalues and their imaginary parts will be nearly identical for both matrices (since the smallest eigenvalue of $A_{\\mathrm{int}}$ is greater than $1$).\n\nHowever, the conditioning tells a different story. The two-norm condition number, $\\kappa_2(A) = \\sigma_{\\max}(A)/\\sigma_{\\min}(A)$, depends on singular values, not eigenvalues. For a non-normal matrix, these can be vastly different. The row modification procedure renders $A_{\\mathrm{bc}}$ highly non-normal. The eigenvectors of $A_{\\mathrm{bc}}$ become nearly linearly dependent, particularly those associated with the eigenvalue $\\lambda=1$. This non-orthogonality of eigenvectors is the hallmark of non-normality and leads to an explosive growth of the condition number.\n\nThe clustering of CGL points near the boundaries causes the corresponding rows of $A_{\\mathrm{full}}$ to be nearly redundant, which is a primary source of ill-conditioning in spectral methods. The row modification method, by replacing these sensitive rows with arbitrary unit vectors, creates a numerically fragile operator. The interior reduction method, by contrast, correctly formulates the smaller, well-behaved interior problem, leading to superior numerical stability. The ratio $\\kappa_2(A_{\\mathrm{bc}})/\\kappa_2(A_{\\mathrm{int}})$ will therefore be large and grow rapidly with $N$, quantifying the degradation in stability caused by row modification.", "answer": "```python\nimport numpy as np\n\ndef build_chebyshev_diff_matrix(N):\n    \"\"\"\n    Constructs the Chebyshev spectral differentiation matrix D on the N+1\n    Chebyshev-Gauss-Lobatto points.\n    \"\"\"\n    if N == 0:\n        return np.array([[0.]])\n        \n    N_plus_1 = N + 1\n    # Define Chebyshev-Gauss-Lobatto points\n    x = np.cos(np.pi * np.arange(N_plus_1) / N)\n    \n    # Define weights c_j\n    c = np.ones(N_plus_1)\n    c[0] = 2.0\n    c[N] = 2.0\n    \n    D = np.zeros((N_plus_1, N_plus_1))\n    \n    # Off-diagonal entries\n    for j in range(N_plus_1):\n        for k in range(N_plus_1):\n            if j != k:\n                D[j, k] = (c[j] / c[k]) * ((-1)**(j + k)) / (x[j] - x[k])\n                \n    # Diagonal entries are set by the row-sum-to-zero property\n    # D_jj = -sum_{k!=j} D_jk\n    row_sums = np.sum(D, axis=1)\n    np.fill_diagonal(D, -row_sums)\n    \n    return D\n\ndef analyze_operators(N):\n    \"\"\"\n    Performs the complete analysis for a given polynomial degree N.\n    \"\"\"\n    # 1. Construct differentiation matrix\n    D = build_chebyshev_diff_matrix(N)\n    \n    # 2. Form second derivative operator\n    D2 = D @ D\n    \n    # 3. Assemble the full discrete operator\n    I_full = np.eye(N + 1)\n    A_full = -D2 + I_full\n    \n    # 4. Enforce boundary conditions\n    # Interior reduction\n    A_int = A_full[1:N, 1:N]\n    \n    # Row modification\n    A_bc = A_full.copy()\n    A_bc[0, :] = 0.0\n    A_bc[0, 0] = 1.0\n    A_bc[N, :] = 0.0\n    A_bc[N, N] = 1.0\n    \n    # 5. Compute required quantities\n    # Condition numbers\n    kappa_bc = np.linalg.cond(A_bc, 2)\n    kappa_int = np.linalg.cond(A_int, 2)\n    ratio_kappa = kappa_bc / kappa_int\n    \n    # Eigenvalues\n    eig_bc = np.linalg.eigvals(A_bc)\n    eig_int = np.linalg.eigvals(A_int)\n    \n    # Spectral characteristics\n    # For A_bc\n    min_re_bc = np.min(np.real(eig_bc)) if eig_bc.size > 0 else 0.0\n    max_re_bc = np.max(np.real(eig_bc)) if eig_bc.size > 0 else 0.0\n    max_im_abs_bc = np.max(np.abs(np.imag(eig_bc))) if eig_bc.size > 0 else 0.0\n    \n    # For A_int\n    min_re_int = np.min(np.real(eig_int)) if eig_int.size > 0 else 0.0\n    max_re_int = np.max(np.real(eig_int)) if eig_int.size > 0 else 0.0\n    max_im_abs_int = np.max(np.abs(np.imag(eig_int))) if eig_int.size > 0 else 0.0\n\n    return [\n        kappa_bc,\n        kappa_int,\n        ratio_kappa,\n        min_re_bc,\n        max_re_bc,\n        min_re_int,\n        max_re_int,\n        max_im_abs_bc,\n        max_im_abs_int\n    ]\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for the specified test cases and print results.\n    \"\"\"\n    test_cases = [6, 16, 32, 64]\n    all_results = []\n\n    for N in test_cases:\n        result_for_N = analyze_operators(N)\n        all_results.append(result_for_N)\n\n    # Format the final output string as specified\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3367677"}, {"introduction": "The conditioning problems associated with strong imposition motivate the search for more stable techniques, such as the classical Tau method. Instead of modifying operator rows corresponding to physical boundary points, the Tau method enforces boundary conditions by sacrificing the highest-frequency equations in the spectral domain. This hands-on coding task challenges you to implement the Tau method for a model Poisson problem, building the necessary operators and linear system to appreciate its elegance and robustness. [@problem_id:3367693]", "problem": "Consider the one-dimensional Poisson problem on the interval $\\left[-1,1\\right]$,\n$$-u''(x)=f(x),\\quad x\\in(-1,1),\\qquad u(-1)=0,\\quad u(1)=0,$$\nto be approximated by a spectral tau method using Chebyshev polynomials of the first kind. Let $\\{T_k(x)\\}_{k=0}^N$ denote the Chebyshev basis up to degree $N$, and represent the approximate solution as $u_N(x)=\\sum_{k=0}^N a_k T_k(x)$ with unknown modal coefficients $\\{a_k\\}_{k=0}^N$. The tau formulation enforces the differential equation in the subspace spanned by $\\{T_0,\\dots,T_{N-2}\\}$ and enforces the boundary conditions $u(\\pm 1)=0$ in place of the two highest-order equations, resulting in a square linear system for $\\{a_k\\}_{k=0}^N$.\n\nStarting only from foundational definitions and well-tested constructions:\n- Chebyshev-Gauss-Lobatto nodes $x_j=\\cos\\left(\\pi j/N\\right)$ for $j=0,\\dots,N$,\n- Barycentric-based Chebyshev differentiation matrices yielding the first and second derivative at the nodes,\n- The nodal-to-modal and modal-to-nodal linear maps defined by the Chebyshev-Vandermonde evaluation matrix $V_{jk}=T_k(x_j)$ and its inverse,\n\nderive the modal operator that maps $\\{a_k\\}$ to the Chebyshev coefficients of $-u''$, and then assemble the tau system by replacing the last two equations with the boundary conditions in modal form. Note that $T_k(1)=1$ and $T_k(-1)=(-1)^k$, so the two boundary conditions become linear conditions on $\\{a_k\\}$:\n$$\\sum_{k=0}^N a_k = 0,\\qquad \\sum_{k=0}^N (-1)^k a_k = 0.$$\n\nYour task is to implement a complete program that:\n- Constructs the modal operator for $-d^2/dx^2$ using Chebyshev-Gauss-Lobatto collocation and exact projection between nodal and modal spaces.\n- Assembles the tau system by enforcing the partial differential equation on $\\{T_0,\\dots,T_{N-2}\\}$ and the boundary conditions as the final two rows.\n- For a given right-hand side $f(x)=T_m(x)$, computes the right-hand side modal vector by projecting $f$ onto $\\{T_0,\\dots,T_N\\}$.\n- Solves for the modal coefficients $\\{a_k\\}$.\n\nTo test the correctness and to explicitly highlight the role of the tau terms, for each test case compute and report the following three scalar diagnostics:\n- The infinity norm of the residual of the partial differential equation in modal form restricted to rows $0$ through $N-2$, namely $\\left\\|L_{0:N-1,:}\\,a - b_{0:N-1}\\right\\|_{\\infty}$, where $L$ is the modal operator for $-d^2/dx^2$, $a$ is the solution vector, and $b$ is the modal vector of $f$.\n- The boundary residual magnitude $\\max\\left\\{\\left|\\sum_{k=0}^N a_k\\right|,\\left|\\sum_{k=0}^N (-1)^k a_k\\right|\\right\\}$.\n- The infinity norm measuring that the last two rows of the assembled system coincide with the boundary-enforcing tau rows, i.e., the maximum of the infinity norms of the differences between those two rows and the vectors $\\left[1,1,\\dots,1\\right]$ and $\\left[1,-1,1,-1,\\dots\\right]$.\n\nYour implementation must not use any special-case closed-form operator formulae in modal space; it must construct the operator via the nodal differentiation matrix and exact modal-nodal transforms as described above.\n\nTest suite:\n- Case $1$: $N=8$, $m=5$.\n- Case $2$: $N=5$, $m=0$.\n- Case $3$: $N=6$, $m=6$.\n\nFor each case, set $f(x)=T_m(x)$. Your program should produce a single line of output containing all results concatenated in the order of the test cases, each case contributing three floats in the order described above, as a comma-separated list enclosed in square brackets. For example, the output format must be exactly of the form\n$[\\text{residual}_{\\text{PDE,case 1}},\\text{residual}_{\\text{BC,case 1}},\\text{residual}_{\\text{tau,case 1}},\\text{residual}_{\\text{PDE,case 2}},\\dots,\\text{residual}_{\\text{tau,case 3}}]$.\nAll reported values must be real-valued floats with no unit. Angles, when internally used, are measured in radians.", "solution": "The user-provided problem is assessed as valid. It is scientifically sound, well-posed, and all necessary information for its resolution is provided. The task is to implement a Chebyshev spectral tau method for the one-dimensional Poisson equation, with a specific requirement to construct the modal operator via nodal space operations.\n\nWe are tasked with finding an approximate solution $u_N(x)$ to the Poisson problem\n$$ -u''(x) = f(x), \\quad x \\in (-1, 1) $$\nwith homogeneous Dirichlet boundary conditions $u(-1) = 0$ and $u(1) = 0$. The approximation is sought in the space of polynomials of degree up to $N$, represented as a sum of Chebyshev polynomials of the first kind, $T_k(x)$:\n$$ u_N(x) = \\sum_{k=0}^N a_k T_k(x) $$\nThe goal is to determine the unknown modal coefficients $a = \\{a_k\\}_{k=0}^N$.\n\nThe spectral tau method enforces the differential equation not at all points, but by requiring the residual to be orthogonal to a set of test functions. For this problem, the residual $r(x) = -u_N''(x) - f(x)$ is required to be orthogonal to the first $N-1$ Chebyshev polynomials, $\\{T_0(x), \\dots, T_{N-2}(x)\\}$. In the space of modal coefficients, this is equivalent to demanding that the first $N-1$ coefficients of the residual are zero. The two remaining degrees of freedom in the system are used to enforce the two boundary conditions.\n\nThe core of the problem is to construct the linear operator $L$ that maps the coefficients of $u_N(x)$, i.e., the vector $a$, to the coefficients of $-u_N''(x)$. The problem explicitly forbids using the known sparse structure of this operator in modal space and instead mandates its construction via transformations to and from physical space (nodal representation). This process involves three main steps:\n\n1.  **Modal-to-Nodal Transform:** Given the modal coefficients $a = (a_0, \\dots, a_N)^T$, we evaluate the function $u_N(x)$ at the $N+1$ Chebyshev-Gauss-Lobatto (CGL) nodes, $x_j = \\cos(\\frac{\\pi j}{N})$ for $j=0, \\dots, N$. The vector of nodal values $\\hat{u} = (u_N(x_0), \\dots, u_N(x_N))^T$ is obtained via the Chebyshev-Vandermonde matrix $V$, whose entries are $V_{jk} = T_k(x_j) = \\cos(\\frac{jk\\pi}{N})$. The transform is a matrix-vector product:\n    $$ \\hat{u} = V a $$\n\n2.  **Differentiation in Nodal Space:** The second derivative of $u_N$ at the CGL nodes, denoted $\\hat{u}''$, can be computed by applying a second-order Chebyshev differentiation matrix $D^{(2)}$ to the nodal values $\\hat{u}$. This matrix is obtained by squaring the first-order differentiation matrix, $D^{(2)} = (D^{(1)})^2$. The entries of $D^{(1)}$ are given by well-established formulas:\n    $$ (D^{(1)})_{ij} = \\begin{cases} \\frac{\\bar{c}_i (-1)^{i+j}}{\\bar{c}_j (x_i - x_j)} & i \\neq j \\\\ -\\frac{x_j}{2(1-x_j^2)} & 0 < j < N \\\\ \\frac{2N^2+1}{6} & i=j=0 \\\\ -\\frac{2N^2+1}{6} & i=j=N \\end{cases} $$\n    where $\\bar{c}_0 = \\bar{c}_N = 2$ and $\\bar{c}_j = 1$ for $j=1, \\dots, N-1$. Thus, the nodal values of the second derivative are:\n    $$ \\hat{u}'' = D^{(2)} \\hat{u} = (D^{(1)})^2 V a $$\n\n3.  **Nodal-to-Modal Transform:** The nodal values of the second derivative, $\\hat{u}''$, are projected back onto the Chebyshev basis to obtain their modal coefficients. This is achieved by applying the inverse of the Vandermonde matrix, $V^{-1}$. Let $c$ be the vector of modal coefficients of $-u_N''(x)$. Then:\n    $$ c = -V^{-1} \\hat{u}'' = -V^{-1} (D^{(1)})^2 V a $$\n    The matrix $L = -V^{-1} (D^{(1)})^2 V$ is the desired modal operator for $-d^2/dx^2$. The inverse Vandermonde matrix has entries $(V^{-1})_{kj} = \\frac{2}{N \\bar{c}_k \\bar{c}_j} T_k(x_j)$.\n\nWith the an $(N+1) \\times (N+1)$ operator $L$ constructed, we can formulate the linear system for the coefficients $a$. The equation we wish to solve is $La = b$, where $b$ is the vector of modal coefficients for the right-hand side function $f(x)$. For the test cases, $f(x) = T_m(x)$, so its modal coefficient vector $b$ is the standard basis vector $e_m$, with a $1$ at index $m$ and zeros elsewhere.\n\nThe tau system, denoted $A_{\\text{tau}} a = b_{\\text{tau}}$, is assembled as follows:\n-   **PDE Enforcement:** For the first $N-1$ equations (rows $k=0, \\dots, N-2$), we enforce the PDE. This means we take the first $N-1$ rows of the operator $L$ and the corresponding entries from the RHS vector $b$.\n    $$ (A_{\\text{tau}})_{k,:} = L_{k,:} \\quad \\text{and} \\quad (b_{\\text{tau}})_k = b_k, \\quad \\text{for } k = 0, \\dots, N-2 $$\n-   **Boundary Condition Enforcement:** The last two equations are replaced by the boundary conditions.\n    -   $u(1)=0 \\implies \\sum_{k=0}^N a_k T_k(1) = \\sum_{k=0}^N a_k = 0$. This gives the row vector $[1, 1, \\dots, 1]$ for the matrix $A_{\\text{tau}}$ and a $0$ for $b_{\\text{tau}}$.\n    -   $u(-1)=0 \\implies \\sum_{k=0}^N a_k T_k(-1) = \\sum_{k=0}^N (-1)^k a_k = 0$. This gives the row vector $[1, -1, 1, \\dots, (-1)^N]$ for $A_{\\text{tau}}$ and a $0$ for $b_{\\text{tau}}$.\n\nSpecifically, the last two rows of the system are:\n$$ (A_{\\text{tau}})_{N-1,:} = [1, 1, \\dots, 1], \\quad (b_{\\text{tau}})_{N-1} = 0 $$\n$$ (A_{\\text{tau}})_{N,:} = [1, -1, 1, \\dots, (-1)^N], \\quad (b_{\\text{tau}})_{N} = 0 $$\n\nSolving the square linear system $A_{\\text{tau}} a = b_{\\text{tau}}$ yields the coefficient vector $a$. The problem then requires the computation of three diagnostics to verify the correct implementation of the method:\n1.  The infinity norm of the PDE residual in the enforced subspace: $\\|L_{0:N-1,:} a - b_{0:N-1}\\|_{\\infty}$. By construction of the tau system, this should be close to machine precision.\n2.  The maximum absolute error in satisfying the boundary conditions: $\\max\\{|\\sum_{k=0}^N a_k|, |\\sum_{k=0}^N (-1)^k a_k|\\}$. This should also be near machine precision.\n3.  A check that the tau rows in the system matrix are constructed correctly: $\\max \\{\\| (A_{\\text{tau}})_{N-1,:} - \\mathbf{1}^T \\|_{\\infty}, \\| (A_{\\text{tau}})_{N,:} - ((-1)^k)^T \\|_{\\infty} \\}$. This should be exactly zero.\n\nThe following implementation carries out this procedure for each test case.", "answer": "```python\nimport numpy as np\n\ndef chebyshev_diff_matrix(N):\n    \"\"\"\n    Constructs the first-order Chebyshev differentiation matrix on N+1 CGL nodes.\n    \n    Args:\n        N (int): The degree of the polynomial approximation.\n        \n    Returns:\n        np.ndarray: The (N+1)x(N+1) differentiation matrix D.\n    \"\"\"\n    if N == 0:\n        return np.array([[0.0]])\n        \n    n_plus_1 = N + 1\n    j = np.arange(n_plus_1)\n    x = np.cos(np.pi * j / N)\n    \n    # c_bar coefficients\n    c_bar = np.ones(n_plus_1)\n    c_bar[0] = 2.0\n    c_bar[N] = 2.0\n    \n    # Off-diagonal entries\n    X = np.tile(x, (n_plus_1, 1))\n    dX = X - X.T\n    # Add identity to avoid division by zero on the diagonal\n    dX += np.eye(n_plus_1) \n    \n    # Alternating sign matrix\n    signs = (-1.0)**(j[:, None] + j[None, :])\n    \n    # C_ij = c_bar_i / c_bar_j\n    C = c_bar[:, None] / c_bar[None, :]\n    \n    D = C * signs / dX\n    \n    # Diagonal entries\n    D[0, 0] = (2 * N**2 + 1) / 6.0\n    D[N, N] = -(2 * N**2 + 1) / 6.0\n    \n    for jj in range(1, N):\n        D[jj, jj] = -x[jj] / (2.0 * (1.0 - x[jj]**2))\n        \n    return D\n\ndef solve_case(N, m):\n    \"\"\"\n    Solves the Poisson problem for a given N and m, and computes diagnostics.\n    \n    Args:\n        N (int): The degree of the polynomial approximation.\n        m (int): The index of the Chebyshev polynomial for the RHS f(x) = T_m(x).\n    \n    Returns:\n        tuple[float, float, float]: A tuple containing the three diagnostics:\n                                    1. PDE residual norm.\n                                    2. Boundary condition residual.\n                                    3. Tau row construction residual.\n    \"\"\"\n    n_plus_1 = N + 1\n    \n    # 1. Build the foundational matrices\n    # Chebyshev-Gauss-Lobatto nodes and indices\n    j = np.arange(n_plus_1)\n    k = j\n    \n    # First and second differentiation matrices in nodal space\n    D1 = chebyshev_diff_matrix(N)\n    D2 = D1 @ D1\n    \n    # Modal-to-nodal transform (Vandermonde matrix V)\n    V = np.cos(np.outer(j, k) * np.pi / N)\n\n    # Nodal-to-modal transform (Inverse Vandermonde matrix V_inv)\n    c_bar = np.ones(n_plus_1)\n    c_bar[0] = 2.0\n    c_bar[N] = 2.0\n    \n    # Entry (k,j) of V_inv is (2/N) * (1/(c_k * c_j)) * T_k(x_j)\n    # T_k(x_j) is V[j,k] = V.T[k,j]\n    scaling = (2.0 / N) / np.outer(c_bar, c_bar)\n    V_inv = scaling * V.T\n    \n    # 2. Construct the modal operator for -d^2/dx^2\n    L = -V_inv @ D2 @ V\n    \n    # 3. Assemble the tau system A_tau * a = b_tau\n    A_tau = np.zeros((n_plus_1, n_plus_1))\n    b_tau = np.zeros(n_plus_1)\n    \n    # Full RHS modal vector for f(x) = T_m(x)\n    b_full = np.zeros(n_plus_1)\n    if m = N:\n        b_full[m] = 1.0\n        \n    # Enforce PDE on T_0, ..., T_{N-2}\n    # This corresponds to rows 0 to N-2 of the system\n    num_pde_eqs = N - 1\n    if num_pde_eqs > 0:\n        A_tau[:num_pde_eqs, :] = L[:num_pde_eqs, :]\n        b_tau[:num_pde_eqs] = b_full[:num_pde_eqs]\n\n    # Enforce boundary conditions\n    # u(1) = 0  => sum(a_k) = 0\n    A_tau[N - 1, :] = 1.0\n    b_tau[N - 1] = 0.0\n    \n    # u(-1) = 0 => sum((-1)^k * a_k) = 0\n    A_tau[N, :] = (-1.0)**k\n    b_tau[N] = 0.0\n    \n    # 4. Solve for the modal coefficients {a_k}\n    a = np.linalg.solve(A_tau, b_tau)\n    \n    # 5. Compute diagnostics\n    # Diagnostic 1: PDE residual norm\n    res_pde = 0.0\n    if num_pde_eqs > 0:\n        pde_residual_vec = L[:num_pde_eqs, :] @ a - b_full[:num_pde_eqs]\n        res_pde = np.linalg.norm(pde_residual_vec, np.inf)\n\n    # Diagnostic 2: Boundary condition residual magnitude\n    bc_res_1 = np.abs(np.sum(a))\n    bc_res_2 = np.abs(np.sum(a * ((-1.0)**k)))\n    res_bc = np.max([bc_res_1, bc_res_2])\n    \n    # Diagnostic 3: Tau row construction residual\n    tau_row_1 = np.ones(n_plus_1)\n    tau_row_2 = (-1.0)**k\n    res_tau_1 = np.linalg.norm(A_tau[N-1, :] - tau_row_1, np.inf)\n    res_tau_2 = np.linalg.norm(A_tau[N, :] - tau_row_2, np.inf)\n    res_tau = np.max([res_tau_1, res_tau_2])\n\n    return res_pde, res_bc, res_tau\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        (8, 5),  # Case 1\n        (5, 0),  # Case 2\n        (6, 6)   # Case 3\n    ]\n\n    results = []\n    for N, m in test_cases:\n        pde_res, bc_res, tau_res = solve_case(N, m)\n        results.extend([pde_res, bc_res, tau_res])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3367693"}]}