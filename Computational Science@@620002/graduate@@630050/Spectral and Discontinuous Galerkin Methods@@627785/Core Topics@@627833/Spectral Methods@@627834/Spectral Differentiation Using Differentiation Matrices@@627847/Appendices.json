{"hands_on_practices": [{"introduction": "The hallmark of spectral methods is their remarkable accuracy. For functions that are fully resolved by the computational grid, spectral differentiation is not merely an approximation but an exact operation. This exercise provides a direct verification of this fundamental principle by applying Fourier spectral differentiation to a simple trigonometric polynomial [@problem_id:3417589]. By working through the steps in the frequency domain and comparing the result to the analytical derivative, you will gain a concrete understanding of what 'spectral accuracy' means in its ideal form.", "problem": "Consider the periodic interval $[0,2\\pi]$ and the equispaced grid with $N=8$ nodes $x_{j}=\\frac{2\\pi j}{N}$ for $j=0,1,\\dots,7$. Let the nodal values be $u_{j}=\\sin(x_{j})+\\frac{1}{2}\\cos(3x_{j})$. Angles are in radians. In the Fourier spectral approach, the first-derivative differentiation matrix $D$ is the linear operator on nodal values whose action is defined by representing a periodic function at the grid nodes by its discrete trigonometric expansion and applying the appropriate operation corresponding to differentiation in the spectral (frequency) domain. Using only this representation and the correspondence between differentiation in physical space and an operation in spectral space, perform the following:\n\n1. Construct the action of the Fourier spectral differentiation matrix $D$ on the vector $u=\\{u_{j}\\}_{j=0}^{7}$ and compute $Du$ at the nodes.\n2. Derive the exact derivative $u^{\\prime}(x)$ of the underlying function $u(x)=\\sin(x)+\\frac{1}{2}\\cos(3x)$ and evaluate $u^{\\prime}(x_{j})$ at the nodes.\n3. Justify, from first principles of the discrete Fourier representation on equispaced periodic grids, whether $Du$ must coincide with $\\{u^{\\prime}(x_{j})\\}_{j=0}^{7}$ for this choice of $N$ and this $u(x)$.\n4. Define the discrete infinity-norm error $E=\\max_{0\\leq j\\leq 7}\\left| (Du)_{j}-u^{\\prime}(x_{j}) \\right|$ and compute $E$.\n\nReport the value of $E$ as your final answer. If a numerical approximation were required, instructions would be given to round to a specified number of significant figures; however, for this problem, provide the exact value as a real number.", "solution": "The problem asks for an analysis of the Fourier spectral differentiation method applied to a specific function on an equispaced grid. We must validate the problem, and if valid, solve it by following four specified steps and reporting a final error value.\n\nFirst, the problem is validated.\n**Givens Extracted:**\n- Domain: Periodic interval $[0, 2\\pi]$.\n- Grid: $N=8$ equispaced nodes $x_{j}=\\frac{2\\pi j}{N}$ for $j=0,1,\\dots,7$.\n- Nodal values: $u_{j}=\\sin(x_{j})+\\frac{1}{2}\\cos(3x_{j})$.\n- Underlying function: $u(x)=\\sin(x)+\\frac{1}{2}\\cos(3x)$.\n- Operator: First-derivative Fourier spectral differentiation matrix, $D$.\n- Task 1: Compute the action of $D$ on the vector of nodal values, $Du$.\n- Task 2: Compute the exact derivative $u^{\\prime}(x_j)$ at the nodes.\n- Task 3: Justify if the results of Task 1 and Task 2 coincide.\n- Task 4: Compute the discrete infinity-norm error $E=\\max_{0\\leq j\\leq 7}\\left| (Du)_{j}-u^{\\prime}(x_{j}) \\right|$.\n\n**Validation Verdict:**\nThe problem is scientifically grounded, well-posed, and objective. It is a standard problem in the field of numerical analysis, specifically concerning spectral methods. The concepts used, such as the Discrete Fourier Transform (DFT), spectral differentiation, and aliasing, are fundamental to this area. The problem is self-contained, providing all necessary information ($N$, $u(x)$, grid definition) to arrive at a unique solution. There are no contradictions, ambiguities, or violations of scientific principles. The problem is deemed **valid**. We proceed to the solution.\n\nThe solution is constructed by addressing the four parts of the problem statement in sequence.\n\n**1. Construction of the action of the Fourier spectral differentiation matrix $D$ on $u$.**\n\nThe action of the Fourier spectral differentiation operator is defined in the frequency domain. First, the function $u(x)$ is represented by its Fourier series. We express $u(x)$ using complex exponentials:\n$$u(x) = \\sin(x) + \\frac{1}{2}\\cos(3x) = \\frac{e^{ix} - e^{-ix}}{2i} + \\frac{1}{2}\\left(\\frac{e^{i3x} + e^{-i3x}}{2}\\right)$$\n$$u(x) = -\\frac{i}{2}e^{ix} + \\frac{i}{2}e^{-ix} + \\frac{1}{4}e^{i3x} + \\frac{1}{4}e^{-i3x}$$\nThis is the continuous Fourier series representation of $u(x)$, which is a trigonometric polynomial. The non-zero Fourier coefficients $c_k$ in the expansion $u(x) = \\sum_{k=-\\infty}^{\\infty} c_k e^{ikx}$ are:\n$$c_1 = -\\frac{i}{2}, \\quad c_{-1} = \\frac{i}{2}, \\quad c_3 = \\frac{1}{4}, \\quad c_{-3} = \\frac{1}{4}$$\nAll other coefficients $c_k$ are zero.\n\nThe discrete values $u_j = u(x_j)$ are sampled on a grid of $N=8$ points. The highest wavenumber magnitude present in $u(x)$ is $k_{max}=3$. For an $N$-point grid, the Nyquist-Shannon sampling theorem for periodic functions implies that a trigonometric polynomial is perfectly resolved (i.e., not aliased) if its maximum wavenumber magnitude $k_{max}$ satisfies $|k_{max}| < N/2$.\nIn our case, $N=8$, so $N/2 = 4$. The condition is $3 < 4$, which is true. Therefore, the function $u(x)$ is not aliased on the grid $\\{x_j\\}$. This lack of aliasing ensures that the discrete Fourier coefficients $\\hat{u}_k$ calculated from the nodal values $\\{u_j\\}$ are identical to the continuous Fourier coefficients $c_k$ for all resolvable wavenumbers.\n\nSpectral differentiation corresponds to multiplying the Fourier coefficients $\\hat{u}_k$ by $ik$. Let $v(x)$ be the function obtained by spectrally differentiating $u(x)$. Its Fourier coefficients, $\\hat{v}_k$, are given by $\\hat{v}_k = ik\\hat{u}_k$. Using the coefficients of $u(x)$:\n$$\\hat{v}_1 = i(1)\\hat{u}_1 = i\\left(-\\frac{i}{2}\\right) = \\frac{1}{2}$$\n$$\\hat{v}_{-1} = i(-1)\\hat{u}_{-1} = (-i)\\left(\\frac{i}{2}\\right) = \\frac{1}{2}$$\n$$\\hat{v}_3 = i(3)\\hat{u}_3 = 3i\\left(\\frac{1}{4}\\right) = \\frac{3i}{4}$$\n$$\\hat{v}_{-3} = i(-3)\\hat{u}_{-3} = -3i\\left(\\frac{1}{4}\\right) = -\\frac{3i}{4}$$\nAll other coefficients $\\hat{v}_k$ are zero.\n\nThe spectrally differentiated function values at the grid points, which we denote by $(Du)_j$, are obtained by the inverse Fourier transform:\n$$(Du)_j = v(x_j) = \\sum_{k} \\hat{v}_k e^{ikx_j}$$\n$$(Du)_j = \\hat{v}_{-3}e^{-i3x_j} + \\hat{v}_{-1}e^{-ix_j} + \\hat{v}_{1}e^{ix_j} + \\hat{v}_{3}e^{i3x_j}$$\n$$(Du)_j = \\left(-\\frac{3i}{4}\\right)e^{-i3x_j} + \\left(\\frac{1}{2}\\right)e^{-ix_j} + \\left(\\frac{1}{2}\\right)e^{ix_j} + \\left(\\frac{3i}{4}\\right)e^{i3x_j}$$\nGrouping terms:\n$$(Du)_j = \\frac{1}{2}\\left(e^{ix_j} + e^{-ix_j}\\right) + \\frac{3i}{4}\\left(e^{i3x_j} - e^{-i3x_j}\\right)$$\nUsing Euler's formulas, $2\\cos(\\theta) = e^{i\\theta} + e^{-i\\theta}$ and $2i\\sin(\\theta) = e^{i\\theta} - e^{-i\\theta}$:\n$$(Du)_j = \\cos(x_j) + \\frac{3i}{4}(2i\\sin(3x_j))$$\n$$(Du)_j = \\cos(x_j) - \\frac{3}{2}\\sin(3x_j)$$\n\n**2. Derivation of the exact derivative $u^{\\prime}(x)$ and its evaluation at the nodes.**\n\nThe underlying function is $u(x) = \\sin(x) + \\frac{1}{2}\\cos(3x)$. We compute its first derivative with respect to $x$ using standard rules of calculus:\n$$u^{\\prime}(x) = \\frac{d}{dx}\\left(\\sin(x) + \\frac{1}{2}\\cos(3x)\\right)$$\n$$u^{\\prime}(x) = \\frac{d}{dx}(\\sin(x)) + \\frac{1}{2}\\frac{d}{dx}(\\cos(3x))$$\n$$u^{\\prime}(x) = \\cos(x) + \\frac{1}{2}(-\\sin(3x) \\cdot 3)$$\n$$u^{\\prime}(x) = \\cos(x) - \\frac{3}{2}\\sin(3x)$$\nEvaluating this exact derivative at the grid nodes $x_j$:\n$$u^{\\prime}(x_j) = \\cos(x_j) - \\frac{3}{2}\\sin(3x_j)$$\n\n**3. Justification of the coincidence of $(Du)_j$ and $u^{\\prime}(x_j)$.**\n\nFrom part 1, we found $(Du)_j = \\cos(x_j) - \\frac{3}{2}\\sin(3x_j)$.\nFrom part 2, we found $u^{\\prime}(x_j) = \\cos(x_j) - \\frac{3}{2}\\sin(3x_j)$.\nClearly, $(Du)_j = u^{\\prime}(x_j)$ for all $j=0, 1, \\dots, 7$.\n\nThe justification from first principles rests on the properties of the Fourier spectral method. This method is exact for any function that is a trigonometric polynomial whose wavenumbers are fully resolved by the grid.\nThe function $u(x)=\\sin(x)+\\frac{1}{2}\\cos(3x)$ is a trigonometric polynomial with the highest wavenumber magnitude $k_{max}=3$.\nThe grid consists of $N=8$ points. The sampling theorem for periodic signals states that all frequency components with wavenumber magnitude $|k| < N/2$ can be uniquely represented. For even $N$, this condition ensures that no aliasing occurs.\nIn this problem, $N/2=4$. The condition is $|k_{max}| < 4$, which is $3<4$. This condition is satisfied.\nBecause there is no aliasing, the Discrete Fourier Transform of the sampled values $\\{u_j\\}$ yields discrete Fourier coefficients $\\hat{u}_k$ that are exactly equal to the continuous Fourier series coefficients $c_k$ of the function $u(x)$ (for the relevant range of $k$).\nThe analytical differentiation of $u(x) = \\sum_k c_k e^{ikx}$ yields $u'(x) = \\sum_k (ik c_k) e^{ikx}$.\nThe numerical spectral differentiation process computes $\\hat{v}_k = ik \\hat{u}_k = ik c_k$ and then reconstructs the derivative via an inverse transform, resulting in $\\sum_k (ik c_k) e^{ikx_j}$.\nSince the spectral coefficients are exact, the reconstructed derivative at the grid points is identical to the exact derivative evaluated at those same points. Hence, $(Du)_j$ must coincide with $u^{\\prime}(x_j)$.\n\n**4. Computation of the error $E$.**\n\nThe discrete infinity-norm error is defined as:\n$$E = \\max_{0\\leq j\\leq 7}\\left| (Du)_{j}-u^{\\prime}(x_{j}) \\right|$$\nAs established in part 3, the spectral derivative and the exact derivative are identical at every grid node for this specific function and grid.\nTherefore, for each $j \\in \\{0, 1, \\dots, 7\\}$:\n$$(Du)_j - u^{\\prime}(x_j) = 0$$\nThe absolute value of this difference is also zero:\n$$\\left| (Du)_{j}-u^{\\prime}(x_{j}) \\right| = 0$$\nThe maximum value of a set of zeros is zero.\n$$E = \\max \\{0, 0, 0, 0, 0, 0, 0, 0\\} = 0$$\nThe error is exactly $0$.", "answer": "$$\\boxed{0}$$", "id": "3417589"}, {"introduction": "While Fourier methods are ideal for periodic problems, many applications involve non-periodic domains, where Chebyshev polynomial-based methods excel. This practice guides you through the construction of a Chebyshev differentiation matrix from first principles, using the elegant framework of barycentric interpolation [@problem_id:3417571]. More than just an implementation exercise, you will investigate the critical role of the matrix's boundary rows and quantify how even small perturbations can lead to a catastrophic loss of spectral accuracy, highlighting the method's delicate and powerful structure.", "problem": "You are to implement and analyze spectral differentiation using Chebyshev–Gauss–Lobatto (CGL) differentiation matrices. All trigonometric functions must use angles in radians. Your implementation must be based solely on first principles: Lagrange interpolation on CGL nodes and the fact that the derivative of the degree-$N$ Lagrange interpolant is a linear map from nodal values to nodal derivatives. No prederived formulas for the differentiation matrix are allowed in your reasoning; all identities must follow from these bases. The Chebyshev–Gauss–Lobatto (CGL) nodes are defined by $x_j=\\cos(\\pi j/N)$ for $j=0,1,\\dots,N$.\n\nTask A (construct the exact operator): Construct the CGL differentiation matrix $D\\in\\mathbb{R}^{(N+1)\\times(N+1)}$ that, when applied to a vector of nodal values of a function $f$, returns the nodal values of the derivative of the degree-$N$ Lagrange interpolant of $f$ at the CGL nodes. Your construction must begin from the Lagrange basis functions and their barycentric weights and be consistent with the interpolation property. No external data files are permitted.\n\nTask B (isolate the role of the boundary rows): Design a test that isolates how the first and last rows of $D$ control exactness for polynomials. To do this, build a modified operator $\\widetilde{D}$ that coincides with $D$ on all interior rows but replaces the first and last rows by zero rows. Demonstrate, with a single polynomial $p\\in\\mathbb{P}_N$, that $(\\widetilde{D}p)_j-(p')_j$ vanishes at all interior nodes $j\\in\\{1,2,\\dots,N-1\\}$ within numerical tolerance but fails at the boundary nodes $j\\in\\{0,N\\}$, thereby isolating the control of exactness by the boundary rows.\n\nTask C (perturb boundary weights and quantify loss of spectral accuracy): The exact matrix $D$ arises from barycentric weights that include special endpoint weights. Perturb this construction by multiplying the two endpoint barycentric weights by a factor $(1+\\delta)$ for a given $\\delta>0$, leaving all interior weights unchanged. Build the perturbed differentiation matrix $D^{(\\delta)}$ by the same interpolation-to-differentiation logic as in Task A using these perturbed weights. For the analytic function $f(x)=\\mathrm{e}^x$, evaluate the maximum nodal differentiation error\n$$\nE_N(\\delta)=\\max_{0\\le j\\le N}\\left| \\left(D^{(\\delta)} f\\right)_j - f'(x_j)\\right|.\n$$\nQuantify spectral accuracy in two ways:\n- Baseline spectral rate (exact weights, $\\delta=0$): estimate the exponential decay rate by a least-squares fit of $y=\\log_{10}E_N(0)$ as an affine function of $N$, i.e., $y\\approx a+bN$. Report the fitted slope $b$.\n- Loss under boundary perturbation ($\\delta=10^{-3}$): estimate the empirical algebraic growth/decay rate by a least-squares fit of $y=\\log_{10}E_N(10^{-3})$ as an affine function of $x=\\log_{10}N$, i.e., $y\\approx \\alpha+\\beta x$. Report the fitted exponent $\\beta$.\n\nFundamental base you may assume:\n- The Lagrange interpolant $I_N f$ at distinct nodes $\\{x_j\\}_{j=0}^N$ is uniquely defined by $I_N f(x_j)=f(x_j)$ for all $j$.\n- The barycentric Lagrange formula is a well-tested representation of $I_N f$ that uses barycentric weights and yields a stable expression for $I_N f$ and its derivative.\n- For polynomials $p\\in\\mathbb{P}_N$, $I_N p=p$, so the exact differentiation matrix reproduces $p'$ at the nodes.\n\nNumerical units and angles:\n- All trigonometric evaluations must use angles in radians.\n- There are no physical units; all quantities are dimensionless.\n\nTest Suite and required outputs:\n- Test Case $1$ (polynomial exactness): Let $N=8$. For each integer degree $m\\in\\{0,1,\\dots,8\\}$, test that $D$ applied to nodal values of $p_m(x)=x^m$ agrees with nodal values of $p_m'(x)$ within a tolerance of $10^{-12}$ in the maximum norm. Output a single boolean equal to true if and only if all degrees pass.\n- Test Case $2$ (boundary-row isolation): Let $N=12$ and $p(x)=T_5(x)$, the degree-$5$ Chebyshev polynomial of the first kind. Build $\\widetilde{D}$ by replacing the first and last rows of $D$ by zero rows. Compute the error vector $(\\widetilde{D}p)-p'$ at the nodes. Output a single boolean that is true if and only if the maximum absolute error over interior nodes is below $10^{-12}$ and the absolute error at each boundary node exceeds $10^{-8}$.\n- Test Case $3$ (baseline spectral rate): For $N\\in\\{16,32,64,128\\}$ and $\\delta=0$, compute $E_N(0)$ for $f(x)=\\mathrm{e}^x$, then perform a least-squares fit of $y=\\log_{10}E_N(0)$ against $N$ to estimate the slope $b$ of $y\\approx a+bN$. Output the fitted slope $b$ as a float. Also output $E_{128}(0)$ as a float.\n- Test Case $4$ (loss under perturbation): For the same $N\\in\\{16,32,64,128\\}$ and $\\delta=10^{-3}$, compute $E_N(10^{-3})$, then perform a least-squares fit of $y=\\log_{10}E_N(10^{-3})$ against $x=\\log_{10}N$ to estimate the exponent $\\beta$ of $y\\approx \\alpha+\\beta x$. Output the fitted exponent $\\beta$ as a float. Also output $E_{128}(10^{-3})$ as a float.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the following order:\n$[$boolean for Test Case $1$, boolean for Test Case $2$, fitted slope $b$ from Test Case $3$, fitted exponent $\\beta$ from Test Case $4$, $E_{128}(0)$ from Test Case $3$, $E_{128}(10^{-3})$ from Test Case $4]$.", "solution": "The solution to this problem is structured into three main parts: the derivation and construction of the Chebyshev differentiation matrix from first principles, an analysis of the role played by the boundary rows of this matrix, and an investigation into the loss of spectral accuracy when the underlying barycentric structure is perturbed.\n\n**Part A: Construction of the Chebyshev Differentiation Matrix**\n\nThe primary objective is to construct the differentiation matrix $D \\in \\mathbb{R}^{(N+1) \\times (N+1)}$ that transforms a vector of function values at the Chebyshev–Gauss–Lobatto (CGL) nodes into a vector of the derivative values of the function's interpolating polynomial at those same nodes. The CGL nodes are defined as $x_j = \\cos(\\frac{\\pi j}{N})$ for $j=0, 1, \\dots, N$.\n\nLet $f(x)$ be a function and let $p(x)$ be its unique polynomial interpolant of degree at most $N$ passing through the points $(x_j, f_j)$, where $f_j = f(x_j)$. This interpolant can be written in the Lagrange form:\n$$p(x) = \\sum_{k=0}^{N} f_k L_k(x)$$\nwhere $L_k(x)$ are the Lagrange basis polynomials, defined by the property $L_k(x_j) = \\delta_{jk}$ (the Kronecker delta).\n\nThe derivative of the interpolant is obtained by differentiating this sum term-by-term:\n$$p'(x) = \\sum_{k=0}^{N} f_k L_k'(x)$$\nEvaluating this derivative at the CGL nodes $x_j$ gives the desired nodal derivative values:\n$$p'(x_j) = \\sum_{k=0}^{N} f_k L_k'(x_j)$$\nThis equation describes a linear map from the vector of function values $\\mathbf{f} = [f_0, f_1, \\dots, f_N]^T$ to the vector of derivative values $\\mathbf{f'} = [p'(x_0), p'(x_1), \\dots, p'(x_N)]^T$. The matrix $D$ that represents this map has entries $D_{jk} = L_k'(x_j)$.\n\nTo derive an explicit formula for $D_{jk}$, we leverage the barycentric form of the Lagrange interpolant. This form relies on barycentric weights, which for the CGL nodes are given by $w_j = (-1)^j c_j$, where $c_0 = c_N = 1/2$ and $c_j = 1$ for $j \\in \\{1, 2, \\dots, N-1\\}$. The problem statement authorizes the use of these weights as a starting point.\n\nFrom the definition of the Lagrange basis polynomials and the barycentric weights, one can derive the entries of the differentiation matrix. For the off-diagonal elements ($j \\neq k$), the formula is:\n$$D_{jk} = L_k'(x_j) = \\frac{w_k/w_j}{x_j - x_k}$$\nSubstituting the CGL weights, we get:\n$$D_{jk} = \\frac{(-1)^k c_k / ((-1)^j c_j)}{x_j - x_k} = \\frac{c_k}{c_j} \\frac{(-1)^{j+k}}{x_j - x_k} \\quad \\text{for } j \\neq k$$\n\nFor the diagonal elements $D_{jj} = L_j'(x_j)$, a robust and common approach is to use the property that the derivative of a constant function is zero. Consider the function $f(x) = 1$. It is a polynomial of degree $0$, so for $N \\ge 0$, its interpolant is exact: $p(x) = 1$. The derivative is $p'(x) = 0$. Consequently, the nodal derivative values must all be zero: $p'(x_j) = 0$ for all $j=0, \\dots, N$. In matrix notation, this means $D \\mathbf{1} = \\mathbf{0}$, where $\\mathbf{1}$ is the vector of all ones. This implies that the sum of the elements in each row of $D$ must be zero:\n$$\\sum_{k=0}^{N} D_{jk} = 0$$\nFrom this property, we can solve for the diagonal elements:\n$$D_{jj} = - \\sum_{k=0, k \\neq j}^{N} D_{jk}$$\nThis completes the construction of the differentiation matrix $D$ from first principles as required.\n\n**Part B: Role of Boundary Rows in Polynomial Exactness**\n\nA fundamental property of the spectral differentiation matrix $D$ is its exactness for polynomials in $\\mathbb{P}_N$ (the space of polynomials of degree at most $N$). This means that for any $p(x) \\in \\mathbb{P}_N$, the operation $D\\mathbf{p}$ (where $\\mathbf{p}$ is the vector of nodal values of $p(x)$) exactly yields the nodal values of the derivative $p'(x)$. This is because the interpolant of $p(x)$ is $p(x)$ itself.\n\nTask B requires us to demonstrate the critical role of the boundary rows of $D$ (the first and last rows, indexed by $j=0$ and $j=N$) in maintaining this property. We construct a modified matrix $\\widetilde{D}$ which is identical to $D$ except that its first and last rows are zeroed out.\nApplying this modified operator to the nodal values of a polynomial $p(x) \\in \\mathbb{P}_N$, we find that for the interior nodes ($j \\in \\{1, \\dots, N-1\\}$), the derivative is still computed correctly because the interior rows are unchanged:\n$$(\\widetilde{D}\\mathbf{p})_j = \\sum_{k=0}^N \\widetilde{D}_{jk} p_k = \\sum_{k=0}^N D_{jk} p_k = (D\\mathbf{p})_j = p'(x_j)$$\nHowever, at the boundary nodes ($j=0$ and $j=N$), the result is forced to be zero:\n$$(\\widetilde{D}\\mathbf{p})_0 = 0 \\quad \\text{and} \\quad (\\widetilde{D}\\mathbf{p})_N = 0$$\nUnless $p'(x_0)$ and $p'(x_N)$ happen to be zero, this will introduce an error. The test case with $p(x) = T_5(x)$, the Chebyshev polynomial of degree $5$, on a grid with $N=12$ illustrates this perfectly. Since $5 \\le 12$, $D$ is exact for $T_5(x)$. The derivatives at the boundaries are non-zero: $T_5'(1) = 5^2=25$ and $T_5'(-1)=(-1)^{5-1}5^2=25$. Thus, the error $(\\widetilde{D}\\mathbf{p}) - \\mathbf{p'}$ will be negligible at the interior nodes but large at the boundary nodes.\n\n**Part C: Spectral Accuracy and its Loss under Perturbation**\n\nSpectral methods are renowned for their \"spectral accuracy\": for analytic functions, the differentiation error $E_N = \\max_j |(Df)_j - f'(x_j)|$ decreases exponentially with $N$, i.e., $E_N \\sim O(e^{-cN})$ for some $c>0$. This corresponds to a linear relationship between $\\log E_N$ and $N$. The first part of Task C quantifies this for $f(x)=\\mathrm{e}^x$ by fitting $\\log_{10} E_N(0)$ to the model $y \\approx a+bN$. The slope $b$ captures this exponential decay rate.\n\nThis high-order accuracy is critically dependent on the precise algebraic structure of $D$, which is determined by the specific properties of the Chebyshev nodes and their barycentric weights. To demonstrate this sensitivity, Task C introduces a perturbed differentiation matrix $D^{(\\delta)}$. This matrix is constructed using the same logic as $D$, but with barycentric weights whose endpoint values $w_0$ and $w_N$ are multiplied by a factor $(1+\\delta)$.\n\nEven a small perturbation, such as $\\delta = 10^{-3}$, breaks the exact polynomial differentiation property for degree $N$. This violation degrades the convergence behavior for analytic functions. The error no longer decays exponentially but rather algebraically, i.e., $E_N(\\delta) \\sim O(N^{-\\beta})$ for some power $\\beta > 0$. An algebraic convergence rate corresponds to a linear relationship in a log-log plot: $\\log E_N(\\delta) \\approx \\alpha - \\beta \\log N$. The second part of Task C quantifies this by fitting $\\log_{10} E_N(10^{-3})$ against $\\log_{10} N$. The resulting slope, $\\beta$ (given with a minus sign in the fit), represents the algebraic order of convergence. This experiment highlights that spectral accuracy is not a generic feature of high-order methods but a consequence of a specific, delicately balanced algebraic structure. Theoretical analysis predicts that for this type of perturbation, the order of convergence $\\beta$ should be approximately $2$.", "answer": "```python\nimport numpy as np\n\ndef construct_cheb_diff_matrix(N, delta=0.0):\n    \"\"\"\n    Constructs the Chebyshev differentiation matrix for a given N and perturbation delta.\n\n    Args:\n        N (int): The degree of the polynomial interpolant (N+1 points).\n        delta (float): The perturbation factor for the endpoint barycentric weights.\n\n    Returns:\n        numpy.ndarray: The (N+1)x(N+1) differentiation matrix.\n    \"\"\"\n    if N == 0:\n        return np.array([[0.0]])\n        \n    j = np.arange(N + 1)\n    x = np.cos(np.pi * j / N)\n\n    # Barycentric weights for CGL nodes\n    c = np.ones(N + 1)\n    c[0] = 0.5\n    c[N] = 0.5\n    w = ((-1.0)**j) * c\n    \n    # Introduce perturbation to endpoint weights\n    w[0] *= (1.0 + delta)\n    w[N] *= (1.0 + delta)\n\n    # Off-diagonal entries D_jk = (w_k/w_j) / (x_j - x_k)\n    # Using broadcasting to compute all at once\n    X = np.tile(x, (N + 1, 1))\n    dX = X - X.T\n    \n    W_ratio = np.tile(w, (N + 1, 1)).T / np.tile(w, (N + 1, 1))\n    \n    # Fill diagonal of dX with 1s to avoid division by zero\n    np.fill_diagonal(dX, 1.0)\n    D = W_ratio / dX\n    \n    # Diagonal entries D_jj = -sum(D_jk) for k != j\n    np.fill_diagonal(D, 0.0)\n    D_diag = -np.sum(D, axis=1)\n    np.fill_diagonal(D, D_diag)\n    \n    return D\n\ndef solve():\n    \"\"\"\n    Main function to execute all test cases and print the final result.\n    \"\"\"\n    results = []\n\n    # Test Case 1: Polynomial Exactness\n    N1 = 8\n    tol1 = 1e-12\n    D1 = construct_cheb_diff_matrix(N1, delta=0.0)\n    x1 = np.cos(np.pi * np.arange(N1 + 1) / N1)\n    \n    passed_all_degrees = True\n    for m in range(N1 + 1):\n        p_vals = x1**m\n        if m == 0:\n            p_prime_vals = np.zeros_like(x1)\n        else:\n            p_prime_vals = m * (x1**(m - 1))\n        \n        p_prime_num = D1 @ p_vals\n        error = np.max(np.abs(p_prime_num - p_prime_vals))\n        \n        if error > tol1:\n            passed_all_degrees = False\n            break\n    results.append(passed_all_degrees)\n\n    # Test Case 2: Boundary-Row Isolation\n    N2 = 12\n    tol2_int = 1e-12\n    tol2_bnd = 1e-8\n    D2 = construct_cheb_diff_matrix(N2, delta=0.0)\n    \n    D2_tilde = D2.copy()\n    D2_tilde[0, :] = 0.0\n    D2_tilde[N2, :] = 0.0\n    \n    j2 = np.arange(N2 + 1)\n    x2 = np.cos(np.pi * j2 / N2)\n    theta2 = np.pi * j2 / N2\n    m = 5\n    \n    # Nodal values of p(x) = T_5(x)\n    p_vals = np.cos(m * theta2)\n    \n    # Nodal values of p'(x)\n    p_prime_vals = np.empty_like(x2)\n    interior_indices = (j2 > 0)  (j2  N2)\n    p_prime_vals[interior_indices] = m * np.sin(m * theta2[interior_indices]) / np.sin(theta2[interior_indices])\n    p_prime_vals[0] = m**2\n    p_prime_vals[N2] = (-1)**(m - 1) * m**2\n    \n    p_prime_num = D2_tilde @ p_vals\n    error_vec = p_prime_num - p_prime_vals\n    \n    max_interior_error = np.max(np.abs(error_vec[1:-1]))\n    error_at_j0 = np.abs(error_vec[0])\n    error_at_jN = np.abs(error_vec[-1])\n    \n    test2_passed = (max_interior_error  tol2_int) and (error_at_j0 > tol2_bnd) and (error_at_jN > tol2_bnd)\n    results.append(test2_passed)\n    \n    # Test Cases 3 and 4: Spectral Rate and Loss Analysis\n    Ns = np.array([16, 32, 64, 128])\n    delta3 = 0.0\n    delta4 = 1e-3\n    \n    errors_exact = []\n    errors_perturbed = []\n    \n    for N in Ns:\n        j_N = np.arange(N + 1)\n        x_N = np.cos(np.pi * j_N / N)\n        f_vals = np.exp(x_N)\n        f_prime_vals = np.exp(x_N)\n        \n        # Exact matrix (delta=0)\n        D_exact = construct_cheb_diff_matrix(N, delta=delta3)\n        f_prime_num_exact = D_exact @ f_vals\n        error_exact = np.max(np.abs(f_prime_num_exact - f_prime_vals))\n        errors_exact.append(error_exact)\n\n        # Perturbed matrix (delta=1e-3)\n        D_perturbed = construct_cheb_diff_matrix(N, delta=delta4)\n        f_prime_num_perturbed = D_perturbed @ f_vals\n        error_perturbed = np.max(np.abs(f_prime_num_perturbed - f_prime_vals))\n        errors_perturbed.append(error_perturbed)\n        \n    # Case 3: Baseline spectral rate\n    log10_errors_exact = np.log10(errors_exact)\n    # Fit y = a + b*x where y = log10(E), x = N\n    coeffs_exact = np.polyfit(Ns, log10_errors_exact, 1)\n    b = coeffs_exact[0]\n    \n    # Case 4: Loss under perturbation\n    log10_Ns = np.log10(Ns)\n    log10_errors_perturbed = np.log10(errors_perturbed)\n    # Fit y = alpha + beta*x where y = log10(E), x = log10(N)\n    coeffs_perturbed = np.polyfit(log10_Ns, log10_errors_perturbed, 1)\n    beta = coeffs_perturbed[0]\n\n    E128_exact = errors_exact[-1]\n    E128_perturbed = errors_perturbed[-1]\n    \n    results.append(b)\n    results.append(beta)\n    results.append(E128_exact)\n    results.append(E128_perturbed)\n\n    print(f\"[{results[0]},{results[1]},{results[2]},{results[3]},{results[4]},{results[5]}]\")\n\nsolve()\n```", "id": "3417571"}, {"introduction": "A primary use of spectral methods is in solving nonlinear partial differential equations, where terms like $u(x)^2$ are common. A naive, point-wise product in physical space introduces aliasing errors, which corrupt the solution by misrepresenting high-frequency components as low-frequency ones. This exercise demonstrates the standard technique for overcoming this challenge: the $3/2$-rule for dealiasing [@problem_id:3417612]. You will implement and compare the naive and dealiased derivatives to quantify the dramatic improvement in accuracy gained by properly handling nonlinear terms in Fourier space.", "problem": "You are asked to implement Fourier spectral differentiation on a periodic domain using a differentiation matrix implicitly defined by diagonalization in Fourier space, and to incorporate the three-halves rule (also called the $3/2$-rule) dealiasing by zero-padding in spectral space. Your implementation must compute the spatial derivative of a nonlinear term without and with dealiasing, and quantify the effect of dealiasing on the error by comparing to a known exact derivative sampled on the same grid.\n\nThe problem setting is purely mathematical and uses the following fundamental bases:\n- On a periodic domain of length $L = 2\\pi$ (angles in radians), a $N$-point equispaced grid is $x_j = j \\Delta x$ with $\\Delta x = L/N$.\n- The discrete Fourier transform (Fast Fourier Transform (FFT)) and its inverse represent a function through its discrete Fourier coefficients. The Fourier differentiation operator on the grid can be expressed as a differentiation matrix $\\mathbf{D}_N = \\mathbf{F}_N^{-1} \\operatorname{diag}(\\mathrm{i} k) \\mathbf{F}_N$, where $\\mathbf{F}_N$ is the discrete Fourier transform matrix, $k$ is the vector of angular wavenumbers consistent with the grid, and $\\mathrm{i}$ is the imaginary unit. Applying $\\mathbf{D}_N$ to values of a function on the grid is equivalent to transforming to spectral space, multiplying by $\\mathrm{i} k$, and transforming back.\n- The product of functions in physical space corresponds to convolution of their Fourier coefficients. On a finite grid with $N$ modes, this convolution is computed modulo $N$, which causes aliasing unless mitigated. The three-halves rule mitigates aliasing for quadratic nonlinearities by padding the Fourier coefficients to $M = \\tfrac{3}{2} N$ before transforming to physical space to compute the product, and then truncating back to $N$.\n\nYour tasks:\n1. Implement a routine that applies the Fourier differentiation matrix $\\mathbf{D}_N$ implicitly via the diagonalization in Fourier space to compute the derivative of a function given on an $N$-point grid on $[0,2\\pi)$ (radians). Use the standard angular wavenumber vector $k$ produced from the discrete frequencies appropriate to the grid and the domain length $L=2\\pi$.\n2. Implement the naive pseudo-spectral derivative of the nonlinear term $g(x) = u(x)^2$ on the $N$-point grid by:\n   - Evaluating $u$ on the grid,\n   - Forming $g = u^2$ in physical space,\n   - Applying the Fourier differentiation matrix to $g$ to approximate $g'(x)$.\n3. Implement the three-halves rule ($3/2$-rule) dealiased version for the derivative of $g(x) = u(x)^2$ by:\n   - Transforming $u$ to spectral space on the $N$-grid,\n   - Zero-padding the spectrum to length $M = \\tfrac{3}{2} N$ by symmetric embedding of low modes and appropriate normalization so that the inverse transform on the $M$-grid reproduces the continuous trigonometric interpolant sampled at $M$ points,\n   - Transforming to physical space on the $M$-grid,\n   - Forming $g_M = u_M^2$ in physical space on the $M$-grid,\n   - Transforming $g_M$ back to spectral space of length $M$,\n   - Truncating the spectrum back to length $N$ with the inverse normalization to be consistent with the $N$-grid,\n   - Applying the Fourier differentiation matrix on the $N$-grid to the truncated spectrum of $g$ to obtain a dealiased approximation of $g'(x)$ on the $N$-grid.\n\nYou must use the following test suite, all on the domain $[0,2\\pi)$ with angles measured in radians:\n- Case A (happy path, moderate nonlinearity): $N = 32$, define $u(x) = \\sin(3 x) + 0.5 \\cos(5 x)$.\n- Case B (high-frequency content near Nyquist limit on $N$): $N = 32$, define $u(x) = 0.7 \\sin(10 x) + 0.6 \\cos(11 x)$.\n- Case C (very coarse grid, strong aliasing): $N = 8$, define $u(x) = \\sin(3 x)$.\n- Case D (moderate grid with modes near truncation): $N = 48$, define $u(x) = \\sin(12 x) + 0.3 \\sin(13 x)$.\n\nFor each case, compute:\n- The exact derivative of $g(x) = u(x)^2$ at the grid points using $g'(x) = 2 u(x) u'(x)$ with the exact analytical derivative $u'(x)$.\n- The naive pseudo-spectral approximation of $g'(x)$ on the $N$-grid as described above.\n- The dealiased ($3/2$-rule) pseudo-spectral approximation of $g'(x)$ on the $N$-grid as described above.\n\nFor each case, report two numbers:\n- The relative discrete $L^2$-error of the naive approximation with respect to the exact sampled derivative, defined as\n  $$ \\varepsilon_{\\mathrm{naive}} = \\frac{\\left( \\sum_{j=0}^{N-1} \\left| g'_{\\mathrm{naive}}(x_j) - g'_{\\mathrm{exact}}(x_j) \\right|^2 \\Delta x \\right)^{1/2}}{\\left( \\sum_{j=0}^{N-1} \\left| g'_{\\mathrm{exact}}(x_j) \\right|^2 \\Delta x \\right)^{1/2}}. $$\n- The relative discrete $L^2$-error of the dealiased approximation\n  $$ \\varepsilon_{3/2} = \\frac{\\left( \\sum_{j=0}^{N-1} \\left| g'_{3/2}(x_j) - g'_{\\mathrm{exact}}(x_j) \\right|^2 \\Delta x \\right)^{1/2}}{\\left( \\sum_{j=0}^{N-1} \\left| g'_{\\mathrm{exact}}(x_j) \\right|^2 \\Delta x \\right)^{1/2}}. $$\n\nYour program must output a single line containing the results aggregated across the four cases in the following format:\n- A single list with four entries, one per case in the order A, B, C, D.\n- Each entry must itself be a two-element list containing $[\\varepsilon_{\\mathrm{naive}}, \\varepsilon_{3/2}]$ as decimal floats.\n- For example, the output should look like: [[eA_naive,eA_3over2],[eB_naive,eB_3over2],[eC_naive,eC_3over2],[eD_naive,eD_3over2]].\n\nNo user input is required. Angles must be interpreted in radians. No physical units beyond radians are involved. The program must be a complete, runnable implementation that follows the described procedure.", "solution": "The problem is valid as it is scientifically grounded in the principles of numerical analysis and spectral methods, is well-posed with a clear objective, and contains no ambiguities or contradictions.\n\nThe task is to compute the spatial derivative of a quadratic nonlinearity, $g(x) = u(x)^2$, on a periodic domain $[0, 2\\pi)$ using a Fourier pseudo-spectral method. We will compare two approaches: a naive computation prone to aliasing errors and a dealiased computation using the three-halves rule. The accuracy of each method is quantified by the relative discrete $L^2$-error against the exact analytical derivative.\n\n### Principles of Fourier Spectral Differentiation\n\nA periodic function $f(x)$ on a domain of length $L=2\\pi$ can be represented by a Fourier series:\n$$\nf(x) = \\sum_{k=-\\infty}^{\\infty} c_k e^{ikx}\n$$\nwhere $k$ are integer wavenumbers and $c_k$ are the Fourier coefficients. The derivative is then:\n$$\nf'(x) = \\sum_{k=-\\infty}^{\\infty} ik c_k e^{ikx}\n$$\nIn a numerical setting, we work with a finite set of $N$ equispaced grid points $x_j = 2\\pi j/N$ for $j=0, \\dots, N-1$. A function is represented by its values $f_j = f(x_j)$ on this grid. The discrete Fourier transform (DFT) and its inverse provide the mapping between the physical space representation $f_j$ and the spectral space representation $\\hat{f}_k$, which approximates the scaled Fourier coefficients $N c_k$. The set of representable integer wavenumbers is finite, typically $k \\in \\{0, \\pm 1, \\dots, \\pm(N/2-1), -N/2\\}$ for an even $N$.\n\nThe derivative $f'(x_j)$ can be computed by transforming $f_j$ to the spectral domain, multiplying each coefficient $\\hat{f}_k$ by $ik$, and transforming back to physical space:\n$$\nf'_j = \\mathcal{F}^{-1}\\{ ik \\hat{f}_k \\}_{k} \\quad \\text{where} \\quad \\hat{f}_k = \\mathcal{F}\\{f_j\\}_{j}\n$$\nHere, $\\mathcal{F}$ denotes the DFT (implemented via the Fast Fourier Transform, FFT) and $\\mathcal{F}^{-1}$ is its inverse. This process is equivalent to applying a dense differentiation matrix $\\mathbf{D}_N$ but is computationally performed in $\\mathcal{O}(N \\log N)$ operations.\n\n### The Aliasing Problem with Nonlinear Terms\n\nWhen computing a nonlinear term like $g(x) = u(x)^2$ in a pseudo-spectral method, the product is evaluated in physical space on the grid: $g_j = (u_j)^2$. If the Fourier series for $u(x)$ contains modes up to wavenumber $k_{max}$, the product $u(x)^2$ will contain modes up to $2k_{max}$. If $2k_{max}$ exceeds the highest representable wavenumber on the $N$-point grid (which is $N/2-1$ for non-aliased modes), the high-frequency content \"folds back\" onto lower frequencies, contaminating their coefficients. This phenomenon is called aliasing.\n\nFor instance, on an $N$-point grid, a mode $e^{i(k+N)x_j} = e^{ikx_j}e^{iNx_j} = e^{ikx_j}e^{i2\\pi j} = e^{ikx_j}$. Thus, a wavenumber $k+N$ is indistinguishable from, or aliases to, wavenumber $k$.\n\n### Dealiasing with the Three-Halves Rule\n\nThe three-halves ($3/2$) rule is a technique to exactly evaluate quadratic nonlinearities without aliasing. The procedure is as follows:\n1.  Start with the function values $u_j$ on an $N$-point grid.\n2.  Transform to spectral space to obtain coefficients $\\hat{u}_k$ for $|k|  N/2$.\n3.  Pad the spectrum with zeros to a larger size $M \\ge \\frac{3}{2}N$. This creates a new spectrally-padded vector $\\hat{u}^{(M)}_k$. The original coefficients are placed in their corresponding spots in the longer vector, and the new high-frequency slots are filled with zeros. The coefficients must be scaled by a factor of $M/N$ to ensure the inverse transform represents the same underlying continuous function sampled on the finer grid.\n4.  Inverse transform $\\hat{u}^{(M)}_k$ to a physical grid of $M$ points, yielding $u^{(M)}_j$. This is equivalent to evaluating the trigonometric interpolant of $u$ on a finer grid.\n5.  Compute the quadratic product on this finer grid: $g^{(M)}_j = (u^{(M)}_j)^2$. Since the grid is finer, it can represent the higher frequencies generated by the product without aliasing. For a quadratic product, if the original modes in $u$ are limited to $|k|  N/2$, the product modes are limited to $|k|  N$. Choosing $M \\ge \\frac{3}{2}N$ ensures that the highest product mode (up to $N-1$) is well within the non-aliased range of the M-grid, which is $|k|  M/2$.\n6.  Transform $g^{(M)}_j$ back to the spectral domain, yielding $\\hat{g}^{(M)}_k$.\n7.  Truncate the spectrum $\\hat{g}^{(M)}_k$ back to size $N$ by discarding the high-frequency coefficients that were generated by the product. This isolates the part of the spectrum that can be represented on the original $N$-point grid. The coefficients are rescaled by $N/M$. The resulting vector $\\hat{g}_k$ is a dealiased representation of the product.\n8.  Apply the spectral differentiation operator ($ik$) to $\\hat{g}_k$ and inverse transform to get the dealiased derivative on the $N$-point grid.\n\n### Implementation Steps\n\nFor each test case, we perform the following calculations:\n1.  **Grid and Wavenumbers**: Define the $N$-point grid $x_j = 2\\pi j/N$ and the corresponding integer wavenumber vector $k$ using `numpy.fft.fftfreq`.\n2.  **Exact Derivative**: Analytically find $u'(x)$, calculate $g'(x) = 2u(x)u'(x)$, and evaluate it on the grid $x_j$ to get the vector $g'_{\\mathrm{exact}}$.\n3.  **Naive Derivative**:\n    a. Evaluate $u(x)$ on the grid to get $u_j$.\n    b. Compute the element-wise square: $g_j = u_j^2$.\n    c. Compute the derivative $g'_{\\mathrm{naive}, j} = \\mathcal{F}^{-1}\\{ik \\cdot \\mathcal{F}\\{g_j\\}\\}$.\n4.  **Dealiased Derivative ($3/2$-Rule)**:\n    a. Compute $\\hat{u}_k = \\mathcal{F}\\{u_j\\}$.\n    b. Create a zero vector $\\hat{u}^{(M)}_k$ of size $M = 3N/2$. Pad by copying the lower-half positive and negative frequency coefficients from $\\hat{u}_k$ into $\\hat{u}^{(M)}_k$ and scaling by $M/N$.\n    c. Compute $u^{(M)}_j = \\mathcal{F}^{-1}\\{\\hat{u}^{(M)}_k\\}$.\n    d. Compute $g^{(M)}_j = (u^{(M)}_j)^2$.\n    e. Compute $\\hat{g}^{(M)}_k = \\mathcal{F}\\{g^{(M)}_j\\}$.\n    f. Truncate by creating a vector $\\hat{g}_k$ of size $N$, filling it with the corresponding low-frequency coefficients from $\\hat{g}^{(M)}_k$ and scaling by $N/M$.\n    g. Compute the derivative $g'_{3/2, j} = \\mathcal{F}^{-1}\\{ik \\cdot \\hat{g}_k\\}$.\n5.  **Error Calculation**: For both naive and dealiased methods, compute the relative discrete $L^2$-error:\n    $$ \\varepsilon = \\frac{\\| g'_{\\mathrm{approx}} - g'_{\\mathrm{exact}} \\|_2}{\\| g'_{\\mathrm{exact}} \\|_2} $$\n    where $\\|\\cdot\\|_2$ is the standard Euclidean norm, calculated using `numpy.linalg.norm`.\n\nThis procedure is applied to all four test cases, and the resulting error pairs $[\\varepsilon_{\\mathrm{naive}}, \\varepsilon_{3/2}]$ are collected. As anticipated, the results will demonstrate that when aliasing occurs (Cases B, C, D), the naive method has a significant error, while the $3/2$-rule dealiasing effectively eliminates this error, yielding results near machine precision. In Case A, where no aliasing occurs, both methods are expected to be highly accurate.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n\n    def run_case(N, u_func, u_prime_func):\n        \"\"\"\n        Computes naive and dealiased derivatives for a single test case\n        and returns their relative L2 errors.\n        \"\"\"\n        # 1. Setup grid, wavenumbers, and exact derivative\n        L = 2 * np.pi\n        x = np.linspace(0, L, N, endpoint=False)\n        k_N = np.fft.fftfreq(N) * N\n\n        u_vec = u_func(x)\n        u_prime_vec = u_prime_func(x)\n        g_prime_exact_vec = 2 * u_vec * u_prime_vec\n\n        # 2. Naive pseudo-spectral derivative\n        g_vec_naive = u_vec**2\n        g_hat_naive = np.fft.fft(g_vec_naive)\n        g_prime_hat_naive = 1j * k_N * g_hat_naive\n        g_prime_naive_vec = np.fft.ifft(g_prime_hat_naive)\n\n        # 3. 3/2-rule dealiased derivative\n        M = 3 * N // 2\n        \n        # Pad u from N to M grid\n        u_hat_N = np.fft.fft(u_vec)\n        u_hat_padded = np.zeros(M, dtype=np.complex128)\n        \n        N_half = N // 2\n        \n        # Copy low-frequency coefficients. The fft output for even N has\n        # k = 0, 1, ..., N/2-1 at indices 0..N/2-1\n        # k = -N/2, -N/2+1, ..., -1 at indices N/2..N-1\n        u_hat_padded[0:N_half] = u_hat_N[0:N_half]\n        u_hat_padded[M - (N - N_half):] = u_hat_N[N_half:]\n        \n        # Rescale coefficients for the larger transform size\n        u_hat_padded *= (M / N)\n\n        # Compute product on the padded grid\n        u_M_vec = np.fft.ifft(u_hat_padded)\n        # Taking .real to discard negligible imaginary parts from round-off\n        g_M_vec = u_M_vec.real**2\n        \n        # Truncate g from M back to N grid\n        g_hat_M = np.fft.fft(g_M_vec)\n        g_hat_N_dealiased = np.zeros(N, dtype=np.complex128)\n        \n        g_hat_N_dealiased[0:N_half] = g_hat_M[0:N_half]\n        g_hat_N_dealiased[N_half:] = g_hat_M[M - (N - N_half):]\n\n        # Rescale coefficients back to the original transform size\n        g_hat_N_dealiased *= (N / M)\n\n        # Differentiate the dealiased spectrum\n        g_prime_hat_32 = 1j * k_N * g_hat_N_dealiased\n        g_prime_32_vec = np.fft.ifft(g_prime_hat_32)\n\n        # 4. Error calculation\n        norm_exact = np.linalg.norm(g_prime_exact_vec)\n        \n        if norm_exact == 0:\n            err_naive = np.linalg.norm(g_prime_naive_vec.real - g_prime_exact_vec)\n            err_32 = np.linalg.norm(g_prime_32_vec.real - g_prime_exact_vec)\n        else:\n            err_naive = np.linalg.norm(g_prime_naive_vec.real - g_prime_exact_vec) / norm_exact\n            err_32 = np.linalg.norm(g_prime_32_vec.real - g_prime_exact_vec) / norm_exact\n\n        return [err_naive, err_32]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: Happy path, moderate nonlinearity\n        (32, lambda x: np.sin(3 * x) + 0.5 * np.cos(5 * x), \n             lambda x: 3 * np.cos(3 * x) - 2.5 * np.sin(5 * x)),\n        # Case B: High-frequency content near Nyquist limit on N\n        (32, lambda x: 0.7 * np.sin(10 * x) + 0.6 * np.cos(11 * x), \n             lambda x: 7 * np.cos(10 * x) - 6.6 * np.sin(11 * x)),\n        # Case C: Very coarse grid, strong aliasing\n        (8,  lambda x: np.sin(3 * x), \n             lambda x: 3 * np.cos(3 * x)),\n        # Case D: Moderate grid with modes near truncation\n        (48, lambda x: np.sin(12 * x) + 0.3 * np.sin(13 * x), \n             lambda x: 12 * np.cos(12 * x) + 3.9 * np.cos(13 * x))\n    ]\n\n    results = []\n    for N, u_func, u_prime_func in test_cases:\n        result = run_case(N, u_func, u_prime_func)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3417612"}]}