{"hands_on_practices": [{"introduction": "A cornerstone of working with any polynomial basis is the ability to evaluate its series representation accurately and efficiently. This exercise focuses on a fundamental computational task: the stable evaluation of a truncated Chebyshev series. By implementing and contrasting the robust Clenshaw algorithm with a naive trigonometric approach, you will gain direct experience with the numerical challenges that can arise and appreciate how exploiting the inherent three-term recurrence relation of the polynomials leads to a superior method [@problem_id:3370007].", "problem": "Given a bounded interval and a polynomial basis, a crucial task in spectral and Discontinuous Galerkin (DG) methods is numerically stable evaluation of truncated series at specified points. Consider the Chebyshev polynomials of the first kind, defined for $x \\in [-1,1]$ by $T_n(x) := \\cos(n \\arccos x)$, where angles are in radians. These polynomials satisfy the three-term recurrence $T_0(x) = 1$, $T_1(x) = x$, and $T_{n+1}(x) = 2 x T_n(x) - T_{n-1}(x)$ for all integers $n \\ge 1$, and are orthogonal on $[-1,1]$ with respect to the weight $w(x) = (1-x^2)^{-1/2}$. For a general physical interval $[a,b]$, the canonical affine mapping from $x \\in [a,b]$ to the Chebyshev domain is $z(x) = \\frac{2x-(a+b)}{b-a}$.\n\nYour task is to implement a numerically stable backward recurrence (Clenshaw algorithm) to evaluate the truncated Chebyshev series $S_N(x) = \\sum_{n=0}^N a_n T_n(x)$ at given points $x \\in [-1,1]$, and also under the affine mapping when $x \\in [a,b]$. You must additionally compute a baseline “naive” evaluation of the same series using the defining identity $T_n(x) = \\cos(n \\arccos x)$, and report the absolute error between the Clenshaw result and the naive result for each test case.\n\nStarting from the core definitions above, derive the necessary backward recurrence structure that makes the Clenshaw algorithm stable for Chebyshev series on $[-1,1]$, and incorporate this structure in your implementation. Use double-precision floating-point arithmetic.\n\nAngles must be treated in radians.\n\nTest Suite:\n- Case $1$ (general interior point): Let $N = 20$ and coefficients $a_n = \\frac{(-1)^n}{n+1}$ for $n = 0,1,\\dots,20$. Evaluate at $x = 0.3$.\n- Case $2$ (right endpoint): Same $N$ and $a_n$ as Case $1$, evaluate at $x = 1$.\n- Case $3$ (left endpoint): Same $N$ and $a_n$ as Case $1$, evaluate at $x = -1$.\n- Case $4$ (near-endpoint stress test): Same $N$ and $a_n$ as Case $1$, evaluate at $x = 1 - 10^{-12}$.\n- Case $5$ (mapped interval evaluation): Consider the physical interval $[a,b] = [2,5]$. Let $N = 7$ with coefficients $b_n$ satisfying $b_7 = 1$ and $b_n = 0$ for $n \\ne 7$. Evaluate the series at $x = 3.5$ by first mapping to $z(x) = \\frac{2x-(a+b)}{b-a}$ and then using the Clenshaw algorithm for $z(x) \\in [-1,1]$. Compare the Clenshaw result to the naive value $T_7(z(x)) = \\cos(7 \\arccos(z(x)))$.\n\nFor each case, compute the absolute error as a float $E = |S_N^{\\mathrm{Clenshaw}}(x) - S_N^{\\mathrm{naive}}(x)|$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the five errors as a comma-separated list enclosed in square brackets, in the order of the cases described above, for example, $[e_1,e_2,e_3,e_4,e_5]$ where each $e_i$ is a float representing the absolute error of Case $i$.", "solution": "The problem requires the implementation and comparison of two methods for evaluating a truncated Chebyshev series, $S_N(x) = \\sum_{n=0}^N a_n T_n(x)$. The first method is the Clenshaw algorithm, a numerically stable backward recurrence. The second is a \"naive\" evaluation based on the trigonometric definition $T_n(x) = \\cos(n \\arccos x)$. We must compute the absolute error between the results of these two methods for several test cases.\n\n### Principle-Based Design and Derivation\n\nThe evaluation of a sum of functions that satisfy a three-term recurrence relation can often be performed efficiently and stably using the Clenshaw algorithm. This algorithm is a generalization of Horner's method for polynomial evaluation and avoids the potential numerical instability of directly summing terms, which may be prone to subtractive cancellation and error accumulation.\n\n#### 1. Derivation of the Clenshaw Algorithm for Chebyshev Series\n\nThe Chebyshev polynomials of the first kind, $T_n(x)$, satisfy the three-term recurrence relation:\n$$ T_{n+1}(x) = 2x T_n(x) - T_{n-1}(x) \\quad \\text{for } n \\ge 1 $$\nThis holds with $T_0(x) = 1$ and $T_1(x) = x$.\n\nWe wish to evaluate the sum $S_N(x) = \\sum_{n=0}^N a_n T_n(x)$. We can rewrite the recurrence as $T_{n-1}(x) = 2x T_n(x) - T_{n+1}(x)$.\n\nThe Clenshaw algorithm defines an auxiliary sequence, let's call it $y_k$, using a backward recurrence. Let us separate the $n=0$ term from the sum and focus on the remaining part, $S'_N(x) = \\sum_{n=1}^N a_n T_n(x)$.\n\nWe define the sequence $y_k$ for $k = N, N-1, \\dots, 1$ as follows:\nInitialize with $y_{N+2} = 0$ and $y_{N+1} = 0$.\nThe recurrence is:\n$$ y_k = a_k + 2x y_{k+1} - y_{k+2} \\quad \\text{for } k = N, N-1, \\dots, 1 $$\nThis allows us to write $a_k = y_k - 2x y_{k+1} + y_{k+2}$. Substituting this into the sum $S'_N(x)$:\n$$ S'_N(x) = \\sum_{n=1}^N (y_n - 2x y_{n+1} + y_{n+2}) T_n(x) $$\nWe can split this into three sums:\n$$ S'_N(x) = \\sum_{n=1}^N y_n T_n(x) - 2x \\sum_{n=1}^N y_{n+1} T_n(x) + \\sum_{n=1}^N y_{n+2} T_n(x) $$\nBy re-indexing the second and third sums and collecting terms for each $y_k$:\n-   The coefficient of $y_k$ for $k \\in [3, N]$ is $T_k(x) - 2x T_{k-1}(x) + T_{k-2}(x)$. From the recurrence relation, this is exactly $0$.\n-   We need to examine the boundary terms for $k=1, 2$ and the terms involving $y_{N+1}, y_{N+2}$ which are defined to be zero.\n\nLet's expand the sums and collect terms for $y_k$:\n\\begin{align*}\nS'_N(x) &= (y_1 T_1 + y_2 T_2 + y_3 T_3 + \\dots) \\\\\n         &\\quad - 2x (y_2 T_1 + y_3 T_2 + y_4 T_3 + \\dots) \\\\\n         &\\quad + (y_3 T_1 + y_4 T_2 + y_5 T_3 + \\dots)\n\\end{align*}\nWait, the re-indexing was not done correctly. Let's be more formal.\n$ \\sum_{n=1}^N y_n T_n(x) = y_1 T_1 + y_2 T_2 + \\sum_{n=3}^N y_n T_n(x) $\n$ -2x\\sum_{n=1}^N y_{n+1} T_n(x) = -2x y_2 T_1 -2x \\sum_{n=2}^{N-1} y_{n+1} T_n(x) -2x y_{N+1}T_N(x) $. Change index $k=n+1$: $-2x \\sum_{k=3}^N y_k T_{k-1}(x)$.\n$ \\sum_{n=1}^N y_{n+2} T_n(x) = \\sum_{k=3}^{N+2} y_k T_{k-2}(x) = \\sum_{k=3}^N y_k T_{k-2}(x) $ since $y_{N+1}=y_{N+2}=0$.\n\nCombining all terms:\n$$ S'_N(x) = y_1 T_1(x) + y_2 T_2(x) - 2x y_2 T_1(x) + \\sum_{k=3}^N y_k (T_k(x) - 2x T_{k-1}(x) + T_{k-2}(x)) $$\nThe sum from $k=3$ to $N$ vanishes due to the recurrence relation. The remaining part is:\n$$ S'_N(x) = y_1 T_1(x) + y_2 (T_2(x) - 2x T_1(x)) $$\nUsing $T_1(x)=x$ and $T_2(x)=2x^2-1$, we get $T_2(x) - 2x T_1(x) = (2x^2-1) - 2x(x) = -1 = -T_0(x)$.\nSo, $S'_N(x) = y_1 T_1(x) - y_2 T_0(x) = x y_1 - y_2$.\n\nThe total sum is $S_N(x) = a_0 T_0(x) + S'_N(x)$. Since $T_0(x)=1$, we have:\n$$ S_N(x) = a_0 + x y_1 - y_2 $$\nThis is the final formula for the Clenshaw evaluation. The algorithm is:\n1.  Initialize $y_{N+1} = 0$, $y_{N+2} = 0$.\n2.  For $k = N, N-1, \\dots, 1$, compute $y_k = a_k + (2x) y_{k+1} - y_{k+2}$.\n3.  The sum is $S_N(x) = a_0 + x y_1 - y_2$.\n\n#### 2. Naive Evaluation Method\n\nThe naive method directly uses the definition $T_n(x) = \\cos(n \\arccos x)$. The sum is computed as:\n$$ S_N(x) = \\sum_{n=0}^N a_n \\cos(n \\arccos x) $$\nThe algorithm is:\n1.  Compute $\\theta = \\arccos(x)$.\n2.  Initialize a sum $S = 0$.\n3.  For $n = 0, 1, \\dots, N$, compute $T_n(x) = \\cos(n\\theta)$ and add $a_n T_n(x)$ to $S$.\n\nThis method can suffer from numerical inaccuracies. For $x$ close to $\\pm 1$, the function $\\arccos(x)$ is ill-conditioned, and small errors in $x$ can lead to larger errors in $\\theta$. Subsequent summation of terms, especially if they are of alternating sign and similar magnitude (as in Cases 1-4), can lead to loss of precision.\n\n### Implementation for Test Cases\n\nThe implementation will consist of two functions, `eval_clenshaw` and `eval_naive`, corresponding to the algorithms derived above. Both functions take the coefficient array and the evaluation point $x$ as input.\n\n-   For Cases 1-4, we use $N=20$ and coefficients $a_n = \\frac{(-1)^n}{n+1}$, evaluated at the given points $x \\in [-1,1]$.\n-   For Case 5, the physical interval is $[a,b] = [2,5]$. The evaluation point is $x_{\\text{phys}} = 3.5$. We first map this to the canonical interval $[-1,1]$ using the given affine map:\n    $$ z(x) = \\frac{2x-(a+b)}{b-a} \\implies z(3.5) = \\frac{2(3.5)-(2+5)}{5-2} = \\frac{7-7}{3} = 0 $$\n    The series has coefficients $b_n$ such that $b_7=1$ and all other $b_n=0$ for $n \\in [0,7]$. So we evaluate the sum for $N=7$ with these coefficients at the point $z=0$.\n\nThe absolute error is computed as $E = |S_N^{\\mathrm{Clenshaw}}(x) - S_N^{\\mathrm{naive}}(x)|$ for each case. We use double-precision floating-point numbers for all calculations.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing and comparing Clenshaw's algorithm and a naive\n    evaluation for Chebyshev series, then reporting the absolute errors.\n    \"\"\"\n\n    def eval_clenshaw(coeffs: np.ndarray, x: float) -> float:\n        \"\"\"\n        Evaluates a Chebyshev series using Clenshaw's algorithm.\n        S_N(x) = sum_{n=0 to N} coeffs[n] * T_n(x)\n        \"\"\"\n        N = len(coeffs) - 1\n        if N < 0:\n            return 0.0\n        if N == 0:\n            return coeffs[0]\n\n        # In correspondence with the derivation: y_{N+1} and y_{N+2} are 0.\n        # d1 holds y_{k+1} and d2 holds y_{k+2} in the loop.\n        d1 = 0.0\n        d2 = 0.0\n        x2 = 2.0 * x\n        \n        # Backward recurrence for k = N, N-1, ..., 1\n        for k in range(N, 0, -1):\n            d_new = coeffs[k] + x2 * d1 - d2\n            d2 = d1\n            d1 = d_new\n        \n        # After the loop, d1 corresponds to y_1 and d2 to y_2.\n        # The sum is a_0 + x*y_1 - y_2\n        return coeffs[0] + x * d1 - d2\n\n    def eval_naive(coeffs: np.ndarray, x: float) -> float:\n        \"\"\"\n        Evaluates a Chebyshev series using the direct definition T_n(x) = cos(n*arccos(x)).\n        \"\"\"\n        N = len(coeffs) - 1\n        if N < 0:\n            return 0.0\n\n        # Clip x to avoid domain errors in arccos due to floating point inaccuracies\n        x_clipped = np.clip(x, -1.0, 1.0)\n        theta = np.arccos(x_clipped)\n        \n        s = 0.0\n        for n in range(N + 1):\n            T_n = np.cos(n * theta)\n            s += coeffs[n] * T_n\n        \n        return s\n\n    test_cases = [\n        # (N, coeffs_generator, x, mapping_params)\n        (20, lambda n: (-1)**n / (n + 1), 0.3, None),  # Case 1\n        (20, lambda n: (-1)**n / (n + 1), 1.0, None),  # Case 2\n        (20, lambda n: (-1)**n / (n + 1), -1.0, None), # Case 3\n        (20, lambda n: (-1)**n / (n + 1), 1.0 - 1e-12, None), # Case 4\n        (7, lambda n: 1.0 if n == 7 else 0.0, 3.5, (2.0, 5.0)), # Case 5\n    ]\n\n    results = []\n    for N, coeff_gen, x_val, mapping in test_cases:\n        coeffs = np.array([coeff_gen(n) for n in range(N + 1)], dtype=np.float64)\n        \n        x_eval = x_val\n        if mapping:\n            a, b = mapping\n            x_eval = (2.0 * x_val - (a + b)) / (b - a)\n\n        val_clenshaw = eval_clenshaw(coeffs, x_eval)\n        val_naive = eval_naive(coeffs, x_eval)\n        \n        error = np.abs(val_clenshaw - val_naive)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3370007"}, {"introduction": "Beyond function approximation, the true power of spectral bases is revealed in solving differential equations. The algebraic structure of the resulting linear systems is critical for computational performance. This practice demonstrates how to leverage the intrinsic parity of Chebyshev polynomials—where even-indexed polynomials are even functions and odd-indexed are odd—to simplify the solution of a boundary value problem [@problem_id:3370039]. You will analytically show how this symmetry decouples the system, leading to a smaller, better-conditioned problem, a key strategy for developing efficient spectral solvers.", "problem": "Consider the interval $[-1,1]$ and the Chebyshev polynomials of the first kind $T_{n}(x)$, defined by $T_{n}(\\cos\\theta)=\\cos(n\\theta)$ for $\\theta\\in[0,\\pi]$. They satisfy the parity relation $T_{n}(-x)=(-1)^{n}T_{n}(x)$ and the weighted orthogonality relation\n$$\n\\int_{-1}^{1}\\frac{T_{m}(x)T_{n}(x)}{\\sqrt{1-x^{2}}}\\,\\mathrm{d}x = \\begin{cases}\n\\pi, & n=m=0, \\\\[4pt]\n\\frac{\\pi}{2}, & n=m\\ge 1, \\\\[4pt]\n0, & n\\ne m.\n\\end{cases}\n$$\nDefine the orthonormal Chebyshev basis $\\widehat{T}_{0}(x)=T_{0}(x)/\\sqrt{\\pi}$ and $\\widehat{T}_{n}(x)=T_{n}(x)/\\sqrt{\\pi/2}$ for $n\\ge 1$ with respect to the inner product $\\langle u,v\\rangle=\\int_{-1}^{1}u(x)v(x)\\,(1-x^{2})^{-1/2}\\,\\mathrm{d}x$. Consider the symmetric boundary value problem on $[-1,1]$ with Dirichlet boundary conditions $u(-1)=0$ and $u(1)=0$, and an even right-hand side so that the exact solution is even. Approximate $u$ in the $(N+1)$-dimensional trial space $\\mathcal{V}_{N}=\\operatorname{span}\\{\\widehat{T}_{0},\\widehat{T}_{1},\\ldots,\\widehat{T}_{N}\\}$ with $N=3$ by the Tau method: enforce the interior equation in a weighted $L^{2}$ sense using the identity operator as the model bilinear form $\\langle u,v\\rangle$ on the first $N-1$ test modes, and impose the two Dirichlet boundary conditions as Tau constraints in the last two equations. This produces a square linear system $A_{\\text{full}}\\in\\mathbb{R}^{4\\times 4}$ for the coefficients of $u$ in the basis $\\{\\widehat{T}_{n}\\}_{n=0}^{3}$.\n\nUsing only the fundamental definitions above, perform the following:\n\n1. Derive the explicit entries of $A_{\\text{full}}$ for $N=3$ by writing down the two interior equations corresponding to testing with $\\widehat{T}_{0}$ and $\\widehat{T}_{1}$ and the two boundary equations $u(1)=0$ and $u(-1)=0$ in terms of the coefficient vector of $u$ in the basis $\\{\\widehat{T}_{n}\\}_{n=0}^{3}$. Then show how parity decouples the system into even and odd subspaces.\n\n2. Exploit the even symmetry of the exact solution to reduce the approximation space to the even subspace $\\mathcal{V}_{N}^{\\text{even}}=\\operatorname{span}\\{\\widehat{T}_{0},\\widehat{T}_{2}\\}$ and the corresponding test space, replacing the two boundary equations by the single constraint $u(1)=0$ appropriate to even functions. Derive the reduced $2\\times 2$ system matrix $A_{\\text{even}}$.\n\n3. Compute the spectral two-norm condition numbers $\\kappa_{2}(A_{\\text{full}})$ and $\\kappa_{2}(A_{\\text{even}})$ directly from the singular values of $A_{\\text{full}}$ and $A_{\\text{even}}$, by forming $A_{\\text{full}}^{\\mathsf{T}}A_{\\text{full}}$ and $A_{\\text{even}}^{\\mathsf{T}}A_{\\text{even}}$, finding their eigenvalues in closed form, and taking square roots. Conclude with the exact analytic expression for the ratio\n$$\nR \\equiv \\frac{\\kappa_{2}(A_{\\text{full}})}{\\kappa_{2}(A_{\\text{even}})}.\n$$\n\nYour final answer must be the closed-form expression for $R$ as a function of $\\pi$. No numerical rounding is permitted, and no units are required. Clearly state any auxiliary algebraic quantities you introduce in your derivations within your solution, but the final answer must be a single analytic expression for $R$.", "solution": "The solution is presented in three parts as requested. The unknown vector of coefficients is $\\hat{\\mathbf{u}} = [\\hat{u}_0, \\hat{u}_1, \\hat{u}_2, \\hat{u}_3]^{\\mathsf{T}}$ for the full system and $\\hat{\\mathbf{u}}_{\\text{even}} = [\\hat{u}_0, \\hat{u}_2]^{\\mathsf{T}}$ for the reduced system.\n\n#### 1. Derivation of $A_{\\text{full}}$ and Parity Decoupling\n\nThe approximation is $u_3(x) = \\sum_{n=0}^{3} \\hat{u}_n \\widehat{T}_n(x)$. The $4 \\times 4$ linear system for the coefficients $\\hat{u}_n$ is constructed as follows:\n\nThe first two equations come from the interior problem, tested against $\\widehat{T}_0$ and $\\widehat{T}_1$:\n- For $k=0$: $\\langle u_3, \\widehat{T}_0 \\rangle = \\sum_{n=0}^{3} \\hat{u}_n \\langle \\widehat{T}_n, \\widehat{T}_0 \\rangle = \\hat{u}_0$. The equation is $\\hat{u}_0 = \\hat{f}_0$.\n- For $k=1$: $\\langle u_3, \\widehat{T}_1 \\rangle = \\sum_{n=0}^{3} \\hat{u}_n \\langle \\widehat{T}_n, \\widehat{T}_1 \\rangle = \\hat{u}_1$. The equation is $\\hat{u}_1 = \\hat{f}_1$.\nThese give the first two rows of $A_{\\text{full}}$ as $[1, 0, 0, 0]$ and $[0, 1, 0, 0]$.\n\nThe next two equations are the boundary constraints. We use the properties $T_n(1)=1$ for all $n \\ge 0$ and $T_n(-1)=(-1)^n$. The orthonormal basis functions at the boundaries are:\n$\\widehat{T}_0(1) = 1/\\sqrt{\\pi}$, $\\widehat{T}_n(1) = \\sqrt{2/\\pi}$ for $n \\ge 1$.\n$\\widehat{T}_0(-1) = 1/\\sqrt{\\pi}$, $\\widehat{T}_n(-1) = (-1)^n \\sqrt{2/\\pi}$ for $n \\ge 1$.\n\n- Boundary condition at $x=1$: $u_3(1) = \\sum_{n=0}^{3} \\hat{u}_n \\widehat{T}_n(1) = \\hat{u}_0 \\frac{1}{\\sqrt{\\pi}} + \\hat{u}_1 \\frac{\\sqrt{2}}{\\sqrt{\\pi}} + \\hat{u}_2 \\frac{\\sqrt{2}}{\\sqrt{\\pi}} + \\hat{u}_3 \\frac{\\sqrt{2}}{\\sqrt{\\pi}} = 0$.\n- Boundary condition at $x=-1$: $u_3(-1) = \\sum_{n=0}^{3} \\hat{u}_n \\widehat{T}_n(-1) = \\hat{u}_0 \\frac{1}{\\sqrt{\\pi}} - \\hat{u}_1 \\frac{\\sqrt{2}}{\\sqrt{\\pi}} + \\hat{u}_2 \\frac{\\sqrt{2}}{\\sqrt{\\pi}} - \\hat{u}_3 \\frac{\\sqrt{2}}{\\sqrt{\\pi}} = 0$.\n\nThe full system matrix $A_{\\text{full}}$ is:\n$$\nA_{\\text{full}} = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n\\frac{1}{\\sqrt{\\pi}} & \\frac{\\sqrt{2}}{\\sqrt{\\pi}} & \\frac{\\sqrt{2}}{\\sqrt{\\pi}} & \\frac{\\sqrt{2}}{\\sqrt{\\pi}} \\\\\n\\frac{1}{\\sqrt{\\pi}} & -\\frac{\\sqrt{2}}{\\sqrt{\\pi}} & \\frac{\\sqrt{2}}{\\sqrt{\\pi}} & -\\frac{\\sqrt{2}}{\\sqrt{\\pi}}\n\\end{pmatrix}.\n$$\n\nTo show parity decoupling, we consider a new set of boundary equations by summing and differencing the original two:\n1. $u_3(1) + u_3(-1) = 2 \\left( \\hat{u}_0 \\frac{1}{\\sqrt{\\pi}} + \\hat{u}_2 \\frac{\\sqrt{2}}{\\sqrt{\\pi}} \\right) = 0$. This equation involves only coefficients of even basis functions ($\\widehat{T}_0, \\widehat{T}_2$).\n2. $u_3(1) - u_3(-1) = 2 \\left( \\hat{u}_1 \\frac{\\sqrt{2}}{\\sqrt{\\pi}} + \\hat{u}_3 \\frac{\\sqrt{2}}{\\sqrt{\\pi}} \\right) = 0$. This equation involves only coefficients of odd basis functions ($\\widehat{T}_1, \\widehat{T}_3$).\n\nThe full system decouples into an even-mode system for $(\\hat{u}_0, \\hat{u}_2)$ and an odd-mode system for $(\\hat{u}_1, \\hat{u}_3)$. Since the exact solution is even, we expect $\\hat{u}_1=0$ and $\\hat{u}_3=0$. Indeed, since the RHS $f(x)$ is even, its projection on the odd mode $\\widehat{T}_1$ is zero, so $\\hat{f}_1=0$, and the second equation of the system gives $\\hat{u}_1=0$. The second decoupled boundary equation then gives $\\hat{u}_3=0$.\n\n#### 2. Derivation of $A_{\\text{even}}$\n\nWe exploit the even symmetry by restricting the trial space to $\\mathcal{V}_{3}^{\\text{even}}=\\operatorname{span}\\{\\widehat{T}_{0},\\widehat{T}_{2}\\}$. The approximation is $u_{\\text{even}}(x) = \\hat{u}_0 \\widehat{T}_0(x) + \\hat{u}_2 \\widehat{T}_2(x)$. We need two equations for the two unknowns $\\hat{u}_0, \\hat{u}_2$.\nFor an even function, the two boundary conditions $u(1)=0$ and $u(-1)=0$ are not independent, as $u(-1)=u(1)$. We use one interior equation and one boundary constraint.\n- The interior equation is obtained by testing against the first even basis function, $\\widehat{T}_0$: $\\langle u_{\\text{even}}, \\widehat{T}_0 \\rangle = \\hat{u}_0$. This gives the row $[1, 0]$.\n- The boundary constraint is $u_{\\text{even}}(1) = \\hat{u}_0 \\widehat{T}_0(1) + \\hat{u}_2 \\widehat{T}_2(1) = \\hat{u}_0 \\frac{1}{\\sqrt{\\pi}} + \\hat{u}_2 \\frac{\\sqrt{2}}{\\sqrt{\\pi}} = 0$. This gives the row $[\\frac{1}{\\sqrt{\\pi}}, \\frac{\\sqrt{2}}{\\sqrt{\\pi}}]$.\n\nThe reduced system matrix $A_{\\text{even}}$ is:\n$$\nA_{\\text{even}} = \\begin{pmatrix}\n1 & 0 \\\\\n\\frac{1}{\\sqrt{\\pi}} & \\frac{\\sqrt{2}}{\\sqrt{\\pi}}\n\\end{pmatrix}.\n$$\n\n#### 3. Condition Numbers and Ratio $R$\n\nThe spectral condition number is $\\kappa_2(A) = \\sigma_{\\max}(A)/\\sigma_{\\min}(A)$, where the singular values $\\sigma_i(A)$ are the square roots of the eigenvalues of $A^{\\mathsf{T}}A$.\n\n**Condition number of $A_{\\text{full}}$**:\nWe compute $A_{\\text{full}}^{\\mathsf{T}}A_{\\text{full}}$:\n$$\nA_{\\text{full}}^{\\mathsf{T}}A_{\\text{full}} = \\frac{1}{\\pi} \\begin{pmatrix}\n\\pi+2 & 0 & 2\\sqrt{2} & 0 \\\\\n0 & \\pi+4 & 0 & 4 \\\\\n2\\sqrt{2} & 0 & 4 & 0 \\\\\n0 & 4 & 0 & 4\n\\end{pmatrix}.\n$$\nThis matrix is block-diagonal under permutation of indices $(1,2)$. The eigenvalues are those of the two $2 \\times 2$ blocks.\nEven block: $M_e = \\frac{1}{\\pi}\\begin{pmatrix} \\pi+2 & 2\\sqrt{2} \\\\ 2\\sqrt{2} & 4 \\end{pmatrix}$. Its characteristic equation for eigenvalues $\\lambda$ is $\\lambda^2 - \\frac{\\pi+6}{\\pi}\\lambda + \\frac{4}{\\pi} = 0$.\nThe eigenvalues are $\\lambda_{e, \\pm} = \\frac{1}{2\\pi} (\\pi+6 \\pm \\sqrt{\\pi^2 - 4\\pi + 36})$.\nOdd block: $M_o = \\frac{1}{\\pi}\\begin{pmatrix} \\pi+4 & 4 \\\\ 4 & 4 \\end{pmatrix}$. Its characteristic equation is $\\lambda^2 - \\frac{\\pi+8}{\\pi}\\lambda + \\frac{4}{\\pi} = 0$.\nThe eigenvalues are $\\lambda_{o, \\pm} = \\frac{1}{2\\pi} (\\pi+8 \\pm \\sqrt{\\pi^2 + 64})$.\n\nTo find $\\sigma_{\\max}$ and $\\sigma_{\\min}$, we must find the largest and smallest of these four eigenvalues. By inspection and analysis, for $\\pi > 0$, we have $\\pi+8+\\sqrt{\\pi^2+64} > \\pi+6+\\sqrt{\\pi^2-4\\pi+36}$ and $\\pi+8-\\sqrt{\\pi^2+64} < \\pi+6-\\sqrt{\\pi^2-4\\pi+36}$, the latter being equivalent to $16\\pi > 0$. Thus, the maximum and minimum eigenvalues both come from the odd block.\n$\\sigma_{\\max}^2(A_{\\text{full}}) = \\lambda_{o,+} = \\frac{\\pi+8 + \\sqrt{\\pi^2+64}}{2\\pi}$.\n$\\sigma_{\\min}^2(A_{\\text{full}}) = \\lambda_{o,-} = \\frac{\\pi+8 - \\sqrt{\\pi^2+64}}{2pi}$.\n\nThe squared condition number is the ratio of these eigenvalues:\n$$\n\\kappa_2(A_{\\text{full}})^2 = \\frac{\\lambda_{o,+}}{\\lambda_{o,-}} = \\frac{\\pi+8 + \\sqrt{\\pi^2+64}}{\\pi+8 - \\sqrt{\\pi^2+64}} = \\frac{(\\pi+8 + \\sqrt{\\pi^2+64})^2}{(\\pi+8)^2 - (\\pi^2+64)} = \\frac{(\\pi+8 + \\sqrt{\\pi^2+64})^2}{16\\pi}.\n$$\nTaking the square root gives:\n$$\n\\kappa_2(A_{\\text{full}}) = \\frac{\\pi+8 + \\sqrt{\\pi^2+64}}{4\\sqrt{\\pi}}.\n$$\n\n**Condition number of $A_{\\text{even}}$**:\nWe compute $A_{\\text{even}}^{\\mathsf{T}}A_{\\text{even}}$:\n$$\nA_{\\text{even}}^{\\mathsf{T}}A_{\\text{even}} = \\begin{pmatrix} 1 & \\frac{1}{\\sqrt{\\pi}} \\\\ 0 & \\frac{\\sqrt{2}}{\\sqrt{\\pi}} \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ \\frac{1}{\\sqrt{\\pi}} & \\frac{\\sqrt{2}}{\\sqrt{\\pi}} \\end{pmatrix} = \\begin{pmatrix} 1+\\frac{1}{\\pi} & \\frac{\\sqrt{2}}{\\pi} \\\\ \\frac{\\sqrt{2}}{\\pi} & \\frac{2}{\\pi} \\end{pmatrix} = \\frac{1}{\\pi} \\begin{pmatrix} \\pi+1 & \\sqrt{2} \\\\ \\sqrt{2} & 2 \\end{pmatrix}.\n$$\nThe eigenvalues $\\lambda$ of this matrix satisfy $\\lambda^2 - \\frac{\\pi+3}{\\pi}\\lambda + \\frac{2}{\\pi}=0$.\nThe eigenvalues are $\\lambda_{\\pm} = \\frac{1}{2\\pi} (\\pi+3 \\pm \\sqrt{(\\pi+3)^2 - 8\\pi}) = \\frac{1}{2\\pi} (\\pi+3 \\pm \\sqrt{\\pi^2 - 2\\pi + 9})$.\nThese are $\\sigma_{\\max}^2(A_{\\text{even}})$ and $\\sigma_{\\min}^2(A_{\\text{even}})$. The squared condition number is:\n$$\n\\kappa_2(A_{\\text{even}})^2 = \\frac{\\lambda_{+}}{\\lambda_{-}} = \\frac{\\pi+3 + \\sqrt{\\pi^2 - 2\\pi + 9}}{\\pi+3 - \\sqrt{\\pi^2 - 2\\pi + 9}} = \\frac{(\\pi+3 + \\sqrt{\\pi^2 - 2\\pi + 9})^2}{(\\pi+3)^2 - (\\pi^2 - 2\\pi + 9)} = \\frac{(\\pi+3 + \\sqrt{\\pi^2 - 2\\pi + 9})^2}{8\\pi}.\n$$\nTaking the square root gives:\n$$\n\\kappa_2(A_{\\text{even}}) = \\frac{\\pi+3 + \\sqrt{\\pi^2 - 2\\pi + 9}}{2\\sqrt{2\\pi}}.\n$$\n\n**Ratio $R$**:\nFinally, we compute the ratio $R = \\kappa_{2}(A_{\\text{full}}) / \\kappa_{2}(A_{\\text{even}})$:\n$$\nR = \\frac{\\frac{\\pi+8 + \\sqrt{\\pi^2+64}}{4\\sqrt{\\pi}}}{\\frac{\\pi+3 + \\sqrt{\\pi^2 - 2\\pi + 9}}{2\\sqrt{2\\pi}}} = \\frac{\\pi+8 + \\sqrt{\\pi^2+64}}{4\\sqrt{\\pi}} \\cdot \\frac{2\\sqrt{2}\\sqrt{\\pi}}{\\pi+3 + \\sqrt{\\pi^2 - 2\\pi + 9}}.\n$$\nSimplifying the expression yields:\n$$\nR = \\frac{2\\sqrt{2}}{4} \\frac{\\pi+8 + \\sqrt{\\pi^2+64}}{\\pi+3 + \\sqrt{\\pi^2 - 2\\pi + 9}} = \\frac{\\sqrt{2}}{2} \\frac{\\pi+8 + \\sqrt{\\pi^2+64}}{\\pi+3 + \\sqrt{\\pi^2 - 2\\pi + 9}}.\n$$", "answer": "$$ \\boxed{ \\frac{\\sqrt{2}}{2} \\frac{\\pi+8 + \\sqrt{\\pi^2+64}}{\\pi+3 + \\sqrt{\\pi^2 - 2\\pi + 9}} } $$", "id": "3370039"}, {"introduction": "This advanced practice moves from solving well-posed forward problems to the challenging domain of inverse problems, where the goal is to infer an initial state from noisy, evolved data. The fact that Chebyshev polynomials are eigenfunctions of the Chebyshev-Gauss-Lobatto Sturm-Liouville operator makes them a natural basis for analyzing diffusion processes. This exercise explores how to regularize an ill-posed inverse heat problem by truncating the Chebyshev spectrum, forcing you to confront the fundamental bias-variance tradeoff that governs the quality of the reconstruction from noisy observations [@problem_id:3370042].", "problem": "Consider a bounded interval $[-1,1]$ and the Chebyshev polynomial basis $\\{T_k(x)\\}_{k=0}^{\\infty}$, where $T_k(x) = \\cos(k \\arccos x)$. Let the weighted inner product be defined with the Chebyshev weight $w(x) = (1-x^2)^{-1/2}$ so that the orthogonality relation is $\\int_{-1}^{1} T_m(x) T_n(x) w(x)\\,dx = 0$ for $m \\neq n$, with $\\int_{-1}^{1} T_0(x)^2 w(x)\\,dx = \\pi$ and $\\int_{-1}^{1} T_k(x)^2 w(x)\\,dx = \\pi/2$ for $k \\ge 1$. Consider the diffusion generated by the Chebyshev Sturm–Liouville operator $\\mathcal{L} u = (1 - x^2) u_{xx} - x u_x$, for which $T_k$ are eigenfunctions satisfying $\\mathcal{L} T_k = -k^2 T_k$. \n\nLet the initial condition be $u_0(x) = \\sum_{k=0}^{\\infty} a_k T_k(x)$ with unknown Chebyshev coefficients $\\{a_k\\}_{k=0}^{\\infty}$, and let the forward model at time $t>0$ be $u(x,t)$ solving $u_t = \\mathcal{L} u$ with $u(x,0) = u_0(x)$. By the spectral decomposition in the Chebyshev basis, the forward map on coefficients is $b_k(t) = a_k e^{-k^2 t}$ for $k \\in \\mathbb{N}_0$. Suppose we observe noisy spectral data in coefficient space,\n$$\n\\tilde{b}_k(t) = b_k(t) + \\varepsilon_k,\n$$\nwhere $\\{\\varepsilon_k\\}$ are independent, zero-mean random variables with variance $\\mathbb{V}[\\varepsilon_k] = \\sigma^2$, the same for all $k$.\n\nWe regularize the inverse problem of estimating $\\{a_k\\}$ from $\\{\\tilde{b}_k(t)\\}$ by spectral truncation at a cut-off $k_c \\in \\mathbb{N}_0$. Define the estimator\n$$\n\\hat{a}_k =\n\\begin{cases}\n\\tilde{b}_k(t)\\, e^{k^2 t}, & 0 \\le k \\le k_c,\\\\\n0, & k > k_c.\n\\end{cases}\n$$\nThe reconstruction is $\\hat{u}_0(x) = \\sum_{k=0}^{\\infty} \\hat{a}_k T_k(x)$. Consider the error measured in the weighted $L^2$ norm associated with the Chebyshev weight,\n$$\n\\|f\\|_{L^2_w}^2 := \\int_{-1}^{1} f(x)^2 w(x)\\,dx.\n$$\n\nTasks:\n1. Starting only from the above operator-eigenfunction relation, orthogonality, and the noise model, derive an exact expression for the expected squared reconstruction error $\\mathbb{E}\\left[\\| \\hat{u}_0 - u_0 \\|_{L^2_w}^2\\right]$ as a function of the cut-off $k_c$, the time $t$, the noise level $\\sigma^2$, and the coefficients $\\{a_k\\}$.\n2. Specialize to the following coefficient sequence for the ground truth:\n$$\na_0 = 1,\\quad a_k = \\frac{(-1)^k}{(k+1)^2}\\ \\text{for}\\ k \\ge 1.\n$$\nBecause the sums are infinite, approximate them numerically by truncating the series at a sufficiently large $K_{\\max}$. For the purposes of this problem, you must set $K_{\\max} = 500$ in your program and treat $\\{a_k\\}_{k=0}^{K_{\\max}}$ exactly as above, with all terms beyond $K_{\\max}$ ignored.\n\n3. Implement a program that computes the expected squared error $\\mathbb{E}\\left[\\| \\hat{u}_0 - u_0 \\|_{L^2_w}^2\\right]$ for each parameter set in the following test suite, where each test case is a tuple $(t, \\sigma, k_c)$:\n- $(t,\\sigma,k_c) = (0.02, 10^{-4}, 0)$,\n- $(t,\\sigma,k_c) = (0.02, 10^{-4}, 5)$,\n- $(t,\\sigma,k_c) = (0.02, 10^{-4}, 20)$,\n- $(t,\\sigma,k_c) = (0.02, 10^{-2}, 20)$,\n- $(t,\\sigma,k_c) = (0.10, 10^{-4}, 10)$.\n\nAll angles, when any occur, must be in radians. No physical units are involved. The final outputs must be real numbers.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4,r_5]$), where $r_i$ is the expected squared error for the $i$-th test case in the given order. The output elements must be floating-point numbers.", "solution": "The objective is to derive an expression for the expected squared reconstruction error, $\\mathbb{E}\\left[\\| \\hat{u}_0 - u_0 \\|_{L^2_w}^2\\right]$.\n\nFirst, we express the reconstruction error, $e(x) = \\hat{u}_0(x) - u_0(x)$, in terms of the Chebyshev basis $\\{T_k(x)\\}$. Given $u_0(x) = \\sum_{k=0}^{\\infty} a_k T_k(x)$ and $\\hat{u}_0(x) = \\sum_{k=0}^{\\infty} \\hat{a}_k T_k(x)$, the difference is:\n$$ e(x) = \\hat{u}_0(x) - u_0(x) = \\sum_{k=0}^{\\infty} (\\hat{a}_k - a_k) T_k(x) $$\nNext, we compute the squared norm of the error with respect to the Chebyshev weight $w(x)=(1-x^2)^{-1/2}$. Using the orthogonality of the Chebyshev polynomials, this simplifies to:\n$$ \\|e\\|_{L^2_w}^2 = \\sum_{k=0}^{\\infty} (\\hat{a}_k - a_k)^2 \\|T_k\\|_{L^2_w}^2 $$\nLet $\\gamma_k = \\|T_k\\|_{L^2_w}^2$, where $\\gamma_0 = \\pi$ and $\\gamma_k = \\pi/2$ for $k \\ge 1$. Taking the expectation, we get:\n$$ \\mathbb{E}\\left[\\|e\\|_{L^2_w}^2\\right] = \\sum_{k=0}^{\\infty} \\mathbb{E}\\left[(\\hat{a}_k - a_k)^2\\right] \\gamma_k $$\nThe term $\\mathbb{E}\\left[(\\hat{a}_k - a_k)^2\\right]$ is the Mean Squared Error (MSE) of the estimator $\\hat{a}_k$. We analyze this for two cases based on the definition of $\\hat{a}_k$.\n\n**Case 1: $k > k_c$ (Truncated modes)**\nHere, $\\hat{a}_k = 0$, so the error is $\\hat{a}_k - a_k = -a_k$. This is a deterministic bias. The MSE is $\\mathbb{E}[(-a_k)^2] = a_k^2$.\n\n**Case 2: $0 \\le k \\le k_c$ (Estimated modes)**\nThe estimator is $\\hat{a}_k = \\tilde{b}_k(t) e^{k^2 t}$. Substituting the model for the noisy data, $\\tilde{b}_k(t) = a_k e^{-k^2 t} + \\varepsilon_k$, we get:\n$$ \\hat{a}_k = (a_k e^{-k^2 t} + \\varepsilon_k)e^{k^2 t} = a_k + \\varepsilon_k e^{k^2 t} $$\nSince $\\mathbb{E}[\\varepsilon_k] = 0$, the estimator is unbiased: $\\mathbb{E}[\\hat{a}_k] = a_k$.\nThe variance is $\\mathbb{V}[\\hat{a}_k] = \\mathbb{V}[a_k + \\varepsilon_k e^{k^2 t}] = \\mathbb{V}[\\varepsilon_k e^{k^2 t}] = (e^{k^2 t})^2 \\mathbb{V}[\\varepsilon_k] = \\sigma^2 e^{2k^2 t}$.\nAs the bias is zero, the MSE for these modes equals the variance: $\\mathbb{E}[(\\hat{a}_k - a_k)^2] = \\sigma^2 e^{2k^2 t}$.\n\n**Total Expected Error**\nWe assemble the total error by splitting the sum at $k_c$:\n$$ \\mathbb{E}\\left[\\|e\\|_{L^2_w}^2\\right] = \\underbrace{\\sum_{k=0}^{k_c} \\left(\\sigma^2 e^{2k^2 t}\\right) \\gamma_k}_{\\text{Variance/Noise Error}} + \\underbrace{\\sum_{k=k_c+1}^{\\infty} a_k^2 \\gamma_k}_{\\text{Bias/Truncation Error}} $$\nThis is the classic bias-variance decomposition. The first term is the error from amplified measurement noise, and the second is the error from truncating the spectral series.\n\nFor the numerical implementation, we use the given coefficients for $a_k$ and truncate the infinite sum at $K_{\\max}=500$. The formula to be implemented is:\n$$ \\mathbb{E}\\left[\\|e\\|_{L^2_w}^2\\right] \\approx \\left(\\pi \\sigma^2 + \\frac{\\pi}{2} \\sigma^2 \\sum_{k=1}^{k_c} e^{2k^2 t}\\right) + \\left(\\frac{\\pi}{2} \\sum_{k=k_c+1}^{K_{\\max}} a_k^2\\right) $$", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of calculating the expected squared reconstruction error\n    for a regularized inverse problem involving a Chebyshev spectral representation.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each case is a tuple (t, sigma, k_c).\n    test_cases = [\n        (0.02, 1e-4, 0),\n        (0.02, 1e-4, 5),\n        (0.02, 1e-4, 20),\n        (0.02, 1e-2, 20),\n        (0.10, 1e-4, 10),\n    ]\n\n    # Global parameter from the problem statement\n    K_max = 500\n\n    # Pre-compute the ground truth coefficients a_k up to K_max\n    # a_k = 0 for k > K_max is implicitly handled by the summation limits.\n    a = np.zeros(K_max + 1)\n    a[0] = 1.0\n    k_vals_for_a = np.arange(1, K_max + 1)\n    a[1:] = ((-1.0)**k_vals_for_a) / ((k_vals_for_a + 1.0)**2)\n\n    results = []\n    for t, sigma, kc in test_cases:\n        # 1. Calculate the Noise Error contribution\n        # Noise Error = sum_{k=0}^{k_c} (sigma^2 * exp(2*k^2*t)) * gamma_k\n        # where gamma_0 = pi and gamma_k = pi/2 for k >= 1.\n        \n        # We can write this as:\n        # pi * sigma^2 + (pi/2) * sigma^2 * sum_{k=1}^{k_c} exp(2*k^2*t)\n        \n        noise_error = 0.0\n        # The term for k=0 is always present as long as kc >= 0\n        noise_error_k0 = np.pi * sigma**2\n        \n        noise_error_k_gt_0 = 0.0\n        if kc > 0:\n            k_noise = np.arange(1, kc + 1)\n            sum_exp = np.sum(np.exp(2 * k_noise**2 * t))\n            noise_error_k_gt_0 = (np.pi / 2.0) * sigma**2 * sum_exp\n            \n        noise_error = noise_error_k0 + noise_error_k_gt_0\n\n        # 2. Calculate the Bias Error contribution\n        # Bias Error = sum_{k=k_c+1}^{K_max} a_k^2 * gamma_k\n        # Since the sum starts from k >= 1, gamma_k is always pi/2.\n        \n        bias_error = 0.0\n        if kc  K_max:\n            # Slicing goes up to K_max, so the upper index is K_max + 1\n            a_sq_sum = np.sum(a[kc + 1 : K_max + 1]**2)\n            bias_error = (np.pi / 2.0) * a_sq_sum\n\n        # 3. Total expected squared error\n        total_error = noise_error + bias_error\n        results.append(total_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10e}' for r in results)}]\")\n\nsolve()\n```", "id": "3370042"}]}