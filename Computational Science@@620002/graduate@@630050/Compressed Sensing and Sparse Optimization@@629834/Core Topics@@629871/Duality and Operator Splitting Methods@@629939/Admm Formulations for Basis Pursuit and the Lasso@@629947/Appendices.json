{"hands_on_practices": [{"introduction": "Many applications, such as in image processing or the estimation of physical quantities, require solutions to be non-negative. This exercise demonstrates how ADMM can elegantly enforce such hard constraints by reformulating them using an indicator function. You will derive how this specific splitting turns one of the ADMM subproblems into a simple projection onto the non-negative orthant, a common and powerful pattern in modern optimization [@problem_id:3429989].", "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) with a nonnegativity constraint in a compressed sensing setting: minimize the sum of a quadratic data fidelity term and a sparsity-inducing regularizer subject to a componentwise nonnegativity constraint. Let $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, and $\\lambda > 0$. Formulate the problem as minimizing the sum of two functions under a consensus constraint:\n$$\\min_{x \\in \\mathbb{R}^{n},\\, z \\in \\mathbb{R}^{n}} f(x) + g(z) \\quad \\text{subject to } x = z,$$\nwhere $f(x)$ is the indicator function of the nonnegative orthant and $g(z)$ is the sum of the quadratic data fidelity and the $\\ell_{1}$-norm regularizer. Use the Alternating Direction Method of Multipliers (ADMM) splitting with a scaled dual variable to derive the update for $x$ at iteration $k+1$ solely in terms of the current values $z^{k}$ and $u^{k}$, given a penalty parameter $\\rho > 0$. Your derivation must start from the augmented Lagrangian associated with the consensus formulation and fundamental properties of indicator functions and projection operators, without assuming any pre-derived proximal formulas.\n\nAfter deriving the analytic expression for the $x$-update, evaluate it for the specific inputs\n$$z^{k} = \\begin{pmatrix} 1.2 \\\\ -0.7 \\\\ 0.0 \\\\ 3.1 \\\\ -2.4 \\end{pmatrix}, \\quad u^{k} = \\begin{pmatrix} -0.5 \\\\ 0.9 \\\\ -0.3 \\\\ 1.2 \\\\ -1.1 \\end{pmatrix}, \\quad \\rho = 1.$$\nProvide the final numerical value of $x^{k+1}$ as a single row matrix. No rounding is required, and no physical units apply.", "solution": "The problem requires the derivation of the $x$-update step in an Alternating Direction Method of Multipliers (ADMM) algorithm for a specific formulation of the non-negative LASSO problem, followed by a numerical evaluation.\n\nFirst, we validate the problem statement.\nThe problem is to minimize a function subject to constraints, formulated for an ADMM approach.\nLet the original problem be the non-negative LASSO:\n$$ \\min_{x \\in \\mathbb{R}^{n}} \\frac{1}{2}\\|Ax-y\\|_2^2 + \\lambda \\|x\\|_1 \\quad \\text{subject to } x \\ge 0. $$\nThe given consensus formulation is:\n$$ \\min_{x \\in \\mathbb{R}^{n},\\, z \\in \\mathbb{R}^{n}} f(x) + g(z) \\quad \\text{subject to } x = z, $$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, and $\\lambda > 0$ are given. The functions are defined as:\n$f(x) = I_{\\mathbb{R}_+^n}(x)$, which is the indicator function of the non-negative orthant. An indicator function $I_C(x)$ of a set $C$ is defined as $I_C(x) = 0$ if $x \\in C$ and $I_C(x) = \\infty$ if $x \\notin C$. Thus, $f(x)$ enforces the non-negativity constraint $x \\ge 0$.\n$g(z)$ is the sum of the quadratic data fidelity term and the $\\ell_1$-norm regularizer, which corresponds to the original LASSO objective function without the non-negativity constraint:\n$$ g(z) = \\frac{1}{2}\\|Az-y\\|_2^2 + \\lambda \\|z\\|_1. $$\nThe problem is scientifically grounded, well-posed, and objective. It is a standard application of ADMM in computational mathematics and signal processing. All necessary information for the derivation and calculation is provided. The problem is valid.\n\nWe now proceed with the derivation. The ADMM algorithm is based on the augmented Lagrangian. For the constrained problem $\\min f(x) + g(z)$ subject to $x-z=0$, the augmented Lagrangian with a scaled dual variable $u$ is given by:\n$$ L_\\rho(x, z, u) = f(x) + g(z) + \\frac{\\rho}{2}\\|x-z+u\\|_2^2 - \\frac{\\rho}{2}\\|u\\|_2^2. $$\nThis form is derived from the standard augmented Lagrangian $L_\\rho(x, z, \\nu) = f(x) + g(z) + \\nu^T(x-z) + \\frac{\\rho}{2}\\|x-z\\|_2^2$ by setting the dual variable $\\nu = \\rho u$ and completing the square. The term $-\\frac{\\rho}{2}\\|u\\|_2^2$ is a constant with respect to $x$ and $z$ and can be ignored during the minimization steps.\n\nThe ADMM algorithm consists of three update steps at each iteration $k$:\n$1$. $x^{k+1} = \\arg\\min_x L_\\rho(x, z^k, u^k)$\n$2$. $z^{k+1} = \\arg\\min_z L_\\rho(x^{k+1}, z, u^k)$\n$3$. $u^{k+1} = u^k + x^{k+1} - z^{k+1}$\n\nWe are tasked with deriving the update for $x$ at iteration $k+1$. This is the first step of the ADMM sequence:\n$$ x^{k+1} = \\arg\\min_{x} L_\\rho(x, z^k, u^k). $$\nSubstituting the expression for the augmented Lagrangian:\n$$ x^{k+1} = \\arg\\min_{x} \\left( f(x) + g(z^k) + \\frac{\\rho}{2}\\|x-z^k+u^k\\|_2^2 \\right). $$\nIn this minimization problem, the variable is $x$. The term $g(z^k)$ is constant with respect to $x$ and can be dropped from the objective function without changing the minimizer:\n$$ x^{k+1} = \\arg\\min_{x} \\left( f(x) + \\frac{\\rho}{2}\\|x-z^k+u^k\\|_2^2 \\right). $$\nLet's define an auxiliary vector $v^k = z^k - u^k$. The expression simplifies to:\n$$ x^{k+1} = \\arg\\min_{x} \\left( f(x) + \\frac{\\rho}{2}\\|x-v^k\\|_2^2 \\right). $$\nThe function $f(x)$ is the indicator function for the non-negative orthant, $I_{\\mathbb{R}_+^n}(x)$. So, the minimization problem is equivalent to:\n$$ x^{k+1} = \\arg\\min_{x \\ge 0} \\frac{\\rho}{2}\\|x-v^k\\|_2^2. $$\nThe positive constant factor $\\frac{\\rho}{2}$ does not affect the location of the minimum, so we can further simplify the problem to:\n$$ x^{k+1} = \\arg\\min_{x \\ge 0} \\|x-v^k\\|_2^2. $$\nThis is the problem of finding the Euclidean projection of the vector $v^k$ onto the convex set $\\mathbb{R}_+^n$. The squared Euclidean norm is separable over the components of the vectors:\n$$ \\|x-v^k\\|_2^2 = \\sum_{i=1}^{n} (x_i - v_i^k)^2. $$\nTherefore, the minimization can be performed independently for each component $x_i$:\n$$ x_i^{k+1} = \\arg\\min_{x_i \\ge 0} (x_i - v_i^k)^2 \\quad \\text{for } i=1, \\dots, n. $$\nFor each component $i$, we consider the univariate quadratic function $h(x_i) = (x_i - v_i^k)^2$. The unconstrained minimizer of $h(x_i)$ is at $x_i = v_i^k$.\nWe must satisfy the constraint $x_i \\ge 0$.\nCase 1: If $v_i^k \\ge 0$, the unconstrained minimizer $x_i=v_i^k$ is in the feasible region $[0, \\infty)$. Thus, the solution is $x_i^{k+1} = v_i^k$.\nCase 2: If $v_i^k < 0$, the unconstrained minimizer is outside the feasible region. The function $h(x_i)$ is a parabola opening upwards with its vertex at $v_i^k < 0$. On the interval $[0, \\infty)$, the function $h(x_i)$ is strictly increasing. Therefore, the minimum value on this interval is achieved at the boundary point $x_i = 0$. The solution is $x_i^{k+1} = 0$.\nCombining both cases, the solution for each component is given by:\n$$ x_i^{k+1} = \\max(0, v_i^k). $$\nIn vector form, this is written as $x^{k+1} = (v^k)_+$, where the $(\\cdot)_+$ operator denotes the element-wise maximum with $0$. Substituting back $v^k = z^k - u^k$, we get the final analytical expression for the $x$-update:\n$$ x^{k+1} = (z^k - u^k)_+. $$\n\nNow, we evaluate this expression for the given inputs:\n$$ z^{k} = \\begin{pmatrix} 1.2 \\\\ -0.7 \\\\ 0.0 \\\\ 3.1 \\\\ -2.4 \\end{pmatrix}, \\quad u^{k} = \\begin{pmatrix} -0.5 \\\\ 0.9 \\\\ -0.3 \\\\ 1.2 \\\\ -1.1 \\end{pmatrix}. $$\nThe penalty parameter $\\rho=1$ is not needed for this specific update step, as it factored out of the minimization.\nFirst, we compute the vector $v^k = z^k - u^k$:\n$$ v^k = z^k - u^k = \\begin{pmatrix} 1.2 \\\\ -0.7 \\\\ 0.0 \\\\ 3.1 \\\\ -2.4 \\end{pmatrix} - \\begin{pmatrix} -0.5 \\\\ 0.9 \\\\ -0.3 \\\\ 1.2 \\\\ -1.1 \\end{pmatrix} = \\begin{pmatrix} 1.2 - (-0.5) \\\\ -0.7 - 0.9 \\\\ 0.0 - (-0.3) \\\\ 3.1 - 1.2 \\\\ -2.4 - (-1.1) \\end{pmatrix} = \\begin{pmatrix} 1.7 \\\\ -1.6 \\\\ 0.3 \\\\ 1.9 \\\\ -1.3 \\end{pmatrix}. $$\nNext, we apply the element-wise projection onto the non-negative orthant, $x^{k+1} = (v^k)_+$:\n$$ x^{k+1} = \\left( \\begin{pmatrix} 1.7 \\\\ -1.6 \\\\ 0.3 \\\\ 1.9 \\\\ -1.3 \\end{pmatrix} \\right)_+ = \\begin{pmatrix} \\max(0, 1.7) \\\\ \\max(0, -1.6) \\\\ \\max(0, 0.3) \\\\ \\max(0, 1.9) \\\\ \\max(0, -1.3) \\end{pmatrix} = \\begin{pmatrix} 1.7 \\\\ 0.0 \\\\ 0.3 \\\\ 1.9 \\\\ 0.0 \\end{pmatrix}. $$\nThe problem asks for the answer as a single row matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1.7 & 0.0 & 0.3 & 1.9 & 0.0\n\\end{pmatrix}\n}\n$$", "id": "3429989"}, {"introduction": "While the standard LASSO promotes sparsity, many problems benefit from encouraging more intricate structural patterns in the solution. This practice explores how to extend the ADMM framework to handle structured regularizers like the group LASSO, which selects entire blocks of variables, and the fused LASSO, which promotes piecewise-constant solutions [@problem_id:3429996]. This exercise highlights the modular power of ADMM, where complex penalties are managed by deriving and applying their corresponding proximal operators.", "problem": "Consider the structured sparse regression problem combining group sparsity and one-dimensional fused sparsity. Let $x \\in \\mathbb{R}^{n}$, $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, $\\lambda_{g} \\ge 0$, $\\lambda_{f} \\ge 0$, and let $\\{G_{1},\\dots,G_{K}\\}$ be a partition of $\\{1,\\dots,n\\}$ defining nonoverlapping groups. Define the group regularizer $R_{g}(x) \\triangleq \\sum_{k=1}^{K} \\|x_{G_{k}}\\|_{2}$ and the fused regularizer $R_{f}(x) \\triangleq \\|D x\\|_{1}$, where $D \\in \\mathbb{R}^{(n-1)\\times n}$ is the first-order difference operator with rows $[0,\\dots,0,-1,1,0,\\dots,0]$ at consecutive positions. Consider the convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|A x - y\\|_{2}^{2} \\ + \\ \\lambda_{g} \\, R_{g}(x) \\ + \\ \\lambda_{f} \\, R_{f}(x).\n$$\nTasks:\n1) Starting from the definition of the proximal operator of a proper, closed, convex function $\\phi$, namely $\\mathrm{prox}_{\\tau \\phi}(v) \\triangleq \\arg\\min_{z} \\left\\{\\frac{1}{2}\\|z - v\\|_{2}^{2} + \\tau \\, \\phi(z)\\right\\}$, derive the proximal operator of the group regularizer $R_{g}$ acting on a vector $v \\in \\mathbb{R}^{n}$, explicitly in terms of the blocks $v_{G_{k}}$.\n2) Introduce auxiliary variables $z_{g} \\in \\mathbb{R}^{n}$ and $z_{f} \\in \\mathbb{R}^{n-1}$ to split the problem with the linear constraints $x = z_{g}$ and $D x = z_{f}$. Using the Alternating Direction Method of Multipliers (ADMM), write the augmented Lagrangian with scaled dual variables and derive the generic ADMM updates for $x$, $z_{g}$, $z_{f}$, and the scaled dual variables, identifying each update as a proximal operator or a linear system solve arising from first-order optimality conditions. Your derivation must start from the definitions of the proximal operator and the method-of-multipliers framework.\n3) Consider the projection problem onto the fused linear constraint graph of $D$:\n$$\n\\min_{x \\in \\mathbb{R}^{n}, \\, u \\in \\mathbb{R}^{n-1}} \\ \\frac{1}{2}\\|x - \\hat{x}\\|_{2}^{2} + \\frac{1}{2}\\|u - \\hat{u}\\|_{2}^{2} \\quad \\text{subject to} \\quad u = D x.\n$$\nDerive a closed-form expression for the projection $(x^{\\star}, u^{\\star})$ in terms of $(\\hat{x}, \\hat{u})$, $D$, and the identity matrices, by eliminating $u$ and solving the resulting normal equations.\n4) Now specialize to $n = 3$, $m = 3$, $A = I_{3}$, and\n$$\nD \\ = \\ \\begin{pmatrix} -1 & 1 & 0 \\\\ 0 & -1 & 1 \\end{pmatrix}, \\quad y \\ = \\ \\begin{pmatrix} 3 \\\\ 1 \\\\ -1 \\end{pmatrix}.\n$$\nLet the groups be $G_{1} = \\{1,2\\}$ and $G_{2} = \\{3\\}$, so that $R_{g}(x) = \\| (x_{1}, x_{2}) \\|_{2} + |x_{3}|$. Set $\\lambda_{g} = 1$, $\\lambda_{f} = 1$, and ADMM penalty parameters $\\rho_{g} = 1$, $\\rho_{f} = 1$. Initialize $z_{g}^{(0)} = 0$, $z_{f}^{(0)} = 0$, $u_{g}^{(0)} = 0$, $u_{f}^{(0)} = 0$. Using your ADMM updates from part 2), compute the first $x$-iterate $x^{(1)}$. Report your final answer as a single row vector. Do not round; provide exact values.", "solution": "The problem will be validated by first extracting all given information and then assessing its scientific and mathematical integrity.\n\n### Step 1: Extract Givens\n\n- **Optimization Variables and Data:**\n  - $x \\in \\mathbb{R}^{n}$\n  - $A \\in \\mathbb{R}^{m \\times n}$\n  - $y \\in \\mathbb{R}^{m}$\n  - $\\lambda_{g} \\ge 0$\n  - $\\lambda_{f} \\ge 0$\n- **Problem Structure:**\n  - A partition of $\\{1,\\dots,n\\}$ into nonoverlapping groups $\\{G_{1},\\dots,G_{K}\\}$.\n  - Group regularizer: $R_{g}(x) \\triangleq \\sum_{k=1}^{K} \\|x_{G_{k}}\\|_{2}$.\n  - Fused regularizer: $R_{f}(x) \\triangleq \\|D x\\|_{1}$, where $D \\in \\mathbb{R}^{(n-1)\\times n}$ is the first-order difference operator.\n- **Optimization Problem:**\n  $$ \\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|A x - y\\|_{2}^{2} \\ + \\ \\lambda_{g} \\, R_{g}(x) \\ + \\ \\lambda_{f} \\, R_{f}(x) $$\n- **Task 1 Definition:**\n  - Proximal operator: $\\mathrm{prox}_{\\tau \\phi}(v) \\triangleq \\arg\\min_{z} \\left\\{\\frac{1}{2}\\|z - v\\|_{2}^{2} + \\tau \\, \\phi(z)\\right\\}$.\n- **Task 2 Definitions:**\n  - Auxiliary variables: $z_{g} \\in \\mathbb{R}^{n}$, $z_{f} \\in \\mathbb{R}^{n-1}$.\n  - Linear constraints: $x = z_{g}$, $D x = z_{f}$.\n- **Task 3 Problem:**\n  $$ \\min_{x \\in \\mathbb{R}^{n}, \\, u \\in \\mathbb{R}^{n-1}} \\ \\frac{1}{2}\\|x - \\hat{x}\\|_{2}^{2} + \\frac{1}{2}\\|u - \\hat{u}\\|_{2}^{2} \\quad \\text{subject to} \\quad u = D x $$\n- **Task 4 Specific Values:**\n  - $n = 3$, $m = 3$, $A = I_{3}$\n  - $D \\ = \\ \\begin{pmatrix} -1 & 1 & 0 \\\\ 0 & -1 & 1 \\end{pmatrix}$\n  - $y \\ = \\ \\begin{pmatrix} 3 \\\\ 1 \\\\ -1 \\end{pmatrix}$\n  - Groups: $G_{1} = \\{1,2\\}$, $G_{2} = \\{3\\}$.\n  - Regularization parameters: $\\lambda_{g} = 1$, $\\lambda_{f} = 1$.\n  - ADMM penalty parameters: $\\rho_{g} = 1$, $\\rho_{f} = 1$.\n  - Initial values: $z_{g}^{(0)} = 0$, $z_{f}^{(0)} = 0$, $u_{g}^{(0)} = 0$, $u_{f}^{(0)} = 0$.\n\n### Step 2: Validate Using Extracted Givens\n\n1.  **Scientifically Grounded:** The problem is a standard exercise in convex optimization, specifically in the area of structured sparse regularization methods for linear regression. The objective function combines a least-squares data fidelity term with group LASSO and fused LASSO regularizers. The Alternating Direction Method of Multipliers (ADMM) is a well-established and powerful algorithm for solving such problems. All concepts (proximal operators, augmented Lagrangians, etc.) are fundamental to the field of mathematical optimization and signal processing. The problem is firmly grounded in established theory.\n2.  **Well-Posed:** The objective function is the sum of three convex functions: a quadratic, the group $\\ell_{2,1}$-norm, and an $\\ell_1$-norm composed with a linear operator. The sum is therefore convex. The minimization problem is well-posed and has a unique solution if the null space of $A$ and the null spaces of the regularizers are suitably disjoint, or more generally, since it is a strictly convex problem for $A$ having full column rank. The questions are specific, non-ambiguous, and lead to uniquely determinable results based on the provided data.\n3.  **Objective:** The problem is stated using precise mathematical language and notation, free of any subjectivity or bias.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. It is a well-posed, scientifically grounded, and objective problem from the field of computational optimization. I will proceed with the solution.\n\n***\n\n### Part 1: Proximal Operator of the Group Regularizer\n\nThe proximal operator of the function $\\tau R_{g}$ at a point $v \\in \\mathbb{R}^{n}$ is defined as:\n$$ \\mathrm{prox}_{\\tau R_{g}}(v) = \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2}\\|z - v\\|_{2}^{2} + \\tau R_{g}(z) \\right\\} $$\nSubstituting the definition of $R_{g}(z) = \\sum_{k=1}^{K} \\|z_{G_{k}}\\|_{2}$, the objective function becomes:\n$$ \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2}\\sum_{i=1}^{n} (z_i - v_i)^2 + \\tau \\sum_{k=1}^{K} \\|z_{G_{k}}\\|_{2} \\right\\} $$\nSince the groups $\\{G_k\\}$ form a partition of the indices, the squared Euclidean norm term is separable across these groups: $\\|z - v\\|_{2}^{2} = \\sum_{k=1}^{K} \\|z_{G_k} - v_{G_k}\\|_{2}^{2}$. Therefore, the minimization problem separates into $K$ independent subproblems, one for each group:\n$$ \\left[\\mathrm{prox}_{\\tau R_{g}}(v)\\right]_{G_k} = \\arg\\min_{z_{G_k} \\in \\mathbb{R}^{|G_k|}} \\left\\{ \\frac{1}{2}\\|z_{G_k} - v_{G_k}\\|_{2}^{2} + \\tau \\|z_{G_k}\\|_{2} \\right\\} $$\nThis is the definition of the proximal operator for the function $\\phi(u) = \\tau \\|u\\|_{2}$. Let's solve this for a generic vector $w = v_{G_k}$ and variable $u = z_{G_k}$. The problem is to find $u^\\star = \\arg\\min_{u} \\{\\frac{1}{2}\\|u - w\\|_{2}^{2} + \\tau \\|u\\|_{2}\\}$.\nThe first-order optimality condition is $0 \\in \\nabla_u \\left(\\frac{1}{2}\\|u - w\\|_{2}^{2}\\right) + \\partial_u(\\tau \\|u\\|_{2})$, which gives $0 \\in (u - w) + \\tau \\partial\\|u\\|_{2}$.\nThe subgradient of the Euclidean norm is $\\partial\\|u\\|_{2} = \\{u/\\|u\\|_{2}\\}$ if $u \\ne 0$ and $\\partial\\|0\\|_{2} = \\{g \\in \\mathbb{R}^{|G_k|} : \\|g\\|_{2} \\le 1\\}$.\n\nCase 1: $u \\ne 0$. The condition is $w - u = \\tau \\frac{u}{\\|u\\|_{2}}$. This implies $w = u(1 + \\frac{\\tau}{\\|u\\|_{2}})$, showing that $u$ must be a positive scaling of $w$, say $u = \\alpha w$ with $\\alpha > 0$. Taking norms, we get $\\|w\\|_{2} = \\|u\\|_{2} + \\tau$. So, $\\|u\\|_{2} = \\|w\\|_{2} - \\tau$. This is valid only if $\\|w\\|_{2} > \\tau$. In this case, $u = w \\frac{\\|u\\|_{2}}{\\|w\\|_{2}} = w \\frac{\\|w\\|_{2} - \\tau}{\\|w\\|_{2}} = \\left(1 - \\frac{\\tau}{\\|w\\|_{2}}\\right)w$.\n\nCase 2: If we assume $\\|w\\|_{2} \\le \\tau$, let's test the solution $u=0$. The optimality condition becomes $0 \\in -w + \\tau \\partial\\|0\\|_{2}$, which means $w \\in \\tau \\partial\\|0\\|_{2}$, or $\\|w\\|_{2} \\le \\tau$. This is consistent with our assumption. Thus, if $\\|w\\|_{2} \\le \\tau$, the minimizer is $u=0$.\n\nCombining both cases, the solution, known as the block soft-thresholding operator, is:\n$$ u^\\star = \\max\\left(0, 1 - \\frac{\\tau}{\\|w\\|_{2}}\\right)w $$\nApplying this result to each group block $G_k$, we get the proximal operator for $R_g$:\n$$ \\left[\\mathrm{prox}_{\\tau R_{g}}(v)\\right]_{G_k} = \\left(1 - \\frac{\\tau}{\\|v_{G_k}\\|_{2}}\\right)_{+} v_{G_k} = \\max\\left(0, 1 - \\frac{\\tau}{\\|v_{G_k}\\|_{2}}\\right) v_{G_k} $$\n\n### Part 2: ADMM Formulation and Updates\n\nThe original problem is split by introducing auxiliary variables $z_g$ and $z_f$. The problem is rewritten as:\n$$ \\min_{x, z_g, z_f} \\ \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda_{g} R_{g}(z_g) + \\lambda_{f} \\|z_f\\|_{1} \\quad \\text{s.t.} \\quad x-z_g = 0, \\ Dx - z_f = 0 $$\nThe augmented Lagrangian using scaled dual variables $u_g, u_f$ is:\n$$ L_{\\rho}(x, z_g, z_f, u_g, u_f) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda_{g} R_{g}(z_g) + \\lambda_{f} \\|z_f\\|_{1} + \\frac{\\rho_g}{2}\\|x - z_g + u_g\\|_{2}^{2} - \\frac{\\rho_g}{2}\\|u_g\\|_{2}^{2} + \\frac{\\rho_f}{2}\\|Dx - z_f + u_f\\|_{2}^{2} - \\frac{\\rho_f}{2}\\|u_f\\|_{2}^{2} $$\nThe ADMM algorithm consists of iteratively minimizing $L_{\\rho}$ with respect to each primal variable and then updating the dual variables. At iteration $k+1$:\n\n1.  **$x$-update:**\n    $x^{(k+1)} = \\arg\\min_{x} \\left\\{ \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\frac{\\rho_g}{2}\\|x - z_g^{(k)} + u_g^{(k)}\\|_{2}^{2} + \\frac{\\rho_f}{2}\\|Dx - z_f^{(k)} + u_f^{(k)}\\|_{2}^{2} \\right\\}$.\n    This is a quadratic minimization. Setting the gradient with respect to $x$ to zero:\n    $A^T(Ax - y) + \\rho_g(x - z_g^{(k)} + u_g^{(k)}) + D^T\\rho_f(Dx - z_f^{(k)} + u_f^{(k)}) = 0$.\n    Rearranging terms gives a linear system for $x$:\n    $(A^T A + \\rho_g I + \\rho_f D^T D) x = A^T y + \\rho_g(z_g^{(k)} - u_g^{(k)}) + \\rho_f D^T(z_f^{(k)} - u_f^{(k)})$.\n    This is a **linear system solve**.\n\n2.  **$z_g$-update:**\n    $z_g^{(k+1)} = \\arg\\min_{z_g} \\left\\{ \\lambda_{g} R_{g}(z_g) + \\frac{\\rho_g}{2}\\|x^{(k+1)} - z_g + u_g^{(k)}\\|_{2}^{2} \\right\\}$.\n    Rearranging the quadratic term:\n    $z_g^{(k+1)} = \\arg\\min_{z_g} \\left\\{ \\frac{\\rho_g}{2}\\|z_g - (x^{(k+1)} + u_g^{(k)})\\|_{2}^{2} + \\lambda_{g} R_{g}(z_g) \\right\\}$.\n    This is the definition of a **proximal operator**:\n    $z_g^{(k+1)} = \\mathrm{prox}_{\\lambda_g/\\rho_g R_g}(x^{(k+1)} + u_g^{(k)})$.\n\n3.  **$z_f$-update:**\n    $z_f^{(k+1)} = \\arg\\min_{z_f} \\left\\{ \\lambda_{f} \\|z_f\\|_{1} + \\frac{\\rho_f}{2}\\|Dx^{(k+1)} - z_f + u_f^{(k)}\\|_{2}^{2} \\right\\}$.\n    Rearranging the quadratic term:\n    $z_f^{(k+1)} = \\arg\\min_{z_f} \\left\\{ \\frac{\\rho_f}{2}\\|z_f - (Dx^{(k+1)} + u_f^{(k)})\\|_{2}^{2} + \\lambda_{f} \\|z_f\\|_{1} \\right\\}$.\n    This is the definition of the **proximal operator** of the $\\ell_1$-norm (element-wise soft-thresholding):\n    $z_f^{(k+1)} = \\mathrm{prox}_{\\lambda_f/\\rho_f \\|\\cdot\\|_1}(Dx^{(k+1)} + u_f^{(k)}) = S_{\\lambda_f/\\rho_f}(Dx^{(k+1)} + u_f^{(k)})$.\n\n4.  **Dual variable updates:**\n    $u_g^{(k+1)} = u_g^{(k)} + x^{(k+1)} - z_g^{(k+1)}$.\n    $u_f^{(k+1)} = u_f^{(k)} + Dx^{(k+1)} - z_f^{(k+1)}$.\n\n### Part 3: Projection onto the Fused Constraint Graph\n\nThe problem is to find $(x^\\star, u^\\star)$ that solve:\n$$ \\min_{x \\in \\mathbb{R}^{n}, \\, u \\in \\mathbb{R}^{n-1}} \\ \\frac{1}{2}\\|x - \\hat{x}\\|_{2}^{2} + \\frac{1}{2}\\|u - \\hat{u}\\|_{2}^{2} \\quad \\text{subject to} \\quad u = D x $$\nWe can eliminate $u$ by substituting the constraint $u = Dx$ into the objective function. This yields an unconstrained minimization problem in $x$ only:\n$$ \\min_{x \\in \\mathbb{R}^{n}} \\ J(x) = \\frac{1}{2}\\|x - \\hat{x}\\|_{2}^{2} + \\frac{1}{2}\\|Dx - \\hat{u}\\|_{2}^{2} $$\nThe function $J(x)$ is quadratic and convex. We find the minimizer $x^\\star$ by setting the gradient $\\nabla J(x)$ to zero.\n$$ \\nabla J(x) = (x - \\hat{x}) + D^T(Dx - \\hat{u}) = 0 $$\nRearranging the terms to solve for $x$:\n$$ x + D^T D x = \\hat{x} + D^T \\hat{u} $$\n$$ (I + D^T D) x = \\hat{x} + D^T \\hat{u} $$\nThe matrix $(I + D^T D)$ is symmetric and positive definite, thus invertible. The solution for $x^\\star$ is:\n$$ x^{\\star} = (I + D^T D)^{-1} (\\hat{x} + D^T \\hat{u}) $$\nThe corresponding optimal $u^\\star$ is found from the constraint:\n$$ u^{\\star} = D x^{\\star} = D(I + D^T D)^{-1} (\\hat{x} + D^T \\hat{u}) $$\n\n### Part 4: First ADMM Iteration Calculation\n\nWe need to compute $x^{(1)}$ using the update rule from Part 2. The initial values are $z_g^{(0)}=0$, $z_f^{(0)}=0$, $u_g^{(0)}=0$, $u_f^{(0)}=0$.\nThe $x$-update is:\n$$ (A^T A + \\rho_g I + \\rho_f D^T D) x^{(1)} = A^T y + \\rho_g(z_g^{(0)} - u_g^{(0)}) + \\rho_f D^T(z_f^{(0)} - u_f^{(0)}) $$\nSubstituting the initial values, the right-hand side (RHS) simplifies significantly:\n$$ \\text{RHS} = A^T y + \\rho_g(0 - 0) + \\rho_f D^T(0 - 0) = A^T y $$\nGiven $A=I_3$, the RHS is $I_3 y = y = \\begin{pmatrix} 3 \\\\ 1 \\\\ -1 \\end{pmatrix}$.\nThe matrix on the left-hand side (LHS) is $M = A^T A + \\rho_g I + \\rho_f D^T D$.\nWith $A = I_3$, $\\rho_g = 1$, and $\\rho_f = 1$, this becomes $M = I_3^T I_3 + 1 \\cdot I_3 + 1 \\cdot D^T D = 2I_3 + D^T D$.\nFirst, compute $D^T D$:\n$$ D = \\begin{pmatrix} -1 & 1 & 0 \\\\ 0 & -1 & 1 \\end{pmatrix}, \\quad D^T = \\begin{pmatrix} -1 & 0 \\\\ 1 & -1 \\\\ 0 & 1 \\end{pmatrix} $$\n$$ D^T D = \\begin{pmatrix} -1 & 0 \\\\ 1 & -1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} -1 & 1 & 0 \\\\ 0 & -1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 & 0 \\\\ -1 & 1+1 & -1 \\\\ 0 & -1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 1 \\end{pmatrix} $$\nNow compute the full matrix $M$:\n$$ M = 2I_3 + D^T D = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix} + \\begin{pmatrix} 1 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 1 \\end{pmatrix} = \\begin{pmatrix} 3 & -1 & 0 \\\\ -1 & 4 & -1 \\\\ 0 & -1 & 3 \\end{pmatrix} $$\nWe must solve the linear system $M x^{(1)} = y$:\n$$ \\begin{pmatrix} 3 & -1 & 0 \\\\ -1 & 4 & -1 \\\\ 0 & -1 & 3 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 1 \\\\ -1 \\end{pmatrix} $$\nThis corresponds to the system of equations:\n1. $3x_1 - x_2 = 3$\n2. $-x_1 + 4x_2 - x_3 = 1$\n3. $-x_2 + 3x_3 = -1$\n\nFrom (1), we have $x_2 = 3x_1 - 3$.\nFrom (3), we have $3x_3 = x_2 - 1$. Substituting for $x_2$: $3x_3 = (3x_1 - 3) - 1 = 3x_1 - 4$, so $x_3 = x_1 - \\frac{4}{3}$.\nSubstitute $x_2$ and $x_3$ into (2):\n$$ -x_1 + 4(3x_1 - 3) - (x_1 - \\frac{4}{3}) = 1 $$\n$$ -x_1 + 12x_1 - 12 - x_1 + \\frac{4}{3} = 1 $$\n$$ 10x_1 = 1 + 12 - \\frac{4}{3} = 13 - \\frac{4}{3} = \\frac{39-4}{3} = \\frac{35}{3} $$\n$$ x_1 = \\frac{35}{30} = \\frac{7}{6} $$\nNow back-substitute to find $x_2$ and $x_3$:\n$$ x_2 = 3\\left(\\frac{7}{6}\\right) - 3 = \\frac{7}{2} - \\frac{6}{2} = \\frac{1}{2} $$\n$$ x_3 = \\frac{7}{6} - \\frac{4}{3} = \\frac{7}{6} - \\frac{8}{6} = -\\frac{1}{6} $$\nThus, the first $x$-iterate is $x^{(1)} = \\begin{pmatrix} 7/6 \\\\ 1/2 \\\\ -1/6 \\end{pmatrix}$.", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{7}{6} & \\frac{1}{2} & -\\frac{1}{6} \\end{pmatrix} } $$", "id": "3429996"}, {"introduction": "The convergence speed and stability of ADMM are highly sensitive to the choice of the penalty parameter $\\rho$. This advanced implementation practice moves beyond fixed parameters and explores an adaptive strategy based on residual balancing, a widely used heuristic for tuning $\\rho$ automatically [@problem_id:3430002]. By contrasting a naive implementation with a theoretically sound one, you will uncover a crucial detail related to scaling the dual variable, which is essential for building robust and high-performance ADMM solvers.", "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) problem in the form of minimizing the sum of a quadratic data fit and an entrywise one-norm penalty:\n$$\n\\min_{x \\in \\mathbb{R}^n}\\ \\frac{1}{2}\\,\\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_1,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, and $\\lambda \\in \\mathbb{R}_{+}$.\n\nUse the Alternating Direction Method of Multipliers (ADMM) with a variable splitting of the form $x=z$ and the scaled dual variable $u$, and consider adaptive updates of the augmented Lagrangian penalty parameter $\\rho \\in \\mathbb{R}_{+}$ based on residual balancing. The primal residual is defined as\n$$\nr^k = x^k - z^k,\n$$\nand the dual residual is defined as\n$$\ns^k = \\rho^k (z^k - z^{k-1}).\n$$\nThe absolute and relative tolerances are $\\varepsilon_{\\text{abs}} > 0$ and $\\varepsilon_{\\text{rel}} > 0$, respectively. The primal feasibility tolerance should be\n$$\n\\varepsilon_{\\text{pri}}^k = \\sqrt{n}\\,\\varepsilon_{\\text{abs}} + \\varepsilon_{\\text{rel}}\\,\\max\\{\\lVert x^k\\rVert_2,\\ \\lVert z^k\\rVert_2,\\ \\lVert r^k\\rVert_2\\},\n$$\nand the dual feasibility tolerance should be\n$$\n\\varepsilon_{\\text{dua}}^k = \\sqrt{n}\\,\\varepsilon_{\\text{abs}} + \\varepsilon_{\\text{rel}}\\,\\lVert \\rho^k u^k\\rVert_2.\n$$\n\nYou must implement two ADMM solvers for the LASSO in its standard two-block, scaled form, using the proximal operator of the one-norm, which is the soft-thresholding operator at level $\\kappa$ defined entrywise by\n$$\n\\operatorname{soft}_{\\kappa}(w)_i = \\operatorname{sign}(w_i)\\,\\max\\{|w_i|-\\kappa,\\,0\\}.\n$$\nBoth solvers use the same residual balancing rule with parameters $\\mu > 1$ and $\\tau > 1$:\n- If $\\lVert r^k\\rVert_2 > \\mu\\,\\lVert s^k\\rVert_2$, then set $\\rho^{k+1} \\gets \\tau\\,\\rho^k$.\n- Else if $\\lVert s^k\\rVert_2 > \\mu\\,\\lVert r^k\\rVert_2$, then set $\\rho^{k+1} \\gets \\rho^k/\\tau$.\n- Otherwise, set $\\rho^{k+1} \\gets \\rho^k$.\n\nThe two solvers differ only in how they handle the scaled dual variable $u^k$ when $\\rho$ changes:\n- Naive residual balancing (Naive-RB): do not rescale $u^k$ when $\\rho$ is changed.\n- Scaled residual balancing (Scaled-RB): when $\\rho$ changes from $\\rho^k$ to $\\rho^{k+1}$, rescale the scaled dual variable as $u^{k} \\gets (\\rho^k/\\rho^{k+1})\\,u^{k}$ before continuing, thereby keeping the unscaled dual variable $y^k = \\rho^k u^k$ invariant.\n\nImplement both solvers with identical initialization $x^0=z^0=u^0=0$, stopping when both $\\lVert r^k\\rVert_2 \\le \\varepsilon_{\\text{pri}}^k$ and $\\lVert s^k\\rVert_2 \\le \\varepsilon_{\\text{dua}}^k$, or when a maximum iteration cap is reached.\n\nYour program must run the following test suite. Each test specifies $(A,b,\\lambda)$, the initial $\\rho$, the residual balancing parameters $(\\mu,\\tau)$, the maximum number of iterations, and the tolerances $(\\varepsilon_{\\text{abs}},\\varepsilon_{\\text{rel}})$. All randomness must be generated from the stated seeds.\n\nTest Suite:\n- Case 1 (well-conditioned, happy path):\n  - Dimensions: $m=60$, $n=40$.\n  - Seed: $0$.\n  - Construction: draw $A$ with independent standard normal entries and then scale each column to unit $\\ell_2$ norm; draw a $k$-sparse ground truth $x^\\star$ with $k=6$ nonzeros at random positions with independent standard normal values; set $b = A x^\\star + \\eta$, with noise $\\eta$ having independent normal entries with standard deviation $10^{-3}$.\n  - Regularization: $\\lambda = 10^{-1}$.\n  - ADMM parameters: $\\rho^0=1$, $\\mu=5$, $\\tau=2$, maximum iterations $1000$, $\\varepsilon_{\\text{abs}}=10^{-4}$, $\\varepsilon_{\\text{rel}}=10^{-3}$.\n\n- Case 2 (boundary, identity design and heavy regularization):\n  - Dimensions: $m=n=30$.\n  - Seed: $1$.\n  - Construction: set $A=I_n$, draw $b$ with independent standard normal entries.\n  - Regularization: $\\lambda = 5\\times 10^{-1}$.\n  - ADMM parameters: $\\rho^0=1$, $\\mu=5$, $\\tau=2$, maximum iterations $1000$, $\\varepsilon_{\\text{abs}}=10^{-6}$, $\\varepsilon_{\\text{rel}}=10^{-4}$.\n\n- Case 3 (ill-conditioned, highly coherent columns; destabilization challenge):\n  - Dimensions: $m=n=50$.\n  - Seed: $2$.\n  - Construction: draw a vector $v \\in \\mathbb{R}^m$ with independent standard normal entries and normalize it to unit $\\ell_2$ norm. For each column index $j \\in \\{1,\\dots,n\\}$, draw an independent random vector $r_j \\in \\mathbb{R}^m$ with standard normal entries. Set $\\delta_j = 10^{-3}$ for all $j$ except exactly three indices chosen uniformly at random without replacement for which $\\delta_j = 10^{-1}$. Define the $j$-th column of $A$ as $A_{:,j} = v + \\delta_j r_j$, then scale each column to unit $\\ell_2$ norm. Draw a two-sparse $x^\\star$ with support chosen uniformly at random with two distinct indices among those with $\\delta_j=10^{-3}$, with nonzero values drawn from the standard normal distribution. Set $b = A x^\\star + \\eta$, with noise $\\eta$ having independent normal entries with standard deviation $10^{-4}$.\n  - Regularization: $\\lambda = 10^{-6}$.\n  - ADMM parameters: $\\rho^0=10^{-6}$, $\\mu=2$, $\\tau=10$, maximum iterations $1000$, $\\varepsilon_{\\text{abs}}=10^{-5}$, $\\varepsilon_{\\text{rel}}=10^{-4}$.\n\nFor each case, run both Naive-RB and Scaled-RB with identical parameters and initialization. Record a boolean indicating whether the method converged according to the stopping criteria within the iteration cap. Your program should therefore output six booleans in the order:\n$$\n[\\text{Case1-Naive},\\ \\text{Case1-Scaled},\\ \\text{Case2-Naive},\\ \\text{Case2-Scaled},\\ \\text{Case3-Naive},\\ \\text{Case3-Scaled}],\n$$\nwhere each entry is $1$ for convergence and $0$ otherwise. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[1,1,1,1,0,1]\").\n\nThe construction of the test suite must be exactly as specified, and the output values must be exactly booleans encoded as integers ($1$ for true, $0$ for false). The aim is to investigate adaptive $\\rho$ update strategies via residual balancing for the LASSO and to construct and verify a case where naive residual balancing destabilizes convergence relative to a properly scaled strategy.", "solution": "The user has provided a well-defined problem in the field of convex optimization, specifically requesting the implementation and comparison of two variants of the Alternating Direction Method of Multipliers (ADMM) for solving the LASSO problem.\n\n### Step 1: Extract Givens\n\nThe problem is to solve the LASSO optimization problem:\n$$\n\\min_{x \\in \\mathbb{R}^n}\\ \\frac{1}{2}\\,\\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_1,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, and $\\lambda > 0$.\n\nThe ADMM algorithm is to be derived from the constrained formulation with variable splitting $x=z$:\n$$\n\\min_{x, z}\\ \\frac{1}{2}\\,\\lVert A x - b \\rVert_2^2 + \\lambda \\lVert z \\rVert_1, \\quad \\text{subject to } x - z = 0.\n$$\n\nThe ADMM updates are given in the scaled dual form, involving primal variables $x, z$ and the scaled dual variable $u$. The specific iterative steps are defined by minimizing the augmented Lagrangian:\n$$\nL_\\rho(x, z, u) = \\frac{1}{2}\\lVert Ax - b \\rVert_2^2 + \\lambda \\lVert z \\rVert_1 + \\frac{\\rho}{2}\\lVert x - z + u \\rVert_2^2 - \\frac{\\rho}{2}\\lVert u \\rVert_2^2.\n$$\n\nThe proximal operator for the $\\ell_1$-norm is the soft-thresholding operator:\n$$\n\\operatorname{soft}_{\\kappa}(w)_i = \\operatorname{sign}(w_i)\\,\\max\\{|w_i|-\\kappa,\\,0\\}.\n$$\n\nThe residuals at iteration $k$ are defined as:\n- Primal residual: $r^k = x^k - z^k$.\n- Dual residual: $s^k = \\rho^k (z^k - z^{k-1})$.\n\nThe stopping criteria are based on absolute and relative tolerances, $\\varepsilon_{\\text{abs}} > 0$ and $\\varepsilon_{\\text{rel}} > 0$:\n- $\\lVert r^k\\rVert_2 \\le \\varepsilon_{\\text{pri}}^k = \\sqrt{n}\\,\\varepsilon_{\\text{abs}} + \\varepsilon_{\\text{rel}}\\,\\max\\{\\lVert x^k\\rVert_2,\\ \\lVert z^k\\rVert_2,\\ \\lVert r^k\\rVert_2\\}$.\n- $\\lVert s^k\\rVert_2 \\le \\varepsilon_{\\text{dua}}^k = \\sqrt{n}\\,\\varepsilon_{\\text{abs}} + \\varepsilon_{\\text{rel}}\\,\\lVert \\rho^k u^k\\rVert_2$.\n\nAn adaptive scheme for the penalty parameter $\\rho$ is specified, with parameters $\\mu > 1$ and $\\tau > 1$:\n- If $\\lVert r^k\\rVert_2 > \\mu\\,\\lVert s^k\\rVert_2$, then $\\rho^{k+1} \\gets \\tau\\,\\rho^k$.\n- Else if $\\lVert s^k\\rVert_2 > \\mu\\,\\lVert r^k\\rVert_2$, then $\\rho^{k+1} \\gets \\rho^k/\\tau$.\n- Otherwise, $\\rho^{k+1} \\gets \\rho^k$.\n\nTwo solver variants are to be implemented, differing in their handling of the scaled dual variable $u^k$ when $\\rho$ is updated:\n1.  **Naive-RB**: $u^k$ is not rescaled when $\\rho$ changes.\n2.  **Scaled-RB**: When $\\rho$ changes from $\\rho^k$ to $\\rho^{k+1}$, $u^k$ is rescaled as $u^{k} \\gets (\\rho^k/\\rho^{k+1})\\,u^{k}$.\n\nThe initialization for all solvers is $x^0=z^0=u^0=0$. A maximum iteration count is specified.\n\nThe problem provides three specific test cases with all necessary parameters, including matrix dimensions, random seeds for data generation, regularization parameter $\\lambda$, and ADMM algorithm parameters $(\\rho^0, \\mu, \\tau, \\text{max\\_iter}, \\varepsilon_{\\text{abs}}, \\varepsilon_{\\text{rel}})$.\n\nThe final output must be a list of six booleans (encoded as $1$ for convergence, $0$ for non-convergence) for the two solvers across the three test cases.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement is analyzed against the validation criteria:\n\n1.  **Scientific or Factual Unsoundness**: The problem is grounded in the established theory of convex optimization and the ADMM algorithm. The LASSO formulation, the ADMM updates, residual balancing, and stopping criteria are all standard or well-documented variants in the literature. It is scientifically and mathematically sound.\n2.  **Non-Formalizable or Irrelevant**: The problem is highly formalizable. It provides precise mathematical definitions and algorithmic steps. It is directly relevant to the topic of ADMM for sparse optimization.\n3.  **Incomplete or Contradictory Setup**: The problem is self-contained. It specifies all necessary data, parameters, and initial conditions for both the problem instances and the solvers. The definitions, though requiring careful interpretation of indices, are not contradictory.\n4.  **Unrealistic or Infeasible**: The problem is computationally feasible. The specified matrix dimensions are small enough for standard hardware. The test cases, including an ill-conditioned one, are designed to test the robustness of the algorithms, which is a standard practice in numerical methods research.\n5.  **Ill-Posed or Poorly Structured**: The LASSO problem is well-posed for the given parameters. The ADMM algorithm is a valid method for solving it. The instructions are precise and unambiguous.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem is non-trivial. It requires a correct implementation of a sophisticated numerical algorithm, including adaptive parameter tuning. The test cases are designed to highlight a specific numerical behavior related to the $\\rho$-update strategy, which is a substantive topic.\n7.  **Outside Scientific Verifiability**: The results are computationally verifiable. Running the specified algorithm with the given parameters and seeds will produce a deterministic outcome.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. It is a well-posed, scientifically sound, and precisely specified task in numerical optimization. A full solution will be provided.\n\n### Solution\n\nThe LASSO problem is solved using ADMM by splitting the variable $x$ into two, $x$ and $z$, with the constraint $x=z$. The objective function becomes $f(x) + g(z)$, where $f(x) = \\frac{1}{2}\\lVert Ax - b \\rVert_2^2$ and $g(z) = \\lambda \\lVert z \\rVert_1$.\n\nThe scaled-form ADMM algorithm consists of the following iterative updates, starting with $k=0$ and initial values $x^0, z^0, u^0, \\rho^0$:\n\n1.  **$x$-update**: We minimize the augmented Lagrangian with respect to $x$:\n    $$\n    x^{k+1} = \\arg\\min_x \\left( \\frac{1}{2}\\lVert Ax - b \\rVert_2^2 + \\frac{\\rho^k}{2}\\lVert x - z^k + u^k \\rVert_2^2 \\right)\n    $$\n    This is a quadratic problem, and the minimizer is found by setting the gradient to zero:\n    $$\n    A^T(Ax - b) + \\rho^k(x - z^k + u^k) = 0\n    $$\n    $$\n    (A^TA + \\rho^k I)x = A^T b + \\rho^k(z^k - u^k)\n    $$\n    This is a linear system for $x^{k+1}$:\n    $$\n    x^{k+1} = (A^TA + \\rho^k I)^{-1} (A^T b + \\rho^k(z^k - u^k))\n    $$\n    The matrix $(A^TA + \\rho^k I)$ is positive definite. We can pre-compute its LU-factorization and use it to solve for $x^{k+1}$. Since $\\rho^k$ may change at each iteration, we must re-factor this matrix whenever $\\rho^k$ is updated. We can cache factorizations for previously seen values of $\\rho$.\n\n2.  **$z$-update**: We minimize the augmented Lagrangian with respect to $z$:\n    $$\n    z^{k+1} = \\arg\\min_z \\left( \\lambda\\lVert z \\rVert_1 + \\frac{\\rho^k}{2}\\lVert x^{k+1} - z + u^k \\rVert_2^2 \\right)\n    $$\n    This is equivalent to evaluating the proximal operator of the $\\ell_1$-norm, which is the soft-thresholding operator:\n    $$\n    z^{k+1} = \\operatorname{prox}_{\\frac{\\lambda}{\\rho^k}\\lVert \\cdot \\rVert_1}(x^{k+1} + u^k) = \\operatorname{soft}_{\\lambda/\\rho^k}(x^{k+1} + u^k)\n    $$\n\n3.  **$u$-update**: The scaled dual variable is updated as:\n    $$\n    u^{k+1} = u^k + x^{k+1} - z^{k+1}\n    $$\n\nAt the end of each iteration $k+1$, we compute the primal and dual residuals:\n$$\nr^{k+1} = x^{k+1} - z^{k+1} \\quad \\text{and} \\quad s^{k+1} = \\rho^k(z^{k+1} - z^k)\n$$\nThe algorithm terminates if $\\lVert r^{k+1} \\rVert_2 \\le \\varepsilon_{\\text{pri}}^{k+1}$ and $\\lVert s^{k+1} \\rVert_2 \\le \\varepsilon_{\\text{dua}}^{k+1}$.\n\nIf the stopping criteria are not met, the penalty parameter $\\rho$ for the next iteration, $\\rho^{k+1}$, is updated based on the norms of the current residuals, $\\lVert r^{k+1} \\rVert_2$ and $\\lVert s^{k+1} \\rVert_2$, using the specified balancing rule.\n\nFinally, for the `Scaled-RB` solver, if $\\rho^{k+1} \\neq \\rho^k$, the newly computed dual variable $u^{k+1}$ is rescaled. The unscaled dual variable is $y = \\rho u$. To keep $y^{k+1}$ invariant through the change in $\\rho$, we must have $\\rho^k u^{k+1}_{\\text{old}} = \\rho^{k+1} u^{k+1}_{\\text{new}}$. The problem statement's rule $u^{k} \\gets (\\rho^k/\\rho^{k+1})\\,u^{k}$ might be a typo in the index of $u$. We will implement the scaling on the just-computed $u^{k+1}$, which aligns with the theoretical motivation:\n$$\nu^{k+1} \\gets \\frac{\\rho^k}{\\rho^{k+1}} u^{k+1}\n$$\nThis ensures that the quantity $y^{k+1} = \\rho^{k+1} u^{k+1}$ is continuous and corresponds to the un-rescaled update. The implementation will follow these steps and generate the test case data exactly as described.", "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for ADMM LASSO solvers.\n    \"\"\"\n\n    def soft_threshold(w, kappa):\n        \"\"\"Entrywise soft-thresholding operator.\"\"\"\n        return np.sign(w) * np.maximum(np.abs(w) - kappa, 0)\n\n    def admm_lasso(A, b, lam, rho0, mu, tau, max_iter, eps_abs, eps_rel, solver_type):\n        \"\"\"\n        Implements ADMM for the LASSO problem with adaptive rho.\n        \n        Args:\n            A (np.ndarray): Design matrix.\n            b (np.ndarray): Measurement vector.\n            lam (float): Regularization parameter lambda.\n            rho0 (float): Initial ADMM penalty parameter.\n            mu (float): Residual balancing parameter.\n            tau (float): Rho update factor.\n            max_iter (int): Maximum number of iterations.\n            eps_abs (float): Absolute tolerance for stopping criteria.\n            eps_rel (float): Relative tolerance for stopping criteria.\n            solver_type (str): 'naive' for Naive-RB, 'scaled' for Scaled-RB.\n\n        Returns:\n            bool: True if converged, False otherwise.\n        \"\"\"\n        m, n = A.shape\n\n        # Initialization\n        x = np.zeros(n)\n        z = np.zeros(n)\n        u = np.zeros(n)\n        rho = rho0\n\n        factorization_cache = {}\n        is_identity_case = (n == m and n > 0 and np.array_equal(A, np.eye(n)))\n        AtA = None if is_identity_case else A.T @ A\n        Atb = A.T @ b\n\n        for k in range(max_iter):\n            z_prev_iter = z\n\n            # x-update step\n            # Cache factorization to avoid re-computation if rho is revisited\n            if rho not in factorization_cache:\n                if is_identity_case:\n                    factorization_cache[rho] = 'identity'\n                else:\n                    factorization_cache[rho] = linalg.lu_factor(AtA + rho * np.eye(n))\n            \n            factor = factorization_cache[rho]\n            \n            if factor == 'identity':\n                x = (1.0 / (1.0 + rho)) * (b + rho * (z - u))\n            else:\n                rhs = Atb + rho * (z - u)\n                x = linalg.lu_solve(factor, rhs)\n\n            # z-update step\n            z = soft_threshold(x + u, lam / rho)\n\n            # u-update step\n            u = u + x - z\n\n            # Calculate residuals and check for convergence\n            r = x - z\n            s_vec = rho * (z - z_prev_iter)\n            \n            r_norm = np.linalg.norm(r)\n            s_norm = np.linalg.norm(s_vec)\n            \n            eps_pri = np.sqrt(n) * eps_abs + eps_rel * max(np.linalg.norm(x), np.linalg.norm(z), r_norm)\n            eps_dua = np.sqrt(n) * eps_abs + eps_rel * np.linalg.norm(rho * u)\n            \n            if r_norm <= eps_pri and s_norm <= eps_dua:\n                return True\n\n            # rho update for the next iteration\n            rho_prev = rho\n            if r_norm > mu * s_norm:\n                rho = tau * rho\n            elif s_norm > mu * r_norm:\n                rho = rho / tau\n\n            # u-scaling for Scaled-RB variant\n            if solver_type == 'scaled' and rho != rho_prev:\n                # Scale the newly computed u to maintain invariant unscaled dual variable y = rho * u\n                u = u * (rho_prev / rho)\n\n        return False\n\n    test_cases = [\n        # Case 1\n        {'id': 1, 'm': 60, 'n': 40, 'k': 6, 'seed': 0, 'noise_std': 1e-3, 'lambda': 1e-1, 'rho0': 1, 'mu': 5, 'tau': 2, 'max_iter': 1000, 'eps_abs': 1e-4, 'eps_rel': 1e-3},\n        # Case 2\n        {'id': 2, 'm': 30, 'n': 30, 'seed': 1, 'lambda': 5e-1, 'rho0': 1, 'mu': 5, 'tau': 2, 'max_iter': 1000, 'eps_abs': 1e-6, 'eps_rel': 1e-4},\n        # Case 3\n        {'id': 3, 'm': 50, 'n': 50, 'seed': 2, 'noise_std': 1e-4, 'lambda': 1e-6, 'rho0': 1e-6, 'mu': 2, 'tau': 10, 'max_iter': 1000, 'eps_abs': 1e-5, 'eps_rel': 1e-4}\n    ]\n\n    results = []\n\n    for params in test_cases:\n        rng = np.random.RandomState(params['seed'])\n        \n        if params['id'] == 1:\n            m, n, k = params['m'], params['n'], params['k']\n            A = rng.randn(m, n)\n            A /= np.linalg.norm(A, axis=0)\n            x_star = np.zeros(n)\n            nonzero_indices = rng.choice(n, k, replace=False)\n            x_star[nonzero_indices] = rng.randn(k)\n            eta = params['noise_std'] * rng.randn(m)\n            b = A @ x_star + eta\n        elif params['id'] == 2:\n            m, n = params['m'], params['n']\n            A = np.eye(n)\n            b = rng.randn(m)\n        elif params['id'] == 3:\n            m, n = params['m'], params['n']\n            v = rng.randn(m)\n            v /= np.linalg.norm(v)\n            \n            delta = np.full(n, 1e-3)\n            high_delta_indices = rng.choice(n, 3, replace=False)\n            delta[high_delta_indices] = 1e-1\n            \n            A = np.zeros((m, n))\n            for j in range(n):\n                r_j = rng.randn(m)\n                A[:, j] = v + delta[j] * r_j\n            A /= np.linalg.norm(A, axis=0)\n\n            low_delta_indices = np.where(delta == 1e-3)[0]\n            nonzero_indices = rng.choice(low_delta_indices, 2, replace=False)\n            x_star = np.zeros(n)\n            x_star[nonzero_indices] = rng.randn(2)\n            eta = params['noise_std'] * rng.randn(m)\n            b = A @ x_star + eta\n        \n        common_args = (A, b, params['lambda'], params['rho0'], params['mu'], params['tau'],\n                       params['max_iter'], params['eps_abs'], params['eps_rel'])\n        \n        # Run Naive-RB solver\n        conv_naive = admm_lasso(*common_args, solver_type='naive')\n        results.append(int(conv_naive))\n        \n        # Run Scaled-RB solver\n        conv_scaled = admm_lasso(*common_args, solver_type='scaled')\n        results.append(int(conv_scaled))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3430002"}]}