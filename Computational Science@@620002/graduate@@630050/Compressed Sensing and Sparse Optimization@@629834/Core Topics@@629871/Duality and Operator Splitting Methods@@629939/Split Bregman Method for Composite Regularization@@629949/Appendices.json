{"hands_on_practices": [{"introduction": "The split Bregman method decomposes a complex optimization task into a sequence of simpler subproblems. This first practice focuses on updating the primary variable, $u$, which typically involves solving a convex quadratic problem. By applying basic multivariate calculus, you will derive the normal equations that govern this update, yielding a closed-form solution that forms the computational core of each iteration.", "problem": "Consider the composite regularization problem in compressed sensing, in which one seeks an estimate $u \\in \\mathbb{R}^{n}$ from data $b \\in \\mathbb{R}^{m}$ by minimizing a sum of a least-squares data fidelity and multiple nonsmooth regularizers applied through linear operators. Specifically, let $A \\in \\mathbb{R}^{m \\times n}$ be a known sensing matrix and let $\\{K_{i}\\}_{i=1}^{r}$ be a collection of known linear operators with $K_{i} \\in \\mathbb{R}^{p_{i} \\times n}$. The data fidelity is $f(u) = \\tfrac{1}{2}\\|A u - b\\|_{2}^{2}$. To handle the nonsmooth terms with multiple transforms, one applies the split Bregman method, introducing split variables $\\{d_{i}\\}_{i=1}^{r}$ and Bregman variables $\\{b_{i}\\}_{i=1}^{r}$. At iteration $k$, given fixed $\\{d_{i}^{k}\\}$ and $\\{b_{i}^{k}\\}$, the $u$-update solves the strictly convex quadratic subproblem\n$$\nu^{k+1} \\in \\arg\\min_{u \\in \\mathbb{R}^{n}} \\left\\{ \\tfrac{1}{2}\\|A u - b\\|_{2}^{2} + \\tfrac{\\mu}{2} \\sum_{i=1}^{r} \\|K_{i} u - d_{i}^{k} + b_{i}^{k}\\|_{2}^{2} \\right\\},\n$$\nwhere $\\mu > 0$ is a fixed penalty parameter. Assume that the matrix\n$$\nA^{\\top} A + \\mu \\sum_{i=1}^{r} K_{i}^{\\top} K_{i}\n$$\nis invertible, which is ensured, for example, if $A$ has full column rank or if $\\sum_{i=1}^{r} K_{i}^{\\top} K_{i}$ is positive definite. Starting from the fundamental facts of multivariate calculus and linear algebra—namely, that a differentiable strictly convex function attains its unique minimizer where its gradient vanishes, and that for any matrices $M$ and vectors $x,y$ with compatible dimensions, $\\nabla_{x} \\left(\\tfrac{1}{2}\\|M x - y\\|_{2}^{2}\\right) = M^{\\top} (M x - y)$—derive the normal equations for the $u$-update and solve them explicitly for $u^{k+1}$.\n\nYour final answer must be a single closed-form analytic expression for $u^{k+1}$ in terms of $A$, $b$, $\\{K_{i}\\}$, $\\{d_{i}^{k}\\}$, $\\{b_{i}^{k}\\}$, and $\\mu$. No numerical approximation or rounding is required.", "solution": "The problem requires the derivation of a closed-form expression for the vector $u^{k+1}$ that minimizes a specific objective function arising in the split Bregman method for composite regularization.\n\nThe objective function to be minimized with respect to $u \\in \\mathbb{R}^{n}$ is given by\n$$\n\\mathcal{L}(u) = \\frac{1}{2}\\|A u - b\\|_{2}^{2} + \\frac{\\mu}{2} \\sum_{i=1}^{r} \\|K_{i} u - d_{i}^{k} + b_{i}^{k}\\|_{2}^{2}\n$$\nwhere $A$, $b$, $\\mu$, $\\{K_i\\}$, $\\{d_i^k\\}$, and $\\{b_i^k\\}$ are given quantities. The problem states that this function is strictly convex and differentiable with respect to $u$. According to the fundamental principles of calculus, the unique minimizer $u^{k+1}$ of a strictly convex differentiable function is found at the point where its gradient with respect to the variable of interest is equal to the zero vector. Therefore, we must solve the equation $\\nabla_{u} \\mathcal{L}(u^{k+1}) = 0$.\n\nWe can compute the gradient of $\\mathcal{L}(u)$ by taking the gradient of each term separately. The objective function is a sum of two components.\nThe first component is the data fidelity term, $\\mathcal{L}_1(u) = \\frac{1}{2}\\|A u - b\\|_{2}^{2}$.\nThe second component is the regularization term, $\\mathcal{L}_2(u) = \\frac{\\mu}{2} \\sum_{i=1}^{r} \\|K_{i} u - d_{i}^{k} + b_{i}^{k}\\|_{2}^{2}$.\n\nThe problem provides the gradient formula for a quadratic form: for matrices $M$ and vectors $x, y$, we have $\\nabla_{x} \\left(\\frac{1}{2}\\|M x - y\\|_{2}^{2}\\right) = M^{\\top} (M x - y)$.\n\nApplying this formula to the first component $\\mathcal{L}_1(u)$ with $M=A$, $x=u$, and $y=b$, we obtain its gradient:\n$$\n\\nabla_{u} \\mathcal{L}_1(u) = A^{\\top}(A u - b)\n$$\n\nFor the second component, $\\mathcal{L}_2(u)$, we use the linearity of the gradient operator. The gradient of the sum is the sum of the gradients. The term inside the norm is $\\|K_i u - (d_i^k - b_i^k)\\|_2^2$. Applying the gradient formula to each term in the summation, with $M = K_{i}$, $x = u$, and $y = d_{i}^{k} - b_{i}^{k}$, we get:\n$$\n\\nabla_{u} \\|K_{i} u - (d_{i}^{k} - b_{i}^{k})\\|_{2}^{2} = 2 K_{i}^{\\top}(K_{i} u - (d_{i}^{k} - b_{i}^{k})) = 2 K_{i}^{\\top}(K_{i} u - d_{i}^{k} + b_{i}^{k})\n$$\nSubstituting this back into the expression for $\\nabla_{u} \\mathcal{L}_2(u)$:\n$$\n\\nabla_{u} \\mathcal{L}_2(u) = \\frac{\\mu}{2} \\sum_{i=1}^{r} 2 K_{i}^{\\top}(K_{i} u - d_{i}^{k} + b_{i}^{k}) = \\mu \\sum_{i=1}^{r} K_{i}^{\\top}(K_{i} u - d_{i}^{k} + b_{i}^{k})\n$$\n\nThe total gradient of $\\mathcal{L}(u)$ is the sum of the gradients of its components:\n$$\n\\nabla_{u} \\mathcal{L}(u) = \\nabla_{u} \\mathcal{L}_1(u) + \\nabla_{u} \\mathcal{L}_2(u) = A^{\\top}(A u - b) + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}(K_{i} u - d_{i}^{k} + b_{i}^{k})\n$$\nSetting the gradient to zero at $u = u^{k+1}$ gives the normal equations for the $u$-update:\n$$\nA^{\\top}(A u^{k+1} - b) + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}(K_{i} u^{k+1} - d_{i}^{k} + b_{i}^{k}) = 0\n$$\nTo solve for $u^{k+1}$, we rearrange this equation by distributing the terms and grouping those containing $u^{k+1}$:\n$$\nA^{\\top}A u^{k+1} - A^{\\top}b + \\mu \\sum_{i=1}^{r} \\left( K_{i}^{\\top}K_{i} u^{k+1} - K_{i}^{\\top}(d_{i}^{k} - b_{i}^{k}) \\right) = 0\n$$\n$$\nA^{\\top}A u^{k+1} + \\mu \\left( \\sum_{i=1}^{r} K_{i}^{\\top}K_{i} \\right) u^{k+1} - A^{\\top}b - \\mu \\sum_{i=1}^{r} K_{i}^{\\top}(d_{i}^{k} - b_{i}^{k}) = 0\n$$\nMove all terms not involving $u^{k+1}$ to the right-hand side:\n$$\nA^{\\top}A u^{k+1} + \\mu \\left( \\sum_{i=1}^{r} K_{i}^{\\top}K_{i} \\right) u^{k+1} = A^{\\top}b + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}(d_{i}^{k} - b_{i}^{k})\n$$\nFactor out $u^{k+1}$ on the left-hand side:\n$$\n\\left( A^{\\top}A + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}K_{i} \\right) u^{k+1} = A^{\\top}b + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}(d_{i}^{k} - b_{i}^{k})\n$$\nThe problem statement guarantees that the matrix $\\left( A^{\\top}A + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}K_{i} \\right)$ is invertible. We can therefore pre-multiply both sides of the equation by its inverse to isolate $u^{k+1}$:\n$$\nu^{k+1} = \\left( A^{\\top}A + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}K_{i} \\right)^{-1} \\left( A^{\\top}b + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}(d_{i}^{k} - b_{i}^{k}) \\right)\n$$\nThis expression is the explicit, closed-form solution for the $u$-update $u^{k+1}$.", "answer": "$$\n\\boxed{\\left( A^{\\top} A + \\mu \\sum_{i=1}^{r} K_{i}^{\\top} K_{i} \\right)^{-1} \\left( A^{\\top} b + \\mu \\sum_{i=1}^{r} K_{i}^{\\top} (d_{i}^{k} - b_{i}^{k}) \\right)}\n$$", "id": "3480428"}, {"introduction": "After updating the primary variable, the next step is to address the non-smooth regularization term by updating an auxiliary variable, $d$. This exercise guides you through the derivation of the proximal operator for the weighted $\\ell_1$ norm, revealing its identity as the elegant and intuitive soft-thresholding operator. Mastering this fundamental \"shrinkage\" step is essential for understanding how the method algorithmically enforces sparsity.", "problem": "Consider the composite-regularized least-squares problem in compressed sensing with a linear measurement operator $A \\in \\mathbb{R}^{m \\times n}$, data $y \\in \\mathbb{R}^{m}$, and a sparsity-promoting penalty on an auxiliary split variable $d \\in \\mathbb{R}^{p}$:\n$$\n\\min_{u \\in \\mathbb{R}^{n},\\, d \\in \\mathbb{R}^{p}} \\; \\frac{1}{2}\\|Au - y\\|_{2}^{2} \\;+\\; \\lambda \\sum_{j=1}^{p} w_{j} |d_{j}| \\quad \\text{subject to} \\quad d = Ku,\n$$\nwhere $K \\in \\mathbb{R}^{p \\times n}$ is a fixed linear operator and $w_{j} > 0$ for all $j$. Using variable splitting and the Split Bregman method (also known to be equivalent to a specific form of the Alternating Direction Method of Multipliers (ADMM)), the $d$-update at iteration $k$ is obtained by minimizing, for fixed $u^{k}$ and a Bregman/dual variable $b^{k}$,\n$$\n\\min_{d \\in \\mathbb{R}^{p}} \\; \\lambda \\sum_{j=1}^{p} w_{j} |d_{j}| \\;+\\; \\frac{\\mu}{2} \\left\\| d - \\left(Ku^{k} + b^{k}\\right) \\right\\|_{2}^{2},\n$$\nwith a fixed penalty parameter $\\mu > 0$. Starting from the definition of the proximal operator of a proper, closed, convex function $g$,\n$$\n\\mathrm{prox}_{\\tau g}(z) \\;=\\; \\arg\\min_{x} \\left\\{ g(x) + \\frac{1}{2\\tau}\\|x - z\\|_{2}^{2} \\right\\},\n$$\nand using only fundamental convex analysis principles (subdifferential optimality condition and separability), derive the closed-form expression of the proximal operator for the weighted $\\ell_{1}$ function $g(d) = \\sum_{j=1}^{p} w_{j}|d_{j}|$, and show that the $d$-update equals $\\mathrm{prox}_{(\\lambda/\\mu) g}\\!\\left(Ku^{k}+b^{k}\\right)$.\n\nThen, apply your derived formula to the following concrete instance with $p = 4$:\n- Weights $w = (w_{1}, w_{2}, w_{3}, w_{4}) = (1, 2, 0.5, 3)$,\n- Parameters $\\lambda = 1$ and $\\mu = 2$,\n- Argument $z = Ku^{k} + b^{k} = (2, -0.3, 0, 1.5)$.\n\nCompute the updated $d^{k+1}$. Express your final answer as a single row vector using standard mathematical notation. No rounding is required, and no physical units apply.", "solution": "The problem asks for two main tasks: first, to derive the closed-form expression for the proximal operator of a weighted $\\ell_1$ norm and relate it to the Split Bregman $d$-update; second, to apply this formula to a specific numerical instance.\n\n**Part 1: Derivation of the Proximal Operator and its Relation to the Split Bregman Update**\n\nLet $g(d) = \\sum_{j=1}^{p} w_{j} |d_{j}|$, where $w_j > 0$. The proximal operator of $g$ with parameter $\\tau > 0$ is defined as:\n$$\n\\mathrm{prox}_{\\tau g}(z) = \\arg\\min_{x \\in \\mathbb{R}^{p}} \\left\\{ \\tau g(x) + \\frac{1}{2}\\|x - z\\|_{2}^{2} \\right\\}\n$$\nSubstituting the expression for $g(x)$, the objective function is:\n$$\nF(x) = \\tau \\sum_{j=1}^{p} w_{j} |x_{j}| + \\frac{1}{2} \\sum_{j=1}^{p} (x_{j} - z_{j})^{2}\n$$\nThis objective function is separable with respect to the components $x_j$. Therefore, the minimization can be performed independently for each component:\n$$\n(\\mathrm{prox}_{\\tau g}(z))_j = \\arg\\min_{x_j \\in \\mathbb{R}} \\left\\{ \\tau w_j |x_j| + \\frac{1}{2}(x_j - z_j)^2 \\right\\}\n$$\nLet $x_j^*$ be the minimizer. Since the objective function is convex, the optimality condition is that $0$ must be in the subdifferential of the objective function at $x_j^*$. The subdifferential of $|x_j|$ is:\n$$\n\\partial|x_j| = \\begin{cases} \\{\\mathrm{sgn}(x_j)\\} & \\text{if } x_j \\neq 0 \\\\ [-1, 1] & \\text{if } x_j = 0 \\end{cases}\n$$\nThe optimality condition is $0 \\in \\partial \\left( \\tau w_j |x_j| + \\frac{1}{2}(x_j - z_j)^2 \\right)$ evaluated at $x_j^*$. This gives:\n$$\n0 \\in \\tau w_j \\partial|x_j^*| + (x_j^* - z_j)\n$$\nRearranging, we get:\n$$\nz_j - x_j^* \\in \\tau w_j \\partial|x_j^*|\n$$\nWe analyze this condition in three cases based on the value of $x_j^*$:\n\n1.  **Case 1: $x_j^* > 0$**. Then $\\partial|x_j^*| = \\{1\\}$. The condition becomes $z_j - x_j^* = \\tau w_j$, which implies $x_j^* = z_j - \\tau w_j$. This case is only consistent if $x_j^* > 0$, so we require $z_j - \\tau w_j > 0$, or $z_j > \\tau w_j$.\n\n2.  **Case 2: $x_j^* < 0$**. Then $\\partial|x_j^*| = \\{-1\\}$. The condition becomes $z_j - x_j^* = -\\tau w_j$, which implies $x_j^* = z_j + \\tau w_j$. This case is only consistent if $x_j^* < 0$, so we require $z_j + \\tau w_j < 0$, or $z_j < -\\tau w_j$.\n\n3.  **Case 3: $x_j^* = 0$**. Then $\\partial|x_j^*| = [-1, 1]$. The condition becomes $z_j - 0 \\in \\tau w_j [-1, 1]$, which is equivalent to $|z_j| \\le \\tau w_j$.\n\nCombining these three cases gives the solution for $x_j^*$ for any $z_j$:\n$$\nx_j^* = \\begin{cases} z_j - \\tau w_j & \\text{if } z_j > \\tau w_j \\\\ 0 & \\text{if } |z_j| \\le \\tau w_j \\\\ z_j + \\tau w_j & \\text{if } z_j < -\\tau w_j \\end{cases}\n$$\nThis is the well-known element-wise soft-thresholding operator, which can be written compactly as:\n$$\nx_j^* = \\mathrm{sgn}(z_j) \\max(0, |z_j| - \\tau w_j)\n$$\nSo, the proximal operator for the weighted $\\ell_1$ norm is the vector-valued function whose $j$-th component is the soft-thresholding of $z_j$ with threshold $\\tau w_j$.\n\nNext, we connect this to the $d$-update in the Split Bregman method. The update is given by the solution to:\n$$\nd^{k+1} = \\arg\\min_{d \\in \\mathbb{R}^{p}} \\; \\lambda \\sum_{j=1}^{p} w_{j} |d_{j}| \\;+\\; \\frac{\\mu}{2} \\left\\| d - z \\right\\|_{2}^{2}\n$$\nwhere we denote $z = Ku^{k} + b^{k}$. We can rewrite the objective function by dividing by the constant $\\mu > 0$ without changing the minimizer:\n$$\nd^{k+1} = \\arg\\min_{d \\in \\mathbb{R}^{p}} \\; \\frac{\\lambda}{\\mu} \\sum_{j=1}^{p} w_{j} |d_{j}| \\;+\\; \\frac{1}{2} \\left\\| d - z \\right\\|_{2}^{2}\n$$\nThis expression is precisely the definition of $\\mathrm{prox}_{\\tau g}(z)$ with $g(d) = \\sum_{j=1}^{p} w_j |d_j|$ and the parameter $\\tau = \\lambda/\\mu$. Thus, the Split Bregman $d$-update is indeed given by a proximal operator:\n$$\nd^{k+1} = \\mathrm{prox}_{(\\lambda/\\mu) g}\\!\\left(Ku^{k}+b^{k}\\right)\n$$\n\n**Part 2: Application to the Concrete Instance**\n\nWe are given the following values:\n- Weights: $w = (1, 2, 0.5, 3)$\n- Parameters: $\\lambda = 1$, $\\mu = 2$\n- Argument vector: $z = Ku^{k} + b^{k} = (2, -0.3, 0, 1.5)$\n\nFirst, we compute the proximal operator parameter $\\tau$:\n$$\n\\tau = \\frac{\\lambda}{\\mu} = \\frac{1}{2} = 0.5\n$$\nNext, we compute the vector of thresholds, $T$, where $T_j = \\tau w_j$:\n$$\nT = \\tau w = 0.5 \\times (1, 2, 0.5, 3) = (0.5, 1.0, 0.25, 1.5)\n$$\nNow we apply the soft-thresholding formula $d_j^{k+1} = \\mathrm{sgn}(z_j) \\max(0, |z_j| - T_j)$ for each component $j=1, 2, 3, 4$:\n\n- For $j=1$: $z_1 = 2$ and $T_1 = 0.5$. Since $z_1 > T_1$, we have:\n  $$ d_1^{k+1} = z_1 - T_1 = 2 - 0.5 = 1.5 = \\frac{3}{2} $$\n\n- For $j=2$: $z_2 = -0.3$ and $T_2 = 1.0$. Since $|z_2| = 0.3 \\le T_2$, we have:\n  $$ d_2^{k+1} = 0 $$\n\n- For $j=3$: $z_3 = 0$ and $T_3 = 0.25$. Since $|z_3| = 0 \\le T_3$, we have:\n  $$ d_3^{k+1} = 0 $$\n\n- For $j=4$: $z_4 = 1.5$ and $T_4 = 1.5$. Since $|z_4| = 1.5 \\le T_4$, we have:\n  $$ d_4^{k+1} = 0 $$\n\nCombining these components, the updated vector $d^{k+1}$ is:\n$$\nd^{k+1} = \\left(\\frac{3}{2}, 0, 0, 0\\right)\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{3}{2} & 0 & 0 & 0\n\\end{pmatrix}\n}\n$$", "id": "3480434"}, {"introduction": "Building on the concept of proximal operators, this practice demonstrates how to handle more sophisticated regularizers like the elastic net. You will derive the update step for a regularizer that combines the $\\ell_1$ and squared $\\ell_2$ norms, revealing how this composite structure introduces both element-wise shrinkage and a uniform scaling. This exercise showcases the modularity of the split Bregman method, preparing you to apply it to problems requiring structured solutions beyond simple sparsity.", "problem": "Consider the elastic net regularizer $g:\\mathbb{R}^{n}\\to\\mathbb{R}$ defined by $g(d)=\\alpha\\|d\\|_{1}+\\frac{\\beta}{2}\\|d\\|_{2}^{2}$ with $\\alpha>0$ and $\\beta\\geq 0$. Let the proximal operator of a proper, closed, convex function $h$ be defined by $\\mathrm{prox}_{\\tau h}(z)=\\arg\\min_{x\\in\\mathbb{R}^{n}}\\left\\{\\frac{1}{2}\\|x-z\\|_{2}^{2}+\\tau h(x)\\right\\}$ for $\\tau>0$. You are given a constrained composite-regularized least-squares model with a linear splitting variable $d\\in\\mathbb{R}^{m}$ and a linear operator $K:\\mathbb{R}^{n}\\to\\mathbb{R}^{m}$:\n\n$$\n\\min_{u\\in\\mathbb{R}^{n},\\,d\\in\\mathbb{R}^{m}} \\;\\; \\frac{1}{2}\\|Au-b\\|_{2}^{2} + g(d) \\quad \\text{subject to} \\quad Ku=d,\n$$\n\nwhere $A\\in\\mathbb{R}^{p\\times n}$ and $b\\in\\mathbb{R}^{p}$ are given. In the split Bregman (SB) method, the $d$-update at iteration $k$ with penalty parameter $\\mu>0$ and current Bregman variable $b^{k}\\in\\mathbb{R}^{m}$ takes the form\n\n$$\nd^{k+1}=\\arg\\min_{d\\in\\mathbb{R}^{m}} \\left\\{ g(d) + \\frac{\\mu}{2}\\left\\|d - \\bigl(Ku^{k+1}+b^{k}\\bigr)\\right\\|_{2}^{2} \\right\\}.\n$$\n\nStarting from the definition of the proximal operator and basic subdifferential optimality conditions for convex functions, derive the closed-form expression of the proximal operator $\\mathrm{prox}_{\\tau g}(z)$ for $z\\in\\mathbb{R}^{n}$ and $\\tau>0$, in terms of the componentwise soft-thresholding operator $\\mathrm{soft}(z,\\kappa)$ defined by $\\bigl[\\mathrm{soft}(z,\\kappa)\\bigr]_{i}=\\mathrm{sign}(z_{i})\\max\\{|z_{i}|-\\kappa,0\\}$. Then, explain how this expression modifies the $d$-update in the SB iteration, explicitly identifying the effective shrinkage level and scaling factor in terms of $\\alpha$, $\\beta$, and $\\mu$.\n\nYour final answer must be a single closed-form analytic expression for $\\mathrm{prox}_{\\tau g}(z)$ using the soft-thresholding notation. No numerical approximation is required.", "solution": "The problem asks for two main results: first, the derivation of the closed-form expression for the proximal operator of the elastic net regularizer, $\\mathrm{prox}_{\\tau g}(z)$; and second, an explanation of how this applies to the $d$-update in the split Bregman iteration.\n\n**Part 1: Derivation of $\\mathrm{prox}_{\\tau g}(z)$**\n\nBy definition, the proximal operator of $g$ is given by:\n$$\n\\mathrm{prox}_{\\tau g}(z) = \\arg\\min_{x\\in\\mathbb{R}^{n}}\\left\\{\\frac{1}{2}\\|x-z\\|_{2}^{2}+\\tau g(x)\\right\\}\n$$\nWe substitute the expression for the elastic net regularizer $g(x) = \\alpha\\|x\\|_{1}+\\frac{\\beta}{2}\\|x\\|_{2}^{2}$ into the objective function. Let's call the objective function $F(x)$.\n$$\nF(x) = \\frac{1}{2}\\|x-z\\|_{2}^{2} + \\tau \\left(\\alpha\\|x\\|_{1} + \\frac{\\beta}{2}\\|x\\|_{2}^{2}\\right)\n$$\nWe want to find $x^* = \\arg\\min_{x} F(x)$. Let's rearrange the terms in $F(x)$:\n$$\nF(x) = \\tau\\alpha\\|x\\|_{1} + \\frac{1}{2}\\|x-z\\|_{2}^{2} + \\frac{\\tau\\beta}{2}\\|x\\|_{2}^{2}\n$$\nExpanding the quadratic term $\\|x-z\\|_{2}^{2} = x^T x - 2x^T z + z^T z = \\|x\\|_2^2 - 2x^T z + \\|z\\|_2^2$, we get:\n$$\nF(x) = \\tau\\alpha\\|x\\|_{1} + \\frac{1}{2}(\\|x\\|_{2}^{2} - 2x^T z + \\|z\\|_{2}^{2}) + \\frac{\\tau\\beta}{2}\\|x\\|_{2}^{2}\n$$\nGrouping terms involving $x$:\n$$\nF(x) = \\tau\\alpha\\|x\\|_{1} + \\frac{1+\\tau\\beta}{2}\\|x\\|_{2}^{2} - x^T z + \\frac{1}{2}\\|z\\|_{2}^{2}\n$$\nSince the term $\\frac{1}{2}\\|z\\|_{2}^{2}$ is constant with respect to $x$, we can ignore it in the minimization. The problem becomes:\n$$\nx^* = \\arg\\min_{x} \\left\\{ \\tau\\alpha\\|x\\|_{1} + \\frac{1+\\tau\\beta}{2}\\|x\\|_{2}^{2} - x^T z \\right\\}\n$$\nWe can complete the square for the quadratic terms involving $x$:\n$$\n\\frac{1+\\tau\\beta}{2}\\|x\\|_{2}^{2} - x^T z = \\frac{1+\\tau\\beta}{2} \\left( \\|x\\|_{2}^{2} - \\frac{2}{1+\\tau\\beta}x^T z \\right) = \\frac{1+\\tau\\beta}{2} \\left\\| x - \\frac{1}{1+\\tau\\beta}z \\right\\|_{2}^{2} - \\frac{1}{2(1+\\tau\\beta)}\\|z\\|_{2}^{2}\n$$\nSubstituting this back and again dropping the term constant in $x$, the minimization is equivalent to:\n$$\nx^* = \\arg\\min_{x} \\left\\{ \\tau\\alpha\\|x\\|_{1} + \\frac{1+\\tau\\beta}{2} \\left\\| x - \\frac{1}{1+\\tau\\beta}z \\right\\|_{2}^{2} \\right\\}\n$$\nWe can divide the objective by the positive constant $(1+\\tau\\beta)$ without changing the minimizer:\n$$\nx^* = \\arg\\min_{x} \\left\\{ \\frac{\\tau\\alpha}{1+\\tau\\beta}\\|x\\|_{1} + \\frac{1}{2} \\left\\| x - \\frac{1}{1+\\tau\\beta}z \\right\\|_{2}^{2} \\right\\}\n$$\nThis is the definition of the proximal operator of the $\\ell_1$-norm, $\\mathrm{prox}_{\\lambda \\|\\cdot\\|_1}(y) = \\mathrm{soft}(y,\\lambda)$, with parameters $\\lambda = \\frac{\\tau\\alpha}{1+\\tau\\beta}$ and $y = \\frac{1}{1+\\tau\\beta}z$.\nTherefore, the solution is:\n$$\nx^* = \\mathrm{soft}\\left(\\frac{1}{1+\\tau\\beta}z, \\frac{\\tau\\alpha}{1+\\tau\\beta}\\right)\n$$\nThis can be simplified as follows:\n\\begin{align*}\nx_i^* &= \\mathrm{sign}\\left(\\frac{z_i}{1+\\tau\\beta}\\right) \\max\\left\\{\\left|\\frac{z_i}{1+\\tau\\beta}\\right| - \\frac{\\tau\\alpha}{1+\\tau\\beta}, 0\\right\\} \\\\\n&= \\mathrm{sign}(z_i) \\frac{1}{1+\\tau\\beta} \\max\\left\\{|z_i| - \\tau\\alpha, 0\\right\\} \\\\\n&= \\frac{1}{1+\\tau\\beta} \\left( \\mathrm{sign}(z_i) \\max\\left\\{|z_i| - \\tau\\alpha, 0\\right\\} \\right) \\\\\n&= \\frac{1}{1+\\tau\\beta} \\bigl[\\mathrm{soft}(z, \\tau\\alpha)\\bigr]_i\n\\end{align*}\nThus, the closed-form expression for the proximal operator is:\n$$\n\\mathrm{prox}_{\\tau g}(z) = \\frac{1}{1+\\tau\\beta} \\mathrm{soft}(z, \\tau\\alpha)\n$$\n\n**Part 2: Application to the Split Bregman $d$-update**\n\nThe $d$-update in the split Bregman iteration is given by:\n$$\nd^{k+1}=\\arg\\min_{d\\in\\mathbb{R}^{m}} \\left\\{ g(d) + \\frac{\\mu}{2}\\left\\|d - \\bigl(Ku^{k+1}+b^{k}\\bigr)\\right\\|_{2}^{2} \\right\\}\n$$\nTo relate this to the proximal operator definition, we can multiply the objective function by $1/\\mu$, which does not change the minimizer:\n$$\nd^{k+1}=\\arg\\min_{d\\in\\mathbb{R}^{m}} \\left\\{ \\frac{1}{\\mu}g(d) + \\frac{1}{2}\\left\\|d - \\bigl(Ku^{k+1}+b^{k}\\bigr)\\right\\|_{2}^{2} \\right\\}\n$$\nThis expression perfectly matches the definition of the proximal operator $\\mathrm{prox}_{\\tau h}(z)$ with the following correspondences:\n-   The function is $h=g$.\n-   The parameter is $\\tau = \\frac{1}{\\mu}$.\n-   The input vector is $z = Ku^{k+1}+b^{k}$.\n\nTherefore, the $d$-update can be written concisely as a proximal step:\n$$\nd^{k+1} = \\mathrm{prox}_{\\frac{1}{\\mu}g}\\left(Ku^{k+1}+b^{k}\\right)\n$$\nNow, we can use the closed-form expression for $\\mathrm{prox}_{\\tau g}(z)$ derived in the first part by substituting $\\tau = 1/\\mu$ and $z = Ku^{k+1}+b^{k}$:\n$$\nd^{k+1} = \\frac{1}{1+\\left(\\frac{1}{\\mu}\\right)\\beta} \\mathrm{soft}\\left(Ku^{k+1}+b^{k}, \\frac{1}{\\mu}\\alpha\\right)\n$$\nSimplifying the scaling factor:\n$$\nd^{k+1} = \\frac{1}{ \\frac{\\mu+\\beta}{\\mu} } \\mathrm{soft}\\left(Ku^{k+1}+b^{k}, \\frac{\\alpha}{\\mu}\\right) = \\frac{\\mu}{\\mu+\\beta} \\mathrm{soft}\\left(Ku^{k+1}+b^{k}, \\frac{\\alpha}{\\mu}\\right)\n$$\nThis expression modifies the standard soft-thresholding update found in methods for $\\ell_1$ regularization (where $\\beta=0$). The elastic net's $\\ell_2^2$ component introduces a scaling factor.\n-   The **effective shrinkage level** (or threshold) is the second argument of the soft-thresholding operator, which is $\\kappa = \\frac{\\alpha}{\\mu}$.\n-   The **scaling factor** is the multiplicative term applied after thresholding, which is $\\frac{\\mu}{\\mu+\\beta}$.\n\nIn summary, the $d$-subproblem in the split Bregman method for the elastic net model is solved by applying soft-thresholding to the vector $Ku^{k+1}+b^{k}$ with a threshold proportional to $\\alpha$ and inversely proportional to $\\mu$, and then scaling the result by a factor that depends on the ratio of $\\mu$ and $\\beta$. If $\\beta=0$ (pure $\\ell_1$ case), the scaling factor becomes $1$, and we recover the standard soft-thresholding update.", "answer": "$$\\boxed{\\frac{1}{1+\\tau\\beta}\\mathrm{soft}(z, \\tau\\alpha)}$$", "id": "3480393"}]}