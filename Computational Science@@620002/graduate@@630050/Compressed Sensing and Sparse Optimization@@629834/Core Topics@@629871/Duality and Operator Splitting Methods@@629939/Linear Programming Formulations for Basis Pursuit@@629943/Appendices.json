{"hands_on_practices": [{"introduction": "This first practice grounds our understanding of Basis Pursuit in geometry. By tackling a simple two-dimensional problem, we can visualize the interplay between the constraint set, which is an affine line, and the objective function, represented by the expanding contours of the $\\ell_1$-ball. This exercise [@problem_id:3458047] reveals not only how the optimal solution is found at the first point of contact but also illustrates the important scenario where the solution is not unique, forming an entire line segment.", "problem": "Consider the Basis Pursuit (BP) problem of minimizing the $\\ell_1$ norm subject to linear equality constraints. Let $A \\in \\mathbb{R}^{1 \\times 2}$ and $b \\in \\mathbb{R}^{1}$ be specified by $A = \\begin{pmatrix} 1  1 \\end{pmatrix}$ and $b = 1$. The BP problem is to minimize $\\|x\\|_1$ over all $x \\in \\mathbb{R}^{2}$ subject to $Ax = b$. Your tasks are as follows:\n\n- Starting from the definition of the $\\ell_1$ norm and linear equality constraints, write BP as a linear program (LP) by introducing nonnegative variables that represent the positive and negative parts of $x$.\n\n- Using only fundamental properties of the absolute value and convex geometry, determine the complete set of optimal solutions to this BP instance and justify that there are infinitely many optimal solutions.\n\n- Compute the Euclidean length of the set of all optimal solutions (viewed as a subset of $\\mathbb{R}^{2}$).\n\n- Explain the geometric reason for the non-uniqueness in terms of the intersection between a face of the $\\ell_1$ ball and the affine set $\\{x \\in \\mathbb{R}^{2} : Ax = b\\}$.\n\nProvide, as your final answer, the Euclidean length of the set of optimal solutions as a single exact expression. Do not round your answer.", "solution": "The problem is an instance of Basis Pursuit (BP), which is an optimization problem of the form:\n$$ \\min_{x \\in \\mathbb{R}^{n}} \\|x\\|_{1} \\quad \\text{subject to} \\quad Ax = b $$\nHere, we are given $n=2$, the matrix $A \\in \\mathbb{R}^{1 \\times 2}$ as $A = \\begin{pmatrix} 1  1 \\end{pmatrix}$, and the vector $b \\in \\mathbb{R}^{1}$ as $b = 1$. The variable is $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathbb{R}^{2}$. The problem is therefore:\n$$ \\min_{x_1, x_2} (|x_1| + |x_2|) \\quad \\text{subject to} \\quad x_1 + x_2 = 1 $$\n\nFirst, we formulate this problem as a linear program (LP). For any real variable $z$, we can write it as the difference of two non-negative variables, $z = u - v$, where $u \\ge 0$ and $v \\ge 0$. A common way to do this is to set $u = \\max(0, z)$ and $v = \\max(0, -z)$. With this choice, the absolute value is given by $|z| = u + v$. We apply this decomposition to both components of $x$:\nLet $x_1 = u_1 - v_1$ and $x_2 = u_2 - v_2$, where $u_1, u_2, v_1, v_2 \\ge 0$.\nThe objective function $\\|x\\|_1 = |x_1| + |x_2|$ is to be minimized. To ensure linearity, we replace $|x_i|$ with $u_i + v_i$. The objective function of the LP becomes $\\min (u_1 + v_1 + u_2 + v_2)$. The minimization process itself ensures that at the optimum, we have $u_i v_i = 0$, which implies $|x_i| = u_i + v_i$.\nThe equality constraint $Ax = b$ becomes:\n$$ \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} u_1 - v_1 \\\\ u_2 - v_2 \\end{pmatrix} = 1 $$\n$$ (u_1 - v_1) + (u_2 - v_2) = 1 $$\nThus, the BP problem can be written as the following LP:\n$$ \\text{Minimize} \\quad u_1 + v_1 + u_2 + v_2 $$\n$$ \\text{Subject to} \\quad u_1 - v_1 + u_2 - v_2 = 1 $$\n$$ u_1, v_1, u_2, v_2 \\ge 0 $$\n\nNext, we determine the set of all optimal solutions. We analyze the original problem statement directly. The constraint is $x_1 + x_2 = 1$. The objective is to minimize $f(x_1, x_2) = |x_1| + |x_2|$.\nBy the triangle inequality, for any real numbers $x_1$ and $x_2$, we have:\n$$ |x_1| + |x_2| \\ge |x_1 + x_2| $$\nUsing the constraint $x_1 + x_2 = 1$, we can establish a lower bound on the objective function:\n$$ |x_1| + |x_2| \\ge |1| = 1 $$\nThis shows that the minimum possible value of the objective function is at least $1$. Now we need to determine if this value can be achieved.\nEquality in the triangle inequality, $|a| + |b| = |a+b|$, holds if and only if $a$ and $b$ have the same sign (i.e., $ab \\ge 0$), meaning $a,b \\ge 0$ or $a,b \\le 0$.\nSo, we seek solutions $(x_1, x_2)$ that satisfy the constraint $x_1 + x_2 = 1$ and the condition $x_1 x_2 \\ge 0$.\nLet's examine the two cases for $x_1 x_2 \\ge 0$:\n1.  Case $x_1 \\ge 0$ and $x_2 \\ge 0$: The constraint is $x_1 + x_2 = 1$. In this case, the objective function becomes $|x_1| + |x_2| = x_1 + x_2$. Due to the constraint, the objective value is exactly $1$. Any point $(x_1, x_2)$ satisfying $x_1 \\ge 0$, $x_2 \\ge 0$, and $x_1+x_2=1$ is therefore an optimal solution. This set of points forms a line segment in the first quadrant of the Cartesian plane, with endpoints $(1,0)$ and $(0,1)$.\n2.  Case $x_1 \\le 0$ and $x_2 \\le 0$: The sum of two non-positive numbers must be non-positive. Therefore, it is impossible for $x_1 + x_2$ to equal $1$. There are no solutions in this case.\n\nFrom this analysis, the minimum value of the objective function is indeed $1$. The complete set of optimal solutions, which we denote by $S_{opt}$, is given by:\n$$ S_{opt} = \\{ (x_1, x_2) \\in \\mathbb{R}^2 \\mid x_1 + x_2 = 1, x_1 \\ge 0, x_2 \\ge 0 \\} $$\nSince this set is a line segment, it contains infinitely many points. For example, $(\\frac{1}{2}, \\frac{1}{2})$, $(\\frac{1}{3}, \\frac{2}{3})$, and $(\\frac{1}{4}, \\frac{3}{4})$ are all optimal solutions.\n\nThe third task is to compute the Euclidean length of this set of optimal solutions. The set $S_{opt}$ is the line segment connecting the point $P_1 = (1, 0)$ and the point $P_2 = (0, 1)$. The Euclidean length $L$ of this segment is the distance between $P_1$ and $P_2$:\n$$ L = \\sqrt{(1-0)^2 + (0-1)^2} = \\sqrt{1^2 + (-1)^2} = \\sqrt{1+1} = \\sqrt{2} $$\n\nFinally, we explain the geometric reason for the non-uniqueness. The problem is equivalent to finding a point in the affine set $\\mathcal{A} = \\{x \\in \\mathbb{R}^2 \\mid Ax=b\\}$ that has the minimum $\\ell_1$-norm. Geometrically, this means finding the smallest radius $c$ for which the $\\ell_1$-ball, $B_1(c) = \\{ x \\in \\mathbb{R}^2 \\mid \\|x\\|_1 \\le c \\}$, intersects the affine set $\\mathcal{A}$.\nIn $\\mathbb{R}^2$, the level sets $\\|x\\|_1 = c$ are squares rotated by $45^\\circ$, with vertices at $(c, 0)$, $(-c, 0)$, $(0, c)$, and $(0, -c)$. The affine set $\\mathcal{A}$ is the line defined by the equation $x_1 + x_2 = 1$.\nWe are essentially \"inflating\" an $\\ell_1$-ball centered at the origin until it first touches the line $x_1 + x_2 = 1$. The smallest such ball corresponds to $c=1$. The boundary of this ball, $\\|x\\|_1 = 1$, is given by $|x_1|+|x_2|=1$.\nThe line $x_1+x_2=1$ intersects this $\\ell_1$-ball. Let's analyze the intersection. The face of the $\\ell_1$-ball in the first quadrant ($x_1 \\ge 0, x_2 \\ge 0$) is described by the equation $x_1+x_2=1$. This is precisely a segment of the line defining the affine constraint set $\\mathcal{A}$.\nTherefore, the intersection of the smallest $\\ell_1$-ball that meets the affine set is not a single point (a vertex) but an entire face (an edge) of the $\\ell_1$-ball. This intersection is the set of optimal solutions. Because the intersection is a line segment, there are infinitely many optimal solutions. The non-uniqueness arises because the affine hyperplane $Ax=b$ is parallel to a face of the polyhedral $\\ell_1$-ball.", "answer": "$$\\boxed{\\sqrt{2}}$$", "id": "3458047"}, {"introduction": "Moving from geometric intuition to algorithmic execution, this practice delves into the computational heart of solving Basis Pursuit via linear programming. You will convert a small-scale problem into the canonical standard form and perform a single, deliberate pivot of the simplex method [@problem_id:3458063]. This exercise provides a crucial hands-on understanding of how the abstract mechanics of the simplex algorithm correspond to the intuitive process of building a sparse solution by iteratively adding and removing elements from its support.", "problem": "Consider the Basis Pursuit problem in compressed sensing: minimize the vector one-norm subject to linear measurements. Specifically, let $A \\in \\mathbb{R}^{2 \\times 3}$ and $b \\in \\mathbb{R}^{2}$ be given by\n$$\nA = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix}, \n\\qquad\nb = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nThe Basis Pursuit problem is\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = b.\n$$\nTasks:\n1) Convert this problem to a linear program in canonical standard form using the variable split $x = u - v$ with $u \\in \\mathbb{R}^{3}$, $v \\in \\mathbb{R}^{3}$, $u \\geq 0$, $v \\geq 0$. Explicitly write the linear program as a minimization with an equality constraint matrix multiplying the nonnegative decision vector, equated to the right-hand side, and with a linear objective.\n2) Using the canonical standard form from part 1), initialize the simplex method at the basic feasible solution that takes as basic variables the first two nonnegative split variables corresponding to the first two columns of $A$ (i.e., use the columns of $A$ that form the identity submatrix). Adopt the classical reduced cost rule for selecting an entering variable in minimization and the standard ratio test for selecting a leaving variable. When ties occur in either selection, break them by choosing the variable with the smallest index.\n3) Perform exactly one simplex pivot from this initial basic feasible solution. Interpret the entering variable as adding an atom to the support and the leaving variable as dropping an atom from the support in the split-variable representation. Then compute the new objective value after this single pivot.\nGive your final answer as the exact value of the new objective after the pivot. No rounding is required, and no units are involved.", "solution": "The problem asks to solve a Basis Pursuit instance, perform one pivot of the simplex method, and report the new objective value.\n\n**1) Conversion to a Linear Program (LP)**\n\nThe Basis Pursuit problem is given by\n$$ \\min_{x \\in \\mathbb{R}^{3}} \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = b $$\nwhere $\\|x\\|_1 = \\sum_{j=1}^{3} |x_j|$. To convert this into a linear program, we use the variable split technique. For each component $x_j$ of the vector $x \\in \\mathbb{R}^3$, we introduce two non-negative variables, $u_j \\geq 0$ and $v_j \\geq 0$, such that $x_j = u_j - v_j$. With this substitution, the absolute value $|x_j|$ can be expressed as $u_j + v_j$, provided that we are minimizing the sum $\\sum_j (u_j+v_j)$, which ensures that for each $j$, at least one of $u_j$ or $v_j$ is zero in an optimal solution.\n\nThe objective function becomes:\n$$ \\min \\sum_{j=1}^{3} (u_j + v_j) $$\nThe constraint $Ax=b$ becomes:\n$$ A(u-v) = b \\quad \\implies \\quad Au - Av = b $$\nwhere $u = \\begin{pmatrix} u_1 \\\\ u_2 \\\\ u_3 \\end{pmatrix}$ and $v = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{pmatrix}$.\n\nWe define a single decision vector $z \\in \\mathbb{R}^6$ and a cost vector $c \\in \\mathbb{R}^6$:\n$$ z = \\begin{pmatrix} u_1 \\\\ u_2 \\\\ u_3 \\\\ v_1 \\\\ v_2 \\\\ v_3 \\end{pmatrix}, \\quad c = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} $$\nThe objective is then to minimize $c^T z$.\n\nThe constraint can be written as $[A \\ \\ -A] z = b$. Let $M = [A \\ \\ -A]$. Using the given matrix $A = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix}$, we have:\n$$ M = \\begin{pmatrix} 1  0  1  -1  0  -1 \\\\ 0  1  1  0  -1  -1 \\end{pmatrix} $$\nThe vector $b$ is given as $b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\nThe problem in canonical standard form is:\n$$ \\min_{z \\in \\mathbb{R}^{6}} c^T z \\quad \\text{subject to} \\quad M z = b, \\ z \\geq 0 $$\nwith $c$, $M$, and $b$ as defined above.\n\n**2) Initialization of the Simplex Method**\n\nWe are instructed to initialize the simplex method with a basic feasible solution (BFS) where the basic variables correspond to the first two non-negative split variables associated with the first two columns of $A$. These are $u_1$ and $u_2$. The set of basic variables is $\\mathcal{B} = \\{u_1, u_2\\}$, and the non-basic variables are $\\mathcal{N} = \\{u_3, v_1, v_2, v_3\\}$.\n\nThe basis matrix $B$ consists of the columns of $M$ corresponding to the basic variables, which are the first and second columns:\n$$ B = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = I_2 $$\nThe values of the basic variables, $x_B = \\begin{pmatrix} u_1 \\\\ u_2 \\end{pmatrix}$, are found by solving $Bx_B=b$:\n$$ x_B = B^{-1}b = I_2 b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\nSo, the initial BFS has $u_1=1$ and $u_2=1$. The non-basic variables are all zero. The full initial solution vector is $z = \\begin{pmatrix} 1, 1, 0, 0, 0, 0 \\end{pmatrix}^T$. This solution is feasible as $z \\geq 0$.\n\nThe initial objective function value is $c^T z = c_B^T x_B$, where $c_B = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ are the costs for the basic variables:\n$$ \\text{Objective Value} = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 1 \\cdot 1 + 1 \\cdot 1 = 2 $$\n\n**3) One Simplex Pivot**\n\nFirst, we calculate the reduced costs ($\\bar{c}_j$) for all non-basic variables $z_j$. The formula is $\\bar{c}_j = c_j - c_B^T B^{-1} M_j$, where $M_j$ is the $j$-th column of $M$. Since $B=I_2$, this simplifies to $\\bar{c}_j = c_j - c_B^T M_j$. Here $c_B^T = \\begin{pmatrix} 1  1 \\end{pmatrix}$.\n\n- For $u_3$ (index $j=3$): $c_3 = 1$, $M_3 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n$$ \\bar{c}_3 = 1 - \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 1 - (1+1) = -1 $$\n- For $v_1$ (index $j=4$): $c_4 = 1$, $M_4 = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$.\n$$ \\bar{c}_4 = 1 - \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix} = 1 - (-1) = 2 $$\n- For $v_2$ (index $j=5$): $c_5 = 1$, $M_5 = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}$.\n$$ \\bar{c}_5 = 1 - \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} = 1 - (-1) = 2 $$\n- For $v_3$ (index $j=6$): $c_6 = 1$, $M_6 = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}$.\n$$ \\bar{c}_6 = 1 - \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix} = 1 - (-2) = 3 $$\n\nThe set of reduced costs for the non-basic variables is $\\{-1, 2, 2, 3\\}$. For a minimization problem, the entering variable is the one with the most negative reduced cost. Here, only $\\bar{c}_3$ is negative. Thus, the entering variable is $u_3$.\n\nNext, we perform the ratio test to find the leaving variable. We compute the vector $d = B^{-1}M_3 = I_2 M_3 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. We find the minimum ratio $\\theta = \\min_i \\{ (x_B)_i / d_i \\mid d_i  0 \\}$. The current basic variables are $(x_B)_1 = u_1 = 1$ and $(x_B)_2 = u_2 = 1$.\n\n- For $u_1$: ratio is $1/1 = 1$.\n- For $u_2$: ratio is $1/1 = 1$.\n\nThere is a tie. The problem specifies to break ties by choosing the variable with the smallest index. The index of $u_1$ in the vector $z$ is $1$, and the index of $u_2$ is $2$. Since $1  2$, the leaving variable is $u_1$.\n\nThe entering variable $u_3$ corresponds to adding the third atom (column $A_3$) to the solution representation. The leaving variable $u_1$ corresponds to removing the first atom (column $A_1$) from the support.\n\nAfter the pivot, the new set of basic variables is $\\{u_2, u_3\\}$. The new basis matrix is $B_{\\text{new}} = \\begin{pmatrix} M_2  M_3 \\end{pmatrix} = \\begin{pmatrix} 0  1 \\\\ 1  1 \\end{pmatrix}$. The new vector of basic variables is $x_{B, \\text{new}} = \\begin{pmatrix} u_2 \\\\ u_3 \\end{pmatrix}$. We solve for their values using $B_{\\text{new}} x_{B, \\text{new}} = b$:\n$$ \\begin{pmatrix} 0  1 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} u_2 \\\\ u_3 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\nFrom the first row, $0 \\cdot u_2 + 1 \\cdot u_3 = 1$, which gives $u_3 = 1$.\nSubstituting into the second row, $1 \\cdot u_2 + 1 \\cdot u_3 = 1 \\implies u_2 + 1 = 1 \\implies u_2 = 0$.\n\nThe new BFS is degenerate since the basic variable $u_2$ has a value of $0$. The new solution vector is $z_{\\text{new}} = \\begin{pmatrix} 0, 0, 1, 0, 0, 0 \\end{pmatrix}^T$.\nThe new objective function value is calculated using this new solution:\n$$ \\text{New Objective Value} = c^T z_{\\text{new}} = 1 \\cdot 0 + 1 \\cdot 0 + 1 \\cdot 1 + 1 \\cdot 0 + 1 \\cdot 0 + 1 \\cdot 0 = 1 $$\nAlternatively, the change in objective value is the product of the minimum ratio $\\theta=1$ and the reduced cost of the entering variable $\\bar{c}_3=-1$.\n$$ \\text{New Objective Value} = \\text{Old Objective Value} + \\theta \\cdot \\bar{c}_3 = 2 + 1 \\cdot (-1) = 1 $$\nBoth methods yield the same result. The objective value after one pivot is $1$.", "answer": "$$\\boxed{1}$$", "id": "3458063"}, {"introduction": "This final practice addresses the concept of degeneracy, a common occurrence in linear programming that leads to non-unique optimal solutions in Basis Pursuit. This exercise [@problem_id:3458070] challenges you to fully characterize the set of all minimizers for a degenerate problem and introduces sophisticated techniques for its analysis. By constructing a dual certificate and applying a secondary optimization criterion as a tie-breaker, you will learn how to prove optimality and select a single, well-behaved solution from an infinite set of candidates.", "problem": "Consider Basis Pursuit (BP), which seeks to recover a sparse vector by solving the convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = b,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^{m}$. Basis Pursuit can be formulated as a Linear Programming (LP) problem by introducing auxiliary variables to linearize the absolute value.\n\nYou are asked to analyze degeneracy and tie-breaking for the BP problem with the specific data\n$$\nA = \\begin{pmatrix} 1  1  0  0 \\\\ 0  0  1  1 \\end{pmatrix}, \\qquad b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix},\n$$\nwhere $A \\in \\mathbb{R}^{2 \\times 4}$ and $b \\in \\mathbb{R}^{2}$.\n\nTasks:\n1. Starting from the definition of the $\\ell_1$ norm and the absolute value, derive a Linear Programming (LP) formulation of BP by introducing auxiliary variables $t \\in \\mathbb{R}^{4}$ and writing the problem with linear equality and inequality constraints only. Do not invoke any pre-derived LP templates; derive the formulation from first principles.\n2. Using only widely accepted facts (e.g., the triangle inequality $|x_{1}| + |x_{2}| \\geq |x_{1} + x_{2}|$), characterize the complete set of BP minimizers for the given $A$ and $b$. In your characterization, identify which primal inequalities are active at optimality and explain precisely why this constitutes degeneracy within the LP (many constraints active simultaneously at an optimum).\n3. Derive the dual problem from the LP in Task 1 using standard Lagrangian duality, and exhibit a dual certificate that proves optimality of your primal characterization in Task 2. Explain how complementary slackness reveals which primal constraints are tight at the solution.\n4. Impose the following tie-breaking rule on the set of BP minimizers: among all minimizers of $\\|x\\|_{1}$, select the one that minimizes the infinity norm $\\|x\\|_{\\infty}$; if multiple vectors share the same infinity norm, select the lexicographically smallest vector (in the standard lexicographic order on $\\mathbb{R}^{4}$). Compute this uniquely selected solution $x^{\\star}$.\n\nProvide your final answer as the vector $x^{\\star}$ in a single row using the `pmatrix` environment. No rounding is needed and no units are involved.", "solution": "### Task 1: Linear Programming Formulation\n\nThe Basis Pursuit (BP) problem is given by\n$$ \\min_{x \\in \\mathbb{R}^{n}} \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = b $$\nwhere $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_i|$. The objective function is non-linear due to the absolute value function. To formulate this as a Linear Programming (LP) problem, we linearize the objective. For each component $x_i$ of the vector $x \\in \\mathbb{R}^{4}$, we introduce an auxiliary variable $t_i \\in \\mathbb{R}$. The core idea is to replace $|x_i|$ with $t_i$ in the objective function, under the constraint that $t_i \\ge |x_i|$. The minimization of $\\sum t_i$ will ensure that at the optimal solution, we have $t_i = |x_i|$.\n\nThe inequality $t_i \\ge |x_i|$ is equivalent to the pair of linear inequalities:\n$$ x_i \\le t_i \\quad \\text{and} \\quad -x_i \\le t_i $$\nThese can be rewritten as:\n$$ x_i - t_i \\le 0 $$\n$$ -x_i - t_i \\le 0 $$\nIntroducing the vector $t = (t_1, t_2, t_3, t_4)^T \\in \\mathbb{R}^{4}$, the BP problem is equivalent to the following LP:\n$$ \\min_{x \\in \\mathbb{R}^{4}, t \\in \\mathbb{R}^{4}} \\sum_{i=1}^{4} t_i $$\nsubject to the constraints:\n1. $A x = b$\n2. $x_i - t_i \\le 0$ for $i=1, 2, 3, 4$\n3. $-x_i - t_i \\le 0$ for $i=1, 2, 3, 4$\n\nThis is a linear program in the $8$ variables $(x_1, x_2, x_3, x_4, t_1, t_2, t_3, t_4)$. The objective function is $1^T t$ (where $1$ is the vector of all ones), and the constraints are all linear equalities or inequalities.\n\n### Task 2: Characterization of Minimizers and Degeneracy\n\nWe are given the specific data:\n$$ A = \\begin{pmatrix} 1  1  0  0 \\\\ 0  0  1  1 \\end{pmatrix}, \\qquad b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} $$\nThe constraint $Ax = b$ expands to two linear equations:\n1. $x_1 + x_2 = 1$\n2. $x_3 + x_4 = 0$\n\nFrom the second equation, we have $x_4 = -x_3$. The objective is to minimize $\\|x\\|_1 = |x_1| + |x_2| + |x_3| + |x_4|$. Substituting the constraints:\n$$ \\|x\\|_1 = |x_1| + |1-x_1| + |x_3| + |-x_3| = |x_1| + |1-x_1| + 2|x_3| $$\nTo minimize this sum, we can minimize its parts independently.\n- For the term $2|x_3|$, the minimum value is $0$, which is achieved when $x_3 = 0$. This implies $x_4 = -x_3 = 0$.\n- For the term $|x_1| + |1-x_1|$, we use the triangle inequality: $|a| + |b| \\ge |a+b|$. Here, let $a=x_1$ and $b=1-x_1$. Then $|x_1| + |1-x_1| \\ge |x_1 + (1-x_1)| = |1| = 1$. Equality holds if and only if $x_1$ and $1-x_1$ have the same sign or one is zero. This condition is $x_1 \\ge 0$ and $1-x_1 \\ge 0$, which simplifies to $0 \\le x_1 \\le 1$.\n\nCombining these findings, the minimum value of $\\|x\\|_1$ is $1+0=1$. This minimum is achieved for any vector $x$ of the form:\n$$ x = (\\alpha, 1-\\alpha, 0, 0)^T \\quad \\text{for any } \\alpha \\in [0, 1] $$\nThe set of all BP minimizers is $S = \\{ (\\alpha, 1-\\alpha, 0, 0)^T \\mid \\alpha \\in [0, 1] \\}$.\n\nThis situation constitutes degeneracy in the context of the LP formulation. An LP is degenerate if a basic feasible solution (a vertex of the feasible polytope) has more than the standard number of active constraints. Our LP has $8$ variables $(x, t)$. A non-degenerate vertex would be determined by exactly $8$ active, linearly independent constraints.\nThe set of optimal solutions $S$ is a line segment whose endpoints are $x^{(0)} = (0, 1, 0, 0)^T$ (for $\\alpha=0$) and $x^{(1)} = (1, 0, 0, 0)^T$ (for $\\alpha=1$). Let's analyze the vertex corresponding to $x^{(1)}$:\nThe primal LP variables are $(x_1, x_2, x_3, x_4, t_1, t_2, t_3, t_4) = (1, 0, 0, 0, 1, 0, 0, 0)$.\nThe active constraints at this point are:\n1. $x_1 + x_2 = 1$ (equality constraint, always active)\n2. $x_3 + x_4 = 0$ (equality constraint, always active)\n3. $x_1 - t_1 = 1 - 1 = 0$ (active)\n4. $x_2 - t_2 = 0 - 0 = 0$ (active)\n5. $-x_2 - t_2 = 0 - 0 = 0$ (active)\n6. $x_3 - t_3 = 0 - 0 = 0$ (active)\n7. $-x_3 - t_3 = 0 - 0 = 0$ (active)\n8. $x_4 - t_4 = 0 - 0 = 0$ (active)\n9. $-x_4 - t_4 = 0 - 0 = 0$ (active)\nThere are $9$ active constraints for an $8$-variable problem. A similar analysis for $x^{(0)}$ also shows $9$ active constraints. This over-determination of the vertices is the definition of primal degeneracy. The existence of a non-unique optimal solution set (the segment $S$) is a direct consequence of this degeneracy.\n\n### Task 3: Dual Problem and Certificate\n\nWe derive the dual problem from the LP in Task 1 using Lagrangian duality.\nPrimal LP:\n$$ \\min_{x,t} \\sum_{i=1}^{4} t_i \\quad \\text{s.t.} \\quad Ax=b, \\quad x-t \\le 0, \\quad -x-t \\le 0. $$\nThe Lagrangian is:\n$$ L(x, t, \\nu, \\mu_1, \\mu_2) = \\sum_{i=1}^{4} t_i + \\nu^T(Ax - b) + \\mu_1^T(x-t) + \\mu_2^T(-x-t) $$\nwhere $\\nu \\in \\mathbb{R}^2$ are the dual variables for $Ax=b$, and $\\mu_1, \\mu_2 \\in \\mathbb{R}^4$ are the non-negative dual variables for the inequality constraints.\nGrouping terms by $x$ and $t$:\n$$ L = (A^T\\nu + \\mu_1 - \\mu_2)^T x + (1 - \\mu_1 - \\mu_2)^T t - b^T\\nu $$\nThe dual function $g(\\nu, \\mu_1, \\mu_2) = \\inf_{x,t} L$ is finite only if the coefficients of $x$ and $t$ are zero:\n1. $A^T\\nu + \\mu_1 - \\mu_2 = 0 \\implies A^T\\nu = \\mu_2 - \\mu_1$\n2. $1 - \\mu_1 - \\mu_2 = 0 \\implies \\mu_1 + \\mu_2 = 1$ (component-wise)\n\nWhen these conditions hold, $L$ simplifies to $-b^T\\nu$. The dual problem is to maximize this value:\n$$ \\max_{\\nu, \\mu_1, \\mu_2} -b^T\\nu \\quad \\text{s.t.} \\quad A^T\\nu = \\mu_2 - \\mu_1, \\quad \\mu_1 + \\mu_2 = 1, \\quad \\mu_1 \\ge 0, \\quad \\mu_2 \\ge 0 $$\nWe can eliminate $\\mu_1, \\mu_2$. Let $z = A^T\\nu$. For each component $i$, $z_i=\\mu_{2i}-\\mu_{1i}$ and $1=\\mu_{1i}+\\mu_{2i}$. Solving for $\\mu_{1i}, \\mu_{2i}$ gives $\\mu_{1i}=(1-z_i)/2$ and $\\mu_{2i}=(1+z_i)/2$. The non-negativity constraints $\\mu_{1i} \\ge 0, \\mu_{2i} \\ge 0$ imply $|z_i| \\le 1$. Thus, the dual problem simplifies to:\n$$ \\max_{\\nu \\in \\mathbb{R}^2} -b^T\\nu \\quad \\text{s.t.} \\quad \\|A^T\\nu\\|_{\\infty} \\le 1 $$\nThe primal optimal value is $p^* = 1$. By strong duality, the dual optimal value must be $d^*=1$. With $b=(1, 0)^T$, the dual objective is $-b^T\\nu = -\\nu_1$. So we must have $-\\nu_1 = 1$, which implies $\\nu_1 = -1$.\nThe dual feasibility constraint is $\\|A^T\\nu\\|_{\\infty} \\le 1$:\n$$ A^T\\nu = \\begin{pmatrix} 1  0 \\\\ 1  0 \\\\ 0  1 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ \\nu_2 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -1 \\\\ \\nu_2 \\\\ \\nu_2 \\end{pmatrix} $$\nThe infinity norm constraint requires $|-1| \\le 1$ and $|\\nu_2| \\le 1$. The first is true, so we only require $\\nu_2 \\in [-1, 1]$.\nA valid dual certificate is any vector $\\nu = (-1, \\nu_2)^T$ with $\\nu_2 \\in [-1, 1]$. For instance, let's choose $\\nu_c = (-1, 0)^T$. This vector proves the optimality of any primal solution in the set $S$.\n\nComplementary slackness conditions state that for an optimal primal-dual pair, the product of a slack variable and its corresponding dual variable must be zero. For our LP this means:\n$\\mu_{1i}(x_i-t_i) = 0$ and $\\mu_{2i}(-x_i-t_i) = 0$ for $i=1,..,4$.\nUsing the dual certificate $\\nu_c = (-1, 0)^T$, we find $A^T\\nu_c = (-1, -1, 0, 0)^T$. The corresponding dual variables are:\n$\\mu_1 = (1 - A^T\\nu_c)/2 = (1, 1, 1/2, 1/2)^T$\n$\\mu_2 = (1 + A^T\\nu_c)/2 = (0, 0, 1/2, 1/2)^T$\nThe CS conditions are:\n- For $i=1$: $\\mu_{11}=1 \\implies x_1-t_1=0 \\implies x_1=t_1$. Since $t_1=|x_1|$, this implies $x_1 \\ge 0$. As $\\mu_{21}=0$, the second CS condition is trivially satisfied.\n- For $i=2$: $\\mu_{12}=1 \\implies x_2-t_2=0 \\implies x_2 \\ge 0$. $\\mu_{22}=0$.\n- For $i=3$: $\\mu_{13}=1/2 \\implies x_3-t_3=0$. Also, $\\mu_{23}=1/2 \\implies -x_3-t_3=0$. Together, they force $x_3=0$ and $t_3=0$.\n- For $i=4$: Similarly, $\\mu_{14}=1/2$ and $\\mu_{24}=1/2$ force $x_4=0$ and $t_4=0$.\nThese conditions derived from CS ($x_1 \\ge 0, x_2 \\ge 0, x_3=0, x_4=0$) combined with the primal feasibility constraint $x_1+x_2=1$ perfectly describe the optimal set $S = \\{(\\alpha, 1-\\alpha, 0, 0)^T \\mid \\alpha \\in [0, 1]\\}$.\n\n### Task 4: Tie-breaking Rule\n\nWe must select a unique solution $x^{\\star}$ from the set of BP optimizers $S$ by applying a tie-breaking rule. The set is $S = \\{ x(\\alpha) = (\\alpha, 1-\\alpha, 0, 0)^T \\mid \\alpha \\in [0, 1] \\}$.\n\nFirst, we minimize the infinity norm $\\|x\\|_{\\infty}$ over this set.\nFor $x(\\alpha) \\in S$, the infinity norm is:\n$$ \\|x(\\alpha)\\|_{\\infty} = \\max(|\\alpha|, |1-\\alpha|, |0|, |0|) $$\nSince $\\alpha \\in [0, 1]$, both $\\alpha$ and $1-\\alpha$ are non-negative, so:\n$$ \\|x(\\alpha)\\|_{\\infty} = \\max(\\alpha, 1-\\alpha) $$\nWe want to find $\\arg\\min_{\\alpha \\in [0, 1]} \\max(\\alpha, 1-\\alpha)$. The function $g(\\alpha) = \\max(\\alpha, 1-\\alpha)$ is minimized when its two arguments are equal, i.e., $\\alpha = 1-\\alpha$. This gives $2\\alpha=1$, so $\\alpha = 1/2$.\nFor $\\alpha=1/2$, the infinity norm is $\\max(1/2, 1/2) = 1/2$. For any other $\\alpha \\in [0,1]$, one of $\\alpha$ or $1-\\alpha$ will be greater than $1/2$. For example, if $\\alpha=0.6$, $\\|x\\|_{\\infty}=0.6 > 0.5$. If $\\alpha=0.4$, $\\|x\\|_{\\infty}=0.6 > 0.5$.\nThus, the minimum infinity norm over the set $S$ is $1/2$, and it is uniquely achieved at $\\alpha=1/2$.\n\nSince the first tie-breaking rule (minimizing $\\|x\\|_{\\infty}$) yields a unique solution, we do not need to apply the second rule (lexicographical ordering).\nThe uniquely selected solution $x^{\\star}$ corresponds to $\\alpha = 1/2$:\n$$ x^{\\star} = \\left(\\frac{1}{2}, 1-\\frac{1}{2}, 0, 0\\right)^T = \\left(\\frac{1}{2}, \\frac{1}{2}, 0, 0\\right)^T $$", "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2}  0  0 \\end{pmatrix}} $$", "id": "3458070"}]}