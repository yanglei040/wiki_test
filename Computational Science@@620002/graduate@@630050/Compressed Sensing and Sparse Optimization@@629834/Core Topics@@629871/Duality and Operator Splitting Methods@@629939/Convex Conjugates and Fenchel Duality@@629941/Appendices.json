{"hands_on_practices": [{"introduction": "The principles of convex duality extend far beyond vector spaces, proving essential in modern large-scale problems involving matrices. This exercise explores the nuclear norm, a key convex surrogate for matrix rank, and asks you to derive its convex conjugate from first principles [@problem_id:3439408]. By completing this practice, you will uncover the fundamental dual relationship between the nuclear norm and the operator norm, a cornerstone of matrix compressed sensing and low-rank recovery.", "problem": "Let $\\mathbb{R}^{m \\times n}$ be equipped with the Frobenius inner product $\\langle X, Y \\rangle = \\operatorname{trace}(X^{\\top} Y)$ and the nuclear norm $\\|X\\|_{*}$ defined as the sum of the singular values of $X$. Consider the linear measurement operator $\\mathcal{A} : \\mathbb{R}^{m \\times n} \\to \\mathbb{R}^{p}$ with adjoint $\\mathcal{A}^{*} : \\mathbb{R}^{p} \\to \\mathbb{R}^{m \\times n}$ defined with respect to the Frobenius and standard Euclidean inner products. Using only first principles and well-tested facts, starting from the definitions of the convex conjugate and Fenchel duality, compute the convex conjugate $f^{*}(Y)$ of $f(X) = \\|X\\|_{*}$ and then derive the Fenchel dual of the matrix compressed sensing problem\n$$\n\\min_{X \\in \\mathbb{R}^{m \\times n}} \\|X\\|_{*} \\quad \\text{subject to} \\quad \\mathcal{A}(X) = b .\n$$\nYour derivation must rely on core definitions (convex conjugate, adjoint operator, subdifferential) and well-tested facts (such as properties of the Singular Value Decomposition (SVD) and trace inequalities), and must remain scientifically realistic and self-consistent. Express the final answer as two closed-form analytic expressions arranged as a row matrix using the $\\mathrm{pmatrix}$ environment, where the first entry is the expression for $f^{*}(Y)$ and the second entry is the expression for the dual program (as a maximization over $y \\in \\mathbb{R}^{p}$ with an explicit constraint). No numerical approximation is required. There are no physical units involved.", "solution": "The user requests the computation of the convex conjugate of the nuclear norm and the derivation of the Fenchel dual for a matrix compressed sensing problem.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- The vector space is $\\mathbb{R}^{m \\times n}$.\n- The inner product is the Frobenius inner product: $\\langle X, Y \\rangle = \\operatorname{trace}(X^{\\top} Y)$.\n- The function to be analyzed is $f(X) = \\|X\\|_{*}$, where $\\|X\\|_{*}$ is the nuclear norm, defined as the sum of the singular values of $X$.\n- A linear operator $\\mathcal{A} : \\mathbb{R}^{m \\times n} \\to \\mathbb{R}^{p}$ is given.\n- Its adjoint operator is $\\mathcal{A}^{*} : \\mathbb{R}^{p} \\to \\mathbb{R}^{m \\times n}$, defined with respect to the Frobenius and standard Euclidean inner products, such that $\\langle \\mathcal{A}(X), y \\rangle_{\\mathbb{R}^p} = \\langle X, \\mathcal{A}^{*}(y) \\rangle_{\\mathbb{R}^{m \\times n}}$ for all $X \\in \\mathbb{R}^{m \\times n}$ and $y \\in \\mathbb{R}^p$.\n- The primal optimization problem is: $\\min_{X \\in \\mathbb{R}^{m \\times n}} \\|X\\|_{*}$ subject to $\\mathcal{A}(X) = b$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly rooted in the mathematical field of convex analysis and optimization, specifically its application to compressed sensing. The concepts of nuclear norm, convex conjugate, Fenchel duality, and adjoint operators are standard and well-defined.\n- **Well-Posed:** The problem asks for the derivation of two specific mathematical expressions based on the givens. These expressions are known to exist and be unique.\n- **Objective:** The problem statement is precise, quantitative, and free of any subjective or opinion-based language.\n- **Self-contained and Consistent:** All necessary definitions and context are provided. There are no internal contradictions.\n\nThe problem does not exhibit any of the flaws listed in the validation criteria. It is a standard, non-trivial problem in optimization theory.\n\n**Step 3: Verdict and Action**\nThe problem is valid. The solution process will proceed.\n\n### Part 1: Convex Conjugate of the Nuclear Norm\n\nThe function is $f(X) = \\|X\\|_{*}$. The convex conjugate, denoted $f^{*}(Y)$, is defined with respect to the given Frobenius inner product as:\n$$\nf^{*}(Y) = \\sup_{X \\in \\mathbb{R}^{m \\times n}} \\{ \\langle Y, X \\rangle - f(X) \\} = \\sup_{X \\in \\mathbb{R}^{m \\times n}} \\{ \\operatorname{trace}(Y^{\\top} X) - \\|X\\|_{*} \\}\n$$\nTo evaluate this supremum, we use the von Neumann trace inequality, which states that for any two matrices $A, B \\in \\mathbb{R}^{m \\times n}$, $\\operatorname{trace}(A^{\\top} B) \\leq \\sum_{i=1}^{\\min(m,n)} \\sigma_i(A) \\sigma_i(B)$, where $\\sigma_i(\\cdot)$ denotes the $i$-th singular value in non-increasing order. Equality holds if and only if $A$ and $B$ admit simultaneous singular value decompositions, i.e., $A = U \\Sigma_A V^{\\top}$ and $B = U \\Sigma_B V^{\\top}$ for some orthogonal matrices $U$ and $V$.\n\nApplying this inequality to our expression, we have:\n$$\n\\operatorname{trace}(Y^{\\top} X) \\leq \\sum_{i} \\sigma_i(Y) \\sigma_i(X)\n$$\nThe nuclear norm is $\\|X\\|_{*} = \\sum_i \\sigma_i(X)$. Substituting these into the definition of the conjugate gives:\n$$\n\\operatorname{trace}(Y^{\\top} X) - \\|X\\|_{*} \\leq \\sum_{i} \\sigma_i(Y) \\sigma_i(X) - \\sum_{i} \\sigma_i(X) = \\sum_{i} (\\sigma_i(Y) - 1) \\sigma_i(X)\n$$\nWe analyze the value of this supremum based on the singular values of $Y$. Let $\\|Y\\|_{op} = \\sigma_1(Y)$ be the operator norm (or spectral norm) of $Y$, which is its largest singular value.\n\nCase 1: $\\|Y\\|_{op}  1$.\nThis implies that $\\sigma_1(Y)  1$. Let the SVD of $Y$ be $Y = U \\Sigma_Y V^{\\top}$, where $u_1$ and $v_1$ are the first left and right singular vectors corresponding to $\\sigma_1(Y)$. We can construct a matrix $X = \\alpha u_1 v_1^{\\top}$ for some scalar $\\alpha  0$. The matrix $X$ has only one non-zero singular value, which is $\\alpha$. Thus, $\\|X\\|_{*} = \\alpha$. The inner product is:\n$$\n\\operatorname{trace}(Y^{\\top} X) = \\operatorname{trace}((U \\Sigma_Y V^{\\top})^{\\top} (\\alpha u_1 v_1^{\\top})) = \\alpha \\operatorname{trace}(V \\Sigma_Y^{\\top} U^{\\top} u_1 v_1^{\\top})\n$$\nSince $U^{\\top} u_1 = e_1$ (the first standard basis vector) and $v_1^{\\top}V = e_1^{\\top}$, we have:\n$$\n\\operatorname{trace}(Y^{\\top} X) = \\alpha \\operatorname{trace}(V \\Sigma_Y^{\\top} e_1 e_1^{\\top} V^{\\top}) = \\alpha \\operatorname{trace}(\\Sigma_Y^{\\top} e_1 e_1^{\\top}) = \\alpha \\sigma_1(Y)\n$$\nThe expression for the supremum becomes:\n$$\n\\operatorname{trace}(Y^{\\top} X) - \\|X\\|_{*} = \\alpha \\sigma_1(Y) - \\alpha = \\alpha (\\sigma_1(Y) - 1)\n$$\nSince $\\sigma_1(Y) - 1  0$, we can make this quantity arbitrarily large by letting $\\alpha \\to \\infty$. Therefore, if $\\|Y\\|_{op}  1$, the supremum is $+\\infty$.\n$$\nf^{*}(Y) = +\\infty \\quad \\text{if } \\|Y\\|_{op}  1\n$$\n\nCase 2: $\\|Y\\|_{op} \\leq 1$.\nThis implies that $\\sigma_i(Y) \\leq 1$ for all $i$. Consequently, $\\sigma_i(Y) - 1 \\leq 0$ for all $i$. Since singular values of any matrix are non-negative, $\\sigma_i(X) \\geq 0$, the product is non-positive:\n$$\n(\\sigma_i(Y) - 1) \\sigma_i(X) \\leq 0\n$$\nSumming over all $i$, we get $\\sum_i (\\sigma_i(Y) - 1) \\sigma_i(X) \\leq 0$. This provides an upper bound on our expression:\n$$\n\\operatorname{trace}(Y^{\\top} X) - \\|X\\|_{*} \\leq \\sum_{i} (\\sigma_i(Y) - 1) \\sigma_i(X) \\leq 0\n$$\nThe supremum is therefore at most $0$. To show that it is exactly $0$, we can choose $X=0$. For $X=0$, we have $\\|X\\|_{*} = 0$ and $\\operatorname{trace}(Y^{\\top} 0) = 0$, so $\\operatorname{trace}(Y^{\\top} X) - \\|X\\|_{*} = 0$.\nThus, the supremum is attained and is equal to $0$.\n$$\nf^{*}(Y) = 0 \\quad \\text{if } \\|Y\\|_{op} \\leq 1\n$$\n\nCombining both cases, the convex conjugate of the nuclear norm is the indicator function of the unit ball defined by the operator norm:\n$$\nf^{*}(Y) = \\iota_{\\{\\ Z \\in \\mathbb{R}^{m \\times n} \\,:\\, \\|Z\\|_{op} \\leq 1 \\ \\}}(Y) = \\begin{cases} 0  \\text{if } \\|Y\\|_{op} \\leq 1 \\\\ +\\infty  \\text{otherwise} \\end{cases}\n$$\n\n### Part 2: Fenchel Dual of the Matrix Compressed Sensing Problem\n\nThe primal problem is given by:\n$$\np^{*} = \\inf_{X \\in \\mathbb{R}^{m \\times n}} \\|X\\|_{*} \\quad \\text{subject to} \\quad \\mathcal{A}(X) = b\n$$\nWe can rewrite this problem in the standard form for Fenchel duality, $\\inf_X \\{ F(X) + G(\\mathcal{A}(X)) \\}$, by defining:\n1. $F(X) = \\|X\\|_{*}$\n2. $G(z) = \\iota_{\\{b\\}}(z)$, which is the indicator function for the singleton set $\\{b\\} \\subset \\mathbb{R}^p$. $G(z) = 0$ if $z=b$ and $G(z) = +\\infty$ if $z \\neq b$.\n\nThe primal problem is then equivalent to $\\inf_{X \\in \\mathbb{R}^{m \\times n}} \\{ F(X) + G(\\mathcal{A}(X)) \\}$.\nThe Fenchel dual problem is given by:\n$$\nd^{*} = \\sup_{y \\in \\mathbb{R}^{p}} \\{ -F^{*}(\\mathcal{A}^{*}(y)) - G^{*}(-y) \\}\n$$\nWe need to compute the convex conjugates $F^*$ and $G^*$.\nFrom Part 1, we have the conjugate of $F(X) = \\|X\\|_{*}$:\n$$\nF^{*}(Y) = \\iota_{\\{\\ Z \\,:\\, \\|Z\\|_{op} \\leq 1 \\ \\}}(Y)\n$$\nApplying this to the argument $\\mathcal{A}^{*}(y)$, we get:\n$$\nF^{*}(\\mathcal{A}^{*}(y)) = \\iota_{\\{\\ Z \\,:\\, \\|Z\\|_{op} \\leq 1 \\ \\}}(\\mathcal{A}^{*}(y))\n$$\nThis term is $0$ if $\\|\\mathcal{A}^{*}(y)\\|_{op} \\leq 1$ and $+\\infty$ otherwise.\n\nNext, we compute the conjugate of $G(z) = \\iota_{\\{b\\}}(z)$. The inner product in $\\mathbb{R}^p$ is the standard dot product $\\langle v, z \\rangle = v^{\\top}z$.\n$$\nG^{*}(v) = \\sup_{z \\in \\mathbb{R}^{p}} \\{ v^{\\top}z - G(z) \\} = \\sup_{z \\in \\mathbb{R}^{p}} \\{ v^{\\top}z - \\iota_{\\{b\\}}(z) \\}\n$$\nThe domain of $G(z)$ is the single point $z=b$, so the supremum is evaluated at this point:\n$$\nG^{*}(v) = v^{\\top}b\n$$\nWe need $G^{*}(-y)$ for the dual formulation:\n$$\nG^{*}(-y) = (-y)^{\\top}b = -y^{\\top}b = -b^{\\top}y\n$$\nSubstituting $F^{*}$ and $G^{*}$ into the dual problem expression:\n$$\nd^{*} = \\sup_{y \\in \\mathbb{R}^{p}} \\{ -F^{*}(\\mathcal{A}^{*}(y)) - (-b^{\\top}y) \\} = \\sup_{y \\in \\mathbb{R}^{p}} \\{ b^{\\top}y - \\iota_{\\{\\ Z \\,:\\, \\|Z\\|_{op} \\leq 1 \\ \\}}(\\mathcal{A}^{*}(y)) \\}\n$$\nThe term $\\iota_{\\{\\ Z \\,:\\, \\|Z\\|_{op} \\leq 1 \\ \\}}(\\mathcal{A}^{*}(y))$ is $0$ if $\\|\\mathcal{A}^{*}(y)\\|_{op} \\leq 1$ and $+\\infty$ otherwise. To maximize the expression, we must avoid the case where the indicator function is $+\\infty$, as this would make the objective $-\\infty$. Therefore, the maximization is implicitly restricted to the domain where the indicator function is finite (i.e., equals $0$). This gives us the constraint $\\|\\mathcal{A}^{*}(y)\\|_{op} \\leq 1$.\nWithin this feasible set, the problem simplifies to:\n$$\nd^{*} = \\sup_{y \\in \\mathbb{R}^{p}} \\{ b^{\\top}y - 0 \\} \\quad \\text{subject to} \\quad \\|\\mathcal{A}^{*}(y)\\|_{op} \\leq 1\n$$\nThus, the Fenchel dual problem is:\n$$\n\\max_{y \\in \\mathbb{R}^{p}} b^{\\top}y \\quad \\text{subject to} \\quad \\|\\mathcal{A}^{*}(y)\\|_{op} \\leq 1\n$$\n\nThe final answer requires both the expression for $f^*(Y)$ and the dual program. I will present them as a row matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\iota_{\\{ Y \\in \\mathbb{R}^{m \\times n} : \\|Y\\|_{op} \\leq 1 \\}}(Y)  \\max_{y \\in \\mathbb{R}^{p}} \\{ b^{\\top}y \\mid \\| \\mathcal{A}^{*}(y) \\|_{op} \\leq 1 \\}\n\\end{pmatrix}\n}\n$$", "id": "3439408"}, {"introduction": "Many optimization problems in data science originate from statistical principles, such as maximizing likelihood. This practice shifts our focus from simple norms to the negative log-likelihood of the Poisson distribution, a function central to modeling count data [@problem_id:3439398]. Deriving its conjugate will reveal how domain constraints in the primal, like the positivity of model parameters, are transformed into \"natural barriers\" in the dual space, offering a deeper understanding of the geometry of statistical models.", "problem": "Consider a Poisson compressed sensing model with sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ and nonnegative data $b \\in \\mathbb{R}_{+}^{m}$. Define the negative Poisson log-likelihood as a separable extended-real-valued convex function $f : \\mathbb{R}^{m} \\to \\mathbb{R} \\cup \\{+\\infty\\}$ by\n$$\nf(u) \\;=\\; \\sum_{i=1}^{m} \\Big( u_i \\;-\\; b_i \\ln u_i \\Big),\n$$\nwith the convention that $f(u) = +\\infty$ whenever any coordinate $u_i \\le 0$. Let $\\iota_{C}(\\cdot)$ denote the indicator function of a set $C$, defined by $\\iota_{C}(z) = 0$ if $z \\in C$ and $\\iota_{C}(z) = +\\infty$ if $z \\notin C$.\n\nYou are to use the fundamental definition of the convex conjugate and Fenchel–Rockafellar duality to analyze the dual structure induced by $f$. In particular:\n\n- Starting from the definition of the convex conjugate,\n$$\nf^{*}(y) \\;=\\; \\sup_{u \\in \\mathbb{R}^{m}} \\big\\{ \\langle y, u \\rangle \\;-\\; f(u) \\big\\},\n$$\nderive a closed-form expression for $f^{*}(y)$, including its effective domain. Your derivation must proceed from first principles (definition of convex conjugate, properties of separable sums, and properties of pointwise supremum), without invoking any pre-memorized transform tables.\n\n- Explain how the effective domain encodes a componentwise constraint of the form $y \\prec \\mathbf{1}$ (that is, $y_i  1$ for each index $i$), and discuss how this domain arises as a natural barrier in the dual.\n\n- Briefly connect your expression for $f^{*}$ to an equivalent representation obtained by the change of variables $s_i = \\ln(1 - y_i)$ inside the Fenchel biconjugate formula for $f$, thereby revealing how terms of the form $u_i \\exp(s_i)$ and $b_i s_i$ appear in the intermediate optimization, and how this relates to exponential structures that often arise in Poisson models.\n\nAnswer specification:\n- Report only the final closed-form expression for $f^{*}(y)$, using elementary functions and, if needed, the indicator function $\\iota_{C}(\\cdot)$ to encode the domain. \n- Your final answer must be a single exact analytic expression. Do not write an inequality or an equation as your final answer.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and contains sufficient information for a rigorous solution. It is a standard exercise in convex analysis, directly relevant to the study of dual methods for optimization problems in statistical signal processing. Therefore, the problem is deemed valid and a full solution follows.\n\nThe primary task is to derive the convex conjugate $f^{*}(y)$ of the function $f(u)$ defined as:\n$$\nf(u) \\;=\\; \\sum_{i=1}^{m} \\Big( u_i \\;-\\; b_i \\ln u_i \\Big)\n$$\nThe effective domain of $f$ is $\\text{dom}(f) = \\{ u \\in \\mathbb{R}^{m} \\mid u_i  0, \\forall i \\}$, as $f(u) = +\\infty$ if any component $u_i \\le 0$. The data vector $b$ has components $b_i \\ge 0$.\n\nThe convex conjugate $f^{*}(y)$ is defined by the supremum:\n$$\nf^{*}(y) \\;=\\; \\sup_{u \\in \\mathbb{R}^{m}} \\big\\{ \\langle y, u \\rangle \\;-\\; f(u) \\big\\}\n$$\nSubstituting the expression for $f(u)$ and the inner product $\\langle y, u \\rangle = \\sum_{i=1}^{m} y_i u_i$:\n$$\nf^{*}(y) \\;=\\; \\sup_{u \\in \\text{dom}(f)} \\left\\{ \\sum_{i=1}^{m} y_i u_i \\;-\\; \\sum_{i=1}^{m} (u_i - b_i \\ln u_i) \\right\\}\n$$\nThe objective function inside the supremum is separable with respect to the components $u_i$. This allows us to interchange the supremum and the summation:\n$$\nf^{*}(y) \\;=\\; \\sum_{i=1}^{m} \\sup_{u_i  0} \\left\\{ y_i u_i - (u_i - b_i \\ln u_i) \\right\\} \\;=\\; \\sum_{i=1}^{m} \\sup_{u_i  0} \\left\\{ (y_i - 1) u_i + b_i \\ln u_i \\right\\}\n$$\nLet $f_i^*(y_i)$ denote the $i$-th term in this sum. We analyze each $f_i^*(y_i)$ by finding the supremum of $g_i(u_i) = (y_i - 1) u_i + b_i \\ln u_i$ over the domain $u_i  0$. We consider two cases based on the value of $b_i$.\n\nCase 1: $b_i  0$.\nTo find the supremum, we use first-order optimality conditions. The derivative of $g_i(u_i)$ with respect to $u_i$ is:\n$$\n\\frac{d g_i}{d u_i} \\;=\\; y_i - 1 + \\frac{b_i}{u_i}\n$$\nSetting the derivative to zero to find critical points:\n$$\ny_i - 1 + \\frac{b_i}{u_i} \\;=\\; 0 \\quad \\implies \\quad \\frac{b_i}{u_i} \\;=\\; 1 - y_i\n$$\nIf $1 - y_i \\le 0$ (i.e., $y_i \\ge 1$), the derivative $\\frac{d g_i}{d u_i}$ is non-negative for all $u_i  0$ (it is strictly positive if $y_i  1$ or if $y_i=1, b_i0$). This means $g_i(u_i)$ is monotonically increasing, so its supremum over $u_i  0$ is unbounded: $\\sup_{u_i0} g_i(u_i) = +\\infty$.\n\nIf $1 - y_i  0$ (i.e., $y_i  1$), there is a unique critical point at:\n$$\nu_i^* \\;=\\; \\frac{b_i}{1 - y_i}\n$$\nSince $b_i  0$ and $1 - y_i  0$, we have $u_i^*  0$, so this critical point is in the domain. The second derivative is $\\frac{d^2 g_i}{d u_i^2} = -\\frac{b_i}{u_i^2}$, which is strictly negative since $b_i  0$. Therefore, $g_i(u_i)$ is a strictly concave function, and $u_i^*$ is its unique global maximizer. The value of the supremum is $g_i(u_i^*)$:\n\\begin{align*}\nf_i^*(y_i) = (y_i - 1) u_i^* + b_i \\ln(u_i^*) \\\\\n= (y_i - 1) \\left(\\frac{b_i}{1 - y_i}\\right) + b_i \\ln\\left(\\frac{b_i}{1 - y_i}\\right) \\\\\n= -b_i + b_i (\\ln(b_i) - \\ln(1 - y_i)) \\\\\n= b_i \\ln(b_i) - b_i - b_i \\ln(1-y_i)\n\\end{align*}\nHere, we assume the standard convention $0 \\ln 0 = 0$ if any $b_i$ happens to be infinitesimally small.\n\nCase 2: $b_i = 0$.\nIn this case, the function to maximize is simply $g_i(u_i) = (y_i - 1) u_i$. The supremum is taken over $u_i  0$.\nIf $y_i - 1  0$ (i.e., $y_i  1$), $g_i(u_i)$ is a strictly increasing linear function, so $\\sup_{u_i0} g_i(u_i) = +\\infty$.\nIf $y_i - 1 \\le 0$ (i.e., $y_i \\le 1$), $g_i(u_i)$ is non-positive and decreasing (or constant if $y_i=1$). The supremum is achieved as $u_i \\to 0^+$, yielding $\\sup_{u_i0} g_i(u_i) = 0$.\nThis can be compactly written using the indicator function for the set $(-\\infty, 1]$: $f_i^*(y_i) = \\iota_{(-\\infty, 1]}(y_i)$.\n\nCombining these results, the total conjugate function $f^*(y) = \\sum_{i=1}^m f_i^*(y_i)$ is finite only if each term $f_i^*(y_i)$ is finite. This defines the effective domain of $f^*$:\n$\\text{dom}(f^*) = \\{ y \\in \\mathbb{R}^m \\mid (b_i  0 \\implies y_i  1) \\text{ and } (b_i = 0 \\implies y_i \\le 1) \\}$.\n\nThe expression for $f^*(y)$ is the sum of the finite values over this domain:\n$$\nf^{*}(y) = \\sum_{i: b_i  0} \\left( b_i \\ln(b_i) - b_i - b_i \\ln(1-y_i) \\right)\n$$\nTo make this a single expression valid for all $y \\in \\mathbb{R}^m$, we incorporate the domain constraints. The logarithmic terms naturally constrain $y_i  1$ for indices where $b_i  0$ (as $\\ln(z)$ is defined for $z0$). For indices where $b_i=0$, we must add the corresponding indicator function. Thus, a complete expression is:\n$$\nf^{*}(y) = \\sum_{i: b_i  0} \\left( b_i \\ln(b_i) - b_i - b_i \\ln(1-y_i) \\right) + \\sum_{i: b_i = 0} \\iota_{(-\\infty, 1]}(y_i)\n$$\n\nThe effective domain $\\text{dom}(f^*)$ encodes a componentwise constraint on $y$. In the common practical case where all observations $b_i  0$, the domain simplifies to $\\{ y \\in \\mathbb{R}^m \\mid y_i  1, \\forall i \\}$, which is precisely $y \\prec \\mathbf{1}$. This arises as a \"natural barrier\" because the supremum in the definition of the conjugate becomes infinite if any $y_i$ reaches or exceeds $1$. This is a manifestation of duality: the barrier at $u_i=0$ in the primal function $f(u)$ (from $\\ln u_i$) induces a mirror barrier at $y_i=1$ in the dual function $f^*(y)$ (appearing as $-\\ln(1-y_i)$).\n\nTo connect this to the biconjugate, we use the fact that $f^{**}=f$ for a closed proper convex function $f$.\n$f(u) = f^{**}(u) = \\sup_{y} \\{ \\langle u, y \\rangle - f^*(y) \\}$.\nUsing the change of variables $s_i = \\ln(1-y_i)$, which implies $y_i = 1 - e^{s_i}$, the optimization can be re-parameterized over $s \\in \\mathbb{R}^m$. Substituting $y_i$ and our expression for $f^*(y)$ (assuming $b_i  0$ for all $i$ for simplicity) into the biconjugate formula:\n\\begin{align*}\nf(u) = \\sup_{s \\in \\mathbb{R}^m} \\left\\{ \\sum_i u_i (1-e^{s_i}) - \\sum_i (b_i \\ln b_i - b_i - b_i s_i) \\right\\} \\\\\n= \\sum_i (u_i - b_i \\ln b_i + b_i) + \\sup_{s \\in \\mathbb{R}^m} \\left\\{ \\sum_i (b_i s_i - u_i e^{s_i}) \\right\\}\n\\end{align*}\nAs requested, we see the terms $b_i s_i$ and $u_i \\exp(s_i)$ emerging in the intermediate optimization over the transformed dual variable $s$. Such exponential structures are characteristic of the exponential family of distributions, to which the Poisson distribution belongs, and this duality illustrates a deep connection between the log-likelihood and its dual representation. Maximizing the inner separable sum term-by-term recovers the original function $f(u) = \\sum_i (u_i - b_i \\ln u_i)$, confirming the correctness of the derived conjugate $f^*(y)$.", "answer": "$$\\boxed{\\sum_{i: b_i  0} \\left( b_i \\ln(b_i) - b_i - b_i \\ln(1-y_i) \\right) + \\sum_{i: b_i = 0} \\iota_{(-\\infty, 1]}(y_i)}$$", "id": "3439398"}, {"introduction": "Fenchel duality is not just a theoretical tool for certifying optimality; it provides a powerful framework for designing more efficient algorithms. This advanced practice tackles the Total Variation Lasso (TV-Lasso) model, a composite problem that is more complex than standard Lasso [@problem_id:3439416]. After deriving the Fenchel dual, you will use the properties of the dual problem to construct a \"safe screening\" region, a technique that can significantly accelerate optimization by provably identifying parts of the solution that will be inactive.", "problem": "Consider the Total Variation Lasso (TV-Lasso) analysis-regularized least-squares problem in sparse optimization, defined for a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$, a difference (analysis) operator $D \\in \\mathbb{R}^{p \\times n}$, a data vector $b \\in \\mathbb{R}^{m}$, and a regularization parameter $\\lambda  0$:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|D x\\|_{1}.\n$$\nLet $f(y) := \\frac{1}{2} \\|y - b\\|_{2}^{2}$ for $y \\in \\mathbb{R}^{m}$ and $h(w) := \\lambda \\|w\\|_{1}$ for $w \\in \\mathbb{R}^{p}$. Using only the fundamental definitions of convex conjugates and Fenchel duality, perform the following steps:\n\n1. Compute the convex conjugates $f^{*}$ and $h^{*}$ from first principles (i.e., from the definition of the convex conjugate), and use them to derive the Fenchel dual of the TV-Lasso in the form of a maximization in dual variables $u \\in \\mathbb{R}^{m}$ and $z \\in \\mathbb{R}^{p}$ with an explicit linear coupling constraint. Clearly state the dual feasible set in terms of an inequality constraint on $z$ and a linear relation between $u$ and $z$. Interpret $z$ as a dual flux associated with the analysis operator.\n\n2. Let $x_{t} \\in \\mathbb{R}^{n}$ be any primal candidate and let $(u_{t}, z_{t})$ be any dual feasible pair. Define the primal-dual gap \n$$\nG_{t} := f(A x_{t}) + h(D x_{t}) + f^{*}(u_{t}) + h^{*}(z_{t}),\n$$\nand recall that $h^{*}$ is the indicator function of the dual norm ball. Using only the strong convexity of $f^{*}$ and elementary inequalities, derive a Euclidean ball in the dual variable $u$ which is guaranteed to contain the dual optimizer $u^{\\star}$. Your derivation must start from the definition of strong convexity and the Fenchel-Young inequality, and proceed without assuming any ready-made safe rule formulas.\n\n3. Express the radius $\\rho_{t}$ of the smallest such globally valid Euclidean ball centered at $u_{t}$ that you can certify solely from $G_{t}$ and the properties of $f^{*}$, in fully simplified closed form. Your final answer must be a single analytical expression for $\\rho_{t}$.\n\nYour final answer must be a single closed-form expression. No numerical evaluation is required and no units are involved. Do not provide inequalities or sets as the final answer; provide only the radius in closed form.", "solution": "The problem asks for a three-part derivation concerning the Fenchel dual of the Total Variation Lasso (TV-Lasso) problem, culminating in an expression for the radius of a ball containing the dual optimizer.\n\nThe primal problem is given by:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|D x\\|_{1}\n$$\nWe define $f(y) := \\frac{1}{2} \\|y - b\\|_{2}^{2}$ and $h(w) := \\lambda \\|w\\|_{1}$. The primal problem can be written as $\\min_{x \\in \\mathbb{R}^{n}} f(Ax) + h(Dx)$.\n\nPart 1: Derivation of the Fenchel Dual\n\nFirst, we compute the convex conjugate $f^*(u)$ of $f(y)$. By definition,\n$$\nf^*(u) = \\sup_{y \\in \\mathbb{R}^{m}} \\{ \\langle u, y \\rangle - f(y) \\} = \\sup_{y \\in \\mathbb{R}^{m}} \\left\\{ \\langle u, y \\rangle - \\frac{1}{2} \\|y - b\\|_{2}^{2} \\right\\}\n$$\nThe expression inside the supremum is a strictly concave quadratic function of $y$. Its unique maximum is found by setting the gradient with respect to $y$ to zero:\n$$\n\\nabla_y \\left( \\langle u, y \\rangle - \\frac{1}{2} (y - b)^T(y - b) \\right) = u - (y - b) = 0\n$$\nThis gives the maximizing value $y_{\\text{opt}} = u + b$. Substituting this back into the expression for $f^*(u)$:\n$$\nf^*(u) = \\langle u, u + b \\rangle - \\frac{1}{2} \\|(u + b) - b\\|_{2}^{2} = \\|u\\|_{2}^{2} + \\langle u, b \\rangle - \\frac{1}{2} \\|u\\|_{2}^{2} = \\frac{1}{2} \\|u\\|_{2}^{2} + \\langle u, b \\rangle\n$$\nNext, we compute the convex conjugate $h^*(z)$ of $h(w)$. By definition,\n$$\nh^*(z) = \\sup_{w \\in \\mathbb{R}^{p}} \\{ \\langle z, w \\rangle - h(w) \\} = \\sup_{w \\in \\mathbb{R}^{p}} \\{ \\langle z, w \\rangle - \\lambda \\|w\\|_{1} \\}\n$$\nWe analyze this in two cases based on the dual norm of the $\\ell_1$-norm, which is the $\\ell_\\infty$-norm, $\\|z\\|_{\\infty} = \\max_i |z_i|$.\nCase 1: $\\|z\\|_{\\infty}  \\lambda$. There exists an index $k$ such that $|z_k|  \\lambda$. Let us choose $w = c \\cdot \\text{sgn}(z_k) e_k$, where $e_k$ is the $k$-th standard basis vector and $c  0$. Then $\\langle z, w \\rangle = c |z_k|$ and $\\|w\\|_1 = c$. The expression becomes $c |z_k| - \\lambda c = c(|z_k| - \\lambda)$. As $c \\to \\infty$, this expression goes to $\\infty$. Thus, $h^*(z) = \\infty$.\nCase 2: $\\|z\\|_{\\infty} \\le \\lambda$. From the Hölder inequality, we have $\\langle z, w \\rangle \\le \\|z\\|_{\\infty} \\|w\\|_{1}$. Therefore, $\\langle z, w \\rangle - \\lambda \\|w\\|_{1} \\le \\|z\\|_{\\infty} \\|w\\|_{1} - \\lambda \\|w\\|_{1} = (\\|z\\|_{\\infty} - \\lambda) \\|w\\|_{1}$. Since $\\|z\\|_{\\infty} \\le \\lambda$, this quantity is non-positive. The supremum value is $0$, which is achieved at $w=0$.\nCombining these cases, $h^*(z)$ is the indicator function of the set $\\{z \\in \\mathbb{R}^p \\mid \\|z\\|_{\\infty} \\le \\lambda\\}$:\n$$\nh^*(z) = I_{\\|z\\|_{\\infty} \\le \\lambda}(z) = \\begin{cases} 0  \\text{if } \\|z\\|_{\\infty} \\le \\lambda \\\\ \\infty  \\text{otherwise} \\end{cases}\n$$\nTo find the Fenchel dual, we introduce auxiliary variables and form the Lagrangian. The problem is equivalent to $\\min_{x, y, w} \\{ f(y) + h(w) \\}$ subject to $y=Ax$ and $w=Dx$. The Lagrangian is:\n$$\n\\mathcal{L}(x, y, w, u, z) = f(y) + h(w) + \\langle u, Ax - y \\rangle + \\langle z, Dx - w \\rangle\n$$\n$$\n\\mathcal{L}(x, y, w, u, z) = \\langle x, A^T u + D^T z \\rangle + (f(y) - \\langle u, y \\rangle) + (h(w) - \\langle z, w \\rangle)\n$$\nThe dual function is $g(u,z) = \\inf_{x,y,w} \\mathcal{L}$. The infimum over $y$ is $-f^*(u)$ and over $w$ is $-h^*(z)$. The infimum over $x$ is $0$ if $A^T u + D^T z = 0$ and $-\\infty$ otherwise. Thus, for a finite dual objective, we must have the constraint $A^T u + D^T z = 0$.\nThe dual problem is to maximize the dual function:\n$$\n\\max_{u, z} \\left\\{ -f^*(u) - h^*(z) \\right\\} \\quad \\text{subject to} \\quad A^T u + D^T z = 0\n$$\nSubstituting the derived conjugates:\n$$\n\\max_{u \\in \\mathbb{R}^{m}, z \\in \\mathbb{R}^{p}} \\left\\{ -\\left(\\frac{1}{2}\\|u\\|_{2}^{2} + \\langle u, b \\rangle\\right) - I_{\\|z\\|_{\\infty} \\le \\lambda}(z) \\right\\} \\quad \\text{subject to} \\quad A^T u + D^T z = 0\n$$\nThe indicator function can be written as an explicit constraint. The dual problem is therefore:\n$$\n\\max_{u \\in \\mathbb{R}^{m}, z \\in \\mathbb{R}^{p}} \\quad -\\frac{1}{2}\\|u\\|_{2}^{2} - \\langle u, b \\rangle \\quad \\text{subject to} \\quad A^T u + D^T z = 0, \\quad \\|z\\|_{\\infty} \\le \\lambda\n$$\nThe dual feasible set is the set of all pairs $(u,z) \\in \\mathbb{R}^m \\times \\mathbb{R}^p$ satisfying the linear coupling constraint $A^T u + D^T z = 0$ and the inequality constraint $\\|z\\|_{\\infty} \\le \\lambda$. The variable $z$ is the dual variable associated with the analysis term $Dx$, and can be interpreted as a dual flux.\n\nPart 2: Derivation of the Containing Ball for the Dual Optimizer\n\nLet $P(x) = f(Ax) + h(Dx)$ be the primal objective. Let $\\mathcal{D}(u,z) = -f^*(u) - h^*(z)$ be the dual objective.\nLet $(x^\\star, u^\\star, z^\\star)$ be a primal-dual optimal pair. By strong duality, $P(x^\\star) = \\mathcal{D}(u^\\star,z^\\star)$.\nThe primal-dual gap at a primal candidate $x_t$ and a dual feasible pair $(u_t, z_t)$ is:\n$$\nG_t = P(x_t) - \\mathcal{D}(u_t,z_t) = f(Ax_t) + h(Dx_t) - (-f^*(u_t) - h^*(z_t)) = f(Ax_t) + h(Dx_t) + f^*(u_t) + h^*(z_t)\n$$\nThis matches the definition provided. Since $(u_t, z_t)$ is dual feasible, $h^*(z_t) = 0$, so $G_t = f(Ax_t) + h(Dx_t) + f^*(u_t)$.\nBy definition of optimality, $P(x_t) \\ge P(x^\\star)$. Thus, the gap $G_t$ satisfies:\n$$\nG_t = P(x_t) - \\mathcal{D}(u_t,z_t) \\ge P(x^\\star) - \\mathcal{D}(u_t,z_t)\n$$\nUsing strong duality $P(x^\\star) = \\mathcal{D}(u^\\star,z^\\star)$:\n$$\nG_t \\ge \\mathcal{D}(u^\\star,z^\\star) - \\mathcal{D}(u_t,z_t) = [-f^*(u^\\star) - h^*(z^\\star)] - [-f^*(u_t) - h^*(z_t)]\n$$\nSince both $(u_t, z_t)$ and $(u^\\star, z^\\star)$ are dual feasible, $h^*(z_t)=0$ and $h^*(z^\\star)=0$. The inequality simplifies to:\n$$\nG_t \\ge f^*(u_t) - f^*(u^\\star)\n$$\nNow, we use the properties of $f^*(u) = \\frac{1}{2}\\|u\\|_2^2 + \\langle u, b \\rangle$. Its Hessian is $\\nabla^2 f^*(u) = I$, which is positive definite. Thus, $f^*$ is $1$-strongly convex. For a differentiable, $1$-strongly convex function, we have for any $u_t, u^\\star$:\n$$\nf^*(u_t) \\ge f^*(u^\\star) + \\langle \\nabla f^*(u^\\star), u_t - u^\\star \\rangle + \\frac{1}{2}\\|u_t - u^\\star\\|_2^2\n$$\nThe dual problem can be viewed as minimizing $f^*(u)$ over the convex set $\\mathcal{U} = \\{ u \\mid \\exists z \\text{ with } \\|z\\|_\\infty \\le \\lambda \\text{ and } A^T u + D^T z = 0 \\}$. The first-order optimality condition for the minimizer $u^\\star$ over the convex set $\\mathcal{U}$ is that for any other point $u_t \\in \\mathcal{U}$:\n$$\n\\langle \\nabla f^*(u^\\star), u_t - u^\\star \\rangle \\ge 0\n$$\nSince the gradient term in the strong convexity inequality is non-negative, we can remove it to obtain a weaker inequality:\n$$\nf^*(u_t) \\ge f^*(u^\\star) + \\frac{1}{2}\\|u_t - u^\\star\\|_2^2\n$$\nRearranging this gives:\n$$\nf^*(u_t) - f^*(u^\\star) \\ge \\frac{1}{2}\\|u_t - u^\\star\\|_2^2\n$$\nCombining our two main inequalities, we get:\n$$\nG_t \\ge f^*(u_t) - f^*(u^\\star) \\ge \\frac{1}{2}\\|u_t - u^\\star\\|_2^2\n$$\nThis yields the final inequality:\n$$\n\\|u_t - u^\\star\\|_2^2 \\le 2 G_t \\quad \\implies \\quad \\|u_t - u^\\star\\|_2 \\le \\sqrt{2 G_t}\n$$\nThis result shows that any dual optimizer $u^\\star$ must lie within the Euclidean ball centered at the current dual iterate $u_t$ with radius $\\sqrt{2 G_t}$.\n\nPart 3: Radius of the Certified Ball\n\nFrom the derivation in Part 2, the radius $\\rho_t$ of the smallest Euclidean ball centered at $u_t$ that is guaranteed to contain the dual optimizer $u^\\star$, based solely on the gap $G_t$ and the properties of $f^*$, is given by:\n$$\n\\rho_t = \\sqrt{2 G_t}\n$$\nThis expression provides a computable, globally valid bound on the location of the dual solution.", "answer": "$$\n\\boxed{\\sqrt{2G_t}}\n$$", "id": "3439416"}]}