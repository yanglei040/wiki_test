{"hands_on_practices": [{"introduction": "Before analyzing the complex dynamics of OMP, it is crucial to have a firm grasp of the key quantities that govern its behavior. This practice focuses on the foundational concepts of mutual coherence $\\mu(A)$ and cumulative coherence $\\mu_{1}(s)$, which are central to the theoretical guarantees of sparse recovery. By deriving these quantities for a hypothetical equiangular dictionary [@problem_id:3441533], where atom correlations are constant, you will develop a concrete intuition for how these metrics are defined and how they relate to a dictionary's geometric structure.", "problem": "Consider a dictionary matrix $A \\in \\mathbb{R}^{m \\times n}$ with columns $\\{a_{1}, \\dots, a_{n}\\}$, where each column is unit-norm, i.e., $\\|a_{j}\\|_{2} = 1$ for all $j \\in \\{1, \\dots, n\\}$. Assume the dictionary is equiangular in the sense that for all distinct indices $i \\neq j$ the absolute inner product satisfies $|\\langle a_{i}, a_{j} \\rangle| = c$ for some fixed constant $c \\in [0,1)$. In the context of compressed sensing, the mutual coherence $\\mu(A)$ of $A$ is defined as the maximum absolute inner product between distinct columns, and the cumulative coherence (also called the Babel function) $\\mu_{1}(s)$ is defined as the maximum, over all sets of $s$ indices and over all column indices not in that set, of the sum of absolute inner products between that column and the $s$ columns indexed by the set. \n\nStarting from the fundamental definitions of mutual coherence and cumulative coherence, carry out the full derivation for the equiangular dictionary described above to compute $\\mu(A)$ and $\\mu_{1}(s)$ explicitly in terms of $c$ and $s$, and verify the identity $\\mu_{1}(s) = s c$. Provide the final answer as analytic expressions. No rounding is required and no units are involved. Your final answer must list the pair $\\mu(A)$ and $\\mu_{1}(s)$.", "solution": "The problem statement will be validated first by a meticulous examination of its premises and definitions, as per the required protocol.\n\n**Step 1: Extract Givens**\nThe problem provides the following data and definitions:\n- A dictionary matrix $A \\in \\mathbb{R}^{m \\times n}$ with columns $\\{a_{1}, \\dots, a_{n}\\}$.\n- The columns are unit-norm: $\\|a_{j}\\|_{2} = 1$ for all $j \\in \\{1, \\dots, n\\}$.\n- The dictionary is equiangular: for all distinct indices $i \\neq j$, the absolute inner product is $|\\langle a_{i}, a_{j} \\rangle| = c$, where $c$ is a fixed constant satisfying $c \\in [0,1)$.\n- Definition of mutual coherence: $\\mu(A) = \\max_{i \\neq j} |\\langle a_{i}, a_{j} \\rangle|$.\n- Definition of cumulative coherence (Babel function): $\\mu_{1}(s) = \\max_{|S|=s} \\max_{j \\notin S} \\sum_{i \\in S} |\\langle a_{j}, a_{i} \\rangle|$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is evaluated against the required criteria:\n- **Scientifically Grounded:** The problem is firmly rooted in the mathematical theory of compressed sensing and sparse representation. The concepts of a dictionary, mutual coherence, cumulative coherence, and equiangularity are standard and well-defined in this field. There are no violations of scientific or mathematical principles.\n- **Well-Posed:** The problem is well-posed. It supplies all necessary definitions and conditions to uniquely determine the quantities $\\mu(A)$ and $\\mu_{1}(s)$. The task is to perform a direct calculation based on the given definitions and the specific structure of the dictionary.\n- **Objective:** The language is formal, precise, and objective, using standard mathematical terminology. There are no subjective or ambiguous statements.\n\nThe problem is found to be free of all specified flaws (Scientific Unsoundness, Non-Formalizable, Incomplete Setup, Unrealistic, Ill-Posed, Pseudo-Profound, Outside Verifiability). The assumption of an equiangular dictionary's existence is a standard device for theoretical analysis in this domain.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A complete, reasoned solution will now be provided.\n\n**Derivation of Mutual Coherence $\\mu(A)$**\n\nThe mutual coherence of a dictionary matrix $A$ is defined as the maximum absolute inner product between any two distinct columns of the matrix. The formal definition is:\n$$\n\\mu(A) = \\max_{1 \\le i  j \\le n} |\\langle a_i, a_j \\rangle|\n$$\nThe problem states that the dictionary is equiangular. This is a specific structural property defined by the condition that for any pair of distinct column indices $i \\neq j$, the absolute value of their inner product is a constant, $c$. That is:\n$$\n|\\langle a_i, a_j \\rangle| = c \\quad \\text{for all } i \\neq j\n$$\nSubstituting this condition directly into the definition of mutual coherence, we observe that the set of values $\\{|\\langle a_i, a_j \\rangle| : i \\neq j\\}$ over which the maximum is taken contains only one element, the constant $c$. Therefore, the maximum of this set is trivially $c$.\n$$\n\\mu(A) = \\max_{i \\neq j} \\{c\\} = c\n$$\nThus, for an equiangular dictionary, the mutual coherence is precisely the constant value of the pairwise absolute inner products.\n\n**Derivation of Cumulative Coherence $\\mu_{1}(s)$**\n\nThe cumulative coherence, or Babel function, $\\mu_{1}(s)$, measures the maximum cumulative correlation between a single column and a set of $s$ other columns. The formal definition is:\n$$\n\\mu_{1}(s) = \\max_{S \\subset \\{1, \\dots, n\\}, |S|=s} \\left( \\max_{j \\notin S} \\sum_{i \\in S} |\\langle a_j, a_i \\rangle| \\right)\n$$\nTo compute this for the given equiangular dictionary, let us first analyze the innermost sum, $\\sum_{i \\in S} |\\langle a_j, a_i \\rangle|$, for an arbitrary set of indices $S$ with cardinality $|S|=s$ and an arbitrary index $j \\notin S$.\n\nAccording to the equiangular property of the dictionary, for any index $i \\in S$ and any index $j \\notin S$, the indices $i$ and $j$ are necessarily distinct. The problem statement guarantees that for any distinct indices, $|\\langle a_j, a_i \\rangle| = c$.\nTherefore, every term in the summation is equal to the constant $c$:\n$$\n|\\langle a_j, a_i \\rangle| = c \\quad \\text{for all } i \\in S \\text{ and } j \\notin S\n$$\nThe sum can now be rewritten as:\n$$\n\\sum_{i \\in S} |\\langle a_j, a_i \\rangle| = \\sum_{i \\in S} c\n$$\nSince the set $S$ contains exactly $s$ elements (i.e., $|S|=s$), the sum consists of $s$ identical terms, each equal to $c$. The value of the sum is therefore:\n$$\n\\sum_{i \\in S} c = s \\cdot c\n$$\nCrucially, this result, $sc$, is independent of the specific choice of the index set $S$ (as long as its cardinality is $s$) and the specific choice of the index $j$ (as long as $j$ is not in $S$).\n\nNow, we must apply the maximization operators from the definition of $\\mu_1(s)$:\n$$\n\\mu_{1}(s) = \\max_{|S|=s} \\left( \\max_{j \\notin S} (sc) \\right)\n$$\nSince the expression to be maximized, $sc$, is a constant value for any valid choice of $S$ and $j$, the maximization operation is trivial. The maximum of a constant value is the constant itself.\n$$\n\\max_{j \\notin S} (sc) = sc\n$$\nAnd subsequently,\n$$\n\\max_{|S|=s} (sc) = sc\n$$\nThis completes the derivation and verifies the identity posed by the problem statement. For an equiangular dictionary with parameter $c$, the cumulative coherence function is a linear function of $s$:\n$$\n\\mu_{1}(s) = sc\n$$\n\nThe final answer requires the explicit expressions for both $\\mu(A)$ and $\\mu_{1}(s)$. Based on the derivations above, these are $\\mu(A) = c$ and $\\mu_1(s) = sc$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nc  sc\n\\end{pmatrix}\n}\n$$", "id": "3441533"}, {"introduction": "The standard OMP selection rule, which maximizes the inner product $|\\langle a_j, r \\rangle|$, is implicitly biased if dictionary columns do not have unit norm, as high-norm atoms can produce large correlations even if they are poorly aligned with the residual. This exercise addresses this critical practical issue by asking you to implement and test a family of column normalization strategies. Through this hands-on coding problem [@problem_id:3441539], you will investigate how uneven scaling can mislead the greedy selection and discover how appropriate normalization is essential for restoring algorithmic stability and correctness.", "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ be a dictionary with columns $a_j \\in \\mathbb{R}^m$ that are not necessarily normalized. Consider the Orthogonal Matching Pursuit (OMP) algorithm applied to measurements $y = A x^\\star + e$, where $x^\\star \\in \\mathbb{R}^n$ is $k$-sparse, $e \\in \\mathbb{R}^m$ is noise, and $k \\in \\mathbb{N}$ is known. The OMP selection step at iteration $t$ chooses the index $j$ that maximizes the absolute inner product $|\\langle a_j, r^{(t)} \\rangle|$, where $r^{(t)}$ is the current residual. This selection rule is sensitive to the column norms $\\|a_j\\|_2$, because $|\\langle a_j, r^{(t)} \\rangle|$ scales with $\\|a_j\\|_2$. Define the following coherence notions:\n- The standard mutual coherence (computed after column normalization) is\n$$\n\\mu(A) = \\max_{i \\neq j} \\left| \\frac{a_i^\\top a_j}{\\|a_i\\|_2 \\, \\|a_j\\|_2} \\right|.\n$$\n- The unnormalized coherence is\n$$\n\\tilde{\\mu}(A) = \\max_{i \\neq j} \\left| a_i^\\top a_j \\right|.\n$$\nTo mitigate the effect of uneven column norms, consider a power-normalization family of diagonal scalings parameterized by $\\alpha \\in [0,1]$: form the scaled dictionary $B_\\alpha$ with columns $b_j = a_j / \\|a_j\\|_2^{\\alpha}$. Note that $\\alpha = 0$ leaves $A$ unchanged, $\\alpha = 1$ normalizes all columns to unit norm, and intermediate $\\alpha$ partially attenuate column-norm disparities.\n\nYour task is to design and implement a program that, for a fixed test suite of problem instances, quantifies how column normalization alters OMP correctness and stability by:\n1. Computing $\\mu(A)$ and $\\tilde{\\mu}(A)$.\n2. Running OMP on $B_\\alpha$ for the choices $\\alpha \\in \\{0, 1\\}$ to obtain two correctness indicators (exact support recovery).\n3. Searching over $\\alpha \\in \\{0, 0.5, 1\\}$ for the smallest exponent $\\alpha^\\star$ that restores exact support recovery.\n\nUse the following precise definitions and assumptions:\n- OMP iterates $k$ steps, where $k$ is the true sparsity level; at each step it selects one new column that maximizes $|\\langle b_j, r^{(t)} \\rangle|$ among indices not yet selected, breaking ties by choosing the smallest index. After each selection, it recomputes the least-squares fit on the selected support and updates the residual.\n- Exact support recovery means that the set of indices selected by OMP after $k$ iterations equals the support of $x^\\star$.\n- For each test case, compute $\\mu(A)$ on the column-normalized version of $A$ (i.e., after scaling each column to unit norm), and compute $\\tilde{\\mu}(A)$ on the original, unscaled $A$.\n- The noise $e$ is arbitrary but fixed per test case; there are no physical units in this problem.\n\nTest suite:\n- Test case T1 (happy path with strong norm imbalance that misleads unnormalized OMP):\n  - $m = 3$, $n = 5$.\n  - Columns:\n    - $a_1 = (1, 0, 0)^\\top$,\n    - $a_2 = (0, 1, 0)^\\top$,\n    - $a_3 = (0, 0, 1)^\\top$,\n    - $a_4 = (1, 1, 0)^\\top$,\n    - $a_5 = 10 \\cdot (0.25, 0.25, \\sqrt{0.875})^\\top$.\n  - Signal: $x^\\star = (1, 1, 0, 0, 0)^\\top$, sparsity $k = 2$.\n  - Noise: $e = (0, 0, 0)^\\top$.\n\n- Test case T2 (boundary case under raw scaling; small noise breaks a tie incorrectly unless scaled):\n  - $m = 3$, $n = 3$.\n  - Columns:\n    - $a_1 = (1, 0, 0)^\\top$,\n    - $a_2 = 5 \\cdot (0.2, 0, \\sqrt{0.96})^\\top$,\n    - $a_3 = (0, 1, 0)^\\top$.\n  - Signal: $x^\\star = (1, 0, 0)^\\top$, sparsity $k = 1$.\n  - Noise: $e = (0, 0, 0.02)^\\top$.\n\n- Test case T3 (moderate coherence and mild norm variation; both raw and normalized OMP are stable):\n  - $m = 4$, $n = 6$.\n  - Columns:\n    - $a_1 = (1, 0, 0, 0)^\\top$,\n    - $a_2 = (0, 1, 0, 0)^\\top$,\n    - $a_3 = (0, 0, 1, 0)^\\top$,\n    - $a_4 = (0, 0, 0, 1)^\\top$,\n    - $a_5 = 0.5 \\cdot (1, 1, 0, 0)^\\top$,\n    - $a_6 = 0.5 \\cdot (0, 1, 1, 0)^\\top$.\n  - Signal: $x^\\star = (1.2, 0, 0.8, 0, 0, 0)^\\top$, sparsity $k = 2$.\n  - Noise: $e = (0.01, -0.02, 0.015, 0)^\\top$.\n\nProgram requirements:\n- For each test case, the program must output a list $[\\mu(A), \\tilde{\\mu}(A), s_0, s_1, \\alpha^\\star]$, where $s_0$ is a boolean indicating exact support recovery by OMP with $\\alpha = 0$, $s_1$ is a boolean indicating exact support recovery by OMP with $\\alpha = 1$, and $\\alpha^\\star$ is the smallest value in $\\{0, 0.5, 1\\}$ for which OMP achieves exact support recovery; if none succeeds, output $-1$ for $\\alpha^\\star$. The values $\\mu(A)$ and $\\tilde{\\mu}(A)$ must be floating-point numbers.\n- Your program should produce a single line of output containing the results for T1, T2, and T3 as a comma-separated list of per-test-case lists enclosed in square brackets, for example: $[[\\dots],[\\dots],[\\dots]]$.", "solution": "The problem requires an analysis of the Orthogonal Matching Pursuit (OMP) algorithm's performance under different column normalization strategies for a given sensing matrix $A \\in \\mathbb{R}^{m \\times n}$. The performance is evaluated based on exact support recovery of a known $k$-sparse signal $x^\\star$ from measurements $y = A x^\\star + e$, where $e$ is noise.\n\nThe core of the task is to implement OMP and test it on a family of scaled dictionaries $B_\\alpha$, where the columns of $B_\\alpha$ are given by $b_j = a_j / \\|a_j\\|_2^{\\alpha}$ for $\\alpha \\in \\{0, 0.5, 1\\}$. We will also compute two coherence measures for the original dictionary $A$: the standard mutual coherence $\\mu(A)$ and the unnormalized coherence $\\tilde{\\mu}(A)$.\n\nThe solution proceeds as follows:\n$1$. For each test case, we first calculate the coherences $\\mu(A)$ and $\\tilde{\\mu}(A)$ as defined.\n$2$. We then implement the OMP algorithm.\n$3$. We run OMP for $k$ iterations for each specified value of $\\alpha \\in \\{0, 0.5, 1\\}$.\n$4$. For each run, we compare the OMP-selected support with the true support of $x^\\star$ to determine if recovery was exact.\n$5$. Finally, we collate the results into the specified output format.\n\n**1. Coherence Calculation**\n- The standard mutual coherence $\\mu(A)$ is defined as $\\mu(A) = \\max_{i \\neq j} \\left| \\frac{a_i^\\top a_j}{\\|a_i\\|_2 \\, \\|a_j\\|_2} \\right|$. This is equivalent to computing the largest absolute off-diagonal entry of the Gram matrix of the column-normalized version of $A$. Let $A'$ be the matrix with columns $a'_j = a_j / \\|a_j\\|_2$. Then $\\mu(A)$ is the maximum absolute value of the off-diagonal elements of $G' = (A')^\\top A'$.\n- The unnormalized coherence $\\tilde{\\mu}(A)$ is defined as $\\tilde{\\mu}(A) = \\max_{i \\neq j} | a_i^\\top a_j |$. This is the largest absolute off-diagonal entry of the Gram matrix $G = A^\\top A$.\n\n**2. Orthogonal Matching Pursuit (OMP) Implementation**\nThe OMP algorithm is implemented to recover the support of a $k$-sparse signal $x^\\star$.\n- **Input**: Dictionary $A \\in \\mathbb{R}^{m \\times n}$, measurement vector $y \\in \\mathbb{R}^m$, sparsity level $k$, and normalization exponent $\\alpha$.\n- **Initialization**:\n    - The residual is initialized to the measurement vector: $r^{(0)} = y$.\n    - The set of selected indices is empty: $S_0 = \\emptyset$.\n    - A scaled dictionary $B_\\alpha$ is formed, with columns $b_j = a_j / \\|a_j\\|_2^{\\alpha}$. For columns with norm $0$, we can consider the norm to be $1$ to avoid division by zero.\n- **Iteration**: For $t = 1, \\dots, k$:\n    - **Selection**: Find the index $j_t$ that is not yet in the support set $S_{t-1}$ and that maximizes the correlation with the current residual:\n    $$\n    j_t = \\arg\\max_{j \\notin S_{t-1}} |\\langle b_j, r^{(t-1)} \\rangle|\n    $$\n    In case of a tie, the smallest index $j$ is chosen.\n    - **Update Support**: The new support set is $S_t = S_{t-1} \\cup \\{j_t\\}$.\n    - **Update Residual**: A new signal estimate is found by solving a least-squares problem on the current support set using the original dictionary $A$:\n    $$\n    x_{S_t} = \\arg\\min_{z \\in \\mathbb{R}^{|S_t|}} \\|y - A_{S_t} z\\|_2^2 = (A_{S_t}^\\top A_{S_t})^{-1} A_{S_t}^\\top y\n    $$\n    where $A_{S_t}$ is the sub-matrix of $A$ containing columns indexed by $S_t$. A pseudoinverse is used for a robust implementation. The new residual is the projection error:\n    $$\n    r^{(t)} = y - A_{S_t} x_{S_t}\n    $$\n- **Output**: The final support set $S_k$.\n\n**3. Analysis of Test Cases**\n\n**Test Case T1:**\n- $A \\in \\mathbb{R}^{3 \\times 5}$, $k=2$, $x^\\star = (1, 1, 0, 0, 0)^\\top$, $e=(0,0,0)^\\top$.\n- True support $S^\\star=\\{0, 1\\}$.\n- $y = 1 \\cdot a_1 + 1 \\cdot a_2 = (1, 1, 0)^\\top$. We note that $y$ is also equal to $a_4$.\n- Column norms are $\\|a_1\\|_2=1, \\|a_2\\|_2=1, \\|a_3\\|_2=1, \\|a_4\\|_2=\\sqrt{2}, \\|a_5\\|_2=10$.\n- **Coherences**:\n    - $\\mu(A) = \\max_{i \\neq j} |(a_i/\\|a_i\\|_2)^\\top (a_j/\\|a_j\\|_2)| = |(a_3/\\|a_3\\|_2)^\\top(a_5/\\|a_5\\|_2)| = \\sqrt{0.875} \\approx 0.9354$.\n    - $\\tilde{\\mu}(A) = \\max_{i \\neq j} |a_i^\\top a_j| = |a_3^\\top a_5| = 10\\sqrt{0.875} \\approx 9.3541$.\n- **OMP runs**:\n    - $\\alpha=0$: The selection rule uses $|\\langle a_j, y \\rangle|$. The correlations are $\\{1, 1, 0, 2, 5\\}$. $a_5$ (index $4$) is selected. This is incorrect. $s_0 = \\text{False}$.\n    - $\\alpha=1$: The selection rule uses $|\\langle a_j/\\|a_j\\|_2, y \\rangle|$. The correlations are $\\{1, 1, 0, 2/\\sqrt{2}, 5/10\\} = \\{1, 1, 0, \\sqrt{2}, 0.5\\}$. $a_4$ (index $3$) is selected. This is incorrect. $s_1 = \\text{False}$.\n    - $\\alpha=0.5$: The selection rule uses $|\\langle a_j/\\|a_j\\|_2^{0.5}, y \\rangle|$. The correlations are $\\{1, 1, 0, 2/2^{0.25}, 5/\\sqrt{10}\\} \\approx \\{1, 1, 0, 1.682, 1.581\\}$. $a_4$ (index $3$) is selected. This is incorrect.\n- **Result**: Neither $\\alpha=0$ nor $\\alpha=1$ yields correct recovery. Since no tested $\\alpha$ succeeds, $\\alpha^\\star = -1$.\n- **Output**: $[\\sqrt{0.875}, 10\\sqrt{0.875}, \\text{False}, \\text{False}, -1.0]$\n\n**Test Case T2:**\n- $A \\in \\mathbb{R}^{3 \\times 3}$, $k=1$, $x^\\star = (1, 0, 0)^\\top$, $e=(0, 0, 0.02)^\\top$.\n- True support $S^\\star=\\{0\\}$.\n- $y = 1 \\cdot a_1 + e = (1, 0, 0.02)^\\top$.\n- Column norms are $\\|a_1\\|_2=1, \\|a_2\\|_2=5, \\|a_3\\|_2=1$.\n- **Coherences**:\n    - $\\mu(A) = |(a_1/\\|a_1\\|_2)^\\top (a_2/\\|a_2\\|_2)| = 1/5 = 0.2$.\n    - $\\tilde{\\mu}(A) = |a_1^\\top a_2| = 1$.\n- **OMP runs** ($k=1$ step):\n    - $\\alpha=0$: We select based on $|\\langle a_j, y \\rangle|$.\n        - $|\\langle a_1, y \\rangle| = 1$.\n        - $|\\langle a_2, y \\rangle| = |1 \\cdot 1 + (5\\sqrt{0.96}) \\cdot 0.02| \\approx 1.098$.\n        - $|\\langle a_3, y \\rangle| = 0$.\n        $a_2$ (index $1$) is selected due to its large norm amplifying the noise component. Incorrect. $s_0 = \\text{False}$.\n    - $\\alpha=1$: We select based on $|\\langle a_j/\\|a_j\\|_2, y \\rangle|$.\n        - $|\\langle a'_1, y \\rangle| = 1/1=1$.\n        - $|\\langle a'_2, y \\rangle| \\approx 1.098/5 \\approx 0.22$.\n        - $|\\langle a'_3, y \\rangle| = 0$.\n        $a_1$ (index $0$) is selected. Correct. $s_1 = \\text{True}$.\n    - $\\alpha=0.5$: We select based on $|\\langle a_j/\\|a_j\\|_2^{0.5}, y \\rangle|$.\n        - correlation for $a_1$: $1/\\sqrt{1}=1$.\n        - correlation for $a_2$: $1.098/\\sqrt{5} \\approx 0.49$.\n        $a_1$ (index $0$) is selected. Correct.\n- **Result**: OMP with $\\alpha=0$ fails, while $\\alpha=0.5$ and $\\alpha=1$ succeed. The smallest successful exponent is $\\alpha^\\star=0.5$.\n- **Output**: $[0.2, 1.0, \\text{False}, \\text{True}, 0.5]$\n\n**Test Case T3:**\n- $A \\in \\mathbb{R}^{4 \\times 6}$, $k=2$, $x^\\star=(1.2, 0, 0.8, 0, 0, 0)^\\top$, $e=(0.01, -0.02, 0.015, 0)^\\top$.\n- True support $S^\\star=\\{0, 2\\}$.\n- $y=1.2 a_1 + 0.8 a_3 + e = (1.21, -0.02, 0.815, 0)^\\top$.\n- Column norms are $\\|a_1..a_4\\|_2=1$, $\\|a_5\\|_2=\\|a_6\\|_2=1/\\sqrt{2}$.\n- **Coherences**:\n    - $\\mu(A) = |a_1^\\top (a_5/\\|a_5\\|_2)| = 0.5 / (1/\\sqrt{2}) = 1/\\sqrt{2} \\approx 0.7071$.\n    - $\\tilde{\\mu}(A) = |a_1^\\top a_5| = 0.5$.\n- **OMP runs**:\n    - $\\alpha=0$:\n        - Iteration 1: $r^{(0)} = y$. Correlations $|\\langle a_j, y \\rangle|$ are $\\{1.21, 0.02, 0.815, 0, 0.595, 0.3975\\}$. Select index $0$. $S_1=\\{0\\}$.\n        - Iteration 2: $r^{(1)} = y - (a_1^\\top y) a_1 = (0, -0.02, 0.815, 0)^\\top$. Correlations $|\\langle a_j, r^{(1)} \\rangle|$ for $j \\notin S_1$ are $\\{0.02, 0.815, 0, 0.01, 0.3975\\}$. Select index $2$.\n        - Final support $\\{0, 2\\}$. Correct. $s_0 = \\text{True}$.\n    - $\\alpha=1$:\n        - Iteration 1: Correlations $|\\langle a_j/\\|a_j\\|_2, y \\rangle|$ are $\\{1.21, 0.02, 0.815, 0, 0.595\\sqrt{2}, 0.3975\\sqrt{2}\\} \\approx \\{1.21, 0.02, 0.815, 0, 0.841, 0.562\\}$. Select index $0$.\n        - Iteration 2: Residual $r^{(1)}$ is the same. Correlations $|\\langle a_j/\\|a_j\\|_2, r^{(1)} \\rangle|$ are $\\{0.02, 0.815, 0, 0.01\\sqrt{2}, 0.3975\\sqrt{2}\\} \\approx \\{0.02, 0.815, 0, 0.014, 0.562\\}$. Select index $2$.\n        - Final support $\\{0, 2\\}$. Correct. $s_1 = \\text{True}$.\n- **Result**: OMP succeeds for $\\alpha=0$ and $\\alpha=1$. The smallest successful exponent is $\\alpha^\\star=0.0$.\n- **Output**: $[1/\\sqrt{2}, 0.5, \\text{True}, \\text{True}, 0.0]$", "answer": "```python\nimport numpy as np\n\ndef run_omp(A, y, k, alpha):\n    \"\"\"\n    Implements the Orthogonal Matching Pursuit algorithm.\n\n    Args:\n        A (np.ndarray): The dictionary/sensing matrix (m x n).\n        y (np.ndarray): The measurement vector (m,).\n        k (int): The sparsity level.\n        alpha (float): The power-normalization exponent.\n\n    Returns:\n        set: The set of selected indices (the support).\n    \"\"\"\n    m, n = A.shape\n\n    # 1. Scale the dictionary to form B_alpha\n    norms = np.linalg.norm(A, axis=0)\n    # Avoid division by zero for zero-norm columns (not in test cases).\n    norms[norms == 0] = 1.0\n    B = A / (norms**alpha)\n\n    support = []\n    residual = y.copy()\n    available_indices = list(range(n))\n\n    for _ in range(k):\n        # 2. Selection step: find index that maximizes correlation\n        if not available_indices:\n            break\n        \n        correlations = np.abs(B[:, available_indices].T @ residual)\n        max_corr = np.max(correlations)\n\n        # Tie-breaking: find all indices with max correlation\n        # and choose the one with the smallest original index.\n        max_indices_in_subset = np.where(np.isclose(correlations, max_corr))[0]\n        \n        # Map back to original indices\n        original_indices_with_max_corr = [available_indices[i] for i in max_indices_in_subset]\n        selected_index = min(original_indices_with_max_corr)\n        \n        # 3. Update support and available indices\n        support.append(selected_index)\n        available_indices.remove(selected_index)\n        \n        # 4. Update residual via least-squares on original A\n        A_S = A[:, sorted(support)]\n        # Solve least squares: x = (A_S^T A_S)^-1 A_S^T y\n        # Using pinv is robust.\n        x_S = np.linalg.pinv(A_S) @ y\n        residual = y - A_S @ x_S\n        \n    return set(support)\n\ndef analyze_case(A, x_star, e):\n    \"\"\"\n    Analyzes a single test case for OMP performance.\n\n    Args:\n        A (np.ndarray): The dictionary matrix.\n        x_star (np.ndarray): The ground truth sparse signal.\n        e (np.ndarray): The noise vector.\n\n    Returns:\n        list: A list containing [mu(A), tilde_mu(A), s0, s1, alpha_star].\n    \"\"\"\n    # Problem setup from givens\n    m, n = A.shape\n    k = np.count_nonzero(x_star)\n    true_support = set(np.flatnonzero(x_star))\n    y = A @ x_star + e\n\n    # 1. Compute coherences\n    # mu(A) on column-normalized A\n    col_norms = np.linalg.norm(A, axis=0)\n    col_norms[col_norms == 0] = 1.0 # Avoid division by zero\n    A_norm = A / col_norms\n    G_norm = A_norm.T @ A_norm\n    np.fill_diagonal(G_norm, 0)\n    mu_A = np.max(np.abs(G_norm))\n    \n    # tilde_mu(A) on unscaled A\n    G_un = A.T @ A\n    np.fill_diagonal(G_un, 0)\n    tilde_mu_A = np.max(np.abs(G_un))\n    \n    # 2  3. Run OMP for alpha={0, 1} and search for alpha_star\n    s0, s1 = False, False\n    alpha_star = -1.0\n    \n    alphas_to_test = [0.0, 0.5, 1.0]\n    \n    for alpha in alphas_to_test:\n        omp_support = run_omp(A, y, k, alpha)\n        is_correct = (omp_support == true_support)\n        \n        if alpha == 0.0:\n            s0 = is_correct\n        if alpha == 1.0:\n            s1 = is_correct\n\n        if is_correct and alpha_star == -1.0:\n            alpha_star = alpha\n            \n    return [mu_A, tilde_mu_A, s0, s1, alpha_star]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run analysis, and print results.\n    \"\"\"\n    # Test case T1\n    A1 = np.array([\n        [1.0, 0.0, 0.0, 1.0, 2.5],\n        [0.0, 1.0, 0.0, 1.0, 2.5],\n        [0.0, 0.0, 1.0, 0.0, 10 * np.sqrt(0.875)]\n    ])\n    x_star1 = np.array([1.0, 1.0, 0.0, 0.0, 0.0])\n    e1 = np.array([0.0, 0.0, 0.0])\n    \n    # Test case T2\n    A2 = np.array([\n        [1.0, 5.0 * 0.2, 0.0],\n        [0.0, 0.0,         1.0],\n        [0.0, 5.0 * np.sqrt(0.96), 0.0]\n    ])\n    x_star2 = np.array([1.0, 0.0, 0.0])\n    e2 = np.array([0.0, 0.0, 0.02])\n\n    # Test case T3\n    A3 = np.array([\n        [1.0, 0.0, 0.0, 0.0, 0.5, 0.0],\n        [0.0, 1.0, 0.0, 0.0, 0.5, 0.5],\n        [0.0, 0.0, 1.0, 0.0, 0.0, 0.5],\n        [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n    ])\n    x_star3 = np.array([1.2, 0.0, 0.8, 0.0, 0.0, 0.0])\n    e3 = np.array([0.01, -0.02, 0.015, 0.0])\n\n    test_cases = [\n        (A1, x_star1, e1),\n        (A2, x_star2, e2),\n        (A3, x_star3, e3)\n    ]\n    \n    results = []\n    for A, x_star, e in test_cases:\n        res = analyze_case(A, x_star, e)\n        results.append(res)\n    \n    # Format the final output string as specified\n    # Example: [[0.935,9.354,False,False,-1.0],[0.2,1.0,False,True,0.5],[0.707,0.5,True,True,0.0]]\n    # Python's str(True) is 'True', which is fine. The problem doesn't specify lowercase.\n    outer_list = []\n    for res_list in results:\n        # Convert each item in the inner list to a string\n        inner_list_str = [str(item) for item in res_list]\n        outer_list.append(f\"[{','.join(inner_list_str)}]\")\n    \n    final_output = f\"[{','.join(outer_list)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3441539"}, {"introduction": "Standard OMP is known to struggle when the dictionary contains highly coherent atoms, as it may incorrectly select an atom that is strongly correlated with a true support element. This advanced practice introduces a powerful modification: a reweighted OMP that dynamically penalizes candidate atoms based on their coherence with the already selected support set. By implementing this algorithm [@problem_id:3441569] and comparing it against the standard version, you will see firsthand how intelligent adjustments to the greedy selection rule can significantly improve recovery performance in challenging, highly coherent environments.", "problem": "Consider a sensing matrix with column-normalized atoms $A \\in \\mathbb{R}^{m \\times n}$, columns $\\{a_j\\}_{j=1}^n$, a $k$-sparse signal $x^\\star \\in \\mathbb{R}^n$ with support $S \\subset \\{1,\\dots,n\\}$ of size $|S| = k$, and measurements $y = A x^\\star + e$, where $e \\in \\mathbb{R}^m$ is additive noise. The goal is to analyze and implement a variant of Orthogonal Matching Pursuit (OMP) that uses reweighted correlations to penalize candidate atoms that are coherent with already selected atoms, and to empirically study correctness improvements under mutual coherence.\n\nUse the following foundational definitions as the base:\n- Mutual coherence $\\mu(A) := \\max_{i \\neq j} |\\langle a_i, a_j \\rangle|$, where $\\langle \\cdot, \\cdot \\rangle$ denotes the standard Euclidean inner product.\n- In standard Orthogonal Matching Pursuit (OMP), at iteration $t$ with selected index set $T_t$, residual $r_t = y - A_{T_t} x_{T_t}$, and correlation vector $c_t = A^\\top r_t$, the next selected index $j_t \\notin T_t$ is chosen to maximize $|\\langle a_j, r_t \\rangle|$ over $j \\notin T_t$.\n\nIn the reweighted selection rule, define for each candidate index $j \\notin T_t$ the cumulative coherence score\n$$\ns_j(t) := \\sum_{i \\in T_t} \\big|\\langle a_j, a_i \\rangle\\big|,\n$$\nand a positive reweighting parameter $\\gamma  0$. Propose a weighting function\n$$\nw_j(t) := \\frac{1}{1 + \\gamma\\, s_j(t)}.\n$$\nModify the OMP selection to choose\n$$\nj_t \\in \\arg\\max_{j \\notin T_t} \\Big( w_j(t)\\, \\big| \\langle a_j, r_t \\rangle \\big| \\Big).\n$$\nThe iteration proceeds with least-squares fitting on the updated set $T_{t+1} := T_t \\cup \\{j_t\\}$, residual update $r_{t+1} := y - A_{T_{t+1}} x_{T_{t+1}}$, and terminates after $k$ iterations.\n\nYour tasks:\n- Implement standard OMP and the reweighted OMP variant defined above.\n- For each test case, construct the matrix $A$, sparse support $S$, and coefficients on the support to form $x^\\star$, generate $y = A x^\\star + e$ where the noise vector $e$ satisfies $\\|e\\|_2 = \\eta \\|y_0\\|_2$ with $y_0 := A x^\\star$ and prescribed noise level $\\eta \\ge 0$, run both algorithms for exactly $k$ iterations, and return the support sets $T_{\\text{std}}$ and $T_{\\text{rw}}$.\n- Define the support recovery error as the integer\n$$\n\\text{err}(T,S) := k - |T \\cap S|,\n$$\nwhich is the number of incorrect picks in $k$ selections. Report the improvement as the integer\n$$\n\\Delta := \\text{err}(T_{\\text{std}},S) - \\text{err}(T_{\\text{rw}},S).\n$$\nA positive $\\Delta$ indicates that reweighted OMP improves support recovery over standard OMP by $\\Delta$ fewer errors; a zero value indicates parity; a negative value indicates worse performance.\n\nTest suite specification and construction details:\n- All columns of $A$ must be scaled to unit $\\ell_2$-norm. Use fixed pseudorandom seeds for reproducibility.\n- For clustered-coherent dictionaries, create coherent pairs $(i,j)$ by replacing column $a_j$ with\n$$\na_j \\leftarrow \\frac{\\alpha\\, a_i + \\sqrt{1-\\alpha^2}\\, u}{\\|\\alpha\\, a_i + \\sqrt{1-\\alpha^2}\\, u\\|_2},\n$$\nwhere $u$ is a random Gaussian vector orthogonalized against $a_i$ to satisfy $\\langle a_i, u \\rangle \\approx 0$, and $\\alpha \\in (0,1)$ controls the induced pairwise coherence $|\\langle a_i, a_j \\rangle| \\approx \\alpha$.\n- Set the nonzero entries of $x^\\star$ on $S$ to positive values that decrease linearly from largest to smallest across the $k$ support indices to avoid trivial ties.\n\nUse the following four test cases:\n- Test case $1$ (low coherence, noise-free):\n    - Dimensions: $m = 60$, $n = 120$, $k = 8$, seed $7$.\n    - Dictionary construction: independent and identically distributed Gaussian entries with zero mean and unit variance, then column-normalize.\n    - Support: $S = \\{5,17,23,42,55,63,88,107\\}$.\n    - Coefficients on $S$: decreasing linearly from $1.0$ to $0.3$ across the eight indices.\n    - Noise level: $\\eta = 0$.\n    - Reweighting parameter: $\\gamma = 0.5$.\n- Test case $2$ (moderate clustered coherence, small noise):\n    - Dimensions: $m = 60$, $n = 120$, $k = 8$, seed $11$.\n    - Dictionary construction: start with Gaussian as before, then induce coherent pairs with $\\alpha = 0.98$ at indices $(10,11)$, $(30,31)$, $(70,71)$, $(95,96)$, and renormalize those pair columns.\n    - Support: $S = \\{10,30,70,95,20,50,90,110\\}$.\n    - Coefficients on $S$: decreasing linearly from $1.0$ to $0.3$ across the eight indices.\n    - Noise level: $\\eta = 0.02$ relative to $\\|y_0\\|_2$.\n    - Reweighting parameter: $\\gamma = 1.0$.\n- Test case $3$ (high clustered coherence, moderate noise):\n    - Dimensions: $m = 60$, $n = 120$, $k = 10$, seed $19$.\n    - Dictionary construction: start with Gaussian, then induce coherent pairs with $\\alpha = 0.99$ at indices $(5,6)$, $(25,26)$, $(45,46)$, $(65,66)$, $(85,86)$, and renormalize those pair columns.\n    - Support: $S = \\{5,25,45,65,85,12,34,56,78,100\\}$.\n    - Coefficients on $S$: decreasing linearly from $1.0$ to $0.2$ across the ten indices.\n    - Noise level: $\\eta = 0.05$ relative to $\\|y_0\\|_2$.\n    - Reweighting parameter: $\\gamma = 1.5$.\n- Test case $4$ (boundary, no reweighting):\n    - Use exactly the same construction as test case $2$, but set $\\gamma = 0$ to verify parity with standard OMP.\n\nYour program should produce a single line of output containing the improvements $\\Delta$ for the four test cases as a comma-separated list enclosed in square brackets, for example, $[d_1,d_2,d_3,d_4]$, where each $d_i$ is an integer computed as defined above. No other text should be printed. There are no physical units or angle units in this problem; all reported quantities are dimensionless.", "solution": "The user has provided a valid, well-posed problem statement from the domain of sparse signal recovery and compressed sensing. The problem is scientifically grounded, formally specified, and computationally verifiable.\n\nThe objective is to implement and evaluate a reweighted variant of Orthogonal Matching Pursuit (OMP) designed to improve support recovery performance when the sensing matrix (or dictionary) $A$ contains highly coherent columns. We will compare this reweighted OMP (RW-OMP) against standard OMP on a suite of test cases with varying levels of dictionary coherence and noise.\n\nThe fundamental challenge addressed is that standard OMP's performance degrades significantly when the dictionary atoms are not mutually incoherent. High coherence, quantified by $\\mu(A) = \\max_{i \\neq j} |\\langle a_i, a_j \\rangle|$, can cause OMP to incorrectly select an atom that is not in the true support $S$ but is highly correlated with an atom that is.\n\nThe algorithmic procedures are as follows:\n\nLet $A \\in \\mathbb{R}^{m \\times n}$ be the sensing matrix with unit-norm columns $\\{a_j\\}$, $y \\in \\mathbb{R}^m$ be the measurement vector, and $k$ be the known sparsity of the signal. Both algorithms aim to identify the support set $S$ of size $k$. They are iterative, running for $k$ steps.\n\nAt iteration $t$ (from $t=1$ to $k$):\nLet $T_{t-1}$ be the set of indices selected in previous iterations, and let $r_{t-1}$ be the corresponding residual. Initially, $T_0 = \\emptyset$ and $r_0 = y$.\n\n**1. Standard Orthogonal Matching Pursuit (OMP)**\nThe standard OMP algorithm greedily selects the atom best correlated with the current residual.\n- **Correlation Calculation**: Compute the inner products of all atoms with the residual: $c_j = \\langle a_j, r_{t-1} \\rangle$ for all $j \\in \\{1, \\dots, n\\} \\setminus T_{t-1}$.\n- **Index Selection**: Identify the index $j_t$ that maximizes the absolute correlation:\n$$\nj_t = \\arg\\max_{j \\notin T_{t-1}} |\\langle a_j, r_{t-1} \\rangle|\n$$\n- **Support Update**: Augment the support set: $T_t = T_{t-1} \\cup \\{j_t\\}$.\n- **Signal and Residual Update**: A new signal estimate $x_t$ is formed by solving a least-squares problem restricted to the atoms in the current support set $A_{T_t}$:\n$$\nx_t = \\arg\\min_{z: \\text{supp}(z) \\subseteq T_t} \\|y - A z\\|_2^2\n$$\nThe solution is given by $x_{T_t} = A_{T_t}^\\dagger y$, where $A_{T_t}^\\dagger = (A_{T_t}^\\top A_{T_t})^{-1} A_{T_t}^\\top$ is the Moore-Penrose pseudoinverse. The new residual is the remaining part of the signal that cannot be explained by the selected atoms:\n$$\nr_t = y - A_{T_t} x_{T_t}\n$$\n\n**2. Reweighted Orthogonal Matching Pursuit (RW-OMP)**\nThe reweighted variant modifies the selection step to penalize candidate atoms that are highly coherent with atoms already in the support set $T_{t-1}$.\n- **Correlation Calculation**: Same as standard OMP, compute $c_j = \\langle a_j, r_{t-1} \\rangle$.\n- **Reweighting**: For each candidate atom $a_j$ (where $j \\notin T_{t-1}$), calculate its cumulative coherence with the already selected atoms:\n$$\ns_j(t-1) = \\sum_{i \\in T_{t-1}} |\\langle a_j, a_i \\rangle|\n$$\nA weighting factor is then computed using a given parameter $\\gamma  0$:\n$$\nw_j(t-1) = \\frac{1}{1 + \\gamma s_j(t-1)}\n$$\nThis weight $w_j(t-1)$ is close to $1$ for atoms with low cumulative coherence and decreases towards $0$ for atoms with high cumulative coherence.\n- **Index Selection**: The next index $j_t$ is chosen by maximizing the weighted correlation:\n$$\nj_t = \\arg\\max_{j \\notin T_{t-1}} \\Big( w_j(t-1) \\cdot |\\langle a_j, r_{t-1} \\rangle| \\Big)\n$$\n- **Support and Residual Update**: These steps are identical to standard OMP.\n\n**Simulation Setup and Evaluation**\nWe will programmatically construct four test cases as specified. For each case, we will generate the matrix $A$, the sparse signal $x^\\star$, and the measurement vector $y = A x^\\star + e$. The noise vector $e$ is scaled such that its norm $\\|e\\|_2$ is a specific fraction $\\eta$ of the noiseless signal norm $\\|y_0\\|_2 = \\|A x^\\star\\|_2$.\nThe dictionaries are generated from i.i.d. Gaussian entries, then column-normalized. For cases with structured coherence, specific columns are replaced by a linear combination of an existing column and an orthogonal random vector to achieve a target coherence value $\\alpha$.\n\nThe performance of each algorithm is measured by the support recovery error, defined as the number of indices selected by the algorithm that are not in the true support $S$:\n$$\n\\text{err}(T,S) = k - |T \\cap S|\n$$\nThe final reported metric is the improvement $\\Delta$, which is the difference in error between the standard and reweighted methods:\n$$\n\\Delta = \\text{err}(T_{\\text{std}}, S) - \\text{err}(T_{\\text{rw}}, S)\n$$\nA positive $\\Delta$ indicates that RW-OMP correctly identified more true support indices than standard OMP. A value of $\\Delta=0$ implies identical performance, which is expected in Test Case 4 where $\\gamma=0$, making the two algorithms equivalent.", "answer": "```python\nimport numpy as np\n\ndef run_omp(A, y, k, gamma=0.0):\n    \"\"\"\n    Performs standard or reweighted Orthogonal Matching Pursuit.\n\n    Args:\n        A (np.ndarray): The sensing matrix (m x n) with unit-norm columns.\n        y (np.ndarray): The measurement vector (m x 1).\n        k (int): The sparsity level.\n        gamma (float): The reweighting parameter. gamma=0 corresponds to standard OMP.\n\n    Returns:\n        list: The estimated support set of size k.\n    \"\"\"\n    m, n = A.shape\n    T = []\n    r = y.copy()\n    available_indices = list(range(n))\n    \n    A_T_cache = np.zeros((m, k))\n\n    for t in range(k):\n        correlations = A.T @ r\n        \n        # Restrict correlations to available indices\n        current_correlations = correlations[available_indices]\n\n        if gamma  0 and t  0:\n            # Get submatrix of already selected atoms\n            A_selected = A_T_cache[:, :t]\n            \n            # Get submatrix of available atoms\n            A_available = A[:, available_indices]\n\n            # Calculate cumulative coherence scores for available indices\n            coherence_scores = np.sum(np.abs(A_available.T @ A_selected), axis=1)\n            \n            # Calculate weights\n            weights = 1.0 / (1.0 + gamma * coherence_scores)\n            \n            # Apply weights\n            objective = weights * np.abs(current_correlations)\n        else:\n            objective = np.abs(current_correlations)\n\n        # Find the index with the maximum objective value among available indices\n        best_idx_in_available = np.argmax(objective)\n        j_t = available_indices.pop(best_idx_in_available)\n        \n        T.append(j_t)\n        A_T_cache[:, t] = A[:, j_t]\n\n        # Update residual via least-squares\n        A_T = A_T_cache[:, :t+1]\n        x_T = np.linalg.pinv(A_T) @ y\n        r = y - A_T @ x_T\n        \n    return sorted(T)\n\ndef solve():\n    \"\"\"\n    Runs the full simulation for all test cases and prints the result.\n    \"\"\"\n    test_cases = [\n        # Case 1: low coherence, noise-free\n        {\n            \"m\": 60, \"n\": 120, \"k\": 8, \"seed\": 7,\n            \"coherent_pairs\": [], \"alpha\": 0,\n            \"S\": {5, 17, 23, 42, 55, 63, 88, 107},\n            \"coeffs\": (1.0, 0.3), \"eta\": 0.0, \"gamma\": 0.5\n        },\n        # Case 2: moderate clustered coherence, small noise\n        {\n            \"m\": 60, \"n\": 120, \"k\": 8, \"seed\": 11,\n            \"coherent_pairs\": [(10, 11), (30, 31), (70, 71), (95, 96)], \"alpha\": 0.98,\n            \"S\": {10, 30, 70, 95, 20, 50, 90, 110},\n            \"coeffs\": (1.0, 0.3), \"eta\": 0.02, \"gamma\": 1.0\n        },\n        # Case 3: high clustered coherence, moderate noise\n        {\n            \"m\": 60, \"n\": 120, \"k\": 10, \"seed\": 19,\n            \"coherent_pairs\": [(5, 6), (25, 26), (45, 46), (65, 66), (85, 86)], \"alpha\": 0.99,\n            \"S\": {5, 25, 45, 65, 85, 12, 34, 56, 78, 100},\n            \"coeffs\": (1.0, 0.2), \"eta\": 0.05, \"gamma\": 1.5\n        },\n        # Case 4: boundary, no reweighting (same as case 2 but gamma=0)\n        {\n            \"m\": 60, \"n\": 120, \"k\": 8, \"seed\": 11,\n            \"coherent_pairs\": [(10, 11), (30, 31), (70, 71), (95, 96)], \"alpha\": 0.98,\n            \"S\": {10, 30, 70, 95, 20, 50, 90, 110},\n            \"coeffs\": (1.0, 0.3), \"eta\": 0.02, \"gamma\": 0.0\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        m, n, k, seed = case[\"m\"], case[\"n\"], case[\"k\"], case[\"seed\"]\n        np.random.seed(seed)\n\n        # 1. Construct the dictionary A\n        A = np.random.randn(m, n)\n        # Column normalize\n        A /= np.linalg.norm(A, axis=0)\n\n        # Induce coherent pairs\n        for i, j in case[\"coherent_pairs\"]:\n            # Convert to 0-based index\n            i_idx, j_idx = i - 1, j - 1\n            alpha = case[\"alpha\"]\n            \n            a_i = A[:, i_idx]\n            u_rand = np.random.randn(m)\n            # Orthogonalize u w.r.t. a_i using Gram-Schmidt\n            u_ortho = u_rand - (u_rand @ a_i) * a_i\n            u_norm = u_ortho / np.linalg.norm(u_ortho)\n            \n            # Construct new coherent column\n            a_j_new = alpha * a_i + np.sqrt(1 - alpha**2) * u_norm\n            # Replace and re-normalize (safeguard)\n            A[:, j_idx] = a_j_new / np.linalg.norm(a_j_new)\n\n        # 2. Construct the sparse signal x_star\n        # Convert 1-based support to 0-based\n        S_true = {s_idx - 1 for s_idx in case[\"S\"]}\n        x_star = np.zeros(n)\n        coeffs = np.linspace(case[\"coeffs\"][0], case[\"coeffs\"][1], k)\n        # Assign coefficients to sorted indices for reproducibility\n        sorted_S_true = sorted(list(S_true))\n        x_star[sorted_S_true] = coeffs\n        \n        # 3. Generate measurements y\n        y0 = A @ x_star\n        eta = case[\"eta\"]\n        if eta  0:\n            noise = np.random.randn(m)\n            noise_norm = np.linalg.norm(noise)\n            y0_norm = np.linalg.norm(y0)\n            scaled_noise = noise * (eta * y0_norm / noise_norm)\n            y = y0 + scaled_noise\n        else:\n            y = y0\n\n        # 4. Run algorithms\n        # Standard OMP (gamma=0)\n        T_std = run_omp(A, y, k, gamma=0.0)\n        \n        # Reweighted OMP\n        T_rw = run_omp(A, y, k, gamma=case[\"gamma\"])\n        \n        # 5. Calculate errors and improvement\n        err_std = k - len(set(T_std)  S_true)\n        err_rw = k - len(set(T_rw)  S_true)\n        \n        delta = err_std - err_rw\n        results.append(delta)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3441569"}]}