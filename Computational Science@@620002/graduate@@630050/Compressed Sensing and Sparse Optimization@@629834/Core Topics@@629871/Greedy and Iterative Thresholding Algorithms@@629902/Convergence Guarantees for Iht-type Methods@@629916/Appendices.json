{"hands_on_practices": [{"introduction": "Mastering any iterative algorithm begins with a deep understanding of a single update step. This exercise provides a concrete opportunity to compute one full iteration of the Iterative Hard Thresholding (IHT) method. By manually performing the gradient descent step and the subsequent hard thresholding projection, you will solidify your grasp of the fundamental mechanics that drive the algorithm towards a sparse solution.", "problem": "Consider the sparse optimization problem of recovering a $k$-sparse signal from linear measurements by minimizing the least-squares loss subject to a sparsity constraint. Let the least-squares objective be $f(x) = \\frac{1}{2}\\|y - A x\\|_{2}^{2}$ with measurement matrix $A \\in \\mathbb{R}^{3 \\times 3}$, measurements $y \\in \\mathbb{R}^{3}$, and an initial iterate $x_{0} \\in \\mathbb{R}^{3}$. The Iterative Hard Thresholding (IHT) method updates an iterate by first taking a gradient step on $f$ and then enforcing $k$-sparsity via the hard thresholding operator $H_{k}$, which returns the nearest $k$-sparse vector in the Euclidean norm. The hard thresholding operator $H_{k}$ selects the $k$ entries of its argument with largest absolute value and sets all other entries to zero.\n\nYou are given\n$$\nA = \\begin{pmatrix}\n2 & 0 & 1 \\\\\n0 & 1 & 1 \\\\\n1 & 1 & 0\n\\end{pmatrix},\\quad\ny = \\begin{pmatrix}\n3 \\\\\n0 \\\\\n2\n\\end{pmatrix},\\quad\nk = 2,\\quad\n\\mu = \\frac{1}{4},\\quad\nx_{0} = \\begin{pmatrix}\n1 \\\\\n-1 \\\\\n0\n\\end{pmatrix}.\n$$\nStarting from the definition of the least-squares gradient and the characterization of $H_{k}$ as the Euclidean projection onto the nonconvex set of $k$-sparse vectors, explicitly compute the first IHT iterate $x_{1}$ obtained by one gradient step from $x_{0}$ followed by hard thresholding. In addition, explain the role of $H_{k}$ in this computation in the context of convergence guarantees for IHT-type methods, including why its projection property is central to maintaining sparsity while controlling error growth.\n\nProvide your final answer as the $3$-entry row matrix corresponding to $x_{1}$. No rounding is required.", "solution": "The problem is valid. It is a well-defined computational and conceptual problem in the field of sparse optimization. The provided data are complete, consistent, and scientifically grounded within the mathematical framework of compressed sensing. The task is to apply a standard algorithm, Iterative Hard Thresholding (IHT), for one iteration and explain the role of one of its components, which is a standard conceptual question in this domain.\n\nThe givens are:\n- Objective function: $f(x) = \\frac{1}{2}\\|y - A x\\|_{2}^{2}$\n- Measurement matrix: $A = \\begin{pmatrix} 2 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 0 \\end{pmatrix} \\in \\mathbb{R}^{3 \\times 3}$\n- Measurement vector: $y = \\begin{pmatrix} 3 \\\\ 0 \\\\ 2 \\end{pmatrix} \\in \\mathbb{R}^{3}$\n- Sparsity level: $k = 2$\n- Step size: $\\mu = \\frac{1}{4}$\n- Initial iterate: $x_{0} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} \\in \\mathbb{R}^{3}$\n- Hard thresholding operator: $H_{k}(v)$ sets all but the $k$ largest-magnitude entries of a vector $v$ to zero.\n\nThe Iterative Hard Thresholding (IHT) update rule is given by:\n$$x_{i+1} = H_k(x_i - \\mu \\nabla f(x_i))$$\nwhere $H_k$ is the hard thresholding operator that projects its argument onto the set of $k$-sparse vectors. The objective function is $f(x) = \\frac{1}{2}\\|y - Ax\\|_2^2$. Its gradient is $\\nabla f(x) = A^T(Ax - y)$.\n\nFirst, we compute the gradient at the initial iterate $x_0$. The given matrix $A$ is symmetric, so $A^T = A$.\nWe calculate the term $Ax_0$:\n$$Ax_{0} = \\begin{pmatrix} 2 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2(1) + 0(-1) + 1(0) \\\\ 0(1) + 1(-1) + 1(0) \\\\ 1(1) + 1(-1) + 0(0) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix}$$\nNext, we compute the residual vector $Ax_0 - y$:\n$$Ax_{0} - y = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -1 \\\\ -2 \\end{pmatrix}$$\nNow, we compute the gradient $\\nabla f(x_0) = A^T(Ax_0 - y) = A(Ax_0 - y)$:\n$$\\nabla f(x_{0}) = \\begin{pmatrix} 2 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 2(-1) + 0(-1) + 1(-2) \\\\ 0(-1) + 1(-1) + 1(-2) \\\\ 1(-1) + 1(-1) + 0(-2) \\end{pmatrix} = \\begin{pmatrix} -4 \\\\ -3 \\\\ -2 \\end{pmatrix}$$\n\nThe next step is to perform the gradient descent update to obtain an intermediate vector, which we denote as $z_0$:\n$$z_0 = x_0 - \\mu \\nabla f(x_0)$$\nSubstituting the given values $\\mu = \\frac{1}{4}$ and the computed gradient:\n$$z_{0} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} - \\frac{1}{4} \\begin{pmatrix} -4 \\\\ -3 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} -1 \\\\ -\\frac{3}{4} \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 1 - (-1) \\\\ -1 - (-\\frac{3}{4}) \\\\ 0 - (-\\frac{1}{2}) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -\\frac{1}{4} \\\\ \\frac{1}{2} \\end{pmatrix}$$\n\nFinally, we compute the first IHT iterate $x_1$ by applying the hard thresholding operator $H_k$ with $k=2$ to $z_0$. The operator $H_2$ identifies the two entries of $z_0$ with the largest absolute values and sets all other entries to zero.\nThe absolute values of the entries of $z_0$ are:\n$$|z_{0,1}| = |2| = 2$$\n$$|z_{0,2}| = \\left|-\\frac{1}{4}\\right| = 0.25$$\n$$|z_{0,3}| = \\left|\\frac{1}{2}\\right| = 0.5$$\nThe two largest magnitudes are $2$ and $0.5$, which correspond to the first and third entries. Therefore, $H_2(z_0)$ retains these two entries and sets the second entry to zero.\n$$x_1 = H_2(z_0) = H_2\\left(\\begin{pmatrix} 2 \\\\ -\\frac{1}{4} \\\\ \\frac{1}{2} \\end{pmatrix}\\right) = \\begin{pmatrix} 2 \\\\ 0 \\\\ \\frac{1}{2} \\end{pmatrix}$$\n\nRegarding the role of $H_k$, it is a non-linear operator that projects its input vector onto the nonconvex set of $k$-sparse vectors, $\\Sigma_k = \\{x \\in \\mathbb{R}^n : \\|x\\|_0 \\le k\\}$. The projection is defined in terms of the Euclidean ($\\ell_2$) norm, meaning $H_k(z)$ is the vector in $\\Sigma_k$ closest to $z$.\n\nThe role of $H_k$ is central to the convergence guarantees for IHT-type methods for two main reasons:\n1.  **Sparsity Enforcement**: The gradient step $x_i - \\mu \\nabla f(x_i)$ minimizes the least-squares error but typically produces a dense vector, thus violating the sparsity constraint. The $H_k$ operator remedies this by projecting the vector back into the feasible set $\\Sigma_k$. This enforcement is mandatory, as the theoretical guarantees for IHT depend on the iterates remaining sparse.\n2.  **Error Control in Convergence Proofs**: The convergence of IHT is typically proven under the assumption that the measurement matrix $A$ satisfies the Restricted Isometry Property (RIP). The RIP states that $A$ approximately preserves the norm of sparse vectors, i.e., $(1-\\delta_S)\\|v\\|_2^2 \\le \\|Av\\|_2^2 \\le (1+\\delta_S)\\|v\\|_2^2$ for all $S$-sparse vectors $v$ and for some small constant $\\delta_S < 1$. IHT proofs analyze the evolution of the error $\\|x_i - x^*\\|_2$, where $x^*$ is the true $k$-sparse solution. The analysis relies on the fact that the difference vector $x_i - x^*$ is $2k$-sparse (since both $x_i$ and $x^*$ are $k$-sparse). The RIP of order $2k$ (or higher, depending on the proof) ensures that the gradient update step, when restricted to the subspace of sparse vectors, acts as a contraction mapping, pulling the iterate closer to the solution. The projection property of $H_k$ is crucial for bounding the error $\\|x_{i+1} - x^*\\|_2 = \\|H_k(z_i) - x^*\\|_2$. While $\\Sigma_k$ is non-convex and $H_k$ is not non-expansive like projections onto convex sets, it can be shown that the projection does not amplify the error detrimentally. Specifically, the fact that $H_k(z_i)$ is the *best* $k$-sparse approximation to the intermediate vector $z_i$ is a key ingredient in showing that the overall iterative process converges. It ensures that the algorithm makes progress towards the true solution in the $\\ell_2$ sense, provided the step size $\\mu$ is chosen appropriately in relation to the restricted isometry constants of $A$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 2 & 0 & \\frac{1}{2} \\end{pmatrix}}\n$$", "id": "3438851"}, {"introduction": "While a single iteration shows us the direction of progress, the true power of IHT lies in its proven convergence guarantees. These guarantees are often expressed as a recursive bound on the error, which dictates how quickly the iterates approach the true signal. This practice moves from mechanics to theory, asking you to use a standard linear convergence model to calculate the number of iterations required to achieve a desired accuracy, thereby making the abstract concept of a 'convergence rate' tangible.", "problem": "Consider the linear measurement model $y = A x^{*} + e$ with a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ and an unknown $k$-sparse signal $x^{*} \\in \\mathbb{R}^{n}$. Assume $A$ satisfies the Restricted Isometry Property (RIP) of order $2k$ with constant $\\delta_{2k}$, meaning that for every $2k$-sparse vector $z \\in \\mathbb{R}^{n}$, the inequalities $(1 - \\delta_{2k}) \\|z\\|_{2}^{2} \\le \\|A z\\|_{2}^{2} \\le (1 + \\delta_{2k}) \\|z\\|_{2}^{2}$ hold. Consider the Iterative Hard Thresholding (IHT) method defined by $x_{t+1} = H_{k}\\!\\left(x_{t} - \\mu A^{\\top}(A x_{t} - y)\\right)$, where $H_{k}$ denotes the hard thresholding operator that retains the $k$ largest entries in magnitude and sets the rest to zero, and $\\mu > 0$ is a fixed step size.\n\nIt is known from standard IHT convergence analyses under the RIP that there exist constants $\\alpha$ and $\\beta$, depending only on $\\mu$ and $\\delta_{2k}$, such that the error sequence $E_{t} := \\|x_{t} - x^{*}\\|_{2}$ obeys a linear recursive bound of the form $E_{t+1} \\le \\alpha E_{t} + \\beta \\|e\\|_{2}$ for all $t \\ge 0$.\n\nIn this problem, take a sensing matrix with $\\delta_{2k} = 0.2$ and a step size $\\mu = 1$. For this configuration, suppose the associated contraction and noise-gain constants specified by the analysis are $\\alpha = 0.4$ and $\\beta = 1$. Starting from an initial error $E_{0} = \\|x_{0} - x^{*}\\|_{2} = 10$ and a noise norm $\\|e\\|_{2} = 1$, determine the smallest integer number of iterations $T$ such that $\\|x_{T} - x^{*}\\|_{2} \\le 2 \\beta \\|e\\|_{2}$ holds.\n\nYour final answer must be a single real-valued number representing $T$. No rounding is necessary beyond identifying the smallest integer $T$ that satisfies the stated requirement.", "solution": "The problem requires finding the smallest integer number of iterations $T$ for the Iterative Hard Thresholding (IHT) algorithm to reach a specified error tolerance. The error dynamics are governed by a given linear recursive inequality.\n\nFirst, let us formalize the given information. The error at iteration $t$, denoted by $E_{t} = \\|x_{t} - x^{*}\\|_{2}$, is bounded by the recursion:\n$$E_{t+1} \\le \\alpha E_{t} + \\beta \\|e\\|_{2}$$\nThe specific parameters provided are:\n- The initial error: $E_{0} = \\|x_{0} - x^{*}\\|_{2} = 10$.\n- The convergence rate constant: $\\alpha = 0.4$.\n- The noise gain constant: $\\beta = 1$.\n- The norm of the noise vector: $\\|e\\|_{2} = 1$.\n\nThe goal is to find the smallest integer $T$ such that the error $E_{T}$ satisfies the condition:\n$$E_{T} \\le 2 \\beta \\|e\\|_{2}$$\nSubstituting the given values, the target error level is $2 \\times 1 \\times 1 = 2$. So, we must find the smallest integer $T$ such that $E_{T} \\le 2$.\n\nTo solve this, we first need to find a closed-form expression for the upper bound on $E_{t}$. The recurrence relation is of the form $E_{t+1} \\le \\alpha E_{t} + C$, where $C = \\beta \\|e\\|_{2} = 1 \\times 1 = 1$. We can unroll this recursion to obtain a bound on $E_{t}$ in terms of $E_{0}$.\nFor $t=1$:\n$$E_{1} \\le \\alpha E_{0} + C$$\nFor $t=2$:\n$$E_{2} \\le \\alpha E_{1} + C \\le \\alpha (\\alpha E_{0} + C) + C = \\alpha^{2} E_{0} + (\\alpha + 1)C$$\nFor $t=3$:\n$$E_{3} \\le \\alpha E_{2} + C \\le \\alpha (\\alpha^{2} E_{0} + (\\alpha + 1)C) + C = \\alpha^{3} E_{0} + (\\alpha^{2} + \\alpha + 1)C$$\nBy induction, for a general integer $t > 0$, the error $E_{t}$ is bounded by:\n$$E_{t} \\le \\alpha^{t} E_{0} + C \\sum_{i=0}^{t-1} \\alpha^{i}$$\nThe summation term is a finite geometric series. Since $\\alpha = 0.4 \\neq 1$, we can use the formula for the sum of a geometric series:\n$$\\sum_{i=0}^{t-1} \\alpha^{i} = \\frac{1 - \\alpha^{t}}{1 - \\alpha}$$\nSubstituting this back into the inequality for $E_{t}$ gives:\n$$E_{t} \\le \\alpha^{t} E_{0} + C \\left( \\frac{1 - \\alpha^{t}}{1 - \\alpha} \\right)$$\nThis expression can be rearranged to clearly show the dependence on the initial error and the steady-state error bound:\n$$E_{t} \\le \\alpha^{t} E_{0} + \\frac{C}{1 - \\alpha} - \\frac{C\\alpha^{t}}{1 - \\alpha} = \\alpha^{t} \\left( E_{0} - \\frac{C}{1 - \\alpha} \\right) + \\frac{C}{1 - \\alpha}$$\nNow we substitute the given numerical values into this expression. We have $E_{0} = 10$, $\\alpha = 0.4$, and $C = 1$. The term $\\frac{C}{1-\\alpha}$ is:\n$$\\frac{C}{1 - \\alpha} = \\frac{1}{1 - 0.4} = \\frac{1}{0.6} = \\frac{10}{6} = \\frac{5}{3}$$\nThe inequality for $E_{t}$ becomes:\n$$E_{t} \\le (0.4)^{t} \\left( 10 - \\frac{5}{3} \\right) + \\frac{5}{3}$$\nSimplifying the term in the parenthesis:\n$$10 - \\frac{5}{3} = \\frac{30}{3} - \\frac{5}{3} = \\frac{25}{3}$$\nThus, the error bound is:\n$$E_{t} \\le (0.4)^{t} \\left( \\frac{25}{3} \\right) + \\frac{5}{3}$$\nWe need to find the smallest integer $T$ such that $E_{T} \\le 2$. We impose this condition on our derived upper bound, which is a sufficient condition for $E_T \\le 2$:\n$$(0.4)^{T} \\left( \\frac{25}{3} \\right) + \\frac{5}{3} \\le 2$$\nNow, we solve for $T$. First, isolate the term containing $T$:\n$$(0.4)^{T} \\left( \\frac{25}{3} \\right) \\le 2 - \\frac{5}{3} = \\frac{6-5}{3} = \\frac{1}{3}$$\n$$(0.4)^{T} \\le \\frac{1}{3} \\cdot \\frac{3}{25}$$\n$$(0.4)^{T} \\le \\frac{1}{25}$$\nTo solve for $T$, we take the natural logarithm of both sides. Note that $0.4 = \\frac{2}{5}$ and $\\frac{1}{25} = 5^{-2}$.\n$$\\ln\\left( (0.4)^{T} \\right) \\le \\ln\\left( \\frac{1}{25} \\right)$$\n$$T \\ln(0.4) \\le -\\ln(25)$$\nSince $0.4 < 1$, its natural logarithm $\\ln(0.4)$ is a negative number. Therefore, when we divide by $\\ln(0.4)$, we must reverse the direction of the inequality:\n$$T \\ge \\frac{-\\ln(25)}{\\ln(0.4)} = \\frac{\\ln(25)}{-\\ln(0.4)} = \\frac{\\ln(25)}{\\ln(1/0.4)} = \\frac{\\ln(25)}{\\ln(2.5)}$$\nWe can express this in terms of logarithms of prime numbers for clarity:\n$$T \\ge \\frac{\\ln(5^2)}{\\ln(5/2)} = \\frac{2\\ln(5)}{\\ln(5) - \\ln(2)}$$\nTo find the numerical value, we use $\\ln(5) \\approx 1.609438$ and $\\ln(2) \\approx 0.693147$:\n$$T \\ge \\frac{2 \\times 1.609438}{1.609438 - 0.693147} = \\frac{3.218876}{0.916291} \\approx 3.5126$$\nSince $T$ must be an integer, we must find the smallest integer that is greater than or equal to $3.5126$. This integer is $4$.\nWe can verify this by checking integer values of $T$ in the inequality $(0.4)^T \\le \\frac{1}{25} = 0.04$.\nFor $T=3$: $(0.4)^{3} = 0.064$. The inequality $0.064 \\le 0.04$ is false.\nFor $T=4$: $(0.4)^{4} = 0.0256$. The inequality $0.0256 \\le 0.04$ is true.\nThus, the smallest integer number of iterations is $T=4$.", "answer": "$$\n\\boxed{4}\n$$", "id": "3438867"}, {"introduction": "The standard IHT algorithm assumes a simple model where any $k$ coefficients can be non-zero. However, many signals exhibit 'structured sparsity,' where non-zero elements appear in predefined groups or blocks. This exercise introduces you to block-IHT, a powerful variant tailored for such structures, and asks you to compare its performance against element-wise IHT, revealing the significant gains possible when the recovery algorithm's model matches the signal's underlying structure.", "problem": "Consider the least-squares data fidelity objective in compressed sensing and sparse optimization,\n$$f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2},$$\nand the Iterative Hard Thresholding (IHT) method, where the basic gradient step is\n$$z^{t} = x^{t} - \\mu A^{\\top}(A x^{t} - y),$$\nfollowed by a projection onto a feasible sparsity model. In block-IHT, the projection is onto a predefined block-sparse model; in elementwise IHT, it is onto the set of vectors with a prescribed number of nonzero entries. Here, Iterative Hard Thresholding (IHT) and block-Iterative Hard Thresholding (block-IHT) are defined by the gradient step above followed by the appropriate projection.\n\nWork with the toy instance defined as follows. Let the ambient dimension be $n=4$ and let the block partition be $\\mathcal{B} = \\{B_{1}, B_{2}\\}$ with $B_{1} = \\{1,2\\}$ and $B_{2} = \\{3,4\\}$. The measurement matrix is $A = I_{4}$, the identity, the true block-sparse signal is\n$$x^{\\star} = \\begin{pmatrix}2 \\\\ -1 \\\\ 0 \\\\ 0\\end{pmatrix},$$\nthe observation is $y = A x^{\\star}$, the initial iterate is $x^{0} = \\begin{pmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0\\end{pmatrix}$, and the step size is $\\mu = \\frac{1}{2}$.\n\nPerform one iteration ($t=0$ to $t=1$) of block-IHT with exactly one active block (i.e., the projection selects the single block with largest block $\\ell_{2}$ norm) and one iteration of elementwise IHT with exactly one active coefficient (i.e., the projection selects the single entry with largest magnitude). Let the resulting iterates be $x^{1,\\mathrm{B}}$ for block-IHT and $x^{1,\\mathrm{E}}$ for elementwise IHT, and define the corresponding errors $e^{1,\\mathrm{B}} = x^{1,\\mathrm{B}} - x^{\\star}$ and $e^{1,\\mathrm{E}} = x^{1,\\mathrm{E}} - x^{\\star}$.\n\nCompute the ratio\n$$r = \\frac{\\|e^{1,\\mathrm{B}}\\|_{2}^{2}}{\\|e^{1,\\mathrm{E}}\\|_{2}^{2}}.$$\nExpress your final answer as an exact value; do not round.", "solution": "The problem is well-posed and scientifically grounded within the field of sparse optimization. All necessary information is provided, and the definitions are standard. We proceed with the solution.\n\nThe objective function is $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$, and the gradient is $\\nabla f(x) = A^{\\top}(A x - y)$. The Iterative Hard Thresholding (IHT) method involves a gradient descent step followed by a projection. For the first iteration (from $t=0$ to $t=1$), the intermediate vector before projection is given by\n$$z^{0} = x^{0} - \\mu \\nabla f(x^0) = x^{0} - \\mu A^{\\top}(A x^{0} - y).$$\n\nFirst, we establish the values of the given parameters.\nThe measurement matrix is the $4 \\times 4$ identity matrix, $A = I_{4}$.\nThe true signal is $x^{\\star} = \\begin{pmatrix}2 \\\\ -1 \\\\ 0 \\\\ 0\\end{pmatrix}$.\nThe observation vector is $y = A x^{\\star} = I_{4} x^{\\star} = x^{\\star} = \\begin{pmatrix}2 \\\\ -1 \\\\ 0 \\\\ 0\\end{pmatrix}$.\nThe initial iterate is the zero vector, $x^{0} = \\begin{pmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0\\end{pmatrix}$.\nThe step size is $\\mu = \\frac{1}{2}$.\n\nNow, we compute the intermediate vector $z^{0}$:\n$$z^{0} = x^{0} - \\mu A^{\\top}(A x^{0} - y) = 0 - \\frac{1}{2} I_{4}^{\\top}(I_{4} \\cdot 0 - y) = \\frac{1}{2} y.$$\nSubstituting the value of $y$, we get:\n$$z^{0} = \\frac{1}{2} x^{\\star} = \\frac{1}{2} \\begin{pmatrix}2 \\\\ -1 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}1 \\\\ -1/2 \\\\ 0 \\\\ 0\\end{pmatrix}.$$\nThis vector $z^{0}$ is the input to the projection step for both block-IHT and elementwise IHT.\n\n**1. Block-Iterative Hard Thresholding (block-IHT)**\nFor block-IHT, we project $z^0$ onto the set of vectors that are non-zero on at most one block from the partition $\\mathcal{B} = \\{B_{1}, B_{2}\\}$, where $B_{1} = \\{1,2\\}$ and $B_{2} = \\{3,4\\}$. The projection rule is to retain the block with the largest block-$\\ell_{2}$ norm and set all other blocks to zero.\n\nLet $z^0_{B_i}$ denote the subvector of $z^0$ corresponding to the indices in block $B_i$.\nFor block $B_{1} = \\{1,2\\}$, we have $z^0_{B_1} = \\begin{pmatrix} 1 \\\\ -1/2 \\end{pmatrix}$.\nFor block $B_{2} = \\{3,4\\}$, we have $z^0_{B_2} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\nNext, we compute the $\\ell_{2}$ norm of each block subvector:\n$$\\|z^0_{B_1}\\|_{2} = \\sqrt{1^2 + \\left(-\\frac{1}{2}\\right)^2} = \\sqrt{1 + \\frac{1}{4}} = \\sqrt{\\frac{5}{4}} = \\frac{\\sqrt{5}}{2}.$$\n$$\\|z^0_{B_2}\\|_{2} = \\sqrt{0^2 + 0^2} = 0.$$\nSince $\\|z^0_{B_1}\\|_{2} > \\|z^0_{B_2}\\|_{2}$, the projection keeps block $B_1$ and nullifies block $B_2$. The resulting iterate is $x^{1,\\mathrm{B}}$:\n$$x^{1,\\mathrm{B}} = \\begin{pmatrix}1 \\\\ -1/2 \\\\ 0 \\\\ 0\\end{pmatrix}.$$\nThe error for the block-IHT iterate is $e^{1,\\mathrm{B}} = x^{1,\\mathrm{B}} - x^{\\star}$:\n$$e^{1,\\mathrm{B}} = \\begin{pmatrix}1 \\\\ -1/2 \\\\ 0 \\\\ 0\\end{pmatrix} - \\begin{pmatrix}2 \\\\ -1 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}-1 \\\\ 1/2 \\\\ 0 \\\\ 0\\end{pmatrix}.$$\nThe squared $\\ell_{2}$ norm of this error is:\n$$\\|e^{1,\\mathrm{B}}\\|_{2}^{2} = (-1)^2 + \\left(\\frac{1}{2}\\right)^2 + 0^2 + 0^2 = 1 + \\frac{1}{4} = \\frac{5}{4}.$$\n\n**2. Elementwise Iterative Hard Thresholding (IHT)**\nFor elementwise IHT, we project $z^0$ onto the set of vectors with at most one non-zero entry. The projection, known as hard thresholding, keeps the entry with the largest magnitude and sets all other entries to zero.\nOur vector is $z^{0} = \\begin{pmatrix}1 \\\\ -1/2 \\\\ 0 \\\\ 0\\end{pmatrix}$.\nThe magnitudes of the components are $|1| = 1$, $|-1/2| = 1/2$, $|0|=0$, and $|0|=0$. The largest magnitude is $1$, corresponding to the first entry.\nThus, the projection retains the first component of $z^0$ and sets the rest to zero. The resulting iterate is $x^{1,\\mathrm{E}}$:\n$$x^{1,\\mathrm{E}} = \\begin{pmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{pmatrix}.$$\nThe error for the elementwise IHT iterate is $e^{1,\\mathrm{E}} = x^{1,\\mathrm{E}} - x^{\\star}$:\n$$e^{1,\\mathrm{E}} = \\begin{pmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{pmatrix} - \\begin{pmatrix}2 \\\\ -1 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}-1 \\\\ 1 \\\\ 0 \\\\ 0\\end{pmatrix}.$$\nThe squared $\\ell_{2}$ norm of this error is:\n$$\\|e^{1,\\mathrm{E}}\\|_{2}^{2} = (-1)^2 + 1^2 + 0^2 + 0^2 = 1 + 1 = 2.$$\n\n**3. Compute the Ratio**\nFinally, we compute the ratio $r$:\n$$r = \\frac{\\|e^{1,\\mathrm{B}}\\|_{2}^{2}}{\\|e^{1,\\mathrm{E}}\\|_{2}^{2}} = \\frac{5/4}{2} = \\frac{5}{8}.$$\nThis specific instance demonstrates that for certain signal structures, block-sparse recovery methods can yield smaller errors compared to elementwise sparse methods, as evidenced by $r < 1$.", "answer": "$$\\boxed{\\frac{5}{8}}$$", "id": "3438866"}]}