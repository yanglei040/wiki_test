{"hands_on_practices": [{"introduction": "The Restricted Isometry Property (RIP) is fundamental to understanding why algorithms like CoSaMP and SP can succeed. This first exercise provides a hands-on look at the consequences of violating this property in the most extreme way. By constructing a simple matrix with duplicated columns, you will directly calculate its RIP constant and prove that unique sparse recovery is impossible, thereby building a strong intuition for the necessity of RIP-based guarantees. [@problem_id:3473250]", "problem": "Let $k \\in \\mathbb{N}$ and let $m \\geq 4k$. Consider a matrix $\\boldsymbol{B} \\in \\mathbb{R}^{m \\times 4k}$ with orthonormal columns, so that $\\boldsymbol{B}^{\\top}\\boldsymbol{B} = \\boldsymbol{I}_{4k}$. Construct the measurement matrix\n$$\n\\boldsymbol{A} \\triangleq \\begin{bmatrix}\\boldsymbol{B}  \\boldsymbol{B}\\end{bmatrix} \\in \\mathbb{R}^{m \\times 8k},\n$$\ni.e., $\\boldsymbol{A}$ consists of two identical copies of the columns of $\\boldsymbol{B}$. Let $\\delta_{s}(\\boldsymbol{A})$ denote the restricted isometry constant (RIC) of order $s$, defined as the smallest $\\delta \\geq 0$ such that for all $s$-sparse vectors $\\boldsymbol{z} \\in \\mathbb{R}^{8k}$,\n$$\n(1-\\delta)\\|\\boldsymbol{z}\\|_{2}^{2} \\leq \\|\\boldsymbol{A}\\boldsymbol{z}\\|_{2}^{2} \\leq (1+\\delta)\\|\\boldsymbol{z}\\|_{2}^{2}.\n$$\nLet $\\tau \\in (0,1)$ denote any fixed sufficient-condition threshold for a greedy sparse recovery algorithm such as Compressive Sampling Matching Pursuit (CoSaMP) or Subspace Pursuit (SP), in the sense that the algorithm has a known correctness guarantee whenever $\\delta_{4k}(\\boldsymbol{A}) \\leq \\tau$. Define $\\varepsilon \\triangleq 1 - \\tau  0$.\n\nTasks:\n- Using only the definition of the restricted isometry property and basic linear algebra, compute the exact value of $\\delta_{4k}(\\boldsymbol{A})$.\n- Briefly justify, from first principles, why there exist two distinct $k$-sparse vectors $\\boldsymbol{x}, \\boldsymbol{x}' \\in \\mathbb{R}^{8k}$ with $\\boldsymbol{A}\\boldsymbol{x} = \\boldsymbol{A}\\boldsymbol{x}'$ despite noiseless measurements, thereby demonstrating that exact recovery can fail when $\\delta_{4k}(\\boldsymbol{A}) = \\tau + \\varepsilon$.\n\nAnswer specification:\n- Your final answer must be only the value of $\\delta_{4k}(\\boldsymbol{A})$ as a single exact real number with no units.", "solution": "We proceed from the definition of the restricted isometry property (RIP) and elementary spectral analysis of Gram matrices.\n\nStep 1: Structure of the measurement matrix and its Gram form on supports of size at most $4k$.\n\nBy construction, $\\boldsymbol{A} = [\\boldsymbol{B}\\ \\boldsymbol{B}]$ with $\\boldsymbol{B}^{\\top}\\boldsymbol{B} = \\boldsymbol{I}_{4k}$. For any index set $\\mathcal{S} \\subset \\{1,2,\\dots,8k\\}$, denote by $\\boldsymbol{A}_{\\mathcal{S}}$ the submatrix formed by selecting the columns of $\\boldsymbol{A}$ indexed by $\\mathcal{S}$. The restricted Gram matrix on $\\mathcal{S}$ is\n$$\n\\boldsymbol{G}_{\\mathcal{S}} \\triangleq \\boldsymbol{A}_{\\mathcal{S}}^{\\top}\\boldsymbol{A}_{\\mathcal{S}}.\n$$\nBecause the columns of $\\boldsymbol{A}$ consist of two identical copies of the columns of $\\boldsymbol{B}$, the structure of $\\boldsymbol{G}_{\\mathcal{S}}$ depends on how $\\mathcal{S}$ picks columns from the first or second copy.\n\nIf $\\mathcal{S}$ contains no pair of duplicated columns (i.e., never contains both the $j$-th column of the first copy and the $j$-th column of the second copy), then the selected columns are orthonormal: $\\boldsymbol{G}_{\\mathcal{S}} = \\boldsymbol{I}_{|\\mathcal{S}|}$.\n\nIf $\\mathcal{S}$ contains, for some index $j \\in \\{1,\\dots,4k\\}$, both copies of column $j$, then the $2 \\times 2$ principal submatrix of $\\boldsymbol{G}_{\\mathcal{S}}$ corresponding to those two columns equals\n$$\n\\begin{bmatrix}\n1  1\\\\\n1  1\n\\end{bmatrix},\n$$\nbecause the two columns are identical and of unit norm. This $2 \\times 2$ block has eigenvalues $\\lambda_{+} = 2$ and $\\lambda_{-} = 0$ with corresponding orthonormal eigenvectors $\\frac{1}{\\sqrt{2}}[1,1]^{\\top}$ and $\\frac{1}{\\sqrt{2}}[1,-1]^{\\top}$, respectively.\n\nSince any support $\\mathcal{S}$ with at most $4k$ columns can include at most $4k$ indices, it is possible to select a support $\\mathcal{S}$ of size $2$ that contains a duplicated pair for some $j$. Therefore, to compute $\\delta_{4k}(\\boldsymbol{A})$, we must consider the worst-case deviation of $\\boldsymbol{G}_{\\mathcal{S}}$ from the identity among all $|\\mathcal{S}| \\leq 4k$.\n\nStep 2: Restricted isometry constant via extremal eigenvalues.\n\nBy definition of the restricted isometry constant, for any $s$-sparse vector $\\boldsymbol{z}$ supported on $\\mathcal{S}$ with $|\\mathcal{S}| \\leq s$, we have\n$$\n\\|\\boldsymbol{A}\\boldsymbol{z}\\|_{2}^{2} = \\boldsymbol{z}_{\\mathcal{S}}^{\\top}\\boldsymbol{G}_{\\mathcal{S}} \\boldsymbol{z}_{\\mathcal{S}},\n$$\nand the smallest $\\delta_{s}$ is\n$$\n\\delta_{s}(\\boldsymbol{A}) = \\max_{|\\mathcal{S}| \\leq s} \\max\\left\\{\\, 1 - \\lambda_{\\min}(\\boldsymbol{G}_{\\mathcal{S}}),\\ \\lambda_{\\max}(\\boldsymbol{G}_{\\mathcal{S}}) - 1 \\,\\right\\}.\n$$\nFrom Step 1, if $\\mathcal{S}$ contains a duplicated pair, the $2 \\times 2$ block contributes eigenvalues $0$ and $2$. As $\\boldsymbol{G}_{\\mathcal{S}}$ is positive semidefinite and block-embedded into the larger Gram matrix, its extremal eigenvalues satisfy\n$$\n\\lambda_{\\min}(\\boldsymbol{G}_{\\mathcal{S}}) \\leq 0, \\qquad \\lambda_{\\max}(\\boldsymbol{G}_{\\mathcal{S}}) \\geq 2.\n$$\nConversely, if $\\mathcal{S}$ contains no duplicated pair, $\\boldsymbol{G}_{\\mathcal{S}} = \\boldsymbol{I}$ and both extremal eigenvalues equal $1$.\n\nHence, the worst-case deviation occurs when $\\mathcal{S}$ contains a duplicated pair, yielding\n$$\n1 - \\lambda_{\\min}(\\boldsymbol{G}_{\\mathcal{S}}) = 1 - 0 = 1, \\qquad \\lambda_{\\max}(\\boldsymbol{G}_{\\mathcal{S}}) - 1 = 2 - 1 = 1.\n$$\nTherefore,\n$$\n\\delta_{4k}(\\boldsymbol{A}) = 1.\n$$\n\nStep 3: Demonstration of exact recovery failure with noiseless measurements.\n\nWe now exhibit two distinct $k$-sparse vectors $\\boldsymbol{x}, \\boldsymbol{x}' \\in \\mathbb{R}^{8k}$ that produce the same noiseless measurement. Let $\\mathcal{T} \\subset \\{1,\\dots,4k\\}$ be any set of size $k$. Define $\\boldsymbol{x} \\in \\mathbb{R}^{8k}$ by placing arbitrary nonzero coefficients $\\{c_{j}\\}_{j \\in \\mathcal{T}}$ on the indices in the first copy at positions $\\{j\\}_{j \\in \\mathcal{T}}$ and zeros elsewhere; i.e.,\n$$\nx_{j} = c_{j} \\text{ for } j \\in \\mathcal{T}, \\quad x_{\\ell} = 0 \\text{ otherwise}.\n$$\nDefine $\\boldsymbol{x}' \\in \\mathbb{R}^{8k}$ by placing the same coefficients on the duplicated indices in the second copy and zeros elsewhere; i.e.,\n$$\nx'_{4k + j} = c_{j} \\text{ for } j \\in \\mathcal{T}, \\quad x'_{\\ell} = 0 \\text{ otherwise}.\n$$\nThen both are $k$-sparse and\n$$\n\\boldsymbol{A}\\boldsymbol{x} = \\boldsymbol{B}\\boldsymbol{c} + \\boldsymbol{B}\\boldsymbol{0} = \\boldsymbol{B}\\boldsymbol{c}, \\qquad \\boldsymbol{A}\\boldsymbol{x}' = \\boldsymbol{B}\\boldsymbol{0} + \\boldsymbol{B}\\boldsymbol{c} = \\boldsymbol{B}\\boldsymbol{c},\n$$\nwhere $\\boldsymbol{c} \\in \\mathbb{R}^{4k}$ collects the coefficients $c_{j}$ at indices $j \\in \\mathcal{T}$ and zeros elsewhere. Hence $\\boldsymbol{A}\\boldsymbol{x} = \\boldsymbol{A}\\boldsymbol{x}'$ with $\\boldsymbol{x} \\neq \\boldsymbol{x}'$, even though there is no measurement noise. This demonstrates that exact recovery of a $k$-sparse vector can fail: there is no algorithmic rule that can distinguish between $\\boldsymbol{x}$ and $\\boldsymbol{x}'$ based solely on $\\boldsymbol{A}\\boldsymbol{x}$.\n\nStep 4: Relation to the threshold and $\\varepsilon$.\n\nLet $\\tau \\in (0,1)$ be any sufficient-condition threshold for a chosen greedy algorithm. The computed value $\\delta_{4k}(\\boldsymbol{A}) = 1$ satisfies\n$$\n\\delta_{4k}(\\boldsymbol{A}) = \\tau + \\varepsilon \\quad \\text{with} \\quad \\varepsilon = 1 - \\tau  0.\n$$\nThus, the restricted isometry constant exceeds the threshold by a strictly positive $\\varepsilon$, and we have explicitly shown that exact recovery can fail, even with noiseless measurements.\n\nThe requested value is $\\delta_{4k}(\\boldsymbol{A})$, which is a single exact real number.", "answer": "$$\\boxed{1}$$", "id": "3473250"}, {"introduction": "While RIP provides uniform recovery guarantees, the practical performance of a greedy algorithm on a specific signal can reveal subtle differences in their design. This exercise presents a scenario where Orthogonal Matching Pursuit (OMP) successfully identifies the true signal support, while Subspace Pursuit (SP) fails. By carefully tracing the steps of each algorithm, you will uncover how SP's batch identification strategy can be misled in situations where OMP's one-at-a-time selection prevails. [@problem_id:3473283]", "problem": "Consider the linear model $y = A x^{\\star} + e$ with sensing matrix $A \\in \\mathbb{R}^{m \\times n}$, unknown $k$-sparse signal $x^{\\star} \\in \\mathbb{R}^{n}$ with support $S^{\\star}$, and noise $e \\in \\mathbb{R}^{m}$. The Restricted Isometry Property (RIP) of order $s$ with constant $\\delta_s$ for a matrix $A$ is defined by the requirement that for all $s$-sparse vectors $z$, one has $(1-\\delta_s)\\|z\\|_2^2 \\le \\|A z\\|_2^2 \\le (1+\\delta_s)\\|z\\|_2^2$. The mutual coherence $\\mu(A)$ is defined as $\\mu(A) = \\max_{i \\neq j} |a_i^\\top a_j|$ for unit-norm columns $\\{a_i\\}$ of $A$.\n\nOrthogonal Matching Pursuit (OMP) selects at each iteration the single column of $A$ that maximizes the absolute correlation with the current residual and then orthogonally projects the measurements onto the span of the selected columns. Subspace Pursuit (SP) maintains a support estimate of size $k$; at each iteration, it expands the support by taking the indices corresponding to the $k$ largest correlations of the residual (forming a candidate set of size at most $2k$), solves a least-squares problem on this candidate set, and then prunes back to the $k$ largest entries of the solution vector, updating the residual accordingly. In noiseless settings, SP typically terminates when the residual becomes zero.\n\nYou are given the following four candidate instances $(A, x^{\\star}, e)$, all with $m=3$, $n=4$, $k=2$, columns of $A$ normalized to unit $\\ell_2$-norm, and with $e=0$. In each instance, $A = [a_1, a_2, a_3, a_4]$ and $x^{\\star} \\in \\mathbb{R}^4$ is specified. Determine which instance exhibits the phenomenon that OMP exactly recovers the true support $S^{\\star}$ in exactly $k$ iterations, but SP fails by returning a different support (still achieving zero residual), under the same $A$ and $e$. Your reasoning should be grounded in the definitions above and should also explain how the example is consistent with known RIP-style uniform recovery guarantees for SP.\n\nOption A:\n- $a_1 = [1,0,0]^\\top$, $a_2 = [0,1,0]^\\top$, $a_3 = \\dfrac{1}{\\sqrt{26}}[1,5,0]^\\top$, $a_4 = [0,0,1]^\\top$.\n- $x^{\\star} = [1,\\,0.8,\\,0,\\,0]^\\top$, so $S^{\\star} = \\{1,2\\}$.\n- $e = 0$.\n\nOption B:\n- $a_1 = [1,0,0]^\\top$, $a_2 = [0,1,0]^\\top$, $a_3 = \\dfrac{1}{\\sqrt{1.04}}[1,0.2,0]^\\top$, $a_4 = [0,0,1]^\\top$.\n- $x^{\\star} = [1,\\,0.8,\\,0,\\,0]^\\top$, so $S^{\\star} = \\{1,2\\}$.\n- $e = 0$.\n\nOption C:\n- $a_1 = [1,0,0]^\\top$, $a_2 = [0,1,0]^\\top$, $a_3 = \\dfrac{1}{\\sqrt{2}}[1,0,1]^\\top$, $a_4 = [0,0,1]^\\top$.\n- $x^{\\star} = [1,\\,0.8,\\,0,\\,0]^\\top$, so $S^{\\star} = \\{1,2\\}$.\n- $e = 0$.\n\nOption D:\n- $a_1 = [1,0,0]^\\top$, $a_2 = [0,1,0]^\\top$, $a_3 = \\dfrac{1}{\\sqrt{26}}[1,-5,0]^\\top$, $a_4 = [0,0,1]^\\top$.\n- $x^{\\star} = [1,\\,0.8,\\,0,\\,0]^\\top$, so $S^{\\star} = \\{1,2\\}$.\n- $e = 0$.\n\nChoices:\nA. Only Option A exhibits OMP success but SP failure under the same $A$ and $e$.\n\nB. Only Option B exhibits OMP success but SP failure under the same $A$ and $e$.\n\nC. Only Option C exhibits OMP success but SP failure under the same $A$ and $e$.\n\nD. Only Option D exhibits OMP success but SP failure under the same $A$ and $e$.", "solution": "The problem statement is a valid exercise in the analysis of sparse recovery algorithms. It is scientifically grounded in the principles of compressed sensing, well-posed with a unique answer expected among the choices, and is stated objectively using standard mathematical definitions. All necessary data to perform the analysis are provided. We may therefore proceed with the solution.\n\nThe problem asks to identify an instance from four options where Orthogonal Matching Pursuit (OMP) successfully recovers the true support $S^{\\star}$ in $k=2$ iterations, while Subspace Pursuit (SP) fails by returning a different support. The setting is noiseless ($e=0$).\n\nFor all four options, the parameters are:\n- Dimensions: $m=3$, $n=4$\n- Sparsity: $k=2$\n- True signal: $x^{\\star} = [1, 0.8, 0, 0]^\\top$\n- True support: $S^{\\star} = \\{1, 2\\}$\n- Columns of $A$ are normalized to unit $\\ell_2$-norm.\n- The measurement vector $y$ is given by $y = A x^{\\star} = a_1 x_1^{\\star} + a_2 x_2^{\\star}$. Since for all options $a_1 = [1,0,0]^\\top$ and $a_2 = [0,1,0]^\\top$, the measurement vector is constant across all instances:\n$$y = 1 \\cdot \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + 0.8 \\cdot \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0.8 \\\\ 0 \\end{pmatrix}$$\n\nBoth OMP and SP begin by computing the correlations of the columns of $A$ with the initial residual, which is $r_0 = y$. Let $c_i = a_i^\\top r_0 = a_i^\\top y$. For all options, we have:\n- $c_1 = a_1^\\top y = [1,0,0] [1, 0.8, 0]^\\top = 1$.\n- $c_2 = a_2^\\top y = [0,1,0] [1, 0.8, 0]^\\top = 0.8$.\n- The problem specifies $a_4 = [0,0,1]^\\top$ for all options, so $c_4 = a_4^\\top y = [0,0,1] [1, 0.8, 0]^\\top = 0$.\nThe analysis will hinge on the value of $c_3 = a_3^\\top y$ for each option.\n\n### Analysis of Option A\n\n- **Givens**: $a_3 = \\frac{1}{\\sqrt{26}}[1,5,0]^\\top$.\n- **Initial Correlation**: $c_3 = a_3^\\top y = \\frac{1}{\\sqrt{26}}[1,5,0] [1, 0.8, 0]^\\top = \\frac{1(1) + 5(0.8)}{\\sqrt{26}} = \\frac{5}{\\sqrt{26}}$.\nNumerically, $|c_3| = \\frac{5}{\\sqrt{26}} \\approx 0.9806$.\nThe magnitudes of the initial correlations are ordered as: $|c_1| = 1  |c_3| \\approx 0.9806  |c_2| = 0.8  |c_4| = 0$.\n\n- **OMP Simulation**:\n  1. **Iteration 1**: The largest correlation is $|c_1|=1$. OMP selects index $i_1 = 1$. The new support estimate is $S_1 = \\{1\\}$. The residual is updated by projecting $y$ onto the span of $a_1$:\n  $r_1 = y - a_1 (a_1^\\top y) = [1, 0.8, 0]^\\top - [1,0,0]^\\top(1) = [0, 0.8, 0]^\\top$.\n  2. **Iteration 2**: We compute correlations with the new residual $r_1$:\n     - $|\\langle a_2, r_1 \\rangle| = |[0,1,0] [0, 0.8, 0]^\\top| = 0.8$.\n     - $|\\langle a_3, r_1 \\rangle| = |\\frac{1}{\\sqrt{26}}[1,5,0] [0, 0.8, 0]^\\top| = |\\frac{5(0.8)}{\\sqrt{26}}| = \\frac{4}{\\sqrt{26}} \\approx 0.7845$.\n     - $|\\langle a_4, r_1 \\rangle| = |[0,0,1] [0, 0.8, 0]^\\top| = 0$.\n  The largest correlation is $0.8$, corresponding to index $i_2 = 2$. OMP adds this to the support: $S_2 = \\{1, 2\\}$.\n- **OMP Verdict**: OMP terminates after $k=2$ steps with the correct support $S_2 = S^{\\star}$. Thus, **OMP succeeds**.\n\n- **SP Simulation**:\n  1. **Iteration 1**:\n     - **Identify**: SP identifies the $k=2$ indices with the largest initial correlations. From our ordering, these are indices $1$ and $3$. The candidate set is $T_{cand} = \\{1, 3\\}$.\n     - **Merge**: The candidate support is $S_{cand} = S_0 \\cup T_{cand} = \\{1, 3\\}$.\n     - **Least Squares**: SP solves the least-squares problem $\\min_z \\|y - A_{S_{cand}} z\\|_2^2$. Here, $A_{S_{cand}} = [a_1, a_3] = \\begin{pmatrix} 1  1/\\sqrt{26} \\\\ 0  5/\\sqrt{26} \\\\ 0  0 \\end{pmatrix}$. The solution $z = [z_1, z_3]^\\top$ is found by solving $A_{S_{cand}}z = y$:\n     $$ \\begin{pmatrix} 1  1/\\sqrt{26} \\\\ 0  5/\\sqrt{26} \\end{pmatrix} \\begin{pmatrix} z_1 \\\\ z_3 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0.8 \\end{pmatrix} $$\n     From the second row, $(5/\\sqrt{26}) z_3 = 0.8 \\implies z_3 = 0.8 \\sqrt{26} / 5 = 0.16\\sqrt{26}$.\n     From the first row, $z_1 + (1/\\sqrt{26})z_3 = 1 \\implies z_1 = 1 - 0.16 = 0.84$.\n     So, the temporary signal vector is non-zero only on $\\{1,3\\}$, with values $z_1 = 0.84$ and $z_3 = 0.16\\sqrt{26} \\approx 0.8158$.\n     - **Prune**: SP prunes the support $S_{cand}$ to the $k=2$ largest-magnitude entries of $z$. Since there are only two entries, the new support estimate is $S_1=\\{1, 3\\}$.\n     - **Update Residual**: $r_1 = y - A_{S_1} z$. Since $A_{S_1}z$ exactly equals $y$, the residual $r_1$ is the zero vector.\n- **SP Verdict**: SP terminates in one iteration, as the residual is zero. It returns the support $S_1=\\{1, 3\\}$, which is not the true support $S^{\\star}=\\{1, 2\\}$. Thus, **SP fails**.\n\n- **Conclusion**: Option A exhibits the specified behavior: OMP succeeds, and SP fails.\n\n### Analysis of Other Options\n\n- **Option B**: $a_3 = \\frac{1}{\\sqrt{1.04}}[1,0.2,0]^\\top$.\n  $c_3 = a_3^\\top y = \\frac{1}{\\sqrt{1.04}}(1(1) + 0.2(0.8)) = \\frac{1.16}{\\sqrt{1.04}} \\approx 1.137$.\n  The initial correlations are ordered $|c_3| \\approx 1.137  |c_1|=1  |c_2|=0.8  |c_4|=0$.\n  OMP's first step selects index $3$, which is not in the true support $S^{\\star}=\\{1,2\\}$. Therefore, OMP fails to recover $S^{\\star}$. This option is incorrect.\n\n- **Option C**: $a_3 = \\frac{1}{\\sqrt{2}}[1,0,1]^\\top$.\n  $c_3 = a_3^\\top y = \\frac{1}{\\sqrt{2}}(1(1) + 0(0.8) + 1(0)) = \\frac{1}{\\sqrt{2}} \\approx 0.707$.\n  The initial correlations are ordered $|c_1|=1  |c_2|=0.8  |c_3|\\approx0.707  |c_4|=0$.\n  **OMP**: Will select index $1$, then index $2$. OMP succeeds.\n  **SP**: In its identification step, it selects the top $k=2$ correlations, which correspond to indices $1$ and $2$. SP correctly identifies the support in the first iteration. SP succeeds.\n  This option is incorrect as both algorithms succeed.\n\n- **Option D**: $a_3 = \\frac{1}{\\sqrt{26}}[1,-5,0]^\\top$.\n  $c_3 = a_3^\\top y = \\frac{1}{\\sqrt{26}}(1(1) - 5(0.8)) = \\frac{1-4}{\\sqrt{26}} = \\frac{-3}{\\sqrt{26}}$.\n  $|c_3| = \\frac{3}{\\sqrt{26}} \\approx 0.588$.\n  The initial correlations are ordered $|c_1|=1  |c_2|=0.8  |c_3|\\approx0.588  |c_4|=0$.\n  Similar to Option C, both OMP and SP will select indices $1$ and $2$ first and will successfully recover the true support. This option is incorrect.\n\n### Consistency with Recovery Guarantees\n\nThe failure of SP in Option A is consistent with known recovery guarantees, which typically rely on the Restricted Isometry Property (RIP). A common sufficient condition for SP to guarantee recovery of any $k$-sparse signal is that the RIP constant $\\delta_{2k}$ must be sufficiently small (e.g., $\\delta_{2k}  \\sqrt{2}-1$).\nIn our case, $k=2$, so the condition involves $\\delta_4$. The sensing matrix $A$ is in $\\mathbb{R}^{3 \\times 4}$. Any set of $4$ vectors in $\\mathbb{R}^3$ must be linearly dependent. This means there exists a non-zero vector $z \\in \\mathbb{R}^4$ (which is $4$-sparse) such that $Az=0$. The RIP definition states $(1-\\delta_4)\\|z\\|_2^2 \\le \\|Az\\|_2^2$. For this $z$, we have $(1-\\delta_4)\\|z\\|_2^2 \\le 0$. Since $\\|z\\|_2^2  0$, this forces $1-\\delta_4 \\le 0$, or $\\delta_4 \\ge 1$.\nThe RIP constant $\\delta_4$ does not satisfy the condition $\\delta_4  c$ for any $c1$. Therefore, the uniform recovery guarantees for SP do not apply to this matrix $A$. The existence of an instance $(x^{\\star}, e)$ where SP fails is thus entirely consistent with the theory. The failure is caused by the high mutual coherence between columns (e.g., $|a_2^\\top a_3| = 5/\\sqrt{26} \\approx 0.98$), which leads to a \"conspiracy\" where a false support can explain the measurements.\n\nBased on the exhaustive analysis, only Option A demonstrates the phenomenon where OMP succeeds while SP fails under the given conditions.", "answer": "$$\\boxed{A}$$", "id": "3473283"}, {"introduction": "Theoretical guarantees involving RIP constants are powerful but often abstract, as these constants are computationally hard to verify. A crucial skill is translating these abstract conditions into more practical and computable metrics, such as mutual coherence. This final practice guides you through the process of converting the known RIP-based recovery conditions for CoSaMP and SP into explicit bounds on signal sparsity based on the matrix's mutual coherence, offering a concrete tool for comparing their performance. [@problem_id:3473289]", "problem": "Consider a sensing matrix $ \\Phi \\in \\mathbb{R}^{m \\times n} $ with unit-norm columns used to measure a $k$-sparse signal $x \\in \\mathbb{R}^{n}$ via $y = \\Phi x + e$, where $e$ is measurement noise. Let the Restricted Isometry Property (RIP) constant of order $s$ be denoted by $ \\delta_{s} $, defined as the smallest nonnegative number such that for all $s$-sparse vectors $z$, one has $ (1 - \\delta_{s}) \\|z\\|_{2}^{2} \\le \\|\\Phi z\\|_{2}^{2} \\le (1 + \\delta_{s}) \\|z\\|_{2}^{2} $. Let the mutual coherence of $ \\Phi $ be $ \\mu := \\max_{i \\ne j} |\\langle \\phi_{i}, \\phi_{j} \\rangle| $, where $ \\phi_{i} $ denotes the $i$-th column of $ \\Phi $. It is known that for deterministic matrices with unit-norm columns, a general upper bound of the form $ \\delta_{s} \\le (s-1)\\mu $ holds.\n\nSuppose that the Compressive Sampling Matching Pursuit (CoSaMP) algorithm is guaranteed to stably recover any $k$-sparse signal whenever the RIP condition $ \\delta_{4k} \\le \\alpha $ holds, and that the Subspace Pursuit (SP) algorithm is guaranteed to stably recover any $k$-sparse signal whenever the RIP condition $ \\delta_{3k} \\le \\beta $ holds, where $ \\alpha \\in (0,1) $ and $ \\beta \\in (0,1) $ are fixed constants independent of $ \\mu $ and $k$.\n\nUsing only the above definitions and the bound $ \\delta_{s} \\le (s-1)\\mu $, translate these RIP-based guarantees into sufficient mutual coherence conditions that upper bound the admissible sparsity levels $k$ for CoSaMP and SP as explicit functions of $ \\mu, \\alpha, \\beta $. Then, to quantify the relative conservativeness of these coherence-based conditions in the regime of highly incoherent matrices (i.e., $ \\mu \\to 0 $), compute the limit of the ratio of the maximal allowable sparsities for SP to that for CoSaMP as $ \\mu \\to 0 $. Your final answer must be a single closed-form analytic expression in terms of $ \\alpha $ and $ \\beta $ only. Do not include units. Do not provide inequalities. Do not round.", "solution": "The user wants me to validate the problem statement and, if valid, provide a solution.\n\n### Step 1: Extract Givens\n-   Sensing matrix: $ \\Phi \\in \\mathbb{R}^{m \\times n} $ with unit-norm columns $ \\phi_{i} $.\n-   Signal model: $ y = \\Phi x + e $, where $ x \\in \\mathbb{R}^{n} $ is a $k$-sparse signal and $ e $ is measurement noise.\n-   Restricted Isometry Property (RIP) constant of order $ s $: $ \\delta_{s} $ is the smallest number such that $ (1 - \\delta_{s}) \\|z\\|_{2}^{2} \\le \\|\\Phi z\\|_{2}^{2} \\le (1 + \\delta_{s}) \\|z\\|_{2}^{2} $ for all $s$-sparse vectors $ z $.\n-   Mutual coherence: $ \\mu := \\max_{i \\ne j} |\\langle \\phi_{i}, \\phi_{j} \\rangle| $.\n-   Relationship between RIP and coherence: $ \\delta_{s} \\le (s-1)\\mu $.\n-   CoSaMP recovery guarantee: Holds if $ \\delta_{4k} \\le \\alpha $, where $ \\alpha \\in (0,1) $.\n-   SP recovery guarantee: Holds if $ \\delta_{3k} \\le \\beta $, where $ \\beta \\in (0,1) $.\n-   Task 1: Derive sufficient coherence-based conditions that upper bound the admissible sparsity $k$ for CoSaMP and SP.\n-   Task 2: Compute $ \\lim_{\\mu \\to 0} \\frac{k_{SP}}{k_{C}} $, where $k_C$ and $k_{SP}$ are the maximal allowable sparsities for CoSaMP and SP, respectively, derived from the coherence-based conditions. The final answer should be in terms of $ \\alpha $ and $ \\beta $.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically grounded within the established theoretical framework of compressed sensing. All definitions ($ \\delta_s $, $ \\mu $) and relationships ($ \\delta_s \\le (s-1)\\mu $) are standard in the field. The recovery guarantees for CoSaMP and SP are presented in a standard, albeit general, form using symbolic constants $ \\alpha $ and $ \\beta $. The problem is well-posed, self-contained, and formalizable into a solvable mathematical question. It is free of contradictions, ambiguities, or factual errors. The tasks are clear and lead to a unique, meaningful solution.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the solution.\n\nThe solution process involves two main parts: first, deriving the upper bounds on sparsity $k$ for each algorithm based on mutual coherence, and second, computing the limit of the ratio of these bounds.\n\nPart 1: Derivation of Coherence-Based Sparsity Bounds\n\nFor the Compressive Sampling Matching Pursuit (CoSaMP) algorithm, the stated sufficient condition for stable recovery is given by the RIP-based inequality:\n$$ \\delta_{4k} \\le \\alpha $$\nWe are given the general bound that relates the RIP constant $ \\delta_s $ to the mutual coherence $ \\mu $:\n$$ \\delta_{s} \\le (s-1)\\mu $$\nTo find a sufficient condition based on mutual coherence that guarantees the CoSaMP recovery condition, we can substitute $ s = 4k $ into this bound:\n$$ \\delta_{4k} \\le (4k-1)\\mu $$\nTherefore, if we enforce the condition $ (4k-1)\\mu \\le \\alpha $, it will imply that $ \\delta_{4k} \\le \\alpha $ is satisfied. This gives us a sufficient condition based on $ \\mu $. We can now solve for the maximal allowable sparsity level, which we denote by $ k_C $, that satisfies this condition.\n$$ (4k_C - 1)\\mu \\le \\alpha $$\nAssuming $ \\mu  0 $, we can rearrange the inequality:\n$$ 4k_C - 1 \\le \\frac{\\alpha}{\\mu} $$\n$$ 4k_C \\le \\frac{\\alpha}{\\mu} + 1 $$\n$$ k_C \\le \\frac{1}{4} \\left( \\frac{\\alpha}{\\mu} + 1 \\right) $$\nThis expression represents the upper bound on the sparsity level for CoSaMP under the coherence-based condition. The maximal allowable sparsity scales as $ k_C(\\mu) = \\frac{1}{4} \\left( \\frac{\\alpha}{\\mu} + 1 \\right) $.\n\nSimilarly, for the Subspace Pursuit (SP) algorithm, the sufficient condition for stable recovery is given by:\n$$ \\delta_{3k} \\le \\beta $$\nUsing the same bound $ \\delta_s \\le (s-1)\\mu $, we substitute $ s = 3k $:\n$$ \\delta_{3k} \\le (3k-1)\\mu $$\nA sufficient condition based on mutual coherence for SP recovery is thus $ (3k-1)\\mu \\le \\beta $. We solve for the maximal allowable sparsity level, denoted by $ k_{SP} $:\n$$ (3k_{SP} - 1)\\mu \\le \\beta $$\n$$ 3k_{SP} - 1 \\le \\frac{\\beta}{\\mu} $$\n$$ 3k_{SP} \\le \\frac{\\beta}{\\mu} + 1 $$\n$$ k_{SP} \\le \\frac{1}{3} \\left( \\frac{\\beta}{\\mu} + 1 \\right) $$\nThe maximal allowable sparsity for SP under this condition scales as $ k_{SP}(\\mu) = \\frac{1}{3} \\left( \\frac{\\beta}{\\mu} + 1 \\right) $.\n\nPart 2: Computation of the Limit of the Ratio\n\nThe problem requires us to compute the limit of the ratio of the maximal allowable sparsities for SP to CoSaMP in the regime of highly incoherent matrices, i.e., as $ \\mu \\to 0 $.\nThe ratio is given by:\n$$ \\frac{k_{SP}(\\mu)}{k_C(\\mu)} = \\frac{\\frac{1}{3} \\left( \\frac{\\beta}{\\mu} + 1 \\right)}{\\frac{1}{4} \\left( \\frac{\\alpha}{\\mu} + 1 \\right)} $$\nWe can simplify this expression:\n$$ \\frac{k_{SP}(\\mu)}{k_C(\\mu)} = \\frac{4}{3} \\cdot \\frac{\\frac{\\beta}{\\mu} + 1}{\\frac{\\alpha}{\\mu} + 1} $$\nTo evaluate the limit as $ \\mu \\to 0 $, we can multiply the numerator and denominator of the fraction by $ \\mu $:\n$$ \\frac{k_{SP}(\\mu)}{k_C(\\mu)} = \\frac{4}{3} \\cdot \\frac{\\mu \\left( \\frac{\\beta}{\\mu} + 1 \\right)}{\\mu \\left( \\frac{\\alpha}{\\mu} + 1 \\right)} = \\frac{4}{3} \\cdot \\frac{\\beta + \\mu}{\\alpha + \\mu} $$\nNow, we can take the limit as $ \\mu \\to 0 $. Since $ \\alpha \\in (0,1) $ and $ \\beta \\in (0,1) $, the function is continuous at $ \\mu = 0 $.\n$$ \\lim_{\\mu \\to 0} \\frac{k_{SP}(\\mu)}{k_C(\\mu)} = \\lim_{\\mu \\to 0} \\left( \\frac{4}{3} \\cdot \\frac{\\beta + \\mu}{\\alpha + \\mu} \\right) $$\nSubstituting $ \\mu = 0 $:\n$$ \\lim_{\\mu \\to 0} \\frac{k_{SP}(\\mu)}{k_C(\\mu)} = \\frac{4}{3} \\cdot \\frac{\\beta + 0}{\\alpha + 0} = \\frac{4\\beta}{3\\alpha} $$\nThis result quantifies the relative performance of the coherence-based recovery conditions for SP and CoSaMP in the limit of small mutual coherence.", "answer": "$$\\boxed{\\frac{4\\beta}{3\\alpha}}$$", "id": "3473289"}]}