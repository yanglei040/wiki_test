{"hands_on_practices": [{"introduction": "To truly understand an algorithm, there is no substitute for walking through its mechanics step-by-step. This first practice exercise guides you through a single iteration of Iterative Hard Thresholding (IHT) on a small-scale problem [@problem_id:3454156]. By manually calculating the gradient, determining the correct step size $\\mu$ from matrix properties, and applying the hard thresholding operator, you will gain a concrete understanding of the fundamental building blocks of IHT.", "problem": "Consider the sparse recovery objective with least-squares data fidelity given by $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$, where $A \\in \\mathbb{R}^{m \\times n}$ and $y \\in \\mathbb{R}^{m}$. Starting from the core definitions of the gradient of a composition and the norm-induced Lipschitz continuity of gradients for quadratic objectives, you will carry out one step of Iterative Hard Thresholding (IHT) on a concrete instance.\n\nLet $A \\in \\mathbb{R}^{2 \\times 3}$ and $y \\in \\mathbb{R}^{2}$ be defined as\n$$\nA = \\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1\n\\end{pmatrix}, \\qquad\ny = \\begin{pmatrix}\n1 \\\\\n2\n\\end{pmatrix}.\n$$\nLet the initial iterate be $x^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\in \\mathbb{R}^{3}$ and let the sparsity level be $k = 1$. The IHT step consists of a gradient descent step with a step size $ \\mu $ chosen using the Lipschitz constant of the gradient of $f(x)$, followed by the $k$-hard thresholding operator $H_{k}$, which retains the $k$ largest entries in magnitude and sets the rest to zero.\n\nTasks:\n- Starting from $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$, derive the gradient $\\nabla f(x)$ using first principles and compute $\\nabla f(x^{(0)})$.\n- Determine the Lipschitz constant $L$ of $\\nabla f$ from the structure of $A$, and choose the step size $\\mu = \\frac{1}{L}$.\n- Perform one IHT iterate $x^{(1)} = H_{k}\\!\\left(x^{(0)} - \\mu \\nabla f(x^{(0)})\\right)$.\n- Compute the scalar objective value $f\\!\\left(x^{(1)}\\right)$.\n\nProvide your final answer as a single exact real number. No rounding is required.", "solution": "The problem is valid as it is scientifically grounded in the field of sparse optimization, well-posed with all necessary information provided, and stated objectively. We proceed with the solution.\n\nThe objective function is given by $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$, where $A \\in \\mathbb{R}^{m \\times n}$, $x \\in \\mathbb{R}^{n}$, and $y \\in \\mathbb{R}^{m}$. This can be rewritten using the definition of the Euclidean norm as $f(x) = \\frac{1}{2}(Ax-y)^{T}(Ax-y)$.\n\nFirst, we derive the gradient $\\nabla f(x)$. Expanding the expression for $f(x)$:\n$$f(x) = \\frac{1}{2}(x^{T}A^{T} - y^{T})(Ax - y) = \\frac{1}{2}(x^{T}A^{T}Ax - x^{T}A^{T}y - y^{T}Ax + y^{T}y)$$\nThe scalar term $y^{T}Ax$ is equal to its transpose, $(y^{T}Ax)^{T} = x^{T}A^{T}y$. Thus, the expression simplifies to:\n$$f(x) = \\frac{1}{2}(x^{T}A^{T}Ax - 2y^{T}Ax + y^{T}y)$$\nTo find the gradient $\\nabla f(x)$, we differentiate with respect to the vector $x$. Using the standard matrix calculus identities $\\nabla_{x}(x^{T}Bx) = (B+B^{T})x$ for a square matrix $B$ and $\\nabla_{x}(c^{T}x) = c$ for a vector $c$, we proceed. The matrix $A^{T}A$ is symmetric, so $\\nabla_{x}(x^{T}A^{T}Ax) = 2A^{T}Ax$. The term $y^{T}Ax$ can be rewritten as $(A^{T}y)^{T}x$, and its gradient is $A^{T}y$. The term $y^{T}y$ is constant with respect to $x$.\nCombining these results, the gradient is:\n$$\\nabla f(x) = \\frac{1}{2}(2A^{T}Ax - 2A^{T}y) = A^{T}(Ax - y)$$\nThe problem provides $A = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix}$, $y = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$, and the initial iterate $x^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$. We compute the gradient at $x^{(0)}$:\n$$\\nabla f(x^{(0)}) = A^{T}(Ax^{(0)} - y) = A^{T}(A \\cdot 0 - y) = -A^{T}y$$\nThe transpose of $A$ is $A^{T} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}$.\n$$\\nabla f(x^{(0)}) = -\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = -\\begin{pmatrix} 1 \\cdot 1 + 0 \\cdot 2 \\\\ 0 \\cdot 1 + 1 \\cdot 2 \\\\ 1 \\cdot 1 + 1 \\cdot 2 \\end{pmatrix} = -\\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -2 \\\\ -3 \\end{pmatrix}$$\n\nSecond, we determine the Lipschitz constant $L$ of $\\nabla f(x)$. The gradient is $\\nabla f(x) = A^{T}Ax - A^{T}y$. The Hessian of $f(x)$ is $\\nabla^{2}f(x) = A^{T}A$. The Lipschitz constant $L$ for $\\nabla f(x)$ is the spectral norm of its Hessian, $L = \\|A^{T}A\\|_{2}$. For a positive semi-definite matrix like $A^{T}A$, the spectral norm is its largest eigenvalue, $L = \\lambda_{\\max}(A^{T}A)$. We compute $A^{T}A$:\n$$A^{T}A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 2 \\end{pmatrix}$$\nTo find the eigenvalues, we solve the characteristic equation $\\det(A^{T}A - \\lambda I) = 0$:\n$$\\det\\begin{pmatrix} 1-\\lambda & 0 & 1 \\\\ 0 & 1-\\lambda & 1 \\\\ 1 & 1 & 2-\\lambda \\end{pmatrix} = 0$$\nUsing cofactor expansion along the first row:\n$$(1-\\lambda) \\det\\begin{pmatrix} 1-\\lambda & 1 \\\\ 1 & 2-\\lambda \\end{pmatrix} + 1 \\cdot \\det\\begin{pmatrix} 0 & 1-\\lambda \\\\ 1 & 1 \\end{pmatrix} = 0$$\n$$(1-\\lambda)((1-\\lambda)(2-\\lambda) - 1) + (-(1-\\lambda)) = 0$$\n$$(1-\\lambda)[(1-\\lambda)(2-\\lambda) - 1 - 1] = 0$$\n$$(1-\\lambda)(\\lambda^{2} - 3\\lambda + 2 - 2) = 0$$\n$$(1-\\lambda)(\\lambda^{2} - 3\\lambda) = 0$$\n$$\\lambda(1-\\lambda)(\\lambda - 3) = 0$$\nThe eigenvalues are $\\lambda \\in \\{0, 1, 3\\}$. The largest eigenvalue is $\\lambda_{\\max} = 3$. Therefore, the Lipschitz constant is $L = 3$. The step size for the IHT algorithm is chosen as $\\mu = \\frac{1}{L} = \\frac{1}{3}$.\n\nThird, we perform one iteration of IHT. The update rule is $x^{(1)} = H_{k}(x^{(0)} - \\mu \\nabla f(x^{(0)}))$, with sparsity level $k=1$. First, we compute the vector inside the thresholding operator:\n$$v = x^{(0)} - \\mu \\nabla f(x^{(0)}) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\frac{1}{3}\\begin{pmatrix} -1 \\\\ -2 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} 1/3 \\\\ 2/3 \\\\ 1 \\end{pmatrix}$$\nThe hard thresholding operator $H_{k}(v)$ with $k=1$ keeps the single component of $v$ with the largest magnitude and sets all other components to zero. The magnitudes of the components of $v$ are $|1/3| = 1/3$, $|2/3| = 2/3$, and $|1|=1$. The largest magnitude is $1$, which corresponds to the third component.\n$$x^{(1)} = H_{1}(v) = H_{1}\\left(\\begin{pmatrix} 1/3 \\\\ 2/3 \\\\ 1 \\end{pmatrix}\\right) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$$\n\nFinally, we compute the objective value at this new iterate, $f(x^{(1)})$.\n$$f(x^{(1)}) = \\frac{1}{2}\\|Ax^{(1)} - y\\|_{2}^{2}$$\nWe compute the term $Ax^{(1)}$:\n$$Ax^{(1)} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\nNext, we compute the residual vector $Ax^{(1)} - y$:\n$$Ax^{(1)} - y = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}$$\nThe squared $L_2$-norm of this residual is:\n$$\\|Ax^{(1)} - y\\|_{2}^{2} = 0^{2} + (-1)^{2} = 1$$\nSubstituting this back into the expression for $f(x^{(1)})$:\n$$f(x^{(1)}) = \\frac{1}{2} \\cdot 1 = \\frac{1}{2}$$", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "3454156"}, {"introduction": "While IHT directly tackles sparse recovery with a constraint on the $\\| x \\|_{0}$ pseudo-norm, it is one of many available algorithms. This practice moves from manual calculation to implementation, asking you to code both IHT and its famous convex relaxation counterpart, the Iterative Shrinkage-Thresholding Algorithm (ISTA) [@problem_id:3454136]. By comparing their distinct behaviors, you will develop an intuition for the practical trade-offs between the greedy, non-convex approach of IHT and the principled, convex approach of ISTA, which minimizes the $\\| x \\|_{1}$ norm.", "problem": "Construct a complete, runnable program that implements and compares two iterative sparse recovery algorithms for the linear model $y = A x_{\\star} + e$, where $A \\in \\mathbb{R}^{m \\times n}$, $x_{\\star} \\in \\mathbb{R}^{n}$ is $k$-sparse, and $e \\in \\mathbb{R}^{m}$ is additive noise. Your program must implement Iterative Hard Thresholding (IHT) to approximately solve the constrained least-squares problem $\\min_{x} \\tfrac{1}{2}\\| y - A x \\|_{2}^{2}$ subject to $\\| x \\|_{0} \\le k$, and Iterative Shrinkage-Thresholding Algorithm (ISTA) to approximately solve the convex relaxation $\\min_{x} \\tfrac{1}{2}\\| y - A x \\|_{2}^{2} + \\lambda \\| x \\|_{1}$. Begin from the following fundamental base and derive the iterative updates you implement: the gradient of the smooth data-fidelity term is $\\nabla f(x) = A^{\\top}(A x - y)$, the set of $k$-sparse vectors is the union of all $k$-dimensional coordinate subspaces, and the proximity operator of the $\\ell_{1}$-norm is coordinatewise soft-thresholding. Use a constant step size $\\mu$ chosen to satisfy the standard proximal-gradient stability condition $\\mu \\le 1 / L$, where $L$ is the Lipschitz constant of the gradient of the data-fidelity term, and $L = \\| A \\|_{2}^{2}$.\n\nYour program must:\n- Generate measurement matrices $A$ and ground-truth signals $x_{\\star}$ using a fixed random number generator seed per test, with $A$ drawn with independent and identically distributed entries $A_{ij} \\sim \\mathcal{N}(0, 1/m)$, and the $k$ nonzero entries of $x_{\\star}$ drawn independently from $\\mathcal{N}(0,1)$.\n- Form measurements $y = A x_{\\star} + e$, with noise entries $e_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$.\n- Initialize both algorithms at $x^{(0)} = 0$.\n- For IHT, at each iteration, perform a gradient step and then project onto the set of $k$-sparse vectors by keeping the $k$ largest-magnitude coordinates (breaking ties deterministically by index order if needed).\n- For ISTA, at each iteration, perform a gradient step followed by coordinatewise soft-thresholding with the threshold proportional to $\\lambda \\mu$; use a fixed regularization parameter $\\lambda = \\alpha \\| A^{\\top} y \\|_{\\infty}$ with a given scalar $\\alpha \\in (0,1)$.\n- Run both algorithms for $T$ iterations for each test.\n- Record the support trajectory as the sequence of index sets of nonzero coordinates at each iteration.\n- Report for each test whether the IHT and ISTA support trajectories differ at any iteration, the final $\\ell_{2}$-error to the ground truth for IHT and ISTA, and the final support sizes produced by IHT and ISTA.\n\nTest suite. Your program must run the following four tests, each fully specified by $(\\text{seed}, m, n, k, \\sigma, \\text{step\\_scale}, \\alpha, T)$, where the per-iteration step size is $\\mu = \\text{step\\_scale} / L$ with $L = \\| A \\|_{2}^{2}$, and the regularization weight is $\\lambda = \\alpha \\| A^{\\top} y \\|_{\\infty}$:\n- Test $1$: $(\\,$seed $= 42,\\, m = 32,\\, n = 64,\\, k = 6,\\, \\sigma = 0.01,\\, \\text{step\\_scale} = 0.9,\\, \\alpha = 0.15,\\, T = 40\\,)$.\n- Test $2$: $(\\,$seed $= 123,\\, m = 30,\\, n = 60,\\, k = 5,\\, \\sigma = 0,\\, \\text{step\\_scale} = 1,\\, \\alpha = 0.10,\\, T = 60\\,)$.\n- Test $3$: $(\\,$seed $= 7,\\, m = 20,\\, n = 40,\\, k = 1,\\, \\sigma = 0.02,\\, \\text{step\\_scale} = 0.95,\\, \\alpha = 0.20,\\, T = 40\\,)$.\n- Test $4$: $(\\,$seed $= 2025,\\, m = 24,\\, n = 48,\\, k = 10,\\, \\sigma = 0.03,\\, \\text{step\\_scale} = 0.9,\\, \\alpha = 0.12,\\, T = 50\\,)$.\n\nFor each test, the program must output a list with the following entries, in order:\n- A boolean indicating whether the supports differ at any iteration (i.e., whether there exists an iteration $t \\in \\{1,\\dots,T\\}$ for which the IHT support set is not equal to the ISTA support set).\n- The final IHT error $\\| x_{\\text{IHT}}^{(T)} - x_{\\star} \\|_{2}$ as a real number rounded to $6$ decimal places.\n- The final ISTA error $\\| x_{\\text{ISTA}}^{(T)} - x_{\\star} \\|_{2}$ as a real number rounded to $6$ decimal places.\n- The final IHT support size (an integer, which should equal $k$ by construction).\n- The final ISTA support size (an integer, the number of nonzero entries after soft-thresholding).\n\nFinal output format. Your program should produce a single line of output containing the results for the $4$ tests as a comma-separated list of $4$ lists enclosed in square brackets, with no spaces, booleans rendered as $True$ or $False$, and floating-point numbers written with exactly $6$ digits after the decimal point. For example, the output must have the form $[[b_{1},e^{\\text{IHT}}_{1},e^{\\text{ISTA}}_{1},s^{\\text{IHT}}_{1},s^{\\text{ISTA}}_{1}],[b_{2},e^{\\text{IHT}}_{2},e^{\\text{ISTA}}_{2},s^{\\text{IHT}}_{2},s^{\\text{ISTA}}_{2}],[b_{3},e^{\\text{IHT}}_{3},e^{\\text{ISTA}}_{3},s^{\\text{IHT}}_{3},s^{\\text{ISTA}}_{3}],[b_{4},e^{\\text{IHT}}_{4},e^{\\text{ISTA}}_{4},s^{\\text{IHT}}_{4},s^{\\text{ISTA}}_{4}]]$.", "solution": "The problem requires the implementation and comparison of two fundamental iterative algorithms for sparse signal recovery: Iterative Hard Thresholding (IHT) and the Iterative Shrinkage-Thresholding Algorithm (ISTA). Both algorithms are instances of a general framework known as proximal gradient methods, designed to solve optimization problems of the form $\\min_{x} f(x) + g(x)$, where $f(x)$ is a smooth, differentiable function and $g(x)$ is a possibly non-smooth, but \"simple\" regularizer.\n\nThe linear model is given by $y = A x_{\\star} + e$, where $y \\in \\mathbb{R}^{m}$ are the measurements, $A \\in \\mathbb{R}^{m \\times n}$ is the measurement matrix, $x_{\\star} \\in \\mathbb{R}^{n}$ is a $k$-sparse ground-truth signal, and $e \\in \\mathbb{R}^{m}$ is additive noise.\n\nThe goal is to recover an approximation of $x_{\\star}$ from $y$ and $A$. This is framed as an optimization problem where we seek a sparse vector $x$ that minimizes the data-fidelity term, $f(x) = \\tfrac{1}{2}\\| y - A x \\|_{2}^{2}$. The gradient of this term is $\\nabla f(x) = A^{\\top}(A x - y)$. The stability of the iterative algorithms depends on the step size $\\mu$, which must be chosen such that $\\mu \\le 1/L$, where $L$ is the Lipschitz constant of $\\nabla f(x)$. For this specific $f(x)$, the Lipschitz constant is $L = \\| A^{\\top}A \\|_{2} = \\| A \\|_{2}^{2}$, where $\\| A \\|_2$ is the spectral norm of $A$, i.e., its largest singular value.\n\nThe general iterative update for a proximal gradient method is:\n$$\nx^{(t+1)} = \\text{prox}_{\\mu g}\\left(x^{(t)} - \\mu \\nabla f(x^{(t)})\\right),\n$$\nwhere $\\text{prox}_{\\mu g}(v) = \\arg\\min_{z} \\left( g(z) + \\tfrac{1}{2\\mu}\\| z - v \\|_{2}^{2} \\right)$ is the proximal operator of the function $\\mu g$.\n\n**Iterative Hard Thresholding (IHT)**\n\nIHT aims to solve the $\\ell_0$-constrained least-squares problem:\n$$\n\\min_{x \\in \\mathbb{R}^n} \\tfrac{1}{2}\\| y - A x \\|_{2}^{2} \\quad \\text{subject to} \\quad \\| x \\|_{0} \\le k.\n$$\nThis problem is non-convex and NP-hard. IHT is a projected gradient descent algorithm that provides an approximate solution. This can be seen as a proximal gradient method where $g(x)$ is an indicator function for the constraint set $C_k = \\{x \\in \\mathbb{R}^n \\mid \\| x \\|_{0} \\le k\\}$. The indicator function is $I_{C_k}(x) = 0$ if $x \\in C_k$ and $I_{C_k}(x) = \\infty$ otherwise.\n\nThe proximal operator of $I_{C_k}(x)$ is the Euclidean projection onto the set $C_k$, denoted $\\Pi_{C_k}(\\cdot)$. The projection of a vector $v$ onto the set of $k$-sparse vectors is obtained by keeping the $k$ entries of $v$ with the largest magnitude and setting the rest to zero. This operation is called hard thresholding, which we denote by $H_k(\\cdot)$.\n\nThe IHT algorithm proceeds as follows:\n1.  Initialize $x^{(0)} = 0$.\n2.  For $t = 0, 1, 2, \\dots, T-1$:\n    a.  Compute the gradient: $g^{(t)} = A^{\\top}(A x^{(t)} - y)$.\n    b.  Perform a gradient descent step: $v^{(t)} = x^{(t)} - \\mu g^{(t)}$.\n    c.  Project onto the set of $k$-sparse vectors: $x^{(t+1)} = H_k(v^{(t)})$.\n\nThe hard thresholding operator $H_k(v)$ is implemented by finding the indices of the $k$ largest-magnitude entries of $v$ and creating a new vector that is zero everywhere except at these indices, where it retains the values from $v$. If ties in magnitude occur, they are broken deterministically by preferring the entry with the smaller index.\n\n**Iterative Shrinkage-Thresholding Algorithm (ISTA)**\n\nISTA addresses the convex relaxation of the $\\ell_0$ problem, known as LASSO (Least Absolute Shrinkage and Selection Operator):\n$$\n\\min_{x \\in \\mathbb{R}^n} \\tfrac{1}{2}\\| y - A x \\|_{2}^{2} + \\lambda \\| x \\|_{1}.\n$$\nThis is an instance of the general form $\\min_x f(x) + g(x)$ with $f(x) = \\tfrac{1}{2}\\| y - A x \\|_{2}^{2}$ and $g(x) = \\lambda \\| x \\|_{1}$. The $\\ell_1$-norm is convex, making the overall problem convex.\n\nThe proximal operator of $\\mu g(x) = \\mu \\lambda \\| x \\|_1$ is the coordinate-wise soft-thresholding operator, $S_{\\mu\\lambda}(\\cdot)$. For a scalar $v_i$, the operation is defined as:\n$$\nS_{\\tau}(v_i) = \\text{sign}(v_i) \\max(|v_i| - \\tau, 0),\n$$\nwhere the threshold is $\\tau = \\mu\\lambda$.\n\nThe ISTA algorithm proceeds as follows:\n1.  Initialize $x^{(0)} = 0$.\n2.  For $t = 0, 1, 2, \\dots, T-1$:\n    a.  Compute the gradient: $g^{(t)} = A^{\\top}(A x^{(t)} - y)$.\n    b.  Perform a gradient descent step: $v^{(t)} = x^{(t)} - \\mu g^{(t)}$.\n    c.  Apply the soft-thresholding operator: $x^{(t+1)} = S_{\\mu\\lambda}(v^{(t)})$.\n\nThe regularization parameter $\\lambda$ controls the trade-off between data fidelity and sparsity. A common heuristic, used here, is to set $\\lambda = \\alpha \\| A^{\\top}y \\|_{\\infty}$ for some $\\alpha \\in (0,1)$. This choice ensures that for $x^{(0)}=0$, the first step will not result in an all-zero vector, provided $\\alpha$ is not too large.\n\nThe program will implement both algorithms, generate data according to the specified distributions, run the iterations, track the support sets (indices of non-zero entries) at each iteration, and report the required comparison metrics and final errors.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_recovery_test(seed, m, n, k, sigma, step_scale, alpha, T):\n    \"\"\"\n    Implements and compares IHT and ISTA for a single test case.\n    \n    Args:\n        seed (int): Random seed.\n        m (int): Number of measurements.\n        n (int): Signal dimension.\n        k (int): Sparsity level.\n        sigma (float): Noise standard deviation.\n        step_scale (float): Scaling factor for the step size.\n        alpha (float): Scaling factor for the LASSO regularization parameter.\n        T (int): Number of iterations.\n        \n    Returns:\n        list: A list containing [supports_differ, err_iht, err_ista, size_iht, size_ista].\n    \"\"\"\n    # 1. Data Generation\n    rng = np.random.default_rng(seed)\n    A = rng.normal(0, 1/np.sqrt(m), size=(m, n))\n    \n    x_star = np.zeros(n)\n    support = rng.choice(n, k, replace=False)\n    x_star[support] = rng.normal(0, 1, size=k)\n    \n    noise = rng.normal(0, sigma, size=m)\n    y = A @ x_star + noise\n\n    # 2. Algorithm Parameters\n    L = np.linalg.svd(A, compute_uv=False)[0]**2\n    mu = step_scale / L\n    lambda_reg = alpha * np.linalg.norm(A.T @ y, ord=np.inf)\n    \n    # 3. Initialization\n    x_iht = np.zeros(n)\n    x_ista = np.zeros(n)\n    \n    supports_iht_traj = []\n    supports_ista_traj = []\n\n    # 4. Iterations\n    for _ in range(T):\n        # IHT step\n        grad_iht = A.T @ (A @ x_iht - y)\n        v_iht = x_iht - mu * grad_iht\n        \n        # Hard thresholding with deterministic tie-breaking.\n        # np.lexsort sorts by the last key first. We want to sort by magnitude\n        # descending (-abs) and then by index ascending (arange).\n        # lexsort sorts in ascending order, so we take the first k.\n        top_k_indices = np.lexsort((np.arange(n), -np.abs(v_iht)))[:k]\n        \n        x_iht_new = np.zeros(n)\n        x_iht_new[top_k_indices] = v_iht[top_k_indices]\n        x_iht = x_iht_new\n        \n        # ISTA step\n        grad_ista = A.T @ (A @ x_ista - y)\n        v_ista = x_ista - mu * grad_ista\n        \n        # Soft thresholding\n        threshold = mu * lambda_reg\n        x_ista = np.sign(v_ista) * np.maximum(np.abs(v_ista) - threshold, 0)\n        \n        # Record supports\n        supports_iht_traj.append(set(np.where(x_iht != 0)[0]))\n        supports_ista_traj.append(set(np.where(x_ista != 0)[0]))\n\n    # 5. Compute Metrics\n    supports_differ = any(s1 != s2 for s1, s2 in zip(supports_iht_traj, supports_ista_traj))\n\n    err_iht = np.linalg.norm(x_iht - x_star)\n    err_ista = np.linalg.norm(x_ista - x_star)\n    \n    size_iht = np.count_nonzero(x_iht)\n    size_ista = np.count_nonzero(x_ista)\n    \n    return [supports_differ, err_iht, err_ista, size_iht, size_ista]\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        (42, 32, 64, 6, 0.01, 0.9, 0.15, 40),\n        (123, 30, 60, 5, 0, 1.0, 0.10, 60),\n        (7, 20, 40, 1, 0.02, 0.95, 0.20, 40),\n        (2025, 24, 48, 10, 0.03, 0.9, 0.12, 50),\n    ]\n\n    all_results_formatted = []\n    for case in test_cases:\n        seed, m, n, k, sigma, step_scale, alpha, T = case\n        results = run_recovery_test(seed, m, n, k, sigma, step_scale, alpha, T)\n        \n        supports_differ, err_iht, err_ista, size_iht, size_ista = results\n        \n        formatted_case = (\n            f\"[{str(supports_differ)},{err_iht:.6f},{err_ista:.6f},\"\n            f\"{size_iht},{size_ista}]\"\n        )\n        all_results_formatted.append(formatted_case)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results_formatted)}]\")\n\nsolve()\n```", "id": "3454136"}, {"introduction": "The performance of gradient-based algorithms like IHT is highly sensitive to the choice of the step size parameter, $\\mu$. An improperly chosen step size can lead to slow convergence or even divergence, rendering the algorithm ineffective. This final practice provides a striking demonstration of this principle by constructing a counterexample where IHT fails to recover a signal, despite knowing the correct sparsity, simply because the step size is too large [@problem_id:3454165]. By empirically testing step sizes both inside and outside the theoretical stability boundary, you will connect the abstract concept of a spectral radius to the concrete, observable success or failure of the algorithm.", "problem": "Consider the canonical linear measurement model in compressed sensing, where a measurement matrix $A \\in \\mathbb{R}^{m \\times n}$ and a $k$-sparse signal $x^\\star \\in \\mathbb{R}^n$ generate measurements $y = A x^\\star$. Iterative Hard Thresholding (IHT), defined here as a combination of gradient descent on the least-squares data fidelity and the hard thresholding operator, seeks a $k$-sparse estimate of $x^\\star$ via the recursion\n$$\nx^{t+1} = H_k\\!\\left(x^t + \\mu A^\\top (y - A x^t)\\right),\n$$\nwhere $H_k(\\cdot)$ denotes the hard thresholding operator that keeps the $k$ largest entries in magnitude and sets the rest to zero, and $\\mu > 0$ is a stepsize parameter. In this problem, you will construct and analyze a counterexample where IHT fails to recover the true $k$-sparse solution despite knowing the correct $k$, due to an overly aggressive $\\mu$. You will characterize the boundary between convergence and divergence by analyzing the spectrum of the restriction of $A^\\top A$ to the active support.\n\nStart from the foundational base: least-squares data fidelity $f(x) = \\tfrac{1}{2}\\|A x - y\\|_2^2$, its gradient $\\nabla f(x) = A^\\top (A x - y)$, the hard thresholding operator $H_k$, the definition of support of a vector $\\mathrm{supp}(x)$, the spectral radius of a matrix, and the eigenvalues of a symmetric positive semidefinite matrix. The active support refers to the index set selected by $H_k$ in an iterate. The analysis must be purely mathematical.\n\nConstruct the following specific instance:\n- Dimensions: $m = 4$, $n = 5$, $k = 2$.\n- Define the orthonormal basis vectors $e_1, e_2, e_3, e_4 \\in \\mathbb{R}^4$.\n- Define the columns of $A$ by\n  - $a_1 = e_1$,\n  - $a_2 = c \\, e_1 + \\sqrt{1 - c^2}\\, e_2$ with $c = \\frac{9}{10}$,\n  - $a_3 = \\beta \\, e_1 + \\gamma \\, e_2 + \\sqrt{1 - \\beta^2 - \\gamma^2}\\, e_3$ with $\\beta = \\frac{3}{5}$ and $\\gamma = \\frac{1}{5}$,\n  - $a_4 = \\alpha_1 \\, e_1 - \\alpha_2 \\, e_2 + \\alpha_3 \\, e_3 + \\sqrt{1 - \\alpha_1^2 - \\alpha_2^2 - \\alpha_3^2}\\, e_4$ with $\\alpha_1 = \\frac{1}{10}$, $\\alpha_2 = \\frac{3}{10}$, $\\alpha_3 = \\frac{1}{5}$,\n  - $a_5 = \\delta_1 \\, e_1 + \\delta_2 \\, e_2 - \\delta_3 \\, e_3 + \\sqrt{1 - \\delta_1^2 - \\delta_2^2 - \\delta_3^2}\\, e_4$ with $\\delta_1 = \\frac{1}{5}$, $\\delta_2 = \\frac{1}{2}$, $\\delta_3 = \\frac{1}{5}$.\n- Let $A = [a_1, a_2, a_3, a_4, a_5] \\in \\mathbb{R}^{4 \\times 5}$ and $x^\\star = [1, 1, 0, 0, 0]^\\top \\in \\mathbb{R}^5$ so that the true support is $S^\\star = \\{1, 2\\}$ (using $1$-based indexing for descriptive clarity; implementation may use $0$-based).\n- Let $y = A x^\\star$.\n\nDefine the IHT algorithm with initialization $x^0 = 0$ and $k = 2$. The key restricted matrix for convergence analysis is $A_{S^\\star}^\\top A_{S^\\star} \\in \\mathbb{R}^{2 \\times 2}$, where $A_{S^\\star}$ denotes the submatrix of $A$ formed by the columns indexed by $S^\\star$. Let $\\lambda_{\\max}$ be the largest eigenvalue of $A_{S^\\star}^\\top A_{S^\\star}$. For gradient descent dynamics restricted to $S^\\star$ (that is, assuming the active support remains $S^\\star$), the linearized error update is governed by the matrix $I - \\mu A_{S^\\star}^\\top A_{S^\\star}$, and the exact stability boundary for nonexpansiveness in the Euclidean norm is given by the condition on the spectral radius $\\rho(I - \\mu A_{S^\\star}^\\top A_{S^\\star}) < 1$, which is equivalent to $\\mu \\in (0, 2/\\lambda_{\\max})$.\n\nYou must empirically demonstrate a counterexample: with correct $k = 2$ and the constructed $A$ and $x^\\star$, pick a sufficiently aggressive stepsize $\\mu$ exceeding the stability boundary, run IHT for a fixed number of iterations, and show failure to recover $x^\\star$ (incorrect support and/or large estimation error). Compare with stepsizes strictly below the boundary, and near the boundary.\n\nTest suite:\n- Compute $\\lambda_{\\max}$ from $A_{S^\\star}^\\top A_{S^\\star}$ and define $\\mu_\\mathrm{crit} = \\frac{2}{\\lambda_{\\max}}$.\n- Use the following four stepsizes:\n  1. $\\mu_1 = \\frac{0.5}{\\lambda_{\\max}}$ (well below the conservative bound),\n  2. $\\mu_2 = 0.99 \\, \\mu_\\mathrm{crit}$ (just below the exact linear stability boundary),\n  3. $\\mu_3 = 1.20 \\, \\mu_\\mathrm{crit}$ (above the boundary, expected to diverge),\n  4. $\\mu_4 = 10$ (far above the boundary, expected to fail decisively).\n- Use a fixed iteration cap $T = 200$ and tolerance $\\varepsilon = 10^{-6}$ to decide successful recovery, where success is defined as both $\\mathrm{supp}(x^{T}) = S^\\star$ and $\\|x^{T} - x^\\star\\|_2 \\le \\varepsilon$.\n\nFor each stepsize $\\mu_i$, compute and record:\n- A boolean indicating whether IHT successfully recovers $x^\\star$ within $T$ iterations under the above success criterion.\n- The spectral radius $\\rho\\!\\left(I - \\mu_i A_{S^\\star}^\\top A_{S^\\star}\\right)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be ordered as\n$$\n[\\mu_\\mathrm{crit},\\, \\text{success}(\\mu_1),\\, \\rho(I - \\mu_1 A_{S^\\star}^\\top A_{S^\\star}),\\, \\text{success}(\\mu_2),\\, \\rho(I - \\mu_2 A_{S^\\star}^\\top A_{S^\\star}),\\, \\text{success}(\\mu_3),\\, \\rho(I - \\mu_3 A_{S^\\star}^\\top A_{S^\\star}),\\, \\text{success}(\\mu_4),\\, \\rho(I - \\mu_4 A_{S^\\star}^\\top A_{S^\\star}) ].\n$$\nNo physical units are involved. Angles are not involved. Percentages must not be used; all scalars must be decimals or integers.\n\nImplement the algorithmic logic from first principles, using only the provided matrix $A$, vector $x^\\star$, and the stepsizes specified above. Ensure scientific realism by verifying column normalization and by computing eigenvalues and spectral radii via standard linear algebra operations. The final output must strictly follow the format specified above.", "solution": "We begin from the least-squares formulation $f(x) = \\tfrac{1}{2}\\|A x - y\\|_2^2$ with gradient $\\nabla f(x) = A^\\top (A x - y)$. Iterative Hard Thresholding (IHT) combines a gradient descent step with hard thresholding of the iterate to enforce $k$-sparsity:\n$$\nx^{t+1} = H_k\\!\\left(x^t - \\mu \\nabla f(x^t)\\right) \n= H_k\\!\\left(x^t + \\mu A^\\top (y - A x^t)\\right),\n$$\nwhere $H_k(\\cdot)$ selects the $k$ entries of largest magnitude and zeros the rest. The core definitions and properties we use are:\n- The support of a vector $x$, denoted $\\mathrm{supp}(x)$, is the set of indices of its nonzero entries.\n- The hard thresholding operator $H_k$ preserves exactly $k$ entries of largest absolute value.\n- For a symmetric positive semidefinite matrix $M$, its eigenvalues are real and nonnegative, and its spectral radius $\\rho(M)$ equals its largest eigenvalue in magnitude.\n- The active support is the index set of entries selected by $H_k$ in an iterate.\n\nThe analysis of IHT is nuanced due to the nonlinearity $H_k$. A standard approach isolates the behavior on the true support $S^\\star$ under the simplifying assumption that the active support is correctly maintained. On this fixed support, the least-squares gradient dynamics reduce to classical linear iterations. Let $A_{S^\\star}$ denote the submatrix of $A$ with columns indexed by $S^\\star$. If $x^t$ is $k$-sparse with $\\mathrm{supp}(x^t) = S^\\star$, and the thresholding preserves this support, then restricted dynamics on $S^\\star$ obey\n$$\nx_{S^\\star}^{t+1} = x_{S^\\star}^{t} + \\mu A_{S^\\star}^\\top \\left(y - A_{S^\\star} x_{S^\\star}^{t}\\right).\n$$\nSet the error $e^{t} = x_{S^\\star}^{t} - x^\\star_{S^\\star}$. Using $y = A_{S^\\star} x^\\star_{S^\\star}$, we have\n$$\ne^{t+1} = x_{S^\\star}^{t+1} - x^\\star_{S^\\star}\n= x_{S^\\star}^{t} + \\mu A_{S^\\star}^\\top \\left(A_{S^\\star} x^\\star_{S^\\star} - A_{S^\\star} x_{S^\\star}^{t}\\right) - x^\\star_{S^\\star}\n= \\left(I - \\mu A_{S^\\star}^\\top A_{S^\\star}\\right) e^{t}.\n$$\nThus, on the correct support, the error update is linear with iteration matrix $I - \\mu A_{S^\\star}^\\top A_{S^\\star}$. Convergence in the Euclidean norm occurs if and only if the spectral radius satisfies\n$$\n\\rho\\!\\left(I - \\mu A_{S^\\star}^\\top A_{S^\\star}\\right) < 1.\n$$\nLet the eigenvalues of $A_{S^\\star}^\\top A_{S^\\star}$ be $\\{\\lambda_i\\}_{i=1}^k$ with $\\lambda_{\\max} = \\max_i \\lambda_i$. The eigenvalues of $I - \\mu A_{S^\\star}^\\top A_{S^\\star}$ are $\\{1 - \\mu \\lambda_i\\}_{i=1}^k$, and the spectral radius is\n$$\n\\rho\\!\\left(I - \\mu A_{S^\\star}^\\top A_{S^\\star}\\right) = \\max_i |1 - \\mu \\lambda_i|.\n$$\nTherefore, the exact linear stability boundary for nonexpansive behavior is\n$$\n\\mu \\in (0, 2 / \\lambda_{\\max}), \\quad \\text{with} \\quad \\mu_\\mathrm{crit} = 2/\\lambda_{\\max}.\n$$\nFor $\\mu \\in (0, 2/\\lambda_{\\max})$, the linear restricted error contracts (possibly with oscillations if $1 - \\mu \\lambda_i < 0$), whereas for $\\mu > 2/\\lambda_{\\max}$ the restricted error diverges. This condition is necessary and sufficient to ensure contraction on the true support under fixed support dynamics.\n\nHowever, IHT applies $H_k$ at every iteration. The thresholding can change the support in response to overshoot and cross-talk induced by $A^\\top$ applied to the residual $y - A x^t$. Even if the first threshold selects the correct support (which depends on the relative magnitudes of $A^\\top y$ across indices), an overly aggressive $\\mu$ can cause large oscillations and amplification of the error on $S^\\star$. As the error grows, the residual $r^t = y - A x^t$ also grows, and $A^\\top r^t$ can produce large off-support entries. Eventually, hard thresholding may switch the active support away from $S^\\star$, causing failure to recover $x^\\star$.\n\nWe now construct the specific counterexample. Let $m = 4$, $n = 5$, $k = 2$. Define the columns of $A$ in $\\mathbb{R}^4$ as follows. Let $e_1, e_2, e_3, e_4$ be the standard orthonormal basis. Choose $c = \\frac{9}{10}$, $\\beta = \\frac{3}{5}$, $\\gamma = \\frac{1}{5}$, $\\alpha_1 = \\frac{1}{10}$, $\\alpha_2 = \\frac{3}{10}$, $\\alpha_3 = \\frac{1}{5}$, $\\delta_1 = \\frac{1}{5}$, $\\delta_2 = \\frac{1}{2}$, $\\delta_3 = \\frac{1}{5}$. Set\n$$\na_1 = e_1,\\quad \na_2 = c e_1 + \\sqrt{1 - c^2} \\, e_2,\\quad\na_3 = \\beta e_1 + \\gamma e_2 + \\sqrt{1 - \\beta^2 - \\gamma^2} \\, e_3,\n$$\n$$\na_4 = \\alpha_1 e_1 - \\alpha_2 e_2 + \\alpha_3 e_3 + \\sqrt{1 - \\alpha_1^2 - \\alpha_2^2 - \\alpha_3^2} \\, e_4,\\quad\na_5 = \\delta_1 e_1 + \\delta_2 e_2 - \\delta_3 e_3 + \\sqrt{1 - \\delta_1^2 - \\delta_2^2 - \\delta_3^2} \\, e_4.\n$$\nThese columns are unit-norm by construction. Let $A = [a_1, a_2, a_3, a_4, a_5]$. Choose the true sparse signal\n$$\nx^\\star = [1, 1, 0, 0, 0]^\\top,\n$$\nwith true support $S^\\star = \\{1, 2\\}$, and let\n$$\ny = A x^\\star = a_1 + a_2.\n$$\nNote that $A_{S^\\star}^\\top A_{S^\\star} = \\begin{bmatrix} 1 & c \\\\ c & 1 \\end{bmatrix}$. Its eigenvalues are $\\lambda_1 = 1 + c$ and $\\lambda_2 = 1 - c$, so $\\lambda_{\\max} = 1 + c = \\frac{19}{10}$. Thus the exact linear stability boundary for the restricted dynamics is\n$$\n\\mu_\\mathrm{crit} = \\frac{2}{\\lambda_{\\max}} = \\frac{2}{1 + c} = \\frac{20}{19}.\n$$\nWe consider four stepsizes:\n$$\n\\mu_1 = \\frac{0.5}{\\lambda_{\\max}},\\quad \\mu_2 = 0.99 \\, \\mu_\\mathrm{crit},\\quad \\mu_3 = 1.20 \\, \\mu_\\mathrm{crit},\\quad \\mu_4 = 10.\n$$\nWe initialize $x^0 = 0$ and apply IHT for $T = 200$ iterations, where each iteration uses the update $x^{t+1} = H_k(x^t + \\mu_i A^\\top (y - A x^t))$. Success is declared if both $\\mathrm{supp}(x^{T}) = S^\\star$ and $\\|x^{T} - x^\\star\\|_2 \\le \\varepsilon$ with $\\varepsilon = 10^{-6}$.\n\nFrom first principles, we can predict:\n- For $\\mu_1$ and $\\mu_2$, the restricted dynamics satisfy $\\rho(I - \\mu A_{S^\\star}^\\top A_{S^\\star}) < 1$, ensuring contraction on $S^\\star$. Provided the thresholding retains the correct support (which it does here initially because $A^\\top y$ has its two largest magnitudes on $S^\\star$), we expect convergence.\n- For $\\mu_3$ and $\\mu_4$, we have $\\rho(I - \\mu A_{S^\\star}^\\top A_{S^\\star}) > 1$, leading to divergence on $S^\\star$. The error grows and induces large off-support entries via $A^\\top (y - A x^t)$, causing the thresholding to eventually switch support and fail to recover $x^\\star$.\n\nThe program will compute $\\lambda_{\\max}$, $\\mu_\\mathrm{crit}$, run IHT for each $\\mu_i$, compute the spectral radius $\\rho(I - \\mu_i A_{S^\\star}^\\top A_{S^\\star}) = \\max\\{|1 - \\mu_i \\lambda_1|,\\, |1 - \\mu_i \\lambda_2|\\}$, and produce the final single-line aggregate output:\n$$\n[\\mu_\\mathrm{crit},\\, \\text{success}(\\mu_1),\\, \\rho(I - \\mu_1 A_{S^\\star}^\\top A_{S^\\star}),\\, \\text{success}(\\mu_2),\\, \\rho(I - \\mu_2 A_{S^\\star}^\\top A_{S^\\star}),\\, \\text{success}(\\mu_3),\\, \\rho(I - \\mu_3 A_{S^\\star}^\\top A_{S^\\star}),\\, \\text{success}(\\mu_4),\\, \\rho(I - \\mu_4 A_{S^\\star}^\\top A_{S^\\star}) ].\n$$\nThis demonstrates the counterexample and delineates the boundary between convergence and divergence through the spectrum of $A^\\top A$ restricted to the active support.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef hard_threshold(x, k):\n    \"\"\"Keep k largest-magnitude entries of x, zero out the rest.\"\"\"\n    if k >= x.size:\n        return x.copy()\n    idx = np.argsort(np.abs(x))[::-1]\n    keep = idx[:k]\n    x_ht = np.zeros_like(x)\n    x_ht[keep] = x[keep]\n    return x_ht\n\ndef iht(A, y, k, mu, max_iters=200, tol=1e-6):\n    \"\"\"Iterative Hard Thresholding with fixed step size mu.\"\"\"\n    x = np.zeros(A.shape[1])\n    for _ in range(max_iters):\n        grad = A.T @ (y - A @ x)\n        z = x + mu * grad\n        x_new = hard_threshold(z, k)\n        # Early stop if converged\n        if np.linalg.norm(x_new - x) <= 1e-12:\n            x = x_new\n            break\n        x = x_new\n    return x\n\ndef spectral_radius_restricted(A_S, mu):\n    \"\"\"Compute spectral radius of I - mu * (A_S^T A_S).\"\"\"\n    M = A_S.T @ A_S\n    evals = np.linalg.eigvalsh(M)\n    # eigenvalues of I - mu M are 1 - mu*lambda_i\n    vals = 1.0 - mu * evals\n    return float(np.max(np.abs(vals)))\n\ndef construct_matrix():\n    \"\"\"Construct the specific A as per problem statement.\"\"\"\n    # Basis vectors e1..e4\n    e1 = np.array([1.0, 0.0, 0.0, 0.0])\n    e2 = np.array([0.0, 1.0, 0.0, 0.0])\n    e3 = np.array([0.0, 0.0, 1.0, 0.0])\n    e4 = np.array([0.0, 0.0, 0.0, 1.0])\n\n    c = 0.9\n    beta = 0.6\n    gamma = 0.2\n    alpha1, alpha2, alpha3 = 0.1, 0.3, 0.2\n    delta1, delta2, delta3 = 0.2, 0.5, 0.2\n\n    a1 = e1\n    a2 = c * e1 + np.sqrt(1 - c**2) * e2\n    a3 = beta * e1 + gamma * e2 + np.sqrt(1 - beta**2 - gamma**2) * e3\n    a4 = alpha1 * e1 - alpha2 * e2 + alpha3 * e3 + np.sqrt(1 - alpha1**2 - alpha2**2 - alpha3**2) * e4\n    a5 = delta1 * e1 + delta2 * e2 - delta3 * e3 + np.sqrt(1 - delta1**2 - delta2**2 - delta3**2) * e4\n\n    A = np.column_stack([a1, a2, a3, a4, a5])\n    # Normalize columns (should already be unit-norm, but enforce numerically)\n    A = A / np.linalg.norm(A, axis=0, keepdims=True)\n    return A\n\ndef solve():\n    # Construct A and true signal\n    A = construct_matrix()\n    n = A.shape[1]\n    k = 2\n    # True support S* = {0, 1} in 0-based indexing\n    S_star = np.array([0, 1], dtype=int)\n    x_star = np.zeros(n)\n    x_star[S_star] = 1.0\n    y = A @ x_star\n\n    # Restricted matrix and stability boundary\n    A_S = A[:, S_star]\n    M = A_S.T @ A_S\n    eigs = np.linalg.eigvalsh(M)\n    lambda_max = float(np.max(eigs))\n    mu_crit = 2.0 / lambda_max\n\n    # Define test cases: step sizes\n    mu1 = 0.5 / lambda_max\n    mu2 = 0.99 * mu_crit\n    mu3 = 1.20 * mu_crit\n    mu4 = 10.0\n\n    test_cases = [mu1, mu2, mu3, mu4]\n\n    results = []\n    # First append mu_crit\n    results.append(mu_crit)\n\n    # Run IHT for each case\n    max_iters = 200\n    tol = 1e-6\n    for mu in test_cases:\n        x_est = iht(A, y, k, mu, max_iters=max_iters, tol=tol)\n        support_est = np.flatnonzero(np.abs(x_est) > 0)\n        # Check success: exact support and small error\n        success = (set(support_est.tolist()) == set(S_star.tolist())) and (np.linalg.norm(x_est - x_star) <= tol)\n        rho = spectral_radius_restricted(A_S, mu)\n        results.append(success)\n        results.append(rho)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3454165"}]}