{"hands_on_practices": [{"introduction": "Understanding an algorithm often begins with a concrete, step-by-step calculation. This first practice provides a foundational exercise in applying the core mechanics of the Normalized Iterative Hard Thresholding (NIHT) algorithm. By manually computing the adaptive step size $\\mu_t$ and performing a single update for a small-scale problem, you will gain a tangible feel for how NIHT adjusts its step size based on the local geometry of the objective function, a key feature that distinguishes it from methods with fixed step sizes [@problem_id:3438872].", "problem": "Consider the least-squares objective $f(x) = \\frac{1}{2}\\|y - A x\\|_{2}^{2}$ in compressed sensing, where $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, and $x \\in \\mathbb{R}^{n}$. The Iterative Hard Thresholding (IHT) method seeks a $k$-sparse estimate by alternating a gradient step with the hard-thresholding operator $H_{k}$, which retains the $k$ largest entries (in magnitude) and zeroes the rest. The Normalized Iterative Hard Thresholding (NIHT) variant selects a data-adaptive step size at each iteration by minimizing the objective along a support-restricted gradient direction determined by the hard-thresholded candidate support. Starting from the fundamental definitions of the least-squares gradient and hard-thresholding, and the principle of line-search minimization along a search direction, determine the adaptive step size for one NIHT iteration as follows.\n\n- Let $A \\in \\mathbb{R}^{3 \\times 5}$, $k = 2$, \n$$\nA = \\begin{pmatrix}\n1  0  0  1  0 \\\\\n0  1  1  0  0 \\\\\n0  0  1  0  1\n\\end{pmatrix}, \\quad\nx_{t} = \\begin{pmatrix} 0 \\\\ 0.5 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\ny = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}.\n$$\n- Define the residual $r_{t} = y - A x_{t}$ and the gradient $g_{t} = \\nabla f(x_{t}) = -A^{\\top} r_{t}$. Work with the steepest-descent direction $-g_{t}$ and its support-restricted version constructed as follows: first form the proxy $x_{t} - g_{t}$, identify its $k$-sparse support through $H_{k}$, and then restrict the descent direction to that support (set entries outside to zero).\n- Choose the step size $\\mu_{t}$ by minimizing $f(x_{t} + \\mu d_{t})$ over $\\mu \\in \\mathbb{R}$ along the support-restricted descent direction $d_{t}$ obtained above, and then perform one NIHT update $x_{t+1} = H_{k}(x_{t} + \\mu_{t}(-g_{t}))$.\n\nCompute the value of the adaptive step size $\\mu_{t}$ dictated by this procedure. Then carry out the corresponding single NIHT update. Finally, briefly comment on how restricting to the hard-thresholded support in the step-size estimate influences the magnitude of $\\mu_{t}$ when compared to using the full (unrestricted) gradient direction. \n\nOnly report the value of $\\mu_{t}$ as your final answer. Round your reported $\\mu_{t}$ to four significant figures.", "solution": "The problem is well-posed, scientifically grounded in the field of sparse optimization, and provides all necessary information for a unique solution. Therefore, it is valid. We proceed with the solution by following the specified steps.\n\nThe objective function is the least-squares loss:\n$$\nf(x) = \\frac{1}{2}\\|y - A x\\|_{2}^{2}\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, and $x \\in \\mathbb{R}^{n}$. The gradient of this function with respect to $x$ is:\n$$\n\\nabla f(x) = A^{\\top}(A x - y) = -A^{\\top}(y - A x)\n$$\n\nThe problem provides the following data:\n$$\nA = \\begin{pmatrix}\n1  0  0  1  0 \\\\\n0  1  1  0  0 \\\\\n0  0  1  0  1\n\\end{pmatrix}, \\quad\nx_{t} = \\begin{pmatrix} 0 \\\\ 0.5 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\ny = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}, \\quad k=2.\n$$\n\nFirst, we calculate the residual $r_t$ at the current iterate $x_t$.\n$$\nA x_t = \\begin{pmatrix}\n1  0  0  1  0 \\\\\n0  1  1  0  0 \\\\\n0  0  1  0  1\n\\end{pmatrix}\n\\begin{pmatrix} 0 \\\\ 0.5 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n= \\begin{pmatrix} 0 \\cdot 0 + 0 \\cdot 0.5 + \\dots \\\\ 0 \\cdot 0 + 1 \\cdot 0.5 + \\dots \\\\ 0 \\cdot 0 + 0 \\cdot 0.5 + \\dots \\end{pmatrix}\n= \\begin{pmatrix} 0 \\\\ 0.5 \\\\ 0 \\end{pmatrix}\n$$\n$$\nr_t = y - A x_t = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0.5 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1.5 \\\\ 1 \\end{pmatrix}\n$$\n\nNext, we compute the gradient $g_t = \\nabla f(x_t) = -A^{\\top} r_t$.\n$$\nA^{\\top} = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  1  1 \\\\ 1  0  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n$$\ng_t = - \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  1  1 \\\\ 1  0  0 \\\\ 0  0  1 \\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ 1.5 \\\\ 1 \\end{pmatrix}\n= - \\begin{pmatrix} 1 \\\\ 1.5 \\\\ 1.5 + 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n= \\begin{pmatrix} -1 \\\\ -1.5 \\\\ -2.5 \\\\ -1 \\\\ -1 \\end{pmatrix}\n$$\nThe steepest-descent direction is $-g_t = \\begin{pmatrix} 1 \\\\ 1.5 \\\\ 2.5 \\\\ 1 \\\\ 1 \\end{pmatrix}$.\n\nAccording to the procedure, we form a proxy vector by taking a unit step along the steepest-descent direction, $z_t = x_t - g_t = x_t + (-g_t)$.\n$$\nz_t = \\begin{pmatrix} 0 \\\\ 0.5 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1.5 \\\\ 2.5 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2.5 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nWe find the support $\\Omega_t$ by applying the hard-thresholding operator $H_k$ with $k=2$. This operator identifies the indices of the $k=2$ largest-magnitude entries of $z_t$. The entries are $1$, $2$, $2.5$, $1$, $1$. The two largest are $2.5$ (at index $3$) and $2$ (at index $2$). Therefore, the support is $\\Omega_t = \\{2, 3\\}$.\n\nThe search direction $d_t$ is the steepest-descent direction $-g_t$ restricted to the support $\\Omega_t$.\n$$\nd_t = \\begin{pmatrix} 0 \\\\ (-g_t)_2 \\\\ (-g_t)_3 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1.5 \\\\ 2.5 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n\nThe adaptive step size $\\mu_t$ is chosen by minimizing $f(x_t + \\mu d_t)$ with respect to $\\mu$. Let $h(\\mu) = f(x_t + \\mu d_t)$.\n$$\nh(\\mu) = \\frac{1}{2} \\|y - A(x_t + \\mu d_t)\\|_2^2 = \\frac{1}{2} \\|(y - A x_t) - \\mu A d_t\\|_2^2 = \\frac{1}{2} \\|r_t - \\mu A d_t\\|_2^2\n$$\nThis is a quadratic in $\\mu$. To find the minimum, we set its derivative to zero:\n$$\n\\frac{dh}{d\\mu} = \\frac{d}{d\\mu} \\left( \\frac{1}{2} (r_t - \\mu A d_t)^\\top (r_t - \\mu A d_t) \\right) = -r_t^\\top A d_t + \\mu (A d_t)^\\top (A d_t) = 0\n$$\nSolving for $\\mu_t$:\n$$\n\\mu_t = \\frac{r_t^\\top A d_t}{\\|A d_t\\|_2^2}\n$$\nWe can simplify the numerator: since $g_t = -A^\\top r_t$, we have $-g_t = A^\\top r_t$, and thus $r_t^\\top A = (-g_t)^\\top$. So, $r_t^\\top A d_t = (-g_t)^\\top d_t$. Since $d_t$ is $-g_t$ projected onto the support $\\Omega_t$, this inner product becomes the squared norm of $d_t$: $(-g_t)^\\top d_t = \\|d_t\\|_2^2$.\nSo, the formula is $\\mu_t = \\frac{\\|d_t\\|_2^2}{\\|A d_t\\|_2^2}$.\n\nWe now compute the numerator and denominator.\nNumerator:\n$$\n\\|d_t\\|_2^2 = 0^2 + (1.5)^2 + (2.5)^2 + 0^2 + 0^2 = 2.25 + 6.25 = 8.5\n$$\nDenominator:\n$$\nA d_t = \\begin{pmatrix} 1  0  0  1  0 \\\\ 0  1  1  0  0 \\\\ 0  0  1  0  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1.5 \\\\ 2.5 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1.5+2.5 \\\\ 2.5 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 4 \\\\ 2.5 \\end{pmatrix}\n$$\n$$\n\\|A d_t\\|_2^2 = 0^2 + 4^2 + (2.5)^2 = 16 + 6.25 = 22.25\n$$\nThe step size is:\n$$\n\\mu_t = \\frac{8.5}{22.25} = \\frac{850}{2225} = \\frac{34}{89} \\approx 0.38202247...\n$$\nRounding to four significant figures, we get $\\mu_t = 0.3820$.\n\nThe updated estimate would be $x_{t+1} = H_k(x_t + \\mu_t(-g_t))$.\n$$\nx_t + \\mu_t(-g_t) \\approx \\begin{pmatrix} 0 \\\\ 0.5 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + 0.3820 \\begin{pmatrix} 1 \\\\ 1.5 \\\\ 2.5 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0.3820 \\\\ 0.5 + 0.573 \\\\ 0.955 \\\\ 0.3820 \\\\ 0.3820 \\end{pmatrix} = \\begin{pmatrix} 0.3820 \\\\ 1.073 \\\\ 0.955 \\\\ 0.3820 \\\\ 0.3820 \\end{pmatrix}\n$$\nApplying $H_2$ yields $x_{t+1} \\approx \\begin{pmatrix} 0 \\\\ 1.073 \\\\ 0.955 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\nFinally, we comment on the magnitude of $\\mu_t$ compared to the step size $\\mu'_t$ that would be obtained using the full (unrestricted) gradient direction $-g_t$.\nThe step size for the full direction is $\\mu'_t = \\frac{\\|-g_t\\|_2^2}{\\|A(-g_t)\\|_2^2}$.\n$$\n\\|-g_t\\|_2^2 = 1^2 + (1.5)^2 + (2.5)^2 + 1^2 + 1^2 = 1 + 2.25 + 6.25 + 1 + 1 = 11.5\n$$\n$$\nA(-g_t) = \\begin{pmatrix} 1  0  0  1  0 \\\\ 0  1  1  0  0 \\\\ 0  0  1  0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1.5 \\\\ 2.5 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1+1 \\\\ 1.5+2.5 \\\\ 2.5+1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 3.5 \\end{pmatrix}\n$$\n$$\n\\|A(-g_t)\\|_2^2 = 2^2 + 4^2 + (3.5)^2 = 4 + 16 + 12.25 = 32.25\n$$\n$$\n\\mu'_t = \\frac{11.5}{32.25} \\approx 0.3566\n$$\nIn this case, the NIHT step size $\\mu_t \\approx 0.3820$ is larger than the steepest-descent step size $\\mu'_t \\approx 0.3566$. Restricting the search direction from $-g_t$ to $d_t$ involves zeroing out some components, which reduces the magnitude of both the numerator ($\\|d_t\\|_2^2  \\|-g_t\\|_2^2$) and the denominator ($\\|A d_t\\|_2^2  \\|A(-g_t)\\|_2^2$) of the step-size formula. The final value of the step size depends on the relative reduction of these two terms. By restricting the problem to the subspace defined by the support $\\Omega_t$, the apparent curvature of the objective function along the search direction, measured by the Rayleigh quotient $\\|A d\\|_2^2 / \\|d\\|_2^2$, can change. In this specific instance, the denominator $\\|A d_t\\|_2^2$ decreased proportionally more than the numerator $\\|d_t\\|_2^2$, leading to an increase in their ratio, $\\mu_t  \\mu'_t$.", "answer": "$$\n\\boxed{0.3820}\n$$", "id": "3438872"}, {"introduction": "Moving from theory to robust implementation requires anticipating and handling potential numerical issues. The adaptive step size in NIHT, while powerful, involves a division that can become unstable if the denominator is close to zero. This practice challenges you to think critically about the practical side of algorithm design, focusing on how to create a principled, scale-aware safeguard to ensure numerical stability without introducing unnecessary bias, a crucial step in developing reliable optimization code [@problem_id:3463050].", "problem": "Consider the normalized Iterative Hard Thresholding (NIHT) scheme for recovering a $k$-sparse signal $x^\\star \\in \\mathbb{R}^n$ from linear measurements $y \\in \\mathbb{R}^m$ given by $y = A x^\\star + w$, where $A \\in \\mathbb{R}^{m \\times n}$ is the sensing matrix and $w \\in \\mathbb{R}^m$ is measurement noise. At iteration $t$, define the residual $r_t = y - A x_t$ and the gradient $g_t = A^\\top r_t$. Let $S_t \\subset \\{1,\\dots,n\\}$ denote the index set of the current support of cardinality $k$, and let $A_{S_t}$ denote the submatrix of $A$ formed by the columns indexed by $S_t$. The NIHT step size at iteration $t$ is computed via a ratio whose denominator is the squared norm of the image of the restricted gradient under the restricted sensing operator, namely\n$$\nD_t \\triangleq \\left\\|A_{S_t} \\, g_{S_t}\\right\\|_2^2 \\;=\\; g_{S_t}^\\top \\left(A_{S_t}^\\top A_{S_t}\\right) g_{S_t},\n$$\nwhere $g_{S_t}$ is the restriction of $g_t$ to $S_t$. In exact arithmetic, $D_t$ is a Rayleigh quotient scaling that reflects the local curvature of the restricted quadratic data-fit function. In floating-point arithmetic and under certain degeneracies of the support, $D_t$ can become numerically near zero, which makes the normalized step size unstable.\n\nAssume the sensing matrix $A$ has unit-norm columns and satisfies the Restricted Isometry Property (RIP) of order $k$ with constant $\\delta_k \\in (0,1)$, meaning that for any $k$-sparse vector $v \\in \\mathbb{R}^n$,\n$$\n(1 - \\delta_k) \\left\\|v\\right\\|_2^2 \\;\\le\\; \\left\\|A v\\right\\|_2^2 \\;\\le\\; (1 + \\delta_k) \\left\\|v\\right\\|_2^2.\n$$\nUnder this assumption, the eigenvalues of the restricted Gram matrix $A_{S_t}^\\top A_{S_t}$ lie in the interval $[(1-\\delta_k), (1+\\delta_k)]$. Consequently, for any nonzero $g_{S_t}$, the denominator $D_t$ obeys\n$$\n(1 - \\delta_k) \\left\\|g_{S_t}\\right\\|_2^2 \\;\\le\\; D_t \\;\\le\\; (1 + \\delta_k) \\left\\|g_{S_t}\\right\\|_2^2.\n$$\nIn practice, the computed $D_t$ may be contaminated by floating-point error with unit roundoff $u$ and may be affected by measurement noise $w$ with variance $\\sigma^2$ per entry. A principled safeguard is to add a small regularization term $\\epsilon$ to the denominator to obtain a stabilized step size, thereby replacing $D_t$ by $D_t + \\epsilon$ when $D_t$ is detected to be near zero. The goal is to detect when $D_t$ is near zero in a scale-aware manner and to choose $\\epsilon$ so that the resulting step size is controlled by the restricted Lipschitz constant of the gradient, which is $\\left\\|A_{S_t}\\right\\|_2^2$, while introducing negligible bias when $D_t$ is well-scaled.\n\nWhich of the following procedures gives a correct, scale-aware detection criterion and an adaptive choice of $\\epsilon$ that guarantees the stabilized normalized step size does not exceed $1 / \\left\\|A_{S_t}\\right\\|_2^2$ when degeneracy is detected, while keeping the bias negligible otherwise?\n\nA. Declare $D_t$ near zero when $D_t \\le 10^{-8}$, and set $\\epsilon = 10^{-8}$ for all iterations. This simple absolute threshold and fixed $\\epsilon$ stabilize the denominator without the need to estimate any matrix norms or noise levels.\n\nB. Compute the restricted spectral norm $L_{S_t} \\triangleq \\left\\|A_{S_t}\\right\\|_2^2$, and declare $D_t$ near zero when $D_t \\le \\beta \\, L_{S_t} \\, \\left\\|g_{S_t}\\right\\|_2^2$, where $\\beta = \\max\\{100\\,u,\\, \\sigma^2 / \\left\\|A\\right\\|_F^2\\}$. Upon detection, set\n$$\n\\epsilon \\;=\\; \\max\\Big\\{\\, L_{S_t} \\, \\left\\|g_{S_t}\\right\\|_2^2 - D_t,\\;\\; \\beta \\, L_{S_t} \\, \\left\\|g_{S_t}\\right\\|_2^2 \\Big\\},\n$$\nand otherwise set $\\epsilon = 0$. This choice ensures $D_t + \\epsilon \\ge L_{S_t} \\, \\left\\|g_{S_t}\\right\\|_2^2$ under detection, so the stabilized step size is at most $1 / L_{S_t}$, while the baseline $\\beta$ uses unit roundoff $u$ and the noise proxy $\\sigma^2$ to avoid scale-dependent thresholds.\n\nC. Declare $D_t$ near zero when $D_t \\le \\beta \\, \\left\\|r_t\\right\\|_2^2$, with $\\beta = 10^{-12}$, and set $\\epsilon = \\beta \\, \\left\\|r_t\\right\\|_2^2$. This residual-based rule avoids computing any matrix norms and ties the regularization to the data misfit.\n\nD. Always set $\\epsilon = u$ (the unit roundoff), since any larger regularization would bias the step size; do not perform any detection, because machine precision already guarantees stability in the denominator for well-conditioned matrices.", "solution": "The problem asks for a principled, scale-aware procedure to stabilize the denominator $D_t$ in the Normalized Iterative Hard Thresholding (NIHT) algorithm step size calculation. The procedure must involve a detection criterion for when $D_t$ is numerically close to zero and an adaptive choice of a regularization parameter $\\epsilon$ to be added to $D_t$. The stabilized step size must satisfy a specific bound when degeneracy is detected, and the procedure should introduce negligible bias otherwise.\n\nLet's first establish the context and requirements. The NIHT algorithm aims to recover a sparse signal $x^\\star$ from measurements $y = A x^\\star + w$. At iteration $t$, the update involves a step size $\\alpha_t$, often chosen to optimize the objective function in the direction of the gradient. The NIHT step size is given by $\\alpha_t = \\frac{\\|g_{S'_t}\\|_2^2}{\\|A_{S'_t} g_{S'_t}\\|_2^2}$, where $S'_t$ is the support of the gradient update before thresholding. The problem statement simplifies this by considering the support $S_t$ fixed for the analysis of the step size denominator, $D_t$.\n\nThe denominator is $D_t = \\|A_{S_t} g_{S_t}\\|_2^2$. The gradient is $g_t = A^\\top r_t = A^\\top(y - A x_t)$, and $g_{S_t}$ is its restriction to the support set $S_t$.\nThe goal is to find a procedure for choosing $\\epsilon$ to form a stabilized denominator $D_t + \\epsilon$ that satisfies two main conditions:\n1.  **Scale-aware detection:** The decision to regularize (i.e., use a non-zero $\\epsilon$) should be based on a criterion that is robust to the overall scale of the problem (e.g., the norm of the signal $x^\\star$ or measurements $y$). A fixed absolute threshold is generally not scale-aware.\n2.  **Step-size guarantee and low bias:** When degeneracy is detected, the choice of $\\epsilon$ must ensure that the resulting step size is \"safe\". The problem defines this safe upper bound as $1 / \\|A_{S_t}\\|_2^2$. Let $L_{S_t} \\triangleq \\|A_{S_t}\\|_2^2$ be the restricted spectral norm (squared), which acts as a local Lipschitz constant for the gradient of the data-fit term. The stabilized step size, which would be proportional to $1/(D_t+\\epsilon)$, needs to be controlled. The problem specifically mentions that the \"stabilized normalized step size does not exceed $1/\\|A_{S_t}\\|_2^2$\". In the context of NIHT, a common formulation involves a step of the form $x_{t+1/2} = x_t + \\alpha_t g_t$. The specific NIHT step size $\\alpha_t = \\|g_{S'_t}\\|_2^2 / \\|A_{S'_t}g_{S'_t}\\|_2^2$ can be seen as a specific instance. The problem's framing suggests controlling a quantity related to the gradient descent step size. Let's analyze the step size $\\alpha'_t = \\|g_{S_t}\\|_2^2 / (D_t+\\epsilon)$. The requirement is $\\alpha'_t \\le 1/L_{S_t}$.\nThis means we need:\n$$ \\frac{\\|g_{S_t}\\|_2^2}{D_t + \\epsilon} \\le \\frac{1}{L_{S_t}} \\implies L_{S_t} \\|g_{S_t}\\|_2^2 \\le D_t + \\epsilon $$\nSo, upon detection, $\\epsilon$ must satisfy $\\epsilon \\ge L_{S_t} \\|g_{S_t}\\|_2^2 - D_t$.\nWhen $D_t$ is not degenerate, $\\epsilon$ should be zero or negligible to avoid biasing the step size.\n\nThe problem states that the sensing matrix $A$ has unit-norm columns and satisfies the RIP with constant $\\delta_k$. This implies that for any $k$-element support set $S_t$, the matrix $A_{S_t}^\\top A_{S_t}$ has eigenvalues in $[1-\\delta_k, 1+\\delta_k]$. Therefore, $L_{S_t} = \\|A_{S_t}\\|_2^2 = \\lambda_{\\max}(A_{S_t}^\\top A_{S_t}) \\le 1+\\delta_k$. Also, for any non-zero vector $v$ on the support $S_t$, we have $(1-\\delta_k)\\|v\\|_2^2 \\le \\|A_{S_t} v\\|_2^2 \\le (1+\\delta_k)\\|v\\|_2^2$. Applying this to $g_{S_t}$:\n$$ (1-\\delta_k)\\|g_{S_t}\\|_2^2 \\le D_t \\le (1+\\delta_k)\\|g_{S_t}\\|_2^2 $$\nLet's use the tighter bound involving the restricted spectral norm:\n$$ \\lambda_{\\min}(A_{S_t}^\\top A_{S_t}) \\|g_{S_t}\\|_2^2 \\le D_t \\le L_{S_t} \\|g_{S_t}\\|_2^2 $$\nThe upper bound $L_{S_t} \\|g_{S_t}\\|_2^2$ provides a natural scale for $D_t$. A scale-aware detection criterion for degeneracy would be to check if $D_t$ is significantly smaller than this upper bound, i.e., $D_t \\le \\beta \\cdot L_{S_t} \\|g_{S_t}\\|_2^2$ for some small $\\beta  0$. The value of $\\beta$ should be related to the sources of numerical uncertainty: floating-point precision (unit roundoff $u$) and measurement noise (related to $\\sigma^2$).\n\nNow, we evaluate each option based on this framework.\n\n**Option A Evaluation**\nThis option proposes:\n- Detection: $D_t \\le 10^{-8}$.\n- Regularization: $\\epsilon = 10^{-8}$ for all iterations (or upon detection).\n\nThis proposal has two major flaws. First, the detection criterion $D_t \\le 10^{-8}$ uses a fixed absolute threshold. The quantities $g_t$ and $D_t$ are not scale-free; their magnitudes depend on the norm of the signal $x^\\star$ and noise $w$. A fixed threshold is not robust across different problem scales. For a very low-energy signal, a perfectly valid $D_t$ could fall below this threshold. For a high-energy signal, a numerically degenerate $D_t$ could be much larger than $10^{-8}$. Therefore, the criterion is not scale-aware.\nSecond, the choice of $\\epsilon = 10^{-8}$ is also fixed and does not guarantee the required step-size bound. There is no reason to believe that $D_t + 10^{-8} \\ge L_{S_t} \\|g_{S_t}\\|_2^2$. If $L_{S_t} \\|g_{S_t}\\|_2^2$ is large, this condition will be violated.\nThe statement \"for all iterations\" is also problematic as it introduces a persistent bias, contrary to the goal.\n\nVerdict: **Incorrect**.\n\n**Option B Evaluation**\nThis option proposes:\n- Detection: $D_t \\le \\beta \\, L_{S_t} \\, \\|g_{S_t}\\|_2^2$, with $\\beta = \\max\\{100\\,u,\\, \\sigma^2 / \\|A\\|_F^2\\}$.\n- Regularization (upon detection): $\\epsilon = \\max\\Big\\{ L_{S_t} \\|g_{S_t}\\|_2^2 - D_t, \\;\\; \\beta \\, L_{S_t} \\, \\|g_{S_t}\\|_2^2 \\Big\\}$.\n- Regularization (otherwise): $\\epsilon = 0$.\n\nLet's analyze this.\n- **Detection:** The criterion compares $D_t$ to a fraction of its natural upper bound, $L_{S_t} \\|g_{S_t}\\|_2^2$. This is an excellent scale-aware approach. The factor $\\beta$ is determined by the machine precision $u$ and a normalized noise-to-signal ratio proxy $\\sigma^2 / \\|A\\|_F^2$, which is physically and numerically motivated. This criterion is sound.\n- **Regularization:** If degeneracy is detected, we set $\\epsilon$ as defined. Let's compute the stabilized denominator $D_t + \\epsilon$:\n$$ D_t + \\epsilon = D_t + \\max\\Big\\{ L_{S_t} \\|g_{S_t}\\|_2^2 - D_t, \\;\\; \\beta \\, L_{S_t} \\, \\|g_{S_t}\\|_2^2 \\Big\\} $$\nThis is equivalent to:\n$$ D_t + \\epsilon = \\max\\Big\\{ D_t + (L_{S_t} \\|g_{S_t}\\|_2^2 - D_t), \\;\\; D_t + \\beta \\, L_{S_t} \\, \\|g_{S_t}\\|_2^2 \\Big\\} $$\n$$ D_t + \\epsilon = \\max\\Big\\{ L_{S_t} \\|g_{S_t}\\|_2^2, \\;\\; D_t + \\beta \\, L_{S_t} \\, \\|g_{S_t}\\|_2^2 \\Big\\} $$\nThis guarantees that $D_t + \\epsilon \\ge L_{S_t} \\|g_{S_t}\\|_2^2$. Consequently, the stabilized step size $\\alpha'_t = \\frac{\\|g_{S_t}\\|_2^2}{D_t+\\epsilon}$ satisfies:\n$$ \\alpha'_t \\le \\frac{\\|g_{S_t}\\|_2^2}{L_{S_t}\\|g_{S_t}\\|_2^2} = \\frac{1}{L_{S_t}} = \\frac{1}{\\|A_{S_t}\\|_2^2} $$\nThis exactly matches the required step-size guarantee. The presence of the second term in the $\\max$ for $\\epsilon$, i.e., $\\beta \\, L_{S_t} \\, \\|g_{S_t}\\|_2^2$, provides a minimum regularization level, which adds robustness against cases where floating-point errors might make $L_{S_t} \\|g_{S_t}\\|_2^2 - D_t$ negative or zero.\n- **Bias:** If no degeneracy is detected, $\\epsilon=0$. This introduces no bias, fulfilling the second requirement.\n\nThis procedure correctly implements a scale-aware detection and an adaptive regularization that enforces the desired stability condition while remaining inactive when not needed.\n\nVerdict: **Correct**.\n\n**Option C Evaluation**\nThis option proposes:\n- Detection: $D_t \\le \\beta \\, \\|r_t\\|_2^2$, with $\\beta = 10^{-12}$.\n- Regularization: $\\epsilon = \\beta \\, \\|r_t\\|_2^2$.\n\nLet's analyze this.\n- **Detection  Regularization:** The rule ties the detection threshold and the regularization term to the squared norm of the residual, $\\|r_t\\|_2^2$. Let's examine the relationship between the quantities. We have $g_t = A^\\top r_t$, so $\\|g_t\\|_2 \\le \\|A\\|_2 \\|r_t\\|_2$. Also, $D_t = \\|A_{S_t} g_{S_t}\\|_2^2 \\le L_{S_t} \\|g_{S_t}\\|_2^2 \\le L_{S_t} \\|g_{t}\\|_2^2 \\le L_{S_t} \\|A\\|_2^2 \\|r_t\\|_2^2$. While $D_t$ and $\\|r_t\\|_2^2$ are related, the relationship is mediated by matrix norms which can vary. Thus, comparing $D_t$ to $\\|r_t\\|_2^2$ is not as direct or robust as comparing $D_t$ to $\\|g_{S_t}\\|_2^2$, its natural scale companion.\n- **Step-size guarantee:** The most critical failure is in the step-size guarantee. The new denominator is $D_t' = D_t + \\epsilon = D_t + \\beta \\|r_t\\|_2^2$. We need to check if $D_t' \\ge L_{S_t} \\|g_{S_t}\\|_2^2$.\n$$ D_t + \\beta \\|r_t\\|_2^2 \\ge L_{S_t} \\|g_{S_t}\\|_2^2 \\quad \\text{(?) } $$\nThere is absolutely no guarantee that this inequality will hold. The ratio $\\|g_{S_t}\\|_2^2 / \\|r_t\\|_2^2$ is not a fixed constant and can be large. With a small fixed $\\beta=10^{-12}$, it is very likely that for a given instance, $\\beta \\|r_t\\|_2^2$ is insufficient to raise the denominator to the required level $L_{S_t} \\|g_{S_t}\\|_2^2$. This method fails to meet the primary design requirement.\n\nVerdict: **Incorrect**.\n\n**Option D Evaluation**\nThis option proposes:\n- Detection: None.\n- Regularization: Always set $\\epsilon = u$ (unit roundoff).\n\nThis approach is flawed for several reasons.\nFirst, it proposes no detection, applying regularization in every iteration regardless of whether it is needed. This introduces a systematic bias in the step size, moving it away from the NIHT-prescribed value even when the calculation is numerically stable. The goal is to correct a problem only when it occurs.\nSecond, the justification that \"machine precision already guarantees stability... for well-conditioned matrices\" is misleading. The problem statement correctly notes that even with a well-conditioned $A_{S_t}$ (due to RIP), the term $D_t = \\|A_{S_t}g_{S_t}\\|_2^2$ can be numerically small if the vector $g_{S_t}$ is small or nearly in a direction that is heavily attenuated by $A_{S_t}$. The magnitude of the gradient $g_t$ naturally approaches zero as the algorithm converges, so small $D_t$ values are expected and must be handled.\nThird, a fixed regularization $\\epsilon = u$ is not adaptive and provides no guarantee of meeting the step-size constraint $D_t + u \\ge L_{S_t} \\|g_{S_t}\\|_2^2$. The right-hand side scales with the problem and can be much larger than $u$.\n\nVerdict: **Incorrect**.", "answer": "$$\\boxed{B}$$", "id": "3463050"}, {"introduction": "The true test of an algorithm's power is witnessing its behavior in action. This final practice is a capstone coding exercise where you will implement NIHT and its variants to explore their dynamics in a carefully designed experiment. By constructing a scenario with a \"wrong support attractor,\" you will directly observe how NIHT's aggressive step size can escape local minima that trap simpler algorithms like IHT, and how adding a backtracking mechanism can provide the stability needed to solve even more challenging sparse recovery problems [@problem_id:3463072].", "problem": "You are asked to construct and analyze a sparse recovery experiment in compressed sensing that concretely demonstrates how a poor initialization can lead an algorithm into a wrong support attractor, and how two design choices—normalization of the gradient step and backtracking—can help escape that attractor. The recovery task is modeled as minimizing the quadratic loss subject to a sparsity constraint:\n- Given a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$, a target sparse signal $x^\\star \\in \\mathbb{R}^n$ with $\\lVert x^\\star \\rVert_0 \\leq k$, and either noiseless measurements $y = A x^\\star$ or noisy measurements $y = A x^\\star + w$, consider the objective $f(x) = \\tfrac{1}{2} \\lVert y - A x \\rVert_2^2$, where $x \\in \\mathbb{R}^n$ is constrained by $\\lVert x \\rVert_0 \\leq k$.\n\nStart from the following core foundations:\n- The gradient of the quadratic loss is $\\nabla f(x) = - A^\\top (y - A x)$.\n- The Lipschitz constant of $\\nabla f$ is $L = \\lVert A \\rVert_2^2$, the square of the spectral norm of $A$.\n- The hard-thresholding operator $H_k(\\cdot)$ projects a vector in $\\mathbb{R}^n$ onto the set $\\{x : \\lVert x \\rVert_0 \\leq k\\}$ by keeping the $k$ largest entries in magnitude and setting the others to zero.\n\nYou must implement and compare three algorithms:\n- Fixed-Step Iterative Hard Thresholding (IHT): $x_{t+1} = H_k\\!\\big(x_t + \\mu A^\\top (y - A x_t)\\big)$ with constant step size $\\mu$.\n- Normalized Iterative Hard Thresholding (NIHT): $x_{t+1} = H_k\\!\\big(x_t + \\mu_t A^\\top (y - A x_t)\\big)$ with a normalized step $\\mu_t$ chosen at each iteration as the steepest-descent line minimizer along the gradient direction, i.e., the unique $\\mu_t$ that minimizes $f(x_t + \\mu A^\\top (y - A x_t))$ over $\\mu \\in \\mathbb{R}$.\n- NIHT with Backtracking (NIHT-BT): combine the NIHT step with a backtracking line search that reduces the step size multiplicatively until a sufficient decrease of $f$ is observed after the $H_k$ projection.\n\nDesign an experiment that induces a wrong support attractor from a poor initialization as follows:\n- Construct a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m = 32$ and $n = 64$ by drawing independent standard normal entries and normalizing columns to unit $\\ell_2$ norm. Then, enforce strong column correlations to create “support confusers”: create pairs of highly correlated columns by replacing certain columns with nearly scaled copies of others (follow the exact recipe specified in the test suite). This makes multiple $k$-sparse supports produce similarly good fits to $y$.\n- Choose a ground-truth $k$-sparse signal $x^\\star$ with $k=2$ supported on a pair of columns that have near-duplicates elsewhere in $A$.\n- Initialize $x_0$ by solving a least-squares fit on a “wrong” $k$-sparse support that uses the duplicate columns, i.e., compute $x_0$ with $\\operatorname{supp}(x_0) = S_{\\text{wrong}}$ and $x_0[S_{\\text{wrong}}] = \\arg\\min_{z \\in \\mathbb{R}^k} \\lVert y - A_{S_{\\text{wrong}}} z \\rVert_2$, while other entries are zero. This initialization produces a wrong support attractor for fixed-step IHT with a sufficiently small $\\mu$ because the gradient inside $S_{\\text{wrong}}$ vanishes and the scaled gradient entries outside $S_{\\text{wrong}}$ remain too small to overcome thresholding.\n\nRequired program behavior:\n- Implement the three methods (IHT, NIHT, NIHT-BT) as described above, using $f(x) = \\tfrac{1}{2} \\lVert y - A x \\rVert_2^2$ and $H_k$ as the hard-threshold operator.\n- Implement the NIHT normalization at iteration $t$ as the exact steepest-descent step along the gradient direction $g_t = A^\\top (y - A x_t)$, i.e., choose $\\mu_t$ that minimizes $\\phi(\\mu) = f(x_t + \\mu g_t)$ over $\\mu \\in \\mathbb{R}$.\n- Implement backtracking with multiplicative shrinkage by a factor $\\beta \\in (0,1)$ until you obtain a sufficient decrease of $f$ after applying $H_k$ to the tentative update; use a standard Armijo-type condition with a small constant $c \\in (0,1)$.\n\nTest suite:\n- Use the following test cases. In all cases, the random number generator must be initialized at the start with seed $123$ to ensure reproducibility, and all columns of $A$ must be scaled to unit $\\ell_2$ norm.\n    1. Case $1$ (wrong support attractor for fixed-step IHT):\n        - Dimensions: $m = 32$, $n = 64$, sparsity $k = 2$.\n        - Matrix $A$: start with independent standard normal entries, normalize columns to unit norm. Then enforce two moderate “confuser” pairs by setting column $10$ to be a renormalized version of column $0$ perturbed by a small orthogonal component of magnitude $0.02$, and column $11$ to be a renormalized version of column $1$ perturbed by a small orthogonal component of magnitude $0.02$.\n        - Ground truth: support $S^\\star = \\{0, 1\\}$ with $x^\\star[0] = 1.0$, $x^\\star[1] = 0.8$, and all other entries zero. Measurements $y = A x^\\star$.\n        - Initialization: $x_0$ is the least-squares solution on $S_{\\text{wrong}} = \\{10, 11\\}$.\n        - Algorithm: fixed-step IHT with $\\mu = 0.2 / L$, where $L = \\lVert A \\rVert_2^2$, run for $500$ iterations.\n        - Output for this case: a boolean indicating whether the recovered support equals $S^\\star$ exactly.\n    2. Case $2$ (normalization helps escape):\n        - Same $A$, $x^\\star$, $y$, and $x_0$ as in Case $1$.\n        - Algorithm: NIHT with steepest-descent normalization at each iteration, run for $200$ iterations, no backtracking.\n        - Output: boolean of exact support recovery.\n    3. Case $3$ (harder confusers without backtracking):\n        - Same $m$, $n$, $k$ and base normalization of $A$ as above. In addition to the modifications in Case $1$, enforce a second, stronger pair of confusers by setting column $20$ to be a renormalized version of column $0$ perturbed by magnitude $0.001$, and column $21$ to be a renormalized version of column $1$ perturbed by magnitude $0.001$.\n        - Ground truth: same $S^\\star$ and $x^\\star$.\n        - Measurements: $y = A x^\\star + w$, where $w$ is Gaussian noise with independent entries with standard deviation $\\sigma = 0.002$.\n        - Initialization: $x_0$ is the least-squares solution on $S_{\\text{wrong}} = \\{20, 21\\}$.\n        - Algorithm: NIHT without backtracking, run for $50$ iterations.\n        - Output: boolean of exact support recovery.\n    4. Case $4$ (backtracking helps in the harder scenario):\n        - Same setting as Case $3$.\n        - Algorithm: NIHT with backtracking using Armijo parameter $c = 10^{-4}$ and shrinkage factor $\\beta = 0.5$, run for $500$ iterations.\n        - Output: boolean of exact support recovery.\n\nImplementation details:\n- The hard-thresholding operator $H_k$ must keep exactly the $k$ entries of largest absolute value and zero out the rest. In the case of ties, break them deterministically by the natural ordering of indices.\n- Use the Euclidean norm for all $\\ell_2$ norms.\n- The termination criterion in each test case is the fixed iteration budget specified; you do not need to implement any adaptive stopping.\n- To evaluate exact support recovery, compare the set of indices of the nonzero entries of the returned estimate with $S^\\star$; report true if they are equal as sets, otherwise false.\n\nFinal output format:\n- Your program should produce a single line of output containing the four boolean results as a comma-separated list enclosed in square brackets (e.g., $[\\text{True},\\text{False},\\text{True},\\text{True}]$), corresponding to Cases $1$ through $4$ in order. No other text must be printed.", "solution": "The problem requires the implementation and comparison of three sparse recovery algorithms—Iterative Hard Thresholding (IHT), Normalized IHT (NIHT), and NIHT with Backtracking (NIHT-BT)—to demonstrate a specific failure mode and its remedies. The core task is to find a $k$-sparse signal $x \\in \\mathbb{R}^n$ that minimizes the quadratic loss $f(x) = \\tfrac{1}{2} \\lVert y - A x \\rVert_2^2$, where $y \\in \\mathbb{R}^m$ are measurements from a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$.\n\nAll three algorithms are based on projected gradient descent. The general update step is $x_{t+1} = P_{\\mathcal{C}_k}(x_t - \\mu_t \\nabla f(x_t))$, where $\\mathcal{C}_k = \\{x : \\lVert x \\rVert_0 \\leq k\\}$ is the non-convex set of $k$-sparse vectors, and the projection $P_{\\mathcal{C}_k}$ is the hard-thresholding operator $H_k(\\cdot)$. The gradient of the objective function is $\\nabla f(x) = A^\\top(Ax-y)$. Substituting this gives the update rule:\n$$x_{t+1} = H_k\\!\\big(x_t + \\mu_t A^\\top (y - A x_t)\\big)$$\nThe algorithms differ primarily in their choice of the step size $\\mu_t$.\n\n**Experimental Design: The Wrong Support Attractor**\nThe experiment is designed to create a scenario where a greedy algorithm can easily get trapped in a local minimum corresponding to an incorrect support set. This is achieved by constructing a sensing matrix $A$ with highly correlated columns.\nSpecifically, if we have two columns $a_i$ and $a_j$ such that $a_i \\approx \\alpha a_j$ for some scalar $\\alpha$, they become confusable. If the true signal $x^\\star$ is supported on index $i$, the model $y = A x^\\star \\approx x^\\star_i a_i$ can be well-approximated by a model using column $j$, i.e., $y \\approx z_j a_j$ for some $z_j$.\nBy initializing the algorithm with a least-squares solution on a \"wrong\" but highly correlated support $S_{\\text{wrong}}$, we start at a point $x_0$ where the residual $r_0 = y - Ax_0$ is nearly orthogonal to the columns in $A_{S_{\\text{wrong}}}$. Consequently, the gradient components $g_0[S_{\\text{wrong}}] = A_{S_{\\text{wrong}}}^\\top r_0$ are very small. If the step size $\\mu$ is small, the update $x_0 + \\mu g_0$ will have its largest magnitude components on the same support $S_{\\text{wrong}}$, causing $H_k$ to select the same incorrect support. The algorithm becomes trapped. This is the \"wrong support attractor\".\n\n**Algorithm 1: Fixed-Step Iterative Hard Thresholding (IHT)**\nIHT uses a fixed step size $\\mu$. A standard choice guaranteeing convergence of the gradient magnitude to zero requires $\\mu  2/L$, where $L = \\lVert A \\rVert_2^2$ is the Lipschitz constant of $\\nabla f$. The problem specifies a conservative step size $\\mu = 0.2/L$. In Case $1$, this small step size is insufficient to overcome the attraction to the wrong support $S_{\\text{wrong}} = \\{10, 11\\}$, and the algorithm is expected to fail to recover the true support $S^\\star = \\{0, 1\\}$.\n\n**Algorithm 2: Normalized Iterative Hard Thresholding (NIHT)**\nNIHT employs an adaptive step size $\\mu_t$ calculated at each iteration. It is chosen to be the optimal step size that minimizes the quadratic objective along the gradient direction, *before* the projection step. Let $g_t = A^\\top(y - Ax_t) = -\\nabla f(x_t)$ be the search direction. We seek to minimize $\\phi(\\mu) = f(x_t + \\mu g_t) = \\tfrac{1}{2} \\lVert (y - Ax_t) - \\mu A g_t \\rVert_2^2$. Taking the derivative with respect to $\\mu$ and setting it to zero yields the optimal step size:\n$$\\mu_t = \\frac{g_t^\\top A^\\top(y - Ax_t)}{\\lVert A g_t \\rVert_2^2} = \\frac{g_t^\\top g_t}{\\lVert A g_t \\rVert_2^2} = \\frac{\\lVert g_t \\rVert_2^2}{\\lVert A g_t \\rVert_2^2}$$\nThis step size is typically much larger and more aggressive than the fixed step in IHT. As demonstrated in Case $2$, this larger step can provide a sufficient \"kick\" to the iterate, moving it far enough from the starting point $x_t$ such that the true support elements in $x_t + \\mu_t g_t$ become large enough to be selected by the thresholding operator $H_k$, thus escaping the attractor. However, as shown in Case $3$, when column correlations are extremely high and noise is present, this aggressive step can cause instability or overshooting, leading to failure.\n\n**Algorithm 3: NIHT with Backtracking (NIHT-BT)**\nTo add robustness, NIHT can be augmented with a backtracking line search. This ensures that each step results in a sufficient decrease of the objective function, preventing the overshooting that can plague vanilla NIHT. After calculating the aggressive NIHT step $\\mu_t$, we test a candidate update. The update is accepted only if it satisfies a sufficient decrease condition. The problem specifies an Armijo-type condition, which for projected gradient methods is commonly expressed as:\n$$f(x_{t+1}) \\leq f(x_t) + c \\cdot \\nabla f(x_t)^\\top (x_{t+1} - x_t)$$\nwhere $x_{t+1} = H_k(x_t - \\mu \\nabla f(x_t))$ and $c \\in (0,1)$ is a control parameter (here given as $c=10^{-4}$). If the condition is not met, the step size $\\mu$ is multiplicatively reduced (by a factor $\\beta=0.5$) and the test is repeated. This procedure starts with the large NIHT step and shrinks it only when necessary, combining the speed of NIHT with the stability of a guaranteed descent method. In Case $4$, this stabilization allows the algorithm to successfully navigate the difficult recovery landscape with very high column correlations and noise, ultimately finding the correct support.\n\nThe hard-thresholding operator $H_k(v)$ is implemented to keep the $k$ elements of $v$ with the largest absolute values. To ensure deterministic behavior, ties in magnitude are broken by choosing the element with the smaller index.\n\nThe sequence of experiments is designed to show:\n1.  **Case 1:** A simple IHT with a conservative step size fails when initialized in a wrong support attractor.\n2.  **Case 2:** The aggressive, normalized step of NIHT successfully escapes this attractor.\n3.  **Case 3:** In a more challenging scenario with higher correlations and noise, the aggressive NIHT step becomes unstable and fails.\n4.  **Case 4:** Adding a backtracking mechanism to NIHT provides the necessary stability to solve the challenging problem.\n\nThis progression concretely demonstrates the trade-offs between step size selection strategies in iterative sparse recovery algorithms.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef hard_threshold(v, k):\n    \"\"\"\n    Performs hard thresholding on a vector v, keeping k elements.\n    Ties in magnitude are broken by the natural ordering of indices.\n    \"\"\"\n    if k = 0:\n        return np.zeros_like(v)\n    if k = len(v):\n        return v.copy()\n    \n    # Use lexsort for deterministic tie-breaking by index.\n    # Sort keys are (-abs(v), index), so it sorts by descending magnitude,\n    # then ascending index for ties.\n    indices_to_keep = np.lexsort((np.arange(len(v)), -np.abs(v)))[:k]\n    \n    result = np.zeros_like(v)\n    result[indices_to_keep] = v[indices_to_keep]\n    return result\n\ndef create_A_with_confusers(A_base, confusers):\n    \"\"\"\n    Modifies a base matrix A to include confuser columns.\n    \"\"\"\n    A = A_base.copy()\n    n = A.shape[1]\n    for new_idx, ref_idx, pert_mag in confusers:\n        # Use a deterministic source for the orthogonal vector to ensure reproducibility.\n        orth_src_idx = (ref_idx + 5) % n \n        if orth_src_idx == new_idx: # Ensure it's not the column we are creating\n            orth_src_idx = (orth_src_idx + 1) % n\n\n        u = A[:, ref_idx]\n        v = A[:, orth_src_idx]\n        \n        # Gram-Schmidt to get a vector orthogonal to u\n        v_orth = v - (v.T @ u) * u\n        v_orth_unit = v_orth / np.linalg.norm(v_orth)\n        \n        # Create the new column by perturbing u and re-normalize to unit norm\n        new_col = u + pert_mag * v_orth_unit\n        A[:, new_idx] = new_col / np.linalg.norm(new_col)\n    return A\n\ndef compute_x0(A, y, S_wrong):\n    \"\"\"\n    Computes the initial guess x0 as the least-squares solution on the wrong support.\n    \"\"\"\n    S_wrong_list = sorted(list(S_wrong)) # Ensure consistent index order\n    A_wrong = A[:, S_wrong_list]\n    \n    # Solve least squares: z = argmin ||y - A_wrong * z||^2\n    z = np.linalg.lstsq(A_wrong, y, rcond=None)[0]\n    \n    x0 = np.zeros(A.shape[1])\n    x0[S_wrong_list] = z\n    return x0\n\ndef run_iht(A, y, x0, k, S_star, iterations, mu_factor):\n    \"\"\"\n    Case 1: Fixed-step Iterative Hard Thresholding.\n    \"\"\"\n    x = x0.copy()\n    L = np.linalg.norm(A, 2)**2\n    mu = mu_factor / L\n    \n    for _ in range(iterations):\n        grad = A.T @ (A @ x - y)\n        x = hard_threshold(x - mu * grad, k)\n        \n    recovered_support = set(np.where(x != 0)[0])\n    return recovered_support == S_star\n\ndef run_niht(A, y, x0, k, S_star, iterations):\n    \"\"\"\n    Case 2  3: Normalized Iterative Hard Thresholding.\n    \"\"\"\n    x = x0.copy()\n    \n    for _ in range(iterations):\n        r = y - A @ x\n        g = A.T @ r # This is -grad f(x), so we will add mu * g\n        \n        if np.linalg.norm(g)  1e-12:\n            break\n\n        Ag = A @ g\n        denom = Ag.T @ Ag\n        if denom  1e-20:\n            break\n        \n        mu = (g.T @ g) / denom\n        x = hard_threshold(x + mu * g, k)\n        \n    recovered_support = set(np.where(x != 0)[0])\n    return recovered_support == S_star\n\ndef run_niht_bt(A, y, x0, k, S_star, iterations, c, beta):\n    \"\"\"\n    Case 4: NIHT with Backtracking.\n    \"\"\"\n    x = x0.copy()\n    \n    for _ in range(iterations):\n        r = y - A @ x\n        fx = 0.5 * (r.T @ r)\n        g = A.T @ r # Corresponds to -grad f(x)\n        \n        if np.linalg.norm(g)  1e-12:\n            break\n            \n        Ag = A @ g\n        denom = Ag.T @ Ag\n        if denom  1e-20:\n            break\n            \n        mu = (g.T @ g) / denom\n        grad = -g\n        \n        while True:\n            x_cand = hard_threshold(x + mu * g, k)\n            \n            r_cand = y - A @ x_cand\n            fx_cand = 0.5 * (r_cand.T @ r_cand)\n            \n            # Armijo condition for proximal gradient methods\n            # f(x_cand) = f(x) + c * grad f(x), x_cand - x\n            armijo_rhs = fx + c * grad.T @ (x_cand - x)\n            \n            if fx_cand = armijo_rhs:\n                x = x_cand\n                break\n            \n            mu *= beta\n            if mu  1e-20: # Failsafe for convergence or precision issues\n                x = x_cand # take the last step even if it fails condition\n                break\n                \n    recovered_support = set(np.where(x != 0)[0])\n    return recovered_support == S_star\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'id': 1}, {'id': 2}, {'id': 3}, {'id': 4},\n    ]\n\n    # Common parameters\n    m, n, k = 32, 64, 2\n    S_star = {0, 1}\n    x_star_vals = {0: 1.0, 1: 0.8}\n    \n    # Reproducibility\n    rng = np.random.default_rng(123)\n    \n    # --- Generate common data structures ---\n    \n    # Base matrix A\n    A_base = rng.standard_normal((m, n))\n    A_base /= np.linalg.norm(A_base, axis=0, keepdims=True)\n    \n    # Ground truth signal x_star\n    x_star = np.zeros(n)\n    for idx, val in x_star_vals.items():\n        x_star[idx] = val\n\n    results = []\n\n    # --- Case 1: IHT failure ---\n    confusers_1_2 = [(10, 0, 0.02), (11, 1, 0.02)]\n    A1 = create_A_with_confusers(A_base, confusers_1_2)\n    y1 = A1 @ x_star\n    x0_1 = compute_x0(A1, y1, {10, 11})\n    \n    res1 = run_iht(A1, y1, x0_1, k, S_star, iterations=500, mu_factor=0.2)\n    results.append(res1)\n    \n    # --- Case 2: NIHT success ---\n    # Uses same A1, y1, x0_1 from Case 1\n    res2 = run_niht(A1, y1, x0_1, k, S_star, iterations=200)\n    results.append(res2)\n    \n    # --- Case 3: NIHT failure (harder problem) ---\n    confusers_3_4 = [(20, 0, 0.001), (21, 1, 0.001)]\n    # Add more confusers to matrix from cases 1/2\n    A2 = create_A_with_confusers(A1, confusers_3_4)\n    w = rng.normal(loc=0.0, scale=0.002, size=m)\n    y2 = A2 @ x_star + w\n    x0_2 = compute_x0(A2, y2, {20, 21})\n    \n    res3 = run_niht(A2, y2, x0_2, k, S_star, iterations=50)\n    results.append(res3)\n\n    # --- Case 4: NIHT-BT success (harder problem) ---\n    # Uses same A2, y2, x0_2 from Case 3\n    res4 = run_niht_bt(A2, y2, x0_2, k, S_star, iterations=500, c=1e-4, beta=0.5)\n    results.append(res4)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3463072"}]}