{"hands_on_practices": [{"introduction": "Before applying any recovery algorithm, a crucial first step is designing a sensing system that captures sufficient information. This practice introduces the fundamental scaling law that relates the required number of measurements $m$ to the signal's dimension $n$ and its sparsity $k$. By applying this well-established heuristic derived from the theory of the Restricted Isometry Property [@problem_id:3436582], you will develop a practical intuition for the resource requirements of a typical compressed sensing setup.", "problem": "Consider a $k$-sparse signal $x \\in \\mathbb{R}^{n}$ with $k \\ll n$, measured through a linear system $y = A x$ where $A \\in \\mathbb{R}^{m \\times n}$ has independent and identically distributed subgaussian entries with zero mean and unit variance. Recovery is performed by Compressive Sampling Matching Pursuit (CoSaMP), a greedy algorithm that exploits sparsity by iteratively identifying and refining a candidate support set. For subgaussian measurement ensembles, a widely accepted well-tested fact from high-dimensional probability and the theory of the Restricted Isometry Property (RIP) states that, with high probability, the matrix $A$ satisfies an order-$2k$ RIP with a sufficiently small constant when the number of measurements scales on the order of $m \\geq C k \\log(n/k)$, where $C$ is an absolute constant depending on the desired RIP level and failure probability.\n\nUsing this scaling law as a heuristic for the minimum measurements needed to enable stable support recovery by CoSaMP, estimate a plausible value of $m$ for $n=1000$ and $k=20$, taking $C=10$. Use the natural logarithm for $\\log$, and round your answer to four significant figures. Briefly discuss the sensitivity of the estimated $m$ to the choice of $C$ based on the dependence implied by the heuristic, but provide only the numerical estimate of $m$ as your final answer.", "solution": "The problem requires an estimation of the minimum number of measurements, denoted by $m$, for a compressive sensing scenario. The estimation is to be based on a provided heuristic scaling law that relates $m$ to the signal dimension $n$, the sparsity level $k$, and an absolute constant $C$.\n\nThe given scaling law is:\n$$m \\geq C k \\log\\left(\\frac{n}{k}\\right)$$\nThis inequality provides a lower bound on the number of measurements $m$ sufficient for stable signal recovery using algorithms like Compressive Sampling Matching Pursuit (CoSaMP), under the condition that the measurement matrix $A$ satisfies the Restricted Isometry Property (RIP). The problem asks for a plausible estimate of $m$, which we will take to be this lower bound. The logarithm is specified to be the natural logarithm, which we will denote as $\\ln$.\n\nThe specific parameters provided are:\n- Signal dimension, $n = 1000$.\n- Sparsity level, $k = 20$.\n- A constant, $C = 10$.\n\nWe substitute these values into the expression for the lower bound of $m$:\n$$m \\approx C k \\ln\\left(\\frac{n}{k}\\right)$$\n$$m \\approx 10 \\times 20 \\times \\ln\\left(\\frac{1000}{20}\\right)$$\n\nFirst, we calculate the ratio inside the logarithm:\n$$\\frac{n}{k} = \\frac{1000}{20} = 50$$\n\nNow, the expression for $m$ simplifies to:\n$$m \\approx 200 \\times \\ln(50)$$\n\nUsing the value of the natural logarithm of $50$:\n$$\\ln(50) \\approx 3.91202300543$$\n\nWe can now compute the estimate for $m$:\n$$m \\approx 200 \\times 3.91202300543$$\n$$m \\approx 782.404601086$$\n\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures are $7$, $8$, $2$, and $4$. The fifth digit is $0$, so we round down.\n$$m \\approx 782.4$$\n\nAs requested, we briefly discuss the sensitivity of the estimated $m$ to the choice of the constant $C$. The relationship is given by $m \\approx C k \\ln(n/k)$. For fixed signal parameters $n$ and $k$, the estimated number of measurements $m$ is directly proportional to the constant $C$. This linear dependence implies that the estimate for $m$ is highly sensitive to the choice of $C$. For instance, if the constant $C$ were doubled to $20$, the estimated number of required measurements would also double to approximately $1565$. The constant $C$ itself is determined by the desired strength of the RIP (i.e., the value of the isometry constant $\\delta_{2k}$) and the acceptable probability of failure for the random matrix ensemble. Stricter requirements (a smaller $\\delta_{2k}$ or a lower failure probability) lead to a larger value of $C$, thereby increasing the number of measurements needed for guaranteed recovery. Therefore, the choice of $C$ is a critical parameter that directly scales the resource requirements of the sensing process.", "answer": "$$\\boxed{782.4}$$", "id": "3436582"}, {"introduction": "With the theoretical requirements for sensing in mind, we now dive into the engine of the CoSaMP algorithm. This exercise provides a concrete, step-by-step walkthrough of a single iteration, from identifying promising support indices via correlation to solving a least-squares problem and pruning the resulting estimate [@problem_id:3434637]. Performing this detailed calculation is essential for moving beyond a \"black-box\" view and truly grasping the mechanics of this powerful greedy pursuit method.", "problem": "Consider a linear measurement model in compressed sensing, where a signal $x \\in \\mathbb{R}^{5}$ is $k$-sparse in the canonical basis (i.e., the change-of-basis matrix is the identity), and the measurements are given by $y = A x$ with $A \\in \\mathbb{R}^{3 \\times 5}$. Let $A = \\begin{bmatrix} 1 & 0 & 1 & 0 & 0 \\\\ 0 & 1 & 1 & 1 & 0 \\\\ 1 & 0 & 0 & 1 & 1 \\end{bmatrix}$, $\\Phi = I_{5}$, and $x = \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\\\ 0 \\\\ 0 \\end{bmatrix}$, so that $y = A x$. Start from the foundational definitions: linear measurements $y = A x$, sparsity of $x$ in the identity basis, support as the set of indices of nonzero entries, and least-squares fitting as minimizing the Euclidean norm of the residual. Perform one full iteration of Compressive Sampling Matching Pursuit (CoSaMP), with target sparsity $k = 2$, starting from the zero initial estimate $x^{(0)} = 0$ and residual $r^{(0)} = y$. Follow these fundamental steps:\n\n- Compute the proxy $u = A^{\\top} r^{(0)}$.\n- Identify the index set $\\Omega$ corresponding to the $2k$ largest entries of $|u|$ in magnitude.\n- Merge with the current support to form $\\mathcal{T} = \\Omega \\cup \\operatorname{supp}(x^{(0)})$.\n- Solve the least-squares problem $b = \\arg\\min_{z \\in \\mathbb{R}^{5},\\, \\operatorname{supp}(z) \\subseteq \\mathcal{T}} \\| y - A z \\|_{2}$; if there are multiple minimizers, choose the one with minimal Euclidean norm.\n- Prune $b$ to its $k$ largest entries in magnitude to obtain $x^{(1)}$.\n- Form the updated residual $r^{(1)} = y - A x^{(1)}$.\n\nWhat is the Euclidean norm $\\| r^{(1)} \\|_{2}$? Provide your final answer as a closed-form analytic expression. Do not approximate or round.", "solution": "The problem requires performing one full iteration of the Compressive Sampling Matching Pursuit (CoSaMP) algorithm to find the Euclidean norm of the updated residual, $\\| r^{(1)} \\|_{2}$. We are given the linear measurement model $y = Ax$, where the signal $x \\in \\mathbb{R}^{5}$ is sparse in the canonical basis.\n\nThe provided data are:\n- Measurement matrix: $A = \\begin{bmatrix} 1 & 0 & 1 & 0 & 0 \\\\ 0 & 1 & 1 & 1 & 0 \\\\ 1 & 0 & 0 & 1 & 1 \\end{bmatrix} \\in \\mathbb{R}^{3 \\times 5}$\n- True signal: $x = \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\\\ 0 \\\\ 0 \\end{bmatrix} \\in \\mathbb{R}^{5}$\n- Target sparsity for the algorithm: $k = 2$\n- Initial signal estimate: $x^{(0)} = 0 \\in \\mathbb{R}^{5}$\n- The change-of-basis matrix is the identity matrix $\\Phi = I_{5}$.\n\nThe iteration follows a prescribed sequence of steps. First, we compute the measurement vector $y$.\n$$y = Ax = \\begin{bmatrix} 1 & 0 & 1 & 0 & 0 \\\\ 0 & 1 & 1 & 1 & 0 \\\\ 1 & 0 & 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1(1) + 1(2) \\\\ 1(2) \\\\ 1(1) \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 2 \\\\ 1 \\end{bmatrix}$$\nThe initial residual is $r^{(0)} = y = \\begin{bmatrix} 3 \\\\ 2 \\\\ 1 \\end{bmatrix}$.\n\nNow, we proceed with the steps of the CoSaMP iteration.\n\n1.  **Compute the proxy**: The proxy $u$ is given by $u = A^{\\top} r^{(0)}$.\n    $$A^{\\top} = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\n    $$u = A^{\\top} r^{(0)} = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} 3 \\\\ 2 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1(3) + 1(1) \\\\ 1(2) \\\\ 1(3) + 1(2) \\\\ 1(2) + 1(1) \\\\ 1(1) \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 2 \\\\ 5 \\\\ 3 \\\\ 1 \\end{bmatrix}$$\n\n2.  **Identify the support**: We identify the index set $\\Omega$ corresponding to the $2k$ largest entries of $|u|$ in magnitude. Here, $k=2$, so $2k=4$. The vector $u$ has all positive entries, so $|u|=u$. The entries of $u$ in descending order of magnitude are $5, 4, 3, 2$, which correspond to indices $3, 1, 4, 2$.\n    $$\\Omega = \\{1, 2, 3, 4\\}$$\n\n3.  **Merge the support**: The new candidate support $\\mathcal{T}$ is the union of $\\Omega$ and the support of the previous estimate $x^{(0)}$.\n    $$\\operatorname{supp}(x^{(0)}) = \\operatorname{supp}(0) = \\emptyset$$\n    $$\\mathcal{T} = \\Omega \\cup \\operatorname{supp}(x^{(0)}) = \\{1, 2, 3, 4\\}$$\n\n4.  **Solve the least-squares problem**: We must find the vector $b$ with support contained in $\\mathcal{T}$ that minimizes $\\|y - Az\\|_2$. This is equivalent to solving for $b_{\\mathcal{T}}$ from the system $A_{\\mathcal{T}} b_{\\mathcal{T}} \\approx y$ in a least-squares sense, where $A_{\\mathcal{T}}$ is the submatrix of $A$ containing the columns indexed by $\\mathcal{T}$.\n    $$A_{\\mathcal{T}} = \\begin{bmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 1 & 1 \\\\ 1 & 0 & 0 & 1 \\end{bmatrix}$$\n    The problem specifies that if there are multiple minimizers, we choose the one with minimal Euclidean norm. This corresponds to the solution $b_{\\mathcal{T}} = A_{\\mathcal{T}}^{\\dagger} y$, where $A_{\\mathcal{T}}^{\\dagger}$ is the Moore-Penrose pseudoinverse of $A_{\\mathcal{T}}$. Since $A_{\\mathcal{T}}$ is a $3 \\times 4$ matrix with rank $3$ (it has full row rank), its pseudoinverse is given by $A_{\\mathcal{T}}^{\\dagger} = A_{\\mathcal{T}}^{\\top}(A_{\\mathcal{T}}A_{\\mathcal{T}}^{\\top})^{-1}$.\n    First, we compute $A_{\\mathcal{T}}A_{\\mathcal{T}}^{\\top}$:\n    $$A_{\\mathcal{T}}A_{\\mathcal{T}}^{\\top} = \\begin{bmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 1 & 1 \\\\ 1 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 & 1 \\\\ 1 & 3 & 1 \\\\ 1 & 1 & 2 \\end{bmatrix}$$\n    Next, we find the inverse of this $3 \\times 3$ matrix. The determinant is $\\det(A_{\\mathcal{T}}A_{\\mathcal{T}}^{\\top}) = 2(6-1) - 1(2-1) + 1(1-3) = 10 - 1 - 2 = 7$.\n    The inverse is:\n    $$(A_{\\mathcal{T}}A_{\\mathcal{T}}^{\\top})^{-1} = \\frac{1}{7} \\begin{bmatrix} 5 & -1 & -2 \\\\ -1 & 3 & -1 \\\\ -2 & -1 & 5 \\end{bmatrix}$$\n    Now we can calculate $b_{\\mathcal{T}}$:\n    $$b_{\\mathcal{T}} = A_{\\mathcal{T}}^{\\top}(A_{\\mathcal{T}}A_{\\mathcal{T}}^{\\top})^{-1} y = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{bmatrix} \\left( \\frac{1}{7} \\begin{bmatrix} 5 & -1 & -2 \\\\ -1 & 3 & -1 \\\\ -2 & -1 & 5 \\end{bmatrix} \\begin{bmatrix} 3 \\\\ 2 \\\\ 1 \\end{bmatrix} \\right)$$\n    $$b_{\\mathcal{T}} = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{bmatrix} \\left( \\frac{1}{7} \\begin{bmatrix} 15-2-2 \\\\ -3+6-1 \\\\ -6-2+5 \\end{bmatrix} \\right) = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{bmatrix} \\left( \\frac{1}{7} \\begin{bmatrix} 11 \\\\ 2 \\\\ -3 \\end{bmatrix} \\right) = \\frac{1}{7} \\begin{bmatrix} 11-3 \\\\ 2 \\\\ 11+2 \\\\ 2-3 \\end{bmatrix} = \\frac{1}{7} \\begin{bmatrix} 8 \\\\ 2 \\\\ 13 \\\\ -1 \\end{bmatrix}$$\n    The full vector $b \\in \\mathbb{R}^5$ is constructed by placing these values at the indices in $\\mathcal{T}$ and zeros elsewhere:\n    $$b = \\begin{bmatrix} 8/7 \\\\ 2/7 \\\\ 13/7 \\\\ -1/7 \\\\ 0 \\end{bmatrix}$$\n\n5.  **Prune the estimate**: We obtain the new signal estimate $x^{(1)}$ by keeping the $k=2$ largest-magnitude components of $b$. The magnitudes of the non-zero components of $b$ are $|8/7|$, $|2/7|$, $|13/7|$, and $|-1/7|$, which are $8/7$, $2/7$, $13/7$, and $1/7$. The two largest are $13/7$ (at index $3$) and $8/7$ (at index $1$).\n    Thus, we set all other components to zero:\n    $$x^{(1)} = \\begin{bmatrix} 8/7 \\\\ 0 \\\\ 13/7 \\\\ 0 \\\\ 0 \\end{bmatrix}$$\n\n6.  **Update the residual**: The new residual $r^{(1)}$ is $r^{(1)} = y - Ax^{(1)}$.\n    $$Ax^{(1)} = \\begin{bmatrix} 1 & 0 & 1 & 0 & 0 \\\\ 0 & 1 & 1 & 1 & 0 \\\\ 1 & 0 & 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 8/7 \\\\ 0 \\\\ 13/7 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 8/7 + 13/7 \\\\ 13/7 \\\\ 8/7 \\end{bmatrix} = \\begin{bmatrix} 21/7 \\\\ 13/7 \\\\ 8/7 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 13/7 \\\\ 8/7 \\end{bmatrix}$$\n    $$r^{(1)} = y - Ax^{(1)} = \\begin{bmatrix} 3 \\\\ 2 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 3 \\\\ 13/7 \\\\ 8/7 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 14/7 - 13/7 \\\\ 7/7 - 8/7 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1/7 \\\\ -1/7 \\end{bmatrix}$$\n\nFinally, we compute the Euclidean norm of the updated residual $r^{(1)}$.\n$$\\|r^{(1)}\\|_{2} = \\sqrt{0^2 + (1/7)^2 + (-1/7)^2} = \\sqrt{\\frac{1}{49} + \\frac{1}{49}} = \\sqrt{\\frac{2}{49}} = \\frac{\\sqrt{2}}{7}$$\nThe Euclidean norm of the residual after one iteration is $\\frac{\\sqrt{2}}{7}$.", "answer": "$$\\boxed{\\frac{\\sqrt{2}}{7}}$$", "id": "3434637"}, {"introduction": "A deep understanding of an algorithm includes knowing not only how it succeeds but also when and why it can fail. This final practice presents a carefully constructed counterexample where CoSaMP fails to recover the true signal, even in a noiseless setting [@problem_id:3436625]. By analyzing a sensing matrix with highly coherent columns, which violates the Restricted Isometry Property (RIP), you will see precisely how the algorithm can be misled, thus reinforcing the critical importance of the theoretical conditions that guarantee successful recovery.", "problem": "Consider the noiseless linear measurement model $y = A x^{\\star}$ with $A \\in \\mathbb{R}^{m \\times n}$ and $x^{\\star} \\in \\mathbb{R}^{n}$, where $x^{\\star}$ is $k$-sparse. The Compressive Sampling Matching Pursuit (CoSaMP) algorithm iteratively identifies a candidate support using correlations with the current residual, solves a least-squares problem on a merged support, and then prunes to $k$ entries before updating the residual. Recovery guarantees for CoSaMP are commonly stated under the Restricted Isometry Property (RIP): a matrix $A$ has the RIP of order $s$ with restricted isometry constant (RIC) $\\delta_{s}$ if, for all $s$-sparse vectors $v$, it holds that $(1 - \\delta_{s}) \\|v\\|_{2}^{2} \\le \\|A v\\|_{2}^{2} \\le (1 + \\delta_{s}) \\|v\\|_{2}^{2}$.\n\nConstruct a sensing matrix $A \\in \\mathbb{R}^{2 \\times 8}$ whose columns are\n$$\na_{1} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix},\\quad\na_{2} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix},\\quad\na_{3} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix},\\quad\na_{4} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix},\\quad\na_{5} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix},\\quad\na_{6} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix},\\quad\na_{7} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix},\\quad\na_{8} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}.\n$$\nLet $k = 2$, and the true signal be $x^{\\star} \\in \\mathbb{R}^{8}$ with entries $x^{\\star}_{1} = 1$, $x^{\\star}_{2} = 1$, and $x^{\\star}_{j} = 0$ for $j \\in \\{3,4,5,6,7,8\\}$. Hence $y = A x^{\\star} = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$.\n\nStarting from the definitions of the Restricted Isometry Property and the CoSaMP iteration (residual update, correlation-based identification of $2k$ indices, least-squares on the merged support, pruning to $k$ by coefficient magnitudes, and residual recomputation), do the following:\n\n- Using only the definitions above and standard linear algebra, determine the set of $2k$ indices CoSaMP identifies in its first iteration when applied to $(A, y)$, and show that after the least-squares and pruning steps, CoSaMP can select a support that excludes the true indices $\\{1, 2\\}$ while achieving a zero residual.\n- Compute the restricted isometry constant $\\delta_{4k}$ of the constructed matrix $A$ by analyzing the extremal deviations of $\\|A v\\|_{2}^{2}$ from $\\|v\\|_{2}^{2}$ over all $4k$-sparse vectors $v$.\n\nExpress the final answer as the exact value of $\\delta_{4k}$ of $A$. No rounding is required. No physical units are involved.", "solution": "The problem is divided into two parts. First, we will analyze the behavior of the CoSaMP algorithm. Second, we will compute the restricted isometry constant $\\delta_{4k}$.\n\n**Part 1: Analysis of the CoSaMP Algorithm's First Iteration**\n\nThe CoSaMP algorithm is initialized with an estimate $x^0 = 0$ and a corresponding support $T^0 = \\emptyset$. The initial residual is $r^0 = y - Ax^0 = y$.\nThe given parameters are:\n- Sparsity level: $k=2$.\n- True signal: $x^{\\star} \\in \\mathbb{R}^{8}$ with $x^{\\star}_{1} = 1$, $x^{\\star}_{2} = 1$, and $x^{\\star}_{j} = 0$ for $j > 2$. The true support is $\\{1, 2\\}$.\n- Sensing matrix $A \\in \\mathbb{R}^{2 \\times 8}$ with columns given by\n$a_{1} = a_5 = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$, $a_{2} = a_6 = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$, and $a_{3} = a_{4} = a_{7} = a_{8} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$.\n- Measurement vector: $y = A x^{\\star} = a_1 x^{\\star}_1 + a_2 x^{\\star}_2 = 1 \\cdot \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} + 1 \\cdot \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$.\n\nThe first iteration ($\\ell=1$) of CoSaMP proceeds as follows:\n\n1.  **Identify Support using Correlations:**\n    We compute the \"proxy\" vector $p = A^T r^0 = A^T y$.\n    $$\n    A^T y =\n    \\begin{pmatrix}\n    a_1^T \\\\ a_2^T \\\\ a_3^T \\\\ a_4^T \\\\ a_5^T \\\\ a_6^T \\\\ a_7^T \\\\ a_8^T\n    \\end{pmatrix}\n    \\begin{pmatrix}1 \\\\ 1\\end{pmatrix} =\n    \\begin{pmatrix}\n    a_1^T y \\\\ a_2^T y \\\\ a_3^T y \\\\ a_4^T y \\\\ a_5^T y \\\\ a_6^T y \\\\ a_7^T y \\\\ a_8^T y\n    \\end{pmatrix}\n    $$\n    The correlations are:\n    - $a_1^T y = a_5^T y = \\begin{pmatrix}1 & 0\\end{pmatrix} \\begin{pmatrix}1 \\\\ 1\\end{pmatrix} = 1$.\n    - $a_2^T y = a_6^T y = \\begin{pmatrix}0 & 1\\end{pmatrix} \\begin{pmatrix}1 \\\\ 1\\end{pmatrix} = 1$.\n    - $a_3^T y = a_4^T y = a_7^T y = a_8^T y = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 & 1\\end{pmatrix} \\begin{pmatrix}1 \\\\ 1\\end{pmatrix} = \\frac{2}{\\sqrt{2}} = \\sqrt{2}$.\n\n    The proxy vector is $p = (1, 1, \\sqrt{2}, \\sqrt{2}, 1, 1, \\sqrt{2}, \\sqrt{2})^T$.\n    CoSaMP identifies the set of indices $\\Omega$ corresponding to the $2k = 4$ largest-magnitude entries of $p$. The four largest magnitudes are all $\\sqrt{2}$, which correspond to the indices $\\{3, 4, 7, 8\\}$. Thus, $\\Omega = \\{3, 4, 7, 8\\}$. Note that the true support indices $\\{1, 2\\}$ are not selected as their correlation values are smaller.\n\n2.  **Merge Supports:**\n    The candidate support is formed by merging the previous support with the newly identified indices: $S^1 = T^0 \\cup \\Omega = \\emptyset \\cup \\{3, 4, 7, 8\\} = \\{3, 4, 7, 8\\}$.\n\n3.  **Least-Squares Estimation:**\n    We find a signal estimate $b$ supported on $S^1$ that best explains the measurement $y$. This is done by solving the least-squares problem:\n    $$\n    b_{S^1} = \\arg\\min_z \\| y - A_{S^1} z \\|_2^2\n    $$\n    where $A_{S^1}$ is the submatrix of $A$ with columns indexed by $S^1$. The columns are $a_3, a_4, a_7, a_8$, which are all identical: $a_i = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$ for $i \\in S^1$.\n    The column space of $A_{S^1}$ is one-dimensional, spanned by the vector $v_0 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$. The vector $y = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix} = \\sqrt{2} v_0$ lies entirely in this column space. Therefore, the least-squares problem has zero residual, and any vector $z \\in \\mathbb{R}^4$ satisfying $A_{S^1} z = y$ is a valid solution.\n    Let $z = (z_1, z_2, z_3, z_4)^T$. The equation $A_{S^1} z = y$ is:\n    $$\n    z_1 a_3 + z_2 a_4 + z_3 a_7 + z_4 a_8 = y\n    $$\n    $$\n    (z_1 + z_2 + z_3 + z_4) \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}\n    $$\n    This simplifies to the single linear equation $z_1 + z_2 + z_3 + z_4 = \\sqrt{2}$.\n    This equation has an infinite number of solutions. The problem states that CoSaMP \"can select\" a support, implying we may choose a particular solution from this set. Let us select a $2$-sparse solution. For example, let $z_1 = \\frac{\\sqrt{2}}{2}$, $z_2 = \\frac{\\sqrt{2}}{2}$, and $z_3=z_4=0$.\n    This corresponds to an intermediate signal estimate $b$ with non-zero entries $b_3 = \\frac{\\sqrt{2}}{2}$ and $b_4 = \\frac{\\sqrt{2}}{2}$.\n\n4.  **Pruning:**\n    The algorithm prunes the intermediate signal $b$ to obtain the new estimate $x^1$. The new support $T^1$ consists of the indices of the $k=2$ largest-magnitude entries of $b$. In our case, the non-zero entries are $b_3$ and $b_4$, both with magnitude $\\frac{\\sqrt{2}}{2}$. Thus, the new support is $T^1 = \\{3, 4\\}$.\n    The new signal estimate $x^1$ is $b$ restricted to this support: $x^1_3 = \\frac{\\sqrt{2}}{2}$, $x^1_4 = \\frac{\\sqrt{2}}{2}$, and all other entries are zero.\n    This new support $T^1=\\{3, 4\\}$ does not contain the true support indices $\\{1, 2\\}$.\n\n5.  **Residual Update:**\n    The final step in the iteration is to compute the new residual $r^1 = y - Ax^1$.\n    $$\n    A x^1 = a_3 x^1_3 + a_4 x^1_4 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix} \\left(\\frac{\\sqrt{2}}{2}\\right) + \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix} \\left(\\frac{\\sqrt{2}}{2}\\right) = \\begin{pmatrix}1/2 \\\\ 1/2\\end{pmatrix} + \\begin{pmatrix}1/2 \\\\ 1/2\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}\n    $$\n    Since $A x^1 = y$, the residual is $r^1 = y - y = 0$.\n    With a zero residual, the algorithm terminates, having recovered an incorrect sparse signal $x^1$ on the support $\\{3, 4\\}$. This demonstrates a failure case for CoSaMP due to the structure of the sensing matrix $A$.\n\n**Part 2: Calculation of the RIC $\\delta_{4k}$**\n\nThe Restricted Isometry Constant (RIC) $\\delta_s$ of a matrix $A$ is the smallest non-negative number such that for all $s$-sparse vectors $v$, the following inequality holds:\n$$ (1 - \\delta_s) \\|v\\|_2^2 \\le \\|Av\\|_2^2 \\le (1 + \\delta_s) \\|v\\|_2^2 $$\nAn equivalent characterization for $\\delta_s$ is:\n$$ \\delta_s = \\max \\left( \\max_{S:|S|=s} \\lambda_{\\max}(A_S^T A_S) - 1, 1 - \\min_{S:|S|=s} \\lambda_{\\min}(A_S^T A_S) \\right) $$\nwhere $A_S$ is the submatrix of $A$ containing columns indexed by the set $S$, and $\\lambda_{\\max}$ and $\\lambda_{\\min}$ denote the maximum and minimum eigenvalues, respectively.\n\nWe need to compute $\\delta_{4k}$ for $k=2$, which is $\\delta_8$. The matrix $A$ has $n=8$ columns. An $8$-sparse vector in $\\mathbb{R}^8$ can be any vector in $\\mathbb{R}^8$. Therefore, we only need to consider the full matrix $A$ itself. The set of submatrices of size $8$ contains only one element, $A$.\nThe formula for $\\delta_8$ simplifies to:\n$$ \\delta_8 = \\max \\left( \\lambda_{\\max}(A^T A) - 1, 1 - \\lambda_{\\min}(A^T A) \\right) $$\nTo find the eigenvalues of the $8 \\times 8$ matrix $A^T A$, it is computationally simpler to find the eigenvalues of the $2 \\times 2$ matrix $A A^T$. The non-zero eigenvalues of $A^T A$ are identical to the eigenvalues of $A A^T$.\n\nLet $c=1/\\sqrt{2}$. The matrix $A$ is:\n$$\nA = \\begin{pmatrix}\n1 & 0 & c & c & 1 & 0 & c & c \\\\\n0 & 1 & c & c & 0 & 1 & c & c\n\\end{pmatrix}\n$$\nWe compute $A A^T$:\n$$\nA A^T = \\begin{pmatrix}\n1 & 0 & c & c & 1 & 0 & c & c \\\\\n0 & 1 & c & c & 0 & 1 & c & c\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 \\\\ 0 & 1 \\\\ c & c \\\\ c & c \\\\ 1 & 0 \\\\ 0 & 1 \\\\ c & c \\\\ c & c\n\\end{pmatrix}\n$$\nThe entry $(A A^T)_{11}$ is the sum of the squares of the elements in the first row of $A$:\n$(A A^T)_{11} = 1^2 + 0^2 + c^2 + c^2 + 1^2 + 0^2 + c^2 + c^2 = 2 + 4c^2 = 2 + 4(1/2) = 4$.\nThe entry $(A A^T)_{22}$ is the sum of the squares of the elements in the second row of $A$:\n$(A A^T)_{22} = 0^2 + 1^2 + c^2 + c^2 + 0^2 + 1^2 + c^2 + c^2 = 2 + 4c^2 = 2 + 4(1/2) = 4$.\nThe off-diagonal entry $(A A^T)_{12} = (A A^T)_{21}$ is the dot product of the two rows of $A$:\n$(A A^T)_{12} = 1(0) + 0(1) + c(c) + c(c) + 1(0) + 0(1) + c(c) + c(c) = 4c^2 = 4(1/2) = 2$.\nSo, the matrix is:\n$$\nA A^T = \\begin{pmatrix} 4 & 2 \\\\ 2 & 4 \\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ are found by solving the characteristic equation $\\det(A A^T - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} 4-\\lambda & 2 \\\\ 2 & 4-\\lambda \\end{pmatrix} = (4-\\lambda)^2 - 4 = 0\n$$\n$$\n(4-\\lambda)^2 = 4 \\implies 4-\\lambda = \\pm 2\n$$\nThe eigenvalues are $\\lambda_1 = 4-2=2$ and $\\lambda_2 = 4+2=6$.\n\nThe matrix $A$ has rank $2$. Thus, the $8 \\times 8$ matrix $A^T A$ also has rank $2$ and possesses two non-zero eigenvalues, which are $2$ and $6$. The remaining $8-2=6$ eigenvalues are $0$.\nThe eigenvalues of $A^T A$ are $\\{6, 2, 0, 0, 0, 0, 0, 0\\}$.\nTherefore, $\\lambda_{\\max}(A^T A) = 6$ and $\\lambda_{\\min}(A^T A) = 0$.\n\nSubstituting these values into the formula for $\\delta_8$:\n$$\n\\delta_8 = \\max(6 - 1, 1 - 0) = \\max(5, 1) = 5\n$$\nThe restricted isometry constant $\\delta_{4k}$ of the constructed matrix $A$ is $5$.", "answer": "$$\n\\boxed{5}\n$$", "id": "3436625"}]}