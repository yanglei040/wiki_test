{"hands_on_practices": [{"introduction": "To build a practical understanding of recovery thresholds, we begin with the fundamental concepts of the Exact Recovery Condition (ERC) and the empirical weak threshold. This exercise will guide you through the process of numerically verifying the ERC for a specific signal support, which provides a deterministic guarantee for recovery via Basis Pursuit. You will then implement a Monte Carlo simulation to estimate the weak recovery threshold, giving you a tangible sense of the sparsity level at which a given sensing matrix typically succeeds [@problem_id:3494376].", "problem": "You are given a fixed linear inverse model with a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$, a support set $S \\subset \\{0,1,\\dots,n-1\\}$, and the classical sparse recovery problem of recovering $x \\in \\mathbb{R}^n$ from noiseless data $y = A x$ by solving the convex program known as Basis Pursuit. Your task is to implement a computational procedure that verifies the Exact Recovery Condition (ERC) numerically for the specified support, and to empirically estimate the weak threshold of recovery for the given matrix instance by sampling random supports and signals. All computations are purely mathematical and no physical units are involved.\n\nStart from the following foundational definitions and facts:\n- For a matrix $A \\in \\mathbb{R}^{m \\times n}$, a $k$-sparse vector $x \\in \\mathbb{R}^n$ satisfies $\\|x\\|_0 = k$ if exactly $k$ coordinates of $x$ are nonzero. For noiseless measurements $y = A x$, the convex program Basis Pursuit (BP) is the optimization problem\n$$\n\\min_{z \\in \\mathbb{R}^n} \\|z\\|_1 \\quad \\text{subject to} \\quad A z = y.\n$$\n- The Exact Recovery Condition (ERC) for a support $S$ of size $|S|=k$ is defined as\n$$\n\\max_{j \\notin S} \\left\\| A_S^\\dagger a_j \\right\\|_1  1,\n$$\nwhere $A_S$ is the submatrix of $A$ formed by the columns indexed by $S$, $A_S^\\dagger$ is the Moore–Penrose pseudoinverse of $A_S$, and $a_j$ denotes the $j$-th column of $A$. When this strict inequality holds, BP recovers all $k$-sparse vectors supported on $S$ and arbitrary coefficients on $S$.\n- The empirical weak threshold for a fixed matrix instance is operationally defined as follows. For each sparsity level $k$ in a prescribed set, draw multiple independent random supports $S$ of size $k$ (uniformly over all supports) and random nonzero coefficients (drawn from a continuous distribution symmetric about zero) on each $S$. For each instance, generate $y = A x$ and attempt recovery by BP. Let the empirical success rate at sparsity $k$ be the fraction of trials recovered exactly. The empirical weak threshold $k_{\\mathrm{weak}}$ is the largest $k$ in the prescribed set for which the empirical success rate is at least $0.5$ (expressed as a decimal).\n\nYour program must implement the following:\n1. Numerically verify the ERC for the provided $(A,S)$ pair by computing the quantity\n$$\n\\theta(S;A) \\triangleq \\max_{j \\notin S} \\left\\| A_S^\\dagger a_j \\right\\|_1,\n$$\nand declaring ERC as satisfied if $\\theta(S;A)  1$ within a strict inequality check robustified by a small numerical tolerance. If $A_S$ is not full column rank, declare ERC as not satisfied.\n2. Empirically estimate the weak threshold $k_{\\mathrm{weak}}$ for the given $A$ by:\n   - Fixing a finite candidate set of sparsity levels $\\mathcal{K}$.\n   - For each $k \\in \\mathcal{K}$, performing $T$ random trials. In each trial, select a support $S$ of size $k$ uniformly at random, draw coefficients on $S$ independently from a standard normal distribution, form $x$, compute $y = A x$, and solve BP. Count a success if both the support of the recovered $\\hat{x}$ matches $S$ (up to a small magnitude threshold on entries) and the relative error $\\|\\hat{x} - x\\|_2 / \\|x\\|_2$ is below a small numerical tolerance. The empirical weak threshold is the largest $k$ in $\\mathcal{K}$ whose empirical success rate is at least $0.5$ (as a decimal).\n3. Output the results for all test cases in the precise format specified at the end.\n\nImplementation details and constraints:\n- Use the linear programming reformulation of BP by writing any $z \\in \\mathbb{R}^n$ as $z = u - v$ with $u,v \\in \\mathbb{R}^n$, $u \\ge 0$, $v \\ge 0$. Then BP is equivalent to\n$$\n\\min_{u,v \\in \\mathbb{R}^n} \\mathbf{1}^\\top (u+v) \\quad \\text{subject to} \\quad A(u - v) = y, \\quad u \\ge 0, \\; v \\ge 0,\n$$\nwhich is a standard Linear Programming (LP) problem.\n- For numerical stability, normalize the columns of $A$ to have unit Euclidean norm prior to all computations. Use a strict inequality with a small numerical tolerance when checking $\\theta(S;A)  1$. Use small tolerances to decide support equality and relative error in the success test.\n\nTest suite:\nYou must implement your solution for the following three matrix instances and associated support sets. For each instance, compute both the ERC verdict for the given $S$ and the empirical weak threshold $k_{\\mathrm{weak}}$ over the specified candidate set with the specified number of trials per $k$.\n- Test case $1$ (random Gaussian, moderate dimensions):\n  - Dimensions: $m = 20$, $n = 40$.\n  - Construction: Draw entries of $A$ i.i.d. standard normal with pseudorandom seed $7$, then normalize columns to unit norm.\n  - Support for ERC: $S = \\{0,7,13\\}$.\n  - Empirical weak threshold settings: candidate set $\\mathcal{K} = \\{1,2,3,4,5,6\\}$, trials per $k$: $T = 12$, with a fixed pseudorandom seed $101$ for reproducibility of trials.\n- Test case $2$ (duplicate column, ill-posed for ERC on a $1$-sparse support):\n  - Dimensions: $m = 10$, $n = 12$.\n  - Construction: Draw a base $A$ with i.i.d. standard normal entries using pseudorandom seed $3$. Then set column index $5$ exactly equal to column index $0$. Finally, normalize all columns to unit norm.\n  - Support for ERC: $S = \\{0\\}$.\n  - Empirical weak threshold settings: candidate set $\\mathcal{K} = \\{1,2,3,4,5\\}$, trials per $k$: $T = 12$, with a fixed pseudorandom seed $202$.\n- Test case $3$ (near-duplicate column with prescribed correlation):\n  - Dimensions: $m = 10$, $n = 12$.\n  - Construction: Draw a base $A$ with i.i.d. standard normal entries using pseudorandom seed $4$. Let $a_0$ denote column index $0$. Construct a unit-norm vector $r$ orthogonal to $a_0$ by projecting a random standard normal vector onto the orthogonal complement of $\\mathrm{span}\\{a_0\\}$ and normalizing. Set column index $5$ to $0.99\\,a_0 + \\sqrt{1 - 0.99^2}\\, r$, and then normalize all columns to unit norm.\n  - Support for ERC: $S = \\{0\\}$.\n  - Empirical weak threshold settings: candidate set $\\mathcal{K} = \\{1,2,3,4,5\\}$, trials per $k$: $T = 12$, with a fixed pseudorandom seed $303$.\n\nSuccess criteria and numerical tolerances:\n- ERC strict inequality check tolerance: declare ERC satisfied if $\\theta(S;A)  1 - 10^{-9}$, and not satisfied otherwise. If $A_S$ is not full column rank, declare not satisfied.\n- Recovery success test for empirical weak threshold: declare success if the support of $\\hat{x}$ matches the true support when thresholding entries with absolute value greater than $10^{-4}$, and the relative error satisfies $\\|\\hat{x} - x\\|_2 / \\|x\\|_2  10^{-3}$.\n- Use the Linear Programming solver to compute BP exactly as above.\n\nFinal output format:\n- For each of the three test cases, produce a pair consisting of the ERC verdict and the empirical weak threshold, in that order: $[\\text{erc\\_bool}, k_{\\mathrm{weak}}]$ where $\\text{erc\\_bool}$ is a boolean and $k_{\\mathrm{weak}}$ is a nonnegative integer.\n- Your program should produce a single line of output containing the three pairs in a single comma-separated list with no spaces, for example: $[[\\text{True},3],[\\text{False},2],[\\text{True},4]]$.\n\nYour program must be self-contained, must not read any input, and must use only the specified runtime environment. Ensure deterministic behavior by using the specified pseudorandom seeds exactly as described. Angles and physical units do not appear in this problem. Percentages must be expressed as decimals; the threshold criterion uses $0.5$ as the success-rate cutoff.", "solution": "The problem proposed is a well-defined computational exercise within the mathematical framework of compressed sensing and sparse signal recovery. It is scientifically sound, self-contained, and all parameters and procedures are specified unambiguously. The problem is therefore deemed valid. We shall proceed with a complete solution.\n\nThe task is twofold: first, to verify the Exact Recovery Condition (ERC) for a given matrix $A \\in \\mathbb{R}^{m \\times n}$ and a specific support set $S$; second, to empirically estimate the weak threshold of recovery $k_{\\mathrm{weak}}$ for the same matrix $A$. We will address each part methodologically.\n\n**Part 1: Verification of the Exact Recovery Condition (ERC)**\n\nThe Exact Recovery Condition provides a sufficient (and often necessary) condition for the unique recovery of any sparse signal $x$ with support $S$ from its noiseless measurements $y=Ax$ via the Basis Pursuit (BP) convex program:\n$$\n\\min_{z \\in \\mathbb{R}^n} \\|z\\|_1 \\quad \\text{subject to} \\quad A z = y.\n$$\nThe support of $x$ is the set of indices of its non-zero entries, $S = \\{i : x_i \\neq 0\\}$. Let $|S| = k$.\n\nThe ERC for a support set $S$ is satisfied if and only if\n$$\n\\theta(S;A) \\triangleq \\max_{j \\notin S} \\left\\| A_S^\\dagger a_j \\right\\|_1  1,\n$$\nwhere $A_S$ is the submatrix of $A$ with columns indexed by $S$, $a_j$ is the $j$-th column of $A$, and $A_S^\\dagger$ is the Moore-Penrose pseudoinverse of $A_S$. This condition is well-defined only if $A_S$ has full column rank, i.e., $\\mathrm{rank}(A_S) = k$. If $A_S$ is rank-deficient, its columns are linearly dependent, and it's impossible to uniquely determine the coefficients of a signal supported on $S$ from $A_Sx_S$. In this case, the ERC is considered not satisfied.\n\nThe computational procedure to verify the ERC is as follows:\n1.  **Matrix Normalization**: As specified, we first normalize each column $a_j$ of the matrix $A$ to have unit Euclidean norm: $a_j \\leftarrow a_j / \\|a_j\\|_2$ for all $j \\in \\{0, \\dots, n-1\\}$. All subsequent computations use this normalized matrix.\n2.  **Submatrix Extraction**: Given the support set $S$, we construct the submatrix $A_S \\in \\mathbb{R}^{m \\times k}$. The indices not in $S$ form the complement set $S^c = \\{0, \\dots, n-1\\} \\setminus S$.\n3.  **Rank Check**: We verify if $A_S$ has full column rank, i.e., if $\\mathrm{rank}(A_S) = k$. This can be done by checking if the number of singular values of $A_S$ greater than a small tolerance is equal to $k$. If $\\mathrm{rank}(A_S)  k$, the ERC is declared unsatisfied.\n4.  **Pseudoinverse Calculation**: If $A_S$ has full column rank, we compute its pseudoinverse $A_S^\\dagger$. For a full-rank matrix $A_S$, this is given by $A_S^\\dagger = (A_S^\\top A_S)^{-1} A_S^\\top$. Numerically, a stable method like Singular Value Decomposition (SVD) is used, as implemented in standard numerical libraries.\n5.  **ERC Quantity Calculation**: For each column $a_j$ with index $j \\in S^c$, we compute the vector $v_j = A_S^\\dagger a_j$. We then calculate its $\\ell_1$-norm, $\\|v_j\\|_1 = \\sum_i |(v_j)_i|$.\n6.  **Maximum and Comparison**: We find the maximum of these norms, $\\theta(S;A) = \\max_{j \\in S^c} \\|v_j\\|_1$.\n7.  **Verdict**: The ERC is satisfied if this maximum value is strictly less than 1. To account for finite-precision arithmetic, we check against a tolerance: $\\theta(S;A)  1 - \\epsilon$, where the problem specifies $\\epsilon = 10^{-9}$.\n\n**Part 2: Empirical Estimation of the Weak Threshold of Recovery**\n\nThe weak threshold of recovery, $k_{\\mathrm{weak}}$, for a given matrix $A$, is an empirical measure of the sparsity level up to which recovery is \"typically\" successful. For a fixed matrix instance, it is the largest sparsity $k$ for which a randomly chosen $k$-sparse signal is recovered with high probability. The problem specifies an operational definition, which we formalize into the following algorithm.\n\n1.  **Matrix Normalization**: As before, the columns of $A$ are normalized to unit $\\ell_2$-norm. A separate, reproducible stream of random numbers is initialized for the trials.\n2.  **Iterate Over Sparsity Levels**: We iterate through each candidate sparsity level $k \\in \\mathcal{K}$.\n3.  **Monte Carlo Simulation**: For each $k$, we perform $T$ independent trials:\n    a. **Generate a random sparse signal**:\n        i.  A support set $S$ of size $k$ is chosen uniformly at random from all $\\binom{n}{k}$ possible supports of size $k$.\n        ii. A signal $x \\in \\mathbb{R}^n$ is constructed. It is zero everywhere except on $S$. The $k$ non-zero entries, $x_S$, are drawn independently from a standard normal distribution, $\\mathcal{N}(0,1)$.\n    b. **Generate measurements**: The noiseless measurement vector is computed as $y = Ax$.\n    c. **Attempt recovery via Basis Pursuit**: We solve the BP optimization problem to find an estimate $\\hat{x}$. As specified, we use the equivalent Linear Program (LP):\n        $$\n        \\min_{u,v} \\sum_{i=0}^{n-1} (u_i+v_i) \\quad \\text{subject to} \\quad A(u-v) = y, \\quad u \\ge 0, \\; v \\ge 0.\n        $$\n        A standard LP solver is used. The solution vector $[u^\\star; v^\\star]$ gives the recovered signal $\\hat{x} = u^\\star - v^\\star$.\n    d. **Check for success**: A trial is deemed successful if both of the following criteria are met:\n        i.  **Support recovery**: The support of the estimate, $\\hat{S} = \\{i : |\\hat{x}_i|  10^{-4}\\}$, must be identical to the true support $S$.\n        ii. **Signal fidelity**: The relative reconstruction error must be small: $\\|\\hat{x} - x\\|_2 / \\|x\\|_2  10^{-3}$.\n4.  **Calculate Success Rate**: For each $k$, the empirical success rate is the fraction of successful trials: $P_k = (\\text{number of successes}) / T$.\n5.  **Determine Weak Threshold**: After computing the success rates for all $k \\in \\mathcal{K}$, the weak threshold $k_{\\mathrm{weak}}$ is the largest value of $k \\in \\mathcal{K}$ for which $P_k \\ge 0.5$. If no $k$ in $\\mathcal{K}$ meets this criterion, $k_{\\mathrm{weak}}$ is taken to be $0$.\n\nThese two procedures will be applied to the three test cases specified. Test case 2 presents a matrix with two identical columns, which guarantees failure of the ERC for any support containing one of the columns but not the other, as $\\|A_S^\\dagger a_j\\|_1$ will be exactly $1$. Test case 3 involves highly, but not perfectly, correlated columns, providing a test near the boundary of the ERC. The implementation will faithfully adhere to the specified random seeds for complete reproducibility.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1\n        {\n            \"m\": 20, \"n\": 40, \"A_seed\": 7, \"erc_support\": {0, 7, 13},\n            \"k_set\": [1, 2, 3, 4, 5, 6], \"trials_per_k\": 12, \"trial_seed\": 101,\n            \"case_id\": 1\n        },\n        # Test case 2\n        {\n            \"m\": 10, \"n\": 12, \"A_seed\": 3, \"erc_support\": {0},\n            \"k_set\": [1, 2, 3, 4, 5], \"trials_per_k\": 12, \"trial_seed\": 202,\n            \"case_id\": 2\n        },\n        # Test case 3\n        {\n            \"m\": 10, \"n\": 12, \"A_seed\": 4, \"erc_support\": {0},\n            \"k_set\": [1, 2, 3, 4, 5], \"trials_per_k\": 12, \"trial_seed\": 303,\n            \"case_id\": 3\n        },\n    ]\n\n    results = []\n    for params in test_cases:\n        A = generate_matrix(params)\n        erc_verdict = compute_erc_verdict(A, params[\"erc_support\"])\n        k_weak = estimate_weak_threshold(A, params[\"k_set\"], params[\"trials_per_k\"], params[\"trial_seed\"])\n        results.append([erc_verdict, k_weak])\n    \n    # Format output to be exactly [[Bool,int],[Bool,int],...] with no spaces\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\ndef generate_matrix(params):\n    \"\"\"\n    Generates the sensing matrix A based on test case parameters.\n    \"\"\"\n    m, n, seed = params[\"m\"], params[\"n\"], params[\"A_seed\"]\n    case_id = params[\"case_id\"]\n    rng = np.random.default_rng(seed)\n    \n    if case_id == 1:\n        A = rng.standard_normal((m, n))\n    elif case_id == 2:\n        A = rng.standard_normal((m, n))\n        A[:, 5] = A[:, 0]\n    elif case_id == 3:\n        A_base = rng.standard_normal((m, n))\n        a0_unnorm = A_base[:, 0].copy()\n        \n        # Construct a random vector orthogonal to a0_unnorm\n        rand_vec = rng.standard_normal(m)\n        proj_a0_rand = (rand_vec @ a0_unnorm) / (a0_unnorm @ a0_unnorm) * a0_unnorm\n        r_unorth = rand_vec - proj_a0_rand\n        r = r_unorth / np.linalg.norm(r_unorth)\n        \n        # Construct the new highly correlated column\n        a5_new = 0.99 * a0_unnorm + np.sqrt(1 - 0.99**2) * r\n        A = A_base.copy()\n        A[:, 5] = a5_new\n\n    # Normalize all columns to unit Euclidean norm\n    norms = np.linalg.norm(A, axis=0)\n    # Avoid division by zero for zero columns, although not expected here\n    norms[norms == 0] = 1\n    A = A / norms\n\n    return A\n\ndef compute_erc_verdict(A, S_set):\n    \"\"\"\n    Numerically verifies the Exact Recovery Condition (ERC).\n    \"\"\"\n    m, n = A.shape\n    S = sorted(list(S_set))\n    k = len(S)\n\n    if k == 0:\n        # ERC is trivially true for k=0 but let's handle it\n        return True\n\n    A_S = A[:, S]\n    \n    # Check if A_S is full column rank\n    if np.linalg.matrix_rank(A_S)  k:\n        return False\n    \n    # Compute pseudoinverse of A_S\n    A_S_dagger = np.linalg.pinv(A_S)\n    \n    # Get indices not in S\n    S_c = sorted(list(set(range(n)) - S_set))\n    \n    max_norm = 0.0\n    for j in S_c:\n        a_j = A[:, j]\n        v_j = A_S_dagger @ a_j\n        norm_v_j = np.linalg.norm(v_j, 1)\n        if norm_v_j  max_norm:\n            max_norm = norm_v_j\n            \n    erc_tolerance = 1e-9\n    return max_norm  1.0 - erc_tolerance\n\ndef estimate_weak_threshold(A, k_set, T, seed):\n    \"\"\"\n    Empirically estimates the weak threshold of recovery.\n    \"\"\"\n    m, n = A.shape\n    rng = np.random.default_rng(seed)\n    \n    k_weak = 0\n    \n    for k in k_set:\n        if k == 0: continue\n        success_count = 0\n        for _ in range(T):\n            # 1. Generate random sparse signal\n            S_indices = rng.choice(n, k, replace=False)\n            S = set(S_indices)\n            x = np.zeros(n)\n            x[S_indices] = rng.standard_normal(k)\n            \n            # 2. Generate measurements\n            y = A @ x\n            \n            # 3. Solve Basis Pursuit via LP\n            c = np.ones(2 * n)\n            A_eq = np.hstack([A, -A])\n            b_eq = y\n            \n            # Scipy's linprog with 'highs' is robust\n            res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=(0, None), method='highs-ds')\n            \n            if not res.success:\n                continue\n                \n            x_hat = res.x[:n] - res.x[n:]\n\n            # 4. Check for success\n            support_tol = 1e-4\n            error_tol = 1e-3\n            \n            S_hat = set(np.where(np.abs(x_hat)  support_tol)[0])\n            \n            support_recovered = (S == S_hat)\n            \n            # Avoid division by zero if x is zero vector\n            norm_x = np.linalg.norm(x)\n            if norm_x == 0:\n                relative_error = np.linalg.norm(x_hat) # Should also be near zero\n            else:\n                relative_error = np.linalg.norm(x_hat - x) / norm_x\n            \n            error_small = (relative_error  error_tol)\n            \n            if support_recovered and error_small:\n                success_count += 1\n\n        success_rate = success_count / T\n        \n        if success_rate = 0.5:\n            k_weak = k\n            \n    return k_weak\n\nsolve()\n```", "id": "3494376"}, {"introduction": "While empirical thresholds offer a view into typical performance, strong thresholds provide worst-case guarantees. This practice explores the limitations of one of the simplest strong guarantees, which is based on the matrix's mutual coherence $\\mu$. You will analyze two carefully constructed matrices that share the same coherence but differ in a more subtle structural property known as the spark, revealing how coherence-based bounds can be overly pessimistic and fail to capture the true recovery capabilities of a matrix [@problem_id:3494441].", "problem": "Consider a deterministic, noiseless compressed sensing model with a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ and a signal $x \\in \\mathbb{R}^{n}$ measured as $y = A x$. Let the columns of $A$ be denoted by $\\{a_i\\}_{i=1}^n$, and assume they are normalized to unit $\\ell_2$-norm. Two structural invariants are central for recovery analysis: the mutual coherence $\\mu$ and the spark of $A$. The mutual coherence is defined as $\\mu = \\max_{i \\neq j} \\left| \\langle a_i, a_j \\rangle \\right|$, and the spark is defined as $\\operatorname{spark}(A) = \\min \\left\\{ s : \\text{there exists a set of } s \\text{ columns of } A \\text{ that are linearly dependent} \\right\\}$. A classical sufficient condition for exact recovery of every $k$-sparse signal by $\\ell_1$-minimization, sometimes called the strong threshold, depends only on $\\mu$. In contrast, weak empirical thresholds quantify typical recoverability for a given algorithm and a finite set of instances, and can diverge from coherence-based guarantees when the spark varies, even if $\\mu$ is held fixed.\n\nYour task is to examine the interplay between $\\mu$ and $\\operatorname{spark}(A)$ by constructing two matrices with identical mutual coherence but different spark, and then quantifying the resulting divergence between coherence-based strong thresholds and weak empirical ones assessed via Orthogonal Matching Pursuit (OMP). Use only purely mathematical and algorithmic reasoning; no physical units are involved.\n\nDefinitions and requirements to be implemented and used by your program:\n1. The mutual coherence $\\mu$ must be computed from the normalized columns of $A$ using the definition $\\mu = \\max_{i \\neq j} \\left| \\langle a_i, a_j \\rangle \\right|$.\n2. The spark must be computed exactly for the provided small matrices by exhaustive search over all column subsets, using linear algebra rank tests to detect dependence.\n3. The coherence-based strong threshold $k_{\\mu,\\text{strong}}$ is the largest integer $k$ guaranteed by the classical mutual coherence sufficient condition for exact recovery of every $k$-sparse signal by $\\ell_1$-minimization (Basis Pursuit).\n4. The weak empirical threshold $k_{\\text{empirical}}$ must be computed by running Orthogonal Matching Pursuit (OMP) on a finite test suite of sparse signals with specified supports and coefficients, and selecting the largest $k$ for which OMP exactly reconstructs all signals in the corresponding $k$-suite (within a numerical tolerance).\n5. Orthogonal Matching Pursuit (OMP) is defined as the greedy algorithm that, given $A$, $y$, and an iteration cap $k_{\\max}$: iteratively selects the column with maximal absolute inner product with the current residual, refits by least squares on the selected support, and updates the residual; it stops early if the residual $\\ell_2$-norm is below a small tolerance.\n\nTest suite and matrices:\n- Matrix $A^{(1)} \\in \\mathbb{R}^{3 \\times 4}$ has columns\n  $c_1 = [\\,1,\\,0,\\,0\\,]^\\top$,\n  $c_2 = [\\,-0.5,\\,\\sqrt{3}/2,\\,0\\,]^\\top$,\n  $c_3 = [\\,0.5,\\,0.2886751345948129,\\,0.816496580927726\\,]^\\top$,\n  $c_4 = [\\,-0.5,\\,-0.2886751345948129,\\,0.816496580927726\\,]^\\top$.\n  The matrix $A^{(1)}$ is formed by stacking these columns.\n- Matrix $A^{(2)} \\in \\mathbb{R}^{3 \\times 4}$ has columns\n  $c_1 = [\\,1,\\,0,\\,0\\,]^\\top$,\n  $c_2 = [\\,-0.5,\\,\\sqrt{3}/2,\\,0\\,]^\\top$,\n  $c_3 = [\\,0.5,\\,\\sqrt{3}/2,\\,0\\,]^\\top$,\n  $c_4 = [\\,-0.5,\\,-0.2886751345948129,\\,0.816496580927726\\,]^\\top$.\n  Note that $c_3 = c_1 + c_2$ and has unit norm because $\\langle c_1, c_2 \\rangle = -0.5$, ensuring a dependent triple in $A^{(2)}$.\n\nFor both matrices, the weak empirical threshold must be evaluated using the following deterministic signal suites:\n- For $k = 1$: supports $\\{\\,\\{1\\},\\,\\{2\\},\\,\\{3\\},\\,\\{4\\}\\,\\}$ with coefficient $1$ on the support and $0$ elsewhere.\n- For $k = 2$: supports $\\{\\,\\{1,2\\},\\,\\{1,3\\}\\,\\}$ with coefficients $1$ on each index in the support and $0$ elsewhere.\n\nThe program must:\n- Normalize columns to unit $\\ell_2$-norm internally before computing coherence, spark, and performing OMP. The provided columns are already unit-norm, but normalization should be applied to ensure robustness.\n- Compute $\\mu$, $\\operatorname{spark}(A)$, $k_{\\mu,\\text{strong}}$, and $k_{\\text{empirical}}$ for each matrix.\n- Use a numerical tolerance of $10^{-8}$ for residual stopping in OMP and for equality checks when comparing the reconstructed $x$ to the ground truth $x$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry corresponds to one matrix and is itself a list of the form $[\\mu, \\operatorname{spark}(A), k_{\\mu,\\text{strong}}, k_{\\text{empirical}}]$. For example, the output must look like $[[\\text{for } A^{(1)}],[\\text{for } A^{(2)}]]$ with numbers in place of the placeholders.\n\nThe test suite covers:\n- A general case where identical mutual coherence coincides with a larger spark and yields higher weak empirical recoverability.\n- A boundary case where identical mutual coherence coincides with a smaller spark due to an explicit dependent triple, producing a lower weak empirical threshold.\n- Deterministic $k$-sparse signal families for $k = 1$ and $k = 2$ that probe both happy-path and failure scenarios.\n\nYour program must be a complete, runnable implementation in a modern language and produce exactly the specified final output format.", "solution": "The problem is subjected to a rigorous validation process before a solution is attempted.\n\n### Step 1: Extract Givens\n\n-   **Model**: Deterministic, noiseless compressed sensing, $y = A x$, with $A \\in \\mathbb{R}^{m \\times n}$ and $x \\in \\mathbb{R}^{n}$.\n-   **Matrix Columns**: $\\{a_i\\}_{i=1}^n$ are the columns of $A$, normalized to unit $\\ell_2$-norm.\n-   **Mutual Coherence**: $\\mu = \\max_{i \\neq j} \\left| \\langle a_i, a_j \\rangle \\right|$.\n-   **Spark**: $\\operatorname{spark}(A) = \\min \\left\\{ s : \\text{there exists a set of } s \\text{ columns of } A \\text{ that are linearly dependent} \\right\\}$.\n-   **Strong Threshold ($k_{\\mu,\\text{strong}}$)**: The largest integer $k$ guaranteed by the classical mutual coherence sufficient condition, $k  \\frac{1}{2}\\left(1 + \\frac{1}{\\mu}\\right)$, for exact recovery of every $k$-sparse signal by $\\ell_1$-minimization.\n-   **Weak Empirical Threshold ($k_{\\text{empirical}}$)**: The largest integer $k$ for which Orthogonal Matching Pursuit (OMP) exactly reconstructs all signals in a specified finite $k$-suite.\n-   **Orthogonal Matching Pursuit (OMP)**: A greedy algorithm that, given $A$, $y$, and an iteration cap $k_{\\max}$, iteratively selects the column with the maximal absolute inner product with the current residual, refits by least squares on the selected support, and updates the residual. It stops if the residual $\\ell_2$-norm is below a tolerance.\n-   **Matrix $A^{(1)}$**: $A^{(1)} \\in \\mathbb{R}^{3 \\times 4}$ has columns:\n    $c_1 = [\\,1,\\,0,\\,0\\,]^\\top$\n    $c_2 = [\\,-0.5,\\,\\sqrt{3}/2,\\,0\\,]^\\top$\n    $c_3 = [\\,0.5,\\,0.2886751345948129,\\,0.816496580927726\\,]^\\top$\n    $c_4 = [\\,-0.5,\\,-0.2886751345948129,\\,0.816496580927726\\,]^\\top$\n-   **Matrix $A^{(2)}$**: $A^{(2)} \\in \\mathbb{R}^{3 \\times 4}$ has columns:\n    $c_1 = [\\,1,\\,0,\\,0\\,]^\\top$\n    $c_2 = [\\,-0.5,\\,\\sqrt{3}/2,\\,0\\,]^\\top$\n    $c_3 = [\\,0.5,\\,\\sqrt{3}/2,\\,0\\,]^\\top$\n    $c_4 = [\\,-0.5,\\,-0.2886751345948129,\\,0.816496580927726\\,]^\\top$\n    with the property that the third column is the sum of the first two.\n-   **Signal Test Suites**:\n    -   $k=1$: supports $\\{\\,\\{1\\},\\,\\{2\\},\\,\\{3\\},\\,\\{4\\}\\,\\}$, coefficient $1$.\n    -   $k=2$: supports $\\{\\,\\{1,2\\},\\,\\{1,3\\}\\,\\}$, coefficients $1$ on each support index.\n-   **Numerical Tolerance**: $10^{-8}$ for OMP residual stopping and for equality checks on the final reconstruction.\n-   **Computational Requirements**:\n    1.  Normalize columns internally.\n    2.  Compute $\\mu$, $\\operatorname{spark}(A)$, $k_{\\mu,\\text{strong}}$, and $k_{\\text{empirical}}$ for both matrices.\n    3.  Compute spark by exhaustive search over column subsets and rank tests.\n-   **Final Output Format**: A single line: `[[mu_1, spark_1, k_strong_1, k_empirical_1],[mu_2, spark_2, k_strong_2, k_empirical_2]]`.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem is located within the established mathematical framework of compressed sensing and sparse recovery. All definitions—mutual coherence, spark, OMP, and recovery conditions—are standard in this field. The matrices are constructed based on valid geometric and algebraic principles. The problem is scientifically sound.\n-   **Well-Posed**: All quantities to be computed ($\\mu$, spark, $k_{\\mu,\\text{strong}}$, $k_{\\text{empirical}}$) are unambiguously defined. The inputs (matrices, test signals) are explicit and sufficient for computation. The exhaustive search for spark is feasible for the given $3 \\times 4$ matrices. The problem admits a unique, computable solution.\n-   **Objective**: The problem statement is expressed in precise, mathematical language, free from subjective or biased terminology.\n-   **Flaw Checklist**:\n    1.  **Scientific Unsoundness**: None. The mathematical premises are correct.\n    2.  **Non-Formalizable/Irrelevant**: None. The problem is a formal and relevant exploration of thresholds in sparse recovery.\n    3.  **Incomplete/Contradictory**: None. All necessary information is provided. The stated linear dependence in $A^{(2)}$ is a feature, not a contradiction.\n    4.  **Unrealistic/Infeasible**: None. The computations are feasible. The problem is purely mathematical.\n    5.  **Ill-Posed**: None. The solution is unique and stable.\n    6.  **Pseudo-Profound/Trivial**: None. The problem thoughtfully constructs a scenario to highlight a non-trivial distinction between coherence-based guarantees and empirical performance, which is a core concept in the field.\n    7.  **Unverifiable**: None. All results are algorithmically computable and verifiable.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. A complete solution will be provided.\n\n### Solution\n\nThe objective is to compute and compare four key metrics—mutual coherence ($\\mu$), spark, the coherence-based strong recovery threshold ($k_{\\mu,\\text{strong}}$), and the OMP-based weak empirical recovery threshold ($k_{\\text{empirical}}$)—for two specified matrices, $A^{(1)}$ and $A^{(2)}$. This analysis will demonstrate how matrices with identical coherence can exhibit different recovery performance due to differing spark values.\n\nFirst, we establish the computational procedures for the four metrics.\n\n1.  **Mutual Coherence, $\\mu$**: The columns of the input matrix $A$ are first normalized to have unit $\\ell_2$-norm. Let the normalized matrix be $\\tilde{A}$. The Gram matrix $G = \\tilde{A}^T \\tilde{A}$ is computed. The mutual coherence $\\mu$ is the maximum absolute value of the off-diagonal entries of $G$: $\\mu = \\max_{i \\neq j} |G_{ij}|$.\n\n2.  **Spark, $\\operatorname{spark}(A)$**: For an $m \\times n$ matrix $A$, we search for the smallest integer $s$ such that some subset of $s$ columns of $A$ is linearly dependent. This is done by iterating $s$ from $1$ to $m+1$. For each $s$, we form all $\\binom{n}{s}$ submatrices of $A$ with $s$ columns. If any such submatrix has a rank less than $s$, then $\\operatorname{spark}(A) = s$ and the search terminates. Given $m=3, n=4$, the spark can be at most $m+1 = 4$.\n\n3.  **Strong Threshold, $k_{\\mu,\\text{strong}}$**: This is a theoretical guarantee for $\\ell_1$-minimization. The sufficient condition for unique recovery of any $k$-sparse signal is $k  \\frac{1}{2}\\left(1 + \\frac{1}{\\mu}\\right)$. Therefore, $k_{\\mu,\\text{strong}}$ is the largest integer $k$ that satisfies this strict inequality.\n\n4.  **Weak Empirical Threshold, $k_{\\text{empirical}}$**: This is determined by testing the Orthogonal Matching Pursuit (OMP) algorithm on a specific, finite set of signals. For a given sparsity level $k$, a suite of $k$-sparse signals $\\{x_i\\}$ is generated. For each $x_i$, we compute $y_i = Ax_i$ and run OMP to obtain a reconstruction $\\hat{x}_i$. If $\\hat{x}_i$ equals $x_i$ for all signals in the $k$-suite (within a numerical tolerance), recovery is successful for level $k$. $k_{\\text{empirical}}$ is the largest $k$ for which recovery is successful for the corresponding suite.\n\nThe OMP algorithm itself proceeds as follows for a signal $y$ and sparsity level $S$:\n- Initialize residual $r_0 = y$, support set $\\Lambda_0 = \\emptyset$.\n- For $t = 1, \\dots, S$:\n    - Find the index $j_t$ of the column of $A$ most correlated with the current residual: $j_t = \\arg\\max_j |\\langle a_j, r_{t-1} \\rangle|$.\n    - Augment the support: $\\Lambda_t = \\Lambda_{t-1} \\cup \\{j_t\\}$.\n    - Solve a least-squares problem to find the new signal estimate on the support: $x_t = \\arg\\min_z \\|y - A_{\\Lambda_t}z\\|_2$, where $A_{\\Lambda_t}$ is the submatrix of $A$ with columns from $\\Lambda_t$.\n    - Update the residual: $r_t = y - A_{\\Lambda_t}x_t$.\n- The final estimate has non-zero entries from $x_S$ at indices in $\\Lambda_S$.\n\nWe now apply this framework to $A^{(1)}$ and $A^{(2)}$. The provided column vectors are already normalized, but we apply normalization as a procedural step.\n\n**Analysis of Matrix $A^{(1)}$**\n\n-   **Columns**: The four columns $a_1, a_2, a_3, a_4$ are vectors in $\\mathbb{R}^3$.\n-   **Mutual Coherence $\\mu^{(1)}$**: The pairwise inner products are computed. The maximum absolute off-diagonal value is $|\\langle a_1, a_2 \\rangle| = |\\langle a_1, a_3 \\rangle| = |\\langle a_1, a_4 \\rangle| = 0.5$. Thus, $\\mu^{(1)} = 0.5$.\n-   **Strong Threshold $k_{\\mu,\\text{strong}}^{(1)}$**: With $\\mu = 0.5$, we have the condition $k  \\frac{1}{2}\\left(1 + \\frac{1}{0.5}\\right) = \\frac{1}{2}(1 + 2) = 1.5$. The largest integer $k$ satisfying this is $k=1$. So, $k_{\\mu,\\text{strong}}^{(1)} = 1$.\n-   **Spark $\\operatorname{spark}(A^{(1)})$**: We check for linear dependence in subsets of columns.\n    -   Subsets of size $s=1, 2$: All columns are non-zero and no two are collinear, so they are independent.\n    -   Subsets of size $s=3$: We test the rank of all $\\binom{4}{3}=4$ submatrices of size $3 \\times 3$. All four submatrices can be shown to have rank $3$ (i.e., they are invertible). For instance, the submatrix $[a_1, a_2, a_3]$ is upper triangular with a non-zero diagonal, hence it is full rank. All other combinations are also found to be full rank.\n    -   Since no subset of size $3$ is linearly dependent, the spark must be greater than $3$. As the columns are vectors in $\\mathbb{R}^3$, any set of $4$ columns must be linearly dependent. Thus, $\\operatorname{spark}(A^{(1)}) = 4$.\n-   **Empirical Threshold $k_{\\text{empirical}}^{(1)}$**:\n    -   $k=1$ suite: OMP perfectly recovers all $1$-sparse signals (which are just the columns of $A^{(1)}$ scaled by $1$), as it selects the correct column in the first step.\n    -   $k=2$ suite: The test signals are $x_a = [1,1,0,0]^T$ and $x_b = [1,0,1,0]^T$. For both signals, OMP correctly identifies the true support set $\\{1,2\\}$ and $\\{1,3\\}$, respectively, and reconstructs the signals exactly. For example, for $y_a = a_1 + a_2$, OMP correctly finds the support $\\{1,2\\}$ in two iterations.\n    -   Since OMP succeeds for $k=1$ and $k=2$, $k_{\\text{empirical}}^{(1)} = 2$.\n\n**Analysis of Matrix $A^{(2)}$**\n\n-   **Columns**: Let the columns be $b_1, b_2, b_3, b_4$. They are identical to those of $A^{(1)}$ except for the third column, $b_3 = [0.5, \\sqrt{3}/2, 0]^T$.\n-   **Mutual Coherence $\\mu^{(2)}$**: The pairwise inner products are computed. The maximum absolute off-diagonal value is again found to be $0.5$ (e.g., $|\\langle b_1, b_2 \\rangle| = 0.5$, $|\\langle b_2, b_3 \\rangle| = 0.5$, $|\\langle b_3, b_4 \\rangle| = 0.5$). Thus, $\\mu^{(2)} = 0.5$.\n-   **Strong Threshold $k_{\\mu,\\text{strong}}^{(2)}$**: Since $\\mu^{(2)} = \\mu^{(1)} = 0.5$, the strong threshold is identical: $k_{\\mu,\\text{strong}}^{(2)} = 1$.\n-   **Spark $\\operatorname{spark}(A^{(2)})$**:\n    -   The problem is constructed such that $b_3 = b_1 + b_2$, which implies a linear dependency $b_1 + b_2 - b_3 = 0$. This is a set of $3$ linearly dependent columns.\n    -   No two columns are collinear, so the minimum size of a dependent set is not $2$.\n    -   Therefore, $\\operatorname{spark}(A^{(2)}) = 3$.\n-   **Empirical Threshold $k_{\\text{empirical}}^{(2)}$**:\n    -   $k=1$ suite: Recovery is successful, as for $A^{(1)}$.\n    -   $k=2$ suite: Consider the signal $x = [1,1,0,0]^T$. The measurement is $y = 1 \\cdot b_1 + 1 \\cdot b_2 = b_1 + b_2$. Due to the matrix construction, $y=b_3$. When OMP is run with this $y$:\n        -   It computes correlations $|\\langle b_i, y \\rangle| = |\\langle b_i, b_3 \\rangle|$. Since $\\langle b_3, b_3 \\rangle = 1$ and all other $|\\langle b_i, b_3 \\rangle| \\le \\mu = 0.5$, OMP will unambiguously select column $3$ first.\n        -   The algorithm places index $3$ in the support. The least squares solution on this support is $x_3=1$. The residual becomes $y - 1 \\cdot b_3 = b_3 - b_3 = 0$.\n        -   Since the residual is zero, OMP terminates and returns the $1$-sparse solution $\\hat{x} = [0,0,1,0]^T$.\n        -   This does not match the true $2$-sparse signal $x = [1,1,0,0]^T$.\n    -   Since OMP fails for a signal in the $k=2$ suite, the empirical threshold for $k=2$ is not met. The largest $k$ for which all tests passed is $1$. Thus, $k_{\\text{empirical}}^{(2)} = 1$.\n\n**Conclusion**\n\n| Matrix | $\\mu$ | $\\operatorname{spark}(A)$ | $k_{\\mu,\\text{strong}}$ | $k_{\\text{empirical}}$ |\n| :---: | :---: | :---: | :---: | :---: |\n| $A^{(1)}$ | $0.5$ | $4$ | $1$ | $2$ |\n| $A^{(2)}$ | $0.5$ | $3$ | $1$ | $1$ |\n\nThe results confirm the central thesis. Both matrices share the same mutual coherence $\\mu=0.5$, leading to an identical and pessimistic strong threshold of $k_{\\mu,\\text{strong}}=1$. However, their structural properties, as captured by the spark, are different. $A^{(1)}$ has the maximal possible spark for its dimensions ($4$), while $A^{(2)}$ has a lower spark ($3$) due to an explicit linear dependency. This difference directly impacts the empirical performance of OMP. $A^{(1)}$ allows for the recovery of certain $2$-sparse signals, yielding $k_{\\text{empirical}}^{(1)}=2$, whereas the specific structure of $A^{(2)}$ causes OMP to fail on a $2$-sparse signal, restricting its empirical threshold to $k_{\\text{empirical}}^{(2)}=1$. This illustrates that while coherence provides a universal, worst-case guarantee, spark offers a more refined (though computationally harder) predictor of a matrix's performance for sparse recovery.", "answer": "```python\nimport numpy as np\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Main function to solve the compressed sensing problem.\n    It computes and compares metrics for two matrices A1 and A2.\n    \"\"\"\n\n    # --- Helper Functions ---\n\n    def normalize_columns(A):\n        \"\"\"Normalizes the columns of a matrix to unit l2-norm.\"\"\"\n        norms = np.linalg.norm(A, axis=0)\n        # Avoid division by zero for zero columns\n        norms[norms == 0] = 1\n        return A / norms\n\n    def compute_mu(A):\n        \"\"\"Computes the mutual coherence of a matrix with normalized columns.\"\"\"\n        G = A.T @ A\n        np.fill_diagonal(G, 0)\n        return np.max(np.abs(G))\n\n    def compute_spark(A):\n        \"\"\"Computes the spark of a matrix by exhaustive search.\"\"\"\n        m, n = A.shape\n        # Iterate over subset sizes s from 1 to m+1 (or n+1, whichever is smaller)\n        for s in range(1, min(m, n) + 2):\n            for indices in combinations(range(n), s):\n                sub_matrix = A[:, list(indices)]\n                if np.linalg.matrix_rank(sub_matrix)  s:\n                    return s\n        # This part should theoretically not be reached for m  n\n        return m + 1\n\n    def omp(A, y, k_max, tol=1e-8):\n        \"\"\"\n        Orthogonal Matching Pursuit algorithm.\n        \n        Args:\n            A (np.ndarray): Sensing matrix.\n            y (np.ndarray): Measurement vector.\n            k_max (int): Maximum number of iterations (sparsity level).\n            tol (float): Tolerance for residual norm to stop.\n\n        Returns:\n            np.ndarray: The recovered sparse signal.\n        \"\"\"\n        m, n = A.shape\n        x_rec = np.zeros(n)\n        residual = np.copy(y)\n        support = []\n\n        for _ in range(k_max):\n            if np.linalg.norm(residual)  tol:\n                break\n            \n            # Find the atom that is most correlated with the residual\n            correlations = np.abs(A.T @ residual)\n            # Break ties by choosing the smallest index\n            new_idx = np.argmax(correlations)\n            \n            if new_idx not in support:\n                support.append(new_idx)\n            \n            # Solve least squares on the current support\n            A_support = A[:, support]\n            try:\n                # Use pseudoinverse for stable solution\n                x_support = np.linalg.pinv(A_support) @ y\n            except np.linalg.LinAlgError:\n                # This should not happen with pinv, but as a safeguard\n                break\n\n            # Update residual\n            residual = y - A_support @ x_support\n        \n        # Construct the final sparse signal\n        if support:\n            x_rec[support] = x_support\n            \n        return x_rec\n\n    def compute_strong_threshold(mu):\n        \"\"\"\n        Calculates the largest integer k satisfying k  0.5 * (1 + 1/mu).\n        \"\"\"\n        if mu == 0:\n            return np.inf  # Or a very large number, practically limited by dimensions\n        val = 0.5 * (1.0 + 1.0 / mu)\n        # We need the largest integer strictly less than val\n        return int(np.floor(val - 1e-9))\n\n    def compute_empirical_threshold(A, signal_suites, tol=1e-8):\n        \"\"\"\n        Computes the weak empirical threshold k_empirical.\n        \"\"\"\n        max_k_passed = 0\n        for k in sorted(signal_suites.keys()):\n            all_passed_for_k = True\n            for x_true in signal_suites[k]:\n                y = A @ x_true\n                # Run OMP with iteration cap equal to the true sparsity\n                x_rec = omp(A, y, k_max=k, tol=tol)\n                \n                # Check if reconstruction is accurate\n                if np.linalg.norm(x_true - x_rec)  tol:\n                    all_passed_for_k = False\n                    break\n            \n            if all_passed_for_k:\n                max_k_passed = k\n            else:\n                # If it fails for k, it can't be higher\n                break\n        return max_k_passed\n    \n    # --- Problem Setup ---\n    \n    # Define matrices\n    A1_cols = [\n        [1.0, 0.0, 0.0],\n        [-0.5, np.sqrt(3)/2, 0.0],\n        [0.5, 0.2886751345948129, 0.816496580927726],\n        [-0.5, -0.2886751345948129, 0.816496580927726]\n    ]\n    A1 = np.array(A1_cols).T\n\n    A2_cols = [\n        [1.0, 0.0, 0.0],\n        [-0.5, np.sqrt(3)/2, 0.0],\n        [0.5, np.sqrt(3)/2, 0.0],\n        [-0.5, -0.2886751345948129, 0.816496580927726]\n    ]\n    A2 = np.array(A2_cols).T\n    \n    matrices = [A1, A2]\n\n    # Define signal suites\n    n_dims = 4\n    signal_suites = {\n        1: [np.eye(n_dims)[i] for i in range(n_dims)],\n        2: [\n            np.array([1.0, 1.0, 0.0, 0.0]),\n            np.array([1.0, 0.0, 1.0, 0.0])\n        ]\n    }\n    \n    # --- Main Calculation Loop ---\n    \n    results = []\n    for A in matrices:\n        A_norm = normalize_columns(A)\n        \n        mu = compute_mu(A_norm)\n        spark = compute_spark(A_norm)\n        k_strong = compute_strong_threshold(mu)\n        k_empirical = compute_empirical_threshold(A_norm, signal_suites)\n        \n        results.append([mu, spark, k_strong, k_empirical])\n\n    # --- Final Output ---\n    \n    # Format the results exactly as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3494441"}, {"introduction": "Building on our understanding of different matrix properties, we now turn to a comparative analysis of the theoretical guarantees for a suite of popular recovery algorithms. This practice introduces the Restricted Isometry Property (RIP), a cornerstone of modern compressed sensing theory, and asks you to verify the distinct RIP-based sufficient conditions for Basis Pursuit, IHT, CoSaMP, and others. By doing so, you will gain insight into why different algorithms have different performance guarantees and identify scenarios that highlight the theoretical advantages of convex optimization methods [@problem_id:3494329].", "problem": "Consider the linear inverse model in compressed sensing, where a signal $x_{\\star} \\in \\mathbb{R}^{n}$ with at most $k$ nonzero entries (that is, $x_{\\star}$ is $k$-sparse) is measured via a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ with independent and identically distributed Gaussian entries of variance $1/m$, producing noiseless data $y = A x_{\\star}$. Two central notions are the weak and strong recovery thresholds: the weak threshold refers to typical-case recovery (with respect to random supports and signs) in the high-dimensional limit, while the strong threshold refers to uniform recovery guarantees that hold simultaneously for all $k$-sparse signals under the same measurement operator. For matrices with Gaussian entries, it is known from geometric and probabilistic theory that convex $\\ell_{1}$-minimization has sharp asymptotic thresholds; however, algorithmic performance and nonconvex/greedy methods can exhibit more conservative provable thresholds.\n\nYour task is to implement a program that, for small, finite dimensions, compares provable strong-threshold style guarantees for five algorithms:\n- Orthogonal Matching Pursuit (OMP),\n- Compressive Sampling Matching Pursuit (CoSaMP),\n- Iterative Hard Thresholding (IHT),\n- Basis Pursuit (BP),\n- Approximate Message Passing (AMP).\n\nYou must work from foundational definitions and well-tested sufficient conditions:\n\n1) Restricted Isometry Property (RIP). For a matrix $A \\in \\mathbb{R}^{m \\times n}$ and a positive integer $s$, the restricted isometry constant $\\delta_{s}$ is the smallest nonnegative number such that\n$$\n(1 - \\delta_{s}) \\lVert x \\rVert_{2}^{2} \\le \\lVert A x \\rVert_{2}^{2} \\le (1 + \\delta_{s}) \\lVert x \\rVert_{2}^{2}\n$$\nfor all $s$-sparse $x$. For a subset $S \\subset \\{1,\\dots,n\\}$ with $\\lvert S \\rvert = s$, letting $A_{S}$ denote the submatrix formed by the columns indexed by $S$, the above holds if and only if every eigenvalue of the Gram matrix $G_{S} = A_{S}^{\\top} A_{S}$ lies in $[1-\\delta_{s}, 1+\\delta_{s}]$. If $s  m$, then $\\delta_{s} \\ge 1$ because $A_{S}$ cannot have full column rank and thus some eigenvalue of $G_{S}$ is zero.\n\n2) Mutual coherence. For column-normalized $A$ with columns $a_{1},\\dots,a_{n}$, the mutual coherence is\n$$\n\\mu(A) = \\max_{i \\ne j} \\lvert a_{i}^{\\top} a_{j} \\rvert.\n$$\n\n3) Classical sufficient conditions (finite-dimensional, noiseless case; all constants below are standard, conservative values appearing in the literature, used here as well-tested facts to certify uniform, strong-type guarantees):\n- Basis Pursuit (BP): If $\\delta_{2k}  \\sqrt{2} - 1$, then $\\ell_{1}$-minimization uniquely recovers every $k$-sparse $x_{\\star}$.\n- Approximate Message Passing (AMP) with optimally tuned soft-threshold denoiser for Gaussian $A$: shares the same sharp phase transition as BP in the high-dimensional limit; here, we adopt the same sufficient condition as BP to certify strong-type recovery, i.e., declare success if $\\delta_{2k}  \\sqrt{2} - 1$.\n- Iterative Hard Thresholding (IHT): If $\\delta_{3k}  1/\\sqrt{32}$ and the step size is chosen below the inverse Lipschitz constant of the gradient, then IHT converges to the $k$-sparse solution.\n- Compressive Sampling Matching Pursuit (CoSaMP): If $\\delta_{4k}  0.1$, then CoSaMP exactly recovers every $k$-sparse $x_{\\star}$ in the noiseless case.\n- Orthogonal Matching Pursuit (OMP): If $\\mu(A)  \\frac{1}{2k-1}$, then OMP exactly recovers every $k$-sparse $x_{\\star}$.\n\nBecause computing $\\delta_{s}$ exactly requires testing all $\\binom{n}{s}$ column subsets, which is combinatorial, your program must:\n- Compute $\\delta_{s}$ exactly when $s  m$ by returning $\\delta_{s} = 1$.\n- Otherwise, estimate an upper bound on $\\delta_{s}$ by randomly sampling a fixed number of subsets $S$ of size $s$ and taking the maximum deviation of eigenvalues of $G_{S} = A_{S}^{\\top} A_{S}$ from $1$ across the sampled subsets. This produces a data-driven, empirically certified upper bound on $\\delta_{s}$ with respect to the sampled family. Use a fixed pseudorandom number generator seed for reproducibility.\n\nUsing the above, for each test case you must:\n- Generate a Gaussian matrix $A$ with entries distributed as $\\mathcal{N}(0, 1/m)$ with a specified seed.\n- Compute an estimate of $\\delta_{2k}$, $\\delta_{3k}$, $\\delta_{4k}$ by sampling subsets, and compute $\\mu(A)$ from column-normalized $A$.\n- Produce five booleans indicating whether each algorithm’s stated sufficient condition is satisfied (interpreted as a provable strong-type recovery certificate): $b_{\\mathrm{BP}}$, $b_{\\mathrm{AMP}}$, $b_{\\mathrm{OMP}}$, $b_{\\mathrm{CoSaMP}}$, $b_{\\mathrm{IHT}}$.\n- Also produce one additional boolean $b_{\\mathrm{adv}}$ indicating whether this instance falls into a region where the convex strong-type condition holds but at least one of the nonconvex/greedy certificates fails, i.e.,\n$$\nb_{\\mathrm{adv}} = \\Big( b_{\\mathrm{BP}} \\ \\text{is True} \\Big) \\ \\wedge \\ \\Big( (b_{\\mathrm{OMP}} \\ \\text{is False}) \\ \\lor \\ (b_{\\mathrm{CoSaMP}} \\ \\text{is False}) \\ \\lor \\ (b_{\\mathrm{IHT}} \\ \\text{is False}) \\ \\lor \\ (b_{\\mathrm{AMP}} \\ \\text{is False}) \\Big).\n$$\n\nTest Suite:\nProvide results for the following three cases, each with dimensions $(m,n,k)$ and a starting seed $s_{0}$:\n- Case $1$: $m = 12$, $n = 14$, $k = 2$, $s_{0} = 7$.\n- Case $2$: $m = 10$, $n = 14$, $k = 3$, $s_{0} = 1$. For this case only, to robustly identify an advantage region, you must search over seeds $s = s_{0}, s_{0} + 1, \\dots$ up to an internal cap until you find a matrix for which $b_{\\mathrm{adv}}$ is True; if none is found within the cap, return the results for the last tried seed.\n- Case $3$: $m = 7$, $n = 14$, $k = 4$, $s_{0} = 3$.\n\nConstants:\n- Use $\\sqrt{2} - 1$ for the Basis Pursuit and Approximate Message Passing certificate threshold on $\\delta_{2k}$.\n- Use $1/\\sqrt{32}$ for the Iterative Hard Thresholding certificate threshold on $\\delta_{3k}$.\n- Use $0.1$ for the Compressive Sampling Matching Pursuit certificate threshold on $\\delta_{4k}$.\n- Use $\\frac{1}{2k-1}$ for the Orthogonal Matching Pursuit certificate threshold on $\\mu(A)$.\n\nAngle units and physical units are not applicable here.\n\nFinal Output Format:\nYour program should produce a single line of output containing the aggregated boolean results for all three cases as a flat, comma-separated Python-style list of length $18$, grouped in blocks of six per case in the fixed algorithm order $[b_{\\mathrm{BP}}, b_{\\mathrm{AMP}}, b_{\\mathrm{OMP}}, b_{\\mathrm{CoSaMP}}, b_{\\mathrm{IHT}}, b_{\\mathrm{adv}}]$. For example,\n\"[True,False,True,True,False,True, ... for case 2 ..., ... for case 3 ...]\".", "solution": "The user-provided problem has been validated and is determined to be a valid, well-posed, and scientifically grounded task.\n\nThe problem asks for a comparison of theoretical strong-recovery guarantees for five common algorithms used in compressed sensing: Basis Pursuit ($BP$), Approximate Message Passing ($AMP$), Orthogonal Matching Pursuit ($OMP$), Compressive Sampling Matching Pursuit ($CoSaMP$), and Iterative Hard Thresholding ($IHT$). The comparison is to be performed by checking if certain sufficient conditions for exact signal recovery are met for specific problem instances defined by dimensions $(m, n, k)$ and a randomly generated measurement matrix $A \\in \\mathbb{R}^{m \\times n}$.\n\nA signal $x_{\\star} \\in \\mathbb{R}^{n}$ is called $k$-sparse if it has at most $k$ non-zero entries. We observe measurements $y = A x_{\\star}$. The goal of a recovery algorithm is to reconstruct $x_{\\star}$ from $y$ and $A$. A strong-recovery guarantee ensures that the algorithm succeeds for *all* $k$-sparse signals $x_{\\star}$ for a *fixed* matrix $A$. Such guarantees are typically expressed as conditions on geometric properties of the matrix $A$.\n\nThe two central properties are the Restricted Isometry Property (RIP) and the mutual coherence.\n\nThe Restricted Isometry Constant $\\delta_s$ of a matrix $A$ measures how close to an isometry the action of $A$ is on all $s$-sparse vectors. For any $s$-sparse vector $x$, we require $\\lVert A x \\rVert_{2}^{2}$ to be close to $\\lVert x \\rVert_{2}^{2}$. The condition is:\n$$\n(1 - \\delta_{s}) \\lVert x \\rVert_{2}^{2} \\le \\lVert A x \\rVert_{2}^{2} \\le (1 + \\delta_{s}) \\lVert x \\rVert_{2}^{2}\n$$\nThis is equivalent to stating that for any subset $S \\subseteq \\{1, \\dots, n\\}$ with size $\\lvert S \\rvert = s$, all eigenvalues of the Gram matrix $G_S = A_S^{\\top} A_S$ (where $A_S$ is the submatrix of $A$ with columns indexed by $S$) must lie in the interval $[1 - \\delta_s, 1 + \\delta_s]$. A small $\\delta_s$ indicates that the matrix $A$ preserves the lengths of all $s$-sparse vectors well. If the number of columns $s$ in the submatrix $A_S$ is greater than the number of rows $m$, i.e., $s  m$, then $A_S$ is rank-deficient. Consequently, $G_S$ must have a zero eigenvalue, which implies $\\delta_s \\ge 1$. The problem specifies that in this case, we should set $\\delta_s = 1$.\n\nThe mutual coherence $\\mu(A)$ is defined for a matrix with column-normalized columns $a_i$ as the largest absolute inner product between any two distinct columns:\n$$\n\\mu(A) = \\max_{i \\ne j} \\lvert a_{i}^{\\top} a_{j} \\rvert\n$$\nA small $\\mu(A)$ means the columns of $A$ are nearly orthogonal, which is beneficial for greedy algorithms like $OMP$.\n\nSince computing $\\delta_s$ exactly is an NP-hard problem (it requires checking all $\\binom{n}{s}$ submatrices), we must resort to an estimation procedure. The problem instructs us to estimate an upper bound on $\\delta_s$ by sampling a fixed number of column subsets of size $s$, computing the maximum eigenvalue deviation from $1$ for each corresponding Gram matrix, and taking the maximum over all sampled subsets. For reproducibility, a fixed number of $1000$ random subsets will be sampled for each estimation of $\\delta_s$.\n\nThe sufficient conditions provided in the problem statement for the five algorithms are used to certify recovery. For each test case with parameters $(m, n, k)$ and a generated matrix $A$:\n1.  **Basis Pursuit ($BP$)**: A convex relaxation method. The certificate is $b_{\\mathrm{BP}} = (\\delta_{2k}  \\sqrt{2} - 1)$. The value $\\sqrt{2} - 1 \\approx 0.4142$.\n2.  **Approximate Message Passing ($AMP$)**: An iterative algorithm inspired by statistical physics. The problem states to use the same certificate as $BP$: $b_{\\mathrm{AMP}} = (\\delta_{2k}  \\sqrt{2} - 1)$. This implies $b_{\\mathrm{BP}}$ and $b_{\\mathrm{AMP}}$ will always be identical.\n3.  **Orthogonal Matching Pursuit ($OMP$)**: A classic greedy algorithm. The certificate is based on coherence: $b_{\\mathrm{OMP}} = (\\mu(A)  \\frac{1}{2k-1})$.\n4.  **Compressive Sampling Matching Pursuit ($CoSaMP$)**: A more advanced greedy algorithm. The certificate is $b_{\\mathrm{CoSaMP}} = (\\delta_{4k}  0.1)$.\n5.  **Iterative Hard Thresholding ($IHT$)**: Another major iterative algorithm. The certificate is $b_{\\mathrm{IHT}} = (\\delta_{3k}  1/\\sqrt{32})$. The value $1/\\sqrt{32} \\approx 0.1768$.\n\nFinally, we compute an \"advantage\" boolean, $b_{\\mathrm{adv}}$. This flag is true if the convex method's ($BP$) guarantee holds, while at least one of the nonconvex/greedy methods' guarantees (for $OMP$, $CoSaMP$, $IHT$) fails. The provided definition is:\n$$\nb_{\\mathrm{adv}} = (b_{\\mathrm{BP}}) \\wedge ((\\neg b_{\\mathrm{OMP}}) \\lor (\\neg b_{\\mathrm{CoSaMP}}) \\lor (\\neg b_{\\mathrm{IHT}}) \\lor (\\neg b_{\\mathrm{AMP}}))\n$$\nSince $b_{\\mathrm{BP}} \\equiv b_{\\mathrm{AMP}}$, if $b_{\\mathrm{BP}}$ is true, then $\\neg b_{\\mathrm{AMP}}$ is false. The expression simplifies to:\n$$\nb_{\\mathrm{adv}} = b_{\\mathrm{BP}} \\wedge ((\\neg b_{\\mathrm{OMP}}) \\lor (\\neg b_{\\mathrm{CoSaMP}}) \\lor (\\neg b_{\\mathrm{IHT}}))\n$$\nIf $b_{\\mathrm{BP}}$ is false, $b_{\\mathrm{adv}}$ is false.\n\nThe implementation will proceed as follows for each test case:\n- A function `evaluate_case(m, n, k, seed)` will handle the logic for a single instance.\n- It will generate the matrix $A \\in \\mathbb{R}^{m \\times n}$ with entries drawn from $\\mathcal{N}(0, 1/m)$ using the given seed.\n- It will call helper functions to estimate $\\delta_{2k}$, $\\delta_{3k}$, $\\delta_{4k}$ and compute $\\mu(A)$.\n- Specific logic is applied for each case:\n    - **Case 1**: $(m, n, k) = (12, 14, 2)$, $s_0=7$. All required orders of $\\delta_s$ (i.e., $s=4, 6, 8$) are less than $m=12$, so all are estimated via sampling.\n    - **Case 2**: $(m, n, k) = (10, 14, 3)$, $s_0=1$. We need $\\delta_6, \\delta_9, \\delta_{12}$. Since $12  m=10$, $\\delta_{12}$ is set to $1$. This ensures the $CoSaMP$ condition fails. The code will search over seeds starting from $s_0=1$ up to an internal cap of $s_0+100$ to find an instance where $b_{\\mathrm{adv}}$ is true. If none is found, the result for the last seed is used.\n    - **Case 3**: $(m, n, k) = (7, 14, 4)$, $s_0=3$. We need $\\delta_8, \\delta_{12}, \\delta_{16}$. All orders are greater than $m=7$, so all three $\\delta_s$ values are set to $1$. This causes the certificates for $BP$, $AMP$, $CoSaMP$, and $IHT$ to fail deterministically.\n\nThe results from the three cases will be aggregated into a single flat list for the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# from scipy import ...\n\ndef estimate_delta_s(A, s, num_samples, rng):\n    \"\"\"\n    Estimates the restricted isometry constant delta_s of matrix A.\n\n    If s  m, delta_s = 1. We return 1.0 as per problem description.\n    Otherwise, it is estimated by sampling subsets of columns.\n    \"\"\"\n    m, n = A.shape\n    if s  n:\n        # Cannot choose s columns from n if sn. This case may not occur\n        # with problem inputs but is good practice.\n        return 1.0\n    if s  m:\n        return 1.0\n\n    max_dev = 0.0\n    indices = np.arange(n)\n\n    for _ in range(num_samples):\n        S = rng.choice(indices, size=s, replace=False)\n        A_S = A[:, S]\n        G_S = A_S.T @ A_S\n        \n        # eigvalsh is for Hermitian (or real symmetric) matrices.\n        eigenvalues = np.linalg.eigvalsh(G_S)\n        \n        dev = np.max(np.abs(eigenvalues - 1))\n        if dev  max_dev:\n            max_dev = dev\n            \n    return max_dev\n\ndef compute_mu(A):\n    \"\"\"\n    Computes the mutual coherence of matrix A.\n    \"\"\"\n    # Normalize columns of A\n    norm_A = A / np.linalg.norm(A, axis=0)\n    \n    # Compute Gram matrix\n    G = norm_A.T @ norm_A\n    \n    # Set diagonal to zero to ignore self-correlation\n    np.fill_diagonal(G, 0)\n    \n    # Mutual coherence is the max absolute off-diagonal element\n    mu = np.max(np.abs(G))\n    return mu\n\ndef evaluate_case(m, n, k, seed, num_samples):\n    \"\"\"\n    Evaluates recovery conditions for a single (m, n, k, seed) case.\n    \"\"\"\n    # Use a specific Random Number Generator for this evaluation\n    rng = np.random.default_rng(seed)\n    \n    A = rng.normal(0, 1 / np.sqrt(m), (m, n))\n\n    # Calculate RICs\n    # Note: The same RNG is passed, so subset sampling is deterministic for a given seed.\n    s_2k = 2 * k\n    s_3k = 3 * k\n    s_4k = 4 * k\n\n    delta_2k = estimate_delta_s(A, s_2k, num_samples, rng)\n    delta_3k = estimate_delta_s(A, s_3k, num_samples, rng)\n    delta_4k = estimate_delta_s(A, s_4k, num_samples, rng)\n    \n    # Calculate mutual coherence\n    mu = compute_mu(A)\n    \n    # Check sufficient conditions\n    b_BP = delta_2k  (np.sqrt(2) - 1)\n    b_AMP = delta_2k  (np.sqrt(2) - 1) # Same as BP per problem\n    b_OMP = mu  (1 / (2 * k - 1)) if (2 * k - 1) != 0 else False\n    b_CoSaMP = delta_4k  0.1\n    b_IHT = delta_3k  (1 / np.sqrt(32))\n    \n    # Calculate advantage boolean\n    # b_adv = b_BP and (not b_OMP or not b_CoSaMP or not b_IHT or not b_AMP)\n    # Since b_BP == b_AMP, this simplifies.\n    b_adv = b_BP and (not b_OMP or not b_CoSaMP or not b_IHT)\n\n    return [b_BP, b_AMP, b_OMP, b_CoSaMP, b_IHT, b_adv]\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (m, n, k, s0)\n        (12, 14, 2, 7),\n        (10, 14, 3, 1),\n        (7, 14, 4, 3),\n    ]\n\n    # Constants for computation\n    NUM_SAMPLES_DELTA = 1000\n    SEED_SEARCH_CAP = 100\n    \n    all_results = []\n\n    # Case 1\n    m, n, k, s0 = test_cases[0]\n    results_case1 = evaluate_case(m, n, k, s0, NUM_SAMPLES_DELTA)\n    all_results.extend(results_case1)\n    \n    # Case 2\n    m, n, k, s0 = test_cases[1]\n    current_seed = s0\n    found_adv = False\n    results_case2 = []\n    \n    while current_seed  s0 + SEED_SEARCH_CAP:\n        results_case2 = evaluate_case(m, n, k, current_seed, NUM_SAMPLES_DELTA)\n        b_adv = results_case2[-1]\n        if b_adv:\n            found_adv = True\n            break\n        current_seed += 1\n    all_results.extend(results_case2)\n\n    # Case 3\n    m, n, k, s0 = test_cases[2]\n    results_case3 = evaluate_case(m, n, k, s0, NUM_SAMPLES_DELTA)\n    all_results.extend(results_case3)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```", "id": "3494329"}]}