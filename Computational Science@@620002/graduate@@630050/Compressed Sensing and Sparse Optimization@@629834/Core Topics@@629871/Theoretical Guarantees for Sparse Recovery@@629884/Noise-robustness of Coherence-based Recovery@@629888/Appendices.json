{"hands_on_practices": [{"introduction": "The fundamental challenge in sparse recovery is identifying the correct signal components from noisy measurements. This exercise demonstrates how mutual coherence governs the \"crosstalk\" between different components, which can obscure the true signal. By deriving a lower bound on signal amplitude, you will gain a concrete understanding of the trade-off between signal strength, noise level, and the geometric properties of the sensing matrix required for successful support recovery. [@problem_id:3462322]", "problem": "Consider a linear sensing model in compressed sensing with measurement matrix $A \\in \\mathbb{R}^{m \\times n}$ whose columns $\\{a_i\\}_{i=1}^{n}$ are unit-norm and have mutual coherence $\\mu(A) = \\max_{i \\neq j} |a_i^{\\top} a_j|$. A $k$-sparse signal $x \\in \\mathbb{R}^{n}$ is measured as $y = A x + w$, where the noise $w \\in \\mathbb{R}^{m}$ obeys the energy bound $\\|w\\|_{2} \\leq \\Delta$. Assume the nonzero entries of $x$ have equal magnitude $a  0$ and define the support set $S \\subset \\{1, \\dots, n\\}$ with $|S| = k$. The correlation-thresholding rule declares an index $i$ as active if the absolute correlation $|a_i^{\\top} y|$ exceeds a threshold that is chosen to separate the true support from the complement using only $\\mu(A)$, $\\|x\\|_{1}$, and $\\Delta$.\n\nStarting from the definitions of mutual coherence and correlation with unit-norm columns, as well as the triangle inequality and the Cauchyâ€“Schwarz inequality, derive a sufficient lower bound on the minimum nonzero amplitude $a_{\\min}$ that ensures exact support recovery by correlation thresholding in the presence of noise. Then, for the numerical instance with $\\Delta = 10^{-3}$, $m = 500$, $n = 5000$, $\\mu(A) = 0.08$, and $k = 6$, compute the exact value of $a_{\\min}$ implied by your bound. Express the final answer as a single simplified fraction with no units. Also verify, within your derivation, whether the coherence-based sparsity requirement necessary for the bound to be meaningful is satisfied by the given parameters.\n\nYour final answer must be a single calculation (a number or a closed-form expression). No rounding is required.", "solution": "The problem asks for a sufficient lower bound on the magnitude of the nonzero entries of a sparse signal to guarantee exact support recovery in a noisy compressed sensing scenario. We begin by formalizing the condition for successful recovery and then derive the bound using the provided tools.\n\nThe linear measurement model is given by $y = A x + w$, where $y \\in \\mathbb{R}^{m}$ is the measurement vector, $A \\in \\mathbb{R}^{m \\times n}$ is the measurement matrix, $x \\in \\mathbb{R}^{n}$ is the $k$-sparse signal, and $w \\in \\mathbb{R}^{m}$ is a noise vector with bounded energy $\\|w\\|_{2} \\leq \\Delta$. The columns of $A$, denoted $\\{a_i\\}_{i=1}^{n}$, are unit-norm, i.e., $\\|a_i\\|_{2} = 1$ for all $i \\in \\{1, \\dots, n\\}$. The mutual coherence of $A$ is $\\mu(A) = \\max_{i \\neq j} |a_i^{\\top} a_j|$. The signal $x$ is $k$-sparse, meaning it has at most $k$ non-zero entries. Let $S = \\text{supp}(x)$ be the support set of $x$, with $|S|=k$. We are given that for any $j \\in S$, the magnitude of the entry is $|x_j| = a  0$.\n\nRecovery is performed using correlation thresholding: an index $i$ is identified as part of the support if the magnitude of its correlation with the measurement vector, $|a_i^{\\top} y|$, exceeds a certain threshold $\\tau$. For exact support recovery, there must exist a threshold $\\tau$ that perfectly separates the correlations corresponding to the true support $S$ from those corresponding to its complement $S^c$. This requires the following condition to hold:\n$$ \\min_{i \\in S} |a_i^{\\top} y|  \\max_{j \\notin S} |a_j^{\\top} y| $$\nWe will derive a lower bound for the term on the left and an upper bound for the term on the right.\n\nFirst, consider an index $i \\in S$. The correlation is:\n$$ a_i^{\\top} y = a_i^{\\top} (A x + w) = a_i^{\\top} \\left( \\sum_{l=1}^{n} x_l a_l \\right) + a_i^{\\top} w $$\nSince $x_l = 0$ for $l \\notin S$, we can write the sum over the support $S$:\n$$ a_i^{\\top} y = \\sum_{l \\in S} x_l (a_i^{\\top} a_l) + a_i^{\\top} w $$\nWe can separate the term where $l=i$:\n$$ a_i^{\\top} y = x_i (a_i^{\\top} a_i) + \\sum_{l \\in S, l \\neq i} x_l (a_i^{\\top} a_l) + a_i^{\\top} w $$\nSince the columns $a_i$ are unit-norm, $a_i^{\\top} a_i = \\|a_i\\|_{2}^2 = 1$. This simplifies the expression to:\n$$ a_i^{\\top} y = x_i + \\sum_{l \\in S, l \\neq i} x_l (a_i^{\\top} a_l) + a_i^{\\top} w $$\nTo find a lower bound on $|a_i^{\\top} y|$, we use the reverse triangle inequality, $|u+v| \\ge |u| - |v|$:\n$$ |a_i^{\\top} y| \\ge |x_i| - \\left| \\sum_{l \\in S, l \\neq i} x_l (a_i^{\\top} a_l) + a_i^{\\top} w \\right| $$\nApplying the triangle inequality to the subtracted term:\n$$ |a_i^{\\top} y| \\ge |x_i| - \\left( \\left| \\sum_{l \\in S, l \\neq i} x_l (a_i^{\\top} a_l) \\right| + |a_i^{\\top} w| \\right) $$\nWe bound each term. We are given $|x_i|=a$. The sum term is bounded using the triangle inequality and the definition of mutual coherence:\n$$ \\left| \\sum_{l \\in S, l \\neq i} x_l (a_i^{\\top} a_l) \\right| \\le \\sum_{l \\in S, l \\neq i} |x_l| |a_i^{\\top} a_l| \\le \\sum_{l \\in S, l \\neq i} a \\cdot \\mu(A) = (k-1) a \\mu(A) $$\nThe noise term is bounded using the Cauchy-Schwarz inequality:\n$$ |a_i^{\\top} w| \\le \\|a_i\\|_{2} \\|w\\|_{2} \\le 1 \\cdot \\Delta = \\Delta $$\nSubstituting these bounds, we get a lower bound for the on-support correlations:\n$$ \\min_{i \\in S} |a_i^{\\top} y| \\ge a - ((k-1) a \\mu(A) + \\Delta) = a(1 - (k-1)\\mu(A)) - \\Delta $$\nNext, consider an index $j \\notin S$. The correlation is:\n$$ a_j^{\\top} y = \\sum_{l \\in S} x_l (a_j^{\\top} a_l) + a_j^{\\top} w $$\nTo find an upper bound on $|a_j^{\\top} y|$, we use the triangle inequality:\n$$ |a_j^{\\top} y| \\le \\left| \\sum_{l \\in S} x_l (a_j^{\\top} a_l) \\right| + |a_j^{\\top} w| $$\nWe bound the sum term. Since $j \\notin S$ and $l \\in S$, we have $j \\neq l$, so $|a_j^{\\top} a_l| \\le \\mu(A)$.\n$$ \\left| \\sum_{l \\in S} x_l (a_j^{\\top} a_l) \\right| \\le \\sum_{l \\in S} |x_l| |a_j^{\\top} a_l| \\le \\sum_{l \\in S} a \\cdot \\mu(A) = k a \\mu(A) $$\nThe noise term is bounded as before: $|a_j^{\\top} w| \\le \\Delta$.\nSubstituting these bounds, we get an upper bound for the off-support correlations:\n$$ \\max_{j \\notin S} |a_j^{\\top} y| \\le k a \\mu(A) + \\Delta $$\nA sufficient condition for exact support recovery is for the lower bound of the minimum on-support correlation to be greater than the upper bound of the maximum off-support correlation:\n$$ a(1 - (k-1)\\mu(A)) - \\Delta  k a \\mu(A) + \\Delta $$\nWe solve this inequality for $a$:\n$$ a - a(k-1)\\mu(A) - k a \\mu(A)  2\\Delta $$\n$$ a(1 - (k-1)\\mu(A) - k\\mu(A))  2\\Delta $$\n$$ a(1 - k\\mu(A) + \\mu(A) - k\\mu(A))  2\\Delta $$\n$$ a(1 - (2k-1)\\mu(A))  2\\Delta $$\nFor this inequality to yield a meaningful positive lower bound for $a$, the coefficient of $a$ must be positive. This gives the well-known coherence-based sparsity requirement:\n$$ 1 - (2k-1)\\mu(A)  0 \\implies (2k-1)\\mu(A)  1 $$\nIf this condition holds, we can divide to find the lower bound on $a$:\n$$ a  \\frac{2\\Delta}{1 - (2k-1)\\mu(A)} $$\nThe minimum required amplitude, $a_{\\min}$, is therefore:\n$$ a_{\\min} = \\frac{2\\Delta}{1 - (2k-1)\\mu(A)} $$\nNow we use the provided numerical values: $\\Delta = 10^{-3}$, $k = 6$, and $\\mu(A) = 0.08$. The other parameters $m=500$ and $n=5000$ are consistent with a compressed sensing setup but do not enter this specific bound calculation.\n\nFirst, we verify the coherence-based sparsity requirement:\n$$ (2k-1)\\mu(A) = (2 \\cdot 6 - 1) \\cdot 0.08 = (11) \\cdot 0.08 = 0.88 $$\nSince $0.88  1$, the requirement is satisfied, and our derived bound for $a_{\\min}$ is valid and meaningful.\n\nNow, we compute the value of $a_{\\min}$:\n$$ a_{\\min} = \\frac{2 \\cdot 10^{-3}}{1 - 0.88} = \\frac{2 \\cdot 10^{-3}}{0.12} $$\nTo express this as a simplified fraction:\n$$ a_{\\min} = \\frac{2 \\times \\frac{1}{1000}}{\\frac{12}{100}} = \\frac{\\frac{2}{1000}}{\\frac{12}{100}} = \\frac{2}{1000} \\cdot \\frac{100}{12} = \\frac{200}{12000} = \\frac{2}{120} = \\frac{1}{60} $$\nThus, the minimum nonzero amplitude required to guarantee support recovery under these conditions is $\\frac{1}{60}$.", "answer": "$$\\boxed{\\frac{1}{60}}$$", "id": "3462322"}, {"introduction": "Once the signal's support is identified, we must accurately estimate the amplitudes of its nonzero components. This practice explores the stability of this estimation, using the Gershgorin circle theorem to connect mutual coherence to the amplification of noise in the final estimate. Calculating the degradation factor provides a quantitative measure of how sensitive the system's accuracy is to perturbations in the measurement matrix. [@problem_id:3462319]", "problem": "Consider a linear sensing model with measurements given by $y = A x + w$, where $A \\in \\mathbb{R}^{m \\times n}$ has unit-norm columns, $x \\in \\mathbb{R}^{n}$ is a $k$-sparse vector supported on an unknown index set $S$ with $|S| = k$, and $w \\in \\mathbb{R}^{m}$ is an additive noise vector satisfying $\\|w\\|_{2} \\leq \\varepsilon$. The mutual coherence of $A$ is defined as $\\mu(A) = \\max_{i \\neq j} |\\langle a_{i}, a_{j} \\rangle|$, where $a_{i}$ denotes the $i$-th column of $A$. Assume a coherence-based recovery procedure (for example, thresholding or Orthogonal Matching Pursuit (OMP)) correctly identifies the support $S$ and then estimates amplitudes by a least-squares debiasing step on $A_{S}$.\n\nStarting from the mutual coherence definition and the Gershgorin circle theorem, derive a deterministic upper bound on the amplitude estimation error of the form\n$$\n\\|x^{\\star} - x\\|_{2} \\leq c(\\mu(A), k)\\,\\varepsilon,\n$$\nwhere $x^{\\star}$ is the least-squares estimate on the true support $S$, and $c(\\mu(A), k)$ is a constant expressed explicitly in terms of $\\mu(A)$ and $k$ that arises from spectral bounds on the Gram matrix $A_{S}^{\\top} A_{S}$. Then, for an initial coherence $\\mu(A) = 0.2$ and sparsity level $k = 4$, evaluate $c(\\mu(A), k)$. Next, suppose a perturbation in the sensing process increases the mutual coherence by $0.05$, i.e., $\\mu(A)$ becomes $0.25$. Recompute $c(\\mu(A), k)$ under the perturbed coherence and determine the multiplicative degradation factor in the bound, defined as the ratio of the perturbed constant to the original constant.\n\nReport the multiplicative degradation factor as a single closed-form expression. Do not round your answer.", "solution": "The problem asks for a derivation of an error bound for a least-squares estimate of a sparse signal, given that the correct support is known. Let $S$ be the support of the $k$-sparse signal $x$, and let $x_S \\in \\mathbb{R}^k$ be the vector of non-zero entries of $x$. Similarly, let $A_S \\in \\mathbb{R}^{m \\times k}$ be the submatrix of $A$ formed by the columns indexed by $S$. The measurement model restricted to the support is\n$$\ny = A_S x_S + w\n$$\nThe least-squares estimate $x^{\\star}_S$ of $x_S$ is found by solving the normal equations, which yields:\n$$\nx^{\\star}_S = (A_S^{\\top} A_S)^{-1} A_S^{\\top} y\n$$\nTo find the estimation error, we substitute the expression for $y$:\n$$\nx^{\\star}_S = (A_S^{\\top} A_S)^{-1} A_S^{\\top} (A_S x_S + w) = (A_S^{\\top} A_S)^{-1} (A_S^{\\top} A_S) x_S + (A_S^{\\top} A_S)^{-1} A_S^{\\top} w\n$$\nThis simplifies to:\n$$\nx^{\\star}_S = x_S + (A_S^{\\top} A_S)^{-1} A_S^{\\top} w\n$$\nThe estimation error is the difference $x^{\\star}_S - x_S$. Its Euclidean norm is bounded as follows:\n$$\n\\|x^{\\star} - x\\|_{2} = \\|x^{\\star}_S - x_S\\|_{2} = \\|(A_S^{\\top} A_S)^{-1} A_S^{\\top} w\\|_{2} \\leq \\|(A_S^{\\top} A_S)^{-1}\\|_{2} \\|A_S^{\\top} w\\|_{2}\n$$\nLet $G = A_S^{\\top} A_S$ be the Gram matrix of size $k \\times k$. The spectral norm of its inverse is $\\|G^{-1}\\|_{2} = 1/\\sigma_{\\min}(G)$, where $\\sigma_{\\min}(G)$ is the smallest singular value of $G$. Since $G$ is a symmetric positive semi-definite matrix, its singular values are its eigenvalues, so $\\sigma_{\\min}(G) = \\lambda_{\\min}(G)$. The norm of $A_S^{\\top}w$ can be bounded by $\\|A_S^{\\top}w\\|_2 \\le \\|A_S^{\\top}\\|_2 \\|w\\|_2 = \\sqrt{\\lambda_{\\max}(A_S^{\\top}A_S)}\\|w\\|_2 = \\sqrt{\\lambda_{\\max}(G)}\\|w\\|_2$. The error bound becomes:\n$$\n\\|x^{\\star} - x\\|_{2} \\leq \\frac{\\sqrt{\\lambda_{\\max}(G)}}{\\lambda_{\\min}(G)} \\|w\\|_{2}\n$$\nWe use the Gershgorin circle theorem to bound the eigenvalues of $G$. The entries of $G$ are $G_{ij} = \\langle a_i, a_j \\rangle$ for $i,j \\in S$. The diagonal entries are $G_{ii} = \\|a_i\\|_2^2 = 1$ since the columns of $A$ are unit-norm. The magnitudes of the off-diagonal entries are bounded by the mutual coherence: $|G_{ij}| \\leq \\mu(A)$ for $i \\neq j$.\n\nThe Gershgorin circle theorem states that for any eigenvalue $\\lambda$ of $G$, there is at least one diagonal entry $G_{ii}$ such that $|\\lambda - G_{ii}| \\leq \\sum_{j \\neq i, j\\in S} |G_{ij}|$. Since $G_{ii} = 1$ for all $i$ and the sum has $k-1$ terms, we have:\n$$\n|\\lambda - 1| \\leq \\sum_{j \\neq i, j\\in S} \\mu(A) = (k-1)\\mu(A)\n$$\nThis inequality implies that all eigenvalues of $G$ must lie in the interval $[1 - (k-1)\\mu(A), 1 + (k-1)\\mu(A)]$. Thus, we obtain bounds on the minimum and maximum eigenvalues:\n$$\n\\lambda_{\\min}(G) \\geq 1 - (k-1)\\mu(A)\n$$\n$$\n\\lambda_{\\max}(G) \\leq 1 + (k-1)\\mu(A)\n$$\nSubstituting these bounds into the error inequality, and using the given condition $\\|w\\|_{2} \\leq \\varepsilon$:\n$$\n\\|x^{\\star} - x\\|_{2} \\leq \\frac{\\sqrt{1 + (k-1)\\mu(A)}}{1 - (k-1)\\mu(A)} \\varepsilon\n$$\nThis is the desired bound, with the constant $c(\\mu(A), k)$ identified as:\n$$\nc(\\mu(A), k) = \\frac{\\sqrt{1 + (k-1)\\mu(A)}}{1 - (k-1)\\mu(A)}\n$$\nNow, we perform the required calculations.\n\nFirst, for the initial case, we have $\\mu(A) = \\mu_1 = 0.2$ and $k=4$.\n$$\nc_1 = c(0.2, 4) = \\frac{\\sqrt{1 + (4-1)(0.2)}}{1 - (4-1)(0.2)} = \\frac{\\sqrt{1 + 3(0.2)}}{1 - 3(0.2)} = \\frac{\\sqrt{1+0.6}}{1-0.6} = \\frac{\\sqrt{1.6}}{0.4}\n$$\nTo simplify, we write the numbers as fractions: $\\sqrt{1.6} = \\sqrt{16/10} = 4/\\sqrt{10}$ and $0.4 = 4/10$.\n$$\nc_1 = \\frac{4/\\sqrt{10}}{4/10} = \\frac{4}{\\sqrt{10}} \\cdot \\frac{10}{4} = \\frac{10}{\\sqrt{10}} = \\sqrt{10}\n$$\nNext, for the perturbed case, the coherence increases by $0.05$, so $\\mu(A) = \\mu_2 = 0.2 + 0.05 = 0.25$. The sparsity level remains $k=4$.\n$$\nc_2 = c(0.25, 4) = \\frac{\\sqrt{1 + (4-1)(0.25)}}{1 - (4-1)(0.25)} = \\frac{\\sqrt{1 + 3(0.25)}}{1 - 3(0.25)} = \\frac{\\sqrt{1+0.75}}{1-0.75} = \\frac{\\sqrt{1.75}}{0.25}\n$$\nAgain, we use fractions: $\\sqrt{1.75} = \\sqrt{7/4} = \\sqrt{7}/2$ and $0.25 = 1/4$.\n$$\nc_2 = \\frac{\\sqrt{7}/2}{1/4} = \\frac{\\sqrt{7}}{2} \\cdot 4 = 2\\sqrt{7}\n$$\nFinally, we compute the multiplicative degradation factor, which is the ratio of the perturbed constant to the original constant:\n$$\n\\text{Degradation Factor} = \\frac{c_2}{c_1} = \\frac{2\\sqrt{7}}{\\sqrt{10}}\n$$\nTo express this as a single closed-form expression, we rationalize the denominator:\n$$\n\\frac{c_2}{c_1} = \\frac{2\\sqrt{7}\\sqrt{10}}{\\sqrt{10}\\sqrt{10}} = \\frac{2\\sqrt{70}}{10} = \\frac{\\sqrt{70}}{5}\n$$", "answer": "$$\\boxed{\\frac{\\sqrt{70}}{5}}$$", "id": "3462319"}, {"introduction": "Theoretical guarantees derived from coherence often provide worst-case bounds, which can be pessimistic. This computational exercise bridges theory and practice by tasking you with simulating the phase transition of recovery and observing the impact of the Signal-to-Noise Ratio (SNR). By implementing the Orthogonal Matching Pursuit (OMP) algorithm and comparing matrices with different structural properties, you will gain crucial intuition about the gap between worst-case analysis and typical performance in sparse recovery. [@problem_id:3462348]", "problem": "Consider the standard linear measurements model in compressed sensing, where a measurement vector $y \\in \\mathbb{R}^m$ is obtained from a sparse signal $x \\in \\mathbb{R}^n$ by $y = A x + e$, with measurement matrix $A \\in \\mathbb{R}^{m \\times n}$, and additive noise $e \\in \\mathbb{R}^m$. Assume that $x$ is $k$-sparse, meaning it has exactly $k$ nonzero entries. Let the columns of $A$ be unit-norm. Define the mutual coherence of $A$ as $\\mu(A) = \\max_{i \\neq j} |\\langle a_i, a_j \\rangle|$, where $a_i$ denotes the $i$-th column of $A$. Support recovery refers to exactly identifying the index set of nonzero entries of $x$.\n\nThis problem analyzes the phase transition of support recovery in the plane $(k/m, \\mathrm{SNR})$ under a fixed mutual coherence $\\mu(A)$, and validates whether coherence-based thresholds predict the recovery boundary compared to simulations using matrices known to be favorable under the Restricted Isometry Property (RIP).\n\nBase the derivation and algorithmic design on the following fundamental definitions and facts:\n- Mutual coherence $\\mu(A)$ controls the maximum correlation between distinct columns of $A$.\n- Sparse recovery algorithms like Orthogonal Matching Pursuit (OMP) make greedy selections based on correlations with the residual.\n- In the noise-free case, classical coherence-based guarantees provide a sufficient sparsity bound (in terms of $\\mu(A)$) for exact support recovery.\n- For noisy measurements, Signal-to-Noise Ratio (SNR) is defined as $\\mathrm{SNR} = \\|A x\\|_2^2 / \\|e\\|_2^2$, which is a unitless ratio. Higher $\\mathrm{SNR}$ typically improves recovery performance.\n- Gaussian random matrices with normalized columns satisfy the Restricted Isometry Property with high probability for appropriate dimensions and sparsity, leading to robust recovery empirically.\n\nYour program must implement the following:\n- Use Orthogonal Matching Pursuit (OMP) for support recovery with a fixed number of iterations equal to the true sparsity $k$. At each iteration, select the column of $A$ that maximizes the absolute correlation with the current residual, update the support, and recompute the least-squares estimate over the selected support.\n- For each test matrix with fixed mutual coherence $\\mu(A)$, construct $A$ as follows:\n  - Choose a unit vector $u \\in \\mathbb{R}^m$ and a unit vector $w \\in \\mathbb{R}^m$ with $\\langle u, w \\rangle = 0$.\n  - Set two columns $a_1 = u$ and $a_2 = \\alpha u + \\sqrt{1 - \\alpha^2}\\, w$ with a prescribed target $\\alpha \\in (0,1)$, so that $\\langle a_1, a_2 \\rangle = \\alpha$.\n  - Generate the remaining $n-2$ columns as unit vectors in the orthogonal complement of $\\mathrm{span}\\{u, w\\}$ (to ensure zero inner product with $a_1$ and $a_2$), and normalize them.\n  - The resulting matrix has fixed $\\mu(A)$ dominated by the pair $(a_1, a_2)$, with $\\mu(A) \\approx \\alpha$.\n- For each test case, also construct a comparison matrix with independent and identically distributed Gaussian entries and normalized columns (a typical RIP-friendly model).\n- For the noise $e$, use independent Gaussian entries with variance chosen so that the expected energy satisfies $\\mathrm{SNR} = \\|A x\\|_2^2 / \\|e\\|_2^2$. The SNR values in all tests are unitless ratios (not decibels), and must be treated as such.\n\nDefine the phase transition boundary at a fixed $\\mathrm{SNR}$ as the largest $k$ (over a scan of increasing $k$) for which the empirical success rate of exact support recovery is at least a specified threshold. Measure the boundary in terms of the ratio $k/m$.\n\nValidation criteria:\n- Compute a predicted sparsity limit $K_{\\mathrm{coh}}$ for the noise-free case from the classical sufficient condition based on mutual coherence (do not include or assume any numerical constants in the problem statement; derive them in the solution). Treat this as a predicted lower bound on the boundary.\n- For each test case and the highest $\\mathrm{SNR}$ level, compare:\n  1. Whether the observed boundary for the fixed-$\\mu(A)$ matrix is at least the predicted $K_{\\mathrm{coh}}$ (lower-bound validation).\n  2. Whether the observed boundary for the RIP-friendly (Gaussian) matrix is greater than or equal to that of the fixed-$\\mu(A)$ matrix (RIP superiority).\n- Additionally, verify monotonicity of the boundary as a function of $\\mathrm{SNR}$: the boundary should be nondecreasing with increasing $\\mathrm{SNR}$.\n\nExperimental protocol:\n- In all tests, generate sparse signals $x$ with exactly $k$ nonzeros and unit-magnitude coefficients with random signs. For the fixed-$\\mu(A)$ matrix, include the two highly coherent columns $a_1$ and $a_2$ in the support whenever $k \\geq 2$ to probe worst-case behavior under the given $\\mu(A)$; for the RIP-friendly matrix, choose supports uniformly at random without forcing any specific columns.\n- For each $(k, \\mathrm{SNR})$ pair, run multiple independent trials and estimate the success rate. Define success as exact equality of the recovered support to the true support.\n\nTest suite and parameters:\n- Use the following three test cases, each with a fixed mutual coherence target:\n  - Case 1: $m = 120$, $n = 180$, target $\\alpha = 0.45$, $\\mathrm{SNR} \\in \\{5.0, 10.0, 20.0\\}$.\n  - Case 2: $m = 120$, $n = 180$, target $\\alpha = 0.35$, $\\mathrm{SNR} \\in \\{5.0, 10.0, 20.0\\}$.\n  - Case 3: $m = 140$, $n = 160$, target $\\alpha = 0.30$, $\\mathrm{SNR} \\in \\{5.0, 10.0, 20.0\\}$.\n- For each case, scan $k$ over the range $k \\in \\{1, 2, \\dots, \\lfloor 0.12\\, m \\rfloor\\}$.\n- Use $25$ independent trials per $(k, \\mathrm{SNR})$ pair.\n- Use a success-rate threshold of $0.8$ (expressed as a decimal) to define the boundary at each $\\mathrm{SNR}$.\n\nRequired outputs:\n- For each test case, produce three booleans in the order:\n  1. Lower-bound validation: whether the observed boundary at $\\mathrm{SNR}=20.0$ for the fixed-$\\mu(A)$ matrix is at least the predicted $K_{\\mathrm{coh}}$.\n  2. RIP superiority: whether the observed boundary at $\\mathrm{SNR}=20.0$ for the RIP-friendly matrix is greater than or equal to that of the fixed-$\\mu(A)$ matrix.\n  3. SNR monotonicity: whether the boundaries for the fixed-$\\mu(A)$ matrix are nondecreasing as $\\mathrm{SNR}$ increases (from $5.0$ to $10.0$ to $20.0$).\n- Your program should produce a single line of output containing all nine booleans (three per test case) as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True,True,True,False,False,True,True]\").\n\nNo physical units are involved. Angles do not appear. All percentages must be reported as decimals or fractions; the success threshold is given as $0.8$.", "solution": "The user-provided problem is a valid and well-posed computational experiment in the field of compressed sensing. It asks for an a priori theoretical prediction, a numerical simulation to test this prediction, a comparison between different classes of measurement matrices, and an analysis of performance as a function of the signal-to-noise ratio. All parameters and procedures are specified with sufficient clarity to permit a unique and verifiable solution.\n\nThis solution will proceed in three stages. First, we will derive the theoretical coherence-based sparsity limit, $K_{\\mathrm{coh}}$, for noise-free recovery. Second, we will detail the experimental protocol, including the implementation of the Orthogonal Matching Pursuit (OMP) algorithm and the construction of the specified measurement matrices. Finally, we will outline the validation checks to be performed on the simulation results.\n\n### 1. Theoretical Sparsity Limit from Mutual Coherence\n\nThe problem asks for a predicted sparsity limit, $K_{\\mathrm{coh}}$, derived from the mutual coherence $\\mu(A)$ of the measurement matrix $A \\in \\mathbb{R}^{m \\times n}$. The mutual coherence is defined as $\\mu(A) = \\max_{i \\neq j} |\\langle a_i, a_j \\rangle|$, where $a_i$ are the unit-norm columns of $A$. This limit serves as a sufficient condition for an algorithm like OMP to perfectly recover the support of a $k$-sparse signal $x \\in \\mathbb{R}^n$ in the noise-free case, i.e., from measurements $y = Ax$.\n\nOMP is a greedy algorithm. In its first step, it identifies the column of $A$ most correlated with the measurement vector $y$. For successful recovery, this column must belong to the true support of $x$, denoted by $S_0$. Let's analyze this first step. The measurement vector is $y = Ax = \\sum_{j \\in S_0} x_j a_j$.\n\nThe correlation of an arbitrary column $a_i$ with $y$ is $\\langle a_i, y \\rangle$.\nIf $i \\notin S_0$ (an incorrect atom), the correlation is:\n$$c_{incorrect} = \\langle a_i, y \\rangle = \\left\\langle a_i, \\sum_{j \\in S_0} x_j a_j \\right\\rangle = \\sum_{j \\in S_0} x_j \\langle a_i, a_j \\rangle$$\nTaking the absolute value and bounding using the definition of $\\mu(A)$:\n$$|c_{incorrect}| \\le \\sum_{j \\in S_0} |x_j| |\\langle a_i, a_j \\rangle| \\le \\mu(A) \\sum_{j \\in S_0} |x_j| = \\mu(A) \\|x\\|_1$$\n\nIf $i \\in S_0$ (a correct atom), the correlation is:\n$$c_{correct} = \\langle a_i, y \\rangle = \\left\\langle a_i, x_i a_i + \\sum_{j \\in S_0, j \\neq i} x_j a_j \\right\\rangle = x_i \\|a_i\\|_2^2 + \\sum_{j \\in S_0, j \\neq i} x_j \\langle a_i, a_j \\rangle$$\nSince columns are unit-norm, $\\|a_i\\|_2^2=1$. Using the triangle inequality, we can lower-bound its magnitude:\n$$|c_{correct}| \\ge |x_i| - \\left| \\sum_{j \\in S_0, j \\neq i} x_j \\langle a_i, a_j \\rangle \\right| \\ge |x_i| - \\mu(A) \\sum_{j \\in S_0, j \\neq i} |x_j|$$\n\nA sufficient condition for OMP to select a correct atom in the first step is that the correlation with at least one correct atom is strictly greater than the correlation with any incorrect atom. A simpler, more stringent condition guarantees that *any* correct atom is preferred over *any* incorrect atom: $\\min_{i \\in S_0} |c_{correct}|  \\max_{l \\notin S_0} |c_{incorrect}|$. This leads to:\n$$|x_i| - \\mu(A) \\sum_{j \\in S_0, j \\neq i} |x_j|  \\mu(A) \\sum_{j \\in S_0} |x_j|$$\nThis must hold for some $i \\in S_0$. Let's consider the problem's signal model where non-zero entries of $x$ have unit magnitude, i.e., $|x_j| = 1$ for all $j \\in S_0$. In this case, $\\|x\\|_1 = k$ and $|x_i|=1$. The inequality becomes:\n$$1 - \\mu(A) (k-1)  \\mu(A) k$$\n$$1  \\mu(A) k + \\mu(A) (k-1)$$\n$$1  \\mu(A) (2k - 1)$$\n$$1/\\mu(A)  2k - 1$$\n$$1/\\mu(A) + 1  2k$$\n$$k  \\frac{1}{2}\\left(1 + \\frac{1}{\\mu(A)}\\right)$$\nWhile a full proof of OMP recovery requires an inductive argument over all $k$ steps, this condition is the standard sufficient guarantee for several greedy recovery algorithms under certain signal conditions. We therefore define the predicted coherence-based sparsity limit $K_{\\mathrm{coh}}$ as the largest integer $k$ satisfying this strict inequality. For the fixed-$\\mu(A)$ matrix with target coherence $\\alpha$, we use $\\mu(A) \\approx \\alpha$ to compute the prediction.\n\n### 2. Experimental Design and Algorithm\n\nThe simulation aims to find the empirical phase transition boundary for support recovery and validate it against the theoretical prediction and against a benchmark matrix type.\n\n#### Orthogonal Matching Pursuit (OMP)\nThe OMP algorithm is implemented to recover the support of a $k$-sparse signal from measurements $y$. Given a fixed sparsity $k$:\n1.  Initialize the residual $r_0 = y$, the support set $S = \\emptyset$, and the iteration counter $t=1$.\n2.  While $t \\le k$:\n    a.  **Matching step**: Find the index $i_t$ of the column in $A$ that is most correlated with the current residual $r_{t-1}$:\n        $$i_t = \\arg\\max_{j \\notin S} |\\langle a_j, r_{t-1} \\rangle|$$\n    b.  **Support update**: Augment the support set: $S = S \\cup \\{i_t\\}$.\n    c.  **Projection step**: Compute the least-squares estimate of the signal $x_S$ on the current support $S$. Let $A_S$ be the submatrix of $A$ with columns indexed by $S$. The solution is:\n        $$\\hat{x}_S = \\arg\\min_{z} \\|y - A_S z\\|_2^2 = A_S^\\dagger y$$\n        where $A_S^\\dagger = (A_S^T A_S)^{-1} A_S^T$ is the pseudoinverse. In practice, this is solved via a stable linear system solver.\n    d.  **Residual update**: Update the residual by subtracting the contribution of the current estimate:\n        $$r_t = y - A_S \\hat{x}_S$$\n    e.  Increment $t$.\n3.  The final recovered support is $S$.\n\n#### Matrix Construction\n-   **Fixed-$\\mu(A)$ Matrix**: To construct a matrix with a controlled, high coherence between two columns, we proceed as follows for given $m, n, \\alpha$:\n    1.  Choose two orthonormal vectors $u, w \\in \\mathbb{R}^m$. The canonical choice is $u = e_1$ and $w = e_2$.\n    2.  Define the first two columns as $a_1 = u$ and $a_2 = \\alpha u + \\sqrt{1 - \\alpha^2} w$. By construction, $\\|a_1\\|_2 = 1$, $\\|a_2\\|_2 = 1$, and $\\langle a_1, a_2 \\rangle = \\alpha$.\n    3.  The remaining $n-2$ columns must be unit vectors in the orthogonal complement of $\\mathrm{span}\\{u, w\\}$. For $u=e_1, w=e_2$, this subspace is $\\mathrm{span}\\{e_3, \\dots, e_m\\}$. We generate $n-2$ random vectors in $\\mathbb{R}^{m-2}$, normalize them, and embed them into $\\mathbb{R}^m$ by padding with zeros in the first two coordinates. This ensures they are orthogonal to $a_1$ and $a_2$. The mutual coherence of the resulting matrix $A$ will be dominated by $\\alpha$, so $\\mu(A) \\approx \\alpha$.\n\n-   **RIP-friendly (Gaussian) Matrix**: This matrix serves as a benchmark for good empirical performance. It is constructed by generating an $m \\times n$ matrix with entries drawn independently from a standard normal distribution $\\mathcal{N}(0,1)$, and then normalizing each of its $n$ columns to have unit $\\ell_2$-norm.\n\n#### Simulation Protocol\nFor each test case, we scan through sparsity values $k \\in \\{1, \\dots, \\lfloor 0.12m \\rfloor\\}$. For each $(k, \\mathrm{SNR})$ pair, we run $25$ trials:\n1.  **Signal Generation**: A $k$-sparse signal $x$ is created. Its support $S_0$ is chosen based on the matrix type. For the fixed-$\\mu(A)$ matrix, to test the worst-case scenario, if $k \\ge 2$, the support must include the indices of the two highly coherent columns, $\\{0, 1\\}$. For the Gaussian matrix, the support is chosen uniformly at random. The $k$ non-zero coefficients are set to have unit magnitude with random signs.\n2.  **Noise Generation**: The measurement noise $e$ is a vector of $m$ independent samples from a Gaussian distribution $\\mathcal{N}(0, \\sigma^2)$. The variance $\\sigma^2$ is set to satisfy the specified Signal-to-Noise Ratio:\n    $$\\mathrm{SNR} = \\frac{\\|Ax\\|_2^2}{\\mathbb{E}[\\|e\\|_2^2]} = \\frac{\\|Ax\\|_2^2}{m \\sigma^2} \\implies \\sigma^2 = \\frac{\\|Ax\\|_2^2}{m \\cdot \\mathrm{SNR}}$$\n3.  **Recovery and Assessment**: The measurement vector is formed as $y = Ax + e$. OMP is run to obtain an estimated support $\\hat{S}$. A trial is a success if $\\hat{S}$ is identical to the true support $S_0$.\n4.  **Boundary Definition**: The empirical success rate is the fraction of successful trials. The phase transition boundary for a given $\\mathrm{SNR}$ is the largest value of $k$ for which this rate is at least $0.8$.\n\n### 3. Validation Criteria\nFor each test case, we perform three checks based on the computed boundaries:\n\n1.  **Lower-bound Validation**: At the highest $\\mathrm{SNR}$ ($20.0$), the empirically found boundary for the fixed-$\\mu(A)$ matrix, $k^*_{\\mu, 20}$, is compared against the theoretical prediction $K_{\\mathrm{coh}}$. We verify if $k^*_{\\mu, 20} \\ge K_{\\mathrm{coh}}$.\n\n2.  **RIP Superiority**: At the highest $\\mathrm{SNR}$ ($20.0$), the boundary for the Gaussian matrix, $k^*_{G, 20}$, is compared to that of the fixed-$\\mu(A)$ matrix, $k^*_{\\mu, 20}$. We verify if the RIP-friendly matrix yields a better or equal performance, i.e., $k^*_{G, 20} \\ge k^*_{\\mu, 20}$.\n\n3.  **SNR Monotonicity**: For the fixed-$\\mu(A)$ matrix, we check if the recovery boundary is a non-decreasing function of the SNR. Let the boundaries for $\\mathrm{SNR} \\in \\{5.0, 10.0, 20.0\\}$ be $k^*_{\\mu, 5}, k^*_{\\mu, 10}, k^*_{\\mu, 20}$. We verify if $k^*_{\\mu, 5} \\le k^*_{\\mu, 10} \\le k^*_{\\mu, 20}$.\n\nThe final output will consist of nine boolean values corresponding to these three checks for each of the three test cases.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import lstsq\nimport random\n\ndef omp(A, y, k):\n    \"\"\"\n    Orthogonal Matching Pursuit algorithm.\n    :param A: Measurement matrix (m x n)\n    :param y: Measurement vector (m x 1)\n    :param k: Sparsity level\n    :return: A list of indices of the recovered support.\n    \"\"\"\n    m, n = A.shape\n    support = []\n    residual = y.copy()\n\n    for _ in range(k):\n        correlations = np.abs(A.T @ residual)\n        # Mask out already selected columns\n        if support:\n            correlations[support] = -1.0\n        \n        new_idx = np.argmax(correlations)\n        if new_idx in support:\n            # This can happen if all remaining correlations are zero\n            # or if k  rank(A). Stop early.\n            break\n        support.append(new_idx)\n        \n        A_s = A[:, sorted(support)]\n        \n        # Solve least squares problem: A_s @ x_s = y\n        # lstsq is more stable than forming the pseudoinverse directly\n        x_s, _, _, _ = lstsq(A_s, y)\n        \n        residual = y - A_s @ x_s\n    \n    return sorted(support)\n\ndef construct_fixed_mu_matrix(m, n, alpha):\n    \"\"\"Constructs a matrix with two columns having coherence `alpha`.\"\"\"\n    u = np.zeros(m)\n    u[0] = 1.0\n    w = np.zeros(m)\n    w[1] = 1.0\n    \n    a1 = u\n    a2 = alpha * u + np.sqrt(1 - alpha**2) * w\n    \n    A = np.zeros((m, n))\n    A[:, 0] = a1\n    if n  1:\n        A[:, 1] = a2\n    \n    if n  2:\n        rem_cols = np.random.randn(m - 2, n - 2)\n        rem_cols /= np.linalg.norm(rem_cols, axis=0, keepdims=True)\n        A[2:, 2:] = rem_cols\n        \n    return A\n\ndef construct_gaussian_matrix(m, n):\n    \"\"\"Constructs a matrix with normalized i.i.d. Gaussian columns.\"\"\"\n    A = np.random.randn(m, n)\n    A /= np.linalg.norm(A, axis=0, keepdims=True)\n    return A\n\ndef find_boundary(m, n, k_range, snr, matrix_type, alpha):\n    \"\"\"Finds the maximum k for which recovery success rate is = threshold.\"\"\"\n    num_trials = 25\n    success_threshold = 0.8\n\n    if matrix_type == 'fixed_mu':\n        A = construct_fixed_mu_matrix(m, n, alpha)\n    else: # 'gaussian'\n        A = construct_gaussian_matrix(m, n)\n\n    # Scan k from high to low to find the boundary efficiently\n    for k in reversed(k_range):\n        success_count = 0\n        for _ in range(num_trials):\n            # Generate sparse signal x\n            x = np.zeros(n)\n            if matrix_type == 'fixed_mu':\n                if k == 1:\n                    # To be robust, pick randomly from the first two\n                    true_support_list = random.sample(range(2), 1)\n                else:\n                    # Force the two coherent columns into the support\n                    support_set = {0, 1}\n                    while len(support_set)  k:\n                        support_set.add(random.randint(2, n - 1))\n                    true_support_list = sorted(list(support_set))\n            else: # 'gaussian'\n                true_support_list = sorted(random.sample(range(n), k))\n            \n            x[true_support_list] = np.random.choice([-1.0, 1.0], size=k)\n\n            # Generate measurements y = Ax + e\n            Ax = A @ x\n            norm_Ax_sq = np.sum(Ax**2)\n            \n            # Add noise based on SNR\n            if snr  0:\n                noise_var = norm_Ax_sq / (m * snr)\n                sigma = np.sqrt(noise_var)\n                e = np.random.normal(0, sigma, size=m)\n                y = Ax + e\n            else: # Noise-free case\n                y = Ax\n            \n            # Recover support with OMP\n            recovered_support = omp(A, y, k)\n            \n            # Check for success\n            if recovered_support == true_support_list:\n                success_count += 1\n        \n        if success_count / num_trials = success_threshold:\n            return k # This is the boundary\n            \n    return 0 # No k satisfied the condition\n\ndef solve():\n    \"\"\"Main function to run the simulation and validation.\"\"\"\n    test_cases = [\n        # (m, n, alpha, SNRs)\n        (120, 180, 0.45, [5.0, 10.0, 20.0]),\n        (120, 180, 0.35, [5.0, 10.0, 20.0]),\n        (140, 160, 0.30, [5.0, 10.0, 20.0]),\n    ]\n    np.random.seed(42) # For reproducibility\n    random.seed(42)\n\n    final_results = []\n    \n    for m, n, alpha, snrs in test_cases:\n        # Theoretical prediction\n        K_coh = int(np.floor(0.5 * (1.0 / alpha + 1.0) - 1e-9))\n        \n        k_max = int(np.floor(0.12 * m))\n        k_range = list(range(1, k_max + 1))\n        \n        # Run experiments for fixed-mu matrix\n        boundaries_mu = []\n        for snr in snrs:\n            boundary = find_boundary(m, n, k_range, snr, 'fixed_mu', alpha)\n            boundaries_mu.append(boundary)\n            \n        # Run experiment for Gaussian matrix at highest SNR\n        highest_snr = snrs[-1]\n        boundary_G_high_snr = find_boundary(m, n, k_range, highest_snr, 'gaussian', alpha)\n\n        # 1. Lower-bound validation\n        boundary_mu_high_snr = boundaries_mu[-1]\n        val1 = boundary_mu_high_snr = K_coh\n        \n        # 2. RIP superiority\n        val2 = boundary_G_high_snr = boundary_mu_high_snr\n        \n        # 3. SNR monotonicity\n        val3 = (boundaries_mu[0] = boundaries_mu[1]) and (boundaries_mu[1] = boundaries_mu[2])\n        \n        final_results.extend([val1, val2, val3])\n\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n\n```", "id": "3462348"}]}