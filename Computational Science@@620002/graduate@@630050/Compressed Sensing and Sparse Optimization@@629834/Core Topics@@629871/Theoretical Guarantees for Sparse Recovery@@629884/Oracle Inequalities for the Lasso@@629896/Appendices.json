{"hands_on_practices": [{"introduction": "The theoretical power of the LASSO is best understood by deriving its performance guarantees from first principles. This exercise guides you through the proof of a foundational oracle inequality in the idealized setting of an orthonormal design. By working through the steps, from bounding the noise to analyzing the estimator's structure, you will derive explicit bounds on both estimation error and the conditions for correct variable selection, providing a solid grasp of why and how the LASSO succeeds. [@problem_id:3464166]", "problem": "Consider the fixed-design linear model $y = X \\beta^{\\star} + w$ with $w \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$, where $X \\in \\mathbb{R}^{n \\times p}$ has columns normalized to orthogonality in the sense that $X^{\\top} X = n I_{p}$. Let $S = \\operatorname{supp}(\\beta^{\\star})$ with $|S| = s$, and define $\\beta_{\\min} = \\min_{j \\in S} |\\beta^{\\star}_{j}|$. The Least Absolute Shrinkage and Selection Operator (LASSO) estimator $\\hat{\\beta}$ is defined as any minimizer of the convex program\n\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2n} \\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\n\nwith a tuning parameter $\\lambda > 0$. Let $\\delta \\in (0, 1)$ be a prescribed error probability and assume no further structure beyond the stated model and design.\n\nYour tasks are to start from the definition of the estimator and basic probability tail bounds for Gaussian random variables, and then carry out the following steps:\n\n(1) Define $g = \\frac{1}{n} X^{\\top} w$ and use standard Gaussian tail bounds and a union bound to determine the smallest value $\\tau = \\tau(n, p, \\delta, \\sigma)$ such that the event $\\mathcal{E} = \\{\\|g\\|_{\\infty} \\le \\tau\\}$ holds with probability at least $1 - \\delta$, expressed in closed form in terms of $n$, $p$, $\\delta$, and $\\sigma$.\n\n(2) By analyzing the Karush–Kuhn–Tucker (KKT) conditions for the LASSO under the orthogonality condition $X^{\\top} X = n I_{p}$, show that on the event $\\mathcal{E}$ and for any choice of $\\lambda \\ge \\tau$, the LASSO estimator reduces coordinate-wise to a soft-thresholding operation with threshold $\\lambda$. Using only this reduction, derive:\n- An $\\ell_{2}$-estimation inequality of the form $\\|\\hat{\\beta} - \\beta^{\\star}\\|_{2} \\le C \\sqrt{s} \\lambda$ and identify the smallest numerical constant $C$ that you can certify using the basic inequality and Cauchy–Schwarz.\n- A sufficient signal strength condition for exact signed support recovery, expressed as a lower bound on $\\beta_{\\min}$ in terms of $\\lambda$ and $\\tau$, that ensures $\\operatorname{supp}(\\hat{\\beta}) = S$ and $\\operatorname{sign}(\\hat{\\beta}_{S}) = \\operatorname{sign}(\\beta^{\\star}_{S})$ on the event $\\mathcal{E}$.\n\n(3) Optimize the sufficient condition from part (2) over the choice of $\\lambda \\ge \\tau$ to obtain the smallest explicit lower bound on $\\beta_{\\min}$ that guarantees exact signed support recovery with probability at least $1 - \\delta$. Express your final answer for this minimal $\\beta_{\\min}$ as a single closed-form analytic expression in $n$, $p$, $\\delta$, and $\\sigma$.\n\nReport only the expression for the minimal $\\beta_{\\min}$ as your final answer. No numerical approximation or rounding is required, and no units are involved.", "solution": "The user has provided a valid problem statement from the field of high-dimensional statistics. The problem is scientifically grounded, well-posed, and objective. We can proceed with a full derivation.\n\nThe solution is organized according to the three tasks specified in the problem statement.\n\n**(1) Derivation of the noise-level threshold $\\tau$**\n\nThe problem defines the vector $g = \\frac{1}{n} X^{\\top} w$, where $w \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$. Since $g$ is a linear transformation of a Gaussian random vector, it is also Gaussian. We first determine its mean and covariance.\nThe mean is $E[g] = \\frac{1}{n} X^{\\top} E[w] = \\frac{1}{n} X^{\\top} 0 = 0$.\nThe covariance matrix is:\n$$\n\\operatorname{Cov}(g) = E[g g^{\\top}] = E\\left[ \\left(\\frac{1}{n} X^{\\top} w\\right) \\left(\\frac{1}{n} X^{\\top} w\\right)^{\\top} \\right] = \\frac{1}{n^2} X^{\\top} E[w w^{\\top}] X\n$$\nSince $w \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$, its covariance is $E[w w^{\\top}] = \\sigma^2 I_n$. Substituting this and the given design condition $X^{\\top} X = n I_{p}$:\n$$\n\\operatorname{Cov}(g) = \\frac{1}{n^2} X^{\\top} (\\sigma^2 I_n) X = \\frac{\\sigma^2}{n^2} (X^{\\top} X) = \\frac{\\sigma^2}{n^2} (n I_p) = \\frac{\\sigma^2}{n} I_p.\n$$\nThis result implies that the components $g_j$ for $j=1, \\dots, p$ are independent and identically distributed, with $g_j \\sim \\mathcal{N}(0, \\sigma^2/n)$.\n\nWe are interested in the event $\\mathcal{E} = \\{\\|g\\|_{\\infty} \\le \\tau\\}$, where $\\|g\\|_{\\infty} = \\max_{j=1,\\dots,p} |g_j|$. We want to find the smallest $\\tau$ such that $P(\\mathcal{E}) \\ge 1 - \\delta$. This is equivalent to ensuring the probability of the complement event, $\\mathcal{E}^c = \\{\\|g\\|_{\\infty} > \\tau\\}$, is at most $\\delta$.\nThe complement event can be written as a union: $\\mathcal{E}^c = \\bigcup_{j=1}^{p} \\{|g_j| > \\tau\\}$.\nUsing the union bound, we have:\n$$\nP(\\mathcal{E}^c) = P\\left(\\bigcup_{j=1}^{p} \\{|g_j| > \\tau\\}\\right) \\le \\sum_{j=1}^{p} P(|g_j| > \\tau) = p \\cdot P(|g_1| > \\tau),\n$$\nwhere the last equality holds because the $g_j$ are i.i.d.\nLet $Z$ be a standard normal random variable, $Z \\sim \\mathcal{N}(0, 1)$. Then we can write $g_1 = \\frac{\\sigma}{\\sqrt{n}} Z$. The probability becomes:\n$$\nP(|g_1| > \\tau) = P\\left(\\left|\\frac{\\sigma}{\\sqrt{n}} Z\\right| > \\tau\\right) = P\\left(|Z| > \\frac{\\sqrt{n}\\tau}{\\sigma}\\right).\n$$\nUsing the standard Gaussian tail bound $P(|Z| > t) \\le 2 \\exp(-t^2/2)$ for $t > 0$, we get:\n$$\nP(\\mathcal{E}^c) \\le p \\cdot 2 \\exp\\left(-\\frac{1}{2} \\left(\\frac{\\sqrt{n}\\tau}{\\sigma}\\right)^2\\right) = 2p \\exp\\left(-\\frac{n\\tau^2}{2\\sigma^2}\\right).\n$$\nTo ensure $P(\\mathcal{E}^c) \\le \\delta$, we can set the bound equal to $\\delta$ and solve for $\\tau$:\n$$\n2p \\exp\\left(-\\frac{n\\tau^2}{2\\sigma^2}\\right) = \\delta\n$$\n$$\n\\exp\\left(-\\frac{n\\tau^2}{2\\sigma^2}\\right) = \\frac{\\delta}{2p}\n$$\n$$\n-\\frac{n\\tau^2}{2\\sigma^2} = \\ln\\left(\\frac{\\delta}{2p}\\right) = -\\ln\\left(\\frac{2p}{\\delta}\\right)\n$$\n$$\n\\tau^2 = \\frac{2\\sigma^2}{n} \\ln\\left(\\frac{2p}{\\delta}\\right)\n$$\nThe smallest value of $\\tau$ that satisfies the inequality is:\n$$\n\\tau = \\sigma \\sqrt{\\frac{2 \\ln(2p/\\delta)}{n}}.\n$$\n\n**(2) KKT Conditions, Soft-Thresholding Reduction, and Consequences**\n\nThe LASSO objective function is $L(\\beta) = \\frac{1}{2n} \\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$. The Karush–Kuhn–Tucker (KKT) optimality conditions state that a vector $\\hat{\\beta}$ is a minimizer if and only if the zero vector is in the subgradient of $L$ at $\\hat{\\beta}$:\n$$\n0 \\in \\nabla_{\\beta} \\left(\\frac{1}{2n} \\|y - X \\hat{\\beta}\\|_{2}^{2}\\right) + \\lambda \\partial\\|\\hat{\\beta}\\|_{1}.\n$$\nThe gradient of the quadratic term is $\\frac{1}{n} X^{\\top}(X\\hat{\\beta} - y)$. Substituting $y = X\\beta^{\\star} + w$:\n$$\n0 \\in \\frac{1}{n} X^{\\top}(X\\hat{\\beta} - X\\beta^{\\star} - w) + \\lambda \\partial\\|\\hat{\\beta}\\|_{1}\n$$\n$$\n\\frac{1}{n} X^{\\top}w \\in \\frac{1}{n} X^{\\top}X(\\hat{\\beta} - \\beta^{\\star}) + \\lambda \\partial\\|\\hat{\\beta}\\|_{1}.\n$$\nUsing the definitions $g = \\frac{1}{n} X^{\\top}w$ and the orthogonality condition $X^{\\top}X = n I_p$, this simplifies to:\n$$\ng \\in (\\hat{\\beta} - \\beta^{\\star}) + \\lambda \\partial\\|\\hat{\\beta}\\|_{1} \\quad \\implies \\quad \\beta^{\\star} + g \\in \\hat{\\beta} + \\lambda \\partial\\|\\hat{\\beta}\\|_{1}.\n$$\nThis is the KKT condition for the problem $\\min_{\\beta} \\frac{1}{2} \\|\\beta - (\\beta^{\\star} + g)\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$, whose unique solution is given by the component-wise soft-thresholding operator $S_{\\lambda}(\\cdot)$. Thus, for each coordinate $j=1, \\dots, p$:\n$$\n\\hat{\\beta}_j = S_{\\lambda}(\\beta^{\\star}_j + g_j) \\equiv \\operatorname{sign}(\\beta^{\\star}_j + g_j) \\max(0, |\\beta^{\\star}_j + g_j| - \\lambda).\n$$\nThis shows that under the orthogonal design, the LASSO estimator reduces to a simple soft-thresholding of the ordinary least-squares estimator $\\beta^{\\star}+g$ (since here $(X^\\top X)^{-1}X^\\top y = \\frac{1}{n}X^\\top(X\\beta^\\star+w) = \\beta^\\star+g$). This reduction holds for any $\\lambda > 0$.\n\n**$\\ell_2$-estimation inequality:**\nWe analyze the estimation error $\\Delta = \\hat{\\beta} - \\beta^{\\star}$ on the event $\\mathcal{E}$ and for $\\lambda \\ge \\tau$.\nThe error in each coordinate is $\\Delta_j = \\hat{\\beta}_j - \\beta^{\\star}_j = S_{\\lambda}(\\beta^{\\star}_j + g_j) - \\beta^{\\star}_j$.\nFor $j \\in S^c$ (the non-support set), $\\beta^{\\star}_j = 0$. On event $\\mathcal{E}$, we have $|g_j| \\le \\|g\\|_{\\infty} \\le \\tau$. Since we chose $\\lambda \\ge \\tau$, we have $|g_j| \\le \\lambda$. The soft-thresholding operator gives $\\hat{\\beta}_j = S_{\\lambda}(g_j) = 0$. Thus, $\\Delta_j = 0 - 0 = 0$ for all $j \\in S^c$.\nThe total squared error is therefore $\\|\\hat{\\beta} - \\beta^{\\star}\\|_{2}^2 = \\sum_{j \\in S} (\\hat{\\beta}_j - \\beta^{\\star}_j)^2$.\nFor any $z$, it is a known property that $|S_{\\lambda}(z) - z| \\le \\lambda$. Let $u_j = \\beta^{\\star}_j + g_j$. Then $\\hat{\\beta}_j = S_{\\lambda}(u_j)$, and we can write the error as $\\Delta_j = S_{\\lambda}(u_j) - (u_j - g_j) = (S_{\\lambda}(u_j) - u_j) + g_j$.\nBy the triangle inequality, $|\\Delta_j| \\le |S_{\\lambda}(u_j) - u_j| + |g_j|$.\nUsing $|S_{\\lambda}(u_j) - u_j| \\le \\lambda$ and $|g_j| \\le \\tau$ on event $\\mathcal{E}$, we have:\n$|\\Delta_j| \\le \\lambda + \\tau$.\nWith the condition $\\lambda \\ge \\tau$, this implies $|\\Delta_j| \\le \\lambda + \\lambda = 2\\lambda$.\nSumming the squares over the support set $S$:\n$$\n\\|\\hat{\\beta} - \\beta^{\\star}\\|_{2}^2 = \\sum_{j \\in S} \\Delta_j^2 \\le \\sum_{j \\in S} (2\\lambda)^2 = s(4\\lambda^2).\n$$\nTaking the square root gives the $\\ell_2$-estimation inequality:\n$$\n\\|\\hat{\\beta} - \\beta^{\\star}\\|_{2} \\le 2\\sqrt{s}\\lambda.\n$$\nFrom this derivation, the smallest constant we can certify is $C=2$.\n\n**Sufficient condition for exact signed support recovery:**\nWe require two conditions to hold:\n(a) For all $j \\in S^c$, $\\hat{\\beta}_j = 0$.\n(b) For all $j \\in S$, $\\operatorname{sign}(\\hat{\\beta}_j) = \\operatorname{sign}(\\beta^{\\star}_j)$.\n\nCondition (a): For $j \\in S^c$, $\\beta^{\\star}_j = 0$, so $\\hat{\\beta}_j = S_{\\lambda}(g_j)$. We need $S_{\\lambda}(g_j) = 0$, which holds if and only if $|g_j| \\le \\lambda$. On the event $\\mathcal{E}$, we have $|g_j| \\le \\tau$. Thus, choosing any $\\lambda \\ge \\tau$ is a sufficient condition for (a) to hold on $\\mathcal{E}$.\n\nCondition (b): For $j \\in S$, $\\beta^{\\star}_j \\ne 0$. We need $\\operatorname{sign}(S_{\\lambda}(\\beta^{\\star}_j + g_j)) = \\operatorname{sign}(\\beta^{\\star}_j)$. This holds if $\\operatorname{sign}(\\beta^{\\star}_j + g_j) = \\operatorname{sign}(\\beta^{\\star}_j)$ and $|\\beta^{\\star}_j + g_j| > \\lambda$.\nA sufficient condition for both is $|\\beta^{\\star}_j| - |g_j| > \\lambda$. This is because if this inequality holds, then $|g_j| < |\\beta^{\\star}_j|$, which ensures $\\beta^{\\star}_j + g_j$ has the same sign as $\\beta^{\\star}_j$. Also, by the reverse triangle inequality, $|\\beta^{\\star}_j + g_j| \\ge |\\beta^{\\star}_j| - |g_j| > \\lambda$.\nTo ensure this holds for all $j \\in S$ on the event $\\mathcal{E}$, we must guard against the worst-case configuration of noise, where $|g_j|$ is maximal, i.e., $|g_j| = \\tau$. The condition becomes $|\\beta^{\\star}_j| - \\tau > \\lambda$, or $|\\beta^{\\star}_j| > \\lambda + \\tau$.\nThis must hold for all $j \\in S$. The weakest such condition applies to the smallest coefficient on the support: $\\min_{j \\in S} |\\beta^{\\star}_j| > \\lambda + \\tau$.\nThis is precisely $\\beta_{\\min} > \\lambda + \\tau$.\n\nIn summary, on the event $\\mathcal{E}$, the conditions $\\lambda \\ge \\tau$ and $\\beta_{\\min} > \\lambda + \\tau$ are sufficient for exact signed support recovery.\n\n**(3) Minimal Signal Strength for Support Recovery**\n\nWe seek the smallest lower bound on $\\beta_{\\min}$ that guarantees exact signed support recovery with probability at least $1-\\delta$. This occurs on the event $\\mathcal{E}$. The sufficient condition derived is $\\beta_{\\min} > \\lambda + \\tau$, and it requires us to choose a tuning parameter $\\lambda$ such that $\\lambda \\ge \\tau$.\nWe are free to choose $\\lambda$. To obtain the least restrictive condition on $\\beta_{\\min}$ (i.e., the smallest lower bound), we should minimize the quantity $\\lambda+\\tau$ with respect to $\\lambda$, subject to the constraint $\\lambda \\ge \\tau$.\nThe function $f(\\lambda) = \\lambda + \\tau$ is monotonically increasing in $\\lambda$. Therefore, its minimum over the domain $[\\tau, \\infty)$ is achieved at the lower boundary, $\\lambda = \\tau$.\nSubstituting $\\lambda = \\tau$ into the condition on $\\beta_{\\min}$, we obtain the optimized sufficient condition:\n$$\n\\beta_{\\min} > \\tau + \\tau = 2\\tau.\n$$\nThe smallest explicit lower bound on $\\beta_{\\min}$ is thus $2\\tau$. Substituting the expression for $\\tau$ from part (1) gives the final answer:\n$$\n\\beta_{\\min} > 2 \\sigma \\sqrt{\\frac{2 \\ln(2p/\\delta)}{n}}.\n$$\nThe minimal value for this lower bound is the expression on the right-hand side.", "answer": "$$\n\\boxed{2\\sigma \\sqrt{\\frac{2 \\ln\\left(\\frac{2p}{\\delta}\\right)}{n}}}\n$$", "id": "3464166"}, {"introduction": "While oracle inequalities are simplest to derive under orthogonal designs, real-world data often involves correlated predictors. This practice introduces the compatibility constant, a crucial geometric quantity that appears in more general oracle inequalities and quantifies how correlations in the design matrix affect the LASSO's performance. Calculating this constant for a simple, non-trivial correlated design builds concrete intuition for the theoretical tools required to analyze the LASSO in more realistic settings. [@problem_id:3464165]", "problem": "Consider the linear model $y = X \\beta^{\\star} + \\varepsilon$ with design matrix $X \\in \\mathbb{R}^{n \\times p}$, where the columns of $X$ are standardized so that $\\frac{1}{n} X^{\\top} X = \\Sigma$. In the study of the Least Absolute Shrinkage and Selection Operator (LASSO), oracle inequalities involve geometry encoded by a support set $S \\subseteq \\{1, \\dots, p\\}$ and a cone constraint. Let $p = 3$ and suppose the empirical Gram matrix satisfies\n$$\n\\Sigma = \\begin{pmatrix}\n1 & \\rho & 0 \\\\\n\\rho & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix},\n$$\nwith correlation parameter $0 \\leq \\rho < 1$, and consider the support set $S = \\{1, 2\\}$. Define the cone $C(S)$ by the inequality $\\|v_{S^{c}}\\|_{1} \\leq L \\|v_{S}\\|_{1}$ with $L = 1$, where $v \\in \\mathbb{R}^{p}$ and $S^{c}$ denotes the complement of $S$. The compatibility constant associated with $S$ and the cone $C(S)$ is the value\n$$\n\\phi_{\\mathrm{comp}}(S) := \\inf_{v \\in C(S) \\setminus \\{0\\}} \\frac{\\sqrt{|S|} \\, \\sqrt{v^{\\top} \\Sigma v}}{\\|v_{S}\\|_{1}}.\n$$\nDetermine $\\phi_{\\mathrm{comp}}(S)$ as an exact closed-form expression in terms of $\\rho$. Express your final answer as a single simplified analytic expression.", "solution": "The problem asks for the compatibility constant $\\phi_{\\mathrm{comp}}(S)$ for a given support set $S$, Gram matrix $\\Sigma$, and cone constraint.\n\nThe compatibility constant is defined as:\n$$\n\\phi_{\\mathrm{comp}}(S) := \\inf_{v \\in C(S) \\setminus \\{0\\}} \\frac{\\sqrt{|S|} \\, \\sqrt{v^{\\top} \\Sigma v}}{\\|v_{S}\\|_{1}}\n$$\nThe givens are:\n- Dimension $p=3$.\n- Support set $S = \\{1, 2\\}$, so its cardinality is $|S|=2$. The complement is $S^c = \\{3\\}$.\n- For a vector $v = (v_1, v_2, v_3)^{\\top} \\in \\mathbb{R}^3$, the subvector on the support set is $v_S = (v_1, v_2)^{\\top}$ and on its complement is $v_{S^c} = (v_3)$.\n- The relevant norms are the $\\ell_1$-norms: $\\|v_S\\|_1 = |v_1| + |v_2|$ and $\\|v_{S^c}\\|_1 = |v_3|$.\n- The cone $C(S)$ is defined by the inequality $\\|v_{S^c}\\|_1 \\leq L \\|v_S\\|_1$ with $L=1$, which simplifies to $|v_3| \\leq |v_1| + |v_2|$.\n- The empirical Gram matrix is $\\Sigma = \\begin{pmatrix} 1 & \\rho & 0 \\\\ \\rho & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$ with $0 \\leq \\rho < 1$.\n\nLet's expand the quadratic form $v^{\\top} \\Sigma v$:\n$$\nv^{\\top} \\Sigma v = \\begin{pmatrix} v_1 & v_2 & v_3 \\end{pmatrix} \\begin{pmatrix} 1 & \\rho & 0 \\\\ \\rho & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{pmatrix} = v_1^2 + v_2^2 + 2\\rho v_1 v_2 + v_3^2\n$$\nSubstituting the known values into the definition of the compatibility constant, we get:\n$$\n\\phi_{\\mathrm{comp}}(S) = \\inf_{v \\in C(S) \\setminus \\{0\\}} \\frac{\\sqrt{2} \\sqrt{v_1^2 + v_2^2 + 2\\rho v_1 v_2 + v_3^2}}{|v_1| + |v_2|}\n$$\nThe expression to be minimized is homogeneous of degree zero with respect to $v$. That is, for any scalar $c \\neq 0$, if we replace $v$ with $cv$, the value of the expression remains unchanged. This allows us to simplify the problem by imposing a normalization constraint on $v$. A convenient choice is to constrain the search to vectors $v$ that satisfy $\\|v_S\\|_1 = |v_1| + |v_2| = 1$. The infimum over $v \\in C(S) \\setminus \\{0\\}$ is the same as the infimum over the subset where $\\|v_S\\|_1 = 1$.\n\nWith this constraint, the problem is to find $\\phi_{\\mathrm{comp}}(S) = \\inf \\sqrt{2(v_1^2 + v_2^2 + 2\\rho v_1 v_2 + v_3^2)}$ subject to:\n1. $|v_1| + |v_2| = 1$\n2. $|v_3| \\leq |v_1| + |v_2|$, which becomes $|v_3| \\leq 1$.\n\nFinding the infimum of the function is equivalent to finding the infimum of its square. Let's find the minimum of $\\phi_{\\mathrm{comp}}(S)^2$:\n$$\n\\phi_{\\mathrm{comp}}(S)^2 = \\inf \\left\\{ 2(v_1^2 + v_2^2 + 2\\rho v_1 v_2 + v_3^2) \\right\\}\n$$\nsubject to the constraints $|v_1|+|v_2|=1$ and $|v_3| \\leq 1$.\n\nTo minimize this expression, we should choose $v_3$ to make the term $v_3^2$ as small as possible. Since $v_3^2 \\ge 0$, its minimum value is $0$, which is achieved when $v_3=0$. This choice satisfies the constraint $|v_3| \\leq 1$.\nThe problem thus reduces to a lower-dimensional optimization:\n$$\n\\phi_{\\mathrm{comp}}(S)^2 = \\inf_{|v_1|+|v_2|=1} \\left\\{ 2(v_1^2 + v_2^2 + 2\\rho v_1 v_2) \\right\\}\n$$\nLet's analyze the quadratic form $f(v_1, v_2) = v_1^2 + v_2^2 + 2\\rho v_1 v_2$. This can be written in matrix form as $v_S^{\\top} \\Sigma_S v_S$, where $\\Sigma_S$ is the principal submatrix of $\\Sigma$ corresponding to the indices in $S$:\n$$\n\\Sigma_S = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}\n$$\nThe constraint $|v_1| + |v_2| = 1$ defines a square in the $(v_1,v_2)$-plane with vertices at $(1,0), (0,1), (-1,0), (0,-1)$.\n\nTo find the minimum of $v_S^{\\top} \\Sigma_S v_S$ on this square, we can diagonalize $\\Sigma_S$. The eigenvalues $\\lambda$ of $\\Sigma_S$ are given by the characteristic equation $(1-\\lambda)^2 - \\rho^2 = 0$, which yields $1-\\lambda = \\pm\\rho$, so $\\lambda_1 = 1+\\rho$ and $\\lambda_2 = 1-\\rho$. The corresponding normalized eigenvectors are:\n- For $\\lambda_1 = 1+\\rho$: $e_1 = \\frac{1}{\\sqrt{2}}(1, 1)^{\\top}$\n- For $\\lambda_2 = 1-\\rho$: $e_2 = \\frac{1}{\\sqrt{2}}(1, -1)^{\\top}$\n\nAny vector $v_S = (v_1, v_2)^{\\top}$ can be expressed in this eigenbasis as $v_S = c_1 e_1 + c_2 e_2$. The quadratic form becomes:\n$$\nv_S^{\\top} \\Sigma_S v_S = (c_1 e_1 + c_2 e_2)^{\\top} \\Sigma_S (c_1 e_1 + c_2 e_2) = c_1^2 \\lambda_1 + c_2^2 \\lambda_2 = c_1^2 (1+\\rho) + c_2^2 (1-\\rho)\n$$\nWe need to express the constraint $\\|v_S\\|_1 = 1$ in terms of $c_1$ and $c_2$.\nFrom $v_S = c_1 e_1 + c_2 e_2$, we have:\n$v_1 = \\frac{c_1}{\\sqrt{2}} + \\frac{c_2}{\\sqrt{2}}$ and $v_2 = \\frac{c_1}{\\sqrt{2}} - \\frac{c_2}{\\sqrt{2}}$.\nThe constraint becomes:\n$$\n|\\frac{c_1}{\\sqrt{2}} + \\frac{c_2}{\\sqrt{2}}| + |\\frac{c_1}{\\sqrt{2}} - \\frac{c_2}{\\sqrt{2}}| = 1\n$$\nUsing the identity $|a+b| + |a-b| = 2\\max(|a|,|b|)$, with $a=c_1/\\sqrt{2}$ and $b=c_2/\\sqrt{2}$, we get:\n$$\n\\frac{1}{\\sqrt{2}} ( |c_1+c_2| + |c_1-c_2| ) = \\frac{2}{\\sqrt{2}} \\max(|c_1|, |c_2|) = \\sqrt{2} \\max(|c_1|, |c_2|) = 1\n$$\nSo, the constraint on $c_1, c_2$ is $\\max(|c_1|, |c_2|) = \\frac{1}{\\sqrt{2}}$. This describes a square in the $(c_1, c_2)$-plane.\n\nOur problem is now to minimize $g(c_1, c_2) = c_1^2(1+\\rho) + c_2^2(1-\\rho)$ subject to $\\max(|c_1|, |c_2|) = \\frac{1}{\\sqrt{2}}$.\nSince we are given $0 \\leq \\rho < 1$, the coefficients $(1+\\rho)$ and $(1-\\rho)$ are both positive. Furthermore, $(1+\\rho) \\geq (1-\\rho)$.\nTo minimize $g(c_1, c_2)$, we should make the term with the larger coefficient, $c_1^2(1+\\rho)$, as small as possible, and the term with the smaller coefficient, $c_2^2(1-\\rho)$, potentially larger.\nThe minimum value for $|c_1|$ on the constraint set is $0$. This forces $|c_2|$ to be $\\frac{1}{\\sqrt{2}}$ to satisfy $\\max(|c_1|, |c_2|) = \\frac{1}{\\sqrt{2}}$.\nTherefore, the minimum is achieved at $(c_1, c_2) = (0, \\pm \\frac{1}{\\sqrt{2}})$.\n\nThe minimum value of the quadratic form $v_S^{\\top} \\Sigma_S v_S$ is:\n$$\n\\min (v_S^{\\top} \\Sigma_S v_S) = 0^2(1+\\rho) + \\left(\\pm \\frac{1}{\\sqrt{2}}\\right)^2 (1-\\rho) = \\frac{1}{2}(1-\\rho)\n$$\nNow, we substitute this back into the expression for $\\phi_{\\mathrm{comp}}(S)^2$:\n$$\n\\phi_{\\mathrm{comp}}(S)^2 = 2 \\times \\min_{|v_1|+|v_2|=1} (v_1^2 + v_2^2 + 2\\rho v_1 v_2) = 2 \\times \\frac{1-\\rho}{2} = 1-\\rho\n$$\nTaking the square root gives the compatibility constant:\n$$\n\\phi_{\\mathrm{comp}}(S) = \\sqrt{1-\\rho}\n$$\nThis expression is well-defined as $1-\\rho > 0$ for the given range of $\\rho$.", "answer": "$$\n\\boxed{\\sqrt{1-\\rho}}\n$$", "id": "3464165"}, {"introduction": "To deepen our understanding of the LASSO, it is valuable to compare it with other prominent sparse estimation methods. This exercise draws a direct comparison between the LASSO and the Dantzig selector, another key technique in high-dimensional statistics. By deriving the closed-form solutions for both estimators under an orthonormal design, you will discover that they both reduce to the same fundamental soft-thresholding operation, revealing a deep connection between these two different optimization frameworks. [@problem_id:3435559]", "problem": "Consider the linear model $y = X \\beta^{\\star} + w$ with $X \\in \\mathbb{R}^{n \\times p}$ whose columns are orthonormal, meaning $X^{\\top} X = I_{p}$. You are comparing two convex estimators of $\\beta^{\\star}$ in the compressed sensing and sparse optimization setting: the Least Absolute Shrinkage and Selection Operator (LASSO) estimator and the Dantzig selector. The LASSO estimator $\\hat{\\beta}_{\\mathrm{LASSO}}$ is defined as the minimizer of the objective\n$$\\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},$$\nwhere $\\lambda > 0$ is a regularization parameter. The Dantzig selector $\\hat{\\beta}_{\\mathrm{DS}}$ is defined as the minimizer of $\\|\\beta\\|_{1}$ subject to the feasibility constraint\n$$\\|X^{\\top}(y - X \\beta)\\|_{\\infty} \\leq t,$$\nwhere $t > 0$ is a tuning parameter.\n\nStarting from the fundamental base of convex optimality conditions for the LASSO and feasibility geometry for the Dantzig selector, derive the closed-form expressions of $\\hat{\\beta}_{\\mathrm{LASSO}}$ and $\\hat{\\beta}_{\\mathrm{DS}}$ under the orthonormal design condition $X^{\\top} X = I_{p}$. Then, for the specific instance $p = 5$ with the sufficient statistics $X^{\\top} y = z$ given by\n$$z = \\begin{pmatrix} 2.4 \\\\ -0.3 \\\\ 1.1 \\\\ -1.8 \\\\ 0.0 \\end{pmatrix},$$\nand parameters $\\lambda = 0.9$ and $t = 0.7$, compute the squared $\\ell_{2}$ distance between the two estimators,\n$$\\|\\hat{\\beta}_{\\mathrm{LASSO}} - \\hat{\\beta}_{\\mathrm{DS}}\\|_{2}^{2}.$$\n\nRound your final numerical answer to four significant figures. Express your final answer with no units.", "solution": "The problem asks for the squared $\\ell_2$ distance between the LASSO and Dantzig selector estimators under the specific condition of an orthonormal design matrix. We begin by deriving the closed-form solutions for both estimators under this condition.\n\nLet the sufficient statistic be defined as $z = X^{\\top}y$. The orthonormal design condition is $X^{\\top}X = I_p$, where $I_p$ is the $p \\times p$ identity matrix.\n\nFirst, we analyze the LASSO estimator, $\\hat{\\beta}_{\\mathrm{LASSO}}$, which is the solution to the minimization problem:\n$$ \\hat{\\beta}_{\\mathrm{LASSO}} = \\arg \\min_{\\beta \\in \\mathbb{R}^p} \\left( \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} \\right) $$\nThe quadratic term in the objective function can be expanded:\n$$ \\|y - X \\beta\\|_{2}^{2} = (y - X \\beta)^{\\top}(y - X \\beta) = y^{\\top}y - 2y^{\\top}X\\beta + \\beta^{\\top}X^{\\top}X\\beta $$\nUsing the condition $X^{\\top}X = I_p$ and the definition $z = X^{\\top}y$, this simplifies to:\n$$ \\|y - X \\beta\\|_{2}^{2} = y^{\\top}y - 2z^{\\top}\\beta + \\beta^{\\top}\\beta $$\nThe term $y^{\\top}y$ is constant with respect to $\\beta$ and can be dropped from the minimization. We can also add the constant term $\\frac{1}{2}z^{\\top}z$ without changing the minimizer. The objective function is thus equivalent to minimizing:\n$$ L(\\beta) = \\frac{1}{2}(\\beta^{\\top}\\beta - 2z^{\\top}\\beta + z^{\\top}z) + \\lambda \\|\\beta\\|_{1} = \\frac{1}{2}\\|\\beta - z\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} $$\nThis objective function is separable, meaning it can be written as a sum of functions of individual components of $\\beta$:\n$$ L(\\beta) = \\sum_{j=1}^{p} \\left( \\frac{1}{2}(\\beta_j - z_j)^2 + \\lambda |\\beta_j| \\right) $$\nTherefore, we can find the optimal $\\hat{\\beta}_{\\mathrm{LASSO}}$ by minimizing each component-wise term independently:\n$$ \\hat{\\beta}_{j, \\mathrm{LASSO}} = \\arg \\min_{\\beta_j \\in \\mathbb{R}} \\left( \\frac{1}{2}(\\beta_j - z_j)^2 + \\lambda |\\beta_j| \\right) $$\nThis is the proximal operator of the $\\ell_1$ norm, which yields the soft-thresholding function. The solution for each component is:\n$$ \\hat{\\beta}_{j, \\mathrm{LASSO}} = \\mathrm{sign}(z_j) \\cdot \\max(|z_j| - \\lambda, 0) $$\nThis can be denoted as $\\hat{\\beta}_{\\mathrm{LASSO}} = S_{\\lambda}(z)$, where $S_{\\lambda}$ is the element-wise soft-thresholding operator with threshold $\\lambda$.\n\nNext, we analyze the Dantzig selector, $\\hat{\\beta}_{\\mathrm{DS}}$, which is the solution to the constrained optimization problem:\n$$ \\hat{\\beta}_{\\mathrm{DS}} = \\arg \\min_{\\beta \\in \\mathbb{R}^p} \\|\\beta\\|_{1} \\quad \\text{subject to} \\quad \\|X^{\\top}(y - X \\beta)\\|_{\\infty} \\leq t $$\nLet's simplify the constraint using the orthonormal design property $X^{\\top}X = I_p$ and the definition $z = X^{\\top}y$:\n$$ X^{\\top}(y - X \\beta) = X^{\\top}y - X^{\\top}X\\beta = z - \\beta $$\nThe constraint thus becomes $\\|z - \\beta\\|_{\\infty} \\leq t$. This is equivalent to a set of component-wise constraints:\n$$ |z_j - \\beta_j| \\leq t \\quad \\text{for all } j \\in \\{1, \\dots, p\\} $$\nEach inequality can be rewritten as $z_j - t \\leq \\beta_j \\leq z_j + t$. The optimization problem is now to minimize $\\|\\beta\\|_{1} = \\sum_{j=1}^{p} |\\beta_j|$ subject to these box constraints. Since both the objective and the constraints are separable, we can solve for each component independently:\n$$ \\hat{\\beta}_{j, \\mathrm{DS}} = \\arg \\min_{\\beta_j} |\\beta_j| \\quad \\text{subject to} \\quad z_j - t \\leq \\beta_j \\leq z_j + t $$\nTo find the minimum, we consider the location of the interval $[z_j - t, z_j + t]$ relative to $0$:\n1. If the interval contains $0$, i.e., $z_j - t \\leq 0 \\leq z_j + t$, which is equivalent to $|z_j| \\leq t$, the minimum of $|\\beta_j|$ is achieved at $\\hat{\\beta}_{j, \\mathrm{DS}} = 0$.\n2. If the interval is entirely positive, i.e., $z_j - t > 0$ or $z_j > t$, the point in the interval closest to $0$ is the left endpoint. Thus, $\\hat{\\beta}_{j, \\mathrm{DS}} = z_j - t$.\n3. If the interval is entirely negative, i.e., $z_j + t < 0$ or $z_j < -t$, the point in the interval closest to $0$ is the right endpoint. Thus, $\\hat{\\beta}_{j, \\mathrm{DS}} = z_j + t$.\nThese three cases can be summarized by the same soft-thresholding function:\n$$ \\hat{\\beta}_{j, \\mathrm{DS}} = \\mathrm{sign}(z_j) \\cdot \\max(|z_j| - t, 0) $$\nThis can be denoted as $\\hat{\\beta}_{\\mathrm{DS}} = S_{t}(z)$.\n\nNow we use the given numerical values:\n$$ z = \\begin{pmatrix} 2.4 \\\\ -0.3 \\\\ 1.1 \\\\ -1.8 \\\\ 0.0 \\end{pmatrix}, \\quad \\lambda = 0.9, \\quad t = 0.7 $$\nWe compute the LASSO estimate $\\hat{\\beta}_{\\mathrm{LASSO}} = S_{0.9}(z)$:\n$$ \\hat{\\beta}_{1, \\mathrm{LASSO}} = \\mathrm{sign}(2.4) \\max(|2.4| - 0.9, 0) = 1.5 $$\n$$ \\hat{\\beta}_{2, \\mathrm{LASSO}} = \\mathrm{sign}(-0.3) \\max(|-0.3| - 0.9, 0) = 0 $$\n$$ \\hat{\\beta}_{3, \\mathrm{LASSO}} = \\mathrm{sign}(1.1) \\max(|1.1| - 0.9, 0) = 0.2 $$\n$$ \\hat{\\beta}_{4, \\mathrm{LASSO}} = \\mathrm{sign}(-1.8) \\max(|-1.8| - 0.9, 0) = -0.9 $$\n$$ \\hat{\\beta}_{5, \\mathrm{LASSO}} = \\mathrm{sign}(0.0) \\max(|0.0| - 0.9, 0) = 0 $$\nSo, $\\hat{\\beta}_{\\mathrm{LASSO}} = \\begin{pmatrix} 1.5 \\\\ 0 \\\\ 0.2 \\\\ -0.9 \\\\ 0 \\end{pmatrix}$.\n\nNext, we compute the Dantzig selector estimate $\\hat{\\beta}_{\\mathrm{DS}} = S_{0.7}(z)$:\n$$ \\hat{\\beta}_{1, \\mathrm{DS}} = \\mathrm{sign}(2.4) \\max(|2.4| - 0.7, 0) = 1.7 $$\n$$ \\hat{\\beta}_{2, \\mathrm{DS}} = \\mathrm{sign}(-0.3) \\max(|-0.3| - 0.7, 0) = 0 $$\n$$ \\hat{\\beta}_{3, \\mathrm{DS}} = \\mathrm{sign}(1.1) \\max(|1.1| - 0.7, 0) = 0.4 $$\n$$ \\hat{\\beta}_{4, \\mathrm{DS}} = \\mathrm{sign}(-1.8) \\max(|-1.8| - 0.7, 0) = -1.1 $$\n$$ \\hat{\\beta}_{5, \\mathrm{DS}} = \\mathrm{sign}(0.0) \\max(|0.0| - 0.7, 0) = 0 $$\nSo, $\\hat{\\beta}_{\\mathrm{DS}} = \\begin{pmatrix} 1.7 \\\\ 0 \\\\ 0.4 \\\\ -1.1 \\\\ 0 \\end{pmatrix}$.\n\nFinally, we compute the squared $\\ell_2$ distance between the two estimators, $\\|\\hat{\\beta}_{\\mathrm{LASSO}} - \\hat{\\beta}_{\\mathrm{DS}}\\|_{2}^{2}$:\n$$ \\hat{\\beta}_{\\mathrm{LASSO}} - \\hat{\\beta}_{\\mathrm{DS}} = \\begin{pmatrix} 1.5 - 1.7 \\\\ 0 - 0 \\\\ 0.2 - 0.4 \\\\ -0.9 - (-1.1) \\\\ 0 - 0 \\end{pmatrix} = \\begin{pmatrix} -0.2 \\\\ 0 \\\\ -0.2 \\\\ 0.2 \\\\ 0 \\end{pmatrix} $$\nThe squared $\\ell_2$ norm is the sum of the squares of the components of this difference vector:\n$$ \\|\\hat{\\beta}_{\\mathrm{LASSO}} - \\hat{\\beta}_{\\mathrm{DS}}\\|_{2}^{2} = (-0.2)^2 + 0^2 + (-0.2)^2 + (0.2)^2 + 0^2 $$\n$$ \\|\\hat{\\beta}_{\\mathrm{LASSO}} - \\hat{\\beta}_{\\mathrm{DS}}\\|_{2}^{2} = 0.04 + 0 + 0.04 + 0.04 + 0 = 0.12 $$\nThe problem requires the answer to be rounded to four significant figures. The calculated value is $0.12$, which can be written as $0.1200$ to reflect this precision.", "answer": "$$\\boxed{0.1200}$$", "id": "3435559"}]}