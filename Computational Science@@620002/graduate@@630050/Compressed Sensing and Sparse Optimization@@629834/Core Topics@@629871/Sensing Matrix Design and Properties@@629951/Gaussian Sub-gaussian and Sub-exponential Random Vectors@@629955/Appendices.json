{"hands_on_practices": [{"introduction": "Quadratic forms involving Gaussian random vectors, such as $g^\\top A g$, are ubiquitous in statistical analysis, machine learning, and signal processing. Mastering their statistical properties is therefore a crucial skill. This exercise [@problem_id:3447473] guides you through a first-principles derivation of the mean and variance of such a form, reinforcing key concepts like the trace operator and the moment properties of multivariate normal distributions.", "problem": "Consider a compressed sensing setting in which a quadratic penalty is used to measure energy along a weighted metric. Let $n \\in \\mathbb{N}$, let $A \\in \\mathbb{R}^{n \\times n}$ be a deterministic symmetric matrix, and let $g \\in \\mathbb{R}^{n}$ be a zero-mean Gaussian random vector with covariance matrix $\\Sigma \\in \\mathbb{R}^{n \\times n}$, that is $g \\sim \\mathcal{N}(0,\\Sigma)$ with $\\Sigma$ symmetric positive semidefinite. Define the quadratic form $Q := g^{\\top} A g$. Using only foundational definitions of expectation, covariance, and trace, together with standard properties of multivariate Gaussian moments and basic linear algebra identities, derive closed-form expressions for the mean $\\mathbb{E}[Q]$ and the variance $\\operatorname{Var}(Q)$ in terms of $A$ and $\\Sigma$. Express your final answer as an ordered pair $\\left(\\mathbb{E}[Q], \\operatorname{Var}(Q)\\right)$ in a single, closed-form analytic expression. No rounding is required, and no units are involved.", "solution": "The problem as stated is valid. It is a well-posed, scientifically grounded question from the field of probability theory and its applications in engineering, with all necessary information provided and no contradictions. We may proceed with the derivation.\n\nThe quantity of interest is the quadratic form $Q = g^{\\top} A g$, where $g \\in \\mathbb{R}^{n}$ is a random vector following a multivariate normal distribution with mean $0$ and covariance matrix $\\Sigma$, denoted as $g \\sim \\mathcal{N}(0,\\Sigma)$. The matrix $A \\in \\mathbb{R}^{n \\times n}$ is a deterministic symmetric matrix, i.e., $A = A^{\\top}$. We are tasked with finding the mean $\\mathbb{E}[Q]$ and the variance $\\operatorname{Var}(Q)$.\n\nFirst, we calculate the mean of $Q$. Since $Q$ is a scalar quantity, it is equal to its trace.\n$$Q = g^{\\top} A g = \\operatorname{tr}(g^{\\top} A g)$$\nUsing the cyclic property of the trace operator, $\\operatorname{tr}(XYZ) = \\operatorname{tr}(ZXY)$, we can rearrange the terms inside the trace:\n$$Q = \\operatorname{tr}(A g g^{\\top})$$\nWe now compute the expectation. By the linearity of the expectation operator, we can move it inside the trace:\n$$\\mathbb{E}[Q] = \\mathbb{E}[\\operatorname{tr}(A g g^{\\top})] = \\operatorname{tr}(\\mathbb{E}[A g g^{\\top}])$$\nSince $A$ is a deterministic matrix, it can be factored out of the expectation:\n$$\\mathbb{E}[Q] = \\operatorname{tr}(A \\, \\mathbb{E}[g g^{\\top}])$$\nThe term $\\mathbb{E}[g g^{\\top}]$ is the definition of the covariance matrix of a zero-mean random vector. We are given that $g \\sim \\mathcal{N}(0,\\Sigma)$, so its covariance matrix is $\\Sigma$.\n$$\\mathbb{E}[g g^{\\top}] = \\operatorname{Cov}(g) = \\Sigma$$\nSubstituting this result back, we obtain the expression for the mean of $Q$:\n$$\\mathbb{E}[Q] = \\operatorname{tr}(A \\Sigma)$$\n\nNext, we calculate the variance of $Q$. The variance is defined as:\n$$\\operatorname{Var}(Q) = \\mathbb{E}[Q^2] - (\\mathbb{E}[Q])^2$$\nWe have already found $\\mathbb{E}[Q]$, so we need to compute $\\mathbb{E}[Q^2]$. Let's express $Q$ in index notation. Let $g_i$ be the $i$-th component of $g$ and $A_{ij}$ be the element in the $i$-th row and $j$-th column of $A$. Then,\n$$Q = \\sum_{i=1}^{n} \\sum_{j=1}^{n} A_{ij} g_i g_j$$\nSquaring this expression gives:\n$$Q^2 = \\left( \\sum_{i,j} A_{ij} g_i g_j \\right) \\left( \\sum_{k,l} A_{kl} g_k g_l \\right) = \\sum_{i,j,k,l} A_{ij} A_{kl} g_i g_j g_k g_l$$\nTaking the expectation, we get:\n$$\\mathbb{E}[Q^2] = \\sum_{i,j,k,l} A_{ij} A_{kl} \\mathbb{E}[g_i g_j g_k g_l]$$\nTo evaluate the fourth moment $\\mathbb{E}[g_i g_j g_k g_l]$ of the zero-mean Gaussian vector $g$, we use Isserlis' theorem (or Wick's theorem for Gaussian variables). For a zero-mean multivariate Gaussian distribution, the expectation of the product of an even number of variables is the sum of the products of expectations of all possible pairings. For four variables, this is:\n$$\\mathbb{E}[g_i g_j g_k g_l] = \\mathbb{E}[g_i g_j]\\mathbb{E}[g_k g_l] + \\mathbb{E}[g_i g_k]\\mathbb{E}[g_j g_l] + \\mathbb{E}[g_i g_l]\\mathbb{E}[g_j g_k]$$\nLet $\\Sigma_{ab} = \\mathbb{E}[g_a g_b]$ be the elements of the covariance matrix $\\Sigma$. The formula becomes:\n$$\\mathbb{E}[g_i g_j g_k g_l] = \\Sigma_{ij}\\Sigma_{kl} + \\Sigma_{ik}\\Sigma_{jl} + \\Sigma_{il}\\Sigma_{jk}$$\nSubstituting this into the expression for $\\mathbb{E}[Q^2]$ splits the sum into three parts:\n$$\\mathbb{E}[Q^2] = \\sum_{i,j,k,l} A_{ij} A_{kl} \\Sigma_{ij}\\Sigma_{kl} + \\sum_{i,j,k,l} A_{ij} A_{kl} \\Sigma_{ik}\\Sigma_{jl} + \\sum_{i,j,k,l} A_{ij} A_{kl} \\Sigma_{il}\\Sigma_{jk}$$\nLet's analyze each term separately.\n\nThe first term is:\n$$S_1 = \\sum_{i,j,k,l} A_{ij} A_{kl} \\Sigma_{ij}\\Sigma_{kl} = \\left(\\sum_{i,j} A_{ij} \\Sigma_{ij}\\right) \\left(\\sum_{k,l} A_{kl} \\Sigma_{kl}\\right)$$\nThe sum $\\sum_{i,j} A_{ij} \\Sigma_{ij}$ can be written using the trace. Since $\\Sigma$ is symmetric ($\\Sigma_{ij} = \\Sigma_{ji}$), this sum is $\\sum_{i,j} A_{ij} \\Sigma_{ji} = \\sum_i (A\\Sigma)_{ii} = \\operatorname{tr}(A\\Sigma)$. Thus:\n$$S_1 = (\\operatorname{tr}(A\\Sigma))^2 = (\\mathbb{E}[Q])^2$$\n\nThe second term is:\n$$S_2 = \\sum_{i,j,k,l} A_{ij} A_{kl} \\Sigma_{ik}\\Sigma_{jl}$$\nWe can rearrange the terms and group the sums:\n$$S_2 = \\sum_{j,k} \\left( \\sum_i A_{ij}\\Sigma_{ik} \\right) \\left( \\sum_l A_{kl}\\Sigma_{jl} \\right)$$\nThe first parenthesis is $\\sum_i A_{ij}\\Sigma_{ik} = \\sum_i A_{ji}^{\\top}\\Sigma_{ik} = (A^{\\top}\\Sigma)_{jk}$. Since $A$ is symmetric ($A=A^{\\top}$), this is $(A\\Sigma)_{jk}$.\nThe second parenthesis is $\\sum_l A_{kl}\\Sigma_{jl} = \\sum_l A_{kl}\\Sigma_{lj}^{\\top} = (A\\Sigma^{\\top})_{kj}$. Since $\\Sigma$ is symmetric ($\\Sigma=\\Sigma^{\\top}$), this is $(A\\Sigma)_{kj}$.\nSo, we have:\n$$S_2 = \\sum_{j,k} (A\\Sigma)_{jk} (A\\Sigma)_{kj} = \\sum_j \\left( \\sum_k (A\\Sigma)_{jk} (A\\Sigma)_{kj} \\right) = \\sum_j ((A\\Sigma)(A\\Sigma))_{jj} = \\operatorname{tr}((A\\Sigma)^2)$$\n\nThe third term is:\n$$S_3 = \\sum_{i,j,k,l} A_{ij} A_{kl} \\Sigma_{il}\\Sigma_{jk}$$\nThis expression can be recognized as the trace of a product of matrices. Specifically:\n$$S_3 = \\sum_{i,j,k,l} A_{ij} \\Sigma_{jk} A_{kl} \\Sigma_{li} = \\sum_i \\left( \\sum_{j,k,l} A_{ij} \\Sigma_{jk} A_{kl} \\Sigma_{li} \\right) = \\sum_i (A\\Sigma A\\Sigma)_{ii} = \\operatorname{tr}(A\\Sigma A\\Sigma) = \\operatorname{tr}((A\\Sigma)^2)$$\nThus, $S_2 = S_3 = \\operatorname{tr}((A\\Sigma)^2)$.\n\nCombining the three terms, we get the second moment of $Q$:\n$$\\mathbb{E}[Q^2] = S_1 + S_2 + S_3 = (\\operatorname{tr}(A\\Sigma))^2 + 2\\operatorname{tr}((A\\Sigma)^2)$$\nFinally, we compute the variance:\n$$\\operatorname{Var}(Q) = \\mathbb{E}[Q^2] - (\\mathbb{E}[Q])^2 = \\left( (\\operatorname{tr}(A\\Sigma))^2 + 2\\operatorname{tr}((A\\Sigma)^2) \\right) - (\\operatorname{tr}(A\\Sigma))^2$$\n$$\\operatorname{Var}(Q) = 2\\operatorname{tr}((A\\Sigma)^2)$$\n\nThe derived expressions for the mean and variance are $\\mathbb{E}[Q] = \\operatorname{tr}(A\\Sigma)$ and $\\operatorname{Var}(Q) = 2\\operatorname{tr}((A\\Sigma)^2)$. The problem requests the answer as an ordered pair.", "answer": "$$\n\\boxed{\n(\\operatorname{tr}(A\\Sigma), 2\\operatorname{tr}((A\\Sigma)^2))\n}\n$$", "id": "3447473"}, {"introduction": "The power of Gaussian-like behavior extends far beyond the normal distribution itself, to the broader class of sub-gaussian random variables. This practice [@problem_id:3447505] builds a crucial bridge by showing how any bounded, mean-zero random variable is sub-gaussian. Starting from the celebrated Hoeffding’s lemma, you will derive a tight bound on the sub-gaussian norm, solidifying your understanding of how this key parameter quantifies concentration.", "problem": "Let $X$ be a real-valued random variable used to model bounded noise in a compressed sensing acquisition pipeline. Assume $X$ is almost surely supported on the interval $[a,b]$ with $a<b$, and has zero mean, $\\mathbb{E}[X]=0$. Let $M_{X}(\\lambda)=\\mathbb{E}[\\exp(\\lambda X)]$ denote the moment generating function (MGF). You may take as a starting point the following well-tested fact (Hoeffding’s lemma): if $X$ is mean-zero and $X \\in [a,b]$ almost surely, then $M_{X}(\\lambda) \\leq \\exp\\!\\left(\\lambda^{2}(b-a)^{2}/8\\right)$ for all $\\lambda \\in \\mathbb{R}$.\n\nDefine the sub-Gaussian $\\psi_{2}$ norm of $X$ via the MGF-based Orlicz definition\n$$\n\\|X\\|_{\\psi_{2}} := \\inf\\left\\{ s>0 : \\mathbb{E}\\!\\left[\\exp(\\lambda X)\\right] \\le \\exp\\!\\left(\\tfrac{1}{2}s^{2}\\lambda^{2}\\right)\\ \\text{for all}\\ \\lambda \\in \\mathbb{R} \\right\\}.\n$$\nStarting from Hoeffding’s lemma and only using fundamental properties of MGFs, derive a Gaussian-dominance MGF bound for $X$ in terms of $(b-a)$, and use it to determine the smallest universal constant $C>0$ such that the inequality\n$$\n\\|X\\|_{\\psi_{2}} \\le C\\,(b-a)\n$$\nholds for every mean-zero $X$ supported on $[a,b]$. Provide your answer as the exact value of $C$ with no units. The final answer must be a single real number. No rounding is required.", "solution": "The objective is to find the smallest universal constant $C > 0$ such that for any real-valued random variable $X$ with mean $\\mathbb{E}[X]=0$ and almost sure support on the interval $[a,b]$, the inequality $\\|X\\|_{\\psi_{2}} \\le C(b-a)$ holds.\n\nThe sub-Gaussian $\\psi_{2}$ norm is defined as:\n$$\n\\|X\\|_{\\psi_{2}} := \\inf\\left\\{ s>0 : \\mathbb{E}[\\exp(\\lambda X)] \\le \\exp\\left(\\tfrac{1}{2}s^{2}\\lambda^{2}\\right)\\ \\text{for all}\\ \\lambda \\in \\mathbb{R} \\right\\}\n$$\nThe problem provides Hoeffding's lemma as a starting point. For any random variable $X$ with $\\mathbb{E}[X]=0$ and $X \\in [a,b]$ almost surely, its moment generating function (MGF) $M_{X}(\\lambda) = \\mathbb{E}[\\exp(\\lambda X)]$ is bounded by:\n$$\nM_{X}(\\lambda) \\le \\exp\\left(\\frac{\\lambda^{2}(b-a)^{2}}{8}\\right)\n$$\nThis inequality provides a \"Gaussian-dominance\" bound for the MGF of $X$.\n\nWe can use this lemma to establish an upper bound on $\\|X\\|_{\\psi_{2}}$. According to the definition of the $\\psi_{2}$ norm, $\\|X\\|_{\\psi_{2}}$ is the infimum of all $s>0$ that satisfy the inequality:\n$$\n\\mathbb{E}[\\exp(\\lambda X)] \\le \\exp\\left(\\frac{1}{2}s^{2}\\lambda^{2}\\right) \\quad \\text{for all } \\lambda \\in \\mathbb{R}\n$$\nFrom Hoeffding's lemma, we know that the left-hand side is bounded by $\\exp\\left(\\frac{\\lambda^{2}(b-a)^{2}}{8}\\right)$. Therefore, the condition for the $\\psi_{2}$ norm will be satisfied if we choose an $s$ such that:\n$$\n\\exp\\left(\\frac{\\lambda^{2}(b-a)^{2}}{8}\\right) \\le \\exp\\left(\\frac{1}{2}s^{2}\\lambda^{2}\\right)\n$$\nTaking the natural logarithm of both sides, this inequality is equivalent to:\n$$\n\\frac{\\lambda^{2}(b-a)^{2}}{8} \\le \\frac{1}{2}s^{2}\\lambda^{2}\n$$\nFor any $\\lambda \\ne 0$, we can divide by $\\lambda^{2}$ to obtain:\n$$\n\\frac{(b-a)^{2}}{8} \\le \\frac{s^{2}}{2}\n$$\nRearranging this inequality to solve for $s^2$, we get:\n$$\ns^{2} \\ge \\frac{2(b-a)^{2}}{8} = \\frac{(b-a)^{2}}{4}\n$$\nSince $s$ must be positive, we take the square root of both sides:\n$$\ns \\ge \\frac{b-a}{2}\n$$\nThis demonstrates that any value of $s \\ge \\frac{b-a}{2}$ satisfies the defining inequality for the $\\psi_{2}$ norm for any mean-zero random variable $X$ on $[a,b]$. The norm $\\|X\\|_{\\psi_{2}}$ is the infimum of all such valid $s$. Therefore, we must have:\n$$\n\\|X\\|_{\\psi_{2}} \\le \\frac{b-a}{2}\n$$\nThis proves that the inequality $\\|X\\|_{\\psi_{2}} \\le C(b-a)$ holds for $C = \\frac{1}{2}$. This value is an upper bound for the smallest universal constant.\n\nTo show that $C = \\frac{1}{2}$ is indeed the *smallest* such constant, we must show that this bound is tight. That is, we must find at least one random variable $X_{0}$ in the considered class for which $\\|X_{0}\\|_{\\psi_{2}} = \\frac{1}{2}(b-a)$. If such a variable exists, then no constant smaller than $\\frac{1}{2}$ can satisfy the inequality universally.\n\nConsider a symmetric two-point random variable $Y$ whose support interval has length $b-a$. Let this random variable be supported on the set $\\{-\\frac{b-a}{2}, \\frac{b-a}{2}\\}$ with equal probabilities:\n$$\nP\\left(Y = \\frac{b-a}{2}\\right) = \\frac{1}{2} \\quad \\text{and} \\quad P\\left(Y = -\\frac{b-a}{2}\\right) = \\frac{1}{2}\n$$\nThe mean of $Y$ is $\\mathbb{E}[Y] = \\frac{1}{2} \\cdot \\frac{b-a}{2} + \\frac{1}{2} \\cdot \\left(-\\frac{b-a}{2}\\right) = 0$. The support is $[a', b']$ with $a' = -\\frac{b-a}{2}$ and $b' = \\frac{b-a}{2}$, so $b'-a' = b-a$. Thus, $Y$ belongs to the class of random variables under consideration (up to a shift of the interval, which does not affect the length $b-a$).\n\nLet us compute the MGF of $Y$:\n$$\nM_{Y}(\\lambda) = \\mathbb{E}[\\exp(\\lambda Y)] = \\frac{1}{2}\\exp\\left(\\lambda \\frac{b-a}{2}\\right) + \\frac{1}{2}\\exp\\left(-\\lambda \\frac{b-a}{2}\\right) = \\cosh\\left(\\lambda \\frac{b-a}{2}\\right)\n$$\nTo find $\\|Y\\|_{\\psi_2}$, we seek the infimum of $s>0$ such that for all $\\lambda \\in \\mathbb{R}$:\n$$\n\\cosh\\left(\\lambda \\frac{b-a}{2}\\right) \\le \\exp\\left(\\frac{1}{2}s^{2}\\lambda^{2}\\right)\n$$\nWe can analyze this inequality for small values of $\\lambda$ by comparing the Taylor series expansions of both sides around $\\lambda=0$.\nThe expansion for the left-hand side is:\n$$\n\\cosh(u) = 1 + \\frac{u^{2}}{2!} + \\frac{u^{4}}{4!} + \\dots \\implies \\cosh\\left(\\lambda \\frac{b-a}{2}\\right) = 1 + \\frac{1}{2}\\lambda^{2}\\left(\\frac{b-a}{2}\\right)^2 + O(\\lambda^4)\n$$\nThe expansion for the right-hand side is:\n$$\n\\exp(v) = 1 + v + \\frac{v^{2}}{2!} + \\dots \\implies \\exp\\left(\\frac{1}{2}s^{2}\\lambda^{2}\\right) = 1 + \\frac{1}{2}s^{2}\\lambda^{2} + O(\\lambda^4)\n$$\nFor the inequality to hold for all $\\lambda$ in a neighborhood of $0$, the coefficient of the $\\lambda^2$ term on the left cannot exceed that on the right:\n$$\n\\frac{1}{2}\\left(\\frac{b-a}{2}\\right)^2 \\le \\frac{1}{2}s^{2}\n$$\nThis simplifies to $s^{2} \\ge \\left(\\frac{b-a}{2}\\right)^2$. Since $s>0$, we must have $s \\ge \\frac{b-a}{2}$.\nThis implies that for the random variable $Y$, the infimum of possible values for $s$ is bounded below by $\\frac{b-a}{2}$. Thus,\n$$\n\\|Y\\|_{\\psi_{2}} \\ge \\frac{b-a}{2}\n$$\nCombining this with the general upper bound $\\|Y\\|_{\\psi_{2}} \\le \\frac{b-a}{2}$ derived from Hoeffding's lemma, we conclude that for this specific random variable $Y$:\n$$\n\\|Y\\|_{\\psi_{2}} = \\frac{b-a}{2}\n$$\nFor this random variable, the ratio is $\\frac{\\|Y\\|_{\\psi_{2}}}{b-a} = \\frac{(b-a)/2}{b-a} = \\frac{1}{2}$.\nSince we have found a random variable for which the ratio equals $\\frac{1}{2}$, the universal constant $C$ must be at least $\\frac{1}{2}$.\nGiven that we previously established $C \\le \\frac{1}{2}$, the smallest possible value for $C$ is exactly $\\frac{1}{2}$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "3447505"}, {"introduction": "The theoretical elegance of sub-gaussian vectors has profound practical implications, particularly in modern high-dimensional problems like compressed sensing. This hands-on coding exercise [@problem_id:3447515] invites you to empirically verify the 'universality' phenomenon. By comparing the performance of Gaussian and simple bounded (Rademacher) measurement matrices for sparse signal recovery, you will observe firsthand that the sub-gaussian property, rather than the specific distribution, governs the fundamental limits of recovery.", "problem": "Consider the noiseless sparse recovery problem in compressed sensing. Let $x_{0} \\in \\mathbb{R}^{n}$ be $s$-sparse, and let $A \\in \\mathbb{R}^{m \\times n}$ be a random measurement matrix with independent and identically distributed (i.i.d.) entries. You observe $y = A x_{0}$. The recovery method is $\\ell_{1}$ minimization (also called Basis Pursuit (BP)): minimize $\\|x\\|_{1}$ subject to $A x = y$, and declare exact support recovery when the set of indices of nonzero entries in the computed minimizer $\\hat{x}$ matches the support of $x_{0}$ exactly.\n\nTwo measurement ensembles are to be compared:\n- Gaussian: entries of $A$ are i.i.d. $\\mathcal{N}(0, 1/m)$.\n- Bounded sub-Gaussian (Rademacher): entries of $A$ are i.i.d. taking values $\\pm 1/\\sqrt{m}$ with equal probability.\n\nUse the following foundational base and facts as the starting point for your derivation and algorithm design:\n- The definition of a sub-Gaussian random variable $X$ with Orlicz $\\psi_{2}$ norm, namely the existence of a constant $K$ such that $\\mathbb{P}(|X| \\ge t) \\le 2 \\exp\\left(- t^{2}/K^{2}\\right)$ for all $t \\ge 0$.\n- For i.i.d. sub-Gaussian measurement matrices, concentration of measure implies that, with high probability, the Restricted Isometry Property (RIP) holds provided $m$ is on the order of $s \\log(en/s)$, up to absolute constants that depend on sub-Gaussian parameters.\n- Under RIP conditions with appropriate constants, $\\ell_{1}$ minimization achieves exact recovery of $s$-sparse vectors in the noiseless setting.\n\nYour task is to design a program that empirically compares the phase transition behavior for exact support recovery between the Gaussian and the Rademacher ensembles, and quantitatively reports the deviation between the two threshold estimates. The program must implement the following universally applicable mathematical procedure:\n\n- Fix dimension $n = 128$.\n- For each sparsity level $s$ in a provided test suite, consider a small grid of measurement counts $m$ generated by $m = \\lceil c \\, s \\, \\log(e n / s) \\rceil$, where $c$ ranges over a specified finite set of positive constants. Enforce $m \\ge s + 1$ and $m \\le n - 1$.\n- For each pair $(m, s)$ and each ensemble, estimate the empirical success probability of exact support recovery by repeating a fixed number of independent trials. In each trial:\n  - Sample a random matrix $A$ from the specified ensemble with entries scaled as above.\n  - Sample an $s$-sparse signal $x_{0}$ by choosing a support uniformly at random and nonzero entries as independent Rademacher signs $\\pm 1$.\n  - Form $y = A x_{0}$.\n  - Solve the $\\ell_{1}$ minimization problem to obtain $\\hat{x}$ and declare success if the support of $\\hat{x}$ (indices with absolute value exceeding a fixed tolerance) matches the support of $x_{0}$ exactly.\n- For each ensemble and $s$, define the empirical threshold $\\hat{m}^{\\star}$ as the smallest $m$ in the grid whose estimated success probability is at least a fixed target level. If no $m$ achieves the target, define $\\hat{m}^{\\star}$ to be the largest $m$ in the grid.\n- Quantify the deviation between ensembles by reporting, for each $s$ in the test suite, the triplet $\\left[\\hat{m}^{\\star}_{\\mathrm{Gauss}}, \\hat{m}^{\\star}_{\\mathrm{Rad}}, \\hat{m}^{\\star}_{\\mathrm{Rad}} - \\hat{m}^{\\star}_{\\mathrm{Gauss}}\\right]$.\n\nTest suite and parameters to use:\n- Dimension: $n = 128$.\n- Sparsity levels: $s \\in \\{1, 8, 16\\}$.\n- Grid constants: $c \\in \\{0.8, 1.2, 1.6, 2.4\\}$.\n- Trials per $(m, s)$ and ensemble: $T = 4$ independent trials.\n- Target success level: at least $0.5$ (i.e., at least $2$ successes out of $4$ trials).\n- Support detection tolerance: declare an index $i$ as active in $\\hat{x}$ if $|\\hat{x}_{i}| \\ge 10^{-3}$.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of lists, one for each $s$ in the order specified, where each inner list is $\\left[\\hat{m}^{\\star}_{\\mathrm{Gauss}}, \\hat{m}^{\\star}_{\\mathrm{Rad}}, \\hat{m}^{\\star}_{\\mathrm{Rad}} - \\hat{m}^{\\star}_{\\mathrm{Gauss}}\\right]$, with all entries as integers. For example, an output with three sparsity levels should look like $\\left[[a,b,b-a],[c,d,d-c],[e,f,f-e]\\right]$ where $a,b,c,d,e,f$ are integers.", "solution": "The problem requires an empirical comparison of two sub-Gaussian measurement ensembles, Gaussian and Rademacher, for the task of sparse recovery using $\\ell_1$ minimization. The comparison is based on the phase transition behavior, specifically by estimating the minimum number of measurements $m$ required for successful recovery of an $s$-sparse vector in $\\mathbb{R}^n$.\n\n### 1. Theoretical Framework\n\nThe problem is situated in the field of compressed sensing. The fundamental goal is to recover a sparse signal $x_0 \\in \\mathbb{R}^n$, characterized by having only $s \\ll n$ non-zero entries, from a small number of linear measurements, $y = A x_0$, where $y \\in \\mathbb{R}^m$ and $A \\in \\mathbb{R}^{m \\times n}$ is the measurement matrix. The challenge is that the system is underdetermined, i.e., $m < n$.\n\nRecovery is made possible by exploiting the sparsity of $x_0$. The most direct approach, minimizing the number of non-zero entries (the $\\ell_0$-norm) subject to the measurement constraint, is computationally intractable (NP-hard). Basis Pursuit (BP), or $\\ell_1$ minimization, provides a convex relaxation to this problem:\n$$\n\\hat{x} = \\arg\\min_{x \\in \\mathbb{R}^n} \\|x\\|_1 \\quad \\text{subject to} \\quad Ax = y\n$$\nwhere $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$.\n\n### 2. Conversion to a Linear Program (LP)\n\nThe $\\ell_1$ minimization problem can be transformed into a standard Linear Program (LP), which is solvable in polynomial time. This is achieved by decomposing the variable $x$ into its positive and negative parts: $x = x^+ - x^-$, where $x^+ \\ge 0$ and $x^- \\ge 0$. The components of these vectors are defined as $x_i^+ = \\max(x_i, 0)$ and $x_i^- = \\max(-x_i, 0)$. With this substitution, the $\\ell_1$ norm becomes a linear function: $\\|x\\|_1 = \\sum_i |x_i| = \\sum_i (x_i^+ + x_i^-)$.\n\nThe optimization problem is then recast as:\n$$\n\\min_{x^+, x^- \\in \\mathbb{R}^n} \\sum_{i=1}^n (x_i^+ + x_i^-) \\quad \\text{subject to} \\quad A(x^+ - x^-) = y, \\quad x^+ \\ge 0, \\quad x^- \\ge 0.\n$$\nThis is an LP in the $2n$-dimensional variable $z = [x^+; x^-]^T$. In canonical form, we want to solve $\\min_{z} c^T z$ subject to $A_{eq}z = b_{eq}$ and $z \\ge 0$, where:\n- $c = \\mathbf{1}_{2n}$ (a vector of $2n$ ones).\n- $A_{eq} = [A, -A] \\in \\mathbb{R}^{m \\times 2n}$.\n- $b_{eq} = y \\in \\mathbb{R}^m$.\n\nThis formulation allows the use of standard LP solvers, such as `scipy.optimize.linprog`.\n\n### 3. Measurement Matrix Ensembles and RIP\n\nThe success of $\\ell_1$ minimization hinges on the properties of the matrix $A$. A central concept is the Restricted Isometry Property (RIP), which states that $A$ must act as a near-isometry on all sparse vectors. Specifically, a matrix $A$ satisfies RIP of order $s$ with constant $\\delta_s \\in (0, 1)$ if\n$$\n(1 - \\delta_s) \\|v\\|_2^2 \\le \\|Av\\|_2^2 \\le (1 + \\delta_s) \\|v\\|_2^2\n$$\nholds for all $s$-sparse vectors $v$. If $A$ satisfies RIP of order $2s$ with a sufficiently small constant (e.g., $\\delta_{2s} < \\sqrt{2}-1$), then $\\ell_1$ minimization is guaranteed to recover any $s$-sparse signal $x_0$ exactly from $y = Ax_0$.\n\nRandom matrix theory provides the crucial link: matrices with i.i.d. sub-Gaussian entries satisfy RIP with high probability, provided the number of measurements $m$ is of the order $m \\ge C \\cdot s \\log(n/s)$ for some constant $C$. Both ensembles in this problem fall into this category:\n- **Gaussian ensemble**: The entries $A_{ij} \\sim \\mathcal{N}(0, 1/m)$ are sub-Gaussian.\n- **Rademacher ensemble**: The entries $A_{ij} = \\pm 1/\\sqrt{m}$ are drawn from a bounded distribution, and are therefore also sub-Gaussian.\n\nThe scaling factor $1/\\sqrt{m}$ is canonical and ensures the columns of $A$ have expected Euclidean norm of $1$, which is related to the RIP condition. Although theory provides similar asymptotic guarantees for both ensembles, their finite-sample performance can differ, which this problem explores empirically.\n\n### 4. Phase Transition and Empirical Estimation\n\nIn compressed sensing, for a given signal structure and recovery algorithm, there exists a sharp \"phase transition\". For a given $n$ and $s$, as $m$ increases, the probability of successful recovery transitions rapidly from near $0$ to near $1$. The goal of this problem is to empirically locate the point where this transition occurs for the two ensembles.\n\nThe prescribed algorithm implements this search:\n1.  **Parameter Grid**: For each sparsity $s$, a grid of measurement counts $m$ is created using the theoretical scaling law $m \\propto s \\log(en/s)$, with various constants $c$ to probe the region around the expected phase transition.\n2.  **Success Probability Estimation**: For each pair $(m,s)$ and each ensemble, the probability of exact support recovery is estimated by Monte Carlo simulation. A number of trials $T=4$ are run.\n3.  **A Single Trial**: Each trial involves:\n    a. Generating a random $s$-sparse signal $x_0$ with a random support and random non-zero values.\n    b. Generating a random measurement matrix $A$ from the specified ensemble.\n    c. Computing measurements $y = Ax_0$.\n    d. Solving the LP formulation of Basis Pursuit to find the estimate $\\hat{x}$.\n    e. Declaring success if the support of $\\hat{x}$ (the set of indices of non-zero entries, thresholded for numerical precision) exactly matches the support of $x_0$.\n4.  **Empirical Threshold $\\hat{m}^\\star$**: For each ensemble and sparsity $s$, the threshold $\\hat{m}^\\star$ is defined as the smallest $m$ in the grid for which the empirical success probability meets a target (here, $0.5$, which is $2$ out of $4$ trials). This $\\hat{m}^\\star$ serves as a point estimate of the phase transition boundary.\n5.  **Comparison**: The final output quantitatively compares the thresholds $[\\hat{m}^\\star_{\\mathrm{Gauss}}, \\hat{m}^\\star_{\\mathrm{Rad}}]$ and their difference, shedding light on the relative sample efficiency of the two matrix ensembles under these finite-dimensional settings.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Empirically compares phase transition behavior for sparse recovery \n    between Gaussian and Rademacher measurement ensembles.\n    \"\"\"\n    #\n    # --- Problem Parameters ---\n    #\n    N_DIM = 128\n    S_VALS = [1, 8, 16]\n    C_VALS = [0.8, 1.2, 1.6, 2.4]\n    TRIALS = 4\n    SUPPORT_TOL = 1e-3\n    TARGET_SUCCESS_PROB = 0.5\n    TARGET_SUCCESS_COUNT = int(TARGET_SUCCESS_PROB * TRIALS)\n    \n    # Use a single random number generator for the entire simulation\n    rng = np.random.default_rng()\n\n    #\n    # --- Helper Function for a Single Experiment ---\n    #\n    def run_single_experiment(m, s, ensemble_type, n_dim):\n        \"\"\"\n        Runs T=TRIALS for a given (m, s, ensemble) and returns success count.\n        \"\"\"\n        successes = 0\n        for _ in range(TRIALS):\n            # 1. Generate s-sparse signal x0\n            support = rng.choice(n_dim, s, replace=False)\n            x0 = np.zeros(n_dim)\n            x0[support] = rng.choice([-1.0, 1.0], s)\n\n            # 2. Generate measurement matrix A\n            if ensemble_type == 'Gauss':\n                A = rng.standard_normal(size=(m, n_dim)) / np.sqrt(m)\n            else:  # Rademacher\n                A = rng.choice([-1.0, 1.0], size=(m, n_dim)) / np.sqrt(m)\n\n            # 3. Form observation y\n            y = A @ x0\n\n            # 4. Solve l1 minimization via Linear Programming\n            # min ||x||_1 s.t. Ax = y  <=> min 1^Tz s.t. [A -A]z = y, z>=0\n            c_lp = np.ones(2 * n_dim)\n            A_eq = np.hstack([A, -A])\n            \n            # The 'highs' method is robust and default in recent SciPy versions\n            res = linprog(c=c_lp, A_eq=A_eq, b_eq=y, bounds=(0, None), method='highs')\n\n            # 5. Check for exact support recovery\n            if res.success:\n                x_hat = res.x[:n_dim] - res.x[n_dim:]\n                support_hat = np.where(np.abs(x_hat) > SUPPORT_TOL)[0]\n                if set(support) == set(support_hat):\n                    successes += 1\n        return successes\n\n    #\n    # --- Main Simulation Loop ---\n    #\n    all_results = []\n    for s in S_VALS:\n        # Generate the grid of measurement counts m for the current s\n        m_grid = []\n        for c in C_VALS:\n            m_val = int(np.ceil(c * s * np.log(np.e * N_DIM / s)))\n            if s + 1 <= m_val <= N_DIM - 1:\n                m_grid.append(m_val)\n        m_grid = sorted(list(set(m_grid)))\n        \n        # If the grid is empty, something is wrong, but we proceed\n        # as per validation this will not happen with given parameters.\n        if not m_grid:\n            continue\n\n        ensemble_thresholds = {}\n        for ensemble in ['Gauss', 'Rad']:\n            m_star = m_grid[-1]  # Default to largest m if target not met\n            for m in m_grid:\n                success_count = run_single_experiment(m, s, ensemble, N_DIM)\n                if success_count >= TARGET_SUCCESS_COUNT:\n                    m_star = m\n                    break  # Found the smallest m that meets the criteria\n            ensemble_thresholds[ensemble] = m_star\n\n        m_star_gauss = ensemble_thresholds['Gauss']\n        m_star_rad = ensemble_thresholds['Rad']\n        diff = m_star_rad - m_star_gauss\n        all_results.append([m_star_gauss, m_star_rad, diff])\n\n    #\n    # --- Final Output Formatting ---\n    #\n    # The required format is a compact string representation of a list of lists,\n    -    # without spaces after commas. e.g., [[a,b,c],[d,e,f]]\n    #\n    result_strings = [f'[{r[0]},{r[1]},{r[2]}]' for r in all_results]\n    final_output_str = f\"[{','.join(result_strings)}]\"\n    print(final_output_str)\n\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3447515"}]}