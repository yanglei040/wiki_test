{"hands_on_practices": [{"introduction": "A fundamental property of a well-designed measurement matrix $A$ is that its Gram matrix $A^{\\top}A$ acts nearly as the identity, ensuring that geometric structures are preserved. In this foundational practice, we will establish this property for matrices with independent, isotropic subgaussian rows. You will use the matrix Bernstein inequality to derive a high-probability bound on the deviation $\\\\|A^{\\top}A - I_n\\\\|$ and determine the number of measurements $m$ required to ensure this deviation is small, providing a key building block for proving performance guarantees in compressed sensing and randomized algorithms [@problem_id:3472187].", "problem": "Consider an $m \\times n$ random matrix $A$ constructed as follows. Let $\\{x_{i}\\}_{i=1}^{m}$ be independent and identically distributed (i.i.d.) random vectors in $\\mathbb{R}^{n}$ with zero mean and identity covariance, that is, $\\mathbb{E}[x_{i}]=0$ and $\\mathbb{E}[x_{i}x_{i}^{\\top}]=I_{n}$ (such vectors are called isotropic). Assume the subgaussian Orlicz norm satisfies $\\sup_{u \\in S^{n-1}}\\|\\langle x_{i},u\\rangle\\|_{\\psi_{2}} \\leq 1$ for every $i$, where $\\|\\cdot\\|_{\\psi_{2}}$ denotes the subgaussian Orlicz norm and $S^{n-1}$ denotes the unit sphere in $\\mathbb{R}^{n}$. Define the rows of $A$ by $a_{i}^{\\top}=x_{i}^{\\top}/\\sqrt{m}$ so that $\\mathbb{E}[A^{\\top}A]=I_{n}$.\n\nUsing the noncommutative matrix Bernstein inequality (also known as the matrix Laplace transform method) for independent self-adjoint random matrices, derive a high-probability bound on the spectral norm deviation $\\|A^{\\top}A-I_{n}\\|$. Then, for a prescribed tolerance $\\delta \\in (0,1)$ and failure probability $\\eta \\in (0,1)$, compute a sufficient value of $m$ expressed explicitly as a function of $n$, $\\delta$, and $\\eta$ to ensure that $\\mathbb{P}\\big(\\|A^{\\top}A-I_{n}\\| \\leq \\delta\\big) \\geq 1-\\eta$.\n\nYou may treat any absolute numerical constants that arise from well-tested concentration inequalities and moment bounds as fixed universal constants that do not depend on $n$, $m$, $\\delta$, or $\\eta$. Express your final $m$ as a single closed-form analytic expression in terms of $n$, $\\delta$, and $\\eta$. No numerical evaluation is required.", "solution": "The user wants to find a sufficient number of measurements, $m$, for a compressed sensing matrix $A$ to satisfy a certain property with high probability.\n\n### Step 1: Problem Validation\n\n**1. Extraction of Givens:**\n- An $m \\times n$ random matrix $A$.\n- The rows of $A$, denoted $a_i^\\top$, are given by $a_i^\\top = x_i^\\top / \\sqrt{m}$.\n- $\\{x_i\\}_{i=1}^m$ are independent and identically distributed (i.i.d.) random vectors in $\\mathbb{R}^n$.\n- The vectors $x_i$ are isotropic, meaning they have zero mean, $\\mathbb{E}[x_i] = 0$, and identity covariance, $\\mathbb{E}[x_i x_i^\\top] = I_n$.\n- The subgaussian Orlicz norm of the projections of $x_i$ is bounded: $\\sup_{u \\in S^{n-1}} \\|\\langle x_i, u \\rangle\\|_{\\psi_2} \\le 1$ for every $i$.\n- We need to derive a high-probability bound on the spectral norm $\\|A^\\top A - I_n\\|$.\n- For a given tolerance $\\delta \\in (0,1)$ and failure probability $\\eta \\in (0,1)$, we need to compute a sufficient value of $m$ to ensure $\\mathbb{P}(\\|A^\\top A - I_n\\| \\le \\delta) \\ge 1-\\eta$.\n- The tool to be used is specified as the noncommutative matrix Bernstein inequality.\n- Any arising absolute numerical constants can be treated as fixed universal constants.\n\n**2. Validation using Extracted Givens:**\n- **Scientific Soundness:** The problem is firmly grounded in the fields of high-dimensional probability and random matrix theory, which are central to the theory of compressed sensing. The concepts of isotropic subgaussian vectors, Orlicz norms, and matrix concentration inequalities are standard and well-defined. The setup does not violate any mathematical principles.\n- **Well-Posedness:** The problem is well-posed. It asks for a sufficient condition on $m$, which is a standard task in this field. A unique answer for the functional form of $m$ is expected, up to a universal constant.\n- **Objectivity:** The problem is stated in precise, objective mathematical language. Terms like \"isotropic,\" \"subgaussian Orlicz norm,\" and \"spectral norm\" have unambiguous definitions.\n- **Completeness:** The problem provides all necessary information to apply matrix concentration inequalities. The properties of the random vectors $x_i$ are fully specified to determine the necessary parameters for the Bernstein inequality.\n- **Relation to Topic:** The problem is directly related to random matrix ensembles and concentration phenomena, which is a core topic in the analysis of algorithms for compressed sensing and sparse optimization.\n\n**3. Verdict:**\nThe problem is valid.\n\n### Step 2: Derivation of the Bound\n\nThe quantity of interest is the spectral norm of the deviation of the sample covariance matrix from the true covariance matrix. The matrix $A$ has rows $a_i^\\top = x_i^\\top / \\sqrt{m}$. The Gram matrix $A^\\top A$ can be written as:\n$$ A^\\top A = \\left( \\frac{1}{\\sqrt{m}} [x_1, \\dots, x_m] \\right) \\left( \\frac{1}{\\sqrt{m}} \\begin{pmatrix} x_1^\\top \\\\ \\vdots \\\\ x_m^\\top \\end{pmatrix} \\right) = \\frac{1}{m} \\sum_{i=1}^m x_i x_i^\\top $$\nThe expectation of this matrix is $\\mathbb{E}[A^\\top A] = \\frac{1}{m} \\sum_{i=1}^m \\mathbb{E}[x_i x_i^\\top] = \\frac{1}{m} \\sum_{i=1}^m I_n = I_n$, as stated in the problem.\n\nWe want to bound $\\|A^\\top A - I_n\\| = \\left\\|\\frac{1}{m} \\sum_{i=1}^m x_i x_i^\\top - I_n\\right\\|$.\nLet's define the random matrices $Y_i = x_i x_i^\\top - I_n$. These are independent $n \\times n$ self-adjoint matrices. Since $\\mathbb{E}[x_i x_i^\\top] = I_n$, we have $\\mathbb{E}[Y_i] = 0$.\nThe expression to bound is then $\\left\\|\\frac{1}{m} \\sum_{i=1}^m Y_i\\right\\|$.\n\nThe problem specifies using the noncommutative matrix Bernstein inequality. The random vectors $x_i$ are subgaussian, but this does not imply that the norm $\\|x_i\\|$ is bounded. Consequently, the matrices $Y_i$ are not bounded in norm, as $\\|Y_i\\| = \\max(\\|x_i\\|^2 - 1, 1)$. This suggests that a version of the matrix Bernstein inequality suitable for unbounded random matrices is required. The appropriate tool is the matrix Bernstein inequality for sub-exponential random matrices.\n\nLet $S_m = \\sum_{i=1}^m Y_i$. A version of the matrix Bernstein inequality for a sum of independent, zero-mean, self-adjoint, sub-exponential random matrices $Y_i$ states:\n$$ \\mathbb{P}(\\|S_m\\| \\ge t) \\le 2n \\exp \\left( -c \\min\\left( \\frac{t^2}{\\nu}, \\frac{t}{L} \\right) \\right) $$\nwhere $c$ is a universal positive constant, $n$ is the dimension of the matrices, and the parameters $\\nu$ and $L$ are defined as:\n- $\\nu = \\left\\| \\sum_{i=1}^m \\mathbb{E}[Y_i^2] \\right\\|$ (the matrix variance parameter).\n- $L = \\max_{i} \\|Y_i\\|_{\\psi_1}$ (the maximum sub-exponential norm).\n\nWe need to estimate $\\nu$ and $L$.\n\n**Estimation of the variance parameter $\\nu$:**\nSince the $Y_i$ are i.i.d., $\\nu = m \\|\\mathbb{E}[Y_1^2]\\|$.\n$Y_1^2 = (x_1 x_1^\\top - I_n)^2 = (x_1 x_1^\\top)^2 - 2x_1 x_1^\\top + I_n = \\|x_1\\|^2 x_1 x_1^\\top - 2x_1 x_1^\\top + I_n = (\\|x_1\\|^2-2)x_1 x_1^\\top + I_n$.\nTaking the expectation, $\\mathbb{E}[Y_1^2] = \\mathbb{E}[(\\|x_1\\|^2-2)x_1 x_1^\\top] + I_n$.\nFor an isotropic vector $x_1$, we have $\\mathbb{E}[f(\\|x_1\\|^2)x_1 x_1^\\top] = \\left(\\frac{1}{n}\\mathbb{E}[f(\\|x_1\\|^2)\\|x_1\\|^2]\\right)I_n$.\nHere, $f(t) = t-2$. So, $\\mathbb{E}[Y_1^2] = \\left(\\frac{1}{n}\\mathbb{E}[(\\|x_1\\|^2-2)\\|x_1\\|^2]\\right)I_n + I_n = \\left(\\frac{1}{n}(\\mathbb{E}[\\|x_1\\|^4] - 2\\mathbb{E}[\\|x_1\\|^2])\\right)I_n + I_n$.\nWe know $\\mathbb{E}[\\|x_1\\|^2] = \\text{Tr}(\\mathbb{E}[x_1 x_1^\\top]) = \\text{Tr}(I_n) = n$.\nFor a subgaussian vector $x_1$ with $\\sup_{u \\in S^{n-1}}\\|\\langle x_1,u\\rangle\\|_{\\psi_2} \\leq 1$, standard moment bounds give $\\mathbb{E}[\\|x_1\\|^4] \\le C_1 n^2$ for some universal constant $C_1$.\nThus, $\\mathbb{E}[Y_1^2] \\approx \\left(\\frac{1}{n}(C_1 n^2 - 2n)\\right)I_n + I_n = (C_1 n - 2 + 1)I_n = (C_1 n-1)I_n$.\nThe norm is $\\|\\mathbb{E}[Y_1^2]\\| = C_1 n - 1$. So, $\\nu = m(C_1 n-1)$. For large $n$, we have $\\nu \\approx C_v m n$ for some constant $C_v$.\n\n**Estimation of the sub-exponential parameter $L$:**\nThe sub-exponential norm of $Y_1$ is $L = \\|Y_1\\|_{\\psi_1}$.\n$\\|Y_1\\| = \\max(\\|x_1\\|^2 - 1, 1)$. For large $\\|x_1\\|$, this is approximately $\\|x_1\\|^2$.\nThe sub-exponential norm of $Y_1$ is driven by the sub-exponential norm of $\\|x_1\\|^2$. For an isotropic random vector $x_1$ with $\\sup_{u \\in S^{n-1}}\\|\\langle x_1,u\\rangle\\|_{\\psi_2} \\leq 1$, it can be shown that $\\|\\,\\|x_1\\|^2 - n \\,\\|_{\\psi_1} \\le C_2 n$ for some universal constant $C_2$.\nTherefore, $L = \\|Y_1\\|_{\\psi_1} \\approx \\|\\|x_1\\|^2\\|_{\\psi_1} \\le \\|\\,\\|x_1\\|^2 - n\\,\\|_{\\psi_1} + \\|n\\|_{\\psi_1} \\le C_2 n + n/\\ln(2)$.\nSo, $L \\approx C_L n$ for some constant $C_L$.\n\n**Applying the inequality:**\nWe are interested in the event $\\|\\frac{1}{m}S_m\\| > \\delta$, which is $\\|S_m\\| > m\\delta$. We set $t = m\\delta$.\nThe exponent in the Bernstein inequality becomes:\n$$ -c \\min\\left( \\frac{(m\\delta)^2}{\\nu}, \\frac{m\\delta}{L} \\right) \\approx -c \\min\\left( \\frac{m^2\\delta^2}{C_v mn}, \\frac{m\\delta}{C_L n} \\right) = -c \\frac{m}{n} \\min\\left( \\frac{\\delta^2}{C_v}, \\frac{\\delta}{C_L} \\right) $$\nLet's combine the universal constants $c, C_v, C_L$ into new positive constants $c', c''$. The exponent is $- \\frac{m}{n} \\min(c'\\delta^2, c''\\delta)$.\nFor $\\delta \\in (0,1)$, we have $\\delta^2 < \\delta$. Assuming the constants $c'$ and $c''$ are of similar magnitude, the term $c'\\delta^2$ will be smaller. This corresponds to the typical \"sub-gaussian\" decay rate. The bound simplifies to:\n$$ \\mathbb{P}(\\|A^\\top A - I_n\\| > \\delta) \\le 2n \\exp\\left(-C_0 \\frac{m\\delta^2}{n}\\right) $$\nfor some universal constant $C_0 > 0$.\n\n**Computing the sufficient value of $m$:**\nWe want this probability to be at most $\\eta$:\n$$ 2n \\exp\\left(-C_0 \\frac{m\\delta^2}{n}\\right) \\le \\eta $$\n$$ \\exp\\left(-C_0 \\frac{m\\delta^2}{n}\\right) \\le \\frac{\\eta}{2n} $$\nTaking the natural logarithm of both sides:\n$$ -C_0 \\frac{m\\delta^2}{n} \\le \\ln\\left(\\frac{\\eta}{2n}\\right) = -\\ln\\left(\\frac{2n}{\\eta}\\right) $$\nMultiplying by $-1$ reverses the inequality:\n$$ C_0 \\frac{m\\delta^2}{n} \\ge \\ln\\left(\\frac{2n}{\\eta}\\right) $$\nSolving for $m$, we find a sufficient condition:\n$$ m \\ge \\frac{n}{C_0 \\delta^2} \\ln\\left(\\frac{2n}{\\eta}\\right) $$\nSince the problem allows treating absolute numerical constants as a single universal constant, we can define $C = 1/C_0$. This gives the final expression for a sufficient $m$.\n\n$$ m \\ge C\\frac{n}{\\delta^2}\\ln\\left(\\frac{2n}{\\eta}\\right) $$\nwhere $C$ is a positive universal constant that does not depend on $n, m, \\delta,$ or $\\eta$. A sufficient value for $m$ is therefore given by this expression, for some such constant $C$.", "answer": "$$\\boxed{C\\frac{n}{\\delta^2}\\ln\\left(\\frac{2n}{\\eta}\\right)}$$", "id": "3472187"}, {"introduction": "While matrices with i.i.d. entries are theoretically convenient, many practical applications rely on structured matrices, such as the partial Fourier matrix used in MRI. This practice shifts our focus to analyzing such structured ensembles, using the concept of mutual coherence—a measure of the worst-case correlation between columns. Starting from first principles, you will use scalar concentration inequalities and a union bound to derive a high-probability bound on the mutual coherence, connecting it to conditions for exact sparse recovery [@problem_id:3472188]. This exercise highlights how different tools are needed for structured matrices and provides a contrast to the analysis of i.i.d. ensembles.", "problem": "Consider the $n \\times n$ unitary Discrete Fourier Transform (DFT) matrix $F$ with entries $F_{t,k} = n^{-1/2} \\exp\\!\\big(2\\pi \\mathrm{i}\\, tk/n\\big)$ for $t,k \\in \\{0,\\dots,n-1\\}$. Construct a partial Fourier sensing matrix $A \\in \\mathbb{C}^{m \\times n}$ by sampling $m$ rows uniformly at random without replacement from $F$ and then normalizing each column of the resulting submatrix to have unit $\\ell_{2}$-norm. The mutual coherence of $A$ is defined as $\\mu(A) = \\max_{j \\neq k} |\\langle a_{j}, a_{k} \\rangle|$, where $a_{j}$ denotes the $j$-th column of $A$.\n\nStarting only from the definition of mutual coherence, the bounded orthonormal system property for the Fourier system (i.e., entries bounded in magnitude by $1$ after appropriate normalization), and standard concentration for bounded random variables, derive a high-probability upper bound on $\\mu(A)$ that holds with failure probability at most $n^{-2}$. Explicitly carry out the probabilistic reasoning using concentration of measure and a union bound over all distinct column pairs, ensuring that constants are tracked. Then, based on this bound and classical coherence-based exact recovery conditions for $\\ell_{1}$-minimization (Basis Pursuit (BP)) and Orthogonal Matching Pursuit (OMP), explain what scaling of the largest sparsity level $k$ can be certified as a function of $m$ and $n$.\n\nYour final answer must be the explicit high-probability upper bound for $\\mu(A)$ as a single closed-form expression in terms of $m$ and $n$. No numerical substitution is required, and no rounding is requested.", "solution": "The problem asks for a high-probability upper bound on the mutual coherence $\\mu(A)$ of a partial Fourier sensing matrix $A$, and an analysis of the implications for sparse signal recovery. The derivation must start from first principles using concentration of measure.\n\nThe problem statement is validated as follows:\n1.  **Givens Extraction:**\n    -   $F \\in \\mathbb{C}^{n \\times n}$ is the unitary Discrete Fourier Transform (DFT) matrix with entries $F_{t,k} = n^{-1/2} \\exp(2\\pi \\mathrm{i}\\, tk/n)$ for $t,k \\in \\{0,\\dots,n-1\\}$.\n    -   $A \\in \\mathbb{C}^{m \\times n}$ is constructed by sampling $m$ rows of $F$ uniformly at random without replacement, and then normalizing each column of the resulting submatrix to have unit $\\ell_2$-norm.\n    -   Mutual coherence is $\\mu(A) = \\max_{j \\neq k} |\\langle a_{j}, a_{k} \\rangle|$, where $a_j$ is the $j$-th column of $A$.\n    -   The derivation must use the definition of mutual coherence, the bounded orthonormal system property of the Fourier system, and standard concentration inequalities for bounded random variables.\n    -   The target failure probability for the bound is $n^{-2}$.\n    -   Finally, the scaling of the recoverable sparsity level $k$ for Basis Pursuit (BP) and Orthogonal Matching Pursuit (OMP) must be explained based on the derived bound.\n\n2.  **Validation:**\n    -   The problem is scientifically grounded in the fields of compressed sensing and random matrix theory. All concepts (DFT, mutual coherence, concentration of measure) are well-defined and standard.\n    -   The problem is well-posed and objective, specifying the goal, methodology, and constraints clearly.\n    -   There are no scientific or factual errors, contradictions, missing information, or unrealistic assumptions. The construction of the sensing matrix and the subsequent analysis are standard procedures in the literature.\n\n3.  **Verdict:** The problem is valid.\n\nWe now proceed with the solution.\n\nFirst, let us characterize the columns of the matrix $A$. The $k$-th column of the DFT matrix $F$, denoted by $f_k$, has entries $(f_k)_t = F_{t,k} = n^{-1/2} \\exp(2\\pi \\mathrm{i}\\, tk/n)$. The columns of $F$ form an orthonormal basis for $\\mathbb{C}^n$. The magnitude of each entry is $|F_{t,k}| = n^{-1/2}$.\n\nThe matrix $A$ is formed by first selecting a random subset of $m$ row indices, $\\Omega \\subset \\{0, \\dots, n-1\\}$, where $|\\Omega|=m$. Let the resulting $m \\times n$ submatrix be $A'$. The $j$-th column of $A'$, denoted $a'_j$, consists of the entries of $f_j$ corresponding to the indices in $\\Omega$.\nThe squared $\\ell_2$-norm of $a'_j$ is:\n$$\n\\|a'_j\\|_2^2 = \\sum_{t \\in \\Omega} |F_{t,j}|^2 = \\sum_{t \\in \\Omega} |n^{-1/2}|^2 = \\sum_{t \\in \\Omega} \\frac{1}{n} = \\frac{m}{n}\n$$\nThis norm is deterministic and independent of the column index $j$ and the specific choice of $\\Omega$ (as long as $|\\Omega|=m$). The normalization factor for each column is $\\|a'_j\\|_2 = \\sqrt{m/n}$.\nThe columns $a_j$ of the final matrix $A$ are thus given by:\n$$\na_j = \\frac{a'_j}{\\|a'_j\\|_2} = \\frac{a'_j}{\\sqrt{m/n}} = \\sqrt{\\frac{n}{m}} a'_j\n$$\n\nNext, we analyze the mutual coherence $\\mu(A)$, which requires computing the inner products $\\langle a_j, a_k \\rangle$ for $j \\neq k$.\n$$\n\\langle a_j, a_k \\rangle = a_k^* a_j = \\left(\\sqrt{\\frac{n}{m}} a'_k\\right)^* \\left(\\sqrt{\\frac{n}{m}} a'_j\\right) = \\frac{n}{m} (a'_k)^* a'_j\n$$\nThe inner product $(a'_k)^* a'_j$ is computed as:\n$$\n(a'_k)^* a'_j = \\sum_{t \\in \\Omega} \\overline{(a'_k)_t} (a'_j)_t = \\sum_{t \\in \\Omega} \\overline{F_{t,k}} F_{t,j}\n$$\nSubstituting the expressions for the DFT matrix entries:\n$$\n\\overline{F_{t,k}} F_{t,j} = \\left( n^{-1/2} \\exp(-2\\pi \\mathrm{i}\\, tk/n) \\right) \\left( n^{-1/2} \\exp(2\\pi \\mathrm{i}\\, tj/n) \\right) = \\frac{1}{n} \\exp\\left(\\frac{2\\pi \\mathrm{i}\\, t(j-k)}{n}\\right)\n$$\nTherefore, the inner product is:\n$$\n\\langle a_j, a_k \\rangle = \\frac{n}{m} \\sum_{t \\in \\Omega} \\frac{1}{n} \\exp\\left(\\frac{2\\pi \\mathrm{i}\\, t(j-k)}{n}\\right) = \\frac{1}{m} \\sum_{t \\in \\Omega} \\exp\\left(\\frac{2\\pi \\mathrm{i}\\, t(j-k)}{n}\\right)\n$$\nFor a fixed pair $(j,k)$ with $j \\neq k$, this inner product is a complex-valued random variable, let's call it $Y_{j,k}$, because the set $\\Omega$ is random. We need to find a high-probability bound on $|Y_{j,k}|$. We begin by computing its expectation. The sampling is uniform without replacement, so the probability of any index $t$ being in $\\Omega$ is $\\mathbb{P}(t \\in \\Omega) = m/n$.\n$$\n\\mathbb{E}[Y_{j,k}] = \\frac{1}{m} \\mathbb{E}\\left[ \\sum_{t=0}^{n-1} \\mathbf{1}_{t \\in \\Omega} \\exp\\left(\\frac{2\\pi \\mathrm{i}\\, t(j-k)}{n}\\right) \\right] = \\frac{1}{m} \\sum_{t=0}^{n-1} \\mathbb{E}[\\mathbf{1}_{t \\in \\Omega}] \\exp\\left(\\frac{2\\pi \\mathrm{i}\\, t(j-k)}{n}\\right)\n$$\n$$\n\\mathbb{E}[Y_{j,k}] = \\frac{1}{m} \\sum_{t=0}^{n-1} \\frac{m}{n} \\exp\\left(\\frac{2\\pi \\mathrm{i}\\, t(j-k)}{n}\\right) = \\frac{1}{n} \\sum_{t=0}^{n-1} \\exp\\left(\\frac{2\\pi \\mathrm{i}\\, t(j-k)}{n}\\right)\n$$\nThis is a geometric sum of all $n$-th roots of unity, which evaluates to $0$ since $j-k$ is a non-zero integer between $-(n-1)$ and $n-1$ and thus not a multiple of $n$. So, $\\mathbb{E}[Y_{j,k}] = 0$.\n\nWe now use a concentration inequality to bound the deviation of $Y_{j,k}$ from its mean. Let $Y_{j,k} = R_{j,k} + \\mathrm{i} I_{j,k}$, where\n$$\nR_{j,k} = \\frac{1}{m} \\sum_{t \\in \\Omega} \\cos\\left(\\frac{2\\pi t(j-k)}{n}\\right) \\quad \\text{and} \\quad I_{j,k} = \\frac{1}{m} \\sum_{t \\in \\Omega} \\sin\\left(\\frac{2\\pi t(j-k)}{n}\\right)\n$$\nBoth $R_{j,k}$ and $I_{j,k}$ are sample means of $m$ items drawn without replacement from populations of size $n$. The population for $R_{j,k}$ is $\\{\\cos(2\\pi t(j-k)/n)\\}_{t=0}^{n-1}$. Its mean is $0$, and its values are bounded in $[-1, 1]$. We use Hoeffding's inequality for a sample mean $\\bar{X}$ from sampling without replacement, which states that for a population bounded in $[a,b]$, $\\mathbb{P}(|\\bar{X} - \\mathbb{E}[\\bar{X}]| \\ge \\epsilon) \\le 2\\exp(-2m\\epsilon^2 / (b-a)^2)$.\nFor $R_{j,k}$, we have $\\mathbb{E}[R_{j,k}]=0$ and the range is $[-1, 1]$, so $b-a=2$.\n$$\n\\mathbb{P}(|R_{j,k}| \\ge \\epsilon) \\le 2\\exp\\left(\\frac{-2m\\epsilon^2}{2^2}\\right) = 2\\exp\\left(-\\frac{m\\epsilon^2}{2}\\right)\n$$\nThe same bound applies to $I_{j,k}$. We wish to bound $|Y_{j,k}| = \\sqrt{R_{j,k}^2 + I_{j,k}^2}$. The event $|Y_{j,k}| \\ge \\delta$ implies that $|R_{j,k}| \\ge \\delta/\\sqrt{2}$ or $|I_{j,k}| \\ge \\delta/\\sqrt{2}$. Using a union bound:\n$$\n\\mathbb{P}(|Y_{j,k}| \\ge \\delta) \\le \\mathbb{P}\\left(|R_{j,k}| \\ge \\frac{\\delta}{\\sqrt{2}}\\right) + \\mathbb{P}\\left(|I_{j,k}| \\ge \\frac{\\delta}{\\sqrt{2}}\\right)\n$$\nApplying the Hoeffding bound with $\\epsilon = \\delta/\\sqrt{2}$:\n$$\n\\mathbb{P}(|Y_{j,k}| \\ge \\delta) \\le 2\\exp\\left(-\\frac{m(\\delta/\\sqrt{2})^2}{2}\\right) + 2\\exp\\left(-\\frac{m(\\delta/\\sqrt{2})^2}{2}\\right) = 4\\exp\\left(-\\frac{m\\delta^2}{4}\\right)\n$$\nThis bounds the probability of large coherence for a single pair of columns. To bound the mutual coherence $\\mu(A) = \\max_{j \\neq k} |Y_{j,k}|$, we take a union bound over all distinct pairs of columns. There are $\\binom{n}{2} = n(n-1)/2$ such pairs.\n$$\n\\mathbb{P}(\\mu(A) \\ge \\delta) = \\mathbb{P}\\left(\\bigcup_{j<k} \\{|Y_{j,k}| \\ge \\delta\\}\\right) \\le \\sum_{j<k} \\mathbb{P}(|Y_{j,k}| \\ge \\delta)\n$$\n$$\n\\mathbb{P}(\\mu(A) \\ge \\delta) \\le \\frac{n(n-1)}{2} \\cdot 4\\exp\\left(-\\frac{m\\delta^2}{4}\\right) = 2n(n-1)\\exp\\left(-\\frac{m\\delta^2}{4}\\right)\n$$\nUsing the looser inequality $n(n-1) < n^2$, we have:\n$$\n\\mathbb{P}(\\mu(A) \\ge \\delta) < 2n^2\\exp\\left(-\\frac{m\\delta^2}{4}\\right)\n$$\nWe require this failure probability to be at most $n^{-2}$. We set the bound equal to the target failure probability and solve for $\\delta$:\n$$\n2n^2\\exp\\left(-\\frac{m\\delta^2}{4}\\right) = n^{-2}\n$$\n$$\n\\exp\\left(-\\frac{m\\delta^2}{4}\\right) = \\frac{1}{2n^4}\n$$\nTaking the natural logarithm of both sides:\n$$\n-\\frac{m\\delta^2}{4} = \\ln\\left(\\frac{1}{2n^4}\\right) = -\\ln(2n^4) = -(\\ln 2 + 4\\ln n)\n$$\n$$\n\\delta^2 = \\frac{4(\\ln 2 + 4\\ln n)}{m}\n$$\n$$\n\\delta = 2\\sqrt{\\frac{4\\ln n + \\ln 2}{m}}\n$$\nThus, with probability at least $1-n^{-2}$, the mutual coherence is bounded by $\\mu(A) \\le 2\\sqrt{\\frac{4\\ln n + \\ln 2}{m}}$.\n\nFinally, we analyze the implications for sparse recovery. Classical results in compressed sensing provide sufficient conditions for exact recovery based on mutual coherence.\nFor both Basis Pursuit ($\\ell_1$-minimization) and Orthogonal Matching Pursuit (OMP), a sufficient condition for the exact recovery of any $k$-sparse signal $x$ from measurements $y=Ax$ is:\n$$\n\\mu(A) < \\frac{1}{2k-1}\n$$\nCombining this with our high-probability bound on $\\mu(A)$, we can certify recovery for sparsity levels $k$ that satisfy:\n$$\n2\\sqrt{\\frac{4\\ln n + \\ln 2}{m}} < \\frac{1}{2k-1}\n$$\nTo find the scaling of $k$, we solve for $k$:\n$$\n2k-1 < \\frac{1}{2\\sqrt{\\frac{4\\ln n + \\ln 2}{m}}} = \\frac{1}{2}\\sqrt{\\frac{m}{4\\ln n + \\ln 2}}\n$$\n$$\n2k < 1 + \\frac{1}{2}\\sqrt{\\frac{m}{4\\ln n + \\ln 2}}\n$$\n$$\nk < \\frac{1}{2} + \\frac{1}{4}\\sqrt{\\frac{m}{4\\ln n + \\ln 2}}\n$$\nFor large $m$ and $n$, the leading term dominates, and $\\ln 2$ is negligible compared to $4\\ln n$. The scaling is therefore:\n$$\nk = O\\left(\\sqrt{\\frac{m}{\\ln n}}\\right)\n$$\nThis means that for a partial Fourier matrix, both BP and OMP can provably recover sparse signals with a sparsity level $k$ that scales as the square root of the number of measurements $m$, divided by a logarithmic factor in the ambient dimension $n$. This is a standard result in compressed sensing, showing that one can recover sparse signals from a small number of measurements ($m \\ll n$) as long as the sparsity is not too high.", "answer": "$$\\boxed{2\\sqrt{\\frac{4\\ln(n) + \\ln(2)}{m}}}$$", "id": "3472188"}, {"introduction": "The powerful concentration results we often rely on are predicated on the assumption of subgaussianity, which implies that the tails of the random variables decay very quickly. This final practice challenges that assumption by exploring a scenario with heavy-tailed random variables, which are common in fields like finance and signal processing. Using the Paley-Zygmund inequality—a robust tool based on the second moment method—you will compute a non-trivial lower bound on the probability that a quadratic form avoids deviating too far below its mean [@problem_id:3472217]. This demonstrates how to obtain meaningful, albeit weaker, guarantees when the usual assumptions for strong concentration do not hold.", "problem": "Consider a random vector $X \\in \\mathbb{R}^{5}$ with independent and identically distributed (i.i.d.) entries, each defined as $X_{i} = \\sqrt{\\frac{\\nu - 2}{\\nu}}\\,T$, where $T$ has the Student’s $t$ distribution with $\\nu = 6$ degrees of freedom. Let $A \\in \\mathbb{R}^{5 \\times 5}$ be the diagonal positive semidefinite matrix with diagonal entries $3$, $2$, $1$, $1$, and $0.5$. Define the nonnegative quadratic form $Y = X^{\\top} A X$.\n\nStarting from fundamental facts about the moments of the Student’s $t$ distribution and the independence structure of $X$, derive expressions for $\\mathbb{E}[Y]$ and $\\mathbb{E}[Y^{2}]$ in terms of the diagonal entries of $A$ and the second and fourth moments of $X_{i}$. Then, using the Paley–Zygmund inequality and the choice $\\theta = 0.8$, obtain a lower bound on the probability $\\mathbb{P}\\!\\left(Y \\ge \\theta\\,\\mathbb{E}[Y]\\right)$.\n\nYour final answer must be the numerical value of this Paley–Zygmund lower bound. Round your answer to $4$ significant figures. No units are required.\n\nIn your derivation, explicitly justify why $Y$ is nonnegative and why the moments you use are finite. Briefly comment on how this lower-tail bound reflects robustness for heavy-tailed ensembles in the context of compressed sensing concentration phenomena, without relying on any unproven or ad hoc assumptions.", "solution": "The problem requires the calculation of a lower bound on the probability $\\mathbb{P}\\!\\left(Y \\ge \\theta\\,\\mathbb{E}[Y]\\right)$ for a quadratic form $Y = X^{\\top} A X$. The vector $X \\in \\mathbb{R}^{5}$ has independent and identically distributed (i.i.d.) entries drawn from a scaled Student's $t$ distribution, which is a heavy-tailed distribution. We will use the Paley-Zygmund inequality, which necessitates the computation of the first two moments of $Y$.\n\nFirst, we validate the nonnegativity of $Y$ and the existence of the required moments.\nThe quadratic form is $Y = X^{\\top} A X$. The matrix $A$ is a diagonal matrix with diagonal entries $a_i \\in \\{3, 2, 1, 1, 0.5\\}$. A diagonal matrix is positive semidefinite if and only if all of its diagonal entries are nonnegative. Since all $a_i \\ge 0$, $A$ is positive semidefinite. Therefore, the quadratic form $Y = X^{\\top} A X = \\sum_{i=1}^5 a_i X_i^2$ is a sum of nonnegative terms and is thus itself a nonnegative random variable, $Y \\ge 0$.\n\nThe entries of the random vector $X$ are given by $X_i = \\sqrt{\\frac{\\nu-2}{\\nu}} T$, where $T$ has a Student's $t$ distribution with $\\nu=6$ degrees of freedom. The $k$-th moment of a Student's $t$ distribution, $\\mathbb{E}[T^k]$, is finite if and only if $k < \\nu$. To compute $\\mathbb{E}[Y]$ and $\\mathbb{E}[Y^2]$, we will need up to the fourth moment of $X_i$, which in turn require the second and fourth moments of $T$. Since $\\nu=6$, the moments $\\mathbb{E}[T^2]$ and $\\mathbb{E}[T^4]$ are finite because $2 < 6$ and $4 < 6$.\n\nLet's compute the moments of $X_i$. For a Student's $t$ distribution with $\\nu > 2$, the mean is $\\mathbb{E}[T]=0$ and the variance is $\\text{Var}(T) = \\mathbb{E}[T^2] = \\frac{\\nu}{\\nu-2}$.\nFor $\\nu=6$, the second moment of $T$ is:\n$$ \\mathbb{E}[T^2] = \\frac{6}{6-2} = \\frac{6}{4} = \\frac{3}{2} $$\nThe fourth moment for $\\nu > 4$ is given by $\\mathbb{E}[T^4] = \\frac{3\\nu^2}{(\\nu-2)(\\nu-4)}$.\nFor $\\nu=6$, the fourth moment of $T$ is:\n$$ \\mathbb{E}[T^4] = \\frac{3(6^2)}{(6-2)(6-4)} = \\frac{3 \\cdot 36}{4 \\cdot 2} = \\frac{108}{8} = \\frac{27}{2} $$\nThe entries $X_i$ are scaled versions of $T$, $X_i = cT$ with $c = \\sqrt{\\frac{\\nu-2}{\\nu}} = \\sqrt{\\frac{4}{6}} = \\sqrt{\\frac{2}{3}}$.\nThe second moment of $X_i$, denoted $\\mu_2$, is:\n$$ \\mu_2 = \\mathbb{E}[X_i^2] = \\mathbb{E}\\left[\\left(\\sqrt{\\frac{\\nu-2}{\\nu}}T\\right)^2\\right] = \\frac{\\nu-2}{\\nu} \\mathbb{E}[T^2] = \\frac{4}{6} \\cdot \\frac{3}{2} = 1 $$\nThis confirms the scaling factor is chosen to normalize the variance of $X_i$ to $1$.\nThe fourth moment of $X_i$, denoted $\\mu_4$, is:\n$$ \\mu_4 = \\mathbb{E}[X_i^4] = \\mathbb{E}\\left[\\left(\\sqrt{\\frac{\\nu-2}{\\nu}}T\\right)^4\\right] = \\left(\\frac{\\nu-2}{\\nu}\\right)^2 \\mathbb{E}[T^4] = \\left(\\frac{4}{6}\\right)^2 \\cdot \\frac{27}{2} = \\left(\\frac{2}{3}\\right)^2 \\cdot \\frac{27}{2} = \\frac{4}{9} \\cdot \\frac{27}{2} = 6 $$\nNow we derive expressions for $\\mathbb{E}[Y]$ and $\\mathbb{E}[Y^2]$.\nThe quadratic form is $Y = \\sum_{i=1}^5 a_i X_i^2$. By linearity of expectation:\n$$ \\mathbb{E}[Y] = \\mathbb{E}\\left[\\sum_{i=1}^5 a_i X_i^2\\right] = \\sum_{i=1}^5 a_i \\mathbb{E}[X_i^2] = \\mu_2 \\sum_{i=1}^5 a_i $$\nThe diagonal entries of $A$ are $a_1=3$, $a_2=2$, $a_3=1$, $a_4=1$, and $a_5=0.5$.\n$$ \\sum_{i=1}^5 a_i = 3 + 2 + 1 + 1 + 0.5 = 7.5 $$\nThus, the expected value of $Y$ is:\n$$ \\mathbb{E}[Y] = 1 \\cdot 7.5 = 7.5 $$\nNext, we find the second moment of $Y$:\n$$ \\mathbb{E}[Y^2] = \\mathbb{E}\\left[\\left(\\sum_{i=1}^5 a_i X_i^2\\right)^2\\right] = \\mathbb{E}\\left[\\sum_{i=1}^5 \\sum_{j=1}^5 a_i a_j X_i^2 X_j^2\\right] $$\nBy linearity of expectation:\n$$ \\mathbb{E}[Y^2] = \\sum_{i=1}^5 \\sum_{j=1}^5 a_i a_j \\mathbb{E}[X_i^2 X_j^2] $$\nWe split the sum into two cases: $i=j$ and $i \\ne j$.\n$$ \\mathbb{E}[Y^2] = \\sum_{i=1}^5 a_i^2 \\mathbb{E}[X_i^4] + \\sum_{i \\ne j} a_i a_j \\mathbb{E}[X_i^2 X_j^2] $$\nSince the entries $X_i$ are independent, for $i \\ne j$, $\\mathbb{E}[X_i^2 X_j^2] = \\mathbb{E}[X_i^2]\\mathbb{E}[X_j^2] = \\mu_2^2$.\n$$ \\mathbb{E}[Y^2] = \\sum_{i=1}^5 a_i^2 \\mu_4 + \\sum_{i \\ne j} a_i a_j \\mu_2^2 $$\nWe can express the second term using $(\\sum a_i)^2 = \\sum a_i^2 + \\sum_{i \\ne j} a_i a_j$.\n$$ \\mathbb{E}[Y^2] = \\mu_4 \\sum_{i=1}^5 a_i^2 + \\mu_2^2 \\left[ \\left(\\sum_{i=1}^5 a_i\\right)^2 - \\sum_{i=1}^5 a_i^2 \\right] = (\\mu_4 - \\mu_2^2)\\sum_{i=1}^5 a_i^2 + \\mu_2^2 \\left(\\sum_{i=1}^5 a_i\\right)^2 $$\nWe need the sum of squares of the diagonal entries of $A$:\n$$ \\sum_{i=1}^5 a_i^2 = 3^2 + 2^2 + 1^2 + 1^2 + (0.5)^2 = 9 + 4 + 1 + 1 + 0.25 = 15.25 $$\nSubstituting the values $\\mu_2=1$, $\\mu_4=6$, $\\sum a_i = 7.5$, and $\\sum a_i^2 = 15.25$:\n$$ \\mathbb{E}[Y^2] = (6 - 1^2) (15.25) + 1^2 (7.5)^2 = 5 \\cdot 15.25 + 56.25 = 76.25 + 56.25 = 132.5 $$\nThe Paley-Zygmund inequality for a nonnegative random variable $Z$ and $\\theta \\in [0, 1)$ is:\n$$ \\mathbb{P}(Z \\ge \\theta\\,\\mathbb{E}[Z]) \\ge (1-\\theta)^2 \\frac{(\\mathbb{E}[Z])^2}{\\mathbb{E}[Z^2]} $$\nWe apply this inequality to $Y$ with $\\theta = 0.8$:\n$$ \\mathbb{P}(Y \\ge 0.8\\,\\mathbb{E}[Y]) \\ge (1-0.8)^2 \\frac{(\\mathbb{E}[Y])^2}{\\mathbb{E}[Y^2]} $$\nSubstituting the calculated moments $\\mathbb{E}[Y]=7.5$ and $\\mathbb{E}[Y^2]=132.5$:\n$$ \\text{Lower Bound} = (0.2)^2 \\frac{(7.5)^2}{132.5} = 0.04 \\frac{56.25}{132.5} $$\nLet's simplify the fraction:\n$$ \\frac{56.25}{132.5} = \\frac{5625}{13250} = \\frac{1125}{2650} = \\frac{225}{530} = \\frac{45}{106} $$\nThe lower bound is:\n$$ \\text{Lower Bound} = 0.04 \\cdot \\frac{45}{106} = \\frac{4}{100} \\cdot \\frac{45}{106} = \\frac{1}{25} \\cdot \\frac{45}{106} = \\frac{9}{5 \\cdot 106} = \\frac{9}{530} $$\nAs a decimal, this is $9 \\div 530 \\approx 0.01698113...$. Rounding to 4 significant figures gives $0.01698$.\n\nCommentary: In compressed sensing and related fields, performance guarantees often depend on the concentration of random quantities, such as quadratic forms of random vectors. When the underlying random ensemble has Gaussian or sub-gaussian entries, powerful concentration inequalities provide sharp bounds. However, for heavy-tailed ensembles, such as the Student's $t$ distribution used here, these inequalities are not applicable. The Paley-Zygmund inequality, a second-moment method, provides a robust alternative. Although the resulting bound may not be as tight as those for lighter-tailed distributions, its major advantage is its broad applicability, requiring only finite first and second moments. The calculated lower bound $0.01698$ is a concrete, non-trivial floor on the probability that $Y$ is not unusually small. This type of lower-tail guarantee is critical for ensuring the stability and reliability of signal processing algorithms, as it bounds the probability of catastrophic information loss that could occur if key random variables deviate too far below their expected values.", "answer": "$$\\boxed{0.01698}$$", "id": "3472217"}]}