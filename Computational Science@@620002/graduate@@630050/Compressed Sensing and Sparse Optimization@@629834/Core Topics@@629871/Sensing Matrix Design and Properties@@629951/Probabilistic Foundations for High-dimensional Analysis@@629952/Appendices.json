{"hands_on_practices": [{"introduction": "Concentration inequalities are a cornerstone of high-dimensional analysis, providing guarantees that complex random quantities are tightly clustered around their expectation. This first practice provides a direct application of one of the most versatile tools, McDiarmid's bounded differences inequality. You will use it to analyze the behavior of a random projection matrix, a fundamental object in dimensionality reduction and compressed sensing, by bounding the deviation of its empirical covariance from the identity [@problem_id:3468743].", "problem": "Consider a random matrix $A \\in \\mathbb{R}^{m \\times d}$ whose rows $a_1, \\dots, a_m \\in \\mathbb{R}^d$ are independent random vectors supported on the Euclidean ball of radius $R$, that is, $\\|a_i\\|_2 \\le R$ almost surely for each index $i \\in \\{1, \\dots, m\\}$. Define the normalized random projection matrix $\\Pi = \\frac{1}{\\sqrt{m}} A$. Let the function $f$ act on the independent inputs $(a_1, \\dots, a_m)$ by measuring the normalization error of the empirical covariance of $\\Pi$, namely\n$$\nf(a_1, \\dots, a_m) \\;=\\; \\left\\| \\Pi^{\\top} \\Pi - I_d \\right\\|_{F} \\;=\\; \\left\\| \\frac{1}{m} \\sum_{i=1}^{m} a_i a_i^{\\top} - I_d \\right\\|_{F},\n$$\nwhere $\\|\\cdot\\|_F$ denotes the Frobenius norm and $I_d$ denotes the $d \\times d$ identity matrix. For the function $f$, define the coordinate Lipschitz constant $c_i$ as any number satisfying the bounded differences property\n$$\n\\sup_{(a_1, \\dots, a_m),\\,(a_i')} \\left| f(a_1, \\dots, a_i, \\dots, a_m) - f(a_1, \\dots, a_i', \\dots, a_m) \\right| \\;\\le\\; c_i,\n$$\nwhere the supremum is taken over all admissible inputs $(a_1, \\dots, a_m)$ and modified coordinate $a_i'$ that also satisfy $\\|a_i\\|_2 \\le R$ and $\\|a_i'\\|_2 \\le R$. Starting only from the foundational definitions of independence, the Frobenius norm, and the bounded differences property (and invoking McDiarmid’s bounded differences inequality without restating it explicitly), perform the following:\n\n- Derive a valid set of coordinate Lipschitz constants $(c_1, \\dots, c_m)$ for the function $f$ under the stated support constraints on $a_i$.\n- Use these constants to obtain a high-probability upper deviation bound for $f$ about its expectation $\\mathbb{E}[f]$ of the form $\\mathbb{P}\\big( f \\le \\mathbb{E}[f] + t_{\\delta} \\big) \\ge 1 - \\delta$ for a target failure probability $\\delta \\in (0,1)$.\n- Express, in closed form, the smallest additive deviation $t_{\\delta}$ as an analytic expression in terms of $m$, $R$, and $\\delta$ that certifies the bound $\\mathbb{P}\\big( f \\le \\mathbb{E}[f] + t_{\\delta} \\big) \\ge 1 - \\delta$.\n\nYour final answer must be a single closed-form analytic expression for $t_{\\delta}$. No rounding is required, and there are no physical units involved.", "solution": "The problem statement will first be validated against the specified criteria.\n\n### Problem Validation\n\n#### Step 1: Extract Givens\n- A random matrix $A \\in \\mathbb{R}^{m \\times d}$ has rows $a_1, \\dots, a_m \\in \\mathbb{R}^d$ which are independent random vectors.\n- The support of each random vector $a_i$ is the Euclidean ball of radius $R$: $\\|a_i\\|_2 \\le R$ almost surely for each $i \\in \\{1, \\dots, m\\}$.\n- A normalized random projection matrix is defined as $\\Pi = \\frac{1}{\\sqrt{m}} A$.\n- A function $f$ is defined on the inputs $(a_1, \\dots, a_m)$ as:\n$$\nf(a_1, \\dots, a_m) = \\left\\| \\Pi^{\\top} \\Pi - I_d \\right\\|_{F} = \\left\\| \\frac{1}{m} \\sum_{i=1}^{m} a_i a_i^{\\top} - I_d \\right\\|_{F}\n$$\nwhere $\\|\\cdot\\|_F$ is the Frobenius norm and $I_d$ is the $d \\times d$ identity matrix.\n- The coordinate Lipschitz constant $c_i$ for the function $f$ must satisfy the bounded differences property:\n$$\n\\sup_{(a_1, \\dots, a_m),\\,(a_i')} \\left| f(a_1, \\dots, a_i, \\dots, a_m) - f(a_1, \\dots, a_i', \\dots, a_m) \\right| \\le c_i\n$$\nwhere the supremum is over all inputs satisfying $\\|a_j\\|_2 \\le R$ for all $j$ and $\\|a_i'\\|_2 \\le R$.\n- The task is to derive the constants $c_i$ and use them with McDiarmid’s inequality to find the smallest additive deviation $t_{\\delta}$ that certifies the high-probability bound $\\mathbb{P}\\big( f \\le \\mathbb{E}[f] + t_{\\delta} \\big) \\ge 1 - \\delta$ for $\\delta \\in (0,1)$.\n\n#### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in the fields of random matrix theory, high-dimensional probability, and linear algebra. The concepts of random projections, Frobenius norm, and concentration inequalities are standard and rigorously defined. The expression $\\left\\| \\frac{1}{m} \\sum_{i=1}^{m} a_i a_i^{\\top} - I_d \\right\\|_{F}$ represents the error of a sample covariance matrix relative to the identity, a central object of study in high-dimensional statistics and its applications, such as compressed sensing.\n- **Well-Posed**: The problem is well-posed. It asks for the derivation of a specific quantity ($t_{\\delta}$) based on a clearly defined function and set of constraints. The existence and uniqueness of the requested analytic expression are guaranteed by the structure of the problem.\n- **Objective**: The problem is stated using precise mathematical language and definitions. There are no subjective or ambiguous terms.\n- **Self-Contained and Consistent**: The problem provides all necessary definitions and constraints. The identity $\\Pi^{\\top} \\Pi = \\frac{1}{m} \\sum_{i=1}^{m} a_i a_i^{\\top}$ is a direct consequence of the definitions of $A$ and $\\Pi$, and is thus consistent.\n- **Other Flaws**: The problem does not violate any of the other invalidity criteria. It is not metaphorical, trivial, or unverifiable.\n\n#### Step 3: Verdict and Action\nThe problem is valid. I will now proceed with a complete, reasoned solution.\n\n### Solution Derivation\n\nThe goal is to find an expression for $t_{\\delta}$ in terms of $m$, $R$, and $\\delta$. This involves two main steps: first, deriving a set of coordinate Lipschitz constants $c_i$ for the function $f$, and second, applying McDiarmid's bounded differences inequality.\n\n**Step 1: Derive the Coordinate Lipschitz Constants $(c_1, \\dots, c_m)$**\n\nLet the set of input vectors be denoted by $\\mathbf{a} = (a_1, \\dots, a_m)$. Let $\\mathbf{a}'$ be an identical set of inputs, except for the $i$-th vector, which is replaced by $a_i'$. Both $a_i$ and $a_i'$ must satisfy the given support constraint, i.e., $\\|a_i\\|_2 \\le R$ and $\\|a_i'\\|_2 \\le R$.\n\nLet $M(\\mathbf{a}) = \\frac{1}{m} \\sum_{j=1}^{m} a_j a_j^{\\top} - I_d$. The function is $f(\\mathbf{a}) = \\|M(\\mathbf{a})\\|_F$. We need to bound the difference $|f(\\mathbf{a}) - f(\\mathbf{a}')|$.\nBy the reverse triangle inequality for the Frobenius norm, we have:\n$$\n|f(\\mathbf{a}) - f(\\mathbf{a}')| = \\big| \\|M(\\mathbf{a})\\|_F - \\|M(\\mathbf{a}')\\|_F \\big| \\le \\|M(\\mathbf{a}) - M(\\mathbf{a}')\\|_F\n$$\nThe difference of the matrices is:\n$$\nM(\\mathbf{a}) - M(\\mathbf{a}') = \\left(\\frac{1}{m} \\sum_{j=1}^{m} a_j a_j^{\\top} - I_d\\right) - \\left(\\frac{1}{m} \\left(\\sum_{j \\ne i} a_j a_j^{\\top} + a_i' a_i'^{\\top}\\right) - I_d\\right) = \\frac{1}{m} (a_i a_i^{\\top} - a_i' a_i'^{\\top})\n$$\nSubstituting this into the inequality:\n$$\n|f(\\mathbf{a}) - f(\\mathbf{a}')| \\le \\left\\| \\frac{1}{m} (a_i a_i^{\\top} - a_i' a_i'^{\\top}) \\right\\|_F = \\frac{1}{m} \\|a_i a_i^{\\top} - a_i' a_i'^{\\top}\\|_F\n$$\nUsing the triangle inequality for the Frobenius norm on the right-hand side:\n$$\n\\|a_i a_i^{\\top} - a_i' a_i'^{\\top}\\|_F \\le \\|a_i a_i^{\\top}\\|_F + \\|a_i' a_i'^{\\top}\\|_F\n$$\nNow, we evaluate the Frobenius norm of a rank-one matrix of the form $v v^{\\top}$ for a vector $v \\in \\mathbb{R}^d$. The squared Frobenius norm is the sum of the squares of all matrix entries. The entries of $v v^{\\top}$ are $(v v^{\\top})_{jk} = v_j v_k$.\n$$\n\\|v v^{\\top}\\|_F^2 = \\sum_{j=1}^{d} \\sum_{k=1}^{d} ((v v^{\\top})_{jk})^2 = \\sum_{j=1}^{d} \\sum_{k=1}^{d} (v_j v_k)^2 = \\left(\\sum_{j=1}^{d} v_j^2\\right) \\left(\\sum_{k=1}^{d} v_k^2\\right) = (\\|v\\|_2^2) (\\|v\\|_2^2) = \\|v\\|_2^4\n$$\nTaking the square root gives the identity $\\|v v^{\\top}\\|_F = \\|v\\|_2^2$.\n\nApplying this identity to our inequality:\n$$\n\\|a_i a_i^{\\top} - a_i' a_i'^{\\top}\\|_F \\le \\|a_i\\|_2^2 + \\|a_i'\\|_2^2\n$$\nUsing the given support constraints, $\\|a_i\\|_2 \\le R$ and $\\|a_i'\\|_2 \\le R$:\n$$\n\\|a_i a_i^{\\top} - a_i' a_i'^{\\top}\\|_F \\le R^2 + R^2 = 2R^2\n$$\nSubstituting this result back into the bound for the function difference:\n$$\n|f(\\mathbf{a}) - f(\\mathbf{a}')| \\le \\frac{1}{m} (2R^2) = \\frac{2R^2}{m}\n$$\nThis inequality holds for any choice of admissible inputs. The supremum of the left-hand side is therefore bounded by the constant on the right-hand side. This allows us to set the coordinate Lipschitz constants $c_i$ for all $i \\in \\{1, \\dots, m\\}$:\n$$\nc_i = \\frac{2R^2}{m}\n$$\n\n**Step 2: Obtain the High-Probability Bound using McDiarmid's Inequality**\n\nMcDiarmid's bounded differences inequality states that for a function $f$ of $m$ independent variables satisfying the bounded differences property with constants $c_i$, the following holds for any $t>0$:\n$$\n\\mathbb{P}(f(a_1, \\dots, a_m) - \\mathbb{E}[f] \\ge t) \\le \\exp\\left( \\frac{-2t^2}{\\sum_{i=1}^m c_i^2} \\right)\n$$\nWe need to calculate the sum of the squares of our constants:\n$$\n\\sum_{i=1}^m c_i^2 = \\sum_{i=1}^m \\left(\\frac{2R^2}{m}\\right)^2 = m \\cdot \\frac{4R^4}{m^2} = \\frac{4R^4}{m}\n$$\nThe problem requires finding $t_{\\delta}$ such that $\\mathbb{P}(f \\le \\mathbb{E}[f] + t_{\\delta}) \\ge 1 - \\delta$. This is equivalent to finding $t_{\\delta}$ that satisfies $\\mathbb{P}(f - \\mathbb{E}[f] > t_{\\delta}) \\le \\delta$. As $f$ is a continuous random variable, this is equivalent to $\\mathbb{P}(f - \\mathbb{E}[f] \\ge t_{\\delta}) \\le \\delta$.\n\nWe equate the upper bound from McDiarmid's inequality to the target failure probability $\\delta$, with $t = t_{\\delta}$:\n$$\n\\delta = \\exp\\left( \\frac{-2t_{\\delta}^2}{\\sum_{i=1}^m c_i^2} \\right) = \\exp\\left( \\frac{-2t_{\\delta}^2}{4R^4/m} \\right) = \\exp\\left( \\frac{-2mt_{\\delta}^2}{4R^4} \\right) = \\exp\\left( \\frac{-mt_{\\delta}^2}{2R^4} \\right)\n$$\nNow, we solve for $t_{\\delta}$. Taking the natural logarithm of both sides:\n$$\n\\ln(\\delta) = -\\frac{mt_{\\delta}^2}{2R^4}\n$$\nMultiplying by $-1$ gives:\n$$\n-\\ln(\\delta) = \\ln\\left(\\frac{1}{\\delta}\\right) = \\frac{mt_{\\delta}^2}{2R^4}\n$$\nIsolating $t_{\\delta}^2$:\n$$\nt_{\\delta}^2 = \\frac{2R^4}{m} \\ln\\left(\\frac{1}{\\delta}\\right)\n$$\nFinally, taking the square root yields the expression for the smallest additive deviation $t_{\\delta}$ that certifies the bound:\n$$\nt_{\\delta} = \\sqrt{\\frac{2R^4}{m} \\ln\\left(\\frac{1}{\\delta}\\right)} = R^2 \\sqrt{\\frac{2 \\ln(1/\\delta)}{m}}\n$$\nThis is the closed-form analytic expression for $t_{\\delta}$ in terms of $m$, $R$, and $\\delta$.", "answer": "$$\n\\boxed{R^2 \\sqrt{\\frac{2 \\ln(1/\\delta)}{m}}}\n$$", "id": "3468743"}, {"introduction": "Beyond single random variables, many problems in modern statistics require us to control the maximum of a large collection of random variables, i.e., the supremum of a stochastic process. This exercise introduces the powerful concept of chaining and entropy integrals, which are central to this task. You will apply these ideas to bound the expected supremum of a subgaussian process indexed by an $\\ell_1$ ball, revealing how the geometry of the index set dictates the complexity of the process [@problem_id:3468783].", "problem": "Consider a centered subgaussian process $X_{t}$ indexed by a subset $T \\subset \\mathbb{R}^{n}$, where for every $t, s \\in T$ the increment $X_{t} - X_{s}$ is subgaussian with scale parameter $\\sigma > 0$ in the sense that for all $\\lambda \\in \\mathbb{R}$,\n$$\n\\mathbb{E}\\big[\\exp\\big(\\lambda\\,(X_{t} - X_{s})\\big)\\big] \\leq \\exp\\big(\\tfrac{1}{2}\\sigma^{2}\\lambda^{2}\\,d(t,s)^{2}\\big),\n$$\nwith the canonical metric $d(t,s) := \\big(\\mathbb{E}\\big[(X_{t} - X_{s})^{2}\\big]\\big)^{1/2}$. Starting from the definition of covering numbers $N(T,d,\\varepsilon)$ and the entropy integral method based on chaining (Dudley’s entropy integral), derive a general upper bound on $\\mathbb{E}\\big[\\sup_{t \\in T} X_{t}\\big]$ in terms of the metric entropy of $(T,d)$.\n\nThen specialize to the case where $T$ is the $\\ell_1$ ball of radius $R > 0$ in $\\mathbb{R}^{n}$, namely $T = \\{t \\in \\mathbb{R}^{n} : \\|t\\|_{1} \\leq R\\}$, and the process is linear,\n$$\nX_{t} := \\sum_{i=1}^{n} g_{i}\\,t_{i},\n$$\nwith $(g_{i})_{i=1}^{n}$ independent centered subgaussian random variables of scale $\\sigma$ (equivalently, $\\|g_{i}\\|_{\\psi_{2}} \\lesssim \\sigma$). Using the entropy method, compute an explicit, closed-form analytic expression for an upper bound on $\\mathbb{E}\\big[\\sup_{t \\in T} X_{t}\\big]$ as a function of $n$, $R$, and $\\sigma$. No rounding is required, and the final answer must be a single analytic expression without units.", "solution": "The solution is presented in two parts as requested by the problem statement. First, a general upper bound on the supremum of a subgaussian process is derived using the entropy method (Dudley's entropy integral). Second, this general theory is applied to the specific linear process over an $\\ell_1$ ball to compute a closed-form upper bound.\n\n### Part 1: Derivation of Dudley's Entropy Integral Bound\n\nLet $(X_t)_{t \\in T}$ be a centered subgaussian process indexed by a set $T$. The process is defined with respect to the canonical metric $d(t,s) = (\\mathbb{E}[(X_t - X_s)^2])^{1/2}$. The subgaussian increment property is given as:\n$$\n\\mathbb{E}\\big[\\exp\\big(\\lambda (X_t - X_s)\\big)\\big] \\le \\exp\\left(\\frac{1}{2}\\sigma^2 \\lambda^2 d(t,s)^2\\right) \\quad \\forall \\lambda \\in \\mathbb{R}, s,t \\in T.\n$$\nThis inequality implies that for any pair $(s,t)$, the random variable $X_t - X_s$ is subgaussian with a parameter proportional to $\\sigma d(t,s)$.\n\nWe aim to bound $\\mathbb{E}[\\sup_{t \\in T} X_t]$. As the process is centered, we can assume without loss of generality that there exists a $t_0 \\in T$ such that $X_{t_0} = 0$. If not, we can study the process $X'_t = X_t - X_{t_0}$ and note that $\\mathbb{E}[\\sup_t X_t] \\le \\mathbb{E}[\\sup_t X'_t] + \\mathbb{E}[X_{t_0}] = \\mathbb{E}[\\sup_t X'_t]$. Thus, assuming $0 \\in T$ and $X_0=0$ is typical.\n\nThe derivation is based on a \"chaining\" argument. We represent any $X_t$ as a telescoping sum over a sequence of successively finer approximations of $t$. Let $D = \\sup_{s,t \\in T} d(s,t)$ be the diameter of $T$. We define a sequence of scales $\\varepsilon_k = D \\cdot 2^{-k}$ for $k=0, 1, 2, \\dots$. For each $k$, let $T_k$ be a minimal $\\varepsilon_k$-net of $(T,d)$, so its cardinality $|T_k|$ is the covering number $N(T,d,\\varepsilon_k)$. Let $\\pi_k(t)$ be a point in $T_k$ such that $d(t, \\pi_k(t)) \\le \\varepsilon_k$. We define $\\pi_0(t) = t_0$ for all $t \\in T$.\n\nFor any $t \\in T$, we can write $X_t$ as a telescoping sum, which we truncate at a large integer $K$:\n$$\nX_t = X_{\\pi_0(t)} + \\sum_{k=1}^K (X_{\\pi_k(t)} - X_{\\pi_{k-1}(t)}) + (X_t - X_{\\pi_K(t)}).\n$$\nAssuming $X_{t_0} = 0$, taking the supremum over $t \\in T$ and then the expectation, we get:\n$$\n\\mathbb{E}\\left[\\sup_{t \\in T} X_t\\right] \\le \\sum_{k=1}^K \\mathbb{E}\\left[\\sup_{t \\in T} (X_{\\pi_k(t)} - X_{\\pi_{k-1}(t)})\\right] + \\mathbb{E}\\left[\\sup_{t \\in T} (X_t - X_{\\pi_K(t)})\\right].\n$$\nAs $K \\to \\infty$, $\\varepsilon_K = D \\cdot 2^{-K} \\to 0$. Provided the process is continuous in the metric $d$, the last term vanishes. We focus on the sum.\n\nThe term $\\sup_{t \\in T} (X_{\\pi_k(t)} - X_{\\pi_{k-1}(t)})$ is a supremum over the set of random variables $\\{X_u - X_v : u = \\pi_k(t), v = \\pi_{k-1}(t) \\text{ for some } t \\in T\\}$. The number of such pairs $(u,v)$ is at most $|T_k| \\cdot |T_{k-1}| = N(T,d,\\varepsilon_k) N(T,d,\\varepsilon_{k-1})$. For any such pair, the distance is bounded:\n$$\nd(u,v) = d(\\pi_k(t), \\pi_{k-1}(t)) \\le d(\\pi_k(t),t) + d(t, \\pi_{k-1}(t)) \\le \\varepsilon_k + \\varepsilon_{k-1}.\n$$\nLet $M_k = N(T,d,\\varepsilon_k)N(T,d,\\varepsilon_{k-1})$, and let the set of increments be $\\{Y_j\\}_{j=1}^{M'_k}$ where $M'_k \\le M_k$. Let $\\gamma_k = \\mathbb{E}[\\max_j Y_j]$. For any $\\lambda > 0$, by Jensen's inequality:\n$$\n\\exp(\\lambda \\gamma_k) \\le \\mathbb{E}\\left[\\exp\\left(\\lambda \\max_j Y_j\\right)\\right] = \\mathbb{E}\\left[\\max_j \\exp(\\lambda Y_j)\\right] \\le \\sum_j \\mathbb{E}\\left[\\exp(\\lambda Y_j)\\right].\n$$\nUsing the subgaussian property for each increment $Y_j = X_u - X_v$:\n$$\n\\exp(\\lambda \\gamma_k) \\le M_k \\cdot \\sup_{j} \\mathbb{E}\\left[\\exp(\\lambda Y_j)\\right] \\le M_k \\cdot \\exp\\left(\\frac{1}{2}\\sigma^2 \\lambda^2 (\\varepsilon_k+\\varepsilon_{k-1})^2\\right).\n$$\nTaking logarithms, we have $\\lambda \\gamma_k \\le \\ln(M_k) + \\frac{1}{2}\\sigma^2\\lambda^2(\\varepsilon_k+\\varepsilon_{k-1})^2$. This gives a bound on $\\gamma_k$:\n$$\n\\gamma_k \\le \\frac{\\ln(M_k)}{\\lambda} + \\frac{1}{2}\\sigma^2\\lambda(\\varepsilon_k+\\varepsilon_{k-1})^2.\n$$\nThis bound holds for any $\\lambda > 0$. We optimize it by choosing $\\lambda$ to minimize the expression. The minimum is achieved when $\\lambda^2 = \\frac{2\\ln(M_k)}{\\sigma^2(\\varepsilon_k+\\varepsilon_{k-1})^2}$. Substituting this back, we find:\n$$\n\\gamma_k \\le \\sqrt{2 \\ln(M_k)} \\sigma (\\varepsilon_k + \\varepsilon_{k-1}).\n$$\nSince $M_k = N(T,d,\\varepsilon_k) N(T,d,\\varepsilon_{k-1})$ and $N(T,d,\\varepsilon)$ is non-increasing with $\\varepsilon$, $\\ln(M_k) \\le 2\\ln N(T,d,\\varepsilon_k)$. Also, $\\varepsilon_k+\\varepsilon_{k-1} = \\varepsilon_{k-1}(1+2^{-1})$ if $\\varepsilon_k = \\varepsilon_{k-1}/2$. This sum is proportional to $\\varepsilon_{k-1} - \\varepsilon_k$.\nSumming over $k$:\n$$\n\\mathbb{E}\\left[\\sup_{t \\in T} X_t\\right] \\le \\sum_{k=1}^\\infty \\gamma_k \\le C \\sigma \\sum_{k=1}^\\infty (\\varepsilon_{k-1}-\\varepsilon_k) \\sqrt{\\ln N(T,d,\\varepsilon_k)}.\n$$\nThis sum is a Riemann sum approximation of an integral. Taking the limit as the step sizes go to zero, we obtain Dudley's entropy integral bound:\n$$\n\\mathbb{E}\\left[\\sup_{t \\in T} X_t\\right] \\le L \\int_0^{D/2} \\sqrt{\\ln N(T,d,\\varepsilon)} \\,d\\varepsilon,\n$$\nwhere $L$ is a universal constant related to $\\sigma$. The problem statement implies we should track $\\sigma$, so we write the bound as:\n$$\n\\mathbb{E}\\left[\\sup_{t \\in T} X_t\\right] \\le L \\sigma \\int_0^{\\infty} \\sqrt{\\ln N(T,d,\\varepsilon)} \\,d\\varepsilon,\n$$\nwhere the integration limit can be extended to $\\infty$ since $N(T,d,\\varepsilon)=1$ for $\\varepsilon$ larger than the radius of $T$.\n\n### Part 2: Application to the Specific Process\n\nWe are asked to find an explicit upper bound for $\\mathbb{E}[\\sup_{t \\in T} X_t]$ where:\n1.  $T = \\{t \\in \\mathbb{R}^n : \\|t\\|_1 \\le R\\}$ is the $\\ell_1$-ball of radius $R$.\n2.  $X_t = \\sum_{i=1}^n g_i t_i$, where $g_i$ are independent centered subgaussian random variables of scale $\\sigma$. We assume this implies $\\mathbb{E}[\\exp(\\lambda g_i)] \\le \\exp(\\lambda^2 \\sigma^2/2)$.\n\nA crucial simplification arises from the structure of the problem. $X_t = \\langle g, t \\rangle$ is a linear function of $t$. The set $T=B_1^n(R)$ is a convex set (specifically, a polytope). The supremum of a linear function over a convex polytope is always achieved at one of its vertices.\nThe vertices of the $\\ell_1$-ball $B_1^n(R)$ are the points $V = \\{\\pm R e_i : i=1, \\dots, n\\}$, where $e_i$ are the standard basis vectors in $\\mathbb{R}^n$.\nTherefore,\n$$\n\\sup_{t \\in T} X_t = \\sup_{t \\in V} X_t = \\max_{i \\in \\{1,\\dots,n\\}, \\alpha \\in \\{\\pm 1\\}} \\sum_{j=1}^n g_j (\\alpha R (e_i)_j) = \\max_{i, \\alpha} (\\alpha R g_i).\n$$\nThis maximum is clearly $R \\max_i |g_i|$. The problem thus reduces to finding an upper bound for:\n$$\n\\mathbb{E}\\left[\\sup_{t \\in T} X_t\\right] = \\mathbb{E}\\left[R \\max_{i=1, \\dots, n} |g_i|\\right] = R \\cdot \\mathbb{E}\\left[\\max_{i=1,\\dots,n} |g_i|\\right].\n$$\nWe can bound the expected maximum of a finite collection of subgaussian random variables using a standard maximal inequality, which itself is the simplest instance of the entropy method (a union bound, or 0-level chaining). Let $Z_i = |g_i|$. The variables $g_i$ are subgaussian with scale $\\sigma$, which means $\\mathbb{E}[\\exp(\\lambda g_i)] \\le \\exp(\\lambda^2 \\sigma^2/2)$. The variables $|g_i|$ are also subgaussian. We can also consider the set of $2n$ random variables $\\{\\alpha g_i\\}$ for $i=1,\\dots,n$ and $\\alpha=\\pm 1$. These are all subgaussian with scale $\\sigma$. We want to find a bound for $R \\cdot \\mathbb{E}[\\max_{i,\\alpha} (\\alpha g_i)]$.\n\nLet $Y = \\max_{i, \\alpha} (\\alpha g_i)$. For any $\\lambda > 0$, using Jensen's inequality:\n$$\n\\mathbb{E}[Y] = \\frac{1}{\\lambda} \\mathbb{E}[\\ln(\\exp(\\lambda Y))] \\le \\frac{1}{\\lambda} \\ln(\\mathbb{E}[\\exp(\\lambda Y)]).\n$$\n$\\mathbb{E}[\\exp(\\lambda Y)] = \\mathbb{E}[\\exp(\\lambda \\max_{i,\\alpha} \\alpha g_i)] = \\mathbb{E}[\\max_{i,\\alpha} \\exp(\\lambda \\alpha g_i)] \\le \\sum_{i=1}^n \\sum_{\\alpha=\\pm 1} \\mathbb{E}[\\exp(\\lambda \\alpha g_i)]$.\nUsing the subgaussian property of $g_i$:\n$$\n\\sum_{i=1}^n \\sum_{\\alpha=\\pm 1} \\mathbb{E}[\\exp(\\lambda \\alpha g_i)] \\le \\sum_{i=1}^n (\\exp(\\lambda^2 \\sigma^2/2) + \\exp(\\lambda^2 \\sigma^2/2)) = 2n \\exp(\\lambda^2 \\sigma^2/2).\n$$\nPlugging this into the bound for $\\mathbb{E}[Y]$:\n$$\n\\mathbb{E}[\\max_{i, \\alpha} (\\alpha g_i)] \\le \\frac{1}{\\lambda} \\ln\\left(2n \\exp\\left(\\frac{\\lambda^2 \\sigma^2}{2}\\right)\\right) = \\frac{\\ln(2n)}{\\lambda} + \\frac{\\lambda \\sigma^2}{2}.\n$$\nThis inequality holds for any $\\lambda > 0$. We find the tightest bound by minimizing the right-hand side with respect to $\\lambda$. The derivative with respect to $\\lambda$ is $-\\frac{\\ln(2n)}{\\lambda^2} + \\frac{\\sigma^2}{2}$, which is zero for $\\lambda^2 = \\frac{2\\ln(2n)}{\\sigma^2}$. We choose the positive root $\\lambda = \\frac{\\sqrt{2\\ln(2n)}}{\\sigma}$.\nSubstituting this optimal $\\lambda$ back into the bound:\n$$\n\\mathbb{E}[\\max_{i, \\alpha} (\\alpha g_i)] \\le \\frac{\\ln(2n)\\sigma}{\\sqrt{2\\ln(2n)}} + \\frac{\\sqrt{2\\ln(2n)}}{2\\sigma} \\sigma^2 = \\sigma\\sqrt{\\frac{\\ln(2n)}{2}} + \\sigma\\sqrt{\\frac{\\ln(2n)}{2}} = \\sigma\\sqrt{2\\ln(2n)}.\n$$\nFinally, we multiply by $R$ to obtain the bound on $\\mathbb{E}[\\sup_{t \\in T} X_t]$:\n$$\n\\mathbb{E}\\left[\\sup_{t \\in T} X_t\\right] \\le R \\sigma \\sqrt{2 \\ln(2n)}.\n$$", "answer": "$$\\boxed{R \\sigma \\sqrt{2 \\ln(2n)}}$$", "id": "3468783"}, {"introduction": "The ultimate purpose of developing these probabilistic foundations is to analyze and understand the performance of statistical methods in high-dimensional regimes. This final practice applies the principles of concentration and geometric analysis to the LASSO estimator, a workhorse for sparse regression. By combining a high-probability bound on the noise term with a deterministic geometric assumption on the design matrix—the Restricted Eigenvalue (RE) condition—you will derive concrete, non-asymptotic error bounds for the estimator [@problem_id:3468791].", "problem": "Consider the linear model $y = X \\beta^{\\star} + \\varepsilon$ with $X \\in \\mathbb{R}^{n \\times m}$, response $y \\in \\mathbb{R}^{n}$, unknown parameter vector $\\beta^{\\star} \\in \\mathbb{R}^{m}$, and noise $\\varepsilon \\in \\mathbb{R}^{n}$. Assume the following conditions.\n- The parameter $\\beta^{\\star}$ is $s$-sparse with support $S \\subset \\{1,\\dots,m\\}$ satisfying $|S| = s$.\n- The rows of $X$ are independent, mean-zero, sub-Gaussian random vectors in $\\mathbb{R}^{m}$ with identity covariance, and sub-Gaussian norm uniformly bounded by a fixed constant. The noise vector has independent entries $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$, independent of $X$.\n- Let the Least Absolute Shrinkage and Selection Operator (LASSO) estimator be defined by\n$$\n\\widehat{\\beta} \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{m}} \\left\\{ \\frac{1}{2n} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\| \\beta \\|_{1} \\right\\}.\n$$\n- Define the restricted cone $\\mathcal{C}(S,3) = \\{ \\Delta \\in \\mathbb{R}^{m}: \\| \\Delta_{S^{c}} \\|_{1} \\leq 3 \\| \\Delta_{S} \\|_{1} \\}$, and assume a Restricted Eigenvalue (RE) condition: there exists a constant $\\kappa > 0$ such that\n$$\n\\frac{1}{n} \\| X \\Delta \\|_{2}^{2} \\geq \\kappa \\| \\Delta \\|_{2}^{2} \\quad \\text{for all } \\Delta \\in \\mathcal{C}(S,3).\n$$\n- Suppose $n$ is sufficiently large (e.g., $n \\gtrsim s \\ln(m/s)$) so that, with high probability, the RE constant satisfies $\\kappa \\geq \\frac{1}{2}$ and the column norms concentrate so that $\\max_{1 \\leq j \\leq m} \\frac{1}{n} \\| X_{j} \\|_{2}^{2} \\leq \\frac{3}{2}$.\n\nUsing high-dimensional probability tools, choose the regularization parameter\n$$\n\\lambda = 2 \\sqrt{6} \\, \\sigma \\sqrt{ \\frac{ \\ln m }{ n } }.\n$$\nDerive finite-sample upper bounds for the $\\ell_{2}$ and $\\ell_{1}$ estimation errors $\\| \\widehat{\\beta} - \\beta^{\\star} \\|_{2}$ and $\\| \\widehat{\\beta} - \\beta^{\\star} \\|_{1}$ in terms of $s$, $n$, $m$, and the noise variance $\\sigma^{2}$. Your final answer must be two closed-form expressions depending only on $s$, $n$, $m$, and $\\sigma^{2}$, and must not include any probability or inequality statements. No rounding is required.", "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, and objective. It presents a standard theoretical problem in the analysis of the LASSO estimator in high-dimensional linear regression. All provided conditions are canonical within this field of study. We proceed with the derivation of the finite-sample error bounds.\n\nLet $\\widehat{\\beta}$ be the LASSO estimator and $\\beta^{\\star}$ be the true $s$-sparse parameter vector. The estimation error is denoted by $\\Delta = \\widehat{\\beta} - \\beta^{\\star}$. By the definition of the LASSO estimator, $\\widehat{\\beta}$ minimizes the objective function, so for any other vector, including $\\beta^{\\star}$, the objective value at $\\widehat{\\beta}$ is smaller or equal:\n$$\n\\frac{1}{2n} \\| y - X \\widehat{\\beta} \\|_{2}^{2} + \\lambda \\| \\widehat{\\beta} \\|_{1} \\leq \\frac{1}{2n} \\| y - X \\beta^{\\star} \\|_{2}^{2} + \\lambda \\| \\beta^{\\star} \\|_{1}\n$$\nSubstituting the linear model $y = X \\beta^{\\star} + \\varepsilon$ and using the definition $\\Delta = \\widehat{\\beta} - \\beta^{\\star}$, we rearrange the inequality:\n$$\n\\frac{1}{2n} \\| (X \\beta^{\\star} + \\varepsilon) - X (\\beta^{\\star} + \\Delta) \\|_{2}^{2} - \\frac{1}{2n} \\| \\varepsilon \\|_{2}^{2} \\leq \\lambda (\\| \\beta^{\\star} \\|_{1} - \\| \\widehat{\\beta} \\|_{1})\n$$\n$$\n\\frac{1}{2n} \\| \\varepsilon - X \\Delta \\|_{2}^{2} - \\frac{1}{2n} \\| \\varepsilon \\|_{2}^{2} \\leq \\lambda (\\| \\beta^{\\star} \\|_{1} - \\| \\beta^{\\star} + \\Delta \\|_{1})\n$$\nExpanding the squared norm on the left-hand side gives:\n$$\n\\frac{1}{2n} (\\| \\varepsilon \\|_{2}^{2} - 2 \\langle \\varepsilon, X \\Delta \\rangle + \\| X \\Delta \\|_{2}^{2}) - \\frac{1}{2n} \\| \\varepsilon \\|_{2}^{2} \\leq \\lambda (\\| \\beta^{\\star} \\|_{1} - \\| \\beta^{\\star} + \\Delta \\|_{1})\n$$\nThis simplifies to the basic inequality:\n$$\n\\frac{1}{2n} \\| X \\Delta \\|_{2}^{2} \\leq \\frac{1}{n} \\langle \\varepsilon, X \\Delta \\rangle + \\lambda (\\| \\beta^{\\star} \\|_{1} - \\| \\beta^{\\star} + \\Delta \\|_{1})\n$$\nLet $S$ be the support of $\\beta^{\\star}$, with $|S| = s$, and let $S^{c}$ be its complement. We analyze the term involving the $\\ell_1$-norm. Since $\\beta^{\\star}_{S^c} = 0$, we have $\\| \\beta^{\\star} \\|_{1} = \\| \\beta^{\\star}_{S} \\|_{1}$. Using the triangle inequality, we have $\\| \\beta^{\\star} + \\Delta \\|_{1} = \\| \\beta^{\\star}_{S} + \\Delta_{S} \\|_{1} + \\| \\Delta_{S^{c}} \\|_{1} \\geq \\| \\beta^{\\star}_{S} \\|_{1} - \\| \\Delta_{S} \\|_{1} + \\| \\Delta_{S^{c}} \\|_{1}$.\nThis leads to a bound on the difference of norms:\n$$\n\\| \\beta^{\\star} \\|_{1} - \\| \\beta^{\\star} + \\Delta \\|_{1} \\leq \\| \\beta^{\\star}_{S} \\|_{1} - (\\| \\beta^{\\star}_{S} \\|_{1} - \\| \\Delta_{S} \\|_{1} + \\| \\Delta_{S^{c}} \\|_{1}) = \\| \\Delta_{S} \\|_{1} - \\| \\Delta_{S^{c}} \\|_{1}\n$$\nSubstituting this into the basic inequality yields:\n$$\n\\frac{1}{2n} \\| X \\Delta \\|_{2}^{2} \\leq \\frac{1}{n} \\langle \\varepsilon, X \\Delta \\rangle + \\lambda (\\| \\Delta_{S} \\|_{1} - \\| \\Delta_{S^{c}} \\|_{1})\n$$\nThe noise term can be written as $\\frac{1}{n} \\langle X^{T}\\varepsilon, \\Delta \\rangle$. Let $W = \\frac{1}{n} X^{T}\\varepsilon$. The given choice of the regularization parameter $\\lambda = 2 \\sqrt{6} \\, \\sigma \\sqrt{ \\frac{ \\ln m }{ n } }$ is standard in high-dimensional analysis. Under the given assumptions on $X$ and $\\varepsilon$, it ensures that, with high probability, $\\|W\\|_{\\infty} = \\| \\frac{1}{n} X^{T}\\varepsilon \\|_{\\infty} \\leq \\lambda/2$. We proceed conditional on this event.\nWe can bound the noise term:\n$$\n\\left| \\frac{1}{n} \\langle \\varepsilon, X \\Delta \\rangle \\right| = |\\langle W, \\Delta \\rangle| \\leq \\| W \\|_{\\infty} \\| \\Delta \\|_{1} \\leq \\frac{\\lambda}{2} (\\| \\Delta_{S} \\|_{1} + \\| \\Delta_{S^{c}} \\|_{1})\n$$\nPlugging this into our working inequality gives:\n$$\n\\frac{1}{2n} \\| X \\Delta \\|_{2}^{2} \\leq \\frac{\\lambda}{2} (\\| \\Delta_{S} \\|_{1} + \\| \\Delta_{S^{c}} \\|_{1}) + \\lambda (\\| \\Delta_{S} \\|_{1} - \\| \\Delta_{S^{c}} \\|_{1}) = \\frac{3\\lambda}{2} \\| \\Delta_{S} \\|_{1} - \\frac{\\lambda}{2} \\| \\Delta_{S^{c}} \\|_{1}\n$$\nSince the left-hand side is non-negative, we must have $\\frac{3\\lambda}{2} \\| \\Delta_{S} \\|_{1} - \\frac{\\lambda}{2} \\| \\Delta_{S^{c}} \\|_{1} \\geq 0$, which implies $\\| \\Delta_{S^{c}} \\|_{1} \\leq 3 \\| \\Delta_{S} \\|_{1}$. This shows that the error vector $\\Delta$ lies in the restricted cone $\\mathcal{C}(S,3)=\\{ \\Delta \\in \\mathbb{R}^{m}: \\| \\Delta_{S^{c}} \\|_{1} \\leq 3 \\| \\Delta_{S} \\|_{1} \\}$.\n\nThis allows us to apply the Restricted Eigenvalue (RE) condition, which is assumed to hold for all vectors in $\\mathcal{C}(S,3)$. The problem states $\\frac{1}{n} \\| X \\Delta \\|_{2}^{2} \\geq \\kappa \\| \\Delta \\|_{2}^{2}$ with $\\kappa \\geq \\frac{1}{2}$. Thus, we have:\n$$\n\\frac{1}{2} \\| \\Delta \\|_{2}^{2} \\leq \\frac{1}{n} \\| X \\Delta \\|_{2}^{2}\n$$\nCombining this with our derived inequality:\n$$\n\\frac{1}{2} \\cdot \\frac{1}{2} \\| \\Delta \\|_{2}^{2} \\leq \\frac{1}{2n} \\| X \\Delta \\|_{2}^{2} \\leq \\frac{3\\lambda}{2} \\| \\Delta_{S} \\|_{1} - \\frac{\\lambda}{2} \\| \\Delta_{S^{c}} \\|_{1}\n$$\nDropping the negative term on the right, we get a simpler bound:\n$$\n\\frac{1}{4} \\| \\Delta \\|_{2}^{2} \\leq \\frac{3\\lambda}{2} \\| \\Delta_{S} \\|_{1}\n$$\nWe use the Cauchy-Schwarz inequality to bound $\\| \\Delta_{S} \\|_{1}$:\n$$\n\\| \\Delta_{S} \\|_{1} = \\sum_{j \\in S} 1 \\cdot |\\Delta_j| \\leq \\left( \\sum_{j \\in S} 1^2 \\right)^{1/2} \\left( \\sum_{j \\in S} \\Delta_j^2 \\right)^{1/2} = \\sqrt{s} \\| \\Delta_{S} \\|_{2}\n$$\nSince $\\| \\Delta_{S} \\|_{2} \\leq \\| \\Delta \\|_{2}$, we have $\\| \\Delta_{S} \\|_{1} \\leq \\sqrt{s} \\| \\Delta \\|_{2}$.\nSubstituting this into our bound on $\\| \\Delta \\|_{2}^{2}$:\n$$\n\\frac{1}{4} \\| \\Delta \\|_{2}^{2} \\leq \\frac{3\\lambda}{2} \\sqrt{s} \\| \\Delta \\|_{2}\n$$\nAssuming $\\Delta \\neq 0$, we divide by $\\| \\Delta \\|_{2}$ to obtain the $\\ell_2$-error bound:\n$$\n\\| \\widehat{\\beta} - \\beta^{\\star} \\|_{2} = \\| \\Delta \\|_{2} \\leq 4 \\cdot \\frac{3\\lambda}{2} \\sqrt{s} = 6 \\lambda \\sqrt{s}\n$$\nSubstituting the given expression for $\\lambda$:\n$$\n\\| \\Delta \\|_{2} \\leq 6 \\sqrt{s} \\left( 2 \\sqrt{6} \\, \\sigma \\sqrt{ \\frac{ \\ln m }{ n } } \\right) = 12 \\sqrt{6} \\, \\sigma \\sqrt{ \\frac{s \\ln m}{n} }\n$$\nThis is the upper bound for the $\\ell_2$ estimation error.\n\nNext, we derive the bound for the $\\ell_1$ estimation error, $\\| \\widehat{\\beta} - \\beta^{\\star} \\|_{1} = \\| \\Delta \\|_{1}$. We use the cone property $\\| \\Delta_{S^{c}} \\|_{1} \\leq 3 \\| \\Delta_{S} \\|_{1}$:\n$$\n\\| \\Delta \\|_{1} = \\| \\Delta_{S} \\|_{1} + \\| \\Delta_{S^{c}} \\|_{1} \\leq \\| \\Delta_{S} \\|_{1} + 3 \\| \\Delta_{S} \\|_{1} = 4 \\| \\Delta_{S} \\|_{1}\n$$\nUsing $\\| \\Delta_{S} \\|_{1} \\leq \\sqrt{s} \\| \\Delta_{S} \\|_{2} \\leq \\sqrt{s} \\| \\Delta \\|_{2}$ once more:\n$$\n\\| \\Delta \\|_{1} \\leq 4 \\sqrt{s} \\| \\Delta \\|_{2}\n$$\nNow, we substitute the $\\ell_2$-error bound we just found:\n$$\n\\| \\Delta \\|_{1} \\leq 4 \\sqrt{s} (6 \\lambda \\sqrt{s}) = 24 \\lambda s\n$$\nSubstituting the expression for $\\lambda$:\n$$\n\\| \\Delta \\|_{1} \\leq 24 s \\left( 2 \\sqrt{6} \\, \\sigma \\sqrt{ \\frac{ \\ln m }{ n } } \\right) = 48 \\sqrt{6} \\, \\sigma s \\sqrt{ \\frac{\\ln m}{n} }\n$$\nThis is the upper bound for the $\\ell_1$ estimation error.\n\nThe problem requires the two closed-form expressions for the upper bounds. These are the derived right-hand sides of the final inequalities for $\\| \\Delta \\|_{2}$ and $\\| \\Delta \\|_{1}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n12 \\sqrt{6} \\sigma \\sqrt{\\frac{s \\ln(m)}{n}}  48 \\sqrt{6} \\sigma s \\sqrt{\\frac{\\ln(m)}{n}}\n\\end{pmatrix}\n}\n$$", "id": "3468791"}]}