{"hands_on_practices": [{"introduction": "Before running an iterative algorithm, it is crucial to understand its destination: the optimal solution. This first practice focuses on the fundamental first-order optimality conditions for the LASSO objective function, which involve the concept of the subdifferential for non-smooth terms. By working through a concrete numerical example, you will learn how to verify whether a given point is a solution and how to quantify its deviation from optimality, providing a solid foundation for evaluating algorithmic performance [@problem_id:3392935].", "problem": "Consider the composite convex objective underlying the Iterative Soft-Thresholding Algorithm (ISTA) in inverse problems and data assimilation,\n$$\nF(x) \\equiv \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, $\\lambda > 0$, and $x \\in \\mathbb{R}^{n}$. The first-order optimality condition for this problem can be expressed using the subdifferential of the $\\ell_{1}$ norm. Using only fundamental definitions from convex analysis and linear algebra, derive the stationarity condition for $F$, and then use it to evaluate the quality of a candidate solution.\n\nLet $A = I_{3} \\in \\mathbb{R}^{3 \\times 3}$, $b = \\begin{pmatrix}2 \\\\ 0.4 \\\\ 0\\end{pmatrix}$, $\\lambda = \\frac{1}{2}$, and the candidate vector $x = \\begin{pmatrix}1 \\\\ 0 \\\\ 1\\end{pmatrix}$. Construct a valid subgradient $s \\in \\partial \\|x\\|_{1}$ at the given $x$ that minimizes the Euclidean norm of the stationarity residual, and compute the minimal possible value of\n$$\n\\|A^{\\top}(A x - b) + \\lambda s\\|_{2}.\n$$\nGive your final numerical answer rounded to four significant figures. No units are required.", "solution": "The problem is valid as it is scientifically grounded in convex optimization, well-posed, objective, and internally consistent.\n\nThe objective function to be analyzed is a composite function $F(x) = f(x) + g(x)$, where $f(x)$ is a smooth, convex, differentiable term and $g(x)$ is a convex, non-smooth term. Specifically, we have:\n$$\nf(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}\n$$\n$$\ng(x) = \\lambda \\|x\\|_{1}\n$$\nAccording to Fermat's rule in convex analysis, a point $x^*$ is a global minimizer of $F(x)$ if and only if the zero vector is an element of the subdifferential of $F(x)$ at $x^*$. This is the first-order optimality or stationarity condition:\n$$\n0 \\in \\partial F(x^*)\n$$\nSince $f(x)$ is differentiable, the subdifferential of the sum $F(x)$ is the sum of the gradient of $f(x)$ and the subdifferential of $g(x)$:\n$$\n\\partial F(x) = \\nabla f(x) + \\partial g(x)\n$$\nFirst, we find the gradient of $f(x)$. The function is $f(x) = \\frac{1}{2}(Ax - b)^{\\top}(Ax - b) = \\frac{1}{2}(x^{\\top}A^{\\top}Ax - 2b^{\\top}Ax + b^{\\top}b)$. The gradient with respect to $x$ is:\n$$\n\\nabla f(x) = \\frac{1}{2}(2A^{\\top}Ax - 2A^{\\top}b) = A^{\\top}(Ax - b)\n$$\nNext, we characterize the subdifferential of $g(x) = \\lambda \\|x\\|_{1}$. This is $\\partial g(x) = \\lambda \\partial \\|x\\|_{1}$. The $\\ell_1$-norm is $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$. Its subdifferential is the Cartesian product of the subdifferentials of its components. A vector $s \\in \\mathbb{R}^n$ is a subgradient of the $\\ell_1$-norm at $x$, denoted $s \\in \\partial \\|x\\|_1$, if its components $s_i$ satisfy:\n$$\ns_i =\n\\begin{cases}\n\\text{sgn}(x_i)  \\text{if } x_i \\neq 0 \\\\\nc_i \\in [-1, 1]  \\text{if } x_i = 0\n\\end{cases}\n$$\nThe stationarity condition for a point $x$ is therefore the existence of a subgradient $s \\in \\partial\\|x\\|_1$ such that:\n$$\nA^{\\top}(Ax - b) + \\lambda s = 0\n$$\nThe vector $R(s) = A^{\\top}(A x - b) + \\lambda s$ is the stationarity residual. The problem requires us to find a subgradient $s$ that minimizes the Euclidean norm of this residual, $\\|R(s)\\|_2$, for the given candidate solution $x$.\n\nWe are given the specific values:\n$A = I_{3}$ (the $3 \\times 3$ identity matrix)\n$b = \\begin{pmatrix} 2 \\\\ 0.4 \\\\ 0 \\end{pmatrix}$\n$\\lambda = \\frac{1}{2}$\n$x = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$\n\nFirst, we compute the gradient term $\\nabla f(x) = A^{\\top}(Ax-b)$. Since $A = I_3$, $A^{\\top}=I_3$ and the expression simplifies to $x-b$:\n$$\n\\nabla f(x) = x - b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 0.4 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -0.4 \\\\ 1 \\end{pmatrix}\n$$\nNext, we characterize the set of valid subgradients $s \\in \\partial \\|x\\|_1$ for the given $x = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$:\nFor the first component, $x_1 = 1 \\neq 0$, so $s_1 = \\text{sgn}(1) = 1$.\nFor the second component, $x_2 = 0$, so $s_2$ can be any value in the interval $[-1, 1]$.\nFor the third component, $x_3 = 1 \\neq 0$, so $s_3 = \\text{sgn}(1) = 1$.\nThus, any valid subgradient $s$ must be of the form $s = \\begin{pmatrix} 1 \\\\ s_2 \\\\ 1 \\end{pmatrix}$ where $s_2 \\in [-1, 1]$.\n\nThe stationarity residual is $R(s) = (x-b) + \\lambda s$. Substituting the known quantities:\n$$\nR(s) = \\begin{pmatrix} -1 \\\\ -0.4 \\\\ 1 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 \\\\ s_2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1 + 0.5 \\\\ -0.4 + 0.5 s_2 \\\\ 1 + 0.5 \\end{pmatrix} = \\begin{pmatrix} -0.5 \\\\ -0.4 + 0.5 s_2 \\\\ 1.5 \\end{pmatrix}\n$$\nWe need to find the value of $s_2 \\in [-1, 1]$ that minimizes the Euclidean norm of this residual, $\\|R(s)\\|_2$. Minimizing $\\|R(s)\\|_2$ is equivalent to minimizing its square, $\\|R(s)\\|_2^2$:\n$$\n\\|R(s)\\|_2^2 = (-0.5)^2 + (-0.4 + 0.5 s_2)^2 + (1.5)^2\n$$\n$$\n\\|R(s)\\|_2^2 = 0.25 + (-0.4 + 0.5 s_2)^2 + 2.25 = 2.5 + (-0.4 + 0.5 s_2)^2\n$$\nTo minimize this expression, we must minimize the term $(-0.4 + 0.5 s_2)^2$. The minimum value of a squared real number is $0$, which occurs when the term inside the square is zero. We solve for $s_2$:\n$$\n-0.4 + 0.5 s_2 = 0 \\implies 0.5 s_2 = 0.4 \\implies s_2 = \\frac{0.4}{0.5} = 0.8\n$$\nThe value $s_2 = 0.8$ is within the allowed interval $[-1, 1]$. Therefore, the subgradient that minimizes the residual norm is $s = \\begin{pmatrix} 1 \\\\ 0.8 \\\\ 1 \\end{pmatrix}$.\n\nWith this optimal choice of $s_2$, the minimal value of the squared norm of the residual is:\n$$\n\\min \\|R(s)\\|_2^2 = 2.5 + (-0.4 + 0.5 \\times 0.8)^2 = 2.5 + (-0.4 + 0.4)^2 = 2.5 + 0^2 = 2.5\n$$\nThe minimal possible value of the norm is the square root of this value:\n$$\n\\min \\|R(s)\\|_2 = \\sqrt{2.5}\n$$\nThe problem requires a numerical answer rounded to four significant figures.\n$$\n\\sqrt{2.5} \\approx 1.5811388...\n$$\nRounding to four significant figures, we get $1.581$.", "answer": "$$\\boxed{1.581}$$", "id": "3392935"}, {"introduction": "The effectiveness of ISTA is highly sensitive to the choice of the regularization parameter, $\\lambda$. This exercise explores the practical consequences of this choice, demonstrating how an overly large $\\lambda$ can cause the algorithm to stagnate at a trivial, non-informative solution. You will then move from a simple counterexample to a principled method, deriving a statistically-motivated \"universal threshold\" for $\\lambda$ that robustly handles noise, a critical skill in real-world data assimilation and inverse problems [@problem_id:3392955].", "problem": "Consider the linear inverse problem with unknown vector $x^{\\star} \\in \\mathbb{R}^{n}$, sensing matrix $A \\in \\mathbb{R}^{n \\times n}$, and observed data $b \\in \\mathbb{R}^{n}$ generated by $b = A x^{\\star} + \\eta$, where $\\eta$ is an additive noise vector. The Iterative Soft-Thresholding Algorithm (ISTA) for the $\\ell_{1}$-regularized least squares problem $\\min_{x \\in \\mathbb{R}^{n}} \\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$ is defined by the iterative map $x^{k+1} = \\mathcal{S}_{\\lambda \\tau}\\!\\left(x^{k} + \\tau A^{\\top}(b - A x^{k})\\right)$, where $\\tau  0$ is a step size satisfying a standard Lipschitz condition and $\\mathcal{S}_{\\theta}$ denotes the componentwise soft-thresholding operator $\\big(\\mathcal{S}_{\\theta}(z)\\big)_{i} = \\mathrm{sign}(z_{i}) \\max\\{|z_{i}| - \\theta, 0\\}$.\n\n(a) Construct a counterexample demonstrating stagnation (plateau at the zero vector) under naive threshold selection when the regularization parameter $\\lambda$ is too large relative to the data term. Specifically, take $n = 3$, $A = I_{3}$, $\\tau = 1$, and $b = (0.12,\\,-0.09,\\,0)^{\\top}$. Starting from the definitions above and no further shortcuts, use first-principles reasoning to show precisely why the ISTA iterates $x^{k}$ become identically zero for all $k \\geq 1$ whenever $\\lambda \\geq \\|A^{\\top} b\\|_{\\infty}$ in this configuration.\n\n(b) In a noisy data assimilation setting, suppose the noise $\\eta$ has independent and identically distributed (i.i.d.) Gaussian entries with zero mean and variance $\\sigma^{2}$, i.e., $\\eta \\sim \\mathcal{N}(0,\\,\\sigma^{2} I_{n})$. Derive, from a probabilistic argument based on the distribution of the maximum of $n$ i.i.d. Gaussian variables and without invoking pre-stated shortcuts, an analytically justified scaling for $\\lambda$ as a function of $\\sigma$ and $n$ that guards against noise-dominated updates while avoiding trivial zero solutions for signals with entries exceeding the typical noise-driven plateaus. Use the resulting scaling to compute a recommended $\\lambda$ for $n = 1024$ and $\\sigma = 0.03$.\n\nRound your final numerical value of $\\lambda$ to four significant figures. Express your answer as a pure number without units.", "solution": "The problem presents two distinct tasks related to the Iterative Soft-Thresholding Algorithm (ISTA). Part (a) requires a demonstration of iterate stagnation under specific conditions, while Part (b) calls for the derivation of a principled scaling for the regularization parameter $\\lambda$ in a stochastic setting.\n\n### Part (a): Stagnation at the Zero Vector\n\nWe are given the ISTA update rule for the minimization problem $\\min_{x \\in \\mathbb{R}^{n}} \\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$:\n$$\nx^{k+1} = \\mathcal{S}_{\\lambda \\tau}\\!\\left(x^{k} + \\tau A^{\\top}(b - A x^{k})\\right)\n$$\nThe problem specifies the parameters for this part as $n = 3$, the identity matrix $A = I_{3}$, a step size $\\tau = 1$, and the data vector $b = (0.12,\\,-0.09,\\,0)^{\\top}$.\n\nSubstituting $A=I_{3}$ and $\\tau=1$ into the update rule yields a significant simplification. For any iterate $x^k$:\n$$\nx^{k+1} = \\mathcal{S}_{\\lambda \\cdot 1}\\!\\left(x^{k} + 1 \\cdot I_{3}^{\\top}(b - I_{3} x^{k})\\right) = \\mathcal{S}_{\\lambda}\\!\\left(x^{k} + (b - x^{k})\\right) = \\mathcal{S}_{\\lambda}(b)\n$$\nThis result shows that for any $k \\geq 0$, the next iterate $x^{k+1}$ is solely determined by the action of the soft-thresholding operator $\\mathcal{S}_{\\lambda}$ on the data vector $b$. It is independent of the current iterate $x^k$. Consequently, all iterates from $k=1$ onwards are identical:\n$$\nx^{1} = \\mathcal{S}_{\\lambda}(b)\n$$\n$$\nx^{2} = \\mathcal{S}_{\\lambda}(b)\n$$\n$$\n\\vdots\n$$\n$$\nx^{k} = \\mathcal{S}_{\\lambda}(b) \\quad \\text{for all } k \\geq 1.\n$$\nThe problem asks us to show that these iterates become identically zero, i.e., $x^k = 0$ for all $k \\geq 1$, under the condition $\\lambda \\geq \\|A^{\\top} b\\|_{\\infty}$.\n\nFor our specific configuration where $A=I_3$, the condition is $\\lambda \\geq \\|I_3^{\\top} b\\|_{\\infty} = \\|b\\|_{\\infty}$. The $\\ell_{\\infty}$-norm of the vector $b=(0.12,\\,-0.09,\\,0)^{\\top}$ is:\n$$\n\\|b\\|_{\\infty} = \\max\\big(|0.12|, |-0.09|, |0|\\big) = 0.12\n$$\nSo the condition becomes $\\lambda \\geq 0.12$.\n\nWe now analyze the expression for the iterates, $x^k = \\mathcal{S}_{\\lambda}(b)$, under this condition. The soft-thresholding operator $\\mathcal{S}_{\\theta}$ is defined componentwise as:\n$$\n\\big(\\mathcal{S}_{\\theta}(z)\\big)_{i} = \\mathrm{sign}(z_{i}) \\max\\{|z_{i}| - \\theta, 0\\}\n$$\nFor an entire vector $\\mathcal{S}_{\\theta}(z)$ to be the zero vector, it is necessary and sufficient that for every component $i$, $\\max\\{|z_{i}| - \\theta, 0\\} = 0$. This, in turn, is equivalent to the condition $|z_i| \\leq \\theta$ for all $i$. This can be expressed compactly using the $\\ell_{\\infty}$-norm as $\\|z\\|_{\\infty} \\leq \\theta$.\n\nApplying this to our problem, the condition for $x^k = \\mathcal{S}_{\\lambda}(b)$ to be the zero vector is that $\\|b\\|_{\\infty} \\leq \\lambda$. This is precisely the condition $\\lambda \\geq \\|A^{\\top} b\\|_{\\infty}$ given in the problem statement for our choice of $A=I_3$.\n\nTo be explicit, with $\\lambda \\geq 0.12$:\nFor the first component, $b_1 = 0.12$: $|b_1|=0.12$. Since $\\lambda \\geq 0.12$, $|b_1| - \\lambda \\leq 0$, so $\\max\\{|b_1|-\\lambda, 0\\}=0$.\nFor the second component, $b_2 = -0.09$: $|b_2|=0.09$. Since $\\lambda \\geq 0.12$, $|b_2| - \\lambda  0$, so $\\max\\{|b_2|-\\lambda, 0\\}=0$.\nFor the third component, $b_3 = 0$: $|b_3|=0$. Since $\\lambda \\geq 0.12$, $|b_3| - \\lambda  0$, so $\\max\\{|b_3|-\\lambda, 0\\}=0$.\n\nSince all components of $\\mathcal{S}_{\\lambda}(b)$ are zero, we have $\\mathcal{S}_{\\lambda}(b) = 0$.\nTherefore, for any choice of initial vector $x^0$, the first iterate is $x^1 = \\mathcal{S}_{\\lambda}(b) = 0$, and all subsequent iterates $x^k$ for $k \\geq 1$ remain at the zero vector. This demonstrates the specified stagnation.\n\n### Part (b): Probabilistic Scaling of the Regularization Parameter\n\nIn this part, we are to derive a scaling for $\\lambda$ that prevents the algorithm from being dominated by noise. The model is $b = Ax^\\star + \\eta$, where $\\eta$ is a vector of i.i.d. Gaussian noise components, $\\eta_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nA key principle in regularization is to choose $\\lambda$ large enough to suppress noise but not so large that it erroneously eliminates true signal components. A common strategy is to choose $\\lambda$ to be just above the level of noise that one would expect to see in the term being thresholded.\n\nConsider the first iteration of ISTA starting from the natural initial guess $x^0 = 0$. The argument of the soft-thresholding operator is $x^0 + \\tau A^{\\top}(b - A x^0) = \\tau A^\\top b$. If we consider a scenario with no true signal ($x^\\star=0$), then $b=\\eta$, and this argument becomes $\\tau A^\\top \\eta$. To prevent the algorithm from fitting to noise, we want this term to be thresholded to zero. This requires that every component of $\\tau A^\\top\\eta$ has a magnitude less than or equal to the threshold $\\lambda\\tau$. This can be written as:\n$$\n\\|\\tau A^\\top \\eta\\|_{\\infty} \\leq \\lambda\\tau \\implies \\|A^\\top \\eta\\|_{\\infty} \\leq \\lambda\n$$\nThis gives us a criterion for choosing $\\lambda$: it should be an upper bound on the likely values of $\\|A^\\top \\eta\\|_{\\infty}$.\n\nTo proceed with an analytical derivation, we must characterize the distribution of the vector $v = A^\\top \\eta$. This distribution depends on the matrix $A$. A standard assumption made for this type of universal threshold derivation is that the sensing matrix $A$ is orthonormal, i.e., $A^\\top A = I_n$. Under this assumption, the covariance of $v$ is:\n$$\n\\mathrm{Cov}(v) = E[v v^\\top] = E[A^\\top \\eta \\eta^\\top A] = A^\\top E[\\eta \\eta^\\top] A = A^\\top (\\sigma^2 I_n) A = \\sigma^2 A^\\top A = \\sigma^2 I_n\n$$\nSince $E[v] = A^\\top E[\\eta] = 0$, the components $v_i$ of $v = A^\\top \\eta$ are i.i.d. Gaussian random variables with mean $0$ and variance $\\sigma^2$, just like the original noise components $\\eta_i$.\n\nOur task now reduces to finding a high-probability upper bound on the random variable $M_n = \\|v\\|_{\\infty} = \\max_{1 \\leq i \\leq n} |v_i|$. Let $Z_i = v_i/\\sigma$ be i.i.d. standard normal variables, $Z_i \\sim \\mathcal{N}(0, 1)$. We seek an estimate for $\\max_i |Z_i|$, and then scale the result by $\\sigma$.\n\nLet $Y = \\max_{1 \\leq i \\leq n} |Z_i|$. The cumulative distribution function (CDF) of $|Z_i|$ is $F_{|Z|}(y) = P(|Z_i| \\leq y) = 2\\Phi(y)-1$ for $y \\geq 0$, where $\\Phi$ is the CDF of the standard normal distribution. Since the $|Z_i|$ are i.i.d., the CDF of their maximum is:\n$$\nF_Y(y) = P(Y \\leq y) = [F_{|Z|}(y)]^n = (2\\Phi(y)-1)^n\n$$\nWe want to find a threshold $y_n$ such that $P(Y  y_n)$ is small for large $n$. We can use the union bound as an approximation for this tail probability:\n$$\nP(Y  y) = P(\\exists i: |Z_i|  y) \\leq \\sum_{i=1}^n P(|Z_i|y) = n P(|Z_1|y)\n$$\nThe probability $P(|Z_1|y)$ is $2(1-\\Phi(y))$. For large $y$, we can use the standard Gaussian tail approximation:\n$$\n1-\\Phi(y) \\approx \\frac{\\phi(y)}{y} = \\frac{1}{\\sqrt{2\\pi}y}\\exp\\left(-\\frac{y^2}{2}\\right)\n$$\nThus, $P(Yy) \\approx 2n(1-\\Phi(y)) \\approx \\frac{2n}{\\sqrt{2\\pi}y}\\exp\\left(-\\frac{y^2}{2}\\right)$. We seek a scaling for $y$ with $n$ such that this probability vanishes as $n \\to \\infty$. Let us test the candidate scaling $y = \\sqrt{2\\ln n}$:\n$$\nP(Y  \\sqrt{2\\ln n}) \\approx \\frac{2n}{\\sqrt{2\\pi}\\sqrt{2\\ln n}}\\exp\\left(-\\frac{( \\sqrt{2\\ln n} )^2}{2}\\right) = \\frac{2n}{\\sqrt{4\\pi\\ln n}}\\exp(-\\ln n) = \\frac{2n}{2\\sqrt{\\pi\\ln n}} \\left(\\frac{1}{n}\\right) = \\frac{1}{\\sqrt{\\pi\\ln n}}\n$$\nAs $n \\to \\infty$, this probability tends to $0$. This indicates that for large $n$, it is highly probable that the maximum of $n$ i.i.d. $|Z_i|$ variables will not exceed $\\sqrt{2\\ln n}$. This justifies choosing this value as a threshold.\n\nScaling back by $\\sigma$, we obtain the analytically justified scaling for $\\lambda$:\n$$\n\\lambda = \\sigma \\sqrt{2 \\ln n}\n$$\nThis is famously known as the universal threshold. It provides a parameter choice that adapts to the problem dimension $n$ and the noise level $\\sigma$.\n\nNow, we compute the recommended $\\lambda$ for $n = 1024$ and $\\sigma = 0.03$.\nFirst, calculate $\\ln(1024)$:\n$$\n\\ln(1024) = \\ln(2^{10}) = 10 \\ln(2)\n$$\nUsing a standard value for $\\ln(2) \\approx 0.69314718$:\n$$\n\\ln(1024) \\approx 6.9314718\n$$\nNow, substitute this into the formula for $\\lambda$:\n$$\n\\lambda = 0.03 \\times \\sqrt{2 \\times 6.9314718} = 0.03 \\times \\sqrt{13.8629436} \\approx 0.03 \\times 3.7232974\n$$\n$$\n\\lambda \\approx 0.1116989\n$$\nRounding this value to four significant figures gives:\n$$\n\\lambda \\approx 0.1117\n$$", "answer": "$$\n\\boxed{0.1117}\n$$", "id": "3392955"}, {"introduction": "While ISTA provides guaranteed convergence, it can be slow. Accelerated methods like FISTA often converge much faster by incorporating a \"momentum\" term, but this speed can come at the cost of non-monotone behavior where the objective value may temporarily increase. This advanced practice challenges you to first analyze the mathematical reason for this non-monotonicity and then to design, implement, and test a safeguarded, monotone version of FISTA, giving you hands-on experience in the analysis and improvement of cutting-edge optimization algorithms [@problem_id:3392933].", "problem": "Consider the composite convex optimization problem that arises in sparse linear inverse problems and sparse data assimilation: minimize the sum of a smooth data misfit and a sparsity-promoting penalty,\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; F(x) \\equiv f(x) + g(x),\n$$\nwhere\n$$\nf(x) \\equiv \\tfrac{1}{2}\\|A x - b\\|_2^2, \\quad g(x) \\equiv \\lambda \\|x\\|_1,\n$$\nwith a given matrix $A \\in \\mathbb{R}^{m \\times n}$, observation vector $b \\in \\mathbb{R}^m$, and regularization parameter $\\lambda \\ge 0$. Assume $f$ has a Lipschitz continuous gradient with constant $Lgt;0$, that is, for all $u,v \\in \\mathbb{R}^n$,\n$$\n\\|\\nabla f(u) - \\nabla f(v)\\|_2 \\le L \\|u - v\\|_2,\n$$\nequivalently, the descent lemma holds:\n$$\nf(u) \\le f(v) + \\nabla f(v)^\\top(u-v) + \\tfrac{L}{2}\\|u-v\\|_2^2.\n$$\nThe Iterative Soft-Thresholding Algorithm (ISTA), also called the proximal gradient method, uses the proximal operator of $g$ and a gradient step on $f$ to generate a sequence $\\{x^k\\}$ with step size $\\tau \\in (0,1/L]$. The Fast Iterative Soft-Thresholding Algorithm (FISTA) augments ISTA with an extrapolation (momentum) step that uses an auxiliary sequence $\\{y^k\\}$ constructed from $\\{x^k\\}$.\n\nTask 1. Starting from the descent lemma for $f$ and the definition of the proximal operator of $g$, provide a clear, rigorous argument explaining why FISTA may produce a nonmonotone objective sequence $\\{F(x^k)\\}$; that is, it may happen that $F(x^{k+1}) gt; F(x^k)$ for some $k$. Your explanation should be grounded in first principles, specifically:\n- the majorization of $f$ at a base point implied by Lipschitz continuity of $\\nabla f$,\n- the definition of the proximal update as the minimizer of the sum of a quadratic upper bound and $g$,\n- the difference between using the current point $x^k$ (ISTA) and an extrapolated point $y^k$ (FISTA) as the base for the proximal gradient step.\n\nDo not appeal to any pre-stated theorems about FISTA’s behavior; derive the reasoning directly from the above fundamentals.\n\nTask 2. Design a monotone variant of FISTA by safeguarding the update $x^{k+1}$ with a local check. The safeguard must be purely local (depending only on quantities computable at iteration $k$) and must guarantee that the resulting objective sequence is monotonically nonincreasing. Precisely define the safeguard decision and the two possible updates it selects between. Your design must include:\n- a condition that compares the candidate accelerated update to a baseline reference,\n- a fallback to a non-accelerated proximal gradient step whenever the condition is violated,\n- a rule for resetting or updating the momentum/extrapolation parameter to preserve monotonicity after a fallback.\n\nJustify, from the descent lemma and proximal optimality conditions, why your safeguarded scheme yields a monotonically nonincreasing sequence $\\{F(x^k)\\}$.\n\nTask 3. Implement a complete program that constructs several test instances and numerically compares ISTA, FISTA, and your monotone FISTA variant (MFISTA) in terms of monotonicity and final objective values. Follow all of the specifications below to ensure reproducibility and coverage.\n\nDefinitions and implementation requirements:\n- Use the proximal operator of $g(x)=\\lambda\\|x\\|_1$,\n$$\n\\operatorname{prox}_{\\tau g}(v) \\equiv \\arg\\min_{u \\in \\mathbb{R}^n}\\left\\{\\lambda\\|u\\|_1 + \\tfrac{1}{2\\tau}\\|u - v\\|_2^2\\right\\}.\n$$\n- Use the step size $\\tau = 1/L$, where $L$ is the Lipschitz constant of $\\nabla f$. For $f(x) = \\tfrac{1}{2}\\|A x - b\\|_2^2$, take $L$ to be the largest eigenvalue of $A^\\top A$ (that is, the square of the largest singular value of $A$).\n- Initialize all methods at the zero vector $x^0 = 0$, and for accelerated methods use $t^0 = 1$ in the standard extrapolation parameter sequence.\n- Run a fixed number of iterations $K = 200$ for each method on each test instance.\n\nTest suite (four cases):\n- Case 1 (ill-conditioned forward model, moderate regularization): Set $m=50$, $n=100$, random seed $s=7$. Construct $A$ with prescribed singular values by generating a Gaussian matrix with independent standard normal entries, computing its compact singular value decomposition, replacing its singular values with a linearly spaced vector from $1$ down to $10^{-3}$, and reassembling $A$. Construct a $10$-sparse ground truth $x^\\star$ by selecting $10$ unique indices uniformly at random and setting those entries to independent standard normal values, all others zero. Generate $b = A x^\\star + \\eta$ with independent Gaussian noise $\\eta$ of variance $\\sigma^2 = 10^{-4}$ (standard deviation $\\sigma = 10^{-2}$). Use $\\lambda = 10^{-2}$.\n- Case 2 (well-conditioned forward model, moderate regularization): Set $m=80$, $n=100$, seed $s=11$. Generate $A$ with independent standard normal entries and then scale each column to have unit Euclidean norm. Generate $x^\\star$ as in Case 1 with $10$ nonzeros and noise with $\\sigma = 10^{-2}$. Use $\\lambda = 10^{-2}$.\n- Case 3 (well-conditioned, strong regularization): Same as Case 2 but with seed $s=13$ and $\\lambda = 1$.\n- Case 4 (well-conditioned, no regularization): Same as Case 2 but with seed $s=17$ and $\\lambda = 0$.\n\nFor each case:\n- Compute the objective values $\\{F(x^k)\\}$ across iterations for ISTA, FISTA, and your MFISTA.\n- Determine whether each method’s objective sequence is monotonically nonincreasing, i.e., whether $F(x^{k+1}) \\le F(x^k)$ holds for all $k \\in \\{0,\\dots,K-1\\}$, up to numerical tolerance.\n- Record the final objective values after $K$ iterations for each method.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of length four (one per test case). Each element must itself be a list containing exactly five entries in the following order:\n  1. a boolean indicating whether the FISTA objective sequence is monotone,\n  2. a boolean indicating whether the MFISTA objective sequence is monotone,\n  3. the final ISTA objective value after $K$ iterations (a float),\n  4. the final FISTA objective value after $K$ iterations (a float),\n  5. the final MFISTA objective value after $K$ iterations (a float).\n- The top-level list must be printed as a single line, formatted as a comma-separated list enclosed in square brackets (for example, \"[[...],[...],[...],[...]]\"). No additional text may be printed.", "solution": "We consider the composite convex objective $F(x) \\equiv f(x) + g(x)$ with $f(x) \\equiv \\tfrac{1}{2}\\|A x - b\\|_2^2$ and $g(x) \\equiv \\lambda\\|x\\|_1$. The gradient $\\nabla f(x) = A^\\top(Ax - b)$ is Lipschitz with constant $L = \\|A^\\top A\\|_2$, the spectral norm (largest eigenvalue) of $A^\\top A$, equivalently the square of the largest singular value of $A$.\n\nPrinciple 1: Descent lemma and quadratic upper bound. For any $u,v \\in \\mathbb{R}^n$, Lipschitz continuity of $\\nabla f$ implies the descent lemma:\n$$\nf(u) \\le f(v) + \\nabla f(v)^\\top(u-v) + \\tfrac{L}{2}\\|u-v\\|_2^2 \\equiv Q_L(u;v).\n$$\nThus $Q_L(u;v)$ is a global upper bound (majorizer) for $f$ built around the base point $v$.\n\nPrinciple 2: Proximal gradient step minimizes the majorizer plus $g$. For a step size $\\tau \\in (0,1/L]$, the proximal gradient update from a base point $v$ computes\n$$\nx^+ \\equiv \\operatorname{prox}_{\\tau g}\\!\\left(v - \\tau \\nabla f(v)\\right) \\in \\arg\\min_{u} \\left\\{g(u) + \\tfrac{1}{2\\tau}\\|u - (v - \\tau \\nabla f(v))\\|_2^2\\right\\}.\n$$\nEquivalently, $x^+$ minimizes $g(u) + Q_{1/\\tau}(u;v) - f(v)$, hence\n$$\ng(x^+) + Q_{1/\\tau}(x^+;v) \\le g(v) + Q_{1/\\tau}(v;v) = g(v) + f(v).\n$$\nFor $\\tau \\le 1/L$, we have $Q_{1/\\tau}(u;v) \\ge f(u)$ by the descent lemma, therefore\n$$\nF(x^+) = f(x^+) + g(x^+) \\le g(x^+) + Q_{1/\\tau}(x^+;v) \\le g(v) + f(v) = F(v).\n$$\n\nInterpretation for ISTA. In ISTA, we take $v = x^k$, and define\n$$\nx^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\left(x^k - \\tau \\nabla f(x^k)\\right).\n$$\nApplying the above inequality with $v = x^k$ shows the monotone decrease:\n$$\nF(x^{k+1}) \\le F(x^k) \\quad \\text{for all } k \\text{ when } \\tau \\le 1/L.\n$$\n\nWhy FISTA may be nonmonotone. FISTA constructs an extrapolated point $y^k$ from $x^k$ and $x^{k-1}$, and then performs a proximal gradient step at $y^k$:\n$$\nx^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\left(y^k - \\tau \\nabla f(y^k)\\right), \\quad \\text{with a momentum-defined } y^k \\ne x^k.\n$$\nFrom Principle 2 with $v = y^k$, we have\n$$\nF(x^{k+1}) \\le F(y^k).\n$$\nHowever, the extrapolated point $y^k$ is not constrained to satisfy $F(y^k) \\le F(x^k)$; the momentum step is chosen to improve convergence rate, not to enforce descent, and may overshoot. Consequently, the chain of inequalities that guarantees monotonicity for ISTA, namely\n$$\nF(x^{k+1}) \\le F(y^k) \\le F(x^k),\n$$\ncan fail at the second inequality. When $F(y^k)  F(x^k)$, it is entirely possible that $F(x^{k+1}) \\in [F(x^k), F(y^k)]$, implying $F(x^{k+1})  F(x^k)$. Therefore, FISTA can produce a nonmonotone objective sequence, because its proximal step is anchored at $y^k$, and the descent lemma only provides an upper bound with respect to the base point $y^k$, not with respect to the current iterate $x^k$.\n\nDesign of a monotone variant via a local safeguard. To enforce monotonicity, we combine the accelerated candidate with a local acceptance test. At iteration $k$ with current point $x^k$ and extrapolated point $y^k$, construct the accelerated candidate\n$$\n\\hat{x}^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\left(y^k - \\tau \\nabla f(y^k)\\right).\n$$\nPerform the local check: if $F(\\hat{x}^{k+1}) \\le F(x^k)$, accept the accelerated candidate and update the momentum parameter as in FISTA; else, reject the candidate, compute the fallback ISTA step from $x^k$,\n$$\n\\tilde{x}^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\left(x^k - \\tau \\nabla f(x^k)\\right),\n$$\nand reset the momentum (for example, set the extrapolation parameter to its initial value, so that the next $y^{k+1}$ equals $x^{k+1}$). The accepted $x^{k+1}$ is\n$$\nx^{k+1} = \\begin{cases}\n\\hat{x}^{k+1},  \\text{if } F(\\hat{x}^{k+1}) \\le F(x^k),\\\\\n\\tilde{x}^{k+1},  \\text{otherwise,}\n\\end{cases}\n$$\nand the accepted extrapolation state $y^{k+1}$ is taken as $x^{k+1}$ whenever a fallback occurs (momentum reset), or as the usual FISTA extrapolation otherwise.\n\nMonotonicity justification. If the accelerated candidate is accepted, we have $F(x^{k+1}) \\le F(x^k)$ by the acceptance rule. If it is rejected, then by applying Principle 2 with $v = x^k$ and $\\tau \\le 1/L$, the fallback ISTA step satisfies\n$$\nF(\\tilde{x}^{k+1}) \\le F(x^k).\n$$\nThus, in either branch, $F(x^{k+1}) \\le F(x^k)$. By induction, the entire sequence $\\{F(x^k)\\}$ is monotonically nonincreasing. Crucially, the safeguard is local: it uses only $F(x^k)$ and $F(\\hat{x}^{k+1})$, which are computable at iteration $k$, and it guarantees descent without requiring any global knowledge.\n\nProximal operator for the $\\ell_1$ norm. To implement the updates, we need the proximal operator for $g(x) = \\lambda\\|x\\|_1$ with step size $\\tau$. By separability, for $v \\in \\mathbb{R}^n$, the proximal point $u = \\operatorname{prox}_{\\tau g}(v)$ is componentwise optimal:\n$$\nu_i \\in \\arg\\min_{t \\in \\mathbb{R}} \\left\\{\\lambda|t| + \\tfrac{1}{2\\tau}(t - v_i)^2\\right\\}.\n$$\nThe subgradient optimality condition for $t \\ne 0$ is\n$$\n0 \\in \\lambda \\,\\partial |t| + \\tfrac{1}{\\tau}(t - v_i) =\n\\lambda \\,\\mathrm{sign}(t) + \\tfrac{1}{\\tau}(t - v_i),\n$$\nwhich yields $t = v_i - \\tau \\lambda \\,\\mathrm{sign}(t)$ provided $|v_i|  \\tau \\lambda$. If $|v_i| \\le \\tau \\lambda$, the minimum occurs at $t = 0$ by checking subgradient inclusions. This leads to the soft-thresholding operator\n$$\n\\operatorname{prox}_{\\tau g}(v)_i = \\mathrm{sign}(v_i)\\,\\max\\{|v_i| - \\tau \\lambda, 0\\},\n$$\napplied componentwise for $i=1,\\dots,n$.\n\nComputation of the Lipschitz constant. For $f(x) = \\tfrac{1}{2}\\|A x - b\\|_2^2$, we have $\\nabla f(x) = A^\\top (A x - b)$, and the Lipschitz constant is the operator norm of the Hessian surrogate $A^\\top A$:\n$$\nL = \\|A^\\top A\\|_2 = \\sigma_{\\max}(A)^2,\n$$\nwhere $\\sigma_{\\max}(A)$ denotes the largest singular value of $A$. We therefore choose the step size $\\tau = 1/L$.\n\nNumerical experiment design and expected behavior. We compare ISTA, FISTA, and the safeguarded monotone FISTA (MFISTA) across four test cases that probe ill-conditioning, well-conditioning, strong regularization (tending toward sparsity or zeros), and the smooth case with $\\lambda = 0$. ISTA must produce a monotone objective sequence by the descent lemma argument. FISTA can exhibit transient nonmonotonicity due to extrapolation overshoot, especially in ill-conditioned problems or early iterations. MFISTA enforces monotonicity by a local acceptance test and fallback to ISTA when needed, so its objective sequence is guaranteed to be monotone. In practice, MFISTA often combines the faster convergence of FISTA with the robustness of ISTA, yielding final objective values that are no worse than those of ISTA and frequently as good as or better than those of FISTA.\n\nImplementation details. The program constructs the matrices $A$ and vectors $b$ according to the specified seeds and procedures, computes $L$ via the largest singular value of $A$, runs each algorithm for $K=200$ iterations with $\\tau=1/L$, records the objective values during the iterations, checks monotonicity by verifying $F(x^{k+1}) \\le F(x^k)$ up to a small numerical tolerance, and prints the results in the required single-line format aggregating the booleans and the final objective values for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef soft_threshold(v, alpha):\n    # Proximal operator of alpha * ||.||_1\n    return np.sign(v) * np.maximum(np.abs(v) - alpha, 0.0)\n\ndef objective(A, b, x, lam):\n    r = A @ x - b\n    return 0.5 * float(r.T @ r) + lam * float(np.linalg.norm(x, 1))\n\ndef lipschitz_constant(A):\n    # L = ||A^T A||_2 = sigma_max(A)^2\n    # Use SVD to get largest singular value\n    s = np.linalg.svd(A, compute_uv=False)\n    sigma_max = np.max(s)\n    return sigma_max * sigma_max\n\ndef ista(A, b, lam, K):\n    L = lipschitz_constant(A)\n    tau = 1.0 / L if L  0 else 1.0\n    x = np.zeros(A.shape[1])\n    objs = []\n    for _ in range(K):\n        grad = A.T @ (A @ x - b)\n        x = soft_threshold(x - tau * grad, tau * lam)\n        objs.append(objective(A, b, x, lam))\n    return x, np.array(objs)\n\ndef fista(A, b, lam, K):\n    L = lipschitz_constant(A)\n    tau = 1.0 / L if L  0 else 1.0\n    x = np.zeros(A.shape[1])\n    y = x.copy()\n    t = 1.0\n    objs = []\n    for _ in range(K):\n        grad = A.T @ (A @ y - b)\n        x_new = soft_threshold(y - tau * grad, tau * lam)\n        t_new = 0.5 * (1.0 + np.sqrt(1.0 + 4.0 * t * t))\n        y = x_new + ((t - 1.0) / t_new) * (x_new - x)\n        x = x_new\n        t = t_new\n        objs.append(objective(A, b, x, lam))\n    return x, np.array(objs)\n\ndef mfista(A, b, lam, K):\n    L = lipschitz_constant(A)\n    tau = 1.0 / L if L  0 else 1.0\n    x = np.zeros(A.shape[1])\n    y = x.copy()\n    t = 1.0\n    objs = []\n    f_prev = objective(A, b, x, lam)\n    # record initial objective at x^0 for comparison in checks\n    for _ in range(K):\n        # Accelerated candidate from y\n        grad_y = A.T @ (A @ y - b)\n        x_cand = soft_threshold(y - tau * grad_y, tau * lam)\n        f_cand = objective(A, b, x_cand, lam)\n        if f_cand = f_prev + 1e-12:\n            # accept accelerated step\n            x_new = x_cand\n            f_new = f_cand\n            t_new = 0.5 * (1.0 + np.sqrt(1.0 + 4.0 * t * t))\n            y_new = x_new + ((t - 1.0) / t_new) * (x_new - x)\n            t = t_new\n            y = y_new\n            x = x_new\n            f_prev = f_new\n        else:\n            # fallback to ISTA step from x, reset momentum\n            grad_x = A.T @ (A @ x - b)\n            x_new = soft_threshold(x - tau * grad_x, tau * lam)\n            f_new = objective(A, b, x_new, lam)\n            # Guarantee f_new = f_prev by proximal gradient descent\n            x = x_new\n            y = x_new.copy()\n            t = 1.0\n            f_prev = f_new\n        objs.append(f_prev)\n    return x, np.array(objs)\n\ndef is_monotone_nonincreasing(arr, tol=1e-12):\n    return bool(np.all(arr[1:] = arr[:-1] + tol))\n\ndef make_case_ill_conditioned(seed, m, n, k_sparsity, sigma_noise, lam):\n    rng = np.random.default_rng(seed)\n    G = rng.standard_normal((m, n))\n    # compact SVD\n    U, _, Vt = np.linalg.svd(G, full_matrices=False)\n    r = min(m, n)\n    svals = np.linspace(1.0, 1e-3, r)  # linearly spaced singular values\n    S = np.diag(svals)\n    A = U @ S @ Vt\n    # sparse ground truth\n    x_true = np.zeros(n)\n    idx = rng.choice(n, size=k_sparsity, replace=False)\n    x_true[idx] = rng.standard_normal(k_sparsity)\n    noise = sigma_noise * rng.standard_normal(m)\n    b = A @ x_true + noise\n    return A, b, lam\n\ndef make_case_well_conditioned(seed, m, n, k_sparsity, sigma_noise, lam):\n    rng = np.random.default_rng(seed)\n    A = rng.standard_normal((m, n))\n    # scale columns to unit norm\n    col_norms = np.linalg.norm(A, axis=0)\n    col_norms[col_norms == 0.0] = 1.0\n    A = A / col_norms\n    x_true = np.zeros(n)\n    idx = rng.choice(n, size=k_sparsity, replace=False)\n    x_true[idx] = rng.standard_normal(k_sparsity)\n    noise = sigma_noise * rng.standard_normal(m)\n    b = A @ x_true + noise\n    return A, b, lam\n\ndef solve():\n    K = 200\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple with a function to generate (A,b,lam) and parameters.\n    test_cases = [\n        (\"ill\", dict(seed=7, m=50, n=100, k_sparsity=10, sigma_noise=1e-2, lam=1e-2)),\n        (\"well\", dict(seed=11, m=80, n=100, k_sparsity=10, sigma_noise=1e-2, lam=1e-2)),\n        (\"well\", dict(seed=13, m=80, n=100, k_sparsity=10, sigma_noise=1e-2, lam=1.0)),\n        (\"well\", dict(seed=17, m=80, n=100, k_sparsity=10, sigma_noise=1e-2, lam=0.0)),\n    ]\n\n    results = []\n    for case_type, params in test_cases:\n        if case_type == \"ill\":\n            A, b, lam = make_case_ill_conditioned(**params)\n        else:\n            A, b, lam = make_case_well_conditioned(**params)\n\n        # Run ISTA\n        x_i, objs_i = ista(A, b, lam, K)\n        # Run FISTA\n        x_f, objs_f = fista(A, b, lam, K)\n        # Run MFISTA\n        x_m, objs_m = mfista(A, b, lam, K)\n\n        mon_fista = is_monotone_nonincreasing(objs_f)\n        mon_mfista = is_monotone_nonincreasing(objs_m)\n        final_ista = float(objs_i[-1])\n        final_fista = float(objs_f[-1])\n        final_mfista = float(objs_m[-1])\n\n        results.append([mon_fista, mon_mfista, final_ista, final_fista, final_mfista])\n\n    # Final print statement in the exact required format.\n    print(f\"{results}\")\n\nsolve()\n```", "id": "3392933"}]}