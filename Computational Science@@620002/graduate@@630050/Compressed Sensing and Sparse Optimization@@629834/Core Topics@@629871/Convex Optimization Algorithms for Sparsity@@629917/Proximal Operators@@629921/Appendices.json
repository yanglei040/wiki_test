{"hands_on_practices": [{"introduction": "To build a solid foundation, our first practice involves deriving a proximal operator from first principles. We will tackle the negative entropy regularizer, a function of paramount importance in machine learning and statistical inference for enforcing non-negativity and promoting \"spread-out\" solutions. This exercise will guide you through applying optimality conditions to derive a closed-form solution involving a special function, and it will challenge you to analyze its numerical behavior and connection to classical multiplicative update algorithms [@problem_id:3470855].", "problem": "Let $\\lambda > 0$ and define the separable convex function $f : \\mathbb{R}^{n} \\to \\mathbb{R} \\cup \\{+\\infty\\}$ by\n$$\nf(x) \\triangleq \\lambda \\sum_{i=1}^{n} \\big(x_{i} \\ln x_{i} - x_{i}\\big) + \\iota_{\\{x \\in \\mathbb{R}^{n} : x \\ge 0\\}}(x),\n$$\nwhere $\\iota_{\\{x \\ge 0\\}}$ is the indicator of the nonnegative orthant and $x \\ln x$ is extended by continuity at $x=0$ so that $x \\ln x \\to 0$ as $x \\downarrow 0$. Consider the proximal operator of $f$ at a given $y \\in \\mathbb{R}^{n}$,\n$$\n\\operatorname{prox}_{f}(y) \\triangleq \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|x - y\\|_{2}^{2} + f(x) \\right\\}.\n$$\nStarting only from the definition of the proximal operator, elementary properties of convex functions, and standard first-order optimality conditions, do the following:\n\n- Prove that the minimization is separable across coordinates and that the unique minimizer satisfies $x_{i}^{\\star} > 0$ for all $i \\in \\{1,\\dots,n\\}$.\n\n- Derive the optimality condition for each coordinate and solve it in closed form using a standard special function as needed, yielding an explicit analytic formula for $\\operatorname{prox}_{f}(y)$ as a function of $y$ and $\\lambda$.\n\n- Show how the scalar optimality condition can be rearranged into a fixed-point form of the type $x_{i} = c_{i} \\exp(-x_{i}/\\lambda)$ with $c_{i} > 0$, and explain how this gives a multiplicative update iteration $x_{i}^{(t+1)} = c_{i} \\exp\\!\\big(-x_{i}^{(t)}/\\lambda\\big)$ that converges to the proximal point. Briefly discuss the relation of this multiplicative structure to algorithms based on negative-entropy regularization and Kullback–Leibler divergence (KLD).\n\n- Analyze numerical stability of evaluating your closed-form expression when the resulting entries are very small (i.e., when $x_{i}^{\\star} \\ll 1$). Provide the leading-order asymptotic expansion of $x_{i}^{\\star}$ in terms of $y_{i}$ and $\\lambda$ in the regime producing $x_{i}^{\\star} \\to 0^{+}$, and discuss a numerically stable evaluation strategy in that regime.\n\nYour final answer must be a single closed-form analytic expression for $\\operatorname{prox}_{f}(y)$ in terms of $y$ and $\\lambda$ (you may use standard special functions). Do not include any units. Do not provide an inequality or an equation as the final answer.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It is a standard problem in convex optimization concerning the proximal operator of the negative entropy regularizer. All definitions and conditions are provided, and there are no contradictions or ambiguities. The problem is solvable using established mathematical principles.\n\nThe objective is to compute the proximal operator of the function $f : \\mathbb{R}^{n} \\to \\mathbb{R} \\cup \\{+\\infty\\}$ defined by\n$$\nf(x) \\triangleq \\lambda \\sum_{i=1}^{n} \\big(x_{i} \\ln x_{i} - x_{i}\\big) + \\iota_{\\{x \\in \\mathbb{R}^{n} : x \\ge 0\\}}(x)\n$$\nfor a parameter $\\lambda > 0$. The proximal operator at a point $y \\in \\mathbb{R}^{n}$ is defined as\n$$\n\\operatorname{prox}_{f}(y) \\triangleq \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|x - y\\|_{2}^{2} + f(x) \\right\\}.\n$$\nLet the objective function of this minimization problem be denoted by $G(x)$. Substituting the definition of $f(x)$, we have\n$$\nG(x) = \\frac{1}{2} \\|x - y\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} \\big(x_{i} \\ln x_{i} - x_{i}\\big) + \\iota_{\\{x \\ge 0\\}}(x).\n$$\nThe indicator function $\\iota_{\\{x \\ge 0\\}}(x)$ restricts the domain of the minimization to the non-negative orthant, i.e., $x_i \\ge 0$ for all $i=1, \\dots, n$.\n\n**Separability and Positivity of the Solution**\n\nThe objective function $G(x)$ can be written in a separable form by expanding the squared Euclidean norm:\n$$\nG(x) = \\frac{1}{2} \\sum_{i=1}^{n} (x_i - y_i)^2 + \\lambda \\sum_{i=1}^{n} (x_i \\ln x_i - x_i)\n$$\nsubject to the constraint $x_i \\ge 0$ for all $i$. The sums can be combined:\n$$\nG(x) = \\sum_{i=1}^{n} \\left( \\frac{1}{2} (x_i - y_i)^2 + \\lambda (x_i \\ln x_i - x_i) \\right).\n$$\nLet $g_i(x_i) = \\frac{1}{2} (x_i - y_i)^2 + \\lambda (x_i \\ln x_i - x_i)$. The total minimization problem $\\min_{x \\ge 0} G(x)$ decouples into $n$ independent scalar minimization problems:\n$$\n\\min_{x_i \\ge 0} g_i(x_i) \\quad \\text{for each } i \\in \\{1, \\dots, n\\}.\n$$\nThis proves that the minimization is separable across coordinates.\n\nTo prove that the unique minimizer $x_i^\\star$ is strictly positive, we analyze the scalar function $g_i(x_i)$ for $x_i \\ge 0$. The function $\\phi(t) = t \\ln t - t$ is strictly convex for $t>0$. Its second derivative is $\\phi''(t) = 1/t > 0$. The function $\\frac{1}{2}(x_i-y_i)^2$ is also strictly convex. Since $\\lambda > 0$, $g_i(x_i)$ is a sum of a strictly convex function and a convex function, hence it is strictly convex on its domain $\\{x_i : x_i \\ge 0\\}$. A strictly convex function on a closed convex set has a unique minimum.\n\nLet's examine the derivative of $g_i(x_i)$ for $x_i > 0$:\n$$\ng_i'(x_i) = \\frac{d}{dx_i} \\left( \\frac{1}{2}(x_i - y_i)^2 + \\lambda(x_i \\ln x_i - x_i) \\right) = (x_i - y_i) + \\lambda(\\ln x_i + 1 - 1) = x_i - y_i + \\lambda \\ln x_i.\n$$\nWe consider the limit of the derivative as $x_i$ approaches $0$ from the right:\n$$\n\\lim_{x_i \\to 0^+} g_i'(x_i) = \\lim_{x_i \\to 0^+} (x_i - y_i + \\lambda \\ln x_i) = -\\infty.\n$$\nSince the derivative is negative for values of $x_i$ close to $0$, the function $g_i(x_i)$ is decreasing as $x_i$ approaches $0$. Therefore, the minimum cannot occur at the boundary point $x_i = 0$. The unique minimizer $x_i^\\star$ must lie in the interior of the domain, i.e., $x_i^\\star > 0$.\n\n**Derivation of the Closed-Form Solution**\n\nSince the minimizer $x_i^\\star$ is positive, it must satisfy the first-order optimality condition $g_i'(x_i^\\star) = 0$:\n$$\nx_i^\\star - y_i + \\lambda \\ln x_i^\\star = 0.\n$$\nThis transcendental equation can be solved for $x_i^\\star$. Rearranging the terms, we get:\n$$\n\\lambda \\ln x_i^\\star + x_i^\\star = y_i.\n$$\nTo solve this, we will put it into a form compatible with the Lambert W function, which is defined by the relation $W(z) \\exp(W(z)) = z$.\nDivide the equation by $\\lambda$:\n$$\n\\ln x_i^\\star + \\frac{x_i^\\star}{\\lambda} = \\frac{y_i}{\\lambda}.\n$$\nExponentiating both sides gives:\n$$\n\\exp\\left(\\ln x_i^\\star + \\frac{x_i^\\star}{\\lambda}\\right) = \\exp\\left(\\frac{y_i}{\\lambda}\\right)\n$$\n$$\n\\exp(\\ln x_i^\\star) \\exp\\left(\\frac{x_i^\\star}{\\lambda}\\right) = \\exp\\left(\\frac{y_i}{\\lambda}\\right)\n$$\n$$\nx_i^\\star \\exp\\left(\\frac{x_i^\\star}{\\lambda}\\right) = \\exp\\left(\\frac{y_i}{\\lambda}\\right).\n$$\nTo match the form $Z \\exp(Z) = C$, let's multiply by $1/\\lambda$:\n$$\n\\frac{x_i^\\star}{\\lambda} \\exp\\left(\\frac{x_i^\\star}{\\lambda}\\right) = \\frac{1}{\\lambda} \\exp\\left(\\frac{y_i}{\\lambda}\\right).\n$$\nLet $Z_i = \\frac{x_i^\\star}{\\lambda}$. The equation becomes $Z_i \\exp(Z_i) = \\frac{1}{\\lambda} \\exp\\left(\\frac{y_i}{\\lambda}\\right)$. The solution for $Z_i$ is given by the Lambert W function:\n$$\nZ_i = W\\left(\\frac{1}{\\lambda} \\exp\\left(\\frac{y_i}{\\lambda}\\right)\\right).\n$$\nSubstituting back $Z_i = \\frac{x_i^\\star}{\\lambda}$, we find the solution for $x_i^\\star$:\n$$\nx_i^\\star = \\lambda W\\left(\\frac{1}{\\lambda} \\exp\\left(\\frac{y_i}{\\lambda}\\right)\\right).\n$$\nThe argument of the Lambert W function is always positive, so we use the principal branch, denoted by $W$. This formula gives the unique positive minimizer for each component. The full proximal operator result is a vector where each component is given by this expression.\n\n**Fixed-Point Form and Multiplicative Update**\n\nThe optimality condition $x_{i} - y_{i} + \\lambda \\ln x_{i} = 0$ can be rearranged into a fixed-point equation.\n$$\n\\lambda \\ln x_i = y_i - x_i\n$$\n$$\n\\ln x_i = \\frac{y_i - x_i}{\\lambda}\n$$\nExponentiating both sides yields:\n$$\nx_i = \\exp\\left(\\frac{y_i - x_i}{\\lambda}\\right) = \\exp\\left(\\frac{y_i}{\\lambda}\\right) \\exp\\left(-\\frac{x_i}{\\lambda}\\right).\n$$\nThis equation is of the form $x_i = c_i \\exp(-x_i/\\lambda)$, where $c_i = \\exp(y_i/\\lambda)$. Since $y_i \\in \\mathbb{R}$, the constant $c_i$ is always positive. This fixed-point form naturally suggests an iterative update rule:\n$$\nx_{i}^{(t+1)} = c_{i} \\exp\\left(-\\frac{x_{i}^{(t)}}{\\lambda}\\right) = \\exp\\left(\\frac{y_i}{\\lambda}\\right) \\exp\\left(-\\frac{x_{i}^{(t)}}{\\lambda}\\right).\n$$\nThis is a multiplicative update, as the new value is obtained by scaling a factor dependent on the old value. Such updates intrinsically preserve the non-negativity of $x_i$, provided the initial value $x_i^{(0)}$ is positive.\nThis structure is common in algorithms for optimization problems regularized by negative entropy. The function $f(x)$ is, up to scaling and a linear term, the negative entropy. The Bregman divergence associated with the convex generator $\\sum_i (x_i \\ln x_i - x_i)$ is the unnormalized Kullback-Leibler divergence (KLD). Algorithms such as mirror descent or exponentiated gradient for KLD geometry lead to multiplicative updates. The derived iteration can be seen as a special case of such methods for solving this specific proximal problem.\n\n**Numerical Stability Analysis**\n\nThe closed-form solution is $x_{i}^{\\star} = \\lambda W\\left(\\frac{1}{\\lambda} \\exp\\left(\\frac{y_i}{\\lambda}\\right)\\right)$. We analyze its numerical behavior when $x_i^\\star$ is very small, i.e., $x_i^\\star \\to 0^+$. This corresponds to the argument of the Lambert W function becoming very small. Let $z_i = \\frac{1}{\\lambda} \\exp\\left(\\frac{y_i}{\\lambda}\\right)$. For $z_i \\to 0$, we must have $y_i \\to -\\infty$.\nThe Taylor series expansion of the Lambert W function around $z=0$ is $W(z) = z - z^2 + O(z^3)$. Using this, we can find the leading-order asymptotic expansion for $x_i^\\star$:\n$$\nx_i^\\star = \\lambda W(z_i) \\approx \\lambda z_i = \\lambda \\left(\\frac{1}{\\lambda} \\exp\\left(\\frac{y_i}{\\lambda}\\right)\\right) = \\exp\\left(\\frac{y_i}{\\lambda}\\right).\n$$\nThis approximation is valid for $y_i \\ll -\\lambda$.\nNumerically, if $y_i/\\lambda$ is a large negative number (e.g., $y_i/\\lambda < -100$), the term $\\exp(y_i/\\lambda)$ can underflow to zero in standard floating-point arithmetic. A direct evaluation of the Lambert W function might then return $W(0)=0$, leading to an incorrect result $x_i^\\star=0$.\nA numerically stable evaluation strategy should switch to the asymptotic approximation when $z_i$ is small. Thus, for sufficiently negative $y_i/\\lambda$, one should compute $x_i^\\star$ as $\\exp(y_i/\\lambda)$. This expression is numerically stable; if it underflows to zero, it means the true value is smaller than the machine epsilon, and zero is the best floating-point representation.\nFor example, a practical a strategy would be:\nIf $y_i/\\lambda < T$ for some threshold $T$ (e.g., $T=-30$):\n  $x_i^\\star = \\exp(y_i/\\lambda)$\nElse:\n  $x_i^\\star = \\lambda W(\\frac{1}{\\lambda} \\exp(y_i/\\lambda))$ (using a library implementation of $W$).\nThe fixed-point iteration $x_{i}^{(t+1)} = \\exp(y_i/\\lambda) \\exp(-x_{i}^{(t)}/\\lambda)$ is also a stable way to compute small values of $x_i^\\star$. Starting with $x_i^{(0)}=0$, the first step gives $x_i^{(1)} = \\exp(y_i/\\lambda)$, which is already the leading-order approximation.\n\nThe final answer requested is the single closed-form expression for the proximal operator. Interpreting this as the element-wise operation on the input vector $y$, the solution is given below.", "answer": "$$\n\\boxed{\\lambda W\\left(\\frac{1}{\\lambda} \\exp\\left(\\frac{y}{\\lambda}\\right)\\right)}\n$$", "id": "3470855"}, {"introduction": "Beyond calculating individual proximal operators, it is crucial to understand their deep structural properties. This practice explores Moreau's identity, a cornerstone of proximal calculus that elegantly decomposes a point into the sum of the proximal points of a function and its convex conjugate, $f^*$. You will analytically and numerically verify this identity for the $\\ell_\\infty$ norm, revealing its fundamental relationship with projection onto the $\\ell_1$ ball and solidifying your understanding of Fenchel duality in action [@problem_id:3470868].", "problem": "Consider the real finite-dimensional vector space with the standard Euclidean inner product. Let $f : \\mathbb{R}^n \\to \\mathbb{R}$ be a proper, lower semicontinuous convex function. The proximal operator of $f$ with parameter $\\lambda \\ge 0$ is defined by\n$$\n\\operatorname{prox}_{\\lambda f}(x) := \\arg\\min_{u \\in \\mathbb{R}^n} \\left\\{ \\tfrac{1}{2} \\|u - x\\|_2^2 + \\lambda f(u) \\right\\},\n$$\nand the convex conjugate (Fenchel conjugate) of $f$ is defined by\n$$\nf^*(y) := \\sup_{u \\in \\mathbb{R}^n} \\left\\{ \\langle y, u \\rangle - f(u) \\right\\}.\n$$\nDenote by $\\|\\cdot\\|_p$ the usual $\\ell_p$ norm on $\\mathbb{R}^n$, and let $\\mathbb{B}_1 := \\{ y \\in \\mathbb{R}^n : \\|y\\|_1 \\le 1 \\}$ be the unit $\\ell_1$ ball. The indicator function of a set $C \\subseteq \\mathbb{R}^n$ is defined by $I_C(x) = 0$ if $x \\in C$ and $I_C(x) = +\\infty$ otherwise. The exact projection of a vector $v \\in \\mathbb{R}^n$ onto $\\mathbb{B}_1$ is the unique minimizer of $\\tfrac{1}{2} \\|z - v\\|_2^2$ subject to $\\|z\\|_1 \\le 1$.\n\nThe task is to verify Moreau’s identity for a nontrivial function. Take $f(x) = \\|x\\|_\\infty$, the $\\ell_\\infty$ norm, and prove the identity\n$$\nx = \\operatorname{prox}_{\\lambda \\| \\cdot \\|_\\infty}(x) + \\lambda \\operatorname{prox}_{f^*/\\lambda}\\left(\\tfrac{x}{\\lambda}\\right),\n$$\nfor all $x \\in \\mathbb{R}^n$ and $\\lambda \\ge 0$, where $f^*$ is the convex conjugate of $f$. Then, implement both sides numerically in a single program using two independent algorithms: one to compute $\\operatorname{prox}_{\\lambda \\| \\cdot \\|_\\infty}(x)$ that does not invoke Moreau’s identity, and one to compute $\\operatorname{prox}_{f^*/\\lambda}(x/\\lambda)$ via exact projection onto the unit $\\ell_1$ ball. Your implementation must be robust for arbitrary finite vectors.\n\nYour program must use the following test suite of $(x,\\lambda)$ pairs:\n- Test case $1$: $x = [3.2, -1.1, 0.0, 5.5, -2.2]$, $\\lambda = 1.7$.\n- Test case $2$: $x = [0.0, 0.0, 0.0, 0.0]$, $\\lambda = 2.5$.\n- Test case $3$: $x = [10.0, -10.0]$, $\\lambda = 0.0$.\n- Test case $4$: $x = [-0.3, 0.3, -0.3, 0.3, -0.3, 0.3]$, $\\lambda = 0.5$.\n- Test case $5$: For $n = 20$, define $x \\in \\mathbb{R}^{20}$ componentwise by $x_i = \\sin(i) + \\tfrac{1}{2} \\cos(2 i)$ for $i = 1,2,\\dots,20$, with the arguments of the sine and cosine functions in radians, and set $\\lambda = 3.0$.\n\nFor each test case, check numerically whether\n$$\nx \\stackrel{?}{=} \\operatorname{prox}_{\\lambda \\|\\cdot\\|_\\infty}(x) + \\lambda \\operatorname{prox}_{f^*/\\lambda}\\left(\\tfrac{x}{\\lambda}\\right),\n$$\nwithin a numerical tolerance of absolute error $10^{-9}$ and relative error $10^{-10}$ in each coordinate (use the same tolerance across all tests). The output for each test case must be a boolean indicating whether the identity holds to the specified tolerance.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1, result_2, result_3, result_4, result_5]$). No external input is permitted. All angles must be in radians. No physical units are involved. The program must be fully self-contained and executable in a modern language, without requiring any external files or network access. The use of only standard numerical linear algebra operations is permitted.", "solution": "The problem requires the verification of Moreau’s identity for the function $f(x) = \\|x\\|_\\infty$, the $\\ell_\\infty$ norm on $\\mathbb{R}^n$. The identity states that for any $x \\in \\mathbb{R}^n$ and $\\lambda \\ge 0$,\n$$\nx = \\operatorname{prox}_{\\lambda f}(x) + \\lambda \\operatorname{prox}_{f^*/\\lambda}\\left(\\tfrac{x}{\\lambda}\\right)\n$$\nwhere $f^*$ is the convex conjugate of $f$. We will develop independent algorithms for the two proximal terms on the right-hand side and numerically verify the identity for a set of test cases.\n\nThe case $\\lambda=0$ is trivial. The proximal operator $\\operatorname{prox}_{0 \\cdot f}(x)$ reduces to $\\arg\\min_u \\frac{1}{2}\\|u-x\\|_2^2$, whose unique solution is $u=x$. Moreau's identity becomes $x = x + 0 \\cdot \\operatorname{prox}_{f^*/0}(x/0) = x$, which is always true. We will handle this case separately. For the remainder of the derivation, we assume $\\lambda > 0$.\n\n### Term 1: $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_\\infty}(x)$\n\nBy definition, the first term is the solution to the optimization problem:\n$$\nu = \\operatorname{prox}_{\\lambda \\|\\cdot\\|_\\infty}(x) := \\arg\\min_{v \\in \\mathbb{R}^n} \\left\\{ \\tfrac{1}{2} \\|v - x\\|_2^2 + \\lambda \\|v\\|_\\infty \\right\\}\n$$\nLet $t = \\|v\\|_\\infty$. The problem is equivalent to finding the solution to\n$$\n\\min_{t \\ge 0} \\left( \\min_{\\|v\\|_\\infty \\le t} \\tfrac{1}{2} \\|v - x\\|_2^2 + \\lambda t \\right)\n$$\nThe inner minimization is a projection of $x$ onto the $\\ell_\\infty$-ball of radius $t$ (a hypercube), which can be solved component-wise: $v_i = \\operatorname{sign}(x_i) \\min(|x_i|, t)$. Substituting this back, we obtain a a problem in the single variable $t$:\n$$\n\\min_{t \\ge 0} g(t) \\quad \\text{where} \\quad g(t) = \\tfrac{1}{2} \\sum_{i=1}^n (\\operatorname{sign}(x_i) \\min(|x_i|, t) - x_i)^2 + \\lambda t\n$$\nSimplifying the squared term, we get $(\\min(|x_i|, t) - |x_i|)^2 = (\\max(0, |x_i|-t))^2$. So,\n$$\ng(t) = \\tfrac{1}{2} \\sum_{i=1}^n (\\max(0, |x_i|-t))^2 + \\lambda t\n$$\nThe function $g(t)$ is convex and its derivative is $g'(t) = -\\sum_{i=1}^n \\max(0, |x_i|-t) + \\lambda$. Setting the derivative to zero gives the optimality condition for the threshold $t^*$:\n$$\n\\sum_{i=1}^n \\max(0, |x_i|-t^*) = \\lambda\n$$\nThe function $\\phi(t) = \\sum_{i=1}^n \\max(0, |x_i|-t)$ is continuous and monotonically decreasing. If $\\phi(0) = \\sum |x_i| = \\|x\\|_1 \\le \\lambda$, then the minimum of $g(t)$ is at $t^*=0$, which implies $u=0$. Otherwise, a unique root $t^* \\in (0, \\max_i|x_i|]$ exists. We can find this root using a numerical method like bisection or Brent's method. Once $t^*$ is found, the solution is given by $u_i = \\operatorname{sign}(x_i) \\min(|x_i|, t^*)$. This provides an algorithm for computing the first term without invoking Moreau's identity.\n\n### Term 2: $\\lambda \\operatorname{prox}_{f^*/\\lambda}\\left(\\tfrac{x}{\\lambda}\\right)$\n\nFirst, we must find the convex conjugate $f^*$ of $f(x) = \\|x\\|_\\infty$:\n$$\nf^*(y) = \\sup_{u \\in \\mathbb{R}^n} \\{ \\langle y, u \\rangle - \\|u\\|_\\infty \\}\n$$\nBy Hölder's inequality, $|\\langle y, u \\rangle| \\le \\|y\\|_1 \\|u\\|_\\infty$.\nIf $\\|y\\|_1 \\le 1$, then $\\langle y, u \\rangle \\le \\|y\\|_1 \\|u\\|_\\infty \\le \\|u\\|_\\infty$, which implies $\\langle y, u \\rangle - \\|u\\|_\\infty \\le 0$. The supremum is $0$.\nIf $\\|y\\|_1 > 1$, we can choose $u$ with components $u_i = K \\cdot \\operatorname{sign}(y_i)$ for some $K>0$. Then $\\langle y, u \\rangle - \\|u\\|_\\infty = K\\|y\\|_1 - K = K(\\|y\\|_1 - 1)$. As $K \\to \\infty$, this goes to $+\\infty$.\nThus, $f^*$ is the indicator function of the unit $\\ell_1$ ball, $\\mathbb{B}_1 = \\{y \\in \\mathbb{R}^n : \\|y\\|_1 \\le 1\\}$:\n$$\nf^*(y) = I_{\\mathbb{B}_1}(y) = \\begin{cases} 0 & \\text{if } \\|y\\|_1 \\le 1 \\\\ +\\infty & \\text{if } \\|y\\|_1 > 1 \\end{cases}\n$$\nNow we analyze the proximal operator $\\operatorname{prox}_{f^*/\\lambda}\\left(\\frac{x}{\\lambda}\\right)$. Let $g(v) = \\frac{1}{\\lambda}f^*(v)$. For $\\lambda > 0$, $g(v) = I_{\\mathbb{B}_1}(v)$. The proximal operator is:\n$$\n\\operatorname{prox}_{g}\\left(\\tfrac{x}{\\lambda}\\right) = \\arg\\min_{v \\in \\mathbb{R}^n} \\left\\{ \\tfrac{1}{2}\\left\\|v - \\tfrac{x}{\\lambda}\\right\\|_2^2 + I_{\\mathbb{B}_1}(v) \\right\\}\n$$\nThe indicator function $I_{\\mathbb{B}_1}(v)$ restricts the domain of minimization to the set $\\mathbb{B}_1$. The problem is therefore equivalent to finding the Euclidean projection of the vector $z=x/\\lambda$ onto the unit $\\ell_1$ ball:\n$$\n\\operatorname{prox}_{f^*/\\lambda}\\left(\\tfrac{x}{\\lambda}\\right) = \\operatorname{proj}_{\\mathbb{B}_1}\\left(\\tfrac{x}{\\lambda}\\right)\n$$\nThis projection can be computed efficiently. If $\\|z\\|_1 \\le 1$, the projection is $z$ itself. Otherwise, the projection lies on the boundary of the $\\ell_1$ ball. An efficient algorithm exists for this, which involves sorting the absolute values of the components of $z$ and finding a threshold $\\theta$ such that the solution is given by component-wise soft-thresholding: $v_i = \\operatorname{sign}(z_i)\\max(0, |z_i| - \\theta)$. This provides the required independent algorithm for the second term.\n\n### Summary of the Method\nThe program will implement two distinct functions:\n1.  A function to compute $u = \\operatorname{prox}_{\\lambda \\|\\cdot\\|_\\infty}(x)$ by numerically finding the root $t^*$ of $\\sum_{i=1}^n \\max(0, |x_i|-t) = \\lambda$.\n2.  A function to compute $v = \\operatorname{prox}_{f^*/\\lambda}(x/\\lambda)$ by implementing the efficient projection algorithm onto the unit $\\ell_1$ ball for the vector $x/\\lambda$.\n\nFor each test case $(x, \\lambda)$, we compute $u$ and $v$ using these independent methods and verify if $x$ is equal to $u + \\lambda v$ within the specified numerical tolerance for each coordinate.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef compute_prox_linf(x: np.ndarray, lambda_val: float) -> np.ndarray:\n    \"\"\"\n    Computes prox_{lambda * ||.||_inf}(x) using a root-finding method.\n    This is the first term in Moreau's identity.\n    \n    The proximal operator is defined as the solution u to:\n    argmin_u { 1/2 ||u - x||_2^2 + lambda * ||u||_inf }\n\n    The solution is a soft-thresholding of x by a value t_star, where t_star is the root of\n    sum(max(0, |x_i| - t)) = lambda.\n    \"\"\"\n    if lambda_val == 0.0:\n        return x.copy()\n    \n    # Use np.float64 for precision in sums and comparisons\n    x_abs = np.abs(x, dtype=np.float64)\n    norm1_x = np.sum(x_abs)\n\n    if lambda_val >= norm1_x:\n        return np.zeros_like(x, dtype=np.float64)\n\n    # Define the function whose root we need to find\n    def phi(t: float, x_abs_in: np.ndarray, lambda_in: float) -> float:\n        return np.sum(np.maximum(0., x_abs_in - t)) - lambda_in\n\n    t_max = np.max(x_abs)\n    if t_max == 0:  # Happens if x is the zero vector\n        return np.zeros_like(x, dtype=np.float64)\n    \n    # The root must lie in [0, t_max].\n    # phi(0) = norm1_x - lambda_val > 0.\n    # phi(t_max) = -lambda_val < 0.\n    # The interval [0, t_max] is guaranteed to bracket the root.\n    try:\n        t_star = brentq(phi, 0.0, t_max, args=(x_abs, lambda_val))\n    except ValueError:\n        # This may occur if t_max is very close to 0 and phi(t_max) evaluates to a non-negative\n        # number due to floating point inaccuracies. In this case, t_star is effectively t_max.\n        if phi(t_max) >= 0:\n            t_star = t_max\n        else:\n            raise\n\n    # The solution u is found by component-wise operations\n    u = np.sign(x) * np.minimum(x_abs, t_star)\n    return u\n\ndef project_l1_ball(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the Euclidean projection of vector z onto the unit L1 ball.\n    Implements the algorithm by Duchi et al. (2008).\n    \"\"\"\n    z = np.asarray(z, dtype=np.float64)\n    if np.sum(np.abs(z)) <= 1.0:\n        return z\n\n    n = len(z)\n    mu = np.sort(np.abs(z))[::-1]\n    S_mu = np.cumsum(mu)\n    \n    j_vals = np.arange(1, n + 1)\n    test_vals = mu - (S_mu - 1.0) / j_vals\n    \n    try:\n        # Find the index of the last positive value in test_vals.\n        # This corresponds to rho.\n        rho_idx = np.flatnonzero(test_vals > 0)[-1]\n        rho = rho_idx + 1\n    except IndexError:\n        # This block should be unreachable if ||z||_1 > 1.\n        # It's included for robustness.\n        return z\n\n    theta = (S_mu[rho - 1] - 1.0) / rho\n    \n    # The projection w is a soft-thresholding of z by theta\n    w = np.sign(z) * np.maximum(0., np.abs(z) - theta)\n    return w\n\ndef compute_prox_conjugate_linf(x: np.ndarray, lambda_val: float) -> np.ndarray:\n    \"\"\"\n    Computes prox_{f*/lambda}(x/lambda), where f = ||.||_inf.\n    This is equivalent to projecting x/lambda onto the L1 ball.\n    This is the second term (without the lambda multiplier) in Moreau's identity.\n    \"\"\"\n    if lambda_val == 0.0:\n        # The function becomes infinity * I_B1(v). The prox operator projects\n        # onto the domain which is the L1 ball. The problem is argmin{1/2||v-z||^2 + inf*I_B1(v)}.\n        # For this to be finite, v must be in the L1 ball. The formal definition of prox_{inf*g}\n        # can be interpreted as projection onto dom(g) a {0}, which gives {0}.\n        return np.zeros_like(x, dtype=np.float64)\n\n    z = x / lambda_val\n    return project_l1_ball(z)\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and verify Moreau's identity.\n    \"\"\"\n    test_cases = [\n        (np.array([3.2, -1.1, 0.0, 5.5, -2.2]), 1.7),\n        (np.array([0.0, 0.0, 0.0, 0.0]), 2.5),\n        (np.array([10.0, -10.0]), 0.0),\n        (np.array([-0.3, 0.3, -0.3, 0.3, -0.3, 0.3]), 0.5),\n        (np.array([np.sin(i) + 0.5 * np.cos(2*i) for i in range(1, 21)]), 3.0)\n    ]\n\n    results = []\n    atol = 1e-9\n    rtol = 1e-10\n\n    for x_orig, lambda_val in test_cases:\n        # Ensure data types are consistent for high precision\n        x = np.asarray(x_orig, dtype=np.float64)\n\n        # First term of Moreau's identity\n        prox1 = compute_prox_linf(x, lambda_val)\n\n        # Second term (the proximal part only)\n        prox2 = compute_prox_conjugate_linf(x, lambda_val)\n\n        # Right-hand side of Moreau's identity\n        rhs = prox1 + lambda_val * prox2\n        \n        # Numerical verification per coordinate\n        # |x_i - rhs_i| <= atol + rtol * |x_i| for all i\n        is_close = np.all(np.abs(x - rhs) <= atol + rtol * np.abs(x))\n        results.append(is_close)\n    \n    # Format the boolean results as lowercase strings\n    print(f\"[{','.join(map(lambda b: str(b).lower(), results))}]\")\n\nsolve()\n```", "id": "3470868"}, {"introduction": "Our final practice shifts focus from exact vector calculations to efficient large-scale matrix computations, a scenario frequently encountered in modern compressed sensing. We will analyze the proximal operator of the nuclear norm, which is central to solving low-rank matrix recovery problems. This exercise challenges you to move beyond the mathematical formula and analyze the computational complexity of its implementation, comparing deterministic and randomized algorithms for partial Singular Value Decomposition (SVD) and understanding the conditions under which each is justified [@problem_id:3470844].", "problem": "Consider a large sparse matrix $X \\in \\mathbb{R}^{m \\times n}$ with $s=\\operatorname{nnz}(X)$ nonzero entries, and a regularization parameter $\\lambda>0$. The proximal operator of the nuclear norm is defined by\n$$\n\\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(X) \\;=\\; \\arg\\min_{Y \\in \\mathbb{R}^{m \\times n}} \\;\\lambda \\|Y\\|_* \\;+\\; \\tfrac{1}{2}\\|Y - X\\|_F^2.\n$$\nIn practice, for large sparse problems in compressed sensing and sparse optimization, one evaluates this proximal map by computing only the leading singular values and singular vectors of $X$ up to a target rank $r$ that is sufficient to capture the singular values that will survive the shrinkage induced by $\\lambda$. This can be done with Krylov subspace methods such as Lanczos or with Randomized Singular Value Decomposition (RSVD) using an oversampling parameter $p$ and $q$ power iterations.\n\nUsing only the definitions above and standard algorithmic primitives for sparse matrix–vector multiplication and orthogonalization, analyze the computational complexity of computing $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(X)$ via a partial Singular Value Decomposition (SVD), and discuss when RSVD is justified. Select all correct statements below.\n\nA. For a sparse matrix with $s=\\operatorname{nnz}(X)$, a Krylov/Lanczos-type partial SVD to compute rank-$r$ factors typically costs $\\mathcal{O}\\!\\left(s\\,r \\;+\\; (m+n)\\,r^2\\right)$ time, where the $\\mathcal{O}(s\\,r)$ term accounts for repeated multiplications by $X$ and $X^\\top$, and the $\\mathcal{O}((m+n)\\,r^2)$ term accounts for orthogonalization and Rayleigh–Ritz steps. An RSVD with oversampling $p$ and $q$ power iterations typically costs $\\mathcal{O}\\!\\left(s\\,(r{+}p)\\,(q{+}1) \\;+\\; (m{+}n)\\,(r{+}p)^2\\right)$. RSVD is justified when the singular spectrum decays sufficiently fast and $r \\ll \\min\\{m,n\\}$, so that a small $p$ and modest $q$ yield an accurate subspace at lower cost than deterministic methods.\n\nB. Because the proximal map shrinks all singular values, computing $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(X)$ requires a full SVD with complexity $\\mathcal{O}\\!\\left(m\\,n\\,\\min\\{m,n\\}\\right)$ regardless of sparsity. RSVD is not applicable because the subsequent shrinkage introduces bias that invalidates randomized approximations.\n\nC. For a sparse $X$ with $s$ nonzeros, a Lanczos-type partial SVD has complexity $\\mathcal{O}(s\\,r)$ for rank $r$ and no additional lower-order terms, since orthogonalization costs are negligible. RSVD for the proximal map costs $\\mathcal{O}(s)$ independent of $r$ due to oversampling.\n\nD. RSVD is most justified when the singular values are nearly flat (slow decay), because power iterations reduce variance in the estimates, and its complexity scales as $\\mathcal{O}(m\\,n\\,r)$, which is asymptotically better than Krylov methods on sparse matrices.\n\nE. The proximal map of the nuclear norm can be computed by entrywise soft-thresholding of $X$ without any SVD, with complexity $\\mathcal{O}(s)$; therefore, neither partial SVD nor RSVD is necessary.\n\nF. In iterative proximal gradient methods, one can exploit that singular values below $\\lambda$ vanish after shrinkage by adaptively increasing the computed rank until no new singular values exceed $\\lambda$, so the effective per-call complexity is $\\mathcal{O}\\!\\left(s\\,r_{\\mathrm{eff}} \\;+\\; (m{+}n)\\,r_{\\mathrm{eff}}^2\\right)$ for an effective rank $r_{\\mathrm{eff}}$. RSVD is justified when matrix–vector products with $X$ and $X^\\top$ are cheap, the post-shrinkage numerical rank is small, and a few power iterations $q$ suffice to resolve the dominant spectral gap.", "solution": "The user wants me to analyze the computational complexity of computing the proximal operator of the nuclear norm for a large sparse matrix and evaluate several statements about the methods used.\n\n### Problem Validation\n\nFirst, I will validate the problem statement according to the specified criteria.\n\n**Step 1: Extract Givens**\n- A large sparse matrix $X \\in \\mathbb{R}^{m \\times n}$.\n- $s = \\operatorname{nnz}(X)$, the number of nonzero entries.\n- A regularization parameter $\\lambda > 0$.\n- The definition of the proximal operator of the nuclear norm:\n$$\n\\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(X) \\;=\\; \\arg\\min_{Y \\in \\mathbb{R}^{m \\times n}} \\;\\lambda \\|Y\\|_* \\;+\\; \\tfrac{1}{2}\\|Y - X\\|_F^2.\n$$\n- A description of the practical evaluation method: computing the leading singular values and vectors up to a target rank $r$.\n- Methods mentioned for this partial SVD: Krylov subspace methods (e.g., Lanczos) and Randomized SVD (RSVD).\n- RSVD parameters: oversampling $p$ and power iterations $q$.\n- The task is to analyze complexity and justify RSVD using standard algorithmic primitives for sparse matrix–vector multiplication and orthogonalization.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly grounded in the well-established fields of convex optimization, numerical linear algebra, and machine learning. The proximal operator of the nuclear norm is a fundamental building block in algorithms for matrix completion, robust PCA, and other low-rank matrix recovery problems. Its solution via Singular Value Thresholding (SVT) is a classical result. The use of partial SVD methods like Lanczos or RSVD for large-scale problems is standard practice and scientifically sound.\n- **Well-Posed:** The problem is well-posed. It asks for a complexity analysis of standard algorithms and a discussion of their applicability, which is a standard and answerable question in numerical analysis. A unique and meaningful analysis can be performed.\n- **Objective:** The problem statement is written in precise, objective, and technical language. It is free of ambiguity and subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is scientifically sound, well-posed, and objective. I will now proceed with the derivation and evaluation of the options.\n\n### Solution Derivation\n\nThe solution to the proximal operator problem $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(X)$ is given by the Singular Value Thresholding (SVT) operator. Let the Singular Value Decomposition (SVD) of $X$ be $X = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{m \\times k}$, $V \\in \\mathbb{R}^{n \\times k}$, $\\Sigma = \\operatorname{diag}(\\sigma_1, \\sigma_2, \\dots, \\sigma_k)$ with $k = \\min\\{m, n\\}$, and the singular values $\\sigma_i$ are non-negative and sorted in descending order. The solution $Y$ is given by:\n$$\nY = U \\mathcal{S}_{\\lambda}(\\Sigma) V^\\top\n$$\nwhere $\\mathcal{S}_{\\lambda}(\\Sigma)$ is a diagonal matrix with entries $(\\mathcal{S}_{\\lambda}(\\Sigma))_{ii} = \\mathcal{S}_{\\lambda}(\\sigma_i) = \\max(0, \\sigma_i - \\lambda)$.\n\nThis formula reveals a crucial property: any singular value $\\sigma_i \\le \\lambda$ is thresholded to $0$. Consequently, the resulting matrix $Y$ has a rank equal to the number of singular values of $X$ that are strictly greater than $\\lambda$. This means we do not need the full SVD of $X$. We only need to compute the singular triplets $(u_i, \\sigma_i, v_i)$ for which $\\sigma_i > \\lambda$. This justifies the use of partial SVD algorithms that compute only the top $r$ singular values and vectors, where $r$ is an estimate of the number of singular values greater than $\\lambda$.\n\nWe now analyze the complexity of two such algorithms for a sparse matrix $X$ with $s = \\operatorname{nnz}(X)$.\n\n**1. Krylov/Lanczos-type Partial SVD:**\nLanczos bidiagonalization is a Krylov subspace method to compute the leading singular triplets of $X$. The algorithm iteratively builds orthonormal bases for Krylov subspaces generated by applying $X$ and $X^\\top$.\n- The main cost is repeated sparse matrix-vector multiplication. To find $r$ singular values, the method typically requires $\\mathcal{O}(r)$ iterations. Each iteration involves one multiplication by $X$ and one by $X^\\top$. The cost of a sparse matrix-vector product is $\\mathcal{O}(s)$. Thus, the total cost for multiplications is $\\mathcal{O}(s\\,r)$.\n- Another significant cost is maintaining the orthogonality of the generated basis vectors (the Lanczos vectors). In finite precision arithmetic, re-orthogonalization is required. Orthogonalizing a new vector against a basis of $r$ previous vectors in $\\mathbb{R}^m$ and $\\mathbb{R}^n$ costs $\\mathcal{O}(m\\,r)$ and $\\mathcal{O}(n\\,r)$ respectively. Over $r$ steps, this accumulates to a total of $\\mathcal{O}((m+n)\\,r^2)$.\n- The final step involves computing the SVD of a small $r \\times r$ bidiagonal matrix, which costs $\\mathcal{O}(r^2)$, a negligible term.\n- Combining the dominant costs, the total complexity is $\\mathcal{O}(s\\,r + (m+n)\\,r^2)$.\n\n**2. Randomized SVD (RSVD):**\nRSVD approximates the range (column space) of $X$ by projecting it onto a low-dimensional random subspace.\n- **Stage 1 (Range Finding):** Construct an $n \\times (r+p)$ random matrix $\\Omega$, where $p$ is a small oversampling parameter. Compute $Y = X\\Omega$. This costs $\\mathcal{O}(s\\,(r+p))$. To improve accuracy, especially when singular values decay slowly, power iterations are used, forming $Y = (XX^\\top)^q X\\Omega$. This involves $(q+1)$ multiplications with $X$ and $q$ multiplications with $X^\\top$ for each of the $r+p$ random vectors. The total cost for multiplications is $\\mathcal{O}(s\\,(r+p)\\,(2q+1))$, which is reasonably approximated as $\\mathcal{O}(s\\,(r+p)\\,(q+1))$.\n- **Stage 2 (Orthogonalization):** An orthonormal basis $Q$ for the columns of the matrix $Y \\in \\mathbb{R}^{m \\times (r+p)}$ is computed, typically via a QR decomposition. This costs $\\mathcal{O}(m\\,(r+p)^2)$.\n- **Stage 3 (Projection and SVD):** Form the small matrix $B = Q^\\top X$. This requires applying $X^\\top$ to the $r+p$ columns of $Q$, costing $\\mathcal{O}(s\\,(r+p))$. Then, compute the SVD of $B \\in \\mathbb{R}^{(r+p) \\times n}$. A more efficient variant computes the SVD of the $(r+p) \\times (r+p)$ matrix $Q^\\top X (Q^\\top X)^\\top$. In either standard approach, the cost of forming the projected matrix and orthogonalizing the bases for both row and column spaces leads to a combined orthogonalization/projection cost on the order of $\\mathcal{O}((m+n)(r+p)^2)$. The SVD of the small resulting matrix costs $\\mathcal{O}((r+p)^3)$, which is a lower-order term.\n- Combining the dominant costs, the total complexity for RSVD is approximately $\\mathcal{O}(s\\,(r+p)\\,(q+1) + (m+n)\\,(r+p)^2)$.\n- **Justification:** The theoretical error bounds of RSVD depend on the magnitude of the neglected singular values ($\\sigma_{r+1}, \\dots$). The approximation is accurate if these values decay quickly. Power iterations ($q>0$) improve the approximation by effectively increasing the decay rate of the singular values of the iterated matrix, making RSVD effective even for moderate decay. Therefore, RSVD is justified for matrices where the singular spectrum has a sufficiently fast decay, allowing for a small target rank $r$, a small oversampling parameter $p$, and a small number of power iterations $q$.\n\n### Option-by-Option Analysis\n\n**A. For a sparse matrix with $s=\\operatorname{nnz}(X)$, a Krylov/Lanczos-type partial SVD to compute rank-$r$ factors typically costs $\\mathcal{O}\\!\\left(s\\,r \\;+\\; (m+n)\\,r^2\\right)$ time, where the $\\mathcal{O}(s\\,r)$ term accounts for repeated multiplications by $X$ and $X^\\top$, and the $\\mathcal{O}((m+n)\\,r^2)$ term accounts for orthogonalization and Rayleigh–Ritz steps. An RSVD with oversampling $p$ and $q$ power iterations typically costs $\\mathcal{O}\\!\\left(s\\,(r{+}p)\\,(q{+}1) \\;+\\; (m{+}n)\\,(r{+}p)^2\\right)$. RSVD is justified when the singular spectrum decays sufficiently fast and $r \\ll \\min\\{m,n\\}$, so that a small $p$ and modest $q$ yield an accurate subspace at lower cost than deterministic methods.**\n- The complexity estimate for Krylov/Lanczos methods is correct, reflecting the costs of matrix-vector products and orthogonalization.\n- The complexity estimate for RSVD is also a correct and standard representation of its costs for sparse matrices.\n- The justification for RSVD is accurate: it relies on rapid spectral decay and a low-rank structure.\n- **Verdict: Correct.**\n\n**B. Because the proximal map shrinks all singular values, computing $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(X)$ requires a full SVD with complexity $\\mathcal{O}\\!\\left(m\\,n\\,\\min\\{m,n\\}\\right)$ regardless of sparsity. RSVD is not applicable because the subsequent shrinkage introduces bias that invalidates randomized approximations.**\n- The premise that a full SVD is required is false. The SVT operator sets singular values smaller than $\\lambda$ to zero, meaning we only need to compute the singular triplets corresponding to singular values greater than $\\lambda$. This is the motivation for using partial SVD.\n- The claim that RSVD is not applicable is false. RSVD is a method to compute an approximate SVD of $X$. The SVT operator can then be applied to this approximation. This is a very common and valid procedure.\n- **Verdict: Incorrect.**\n\n**C. For a sparse $X$ with $s$ nonzeros, a Lanczos-type partial SVD has complexity $\\mathcal{O}(s\\,r)$ for rank $r$ and no additional lower-order terms, since orthogonalization costs are negligible.**\n- This statement incorrectly claims that the orthogonalization cost, $\\mathcal{O}((m+n)\\,r^2)$, is negligible. This term is crucial and can often dominate the $\\mathcal{O}(s\\,r)$ term, especially as the rank $r$ increases.\n- The claim that RSVD costs $\\mathcal{O}(s)$ is also incorrect. The cost of RSVD is fundamentally dependent on the target rank $r$, oversampling $p$, and power iterations $q$.\n- **Verdict: Incorrect.**\n\n**D. RSVD is most justified when the singular values are nearly flat (slow decay), because power iterations reduce variance in the estimates, and its complexity scales as $\\mathcal{O}(m\\,n\\,r)$, which is asymptotically better than Krylov methods on sparse matrices.**\n- The premise is backward. RSVD is most effective when singular values decay *rapidly*. A flat spectrum is a difficult case for low-rank approximation. While power iterations help, they cannot fully overcome a pathologically slow decay.\n- The complexity $\\mathcal{O}(m\\,n\\,r)$ is for dense matrices and incorrect for sparse matrices, where the dependence should be on $s$, not $m\\,n$. The complexity of RSVD for sparse matrices is not $\\mathcal{O}(m\\,n\\,r)$.\n- **Verdict: Incorrect.**\n\n**E. The proximal map of the nuclear norm can be computed by entrywise soft-thresholding of $X$ without any SVD, with complexity $\\mathcal{O}(s)$; therefore, neither partial SVD nor RSVD is necessary.**\n- This statement incorrectly confuses the proximal operator of the nuclear norm ($\\|\\cdot\\|_*$) with the proximal operator of the element-wise $\\ell_1$ norm ($\\|\\cdot\\|_1$). Entrywise soft-thresholding computes the prox of the $\\ell_1$ norm. The prox of the nuclear norm requires singular value thresholding, which necessitates an SVD.\n- **Verdict: Incorrect.**\n\n**F. In iterative proximal gradient methods, one can exploit that singular values below $\\lambda$ vanish after shrinkage by adaptively increasing the computed rank until no new singular values exceed $\\lambda$, so the effective per-call complexity is $\\mathcal{O}\\!\\left(s\\,r_{\\mathrm{eff}} \\;+\\; (m{+}n)\\,r_{\\mathrm{eff}}^2\\right)$ for an effective rank $r_{\\mathrm{eff}}$. RSVD is justified when matrix–vector products with $X$ and $X^\\top$ are cheap, the post-shrinkage numerical rank is small, and a few power iterations $q$ suffice to resolve the dominant spectral gap.**\n- The description of an adaptive rank strategy is accurate and reflects a common and efficient practice in iterative algorithms like ISTA or FISTA for matrix problems. The complexity formula correctly applies the partial SVD cost to this effective rank $r_{\\mathrm{eff}}$.\n- The justification for using RSVD in this context is also entirely correct. It correctly lists the key conditions for RSVD's efficiency: cheap matrix-vector products (e.g., sparsity), a low effective rank, and a spectral structure amenable to power iterations.\n- **Verdict: Correct.**", "answer": "$$\\boxed{AF}$$", "id": "3470844"}]}