{"hands_on_practices": [{"introduction": "Before solving an optimization problem, we often preprocess the data. A common step is normalizing the columns of the sensing matrix $A$. This exercise explores why this is not merely a cosmetic change but a fundamental re-weighting of the sparsity penalty, which alters the effective regularization applied to each coefficient. By working through this reparameterization, you will gain a deeper understanding of how data scaling directly influences model selection in LASSO [@problem_id:3439923].", "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) in compressed sensing, where the design matrix $A \\in \\mathbb{R}^{m \\times n}$ has nonzero columns $a_{1}, \\dots, a_{n} \\in \\mathbb{R}^{m}$, and the data vector is $y \\in \\mathbb{R}^{m}$. The LASSO estimator is defined as any minimizer of the convex objective\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\left\\| y - A x \\right\\|_{2}^{2} + \\lambda \\left\\| x \\right\\|_{1},\n$$\nwith $\\lambda > 0$. The problem combines a smooth convex data-fidelity term and a nonsmooth convex regularizer. You may use the following fundamental bases: (i) for a convex function $f$, a point $x^{\\star}$ is optimal if and only if $0 \\in \\partial f(x^{\\star})$; (ii) the subdifferential of the $\\ell_{1}$-norm satisfies $\\partial \\|x\\|_{1} = \\{ s \\in \\mathbb{R}^{n} : s_{j} = \\mathrm{sign}(x_{j}) \\text{ if } x_{j} \\neq 0, \\ s_{j} \\in [-1,1] \\text{ if } x_{j} = 0 \\}$; (iii) invertible linear reparameterizations of a variable induce equivalent optimization problems under the corresponding bijection of arguments; and (iv) the gradient of the smooth term is $\\nabla \\left( \\frac{1}{2}\\| y - A x \\|_{2}^{2} \\right) = -A^{\\top}(y - A x)$. Based on these principles, consider the practice of normalizing the columns of $A$ to unit Euclidean norm and the choice of the regularization parameter $\\lambda$. Which of the following statements are correct?\n\nA. For any matrix $A$ with nonzero columns $a_{j}$, defining $D = \\mathrm{diag}\\big( \\|a_{1}\\|_{2}, \\dots, \\|a_{n}\\|_{2} \\big)$, $\\tilde{A} = A D^{-1}$, and $\\tilde{x} = D x$, the problem $\\min_{x} \\frac{1}{2}\\| y - A x \\|_{2}^{2} + \\lambda \\|x\\|_{1}$ is equivalent, under the bijection $x \\leftrightarrow \\tilde{x}$, to $\\min_{\\tilde{x}} \\frac{1}{2}\\| y - \\tilde{A} \\tilde{x} \\|_{2}^{2} + \\sum_{j=1}^{n} \\big( \\lambda / \\|a_{j}\\|_{2} \\big) \\, | \\tilde{x}_{j} |$. Hence one may impose $\\| \\tilde{A}_{j} \\|_{2} = 1$ without loss of generality, at the cost of coordinate-wise penalty rescaling.\n\nB. If one instead enforces a uniform penalty after normalization, i.e., solves $\\min_{u} \\frac{1}{2}\\| y - \\tilde{A} u \\|_{2}^{2} + \\lambda \\|u\\|_{1}$ with $\\tilde{A}$ having unit-norm columns, then under the inverse reparameterization $u = D x$ this corresponds to solving $\\min_{x} \\frac{1}{2}\\| y - A x \\|_{2}^{2} + \\sum_{j=1}^{n} \\lambda \\, \\|a_{j}\\|_{2} \\, |x_{j}|$, i.e., a weighted LASSO whose weights are proportional to the column norms of $A$.\n\nC. The smallest regularization level for which the zero solution is optimal is given by $\\lambda_{\\max} = \\| A^{\\top} y \\|_{\\infty}$ without normalization, and under column normalization it becomes $\\lambda_{\\max} = \\| \\tilde{A}^{\\top} y \\|_{\\infty} = \\max_{j} | a_{j}^{\\top} y | / \\| a_{j} \\|_{2}$, showing that normalization rescales the meaningful range of $\\lambda$ by the column norms.\n\nD. Because normalization leaves both the data-fidelity term and the penalty unchanged for any fixed $\\lambda$, it is a purely cosmetic preprocessing that neither reweights the penalty nor alters thresholding behavior.\n\nE. The rescaling of the meaningful range of $\\lambda$ caused by normalization is always by a single global factor equal to the average column norm $\\frac{1}{n} \\sum_{j=1}^{n} \\| a_{j} \\|_{2}$, independent of $y$ and independent of the distribution of the column norms.", "solution": "The user has provided a problem statement regarding the Least Absolute Shrinkage and Selection Operator (LASSO) and the effects of column normalization. I will first validate the problem statement and then proceed to a detailed analysis of each option.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n-   The LASSO problem is defined as $\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\left\\| y - A x \\right\\|_{2}^{2} + \\lambda \\left\\| x \\right\\|_{1}$.\n-   $A \\in \\mathbb{R}^{m \\times n}$ is the design matrix with nonzero columns $a_{1}, \\dots, a_{n} \\in \\mathbb{R}^{m}$.\n-   $y \\in \\mathbb{R}^{m}$ is the data vector.\n-   $\\lambda > 0$ is the regularization parameter.\n-   Fundamental principles provided:\n    (i) Optimality condition for a convex function $f$: $x^{\\star}$ is a minimizer if and only if $0 \\in \\partial f(x^{\\star})$.\n    (ii) Subdifferential of the $\\ell_{1}$-norm: $\\partial \\|x\\|_{1} = \\{ s \\in \\mathbb{R}^{n} : s_{j} = \\mathrm{sign}(x_{j}) \\text{ if } x_{j} \\neq 0, \\ s_{j} \\in [-1,1] \\text{ if } x_{j} = 0 \\}$.\n    (iii) Equivalence of optimization problems under invertible linear reparameterizations.\n    (iv) Gradient of the least-squares term: $\\nabla \\left( \\frac{1}{2}\\| y - A x \\|_{2}^{2} \\right) = -A^{\\top}(y - A x)$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded**: The problem is grounded in the well-established mathematical theory of convex optimization and its application to high-dimensional statistics (LASSO). All provided principles are standard, correct results from convex analysis.\n-   **Well-Posed**: The problem is well-posed. It asks for an evaluation of several mathematical statements concerning the properties of the LASSO objective function under a specific transformation (column normalization).\n-   **Objective**: The problem is stated in precise, objective mathematical language.\n\n**Step 3: Verdict and Action**\n- The problem statement is valid. It is scientifically sound, well-posed, and objective. I will now proceed with the solution and option-by-option analysis.\n\n**Derivation and Option Analysis**\n\nThe LASSO objective function is $J(x) = \\frac{1}{2}\\left\\| y - A x \\right\\|_{2}^{2} + \\lambda \\left\\| x \\right\\|_{1}$. We will analyze the effect of column normalization on this problem.\n\n**Analysis of Option A**\n\nThis option proposes a reparameterization to handle normalized columns. Let's verify the equivalence.\n-   Define the diagonal matrix of column norms: $D = \\mathrm{diag}\\big( \\|a_{1}\\|_{2}, \\dots, \\|a_{n}\\|_{2} \\big)$. Since all columns $a_j$ are nonzero, $\\|a_j\\|_2 > 0$ for all $j$, and thus $D$ is invertible.\n-   Define the normalized matrix $\\tilde{A} = A D^{-1}$. The $j$-th column of $\\tilde{A}$ is $\\tilde{a}_j = a_j/\\|a_j\\|_2$, which has unit Euclidean norm: $\\|\\tilde{a}_j\\|_2 = \\|a_j / \\|a_j\\|_2\\|_2 = 1$.\n-   Define the reparameterization $\\tilde{x} = D x$. This is an invertible linear transformation (principle iii), with the inverse mapping $x = D^{-1} \\tilde{x}$.\n-   Substitute $x = D^{-1} \\tilde{x}$ into the LASSO objective function:\n    $$ J(D^{-1}\\tilde{x}) = \\frac{1}{2}\\left\\| y - A (D^{-1} \\tilde{x}) \\right\\|_{2}^{2} + \\lambda \\left\\| D^{-1} \\tilde{x} \\right\\|_{1} $$\n-   The first term (data-fidelity) becomes:\n    $$ \\frac{1}{2}\\left\\| y - (A D^{-1}) \\tilde{x} \\right\\|_{2}^{2} = \\frac{1}{2}\\left\\| y - \\tilde{A} \\tilde{x} \\right\\|_{2}^{2} $$\n-   The second term (regularizer) becomes:\n    $$ \\lambda \\left\\| D^{-1} \\tilde{x} \\right\\|_{1} = \\lambda \\sum_{j=1}^{n} \\left| (D^{-1} \\tilde{x})_{j} \\right| = \\lambda \\sum_{j=1}^{n} \\left| \\frac{\\tilde{x}_{j}}{\\|a_{j}\\|_{2}} \\right| = \\sum_{j=1}^{n} \\left( \\frac{\\lambda}{\\|a_j\\|_2} \\right) |\\tilde{x}_j| $$\n-   Combining the terms, the optimization problem in the new variable $\\tilde{x}$ is:\n    $$ \\min_{\\tilde{x} \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\left\\| y - \\tilde{A} \\tilde{x} \\right\\|_{2}^{2} + \\sum_{j=1}^{n} \\frac{\\lambda}{\\|a_{j}\\|_{2}} | \\tilde{x}_{j} | $$\nThis is precisely the form stated in option A. The equivalence holds due to the bijective relationship between $x$ and $\\tilde{x}$. This demonstrates that one can work with a normalized matrix $\\tilde{A}$ by introducing coordinate-specific penalties.\n\n**Verdict for A: Correct.**\n\n**Analysis of Option B**\n\nThis option considers the common practice of normalizing the matrix and then applying a standard LASSO penalty. It asks for the interpretation of this procedure in terms of the original variables.\n-   The new problem is $\\min_{u \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\| y - \\tilde{A} u \\|_{2}^{2} + \\lambda \\|u\\|_{1}$, where $\\tilde{A}$ has unit-norm columns.\n-   The statement proposes relating the solution variable u to the original variable $x$ via the inverse reparameterization $u = D x$, where $D = \\mathrm{diag}(\\|a_1\\|_2, \\dots, \\|a_n\\|_2)$.\n-   Let's substitute $u=Dx$ into this new objective function:\n    $$ \\frac{1}{2}\\| y - \\tilde{A} (Dx) \\|_{2}^{2} + \\lambda \\|Dx\\|_{1} $$\n-   The data-fidelity term becomes:\n    $$ \\frac{1}{2}\\| y - (A D^{-1}) (Dx) \\|_{2}^{2} = \\frac{1}{2}\\| y - A (D^{-1}D) x \\|_{2}^{2} = \\frac{1}{2}\\| y - A x \\|_{2}^{2} $$\n    This is the original data-fidelity term.\n-   The regularization term becomes:\n    $$ \\lambda \\|Dx\\|_{1} = \\lambda \\sum_{j=1}^{n} |(Dx)_j| = \\lambda \\sum_{j=1}^{n} |\\|a_j\\|_2 x_j| = \\sum_{j=1}^{n} \\lambda \\|a_j\\|_2 |x_j| $$\n-   Thus, solving the standard LASSO problem with the normalized matrix $\\tilde{A}$ is equivalent to solving the following weighted LASSO problem for the original variable $x$:\n    $$ \\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\| y - A x \\|_{2}^{2} + \\sum_{j=1}^{n} \\left(\\lambda \\|a_{j}\\|_{2}\\right) |x_{j}| $$\nThe penalty for $x_j$ is $\\lambda_j ' = \\lambda \\|a_j\\|_2$, which is proportional to the norm of the corresponding column $a_j$. This matches the statement in option B.\n\n**Verdict for B: Correct.**\n\n**Analysis of Option C**\n\nThis option discusses the condition under which the zero vector is the optimal solution.\n-   Let the objective be $J(x)$. Based on principle (i), $x^{\\star}=0$ is a minimizer if and only if $0 \\in \\partial J(0)$.\n-   The objective is a sum of a differentiable convex function $f(x) = \\frac{1}{2}\\|y-Ax\\|_2^2$ and a nonsmooth convex function $g(x) = \\lambda \\|x\\|_1$. The subdifferential is the sum of the gradient and the subdifferential: $\\partial J(x) = \\nabla f(x) + \\partial g(x)$.\n-   At $x=0$, the gradient of $f(x)$ is $\\nabla f(0) = -A^{\\top}(y - A \\cdot 0) = -A^{\\top}y$ (principle iv).\n-   The subdifferential of $g(x)$ at $x=0$ is $\\partial g(0) = \\lambda \\partial \\|x\\|_1|_{x=0}$. From principle (ii), $\\partial \\|x\\|_1|_{x=0}$ is the hypercube $[-1, 1]^n = \\{s \\in \\mathbb{R}^n : |s_j| \\le 1 \\text{ for all } j\\}$.\n-   The optimality condition $0 \\in \\partial J(0)$ becomes:\n    $$ 0 \\in -A^{\\top}y + \\lambda [-1, 1]^n \\quad \\iff \\quad A^{\\top}y \\in \\lambda [-1, 1]^n $$\n-   This is equivalent to requiring $|(A^{\\top}y)_j| \\le \\lambda$ for all $j \\in \\{1, \\dots, n\\}$.\n-   This condition holds if and only if $\\lambda \\ge \\max_j |(A^{\\top}y)_j|$. The smallest positive value of $\\lambda$ for which $x=0$ is a solution is therefore $\\lambda_{\\max} = \\max_j |(A^{\\top}y)_j| = \\|A^{\\top}y\\|_{\\infty}$. The first part of the statement is correct.\n-   For the normalized problem, $\\min_{u} \\frac{1}{2}\\| y - \\tilde{A} u \\|_{2}^{2} + \\lambda' \\|u\\|_{1}$, the same derivation applies with $A$ replaced by $\\tilde{A}$. The critical value is $\\lambda'_{\\max} = \\|\\tilde{A}^{\\top}y\\|_{\\infty}$.\n-   Let's expand this term: $(\\tilde{A}^{\\top}y)_j = \\tilde{a}_j^{\\top}y = (a_j/\\|a_j\\|_2)^{\\top}y = (a_j^{\\top}y)/\\|a_j\\|_2$.\n-   Therefore, $\\lambda'_{\\max} = \\max_j |(a_j^{\\top}y)/\\|a_j\\|_2| = \\max_j \\frac{|a_j^{\\top}y|}{\\|a_j\\|_2}$.\nThis is exactly what the statement claims. Normalization changes the critical value $\\lambda_{\\max}$, and the \"meaningful range\" of $\\lambda$ (often taken as $[0, \\lambda_{\\max}]$) is thereby rescaled in a way that depends on the column norms.\n\n**Verdict for C: Correct.**\n\n**Analysis of Option D**\n\nThe statement claims normalization is \"purely cosmetic,\" leaving the terms unchanged and not altering thresholding.\n-   From the analysis of A, we saw that to maintain equivalence, the penalty term must be reweighted to $\\sum_j (\\lambda/\\|a_j\\|_2) |\\tilde{x}_j|$. The penalty is not unchanged.\n-   From the analysis of B, we saw that applying a uniform penalty to the normalized problem is equivalent to solving a weighted LASSO on the original variables, with weights $\\lambda_j' = \\lambda\\|a_j\\|_2$. This reweighting alters the thresholding behavior. The threshold for activating variable $x_j$ is no longer uniform but depends on its column's norm.\n-   Therefore, normalization is a significant transformation, not a cosmetic one. It fundamentally alters the penalty structure of the problem.\n\n**Verdict for D: Incorrect.**\n\n**Analysis of Option E**\n\nThis statement claims the rescaling of $\\lambda_{\\max}$ is by a single global factor equal to the average column norm, $\\frac{1}{n} \\sum_{j=1}^{n} \\| a_{j} \\|_{2}$.\n-   From C, the critical value for the original problem is $\\lambda_{\\max, \\text{orig}} = \\|A^\\top y\\|_\\infty = \\max_j |a_j^\\top y|$.\n-   For the normalized problem, it's $\\lambda_{\\max, \\text{norm}} = \\|\\tilde{A}^\\top y\\|_\\infty = \\max_k \\frac{|a_k^\\top y|}{\\|a_k\\|_2}$.\n-   The ratio of these two values is $\\frac{\\lambda_{\\max, \\text{orig}}}{\\lambda_{\\max, \\text{norm}}} = \\frac{\\max_j |a_j^\\top y|}{\\max_k (|a_k^\\top y|/\\|a_k\\|_2)}$.\n-   This ratio is not a constant. The indices $j$ and $k$ that attain the maximums depend on the vector $y$. Thus, the \"rescaling factor\" is dependent on $y$. Furthermore, even for a fixed $y$, it is not in general equal to the average column norm.\n-   Consider a counterexample: Let $n=2$, $y=[1, 0]^{\\top}$, $a_1=[2, 0]^{\\top}$, $a_2=[0, 1]^{\\top}$.\n-   We have $\\|a_1\\|_2 = 2$ and $\\|a_2\\|_2 = 1$. The average column norm is $(2+1)/2 = 1.5$.\n-   $\\lambda_{\\max, \\text{orig}} = \\max(|a_1^\\top y|, |a_2^\\top y|) = \\max(|2|, |0|) = 2$.\n-   $\\lambda_{\\max, \\text{norm}} = \\max(\\frac{|a_1^\\top y|}{\\|a_1\\|_2}, \\frac{|a_2^\\top y|}{\\|a_2\\|_2}) = \\max(\\frac{2}{2}, \\frac{0}{1}) = 1$.\n-   The ratio of the critical values is $2/1 = 2$, which is not equal to the average norm of $1.5$. The statement is false.\n\n**Verdict for E: Incorrect.**\n\n**Summary of Conclusions**\n-   Option A: Correct.\n-   Option B: Correct.\n-   Option C: Correct.\n-   Option D: Incorrect.\n-   Option E: Incorrect.\n\nThe correct options are A, B, and C.", "answer": "$$\\boxed{ABC}$$", "id": "3439923"}, {"introduction": "Many powerful optimization solvers are designed for standard problem classes like Linear Programs (LPs). This practice demonstrates the essential skill of converting a problem with non-linear norms—specifically, an $\\ell_1$-norm objective and an $\\ell_{\\infty}$-norm constraint—into an equivalent LP. This reformulation is a cornerstone of computational optimization, building a bridge from an abstract mathematical model to a concrete form that can be solved efficiently by a wide array of off-the-shelf tools [@problem_id:3439983].", "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, and $\\epsilon \\in \\mathbb{R}_{+}$. Consider the robust sparse recovery problem defined by minimizing the $\\ell_{1}$ norm of the coefficient vector subject to an $\\ell_{\\infty}$-bounded residual:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\|x\\|_{1} \\quad \\text{subject to} \\quad \\|A x - y\\|_{\\infty} \\le \\epsilon.\n$$\nUse only fundamental definitions of norms and absolute values to reformulate this problem as a Linear Program (LP), where a Linear Program (LP) is given in the canonical inequality form\n$$\n\\min_{z \\in \\mathbb{R}^{p}} \\ c^{\\top} z \\quad \\text{subject to} \\quad G z \\le h,\n$$\nwith $G \\in \\mathbb{R}^{q \\times p}$ and $h \\in \\mathbb{R}^{q}$. Your tasks are:\n1. Derive an equivalent LP by specifying an explicit decision vector $z$, objective vector $c$, constraint matrix $G$, and right-hand side vector $h$, expressed in terms of $A$, $y$, $\\epsilon$, and standard basis blocks (such as identity matrices and zero blocks). You must justify each modeling step starting from the definitions of the $\\ell_{1}$ norm and the $\\ell_{\\infty}$ norm.\n2. Compute a closed-form expression for the total number of scalar inequality constraints and the total number of decision variables in your LP representation as functions of $m$ and $n$. \n\nProvide your final answer as a single $1 \\times 2$ row vector $(N_{\\text{var}}, N_{\\le})$, where $N_{\\text{var}}$ is the number of decision variables and $N_{\\le}$ is the number of scalar inequality constraints. No rounding is required, and no physical units are involved. Express your final answer symbolically in terms of $m$ and $n$ only.", "solution": "The problem is to reformulate the given robust sparse recovery optimization problem into a canonical Linear Program (LP). The original problem is stated as:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\|x\\|_{1} \\quad \\text{subject to} \\quad \\|A x - y\\|_{\\infty} \\le \\epsilon\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, and $\\epsilon \\in \\mathbb{R}_{+}$. A Linear Program in canonical inequality form is given by:\n$$\n\\min_{z \\in \\mathbb{R}^{p}} \\ c^{\\top} z \\quad \\text{subject to} \\quad G z \\le h\n$$\nWe must determine the decision vector $z$, the objective vector $c$, the constraint matrix $G$, and the right-hand side vector $h$. We will also calculate the number of decision variables, $N_{\\text{var}} = p$, and the number of scalar inequality constraints, $N_{\\le} = q$, where $G \\in \\mathbb{R}^{q \\times p}$.\n\nThe reformulation proceeds in two main parts: linearizing the objective function and linearizing the constraint.\n\n**1. Reformulation of the Objective Function**\n\nThe objective function is the $\\ell_1$ norm of $x$, defined as $\\|x\\|_1 = \\sum_{i=1}^{n} |x_i|$. The presence of the absolute value function $|x_i|$ makes the objective function non-linear. To convert this into a linear form, we employ a standard technique by representing the vector $x$ as the difference of two non-negative vectors. Let $x = u - v$, where $u, v \\in \\mathbb{R}^n$, and we enforce the component-wise constraints $u \\ge 0$ and $v \\ge 0$.\n\nFor any real number $x_i$, the representation $x_i = u_i - v_i$ with $u_i \\ge 0, v_i \\ge 0$ allows us to express its absolute value. Specifically, if we seek to minimize a sum involving $|x_i|$, we can replace $|x_i|$ with $u_i + v_i$. The minimum value of $u_i + v_i$ subject to $x_i = u_i - v_i$ and $u_i,v_i \\ge 0$ is precisely $|x_i|$. This minimum is achieved by setting $u_i = \\max(x_i, 0)$ and $v_i = \\max(-x_i, 0)$.\n\nTherefore, the objective of minimizing $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$ is equivalent to minimizing $\\sum_{i=1}^n (u_i + v_i)$, subject to $x = u - v$ and the non-negativity of $u$ and $v$.\n\nThe new decision variables for our LP are the components of $u$ and $v$. We define a new, larger decision vector $z \\in \\mathbb{R}^{2n}$ by concatenating $u$ and $v$:\n$$\nz = \\begin{pmatrix} u \\\\ v \\end{pmatrix}\n$$\nThe objective function $\\sum_{i=1}^n (u_i + v_i)$ can be written in vector form as $\\mathbf{1}_n^\\top u + \\mathbf{1}_n^\\top v$, where $\\mathbf{1}_n$ is an $n \\times 1$ vector of ones. In terms of $z$, this is $c^\\top z$, where the objective vector $c \\in \\mathbb{R}^{2n}$ is:\n$$\nc = \\begin{pmatrix} \\mathbf{1}_n \\\\ \\mathbf{1}_n \\end{pmatrix}\n$$\n\n**2. Reformulation of the Constraints**\n\nThe problem has two types of constraints: the primary constraint $\\|A x - y\\|_{\\infty} \\le \\epsilon$ and the implicit non-negativity constraints on our new variables $u$ and $v$.\n\nFirst, we address $\\|A x - y\\|_{\\infty} \\le \\epsilon$. Substituting $x = u - v$, we get:\n$$\n\\|A(u - v) - y\\|_{\\infty} \\le \\epsilon\n$$\nThe $\\ell_\\infty$ norm of a vector $r \\in \\mathbb{R}^m$ is defined as $\\|r\\|_{\\infty} = \\max_{j=1, \\dots, m} |r_j|$. Applying this definition, our constraint becomes:\n$$\n\\max_{j=1, \\dots, m} |(A(u-v) - y)_j| \\le \\epsilon\n$$\nThis single inequality is equivalent to a set of $m$ inequalities:\n$$\n|(A(u-v) - y)_j| \\le \\epsilon \\quad \\text{for each } j = 1, \\dots, m\n$$\nBased on the fundamental definition of the absolute value, an inequality of the form $|a| \\le b$ (for $b \\ge 0$) is equivalent to the pair of linear inequalities $-b \\le a \\le b$. Applying this, each of the $m$ inequalities above expands to:\n$$\n-\\epsilon \\le (A(u-v) - y)_j \\le \\epsilon\n$$\nLet $A_j$ denote the $j$-th row of $A$ and $y_j$ be the $j$-th element of $y$. The term $(A(u-v) - y)_j$ is $A_j u - A_j v - y_j$. The pair of inequalities for each $j$ is:\n1. $A_j u - A_j v - y_j \\le \\epsilon \\quad \\implies \\quad A_j u - A_j v \\le y_j + \\epsilon$\n2. $A_j u - A_j v - y_j \\ge -\\epsilon \\quad \\implies \\quad -(A_j u - A_j v - y_j) \\le \\epsilon \\quad \\implies \\quad -A_j u + A_j v \\le \\epsilon - y_j$\n\nThese $2m$ inequalities can be expressed in matrix form. The first set of $m$ inequalities is $A u - A v \\le y + \\epsilon \\mathbf{1}_m$. The second set is $-A u + A v \\le \\epsilon \\mathbf{1}_m - y$.\n\nNext, we include the non-negativity constraints $u \\ge 0$ and $v \\ge 0$. To match the canonical form $Gz \\le h$, we write these as:\n-$-u \\le 0_n$\n-$-v \\le 0_n$\nwhere $0_n$ is the $n \\times 1$ zero vector.\n\n**3. Assembling the Linear Program**\n\nWe now assemble the complete LP. The decision vector is $z = \\begin{pmatrix} u \\\\ v \\end{pmatrix}$.\nThe problem is to minimize $c^\\top z$ where $c = \\begin{pmatrix} \\mathbf{1}_n \\\\ \\mathbf{1}_n \\end{pmatrix}$.\nThe constraints are collected into the single matrix inequality $Gz \\le h$:\n$$\n\\begin{pmatrix}\nA & -A \\\\\n-A & A \\\\\n-I_n & 0_{n \\times n} \\\\\n0_{n \\times n} & -I_n\n\\end{pmatrix}\n\\begin{pmatrix} u \\\\ v \\end{pmatrix}\n\\le\n\\begin{pmatrix}\ny + \\epsilon \\mathbf{1}_m \\\\\n\\epsilon \\mathbf{1}_m - y \\\\\n0_n \\\\\n0_n\n\\end{pmatrix}\n$$\nHere, $I_n$ is the $n \\times n$ identity matrix and $0_{n \\times n}$ is the $n \\times n$ zero matrix. This provides the explicit forms for $G$ and $h$.\n\n**4. Counting Variables and Constraints**\n\nThe final task is to determine the total number of decision variables ($N_{\\text{var}}$) and scalar inequality constraints ($N_{\\le}$).\n\n- **Number of variables ($N_{\\text{var}}$):** The decision vector $z$ is the concatenation of $u \\in \\mathbb{R}^n$ and $v \\in \\mathbb{R}^n$. Thus, the total number of scalar variables is:\n$$\nN_{\\text{var}} = p = n + n = 2n\n$$\n\n- **Number of constraints ($N_{\\le}$):** The number of constraints is the number of rows in the matrix $G$. We sum the number of scalar inequalities from each block:\n- $A u - A v \\le y + \\epsilon \\mathbf{1}_m$: $m$ inequalities\n- $-A u + A v \\le \\epsilon \\mathbf{1}_m - y$: $m$ inequalities\n- $-u \\le 0_n$: $n$ inequalities\n- $-v \\le 0_n$: $n$ inequalities\nThe total number of scalar inequality constraints is:\n$$\nN_{\\le} = q = m + m + n + n = 2m + 2n\n$$\nThe final result is $(N_{\\text{var}}, N_{\\le}) = (2n, 2m+2n)$.", "answer": "$$\\boxed{\\begin{pmatrix} 2n & 2m+2n \\end{pmatrix}}$$", "id": "3439983"}, {"introduction": "Sparse recovery problems are often presented in two forms that are sometimes considered interchangeable: a constrained version (like Basis Pursuit) and a penalized version (like LASSO). This thought experiment, based on a hypothetical scenario with a degenerate sensing matrix, reveals that this equivalence can be subtle. By analyzing the solution sets, you will see how these two formulations can systematically lead to different results, highlighting the importance of understanding the precise behavior of the model you choose, especially when solutions are not unique [@problem_id:3439951].", "problem": "Consider a linear inverse model in compressed sensing with a degenerate measurement operator. Let the measurement matrix be $A \\in \\mathbb{R}^{2 \\times 2}$ given by\n$$\nA = \\begin{pmatrix} 1 & 1 \\\\ 0 & 0 \\end{pmatrix},\n$$\nand let the observed data be $y \\in \\mathbb{R}^{2}$ given by\n$$\ny = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\nThe decision variable is $x \\in \\mathbb{R}^{2}$. Consider the following two convex formulations for recovering a sparse $x$:\n1. The constrained $\\ell_{1}$-minimization (also called Basis Pursuit Denoising): minimize the $\\ell_{1}$-norm of $x$ subject to an $\\ell_{2}$ data fidelity constraint,\n$$\n\\text{(C)} \\quad \\min_{x \\in \\mathbb{R}^{2}} \\|x\\|_{1} \\quad \\text{subject to} \\quad \\|A x - y\\|_{2} \\leq \\varepsilon,\n$$\nwhere $\\varepsilon > 0$ is a prescribed tolerance.\n2. The penalized $\\ell_{1}$-regularized least squares (also called Least Absolute Shrinkage and Selection Operator): minimize a weighted sum of squared residual and $\\ell_{1}$-norm,\n$$\n\\text{(P)} \\quad \\min_{x \\in \\mathbb{R}^{2}} \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\nwhere $\\lambda > 0$ is a regularization parameter.\n\nBoth problems are convex. The matrix $A$ is degenerate in the sense that it has rank one and nontrivial nullspace, so minimizers need not be unique. To study selection in the presence of non-uniqueness, adopt the following selection rules:\n- In (C), among all minimizers, select one with the smallest support size (fewest nonzero entries), and if there is still non-uniqueness, select the lexicographically smallest vector in $\\mathbb{R}^{2}$.\n- In (P), among all minimizers, select one with the smallest $\\ell_{2}$-norm.\n\nStarting from the definitions of convexity, the $\\ell_{1}$-norm, and the subdifferential optimality conditions (Karush–Kuhn–Tucker conditions), derive from first principles:\n- The structure of the solution sets of (C) and (P) for the given $A$ and $y$.\n- A choice of $\\lambda$ as a function of $\\varepsilon$ such that the selected solutions from (C) and (P) have matching residual norms, that is, $\\|A x_{\\text{C}} - y\\|_{2} = \\|A x_{\\text{P}} - y\\|_{2}$.\n- Demonstrate explicitly that, under the stated selection rules, the selected solutions $x_{\\text{C}}$ and $x_{\\text{P}}$ are different vectors whenever $\\varepsilon \\in (0,1)$, despite the residual norms matching.\n\nWhat is the value of $\\lambda$ written as a closed-form expression in terms of $\\varepsilon$ that achieves matching residual norms under these rules? Express your final answer exactly. No rounding is required.", "solution": "The problem requires the analysis of two convex optimization problems, (C) and (P), for a specific degenerate linear inverse model. We must derive the structure of the solution sets, find a parameter relationship $\\lambda(\\varepsilon)$ that matches the residual norms of selected solutions, and demonstrate that these selected solutions are distinct.\n\nFirst, let's simplify the expressions involving the given matrix $A = \\begin{pmatrix} 1 & 1 \\\\ 0 & 0 \\end{pmatrix}$ and vector $y = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. For any vector $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathbb{R}^2$, the product $Ax$ is:\n$$\nAx = \\begin{pmatrix} 1 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} x_1 + x_2 \\\\ 0 \\end{pmatrix}\n$$\nThe residual vector is $Ax - y = \\begin{pmatrix} x_1 + x_2 - 1 \\\\ 0 \\end{pmatrix}$.\nThe $\\ell_2$-norm of the residual is therefore $\\|Ax - y\\|_{2} = \\sqrt{(x_1+x_2-1)^2 + 0^2} = |x_1+x_2-1|$.\nThe $\\ell_1$-norm of $x$ is $\\|x\\|_1 = |x_1| + |x_2|$.\n\nWith these simplifications, we can analyze each problem.\n\n**Analysis of Problem (C): Constrained $\\ell_1$-Minimization**\n\nThe problem is stated as:\n$$\n\\text{(C)} \\quad \\min_{x \\in \\mathbb{R}^{2}} \\|x\\|_{1} \\quad \\text{subject to} \\quad \\|A x - y\\|_{2} \\leq \\varepsilon\n$$\nUsing our simplified expression for the residual norm, this becomes:\n$$\n\\min_{x_1, x_2} |x_1| + |x_2| \\quad \\text{subject to} \\quad |x_1 + x_2 - 1| \\leq \\varepsilon\n$$\nThe constraint is equivalent to $-\\varepsilon \\leq x_1 + x_2 - 1 \\leq \\varepsilon$, which can be written as $1-\\varepsilon \\leq x_1 + x_2 \\leq 1+\\varepsilon$. Let $s = x_1+x_2$. The constraint restricts $s$ to the interval $[1-\\varepsilon, 1+\\varepsilon]$.\n\nBy the triangle inequality, the objective function is bounded below by the absolute value of the sum: $\\|x\\|_1 = |x_1| + |x_2| \\geq |x_1+x_2| = |s|$. To minimize $\\|x\\|_1$, we must first minimize its lower bound, $|s|$, over the feasible range $s \\in [1-\\varepsilon, 1+\\varepsilon]$.\nGiven that $\\varepsilon \\in (0,1)$, we have $1-\\varepsilon > 0$. Therefore, any feasible $s$ is positive. The minimum value of $|s|$ (which is simply $s$) on the interval $[1-\\varepsilon, 1+\\varepsilon]$ is attained at the left endpoint, $s = 1-\\varepsilon$.\nThe minimum possible value of the objective function is thus $1-\\varepsilon$.\n\nThis minimum is achieved if and only if two conditions are met for a vector $x$:\n1.  $|x_1+x_2| = 1-\\varepsilon$. Since $s=x_1+x_2$ must be in $[1-\\varepsilon, 1+\\varepsilon]$, this forces $x_1+x_2 = 1-\\varepsilon$.\n2.  $\\|x\\|_1 = |x_1+x_2|$. This equality in the triangle inequality holds if and only if $x_1$ and $x_2$ have the same sign or one is zero, i.e., $x_1x_2 \\geq 0$.\n\nSince the sum $x_1+x_2=1-\\varepsilon$ is positive, it is impossible for both $x_1$ and $x_2$ to be negative. Thus, we must have $x_1 \\geq 0$ and $x_2 \\geq 0$.\nThe set of minimizers for (C), denoted $S_C$, is the set of all points satisfying these conditions:\n$$\nS_C = \\{ x = (x_1, x_2) \\in \\mathbb{R}^2 \\mid x_1+x_2=1-\\varepsilon, x_1 \\geq 0, x_2 \\geq 0 \\}\n$$\nThis is the line segment in the first quadrant connecting the points $(1-\\varepsilon, 0)$ and $(0, 1-\\varepsilon)$.\n\nNow, we apply the selection rule for (C).\n1.  Smallest support size: The support of a vector is the number of its non-zero entries. For any point in the interior of the segment $S_C$, both $x_1$ and $x_2$ are positive, so the support size is $2$. At the endpoints, $(1-\\varepsilon, 0)$ and $(0, 1-\\varepsilon)$, one component is zero, so the support size is $1$. The minimum support size is $1$. The candidate solutions are $(1-\\varepsilon, 0)$ and $(0, 1-\\varepsilon)$.\n2.  Lexicographically smallest: We compare the two candidates. A vector $u$ is lexicographically smaller than $v$ if its first differing component is smaller. Comparing $(1-\\varepsilon, 0)$ and $(0, 1-\\varepsilon)$, the first components are $1-\\varepsilon$ and $0$. Since $\\varepsilon \\in (0,1)$, we have $1-\\varepsilon > 0$. Thus, $0 < 1-\\varepsilon$, and the vector $(0, 1-\\varepsilon)$ is lexicographically smaller.\n\nThe selected solution for (C) is $x_C = \\begin{pmatrix} 0 \\\\ 1-\\varepsilon \\end{pmatrix}$.\nThe residual norm for this solution is $\\|Ax_C - y\\|_2 = |0 + (1-\\varepsilon) - 1| = |-\\varepsilon| = \\varepsilon$.\n\n**Analysis of Problem (P): Penalized $\\ell_1$-Regularized Least Squares**\n\nThe problem is stated as:\n$$\n\\text{(P)} \\quad \\min_{x \\in \\mathbb{R}^{2}} \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1}\n$$\nUsing the simplified expressions, this is:\n$$\n\\min_{x_1, x_2} f(x) = \\frac{1}{2}(x_1+x_2-1)^2 + \\lambda(|x_1|+|x_2|)\n$$\nSince $f(x)$ is convex, a vector $x^*$ is a minimizer if and only if the zero vector is in the subdifferential of $f$ at $x^*$, i.e., $0 \\in \\partial f(x^*)$.\nThe subdifferential is $\\partial f(x) = \\nabla(\\frac{1}{2}(x_1+x_2-1)^2) + \\lambda \\partial(\\|x\\|_1)$.\n$$\n\\partial f(x) = \\begin{pmatrix} x_1+x_2-1 \\\\ x_1+x_2-1 \\end{pmatrix} + \\lambda \\begin{pmatrix} \\partial|x_1| \\\\ \\partial|x_2| \\end{pmatrix}\n$$\nwhere $\\partial|t|$ is the signum function $\\text{sgn}(t)$ for $t \\neq 0$ and the interval $[-1, 1]$ for $t=0$.\nLet $x=(x_1, x_2)$ be a minimizer and let $s=x_1+x_2$. The optimality condition $0 \\in \\partial f(x)$ becomes:\n$$\n\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\in \\begin{pmatrix} s-1 \\\\ s-1 \\end{pmatrix} + \\lambda \\begin{pmatrix} g_1 \\\\ g_2 \\end{pmatrix} \\quad \\text{where } g_1 \\in \\partial|x_1|, g_2 \\in \\partial|x_2|\n$$\nThis implies $-(s-1) = \\lambda g_1$ and $-(s-1) = \\lambda g_2$, so $g_1=g_2$. Let $g=g_1=g_2$.\nThe condition is that there exists a $g$ such that $g = -(s-1)/\\lambda$, $g \\in \\partial|x_1|$, and $g \\in \\partial|x_2|$.\n-   If $x_1>0$ and $x_2>0$, then $g_1=g_2=1$. This implies $g=1$, so $1 = -(s-1)/\\lambda$, yielding $s=1-\\lambda$. For $s$ to be a sum of positive numbers, we need $s>0$, so $1-\\lambda > 0$, implying $\\lambda < 1$.\n-   If $x_1<0$ and $x_2<0$, then $g_1=g_2=-1$. This implies $g=-1$, so $-1 = -(s-1)/\\lambda$, yielding $s=1+\\lambda$. For $s$ to be a sum of negative numbers, we need $s<0$, so $1+\\lambda < 0$, which is impossible as $\\lambda>0$.\n-   If $x_1$ and $x_2$ have opposite signs, say $x_1>0, x_2<0$, we would need $g_1=1$ and $g_2=-1$, but we require $g_1=g_2$. Impossible.\n-   If one component is zero, e.g., $x_1>0, x_2=0$, we need $g_1=1$ and $g_2 \\in [-1,1]$. This implies $g=1$, which is consistent. The condition $s=1-\\lambda$ with $\\lambda<1$ holds.\n-   If $x_1=x_2=0$, then $s=0$. We need $g_{1,2} \\in [-1,1]$. The condition is $g = -(0-1)/\\lambda = 1/\\lambda$. So we need $1/\\lambda \\in [-1,1]$, which means $|1/\\lambda| \\leq 1$, so $\\lambda \\geq 1$.\n\nThe structure of the solution set $S_P$ depends on $\\lambda$:\n- For $0 < \\lambda < 1$: The condition is $s=1-\\lambda$ and $x_1, x_2 \\ge 0$. The solution set is the line segment:\n  $S_P = \\{ x = (x_1, x_2) \\in \\mathbb{R}^2 \\mid x_1+x_2=1-\\lambda, x_1 \\geq 0, x_2 \\geq 0 \\}$.\n- For $\\lambda \\geq 1$: The only solution is $x_1=x_2=0$, so $S_P = \\{ (0,0) \\}$.\n\nNow, we apply the selection rule for (P): smallest $\\ell_2$-norm.\n- For $0 < \\lambda < 1$: We must minimize $\\|x\\|_2^2 = x_1^2+x_2^2$ over the set $S_P$. This is a quadratic function over a line segment. The minimum is achieved at the point closest to the origin, which is the midpoint of the segment: $x_1=x_2=(1-\\lambda)/2$. So the selected solution is $x_P = \\begin{pmatrix} (1-\\lambda)/2 \\\\ (1-\\lambda)/2 \\end{pmatrix}$.\n- For $\\lambda \\geq 1$: The solution is unique, so $x_P = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\n**Matching Residuals and Determining $\\lambda(\\varepsilon)$**\n\nThe problem asks to find $\\lambda$ such that $\\|Ax_C - y\\|_2 = \\|Ax_P - y\\|_2$. We have already found $\\|Ax_C - y\\|_2 = \\varepsilon$.\nLet's compute the residual norm for the selected solution $x_P$.\n- For $0 < \\lambda < 1$, $x_P = ((1-\\lambda)/2, (1-\\lambda)/2)^T$. The sum of components is $x_{P1}+x_{P2} = 1-\\lambda$. The residual norm is $\\|Ax_P - y\\|_2 = |(x_{P1}+x_{P2})-1| = |(1-\\lambda)-1| = |-\\lambda| = \\lambda$.\n- For $\\lambda \\geq 1$, $x_P=(0,0)^T$. The sum is $0$. The residual norm is $\\|Ax_P - y\\|_2 = |0-1|=1$.\n\nWe are given that $\\varepsilon \\in (0,1)$. We set the residual norms equal: $\\varepsilon = \\|Ax_P-y\\|_2$.\n- If we are in the regime $0 < \\lambda < 1$, this gives $\\varepsilon = \\lambda$. This is a consistent solution, as $\\varepsilon \\in (0,1)$ implies $\\lambda \\in (0,1)$.\n- If we are in the regime $\\lambda \\geq 1$, this gives $\\varepsilon = 1$. This contradicts the given condition $\\varepsilon \\in (0,1)$.\n\nTherefore, the only valid relationship is $\\lambda = \\varepsilon$.\n\n**Comparison of Selected Solutions**\n\nWith $\\lambda = \\varepsilon$, we compare the selected solution from (C) with the selected solution from (P) for $\\varepsilon \\in (0,1)$.\nThe solution for (C) is $x_C = \\begin{pmatrix} 0 \\\\ 1-\\varepsilon \\end{pmatrix}$.\nThe solution for (P) is $x_P = \\begin{pmatrix} (1-\\lambda)/2 \\\\ (1-\\lambda)/2 \\end{pmatrix} = \\begin{pmatrix} (1-\\varepsilon)/2 \\\\ (1-\\varepsilon)/2 \\end{pmatrix}$.\n\nTo check if these solutions are identical, we set $x_C = x_P$:\n$$\n\\begin{pmatrix} 0 \\\\ 1-\\varepsilon \\end{pmatrix} = \\begin{pmatrix} (1-\\varepsilon)/2 \\\\ (1-\\varepsilon)/2 \\end{pmatrix}\n$$\nThe first component gives $0 = (1-\\varepsilon)/2$, which implies $1-\\varepsilon=0$, so $\\varepsilon=1$.\nThe second component gives $1-\\varepsilon = (1-\\varepsilon)/2$, which also implies $1-\\varepsilon=0$, so $\\varepsilon=1$.\nThe solutions $x_C$ and $x_P$ are identical only if $\\varepsilon=1$. However, the problem specifies that we must consider $\\varepsilon \\in (0,1)$. For any value in this open interval, $1-\\varepsilon \\neq 0$, so $(1-\\varepsilon)/2 \\neq 0$. Consequently, the first components of $x_C$ and $x_P$ are different.\nThis demonstrates explicitly that under the given selection rules and the condition of matching residuals, the selected solutions from the two formulations are different for all $\\varepsilon \\in (0,1)$.\n\nThe value of $\\lambda$ as a closed-form expression in terms of $\\varepsilon$ that achieves matching residual norms is $\\lambda=\\varepsilon$.", "answer": "$$\\boxed{\\varepsilon}$$", "id": "3439951"}]}