{"hands_on_practices": [{"introduction": "To master any complex algorithm, one must first understand its elementary operations. This initial practice focuses on the core mechanics of the projected subgradient method, a cornerstone for constrained nonsmooth optimization. You will perform a single, complete iteration by hand, a process that solidifies two key skills: selecting a valid subgradient for the $\\ell_1$-norm and computing the Euclidean projection onto the probability simplex [@problem_id:3483166]. This hands-on calculation builds the fundamental intuition needed for more advanced applications.", "problem": "Consider the nonsmooth convex optimization problem of minimizing the $\\ell_1$ norm $f(x) = \\|x\\|_{1}$ over the standard probability simplex $\\Delta := \\{x \\in \\mathbb{R}^{5} : x \\ge 0,\\ \\mathbf{1}^{\\top}x = 1\\}$. One iteration of projected subgradient descent from an initial point $x^{(0)} \\in \\Delta$ with a given step size $\\alpha > 0$ takes the form $x^{+} = \\Pi_{\\Delta}\\big(x^{(0)} - \\alpha\\,g^{(0)}\\big)$, where $g^{(0)} \\in \\partial \\|x^{(0)}\\|_{1}$ is any subgradient of $f$ at $x^{(0)}$, $\\partial \\|x^{(0)}\\|_{1}$ denotes the subdifferential, and $\\Pi_{\\Delta}$ is the Euclidean projection onto $\\Delta$, namely the unique minimizer of $\\frac{1}{2}\\|z - y\\|_{2}^{2}$ over $z \\in \\Delta$ for a given $y$.\n\nUse only the following foundational facts in your derivation:\n- For a convex function $f$, a subgradient $g$ at $x$ is any vector satisfying $f(y) \\ge f(x) + g^{\\top}(y - x)$ for all $y$.\n- For $f(x)=\\|x\\|_{1}$, one valid subgradient is $g_{i} = \\operatorname{sign}(x_{i})$ when $x_{i} \\ne 0$, and any $g_{i} \\in [-1,1]$ when $x_{i} = 0$.\n- The Euclidean projection onto a closed convex set is the unique solution to the corresponding quadratic program, which can be characterized by Karush–Kuhn–Tucker (KKT) conditions.\n\nLet the initial point be\n$$\nx^{(0)} = \\begin{pmatrix} \\frac{3}{5} \\\\ \\frac{1}{4} \\\\ \\frac{1}{10} \\\\ \\frac{1}{20} \\\\ 0 \\end{pmatrix},\n$$\nand the step size be $\\alpha = \\frac{3}{10}$. Choose the subgradient $g^{(0)} \\in \\partial \\|x^{(0)}\\|_{1}$ componentwise by $g^{(0)}_{i} = 1$ when $x^{(0)}_{i} > 0$ and $g^{(0)}_{i} = 0$ when $x^{(0)}_{i} = 0$.\n\nCompute the next iterate $x^{+} = \\Pi_{\\Delta}\\big(x^{(0)} - \\alpha\\,g^{(0)}\\big)$ exactly by deriving the KKT conditions of the Euclidean projection subproblem and solving for the active set and threshold. Express your final answer as a single row vector. No rounding is required and no units are involved.", "solution": "The problem is evaluated as valid. It is a well-defined mathematical problem in the field of convex optimization, with all necessary data and conditions provided consistently. There are no scientific or factual unsoundness, ambiguities, or contradictions.\n\nThe task is to compute one iteration of the projected subgradient method for minimizing the function $f(x) = \\|x\\|_{1}$ over the probability simplex $\\Delta$. The update is given by $x^{+} = \\Pi_{\\Delta}\\big(x^{(0)} - \\alpha\\,g^{(0)}\\big)$.\n\nThe process consists of three main steps:\n1.  Determine the subgradient $g^{(0)}$ at the initial point $x^{(0)}$.\n2.  Compute the intermediate vector $y = x^{(0)} - \\alpha\\,g^{(0)}$.\n3.  Calculate the Euclidean projection of $y$ onto the simplex $\\Delta$, i.e., $x^{+} = \\Pi_{\\Delta}(y)$.\n\n**Step 1: Compute the subgradient $g^{(0)}$**\n\nThe initial point is given as\n$$x^{(0)} = \\begin{pmatrix} \\frac{3}{5} \\\\ \\frac{1}{4} \\\\ \\frac{1}{10} \\\\ \\frac{1}{20} \\\\ 0 \\end{pmatrix}$$\nThe problem specifies the rule for choosing the subgradient $g^{(0)} \\in \\partial \\|x^{(0)}\\|_{1}$ as $g^{(0)}_{i} = 1$ when $x^{(0)}_{i} > 0$ and $g^{(0)}_{i} = 0$ when $x^{(0)}_{i} = 0$.\nThe first four components of $x^{(0)}$ are positive, and the fifth component is zero. Applying this rule yields:\n$g^{(0)}_{1} = 1$ since $x^{(0)}_{1} = \\frac{3}{5} > 0$.\n$g^{(0)}_{2} = 1$ since $x^{(0)}_{2} = \\frac{1}{4} > 0$.\n$g^{(0)}_{3} = 1$ since $x^{(0)}_{3} = \\frac{1}{10} > 0$.\n$g^{(0)}_{4} = 1$ since $x^{(0)}_{4} = \\frac{1}{20} > 0$.\n$g^{(0)}_{5} = 0$ since $x^{(0)}_{5} = 0$.\nThus, the subgradient vector is\n$$g^{(0)} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$$\n\n**Step 2: Compute the intermediate vector $y$**\n\nThe intermediate vector is $y = x^{(0)} - \\alpha\\,g^{(0)}$, with the step size $\\alpha = \\frac{3}{10}$.\n$$y = \\begin{pmatrix} \\frac{3}{5} \\\\ \\frac{1}{4} \\\\ \\frac{1}{10} \\\\ \\frac{1}{20} \\\\ 0 \\end{pmatrix} - \\frac{3}{10} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{5} - \\frac{3}{10} \\\\ \\frac{1}{4} - \\frac{3}{10} \\\\ \\frac{1}{10} - \\frac{3}{10} \\\\ \\frac{1}{20} - \\frac{3}{10} \\\\ 0 - 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{6-3}{10} \\\\ \\frac{5-6}{20} \\\\ \\frac{1-3}{10} \\\\ \\frac{1-6}{20} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{10} \\\\ -\\frac{1}{20} \\\\ -\\frac{2}{10} \\\\ -\\frac{5}{20} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{10} \\\\ -\\frac{1}{20} \\\\ -\\frac{1}{5} \\\\ -\\frac{1}{4} \\\\ 0 \\end{pmatrix}$$\nSo, the vector to be projected is $y = \\begin{pmatrix} \\frac{3}{10} & -\\frac{1}{20} & -\\frac{1}{5} & -\\frac{1}{4} & 0 \\end{pmatrix}^{\\top}$.\n\n**Step 3: Compute the projection $x^{+} = \\Pi_{\\Delta}(y)$**\n\nThe projection $x^{+} = \\Pi_{\\Delta}(y)$ is the unique solution to the convex optimization problem:\n$$ \\min_{z \\in \\mathbb{R}^{5}} \\frac{1}{2}\\|z - y\\|_{2}^{2} \\quad \\text{subject to} \\quad z \\ge 0, \\quad \\mathbf{1}^{\\top}z = 1 $$\nWe form the Lagrangian function with Lagrange multipliers $\\lambda \\in \\mathbb{R}^{5}$ for the non-negativity constraints ($z_i \\ge 0$) and a multiplier $\\nu \\in \\mathbb{R}$ for the equality constraint ($\\sum_{i=1}^{5}z_i = 1$).\n$$ L(z, \\lambda, \\nu) = \\frac{1}{2}\\sum_{i=1}^{5}(z_i - y_i)^2 - \\sum_{i=1}^{5}\\lambda_i z_i + \\nu\\left(\\sum_{i=1}^{5}z_i - 1\\right) $$\nThe Karush–Kuhn–Tucker (KKT) conditions for the optimal solution $x^{+}$ are:\n1.  **Stationarity**: $\\frac{\\partial L}{\\partial z_i} = (x^{+}_i - y_i) - \\lambda_i + \\nu = 0$ for $i=1, \\dots, 5$.\n2.  **Primal Feasibility**: $x^{+}_i \\ge 0$ for all $i$, and $\\sum_{i=1}^{5}x^{+}_i = 1$.\n3.  **Dual Feasibility**: $\\lambda_i \\ge 0$ for all $i$.\n4.  **Complementary Slackness**: $\\lambda_i x^{+}_i = 0$ for all $i$.\n\nFrom the stationarity condition, we have $x^{+}_i = y_i - \\nu + \\lambda_i$.\nFrom complementary slackness, if $x^{+}_i > 0$, then $\\lambda_i = 0$, which implies $x^{+}_i = y_i - \\nu$. For this to be positive, we must have $y_i > \\nu$.\nIf $x^{+}_i = 0$, then $\\lambda_i \\ge 0$. The stationarity condition gives $\\lambda_i = \\nu - y_i$, so we must have $\\nu - y_i \\ge 0$, or $y_i \\le \\nu$.\nCombining these cases, the solution for each component is given by $x^{+}_i = \\max(0, y_i - \\nu)$. Letting $\\theta = \\nu$, the solution takes the form $x^{+}_i = (y_i - \\theta)_+$, where $(v)_+ = \\max(0,v)$.\n\nThe threshold $\\theta$ is determined by the equality constraint $\\sum_{i=1}^{5} x^{+}_i = 1$:\n$$ \\sum_{i=1}^{5} (y_i - \\theta)_+ = 1 $$\nTo find $\\theta$, we use a standard algorithm. First, sort the components of $y$ in descending order: $y_{(1)} \\ge y_{(2)} \\ge y_{(3)} \\ge y_{(4)} \\ge y_{(5)}$.\nThe components of $y$ are: $\\{ \\frac{3}{10}, -\\frac{1}{20}, -\\frac{1}{5}, -\\frac{1}{4}, 0 \\}$. In decimal form: $\\{ 0.3, -0.05, -0.2, -0.25, 0 \\}$.\nThe sorted components are:\n$y_{(1)} = \\frac{3}{10}$ (from $y_1$)\n$y_{(2)} = 0$ (from $y_5$)\n$y_{(3)} = -\\frac{1}{20}$ (from $y_2$)\n$y_{(4)} = -\\frac{1}{5}$ (from $y_3$)\n$y_{(5)} = -\\frac{1}{4}$ (from $y_4$)\n\nWe search for an integer $\\rho \\in \\{1, \\dots, 5\\}$ and a value $\\theta$ such that $\\sum_{i=1}^{\\rho} (y_{(i)} - \\theta) = 1$ and $y_{(\\rho)} > \\theta \\ge y_{(\\rho+1)}$ (with $y_{(6)} = -\\infty$). This implies that exactly $\\rho$ components of $x^{+}$ are positive. The threshold is then $\\theta = \\frac{1}{\\rho}\\left(\\sum_{i=1}^{\\rho} y_{(i)} - 1\\right)$.\n\nLet's test values of $\\rho$:\n- For $\\rho=1$: $\\theta = y_{(1)} - 1 = \\frac{3}{10} - 1 = -\\frac{7}{10}$. Condition check: $y_{(1)} > \\theta$ is true, but $y_{(2)} \\ge \\theta$ ($0 \\ge -0.7$) is also true. The condition $y_{(\\rho)} > \\theta \\ge y_{(\\rho+1)}$ fails.\n- For $\\rho=2$: $\\theta = \\frac{y_{(1)} + y_{(2)} - 1}{2} = \\frac{\\frac{3}{10} + 0 - 1}{2} = -\\frac{7}{20}$. Condition check: $y_{(2)} > \\theta$ is true, but $y_{(3)} \\ge \\theta$ ($-\\frac{1}{20} \\ge -\\frac{7}{20}$) is also true. Fails.\n- For $\\rho=3$: $\\theta = \\frac{y_{(1)} + y_{(2)} + y_{(3)} - 1}{3} = \\frac{\\frac{3}{10} + 0 - \\frac{1}{20} - 1}{3} = \\frac{\\frac{6}{20}-\\frac{1}{20}-\\frac{20}{20}}{3} = \\frac{-15/20}{3} = -\\frac{1}{4}$. Condition check: $y_{(3)} > \\theta$ ($-\\frac{1}{20} > -\\frac{1}{4}$) is true, but $y_{(4)} \\ge \\theta$ ($-\\frac{1}{5} \\ge -\\frac{1}{4}$) is also true. Fails.\n- For $\\rho=4$: $\\theta = \\frac{y_{(1)} + y_{(2)} + y_{(3)} + y_{(4)} - 1}{4} = \\frac{\\frac{3}{10} + 0 - \\frac{1}{20} - \\frac{1}{5} - 1}{4} = \\frac{\\frac{6}{20}-\\frac{1}{20}-\\frac{4}{20}-\\frac{20}{20}}{4} = \\frac{-19/20}{4} = -\\frac{19}{80}$.\nCondition check: $y_{(4)} > \\theta \\ge y_{(5)}$?\n$y_{(4)} = -\\frac{1}{5} = -\\frac{16}{80}$. Is $-\\frac{16}{80} > -\\frac{19}{80}$? Yes.\n$y_{(5)} = -\\frac{1}{4} = -\\frac{20}{80}$. Is $-\\frac{19}{80} \\ge -\\frac{20}{80}$? Yes.\nThe condition holds. Thus, the correct number of active (non-zero) components is $\\rho=4$, and the threshold is $\\theta = -\\frac{19}{80}$.\n\nThe active set consists of the components corresponding to $y_{(1)}, y_{(2)}, y_{(3)}, y_{(4)}$, which are $\\{y_1, y_5, y_2, y_3\\}$. The component $x^{+}_4$ will be zero.\n\nWe now compute the components of $x^{+}$ using $x^{+}_i = (y_i - \\theta)_+$ with $\\theta = -\\frac{19}{80}$:\n$x^{+}_1 = y_1 - \\theta = \\frac{3}{10} - (-\\frac{19}{80}) = \\frac{24}{80} + \\frac{19}{80} = \\frac{43}{80}$.\n$x^{+}_2 = y_2 - \\theta = -\\frac{1}{20} - (-\\frac{19}{80}) = -\\frac{4}{80} + \\frac{19}{80} = \\frac{15}{80} = \\frac{3}{16}$.\n$x^{+}_3 = y_3 - \\theta = -\\frac{1}{5} - (-\\frac{19}{80}) = -\\frac{16}{80} + \\frac{19}{80} = \\frac{3}{80}$.\n$x^{+}_4 = (y_4 - \\theta)_+ = (-\\frac{1}{4} - (-\\frac{19}{80}))_+ = (-\\frac{20}{80} + \\frac{19}{80})_+ = (-\\frac{1}{80})_+ = 0$.\n$x^{+}_5 = y_5 - \\theta = 0 - (-\\frac{19}{80}) = \\frac{19}{80}$.\n\nThe resulting vector is $x^{+} = \\begin{pmatrix} \\frac{43}{80} \\\\ \\frac{3}{16} \\\\ \\frac{3}{80} \\\\ 0 \\\\ \\frac{19}{80} \\end{pmatrix}$.\nAs a check, we confirm that the components are non-negative and sum to $1$:\n$\\frac{43}{80} + \\frac{3}{16} + \\frac{3}{80} + 0 + \\frac{19}{80} = \\frac{43}{80} + \\frac{15}{80} + \\frac{3}{80} + \\frac{19}{80} = \\frac{43+15+3+19}{80} = \\frac{80}{80} = 1$.\nThe vector $x^{+}$ lies in the simplex $\\Delta$. The computation is correct. The final answer should be expressed as a single row vector.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{43}{80} & \\frac{3}{16} & \\frac{3}{80} & 0 & \\frac{19}{80}\n\\end{pmatrix}\n}\n$$", "id": "3483166"}, {"introduction": "An algorithm's steps are only useful if they lead toward a solution. This practice moves beyond mere iteration to the crucial task of diagnosis: how can we tell if our current point is close to optimal? You will compute the Karush-Kuhn-Tucker (KKT) residual for the LASSO problem, which serves as a computable certificate of non-optimality, and then use a provided error bound to translate this residual into an upper bound on the distance to the true solution set [@problem_id:3483136]. This exercise demonstrates how theoretical concepts like error bounds provide powerful, practical insights into an algorithm's performance.", "problem": "Consider the convex optimization problem known as least absolute shrinkage and selection operator (LASSO): minimize the function $F(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$, where $A \\in \\mathbb{R}^{3 \\times 3}$, $b \\in \\mathbb{R}^{3}$, and $\\lambda > 0$. The Karush–Kuhn–Tucker (KKT) stationarity condition for optimality is $0 \\in A^{\\top}(A x - b) + \\lambda \\partial \\|x\\|_{1}$, where $\\partial \\|x\\|_{1}$ is the subdifferential of the $\\ell_1$ norm. The KKT residual at an iterate $x$ is defined as the Euclidean distance from the origin to the set $A^{\\top}(A x - b) + \\lambda \\partial \\|x\\|_{1}$:\n$$\n\\operatorname{dist}\\big(0,\\, A^{\\top}(A x - b) + \\lambda \\partial \\|x\\|_{1}\\big) \\;=\\; \\min_{s \\in \\partial \\|x\\|_{1}} \\big\\|A^{\\top}(A x - b) + \\lambda s\\big\\|_{2}.\n$$\nAssume an error bound condition holds: there exists $\\gamma > 0$ such that for all $x$, $\\operatorname{dist}(x, X^{\\star}) \\leq \\gamma \\,\\operatorname{dist}\\big(0,\\, A^{\\top}(A x - b) + \\lambda \\partial \\|x\\|_{1}\\big)$, where $X^{\\star}$ is the set of minimizers of $F$. For the specific data\n$$\nA = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ -2 \\\\ 0 \\end{pmatrix}, \\quad \\lambda = 0.5, \\quad x = \\begin{pmatrix} 0 \\\\ 1 \\\\ -0.2 \\end{pmatrix},\n$$\nand assuming the error bound constant is given by $\\gamma = \\frac{1}{\\lambda_{\\mathrm{min}}(A^{\\top}A)}$, where $\\lambda_{\\mathrm{min}}(A^{\\top}A)$ denotes the smallest eigenvalue of $A^{\\top}A$, compute the KKT residual $\\operatorname{dist}\\big(0,\\, A^{\\top}(A x - b) + \\lambda \\partial \\|x\\|_{1}\\big)$ and then use the error bound to obtain a numerical upper bound on $\\operatorname{dist}(x, X^{\\star})$. Round your final numerical bound to four significant figures.", "solution": "We start from the fundamental KKT stationarity condition for convex optimization with a nonsmooth term. For the function $F(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$, the set of subgradients is\n$$\n\\partial F(x) = A^{\\top}(A x - b) + \\lambda \\partial \\|x\\|_{1},\n$$\nwhere for each coordinate $i$, the subgradient of the $\\ell_1$ norm satisfies\n$$\n\\big(\\partial \\|x\\|_{1}\\big)_{i} = \n\\begin{cases}\n\\{\\operatorname{sign}(x_{i})\\}, & \\text{if } x_{i} \\neq 0, \\\\\n\\left[-1,\\, 1\\right], & \\text{if } x_{i} = 0.\n\\end{cases}\n$$\nThe KKT residual is the Euclidean distance from $0$ to the set $\\partial F(x)$:\n$$\n\\operatorname{dist}\\big(0,\\, \\partial F(x)\\big) = \\min_{s \\in \\partial \\|x\\|_{1}} \\left\\|A^{\\top}(A x - b) + \\lambda s\\right\\|_{2}.\n$$\n\nWe compute $q := A^{\\top}(A x - b)$ for the given data. First,\n$$\nA x = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix}\n\\begin{pmatrix} 0 \\\\ 1 \\\\ -0.2 \\end{pmatrix}\n= \\begin{pmatrix} 0 \\\\ 2 \\\\ -0.6 \\end{pmatrix},\n\\quad\nA x - b = \\begin{pmatrix} 0 \\\\ 2 \\\\ -0.6 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -2 \\\\ 0 \\end{pmatrix}\n= \\begin{pmatrix} -1 \\\\ 4 \\\\ -0.6 \\end{pmatrix}.\n$$\nSince $A$ is diagonal, $A^{\\top} = A$, hence\n$$\nq = A^{\\top}(A x - b) = A(A x - b)\n= \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix}\n\\begin{pmatrix} -1 \\\\ 4 \\\\ -0.6 \\end{pmatrix}\n= \\begin{pmatrix} -2 \\\\ 8 \\\\ -1.8 \\end{pmatrix}.\n$$\n\nTo minimize $\\left\\|q + \\lambda s\\right\\|_{2}$ over $s \\in \\partial \\|x\\|_{1}$, we treat coordinates according to the subgradient rules. Let $q_{i}$ denote the $i$-th entry of $q$.\n\n- For $i = 1$, $x_{1} = 0$, so $s_{1} \\in [-1,1]$. We minimize $\\left|q_{1} + \\lambda s_{1}\\right|$ by choosing $s_{1}$ as the projection of $-\\frac{q_{1}}{\\lambda}$ onto $[-1,1]$. Here, $-\\frac{q_{1}}{\\lambda} = -\\frac{-2}{0.5} = 4$, so the projection is $s_{1} = 1$. Thus $r_{1} := q_{1} + \\lambda s_{1} = -2 + 0.5 \\cdot 1 = -1.5$ and $|r_{1}| = 1.5$. Equivalently, when $x_{1} = 0$, the minimal achievable magnitude is $\\max\\{|q_{1}| - \\lambda, 0\\} = \\max\\{2 - 0.5, 0\\} = 1.5$.\n\n- For $i = 2$, $x_{2} = 1 \\neq 0$, so $s_{2} = \\operatorname{sign}(x_{2}) = 1$. Then $r_{2} := q_{2} + \\lambda s_{2} = 8 + 0.5 \\cdot 1 = 8.5$ and $|r_{2}| = 8.5$.\n\n- For $i = 3$, $x_{3} = -0.2 \\neq 0$, so $s_{3} = \\operatorname{sign}(x_{3}) = -1$. Then $r_{3} := q_{3} + \\lambda s_{3} = -1.8 + 0.5 \\cdot (-1) = -2.3$ and $|r_{3}| = 2.3$.\n\nCollecting these, the minimizing choice of $s$ yields the residual vector $r = q + \\lambda s = \\begin{pmatrix} -1.5 \\\\ 8.5 \\\\ -2.3 \\end{pmatrix}$, so\n$$\n\\operatorname{dist}\\big(0,\\, A^{\\top}(A x - b) + \\lambda \\partial \\|x\\|_{1}\\big)\n= \\|r\\|_{2}\n= \\sqrt{(-1.5)^{2} + (8.5)^{2} + (-2.3)^{2}}\n= \\sqrt{2.25 + 72.25 + 5.29}\n= \\sqrt{79.79}.\n$$\n\nNext, we relate the KKT residual to the distance to the optimal solution set under an error bound condition derived from strong convexity. The function $g(x) := \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ has Hessian $A^{\\top}A$. If $A^{\\top}A$ is positive definite with smallest eigenvalue $\\lambda_{\\mathrm{min}}(A^{\\top}A) = m > 0$, then $g$ is $m$-strongly convex, and $F(x) = g(x) + \\lambda \\|x\\|_{1}$ is also $m$-strongly convex. For an $m$-strongly convex function $F$, the subdifferential mapping $\\partial F$ is $m$-strongly monotone, which implies the error bound\n$$\n\\operatorname{dist}(x, X^{\\star}) \\leq \\frac{1}{m} \\,\\operatorname{dist}\\big(0,\\, \\partial F(x)\\big).\n$$\nIn our case, $A^{\\top}A = \\operatorname{diag}(4,4,9)$, so $\\lambda_{\\mathrm{min}}(A^{\\top}A) = 4$ and hence $\\gamma = \\frac{1}{\\lambda_{\\mathrm{min}}(A^{\\top}A)} = \\frac{1}{4}$.\n\nTherefore,\n$$\n\\operatorname{dist}(x, X^{\\star}) \\leq \\gamma \\,\\operatorname{dist}\\big(0,\\, A^{\\top}(A x - b) + \\lambda \\partial \\|x\\|_{1}\\big)\n= \\frac{1}{4} \\sqrt{79.79}.\n$$\nNumerically,\n$$\n\\sqrt{79.79} \\approx 8.93253,\n\\quad\n\\frac{1}{4}\\sqrt{79.79} \\approx 2.23313.\n$$\nRounded to four significant figures, the numerical upper bound on $\\operatorname{dist}(x, X^{\\star})$ is $2.233$.", "answer": "$$\\boxed{2.233}$$", "id": "3483136"}, {"introduction": "The theoretical simplicity of the subgradient method can sometimes hide practical pitfalls. One of the most common issues in sparse recovery is the tendency of iterates to oscillate across zero, which severely slows down convergence. This hands-on coding challenge requires you to analyze the geometry of the subdifferential that causes this behavior and then design and implement a principled step-size schedule to prevent it, comparing your improved algorithm to a baseline implementation [@problem_id:3483135]. This practice is a capstone exercise in moving from theory to robust, practical implementation.", "problem": "Consider the convex composite objective commonly used in sparse recovery: $$F(x) = \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\|x\\|_1,$$ where $A \\in \\mathbb{R}^{m \\times n}$ is a sensing matrix, $y \\in \\mathbb{R}^m$ is an observation vector, $\\lambda > 0$ is a regularization parameter, and $x \\in \\mathbb{R}^n$ is the decision variable. The subgradient method updates according to $$x_{k+1} = x_k - t_k g_k,$$ where $g_k \\in \\partial F(x_k)$ is a subgradient and $t_k > 0$ is a step size. The subgradient of the smooth part is given by $$\\nabla \\left(\\frac{1}{2}\\|A x - y\\|_2^2\\right) = A^\\top (A x - y),$$ and the Clarke subdifferential (which coincides with the convex subdifferential for convex functions) of the $\\ell_1$ norm is $$\\partial \\|x\\|_1 = \\{s \\in \\mathbb{R}^n : s_i = \\operatorname{sign}(x_i) \\text{ if } x_i \\neq 0,\\ \\ s_i \\in [-1,1] \\text{ if } x_i = 0\\}.$$ The geometry of the Clarke subdifferential near coordinates with small magnitude $|x_i| \\approx 0$ is characterized by a large set of allowable subgradients $s_i \\in [-1,1]$ and can cause oscillatory behavior in subgradient iterations across multiple near-zero coordinates.\n\nStarting from these fundamental definitions, analyze the update geometry for coordinates $i$ with $|x_{k,i}|$ small, and design a principled step-size schedule that explicitly avoids oscillations across multiple near-zero coordinates during sparse recovery. Your schedule must be derived from first principles by requiring the update not to cross zero on near-zero coordinates at iteration $k$, expressed as an inequality constraint on $t_k$ in terms of $x_k$ and $g_k$, and must incorporate a selection rule for the $\\ell_1$ subgradient $s_k \\in \\partial \\|x_k\\|_1$ that is consistent with the Clarke subdifferential geometry at $x_{k,i} = 0$.\n\nImplement two methods:\n- A baseline subgradient method with a standard diminishing step size.\n- Your proposed subgradient method with the oscillation-avoiding step-size schedule and a subgradient selection rule at $x_{k,i} = 0$ that aligns the non-smooth component with the smooth residual.\n\nFor each method, run a fixed number of iterations and count oscillations across near-zero coordinates as follows: define a threshold $\\tau > 0$; for each coordinate $i$, increment the oscillation count if between any two consecutive iterates $x_{k,i}$ and $x_{k+1,i}$ the product $x_{k,i} x_{k+1,i} < 0$ and both magnitudes satisfy $|x_{k,i}| \\le \\tau$ and $|x_{k+1,i}| \\le \\tau$. The total oscillation count is the sum over all coordinates and iterations.\n\nYour program must output, for each test case, a boolean indicating whether the proposed method achieves strictly fewer oscillations than the baseline and a comparable final objective value (no larger than a factor of $\\alpha$ times the baseline’s final value). Aggregate all results into a single line of output as a comma-separated list enclosed in square brackets.\n\nUse the following test suite. Each test case specifies $(m,n,k_{\\text{true}},\\lambda,\\sigma,T,\\text{seed})$ where $k_{\\text{true}}$ is the sparsity (number of nonzero entries) of the ground truth $x^\\star$, $\\sigma$ is the standard deviation of additive Gaussian noise on $y$, $T$ is the number of iterations, and $\\text{seed}$ is the random seed. Construct $A$ with independent and identically distributed Gaussian entries followed by column normalization to unit $\\ell_2$ norm, except where otherwise specified. Construct $x^\\star$ by placing $k_{\\text{true}}$ nonzero entries with values uniformly in $[0.5,1.5]$ and random signs at random locations; set $y = A x^\\star + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I_m)$.\n\n- Test case $1$ (happy path): $(m,n,k_{\\text{true}},\\lambda,\\sigma,T,\\text{seed}) = (40,100,10,0.05,0.0,600,1234)$.\n- Test case $2$ (small regularization): $(m,n,k_{\\text{true}},\\lambda,\\sigma,T,\\text{seed}) = (30,80,6,0.005,0.0,700,5678)$.\n- Test case $3$ (correlated columns): $(m,n,k_{\\text{true}},\\lambda,\\sigma,T,\\text{seed}) = (35,90,8,0.04,0.0,700,9012)$; construct $A$ by first generating a standard Gaussian matrix and then making columns $1$ and $2$ equal up to a small perturbation by setting $A_{\\cdot,2} = A_{\\cdot,1} + 0.05 \\, u$ where $u \\sim \\mathcal{N}(0,I_m)$, followed by column normalization.\n- Test case $4$ (noisy observations): $(m,n,k_{\\text{true}},\\lambda,\\sigma,T,\\text{seed}) = (50,120,12,0.06,0.02,800,2468)$.\n\nLet the baseline step-size be $t_k^{\\text{base}} = \\frac{c}{\\sqrt{k+1}}$ with $c = \\frac{0.9}{L}$ where $L$ is the squared spectral norm of $A$ (that is, the Lipschitz constant of the gradient of the smooth part), and use the subgradient selection $s_{k,i} = \\operatorname{sign}(x_{k,i})$ for $x_{k,i} \\neq 0$ and $s_{k,i} = 0$ for $x_{k,i} = 0$. For the proposed method, use the same base $t_k^{\\text{base}}$ but enforce a safety cap derived from the no-crossing constraint, $$t_k \\le \\gamma \\min_{i \\in \\mathcal{Z}_k} \\frac{|x_{k,i}| + \\epsilon}{|g_{k,i}| + \\epsilon},$$ where $\\gamma \\in (0,1)$ is a safety factor, $\\epsilon > 0$ is a guard value, and $\\mathcal{Z}_k = \\{i : |x_{k,i}| \\le \\delta\\}$ is the near-zero index set with threshold $\\delta > 0$. Select the $\\ell_1$ subgradient at $x_{k,i} = 0$ as $$s_{k,i} = \\operatorname{clip}\\left(-\\frac{(A^\\top (A x_k - y))_i}{\\lambda},\\ -1,\\ 1\\right),$$ which ensures $g_{k,i} = (A^\\top (A x_k - y))_i + \\lambda s_{k,i}$ aligns with the Clarke subdifferential geometry at zero.\n\nUse the parameters $\\tau = 10^{-3}$, $\\delta = 10^{-2}$, $\\epsilon = 10^{-12}$, and $\\gamma = 0.9$ in all test cases. Initialize $x_0 = 0$ in $\\mathbb{R}^n$ for both methods. For each test case, return a boolean indicating $(\\text{osc}_{\\text{proposed}} < \\text{osc}_{\\text{baseline}})$ and $F(x_T^{\\text{proposed}}) \\le \\alpha F(x_T^{\\text{baseline}})$ with $\\alpha = 1.05$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4]$), where each $result_i$ is either $\\text{True}$ or $\\text{False}$.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the well-established theory of nonsmooth convex optimization, specifically subgradient methods applied to the LASSO objective function. The problem is well-posed, with all necessary data, parameters, and objective criteria for evaluation clearly defined. The language is objective and the setup is formalizable into a computational task.\n\nThe core of the problem is to address the oscillatory behavior of the subgradient method when applied to the nonsmooth objective function $F(x) = \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\|x\\|_1$. This function is a sum of a smooth, differentiable part, $f(x) = \\frac{1}{2}\\|A x - y\\|_2^2$, and a nonsmooth, convex part, $h(x) = \\lambda \\|x\\|_1$.\n\nThe subgradient method iteration is given by:\n$$x_{k+1} = x_k - t_k g_k$$\nwhere $k$ is the iteration index, $t_k > 0$ is the step size, and $g_k$ is a subgradient of $F$ at $x_k$. According to the sum rule for subdifferentials, any subgradient $g_k \\in \\partial F(x_k)$ can be written as:\n$$g_k = \\nabla f(x_k) + \\lambda s_k, \\quad \\text{where } s_k \\in \\partial \\|x_k\\|_1$$\nThe gradient of the smooth part is $\\nabla f(x) = A^\\top(A x - y)$. The subdifferential of the $\\ell_1$-norm, $\\partial \\|x\\|_1$, is the set of vectors $s$ such that for each component $i$:\n$$s_i = \\begin{cases} \\operatorname{sign}(x_i) & \\text{if } x_i \\neq 0 \\\\ v_i \\in [-1, 1] & \\text{if } x_i = 0 \\end{cases}$$\nThe oscillatory behavior arises near coordinates where $|x_{k,i}|$ is small. The update for a single coordinate $i$ is $x_{k+1,i} = x_{k,i} - t_k g_{k,i}$. If $t_k |g_{k,i}|$ is larger than $|x_{k,i}|$, the update will cause $x_{k,i}$ to cross zero, i.e., $\\operatorname{sign}(x_{k+1,i}) \\neq \\operatorname{sign}(x_{k,i})$. Repeated sign changes for a coordinate that should converge to zero are inefficient and undesirable. The problem proposes a principled approach to mitigate this by designing a specialized step-size schedule and subgradient selection rule.\n\n### Derivation of the Proposed Method\n\n**1. Oscillation-Avoiding Step-Size Schedule**\n\nThe primary goal is to prevent iterates from crossing the origin for coordinates that are already close to it. We define a \"near-zero\" set of coordinates at iteration $k$ as $\\mathcal{Z}_k = \\{i : |x_{k,i}| \\le \\delta\\}$ for some threshold $\\delta > 0$.\nFor any coordinate $i \\in \\mathcal{Z}_k$, we wish to enforce the condition that the update does not change its sign. This can be expressed as requiring that the magnitude of the update step, $|t_k g_{k,i}|$, does not exceed the current magnitude of the coordinate, $|x_{k,i}|$.\n$$|t_k g_{k,i}| \\le |x_{k,i}|$$\nAssuming $g_{k,i} \\neq 0$, this yields an upper bound on the step size:\n$$t_k \\le \\frac{|x_{k,i}|}{|g_{k,i}|}$$\nTo ensure this condition holds for all coordinates in the near-zero set $\\mathcal{Z}_k$, the step size must be constrained by the most restrictive of these bounds:\n$$t_k \\le \\min_{i \\in \\mathcal{Z}_k} \\frac{|x_{k,i}|}{|g_{k,i}|}$$\nFor numerical stability (to avoid division by zero when $|g_{k,i}|$ is small) and to introduce a conservative margin, we add a small positive guard value $\\epsilon > 0$ to both the numerator and denominator, and multiply by a safety factor $\\gamma \\in (0,1)$. This gives the step-size cap specified in the problem:\n$$t_k^{\\text{cap}} = \\gamma \\min_{i \\in \\mathcal{Z}_k} \\frac{|x_{k,i}| + \\epsilon}{|g_{k,i}| + \\epsilon}$$\nThe final proposed step size $t_k$ is then chosen as the minimum of a standard diminishing step size $t_k^{\\text{base}}$ and this adaptive cap: $t_k = \\min(t_k^{\\text{base}}, t_k^{\\text{cap}})$. If the set $\\mathcal{Z}_k$ is empty, no cap is applied, and $t_k = t_k^{\\text{base}}$.\n\n**2. Principled Subgradient Selection**\n\nThe second component of the proposed method addresses the choice of subgradient $s_{k,i}$ for coordinates where $x_{k,i} = 0$. At such points, we have the freedom to choose any $s_{k,i} \\in [-1,1]$. A principled choice is one that minimizes the magnitude of the resulting subgradient component $|g_{k,i}| = |\\left(A^\\top (A x_k - y)\\right)_i + \\lambda s_{k,i}|$. This is a strategy to stabilize the algorithm by selecting the \"smallest\" possible subgradient from the subdifferential set $\\partial F(x_k)$.\n\nTo minimize $|g_{k,i}|$, we should choose $s_{k,i}$ to counteract the smooth part of the gradient, $(\\nabla f(x_k))_i = \\left(A^\\top (A x_k - y)\\right)_i$. The ideal value would be $s_{k,i} = -(\\nabla f(x_k))_i / \\lambda$, which would make $g_{k,i} = 0$. However, the choice of $s_{k,i}$ is constrained to the interval $[-1, 1]$. Therefore, the optimal choice for $s_{k,i}$ is the value in $[-1, 1]$ that is closest to $-(\\nabla f(x_k))_i / \\lambda$. This is achieved by clipping (projecting) the value onto the interval:\n$$s_{k,i} = \\operatorname{clip}\\left(-\\frac{(A^\\top (A x_k - y))_i}{\\lambda}, -1, 1\\right)$$\nThis selection rule is consistent with the geometry of the Clarke subdifferential at zero; it represents a deliberate choice to align the nonsmooth component against the smooth residual, thereby promoting stability and convergence towards a sparse solution. This is precisely the rule used in the well-known Iterative Soft-Thresholding Algorithm (ISTA), a more advanced proximal gradient method for this class of problems.\n\nThe combination of the adaptive step-size cap and the principled subgradient selection rule forms a sophisticated subgradient method tailored to the structure of the sparse recovery problem, designed from first principles to mitigate the oscillations that plague the standard subgradient method. The implementation will compare this proposed method against a baseline method using a standard step-size schedule and a simpler subgradient selection rule.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulations and generate the final output.\n    \"\"\"\n    # Define problem parameters\n    test_cases = [\n        # (m, n, k_true, lambda, sigma, T, seed)\n        (40, 100, 10, 0.05, 0.0, 600, 1234),\n        (30, 80, 6, 0.005, 0.0, 700, 5678),\n        (35, 90, 8, 0.04, 0.0, 700, 9012),\n        (50, 120, 12, 0.06, 0.02, 800, 2468),\n    ]\n\n    # Algorithm parameters\n    tau = 1e-3\n    delta = 1e-2\n    epsilon = 1e-12\n    gamma = 0.9\n    alpha = 1.05\n\n    results = []\n\n    for i, case in enumerate(test_cases):\n        m, n, k_true, lambda_val, sigma, T, seed = case\n        \n        # --- Data Generation ---\n        rng = np.random.default_rng(seed)\n        A = rng.normal(size=(m, n))\n\n        if i == 2:  # Correlated columns for Test Case 3\n            u = rng.normal(size=m)\n            A[:, 1] = A[:, 0] + 0.05 * u\n\n        col_norms = np.linalg.norm(A, axis=0)\n        A = A / col_norms[np.newaxis, :]\n        \n        x_star = np.zeros(n)\n        support = rng.choice(n, k_true, replace=False)\n        values = rng.uniform(0.5, 1.5, k_true)\n        signs = rng.choice([-1, 1], k_true)\n        x_star[support] = values * signs\n        \n        noise = rng.normal(0, sigma, m)\n        y = A @ x_star + noise\n\n        # --- Helper Functions ---\n        def objective_function(x, A, y, lambda_val):\n            return 0.5 * np.linalg.norm(A @ x - y)**2 + lambda_val * np.linalg.norm(x, 1)\n\n        def grad_smooth(x, A, y):\n            return A.T @ (A @ x - y)\n\n        L = np.linalg.norm(A, 2)**2\n        c = 0.9 / L\n        x0 = np.zeros(n)\n\n        # --- Baseline Subgradient Method ---\n        x_base = x0.copy()\n        osc_base = 0\n        for k in range(T):\n            x_prev = x_base.copy()\n            grad_h = grad_smooth(x_base, A, y)\n            s_k = np.sign(x_base)\n            g_k = grad_h + lambda_val * s_k\n            t_k = c / np.sqrt(k + 1)\n            x_base = x_base - t_k * g_k\n            \n            # Count oscillations\n            crossed_zero = (x_prev * x_base < 0)\n            small_mag = (np.abs(x_prev) <= tau) & (np.abs(x_base) <= tau)\n            osc_base += np.sum(crossed_zero & small_mag)\n\n        final_obj_base = objective_function(x_base, A, y, lambda_val)\n\n        # --- Proposed Subgradient Method ---\n        x_prop = x0.copy()\n        osc_prop = 0\n        for k in range(T):\n            x_prev = x_prop.copy()\n            \n            grad_h = grad_smooth(x_prop, A, y)\n            \n            s_k = np.zeros(n)\n            nonzero_idx = x_prop != 0\n            zero_idx = x_prop == 0\n            \n            s_k[nonzero_idx] = np.sign(x_prop[nonzero_idx])\n            s_k[zero_idx] = np.clip(-grad_h[zero_idx] / lambda_val, -1, 1)\n            \n            g_k = grad_h + lambda_val * s_k\n            \n            t_base = c / np.sqrt(k + 1)\n            \n            near_zero_idx = np.where(np.abs(x_prop) <= delta)[0]\n            if near_zero_idx.size > 0:\n                numer = np.abs(x_prop[near_zero_idx]) + epsilon\n                denom = np.abs(g_k[near_zero_idx]) + epsilon\n                t_cap = gamma * np.min(numer / denom)\n                t_k = min(t_base, t_cap)\n            else:\n                t_k = t_base\n\n            x_prop = x_prop - t_k * g_k\n            \n            # Count oscillations\n            crossed_zero = (x_prev * x_prop < 0)\n            small_mag = (np.abs(x_prev) <= tau) & (np.abs(x_prop) <= tau)\n            osc_prop += np.sum(crossed_zero & small_mag)\n\n        final_obj_prop = objective_function(x_prop, A, y, lambda_val)\n        \n        # --- Evaluate and Store Result ---\n        osc_check = osc_prop < osc_base\n        obj_check = final_obj_prop <= alpha * final_obj_base\n        results.append(osc_check and obj_check)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3483135"}]}