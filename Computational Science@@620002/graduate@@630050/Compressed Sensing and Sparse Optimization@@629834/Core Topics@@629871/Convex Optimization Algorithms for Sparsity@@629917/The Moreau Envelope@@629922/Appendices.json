{"hands_on_practices": [{"introduction": "The most direct way to understand a new mathematical tool is often to apply it to a simple case and work through the details by hand. This exercise grounds the abstract definition of the Moreau envelope in a concrete calculation, focusing on its most celebrated feature: smoothing. By analytically deriving the envelope for a 1D convex piecewise-affine function [@problem_id:3167906], you will see precisely how this operation transforms non-differentiable \"kinks\" into smooth, quadratic curves.", "problem": "Consider the convex piecewise affine function $f:\\mathbb{R}\\to\\mathbb{R}$ defined by\n$$\nf(y)=|y|+2\\max\\{0,y-1\\}=\n\\begin{cases}\n-y, & y\\leq 0,\\\\\ny, & 0\\leq y\\leq 1,\\\\\n3y-2, & y\\geq 1.\n\\end{cases}\n$$\nLet $\\lambda>0$ be fixed. The Moreau envelope $e_{\\lambda}f:\\mathbb{R}\\to\\mathbb{R}$ is defined for each $x\\in\\mathbb{R}$ by\n$$\ne_{\\lambda}f(x)=\\inf_{y\\in\\mathbb{R}}\\left\\{f(y)+\\frac{1}{2\\lambda}(y-x)^{2}\\right\\}.\n$$\nStarting from the definition of the Moreau envelope and the piecewise affine structure of $f$, derive $e_{\\lambda}f(x)$ analytically on each interval of $x$ induced by the unconstrained stationary points in the regions $y\\leq 0$, $0\\leq y\\leq 1$, and $y\\geq 1$, together with the kink locations $y=0$ and $y=1$. Justify that the infimum is attained uniquely and determine the minimizer by first-order optimality in each region, with boundary checks at $y=0$ and $y=1$ when necessary. Then, explicitly evaluate $e_{\\lambda}f(x)$ in terms of $x$ and $\\lambda$ on each resulting interval, and explain how the nondifferentiable kinks of $f$ at $y=0$ and $y=1$ are smoothed in $e_{\\lambda}f$.\n\nExpress your final answer as a single closed-form analytical expression for $e_{\\lambda}f(x)$ written piecewise in $x$ using standard mathematical notation. No rounding is required, and no physical units are involved.", "solution": "The problem is a valid exercise in convex analysis, specifically the computation of the Moreau envelope for a given convex, piecewise affine function. All terms are well-defined, and the premises are mathematically sound.\n\nThe Moreau envelope of a function $f:\\mathbb{R}\\to\\mathbb{R}$ with parameter $\\lambda > 0$ is defined as:\n$$\ne_{\\lambda}f(x) = \\inf_{y\\in\\mathbb{R}}\\left\\{f(y)+\\frac{1}{2\\lambda}(y-x)^{2}\\right\\}\n$$\nLet $g(y; x) = f(y)+\\frac{1}{2\\lambda}(y-x)^{2}$. The function $f(y)$ is convex. The term $\\frac{1}{2\\lambda}(y-x)^{2}$ is strongly convex in $y$ for a fixed $x$ and $\\lambda > 0$. The sum of a convex and a strongly convex function is strongly convex. Therefore, for any given $x \\in \\mathbb{R}$, $g(y;x)$ is a strongly convex function of $y$, which guarantees that the infimum is uniquely attained at a single point $y^*$. This unique minimizer is the proximal operator of $\\lambda f$ at $x$, denoted $y^* = \\text{prox}_{\\lambda f}(x)$.\n\nThe first-order optimality condition for a convex function states that the minimizer $y^*$ is the point where the subgradient of the objective function contains zero. The subgradient of $g(y;x)$ with respect to $y$ is:\n$$\n\\partial_y g(y;x) = \\partial f(y) + \\frac{1}{\\lambda}(y-x)\n$$\nThe optimality condition is $0 \\in \\partial g(y^*;x)$, which can be rewritten as:\n$$\nx - y^* \\in \\lambda \\partial f(y^*)\n$$\nWe must first compute the subdifferential $\\partial f(y)$ of the given function:\n$$\nf(y)=\n\\begin{cases}\n-y, & y\\leq 0,\\\\\ny, & 0\\leq y\\leq 1,\\\\\n3y-2, & y\\geq 1.\n\\end{cases}\n$$\nThe subdifferential is:\n- For $y < 0$, $f'(y)=-1$, so $\\partial f(y) = \\{-1\\}$.\n- For $0 < y < 1$, $f'(y)=1$, so $\\partial f(y) = \\{1\\}$.\n- For $y > 1$, $f'(y)=3$, so $\\partial f(y) = \\{3\\}$.\n- At the kink point $y=0$, the subdifferential is the interval between the left and right derivatives, $\\partial f(0) = [-1, 1]$.\n- At the kink point $y=1$, the subdifferential is $\\partial f(1) = [1, 3]$.\n\nWe now determine the minimizer $y^*$ and the corresponding value of $e_{\\lambda}f(x)$ by considering five exhaustive and mutually exclusive cases for the location of $y^*$, which partitions the domain of $x$.\n\n**Case 1: The minimizer is in the region $y^* < 0$.**\nHere, $\\partial f(y^*) = \\{-1\\}$. The optimality condition becomes $x - y^* = \\lambda(-1) = -\\lambda$, which implies $y^* = x + \\lambda$. This case is consistent if and only if $y^* < 0$, which means $x + \\lambda < 0$, or $x < -\\lambda$.\nFor $x < -\\lambda$, the minimizer is $y^*=x+\\lambda$. The Moreau envelope is:\n$$\ne_{\\lambda}f(x) = f(x+\\lambda) + \\frac{1}{2\\lambda}((x+\\lambda)-x)^2 = -(x+\\lambda) + \\frac{(-\\lambda)^2}{2\\lambda} = -x - \\lambda + \\frac{\\lambda}{2} = -x - \\frac{\\lambda}{2}\n$$\n\n**Case 2: The minimizer is at the kink point $y^* = 0$.**\nHere, $\\partial f(0) = [-1, 1]$. The optimality condition becomes $x - 0 \\in \\lambda[-1, 1]$, which simplifies to $x \\in [-\\lambda, \\lambda]$.\nFor $x \\in [-\\lambda, \\lambda]$, the minimizer is $y^*=0$. The Moreau envelope is:\n$$\ne_{\\lambda}f(x) = f(0) + \\frac{1}{2\\lambda}(0-x)^2 = 0 + \\frac{x^2}{2\\lambda} = \\frac{x^2}{2\\lambda}\n$$\n\n**Case 3: The minimizer is in the region $0 < y^* < 1$.**\nHere, $\\partial f(y^*) = \\{1\\}$. The optimality condition is $x - y^* = \\lambda(1) = \\lambda$, which implies $y^* = x - \\lambda$. This case is consistent if and only if $0 < y^* < 1$, which means $0 < x - \\lambda < 1$, or $\\lambda < x < 1+\\lambda$.\nFor $\\lambda < x < 1+\\lambda$, the minimizer is $y^*=x-\\lambda$. The Moreau envelope is:\n$$\ne_{\\lambda}f(x) = f(x-\\lambda) + \\frac{1}{2\\lambda}((x-\\lambda)-x)^2 = (x-\\lambda) + \\frac{\\lambda^2}{2\\lambda} = x - \\lambda + \\frac{\\lambda}{2} = x - \\frac{\\lambda}{2}\n$$\n\n**Case 4: The minimizer is at the kink point $y^* = 1$.**\nHere, $\\partial f(1) = [1, 3]$. The optimality condition is $x - 1 \\in \\lambda[1, 3]$, which means $\\lambda \\leq x - 1 \\leq 3\\lambda$, or $1+\\lambda \\leq x \\leq 1+3\\lambda$.\nFor $x \\in [1+\\lambda, 1+3\\lambda]$, the minimizer is $y^*=1$. The Moreau envelope is:\n$$\ne_{\\lambda}f(x) = f(1) + \\frac{1}{2\\lambda}(1-x)^2 = (3(1)-2) + \\frac{(x-1)^2}{2\\lambda} = 1 + \\frac{(x-1)^2}{2\\lambda}\n$$\n\n**Case 5: The minimizer is in the region $y^* > 1$.**\nHere, $\\partial f(y^*) = \\{3\\}$. The optimality condition is $x - y^* = \\lambda(3) = 3\\lambda$, which implies $y^* = x - 3\\lambda$. This case is consistent if and only if $y^* > 1$, which means $x - 3\\lambda > 1$, or $x > 1+3\\lambda$.\nFor $x > 1+3\\lambda$, the minimizer is $y^*=x-3\\lambda$. The Moreau envelope is:\n$$\ne_{\\lambda}f(x) = f(x-3\\lambda) + \\frac{1}{2\\lambda}((x-3\\lambda)-x)^2 = (3(x-3\\lambda)-2) + \\frac{(-3\\lambda)^2}{2\\lambda} = 3x - 9\\lambda - 2 + \\frac{9\\lambda}{2} = 3x - 2 - \\frac{9\\lambda}{2}\n$$\n\nCombining these five cases, we obtain the piecewise analytical expression for the Moreau envelope $e_{\\lambda}f(x)$:\n$$\ne_{\\lambda}f(x) =\n\\begin{cases}\n-x - \\frac{\\lambda}{2}, & x < -\\lambda \\\\\n\\frac{x^2}{2\\lambda}, & -\\lambda \\leq x \\leq \\lambda \\\\\nx - \\frac{\\lambda}{2}, & \\lambda < x < 1+\\lambda \\\\\n1 + \\frac{(x-1)^2}{2\\lambda}, & 1+\\lambda \\leq x \\leq 1+3\\lambda \\\\\n3x - 2 - \\frac{9\\lambda}{2}, & x > 1+3\\lambda\n\\end{cases}\n$$\nThe function $e_{\\lambda}f(x)$ is continuously differentiable ($C^1$). The derivative is given by the general formula $e'_{\\lambda}f(x) = \\frac{1}{\\lambda}(x - \\text{prox}_{\\lambda f}(x))$. Computing this derivative for each piece:\n- for $x < -\\lambda$, $e'_{\\lambda}f(x) = -1$.\n- for $-\\lambda < x < \\lambda$, $e'_{\\lambda}f(x) = \\frac{x}{\\lambda}$.\n- for $\\lambda < x < 1+\\lambda$, $e'_{\\lambda}f(x) = 1$.\n- for $1+\\lambda < x < 1+3\\lambda$, $e'_{\\lambda f}(x) = \\frac{x-1}{\\lambda}$.\n- for $x > 1+3\\lambda$, $e'_{\\lambda}f(x) = 3$.\nIt is straightforward to verify that this derivative is continuous at all the boundary points $x=-\\lambda, \\lambda, 1+\\lambda, 1+3\\lambda$. This $C^1$ property demonstrates the smoothing effect of the Moreau envelope. The nondifferentiable kinks of $f(y)$ at $y=0$ and $y=1$ are smoothed. The jump in the derivative of $f(y)$ from $-1$ to $1$ at $y=0$ is replaced by a smooth linear transition of $e'_{\\lambda}f(x)$ from $-1$ to $1$ over the interval $x \\in [-\\lambda, \\lambda]$. Similarly, the jump in the derivative of $f(y)$ from $1$ to $3$ at $y=1$ is replaced by a smooth linear transition of $e'_{\\lambda}f(x)$ from $1$ to $3$ over the interval $x \\in [1+\\lambda, 1+3\\lambda]$. The quadratic pieces in $e_{\\lambda}f(x)$ serve to \"round off\" the sharp corners of the original function $f(y)$.", "answer": "$$\n\\boxed{\ne_{\\lambda}f(x) =\n\\begin{cases}\n-x - \\frac{\\lambda}{2}, & x < -\\lambda \\\\\n\\frac{x^2}{2\\lambda}, & -\\lambda \\leq x \\leq \\lambda \\\\\nx - \\frac{\\lambda}{2}, & \\lambda < x < 1+\\lambda \\\\\n1 + \\frac{(x-1)^2}{2\\lambda}, & 1+\\lambda \\leq x \\leq 1+3\\lambda \\\\\n3x - 2 - \\frac{9\\lambda}{2}, & x > 1+3\\lambda\n\\end{cases}\n}\n$$", "id": "3167906"}, {"introduction": "Beyond its smoothing properties, the Moreau envelope provides a powerful bridge between non-smooth and smooth optimization. This practice guides you through deriving the gradient of the envelope and then implementing gradient descent to minimize a smoothed version of the non-differentiable $L_1$ norm [@problem_id:3126039]. Through this process, you will uncover an elegant equivalence: gradient descent on the Moreau envelope with a specific step-size is identical to the famous proximal point algorithm.", "problem": "Consider a proper, lower semicontinuous, convex function $f : \\mathbb{R}^d \\to \\mathbb{R} \\cup \\{+\\infty\\}$ and the Moreau envelope at parameter $\\lambda > 0$ defined by\n$$\nM_{\\lambda} f(w) \\triangleq \\inf_{u \\in \\mathbb{R}^d} \\left\\{ f(u) + \\frac{1}{2\\lambda} \\|u - w\\|_2^2 \\right\\}.\n$$\nYour task is to use the Moreau envelope to smooth a non-smooth convex function and to derive its gradient from first principles, then implement gradient descent on this smooth surrogate.\n\nFundamental base to use:\n- The definition of convexity and subgradients: for a convex function $f$, the subdifferential at $u$, denoted $\\partial f(u)$, is the set of all subgradients $g$ satisfying $f(v) \\ge f(u) + g^\\top (v - u)$ for all $v$.\n- The optimality condition for convex minimization: if $u^\\star$ minimizes a convex function $g(u)$, then $0 \\in \\partial g(u^\\star)$.\n- The proximal operator of $f$ at parameter $\\lambda$, defined by\n$$\n\\operatorname{prox}_{\\lambda f}(w) \\triangleq \\arg\\min_{u \\in \\mathbb{R}^d} \\left\\{ f(u) + \\frac{1}{2\\lambda} \\|u - w\\|_2^2 \\right\\}.\n$$\n- Danskinâ€™s theorem (parametric optimization sensitivity): If $\\phi(u,w)$ is convex in $u$ and differentiable in $w$, and the minimizer $u^\\star(w)$ is unique for all $w$, then $g(w) \\triangleq \\min_{u} \\phi(u,w)$ is differentiable and $\\nabla g(w) = \\nabla_w \\phi(u^\\star(w), w)$.\n\nProblem requirements:\n1. Derive the expression for the gradient $\\nabla M_{\\lambda} f(w)$ in terms of the proximal operator $\\operatorname{prox}_{\\lambda f}(w)$, starting only from the definitions above and the optimality condition for convex minimization. Justify all steps so that each follows from the provided fundamental base.\n2. Specialize to the non-smooth convex function $f(w) = \\alpha \\|w\\|_1$ with $\\alpha \\ge 0$ and $w \\in \\mathbb{R}^d$. Derive the closed-form proximal operator for this $f$ and obtain an explicit coordinate-wise formula for $\\nabla M_{\\lambda} f(w)$, ensuring the derivation is consistent with the definition of the proximal operator and the Moreau envelope.\n3. Implement a gradient descent algorithm to minimize $M_{\\lambda} f(w)$ for the specialized $f(w) = \\alpha \\|w\\|_1$. Use a fixed step size $\\eta = \\lambda$. Explain why $\\eta = \\lambda$ is a valid choice based on the Lipschitz continuity of the gradient of $M_{\\lambda} f$.\n4. In your program, implement functions to compute the proximal operator, the value of the Moreau envelope, and its gradient for the specialized $f$. Then run gradient descent from given initial points for a fixed number of iterations and report the final Moreau envelope value $M_{\\lambda} f(w_T)$ where $w_T$ is the final iterate after $T$ steps.\n\nTest suite:\nRun your program on the following parameter sets. In each case, $w_0$ is the initial vector, $d$ is the dimension (which equals the length of $w_0$), $\\alpha$ and $\\lambda$ are scalars with $\\alpha \\ge 0$ and $\\lambda > 0$, and $T$ is the number of gradient descent iterations.\n- Case $1$: $\\alpha = 0.5$, $\\lambda = 0.2$, $d = 3$, $w_0 = [3, -1, 0.05]$, $T = 100$.\n- Case $2$: $\\alpha = 0.5$, $\\lambda = 1.0$, $d = 3$, $w_0 = [-2, 2, 0]$, $T = 50$.\n- Case $3$ (boundary condition): $\\alpha = 0.0$, $\\lambda = 0.3$, $d = 4$, $w_0 = [1, -1, 2, -2]$, $T = 30$.\n- Case $4$ (small $\\lambda$): $\\alpha = 1.0$, $\\lambda = 0.05$, $d = 5$, $w_0 = [1, -0.5, 0.2, -3, 4]$, $T = 200$.\n- Case $5$ (large $\\lambda$): $\\alpha = 0.2$, $\\lambda = 2.0$, $d = 2$, $w_0 = [10, -10]$, $T = 50$.\n\nAnswer specification:\n- For each test case, compute the final scalar value $M_{\\lambda} f(w_T)$ after $T$ gradient descent iterations with step size $\\eta = \\lambda$.\n- Your program should produce a single line of output containing these $5$ final values as a comma-separated list enclosed in square brackets, in the order of the cases given: for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5]$.\n- Each output element must be a real number (float). No physical units are involved in this problem, and no angles or percentages are required.\n\nThe scenario is purely mathematical and consistent with statistical learning. All symbols, variables, functions, operators, and numbers must be expressed in LaTeX in the statements above.", "solution": "The problem is valid as it is mathematically well-defined, self-contained, and grounded in the standard principles of convex optimization and statistical learning. We proceed with the derivation and implementation as requested.\n\n### 1. Derivation of the Gradient of the Moreau Envelope\n\nThe Moreau envelope of a proper, lower semicontinuous, convex function $f : \\mathbb{R}^d \\to \\mathbb{R} \\cup \\{+\\infty\\}$ is defined as\n$$\nM_{\\lambda} f(w) \\triangleq \\inf_{u \\in \\mathbb{R}^d} \\left\\{ f(u) + \\frac{1}{2\\lambda} \\|u - w\\|_2^2 \\right\\}.\n$$\nLet us define the function $\\phi(u, w) = f(u) + \\frac{1}{2\\lambda} \\|u - w\\|_2^2$. Then $M_{\\lambda} f(w) = \\inf_{u} \\phi(u, w)$.\nThe proximal operator is defined as the unique minimizer of this expression:\n$$\n\\operatorname{prox}_{\\lambda f}(w) = \\arg\\min_{u \\in \\mathbb{R}^d} \\phi(u, w).\n$$\nThe uniqueness of the minimizer is guaranteed because $f(u)$ is convex and the quadratic term $\\frac{1}{2\\lambda} \\|u - w\\|_2^2$ is strongly convex in $u$ (for $\\lambda > 0$), making their sum $\\phi(u, w)$ strongly convex in $u$.\n\nWe are given Danskin's theorem, which states that if the minimizer $u^\\star(w)$ is unique, then the gradient of $g(w) \\triangleq \\min_{u} \\phi(u,w)$ is given by $\\nabla g(w) = \\nabla_w \\phi(u^\\star(w), w)$.\nIn our case, $g(w) = M_{\\lambda} f(w)$ and the unique minimizer is $u^\\star(w) = \\operatorname{prox}_{\\lambda f}(w)$. The function $\\phi(u, w)$ is differentiable with respect to $w$. We compute its gradient $\\nabla_w \\phi(u, w)$:\n$$\n\\nabla_w \\phi(u, w) = \\nabla_w \\left( f(u) + \\frac{1}{2\\lambda} \\|u - w\\|_2^2 \\right).\n$$\nSince $f(u)$ does not depend on $w$, its gradient with respect to $w$ is zero. The gradient of the quadratic term is:\n$$\n\\nabla_w \\left( \\frac{1}{2\\lambda} (u - w)^\\top(u - w) \\right) = \\frac{1}{2\\lambda} \\cdot 2(u-w) \\cdot (-1) = -\\frac{1}{\\lambda}(u - w) = \\frac{1}{\\lambda}(w - u).\n$$\nApplying Danskin's theorem, we substitute the minimizer $u = \\operatorname{prox}_{\\lambda f}(w)$ into this expression for the gradient:\n$$\n\\nabla M_{\\lambda} f(w) = \\nabla_w \\phi(\\operatorname{prox}_{\\lambda f}(w), w) = \\frac{1}{\\lambda} (w - \\operatorname{prox}_{\\lambda f}(w)).\n$$\nThis is the desired expression for the gradient of the Moreau envelope.\n\n### 2. Specialization to $f(w) = \\alpha \\|w\\|_1$\n\nWe now consider the specific non-smooth convex function $f(w) = \\alpha \\|w\\|_1 = \\alpha \\sum_{i=1}^d |w_i|$ for $\\alpha \\ge 0$.\nTo find the proximal operator $\\operatorname{prox}_{\\lambda f}(w)$, we must solve the minimization problem:\n$$\n\\operatorname{prox}_{\\lambda f}(w) = \\arg\\min_{u \\in \\mathbb{R}^d} \\left\\{ \\alpha \\sum_{i=1}^d |u_i| + \\frac{1}{2\\lambda} \\sum_{i=1}^d (u_i - w_i)^2 \\right\\}.\n$$\nThis objective function is separable, meaning it can be minimized independently for each coordinate $u_i$:\n$$\n(\\operatorname{prox}_{\\lambda f}(w))_i = \\arg\\min_{u_i \\in \\mathbb{R}} \\left\\{ \\alpha |u_i| + \\frac{1}{2\\lambda} (u_i - w_i)^2 \\right\\}.\n$$\nLet $g(u_i) = \\alpha |u_i| + \\frac{1}{2\\lambda} (u_i - w_i)^2$. From the optimality condition for convex minimization, the minimizer $u_i^\\star$ must satisfy $0 \\in \\partial g(u_i^\\star)$. The subdifferential is $\\partial g(u_i) = \\alpha \\partial|u_i| + \\frac{1}{\\lambda}(u_i - w_i)$.\nThe subdifferential of the absolute value function is $\\partial|x| = \\operatorname{sgn}(x)$ if $x \\ne 0$ and $\\partial|x| = [-1, 1]$ if $x = 0$.\nThe condition $0 \\in \\partial g(u_i^\\star)$ implies $w_i - u_i^\\star \\in \\lambda\\alpha \\partial|u_i^\\star|$. We analyze three cases for $u_i^\\star$:\n1. If $u_i^\\star > 0$, then $\\partial|u_i^\\star| = \\{1\\}$. The condition becomes $w_i - u_i^\\star = \\lambda\\alpha$, so $u_i^\\star = w_i - \\lambda\\alpha$. This is consistent only if $w_i - \\lambda\\alpha > 0$, i.e., $w_i > \\lambda\\alpha$.\n2. If $u_i^\\star < 0$, then $\\partial|u_i^\\star| = \\{-1\\}$. The condition becomes $w_i - u_i^\\star = -\\lambda\\alpha$, so $u_i^\\star = w_i + \\lambda\\alpha$. This is consistent only if $w_i + \\lambda\\alpha < 0$, i.e., $w_i < -\\lambda\\alpha$.\n3. If $u_i^\\star = 0$, then $\\partial|u_i^\\star| = [-1, 1]$. The condition becomes $w_i \\in \\lambda\\alpha[-1, 1]$, which is equivalent to $|w_i| \\le \\lambda\\alpha$.\n\nCombining these cases, we obtain the soft-thresholding operator $S_{\\lambda\\alpha}$:\n$$\n(\\operatorname{prox}_{\\lambda f}(w))_i = S_{\\lambda\\alpha}(w_i) \\triangleq \\begin{cases} w_i - \\lambda\\alpha & \\text{if } w_i > \\lambda\\alpha \\\\ w_i + \\lambda\\alpha & \\text{if } w_i < -\\lambda\\alpha \\\\ 0 & \\text{if } |w_i| \\le \\lambda\\alpha \\end{cases}\n$$\nThis can be written compactly as $(\\operatorname{prox}_{\\lambda f}(w))_i = \\operatorname{sgn}(w_i) \\max(|w_i| - \\lambda\\alpha, 0)$.\n\nUsing this closed-form proximal operator, we can write the explicit formula for the gradient of the Moreau envelope from Part 1:\n$$\n(\\nabla M_{\\lambda} f(w))_i = \\frac{1}{\\lambda}(w_i - (\\operatorname{prox}_{\\lambda f}(w))_i) = \\frac{1}{\\lambda}(w_i - S_{\\lambda\\alpha}(w_i)).\n$$\nThis results in a coordinate-wise formula for the gradient:\n$$\n(\\nabla M_{\\lambda} f(w))_i = \\begin{cases} \\alpha & \\text{if } w_i > \\lambda\\alpha \\\\ -\\alpha & \\text{if } w_i < -\\lambda\\alpha \\\\ w_i/\\lambda & \\text{if } |w_i| \\le \\lambda\\alpha \\end{cases}\n$$\n\n### 3. Gradient Descent Algorithm and Step Size Justification\n\nThe gradient descent update rule for minimizing $M_{\\lambda} f(w)$ is $w_{k+1} = w_k - \\eta \\nabla M_{\\lambda} f(w_k)$. We are to use step size $\\eta = \\lambda$.\nTo justify this choice, we must show that the gradient $\\nabla M_{\\lambda} f$ is Lipschitz continuous. The gradient of the Moreau envelope is known to be $1/\\lambda$-Lipschitz continuous (this is a result of the Baillon-Haddad theorem). For a convex function $f$, its Moreau envelope $M_\\lambda f$ is continuously differentiable with a gradient that is Lipschitz continuous with constant $L = 1/\\lambda$.\nA standard result for gradient descent on a function with an $L$-Lipschitz continuous gradient is that convergence is guaranteed for any fixed step size $\\eta \\in (0, 2/L)$. In our case, $L=1/\\lambda$, so the condition for convergence is $\\eta \\in (0, 2\\lambda)$. The choice $\\eta = \\lambda$ clearly falls within this interval, making it a valid choice.\n\nMoreover, this specific choice leads to a remarkable simplification. Substituting $\\eta = \\lambda$ and the expression for the gradient into the update rule:\n$$\nw_{k+1} = w_k - \\lambda \\cdot \\nabla M_{\\lambda} f(w_k) = w_k - \\lambda \\cdot \\left(\\frac{1}{\\lambda}(w_k - \\operatorname{prox}_{\\lambda f}(w_k))\\right) = w_k - (w_k - \\operatorname{prox}_{\\lambda f}(w_k)).\n$$\nThis simplifies to:\n$$\nw_{k+1} = \\operatorname{prox}_{\\lambda f}(w_k).\n$$\nTherefore, performing gradient descent on the Moreau envelope $M_{\\lambda} f(w)$ with step size $\\eta=\\lambda$ is equivalent to applying the proximal point algorithm to the original function $f(w)$. This is a powerful connection between smoothing techniques and proximal methods.\n\n### 4. Implementation Strategy\n\nBased on the above analysis, the implementation will consist of the following:\n1.  A function `prox_l1(w, alpha, lambda_val)` that implements the soft-thresholding operator $S_{\\lambda\\alpha}(w)$.\n2.  A function `moreau_envelope_l1(w, alpha, lambda_val)` that computes $M_{\\lambda} f(w)$ by first finding $u = \\operatorname{prox}_{\\lambda f}(w)$ and then evaluating $f(u) + \\frac{1}{2\\lambda}\\|u-w\\|_2^2$.\n3.  The main loop will iterate for $T$ steps. In each step, it will update the weight vector $w$ using the simplified proximal point update: $w_{k+1} = \\operatorname{prox}_{\\lambda f}(w_k)$.\n4.  After $T$ iterations, the final Moreau envelope value $M_{\\lambda} f(w_T)$ is calculated for the final iterate $w_T$ and stored. This process is repeated for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing gradient descent on the Moreau envelope\n    for the L1 norm, which simplifies to the proximal point algorithm.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # alpha, lambda, w0, T\n        (0.5, 0.2, np.array([3.0, -1.0, 0.05]), 100),\n        (0.5, 1.0, np.array([-2.0, 2.0, 0.0]), 50),\n        (0.0, 0.3, np.array([1.0, -1.0, 2.0, -2.0]), 30),\n        (1.0, 0.05, np.array([1.0, -0.5, 0.2, -3.0, 4.0]), 200),\n        (0.2, 2.0, np.array([10.0, -10.0]), 50),\n    ]\n\n    def prox_l1(w, alpha, lambda_val):\n        \"\"\"\n        Computes the proximal operator for f(w) = alpha * ||w||_1.\n        This is the soft-thresholding operator.\n        \"\"\"\n        threshold = lambda_val * alpha\n        # Component-wise operation\n        return np.sign(w) * np.maximum(np.abs(w) - threshold, 0.0)\n\n    def moreau_envelope_l1(w, alpha, lambda_val):\n        \"\"\"\n        Computes the value of the Moreau envelope for f(w) = alpha * ||w||_1.\n        M_lambda_f(w) = f(prox(w)) + (1/(2*lambda)) * ||prox(w) - w||^2\n        \"\"\"\n        u_prox = prox_l1(w, alpha, lambda_val)\n        \n        # f(u_prox) = alpha * ||u_prox||_1\n        f_u_prox = alpha * np.linalg.norm(u_prox, 1)\n        \n        # (1/(2*lambda)) * ||u_prox - w||_2^2\n        quadratic_term = (1.0 / (2.0 * lambda_val)) * np.linalg.norm(u_prox - w, 2)**2\n        \n        return f_u_prox + quadratic_term\n\n    results = []\n    for case in test_cases:\n        alpha, lambda_val, w0, T = case\n        \n        w = w0.copy() # Start with the initial vector\n        \n        # Gradient descent with step size eta = lambda simplifies to the proximal point algorithm\n        # w_{k+1} = prox(w_k)\n        for _ in range(T):\n            w = prox_l1(w, alpha, lambda_val)\n        \n        # After T iterations, we have w_T. Compute the final Moreau envelope value.\n        final_value = moreau_envelope_l1(w, alpha, lambda_val)\n        results.append(final_value)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3126039"}, {"introduction": "The beautiful properties of the Moreau envelope, such as guaranteeing a smooth function, hinge critically on the convexity of the original function. This final thought experiment challenges you to explore the boundaries of the theory by removing this key assumption [@problem_id:3167995]. By analyzing a simple non-convex function, you will discover how fundamental properties like smoothness of the envelope and continuity of the proximal map can break down, deepening your appreciation for the role of convexity.", "problem": "Consider the one-dimensional function $f:\\mathbb{R}\\to[0,+\\infty)$ defined by\n$$\nf(y)=\\begin{cases}\n0, & \\text{if } y\\in\\{-1,\\,1\\},\\\\\n1, & \\text{otherwise.}\n\\end{cases}\n$$\nLet the parameter be $\\lambda=1$. Using only the definitions of the proximal operator and the Moreau envelope:\n- Show that the proximal mapping $x\\mapsto \\operatorname{prox}_{f}(x)$ is discontinuous in $x$ by identifying a point $x_{0}\\in\\mathbb{R}$ where a jump occurs and justifying why the discontinuity holds.\n- Derive an explicit expression for the Moreau envelope $e_{1}f(x)$ and demonstrate that it is nonsmooth by identifying at least one point where it fails to be differentiable.\n- Briefly discuss any caveats that arise for nonconvex functions with respect to existence, uniqueness, and smoothness in these constructions.\n\nFinally, compute the exact value of $e_{1}f(0)$ and provide it as your final answer. No rounding is required.", "solution": "This problem requires a detailed analysis of the proximal operator and Moreau envelope for a specific nonconvex function. We will proceed by first validating the problem statement and then providing a step-by-step derivation and discussion as requested.\n\nThe function is $f:\\mathbb{R}\\to[0,+\\infty)$ defined as\n$$\nf(y)=\\begin{cases}\n0, & \\text{if } y\\in\\{-1,\\,1\\},\\\\\n1, & \\text{otherwise.}\n\\end{cases}\n$$\nThe parameter is $\\lambda=1$.\n\nThe **proximal operator** $\\operatorname{prox}_{\\lambda f}(x)$ is defined as the set of minimizers of a composite objective function:\n$$ \\operatorname{prox}_{\\lambda f}(x) = \\underset{y \\in \\mathbb{R}}{\\operatorname{argmin}} \\left( f(y) + \\frac{1}{2\\lambda} (y - x)^2 \\right) $$\nFor this problem, with $\\lambda=1$, this becomes:\n$$ \\operatorname{prox}_{f}(x) = \\underset{y \\in \\mathbb{R}}{\\operatorname{argmin}} \\left( f(y) + \\frac{1}{2} (y - x)^2 \\right) $$\nThe **Moreau envelope** $e_{\\lambda f}(x)$ is the minimum value achieved in the above optimization:\n$$ e_{\\lambda f}(x) = \\inf_{y \\in \\mathbb{R}} \\left( f(y) + \\frac{1}{2\\lambda} (y - x)^2 \\right) $$\nWith $\\lambda=1$, the notation used is $e_{1}f(x)$:\n$$ e_{1}f(x) = \\inf_{y \\in \\mathbb{R}} \\left( f(y) + \\frac{1}{2} (y - x)^2 \\right) $$\n\nTo determine $\\operatorname{prox}_{f}(x)$ and $e_{1}f(x)$, we must find the minimum of the objective function $g(y;x) = f(y) + \\frac{1}{2}(y-x)^2$. We can analyze the problem by splitting the domain of $y$ according to the definition of $f(y)$.\n\nCase 1: $y \\in \\{-1, 1\\}$.\nIn this case, $f(y)=0$. The objective function becomes $g(y;x) = \\frac{1}{2}(y-x)^2$. We must evaluate this at $y=-1$ and $y=1$.\n- At $y=-1$, the value is $\\frac{1}{2}(-1-x)^2 = \\frac{1}{2}(x+1)^2$.\n- At $y=1$, the value is $\\frac{1}{2}(1-x)^2$.\nThe minimum value in this case is $\\min\\left(\\frac{1}{2}(x+1)^2, \\frac{1}{2}(x-1)^2\\right)$. The minimizer is $-1$ if $|x+1| < |x-1|$ (i.e., $x<0$), $1$ if $|x-1| < |x+1|$ (i.e., $x>0$), and both $-1$ and $1$ if $x=0$.\n\nCase 2: $y \\notin \\{-1, 1\\}$.\nIn this case, $f(y)=1$. The objective function is $g(y;x) = 1 + \\frac{1}{2}(y-x)^2$.\nThe term $\\frac{1}{2}(y-x)^2$ is minimized when $y$ is as close as possible to $x$.\n- If $x \\notin \\{-1, 1\\}$, the infimum is attained at $y=x$, and the minimum value is $1 + \\frac{1}{2}(x-x)^2 = 1$.\n- If $x \\in \\{-1, 1\\}$, the infimum is sought over $y \\notin \\{-1, 1\\}$. The value $1$ is approached as $y \\to x$, but is not attained within this set. Thus, the infimum is $1$.\nSo, for any $x \\in \\mathbb{R}$, the infimum value in this case is $1$.\n\nThe global minimum for $g(y;x)$ is the smaller of the values from the two cases. Thus,\n$$ e_{1}f(x) = \\min\\left(1, \\frac{1}{2}(x+1)^2, \\frac{1}{2}(x-1)^2\\right) $$\nThe set $\\operatorname{prox}_{f}(x)$ consists of the value(s) of $y$ that achieve this minimum.\n\n**Part 1: Discontinuity of the Proximal Mapping**\n\nWe construct the proximal mapping $\\operatorname{prox}_{f}(x)$ by identifying the minimizer(s) for each $x$. This depends on the comparison between $1$ and $\\min(\\frac{1}{2}(x+1)^2, \\frac{1}{2}(x-1)^2)$.\n\n- For $x > 0$, the minimum in Case 1 is at $y=1$ with value $\\frac{1}{2}(x-1)^2$. We compare this to $1$.\n  - If $\\frac{1}{2}(x-1)^2 < 1$, i.e., $(x-1)^2 < 2$, which means $|x-1| < \\sqrt{2}$, or $1-\\sqrt{2} < x < 1+\\sqrt{2}$. For $x \\in (0, 1+\\sqrt{2})$, the minimizer is $y=1$. Thus, $\\operatorname{prox}_{f}(x) = \\{1\\}$.\n  - If $\\frac{1}{2}(x-1)^2 > 1$, i.e., $x > 1+\\sqrt{2}$, the minimum value is $1$, attained at $y=x$. Thus, $\\operatorname{prox}_{f}(x) = \\{x\\}$.\n  - If $\\frac{1}{2}(x-1)^2 = 1$, i.e., $x = 1+\\sqrt{2}$, the minimum value is $1$, attained at both $y=1$ and $y=x=1+\\sqrt{2}$. Thus, $\\operatorname{prox}_{f}(1+\\sqrt{2}) = \\{1, 1+\\sqrt{2}\\}$.\n\n- For $x < 0$, by symmetry, the minimum in Case 1 is at $y=-1$ with value $\\frac{1}{2}(x+1)^2$.\n  - If $\\frac{1}{2}(x+1)^2 < 1$, i.e., $x \\in (-1-\\sqrt{2}, 0)$, the minimizer is $y=-1$. Thus, $\\operatorname{prox}_{f}(x) = \\{-1\\}$.\n  - If $\\frac{1}{2}(x+1)^2 > 1$, i.e., $x < -1-\\sqrt{2}$, the minimum value is $1$, attained at $y=x$. Thus, $\\operatorname{prox}_{f}(x) = \\{x\\}$.\n  - If $\\frac{1}{2}(x+1)^2 = 1$, i.e., $x = -1-\\sqrt{2}$, the minimizers are $y=-1$ and $y=x=-1-\\sqrt{2}$. Thus, $\\operatorname{prox}_{f}(-1-\\sqrt{2}) = \\{-1, -1-\\sqrt{2}\\}$.\n\n- For $x=0$, the minimum in Case 1 is $\\frac{1}{2}$, attained at both $y=-1$ and $y=1$. Since $\\frac{1}{2} < 1$, these are the global minimizers. Thus, $\\operatorname{prox}_{f}(0) = \\{-1, 1\\}$.\n\nLet's choose the point $x_0 = 1+\\sqrt{2}$ to demonstrate discontinuity.\n- For $x$ in an interval just below $x_0$, such as $x \\in (0, 1+\\sqrt{2})$, the proximal operator gives a single value: $\\operatorname{prox}_{f}(x) = \\{1\\}$. The limit as $x$ approaches $x_0$ from the left is $\\lim_{x \\to (1+\\sqrt{2})^-} \\operatorname{prox}_{f}(x) = \\{1\\}$.\n- For $x$ in an interval just above $x_0$, such as $x > 1+\\sqrt{2}$, the proximal operator gives $\\operatorname{prox}_{f}(x) = \\{x\\}$. The limit as $x$ approaches $x_0$ from the right is $\\lim_{x \\to (1+\\sqrt{2})^+} \\operatorname{prox}_{f}(x) = \\{1+\\sqrt{2}\\}$.\nSince the left-hand and right-hand limits differ, the mapping has a jump discontinuity at $x_0=1+\\sqrt{2}$. Any single-valued function selected from the (possibly set-valued) mapping $\\operatorname{prox}_f(x)$ would necessarily be discontinuous at this point.\n\n**Part 2: Nonsmoothness of the Moreau Envelope**\n\nFrom the previous analysis, the Moreau envelope $e_{1}f(x)$ is given by:\n$$\ne_{1}f(x) = \\begin{cases}\n\\frac{1}{2}(x+1)^2, & \\text{if } x \\in [-1-\\sqrt{2}, 0] \\\\\n\\frac{1}{2}(x-1)^2, & \\text{if } x \\in [0, 1+\\sqrt{2}] \\\\\n1, & \\text{otherwise}\n\\end{cases}\n$$\nFor clarity, a more detailed piecewise form is:\n$$\ne_{1}f(x) = \\begin{cases}\n1, & \\text{if } x \\le -1-\\sqrt{2} \\\\\n\\frac{1}{2}(x+1)^2, & \\text{if } -1-\\sqrt{2} < x < 0 \\\\\n\\frac{1}{2}(x-1)^2, & \\text{if } 0 \\le x < 1+\\sqrt{2} \\\\\n1, & \\text{if } x \\ge 1+\\sqrt{2}\n\\end{cases}\n$$\nThis function is continuous everywhere. To check for smoothness, we examine its derivative. We can identify potential points of non-differentiability where the definition of the function changes, for instance at $x=0$, $x=1+\\sqrt{2}$, and $x=-1-\\sqrt{2}$.\nLet's analyze the differentiability at $x=0$.\nThe left-hand derivative at $x=0$ is determined by the expression $\\frac{1}{2}(x+1)^2$:\n$$ (e_{1}f)'_{-}(0) = \\frac{d}{dx}\\left(\\frac{1}{2}(x+1)^2\\right)\\bigg|_{x=0} = (x+1)\\big|_{x=0} = 1 $$\nThe right-hand derivative at $x=0$ is determined by the expression $\\frac{1}{2}(x-1)^2$:\n$$ (e_{1}f)'_{+}(0) = \\frac{d}{dx}\\left(\\frac{1}{2}(x-1)^2\\right)\\bigg|_{x=0} = (x-1)\\big|_{x=0} = -1 $$\nSince the left-hand derivative ($1$) and the right-hand derivative ($-1$) are not equal, the function $e_{1}f(x)$ is not differentiable at $x=0$. Thus, the Moreau envelope is nonsmooth.\n\n**Part 3: Caveats for Nonconvex Functions**\n\nThe analysis of this specific function $f$ illustrates general properties and challenges that arise when applying proximal calculus to nonconvex functions.\n\n1.  **Existence**: The existence of the proximal operator and Moreau envelope is generally assured for proper, lower semi-continuous (l.s.c.) functions. The function $f$ is proper and l.s.c. The quadratic term $\\frac{1}{2\\lambda}(y-x)^2$ is coercive, meaning it tends to $+\\infty$ as $|y| \\to \\infty$. The sum $f(y) + \\frac{1}{2\\lambda}(y-x)^2$ is therefore also coercive (since $f$ is bounded below by $0$), which guarantees that the infimum is finite and is attained. Thus, $\\operatorname{prox}_{\\lambda f}(x)$ is always a non-empty set.\n\n2.  **Uniqueness**: For a convex function $f$, the objective $f(y) + \\frac{1}{2\\lambda}(y-x)^2$ is strictly convex, which guarantees a unique minimizer. Consequently, $\\operatorname{prox}_{\\lambda f}(x)$ is single-valued. However, if $f$ is nonconvex, the objective function may not be convex and can have multiple global minimizers. In our specific problem, we found that for $x \\in \\{0, 1+\\sqrt{2}, -1-\\sqrt{2}\\}$, the set of minimizers contains more than one point. This set-valued nature of the proximal operator for nonconvex functions is a fundamental departure from the convex case.\n\n3.  **Smoothness**: For a proper, l.s.c., and convex function $f$, its Moreau envelope $e_{\\lambda f}$ is continuously differentiable ($C^1$). Its gradient is famously given by Moreau's identity: $\\nabla e_{\\lambda f}(x) = \\frac{1}{\\lambda}(x - \\operatorname{prox}_{\\lambda f}(x))$. For nonconvex functions, this smoothness property is lost. As we demonstrated, $e_{1}f(x)$ is not differentiable at points where the identity of the minimizer in the prox computation changes or becomes non-unique (e.g., at $x=0, \\pm(1+\\sqrt{2})$). This lack of smoothness complicates the analysis and convergence guarantees of optimization algorithms based on these constructs, such as proximal gradient descent.\n\n**Final Computation**\n\nWe are asked to compute the exact value of $e_{1}f(0)$. Using the definition:\n$$ e_{1}f(0) = \\inf_{y \\in \\mathbb{R}} \\left( f(y) + \\frac{1}{2} (y-0)^2 \\right) = \\inf_{y \\in \\mathbb{R}} \\left( f(y) + \\frac{1}{2} y^2 \\right) $$\nWe again analyze two cases:\n- Case 1: $y \\in \\{-1, 1\\}$. $f(y)=0$. The objective values are $\\frac{1}{2}(-1)^2 = \\frac{1}{2}$ and $\\frac{1}{2}(1)^2 = \\frac{1}{2}$. The minimum in this case is $\\frac{1}{2}$.\n- Case 2: $y \\notin \\{-1, 1\\}$. $f(y)=1$. The objective is $1 + \\frac{1}{2}y^2$. The minimum of this expression is $1$, which occurs at $y=0$ (although $y=0$ is a valid choice here, the infimum is still $1$).\n\nComparing the minimums from both cases, we find the global infimum:\n$$ e_{1}f(0) = \\min\\left(\\frac{1}{2}, 1\\right) = \\frac{1}{2} $$\nAlternatively, we can use our derived expression for $e_1f(x)$ and evaluate it at $x=0$:\n$$ e_{1}f(0) = \\frac{1}{2}(0-1)^2 = \\frac{1}{2} $$\nThe value is exact and requires no rounding.", "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$", "id": "3167995"}]}