{"hands_on_practices": [{"introduction": "Before diving into a full-scale implementation, it is crucial to understand why accelerated methods for nonsmooth optimization have a specific structure. This first exercise provides that foundational insight through a carefully constructed thought experiment. By analyzing a hypothetical, \"naive\" acceleration scheme that ignores the nonsmooth term, you will see firsthand how and why such an approach can fail, justifying the necessity of the composite proximal gradient step that is central to algorithms like FISTA [@problem_id:3461182].", "problem": "Consider the composite convex optimization problem central to compressed sensing and sparse optimization: minimize the function $F(x) = f(x) + g(x)$, where $f$ has an $L$-Lipschitz continuous gradient and $g$ is a proper, closed, convex but generally nonsmooth regularizer. A canonical instance is the one-dimensional Lasso objective with $f(x) = \\tfrac{1}{2} L x^{2}$ (so that $\\nabla f(x) = L x$) and $g(x) = \\lambda |x|$. The minimizer in this example is $x^{\\star} = 0$ for any $\\lambda > 0$. The Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) uses an extrapolation step followed by a composite proximal step that accounts for $g$. In contrast, a naive acceleration that applies extrapolation but omits the proximal step treats the objective as if $g$ were absent and performs an accelerated gradient step only on $f$.\n\nAnalyze the following naive extrapolated scheme that ignores $g$ entirely:\n- Fix $L = 1$, $\\lambda > 0$, a constant extrapolation parameter $\\beta = 2$, and a step size $t = 0.1$ (which satisfies $t \\leq 1/L$).\n- Define the extrapolated point $y_{k} = x_{k} + \\beta (x_{k} - x_{k-1})$.\n- Update via the smooth-gradient-only rule $x_{k+1} = y_{k} - t \\nabla f(y_{k})$.\n- Initialize with $x_{-1} = 0$ and $x_{0} = 1$.\n\nStarting from the foundational definitions of $L$-Lipschitz gradients, convexity, and the proximal operator, derive the linear recurrence induced by this naive scheme and compute the spectral radius of the associated $2 \\times 2$ companion matrix governing the state evolution $(x_{k}, x_{k-1})$. Use this calculation to demonstrate that the naive extrapolation can diverge even with a step size $t \\leq 1/L$, thereby justifying the necessity of the composite proximal structure in accelerated methods for nonsmooth regularizers such as the $\\ell_{1}$ norm. Your final answer must be the spectral radius as an exact real number. Do not round your answer.", "solution": "The problem asks for an analysis of a naive accelerated gradient method applied to a composite optimization problem. The validity of the problem statement has been confirmed as it is scientifically grounded, well-posed, and objective. We proceed to derive the solution.\n\nThe objective function to minimize is $F(x) = f(x) + g(x)$, where $f(x)$ has an $L$-Lipschitz continuous gradient and $g(x)$ is a nonsmooth regularizer. The specific instance under consideration is the one-dimensional Lasso problem with $f(x) = \\frac{1}{2} L x^2$ and $g(x) = \\lambda |x|$. The problem specifies the following constants: $L=1$, $\\lambda > 0$, a constant extrapolation parameter $\\beta = 2$, and a step size $t = 0.1$. The step size condition $t \\leq \\frac{1}{L}$ is satisfied, as $0.1 \\leq \\frac{1}{1}$.\n\nThe naive scheme ignores the nonsmooth term $g(x)$ and consists of the following steps:\n1. Extrapolation: $y_{k} = x_{k} + \\beta (x_{k} - x_{k-1})$\n2. Gradient update on $f$ only: $x_{k+1} = y_{k} - t \\nabla f(y_{k})$\n\nFor the given function $f(x) = \\frac{1}{2} L x^2$ with $L=1$, we have $f(x) = \\frac{1}{2} x^2$. The gradient of $f$ is $\\nabla f(x) = x$. Therefore, the gradient evaluated at the extrapolated point $y_k$ is $\\nabla f(y_k) = y_k$.\n\nWe can now substitute the given parameters and expressions into the iterative scheme.\nThe extrapolation step with $\\beta=2$ is:\n$$y_{k} = x_{k} + 2(x_{k} - x_{k-1}) = 3x_k - 2x_{k-1}$$\n\nThe update step with $t=0.1$ and $\\nabla f(y_k) = y_k$ is:\n$$x_{k+1} = y_{k} - 0.1 y_{k} = (1 - 0.1) y_{k} = 0.9 y_{k}$$\n\nCombining these two equations, we can establish a linear recurrence relation for the sequence $\\{x_k\\}$:\n$$x_{k+1} = 0.9 (3x_k - 2x_{k-1})$$\n$$x_{k+1} = 2.7 x_k - 1.8 x_{k-1}$$\n\nThis is a second-order homogeneous linear recurrence relation with constant coefficients. To analyze its dynamics, we can represent it in a state-space form. Let the state vector at iteration $k$ be $v_k = \\begin{pmatrix} x_k \\\\ x_{k-1} \\end{pmatrix}$. The state transition is given by $v_{k+1} = M v_k$, where $M$ is the companion matrix.\nThe next state vector is $v_{k+1} = \\begin{pmatrix} x_{k+1} \\\\ x_k \\end{pmatrix}$. We can write the system as:\n$$\n\\begin{pmatrix} x_{k+1} \\\\ x_k \\end{pmatrix} = \\begin{pmatrix} 2.7 & -1.8 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} x_k \\\\ x_{k-1} \\end{pmatrix}\n$$\nThe companion matrix governing the state evolution is therefore:\n$$M = \\begin{pmatrix} 2.7 & -1.8 \\\\ 1 & 0 \\end{pmatrix}$$\n\nThe stability of the system is determined by the spectral radius of $M$, denoted $\\rho(M)$, which is the maximum of the absolute values of its eigenvalues. To find the eigenvalues, we solve the characteristic equation $\\det(M - \\zeta I) = 0$, where $I$ is the identity matrix and $\\zeta$ represents an eigenvalue.\n$$\\det \\begin{pmatrix} 2.7 - \\zeta & -1.8 \\\\ 1 & 0 - \\zeta \\end{pmatrix} = 0$$\n$$(2.7 - \\zeta)(-\\zeta) - (-1.8)(1) = 0$$\n$$-2.7\\zeta + \\zeta^2 + 1.8 = 0$$\n$$\\zeta^2 - 2.7\\zeta + 1.8 = 0$$\n\nWe solve this quadratic equation for $\\zeta$ using the quadratic formula, $\\zeta = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$, with $a=1$, $b=-2.7$, and $c=1.8$.\n$$\\zeta = \\frac{-(-2.7) \\pm \\sqrt{(-2.7)^2 - 4(1)(1.8)}}{2(1)}$$\n$$\\zeta = \\frac{2.7 \\pm \\sqrt{7.29 - 7.2}}{2}$$\n$$\\zeta = \\frac{2.7 \\pm \\sqrt{0.09}}{2}$$\n$$\\zeta = \\frac{2.7 \\pm 0.3}{2}$$\n\nThe two eigenvalues are:\n$$\\zeta_1 = \\frac{2.7 + 0.3}{2} = \\frac{3.0}{2} = 1.5$$\n$$\\zeta_2 = \\frac{2.7 - 0.3}{2} = \\frac{2.4}{2} = 1.2$$\n\nThe spectral radius $\\rho(M)$ is the maximum of the magnitudes of the eigenvalues:\n$$\\rho(M) = \\max(|\\zeta_1|, |\\zeta_2|) = \\max(|1.5|, |1.2|) = 1.5$$\n\nSince the spectral radius $\\rho(M) = 1.5$ is greater than $1$, the iterative scheme is unstable. For the given non-zero initialization ($x_{-1}=0, x_0=1$), the norm of the state vector $\\|v_k\\|$ will grow exponentially, and thus the sequence of iterates $\\{x_k\\}$ will diverge. This demonstrates that applying extrapolation without the corresponding proximal step for the nonsmooth term $g(x)$ can lead to divergence, even when the step size $t$ is chosen small enough to grant convergence for standard gradient descent on the smooth part $f(x)$. This justifies the necessity of the composite proximal structure inherent to algorithms like FISTA, which are designed to handle nonsmooth regularizers and ensure convergence.", "answer": "$$\\boxed{1.5}$$", "id": "3461182"}, {"introduction": "Having established the theoretical motivation for the composite structure, this practice guides you through the process of building a complete and robust accelerated proximal gradient solver. You will implement the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) from the ground up to solve the canonical LASSO problem, incorporating essential features for practical performance like backtracking line search and adaptive restarts. This comprehensive coding exercise will solidify your understanding of the algorithm's mechanics and its behavior across various data regimes [@problem_id:3461192].", "problem": "You are asked to derive and implement an accelerated proximal gradient method with line search and restart for a composite convex objective that is canonical in compressed sensing and sparse optimization. Consider the problem of minimizing a composite function of the form $F(x) = f(x) + g(x)$, where $f(x)$ is a smooth convex function with Lipschitz-continuous gradient and $g(x)$ is a proper closed convex function for which the proximal operator is efficiently computable. In this assignment, you will focus on the least-squares data fidelity with an $\\ell_{1}$-norm regularizer, namely\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; F(x) \\equiv \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, and $\\lambda \\ge 0$.\n\nStarting from the following fundamental bases:\n- The gradient of $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ is $\\nabla f(x) = A^{\\top} (A x - b)$, and is Lipschitz-continuous with a Lipschitz constant $L_{f}$ equal to the squared spectral norm of $A$, that is $L_{f} = \\|A\\|_{2}^{2}$.\n- The proximal operator of the scaled $\\ell_{1}$-norm $g(x)=\\lambda \\|x\\|_{1}$ at a point $z$ with stepsize $\\alpha>0$ is given by componentwise soft-thresholding, that is $\\operatorname{prox}_{\\alpha g}(z) = \\mathcal{S}_{\\alpha \\lambda}(z)$ where $\\left(\\mathcal{S}_{\\tau}(z)\\right)_{i} = \\operatorname{sign}(z_{i}) \\max\\{|z_{i}| - \\tau, 0\\}$.\n- Nesterov acceleration can be used to extrapolate iterates in the proximal gradient method through a momentum sequence defined by a scalar sequence $\\{t_{k}\\}_{k \\ge 0}$ and an extrapolation step $y_{k}$ built from iterates $x_{k}$.\n- A backtracking line search can enforce the quadratic upper bound for $f(x)$ given a current point $y$, a trial $x$, and a candidate Lipschitz constant $L$: \n$$\nf(x) \\le f(y) + \\langle \\nabla f(y), x - y \\rangle + \\frac{L}{2}\\|x - y\\|_{2}^{2}.\n$$\n- An adaptive restart condition can be used to reset the acceleration when the objective value increases.\n\nYour tasks are:\n- Derive, from the bases above, a complete set of updates for an accelerated proximal gradient method with Nesterov momentum, backtracking line search on the local Lipschitz constant, and an objective-based adaptive restart rule.\n- Implement the derived algorithm as a program that solves the sparse least-squares problem for a provided test suite. Your implementation must compute the gradient, the proximal operator, enforce the backtracking inequality above, update the Nesterov momentum, and apply a restart rule when it improves convergence stability.\n\nProgram input and randomness:\n- The program must be self-contained and must not read any user input. When randomness is needed, use a fixed seed so that results are deterministic.\n\nStopping rule:\n- Use a stopping rule that combines both a relative iterate change threshold and a proximal-gradient mapping condition suitable for composite optimization. Let $\\epsilon$ denote a small tolerance. The iterate change test can be based on $\\|x_{k+1} - x_{k}\\|_{2}/\\max\\{1,\\|x_{k}\\|_{2}\\} \\le \\epsilon$. The proximal-gradient mapping for stepsize $L$ at extrapolated point $y$ is $G_{L}(y) = L\\left(y - \\operatorname{prox}_{g/L}\\left(y - \\nabla f(y)/L\\right)\\right)$. Use the infinity norm $\\|G_{L}(y)\\|_{\\infty} \\le \\epsilon'$ with a choice of $\\epsilon'$ commensurate with $\\epsilon$.\n\nTest suite:\nImplement your method and run it on the following four cases. All matrices and vectors must be generated exactly as specified. For each case, the final result must be a single basic type (a boolean, an integer, or a float). Use the Euclidean norm $\\|\\cdot\\|_{2}$ and the infinity norm $\\|\\cdot\\|_{\\infty}$ as appropriate.\n\n- Case A (compressed sensing “happy path”):\n  - Dimensions: $m = 40$, $n = 120$.\n  - Sparsity: $k = 8$ nonzeros in the ground-truth $x_{\\star}$.\n  - Random generation: set the random seed to $12345$. Generate $A$ with independent standard normal entries and scale columns by $1/\\sqrt{m}$. Generate a $k$-sparse $x_{\\star}$ by choosing $k$ indices uniformly without replacement and assigning independent standard normal values to those entries, with zeros elsewhere. Generate noise $e$ with independent normal entries of variance $\\sigma^{2}$ where $\\sigma = 10^{-3}$, and set $b = A x_{\\star} + e$.\n  - Regularization: $\\lambda = 0.01 \\cdot \\|A^{\\top} b\\|_{\\infty}$.\n  - Output for this case: the relative solution error $\\|x_{\\text{alg}} - x_{\\star}\\|_{2} / \\max\\{1, \\|x_{\\star}\\|_{2}\\}$ as a float.\n\n- Case B (boundary condition $\\lambda = 0$):\n  - Dimensions: $m = 50$, $n = 20$.\n  - Random generation: set the random seed to $54321$. Generate $A$ with independent standard normal entries and scale columns by $1/\\sqrt{m}$. Generate $b$ with independent standard normal entries.\n  - Regularization: $\\lambda = 0$.\n  - Baseline: compute the least-squares solution $x_{\\text{LS}}$ as the minimum-norm solution of $\\min_{x} \\|A x - b\\|_{2}^{2}$.\n  - Output for this case: the relative discrepancy $\\|x_{\\text{alg}} - x_{\\text{LS}}\\|_{2} / \\max\\{1, \\|x_{\\text{LS}}\\|_{2}\\}$ as a float.\n\n- Case C (edge case with very strong regularization):\n  - Dimensions: $m = 30$, $n = 50$.\n  - Random generation: set the random seed to $11111$. Generate $A$ with independent standard normal entries and scale columns by $1/\\sqrt{m}$. Generate $b$ with independent standard normal entries.\n  - Regularization: $\\lambda = 100 \\cdot \\|A^{\\top} b\\|_{\\infty}$.\n  - Output for this case: the integer cardinality of the estimated support, i.e., the number of indices $i$ such that $|x_{\\text{alg}, i}| > 10^{-8}$.\n\n- Case D (rank-deficient design matrix):\n  - Dimensions: $m = 30$, $n = 60$ constructed as follows. Set the random seed to $22222$. First generate $A_{0} \\in \\mathbb{R}^{m \\times 30}$ with independent standard normal entries and scale columns by $1/\\sqrt{m}$. Then set $A = [A_{0} \\;\\; A_{0}]$ by concatenating $A_{0}$ with itself. Generate $b$ with independent standard normal entries.\n  - Regularization: $\\lambda = 0.05 \\cdot \\|A^{\\top} b\\|_{\\infty}$.\n  - Output for this case: the Karush–Kuhn–Tucker (KKT) stationarity residual at $x_{\\text{alg}}$, defined as the infinity norm of a smallest subgradient residual\n    $$\n    r(x) \\equiv \\left\\|\\nabla f(x) + \\lambda v \\right\\|_{\\infty},\n    $$\n    where $v \\in \\partial \\|x\\|_{1}$ is any subgradient. Compute $r(x)$ via the componentwise formula\n    $$\n    \\left(\\nabla f(x)\\right)_{i} = \\left(A^{\\top}(A x - b)\\right)_{i}, \\quad \\text{and} \\quad \n    r(x) = \\max\\left\\{ \\max_{i: x_{i} \\ne 0} \\left| \\left(\\nabla f(x)\\right)_{i} + \\lambda \\operatorname{sign}(x_{i}) \\right|, \\; \\max_{i: x_{i} = 0} \\max\\{0, |\\left(\\nabla f(x)\\right)_{i}| - \\lambda\\} \\right\\}.\n    $$\n    Use the convention that “$x_{i} = 0$” is tested numerically by $|x_{i}| \\le 10^{-12}$. Return $r(x)$ as a float.\n\nImplementation constraints:\n- Use Nesterov acceleration and a restart rule that resets the momentum when the objective increases, i.e., if $F(x_{k+1}) > F(x_{k})$, set the momentum scalar to $t_{k+1} = 1$ and the extrapolated point to $y_{k+1} = x_{k+1}$.\n- Use a backtracking line search that starts from a current estimate of a local Lipschitz constant $L$ and multiplies it by a factor $\\eta > 1$ until the quadratic upper bound condition for $f$ is satisfied.\n- Initialize the stepsize using $L_{0} = \\|A\\|_{2}^{2}$ obtained by a singular-value computation or an equivalent spectral norm evaluation.\n\nNumerical tolerances:\n- Use a maximum of $N_{\\max} = 10000$ iterations and a tolerance $\\epsilon = 10^{-8}$ for iterate change. Choose the proximal-gradient mapping tolerance $\\epsilon'$ to be of the same order as $\\epsilon$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results of the four cases as a comma-separated list enclosed in square brackets, in the order Case A, Case B, Case C, Case D. For example, the output format must be exactly like\n$[r_{A}, r_{B}, r_{C}, r_{D}]$\nwhere $r_{A}$, $r_{B}$, and $r_{D}$ are floats and $r_{C}$ is an integer. No additional text should be printed.", "solution": "The user requires the derivation and implementation of an accelerated proximal gradient method tailored for the sparse least-squares problem, commonly known as LASSO. The algorithm must incorporate Nesterov-style momentum, a backtracking line search for the step size, and an adaptive restart mechanism based on the objective function's behavior.\n\n### 1. Problem Formulation\n\nThe optimization problem is to minimize a composite objective function $F(x)$:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; F(x) \\equiv f(x) + g(x)\n$$\nwhere\n- $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ is the smooth, convex data fidelity term. Its gradient is $\\nabla f(x) = A^{\\top} (A x - b)$, which is Lipschitz-continuous with constant $L_f = \\|A\\|_{2}^{2}$.\n- $g(x) = \\lambda \\|x\\|_{1}$ is the convex, non-smooth regularization term. Its proximal operator, $\\operatorname{prox}_{\\alpha g}(z)$, is the soft-thresholding operator $\\mathcal{S}_{\\alpha\\lambda}(z)$, defined component-wise as $(\\mathcal{S}_{\\tau}(z))_i = \\operatorname{sign}(z_i) \\max\\{|z_i| - \\tau, 0\\}$.\n\n### 2. Algorithm Derivation\n\nThe algorithm is a variant of the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA). We will construct the update rules step-by-step, combining the required components. Let the sequence of iterates be $\\{x_k\\}$.\n\n#### 2.1. Core Proximal Gradient Step\n\nThe basic iterative step for minimizing $F(x)$ is the proximal gradient update. At an iterate $y$, and with a step size $\\alpha > 0$, the next iterate $x^{+}$ is found by minimizing a quadratic approximation of $f(x)$ around $y$ plus the non-smooth term $g(x)$:\n$$\nx^{+} = \\arg\\min_{x} \\left( f(y) + \\langle \\nabla f(y), x - y \\rangle + \\frac{1}{2\\alpha}\\|x - y\\|_{2}^{2} + g(x) \\right)\n$$\nCompleting the square, this is equivalent to:\n$$\nx^{+} = \\arg\\min_{x} \\left( \\frac{1}{2\\alpha}\\|x - (y - \\alpha \\nabla f(y))\\|_{2}^{2} + g(x) \\right)\n$$\nThis is the definition of the proximal operator of $g$ scaled by $\\alpha$:\n$$\nx^{+} = \\operatorname{prox}_{\\alpha g}(y - \\alpha \\nabla f(y))\n$$\nIn our case, with step size $\\alpha = 1/L$ where $L$ is a local Lipschitz estimate, the update is:\n$$\nx^{+} = \\mathcal{S}_{\\lambda/L}(y - \\frac{1}{L}A^{\\top}(Ay-b))\n$$\n\n#### 2.2. Nesterov Acceleration\n\nNesterov's acceleration introduces a \"momentum\" term by computing the proximal gradient step not at the current iterate $x_k$, but at an extrapolated point $y_k$. The update sequence for the iterates $\\{x_k\\}$, extrapolated points $\\{y_k\\}$, and momentum scalars $\\{t_k\\}$ is as follows:\n1.  **Compute next iterate**: $x_{k+1} = \\operatorname{prox}_{g/L_k}(y_k - \\frac{1}{L_k}\\nabla f(y_k))$\n2.  **Update momentum scalar**: $t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2}$\n3.  **Extrapolate for next step**: $y_{k+1} = x_{k+1} + \\frac{t_k-1}{t_{k+1}}(x_{k+1} - x_k)$\n\nThe initial conditions are $x_0 = 0$, $y_0 = x_0$, and $t_0 = 1$. Note that some variants exist; we choose a formulation that aligns well with the restart rule.\n\n#### 2.3. Backtracking Line Search\n\nThe global Lipschitz constant $L_f = \\|A\\|_2^2$ can be a pessimistic over-estimate of the local curvature, leading to slow convergence. A backtracking line search adaptively finds a suitable local constant $L_k$ at each iteration. Starting with an initial guess for $L$ (e.g., the one from the previous step, $L_{k-1}$), we check if the following quadratic upper bound on $f$ is satisfied for the candidate point $x_{k+1}$ computed from $y_k$:\n$$\nf(x_{k+1}) \\le f(y_k) + \\langle \\nabla f(y_k), x_{k+1} - y_k \\rangle + \\frac{L_k}{2}\\|x_{k+1} - y_k\\|_{2}^{2}\n$$\nIf the condition fails, we increase $L_k$ by a factor $\\eta > 1$ (e.g., $\\eta=2$) and re-compute $x_{k+1}$ until the condition is met. This ensures sufficient decrease in the smooth part of the objective.\n\n#### 2.4. Adaptive Restart\n\nNesterov acceleration is not a descent method; the objective function $F(x_k)$ is not guaranteed to decrease monotonically. When the objective value increases (i.e., $F(x_{k+1}) > F(x_k)$), it's a sign that the momentum is too aggressive and is overshooting the minimum. The adaptive restart rule handles this by resetting the momentum. We implement the rule as specified: if $F(x_{k+1}) > F(x_k)$:\n- Reset the momentum scalar for the next step: $t_{k+1} = 1$.\n- Reset the extrapolation point for the next step: $y_{k+1} = x_{k+1}$.\n\n### 3. Complete Algorithm\n\nCombining these components, we arrive at the following algorithm.\n\n**Initialization**:\n- Set $k=0$, $x_0 = 0 \\in \\mathbb{R}^n$, $y_0 = x_0$, $t_0=1$.\n- Set max iterations $N_{\\max}$, tolerances $\\epsilon, \\epsilon'$.\n- Set backtracking factor $\\eta > 1$.\n- Compute initial Lipschitz estimate $L_0 = \\|A\\|_2^2$.\n- Set $F_{-1} = \\infty$.\n\n**Main Loop (for $k = 0, 1, \\dots, N_{\\max}-1$)**:\n1.  Let $x_{\\text{prev}} = x_k$, $t_{\\text{prev}} = t_k$.\n2.  **Backtracking Line Search**:\n    a. Initialize trial $L = L_k / \\eta$.\n    b. Repeatedly update $L \\leftarrow \\eta L$ until the condition\n       $f(x_{k+1}) \\le f(y_k) + \\langle \\nabla f(y_k), x_{k+1} - y_k \\rangle + \\frac{L}{2}\\|x_{k+1} - y_k\\|_{2}^{2}$\n       is satisfied, where $x_{k+1} = \\mathcal{S}_{\\lambda/L}(y_k - \\frac{1}{L}\\nabla f(y_k))$.\n    c. Set $L_{k+1} = L$.\n3.  **Check Stopping Criteria**:\n    a. Relative change: $\\delta_x = \\|x_{k+1} - x_k\\|_{2} / \\max\\{1, \\|x_k\\|_{2}\\}$.\n    b. Proximal-gradient stationarity: $\\|G_L(y_k)\\|_\\infty = \\|L(y_k - x_{k+1})\\|_\\infty$.\n    c. If $\\delta_x \\le \\epsilon$ and $\\|G_L(y_k)\\|_\\infty \\le \\epsilon'$, terminate and return $x_{k+1}$.\n4.  **Update for Next Iteration (with Restart)**:\n    a. Compute objective values $F_k = F(x_k)$ and $F_{k+1} = F(x_{k+1})$.\n    b. **If** $F_{k+1} > F_k$:\n        i.  $t_{k+1} = 1$.\n        ii. $y_{k+1} = x_{k+1}$.\n    c. **Else** (no restart):\n        i.  $t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2}$.\n        ii. $y_{k+1} = x_{k+1} + \\frac{t_k-1}{t_{k+1}}(x_{k+1} - x_k)$.\n5.  Increment $k \\leftarrow k+1$.\n\nThis defines the complete set of updates for the required algorithm.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to derive, implement, and test an accelerated proximal gradient method\n    with line search and restart for sparse least-squares problems.\n    \"\"\"\n\n    def fista_with_restart(A, b, lambda_val, max_iter, tol, tol_prox_grad):\n        \"\"\"\n        Implements the FISTA algorithm with backtracking line search and adaptive restart\n        for the LASSO problem: min 0.5*||Ax-b||^2 + lambda*||x||_1.\n\n        Args:\n            A (np.ndarray): The design matrix.\n            b (np.ndarray): The measurement vector.\n            lambda_val (float): The regularization parameter.\n            max_iter (int): Maximum number of iterations.\n            tol (float): Tolerance for relative iterate change.\n            tol_prox_grad (float): Tolerance for the proximal gradient norm.\n\n        Returns:\n            np.ndarray: The optimized solution vector x.\n        \"\"\"\n        m, n = A.shape\n        eta = 2.0\n\n        def f(x_vec, A_mat, b_vec):\n            return 0.5 * np.linalg.norm(A_mat @ x_vec - b_vec)**2\n\n        def g(x_vec, lam):\n            return lam * np.linalg.norm(x_vec, 1)\n\n        def soft_threshold(z, tau):\n            return np.sign(z) * np.maximum(np.abs(z) - tau, 0)\n\n        # Initialization\n        x_curr = np.zeros(n)\n        y_curr = np.zeros(n)\n        t_curr = 1.0\n        \n        # Initial Lipschitz constant estimate from the spectral norm of A.\n        L = np.linalg.norm(A, 2)**2\n\n        # Storing the objective value of the previous iterate for the restart condition.\n        F_x_prev = f(x_curr, A, b) + g(x_curr, lambda_val)\n\n        for k in range(max_iter):\n            x_prev = x_curr\n            t_prev = t_curr\n\n            # Backtracking line search\n            L_trial = L / eta  # Start with a smaller L for efficiency\n            while True:\n                grad_y = A.T @ (A @ y_curr - b)\n                z = y_curr - grad_y / L_trial\n                x_curr = soft_threshold(z, lambda_val / L_trial)\n                \n                f_x = f(x_curr, A, b)\n                f_y = f(y_curr, A, b)\n                \n                quadratic_approx = f_y + np.dot(grad_y, x_curr - y_curr) + (L_trial / 2.0) * np.linalg.norm(x_curr - y_curr)**2\n                \n                if f_x <= quadratic_approx:\n                    L = L_trial\n                    break\n                else:\n                    L_trial *= eta\n            \n            # Stopping conditions\n            rel_change = np.linalg.norm(x_curr - x_prev) / max(1.0, np.linalg.norm(x_prev))\n            prox_grad_norm = L * np.linalg.norm(y_curr - x_curr, np.inf)\n\n            if k > 0 and rel_change <= tol and prox_grad_norm <= tol_prox_grad:\n                break\n            \n            F_x_curr = f_x + g(x_curr, lambda_val)\n            \n            # Restart check and momentum update for the next iteration\n            if k > 0 and F_x_curr > F_x_prev:\n                t_curr = 1.0\n                y_next = x_curr\n            else:\n                t_curr = (1.0 + np.sqrt(1.0 + 4.0 * t_prev**2)) / 2.0\n                y_next = x_curr + ((t_prev - 1.0) / t_curr) * (x_curr - x_prev)\n\n            # Update state for next iteration\n            y_curr = y_next\n            F_x_prev = F_x_curr\n            \n        return x_curr\n\n    # General parameters\n    MAX_ITER = 10000\n    TOL = 1e-8\n    results = []\n    \n    # --- Case A: Compressed sensing \"happy path\" ---\n    m_A, n_A, k_sparse_A = 40, 120, 8\n    rng_A = np.random.default_rng(12345)\n    A_A = rng_A.standard_normal((m_A, n_A)) / np.sqrt(m_A)\n    x_star_A = np.zeros(n_A)\n    support_A = rng_A.choice(n_A, k_sparse_A, replace=False)\n    x_star_A[support_A] = rng_A.standard_normal(k_sparse_A)\n    e_A = rng_A.normal(0, 1e-3, m_A)\n    b_A = A_A @ x_star_A + e_A\n    lambda_A = 0.01 * np.linalg.norm(A_A.T @ b_A, np.inf)\n    \n    x_alg_A = fista_with_restart(A_A, b_A, lambda_A, MAX_ITER, TOL, TOL)\n    rel_err_A = np.linalg.norm(x_alg_A - x_star_A) / max(1.0, np.linalg.norm(x_star_A))\n    results.append(rel_err_A)\n\n    # --- Case B: Boundary condition lambda = 0 ---\n    m_B, n_B = 50, 20\n    rng_B = np.random.default_rng(54321)\n    A_B = rng_B.standard_normal((m_B, n_B)) / np.sqrt(m_B)\n    b_B = rng_B.standard_normal(m_B)\n    lambda_B = 0.0\n    \n    x_alg_B = fista_with_restart(A_B, b_B, lambda_B, MAX_ITER, TOL, TOL)\n    x_ls_B, _, _, _ = np.linalg.lstsq(A_B, b_B, rcond=None)\n    rel_discrepancy_B = np.linalg.norm(x_alg_B - x_ls_B) / max(1.0, np.linalg.norm(x_ls_B))\n    results.append(rel_discrepancy_B)\n\n    # --- Case C: Very strong regularization ---\n    m_C, n_C = 30, 50\n    rng_C = np.random.default_rng(11111)\n    A_C = rng_C.standard_normal((m_C, n_C)) / np.sqrt(m_C)\n    b_C = rng_C.standard_normal(m_C)\n    lambda_C = 100 * np.linalg.norm(A_C.T @ b_C, np.inf)\n    \n    x_alg_C = fista_with_restart(A_C, b_C, lambda_C, MAX_ITER, TOL, TOL)\n    cardinality_C = np.sum(np.abs(x_alg_C) > 1e-8)\n    results.append(cardinality_C)\n\n    # --- Case D: Rank-deficient design matrix ---\n    m_D, n_half_D = 30, 30\n    rng_D = np.random.default_rng(22222)\n    A0_D = rng_D.standard_normal((m_D, n_half_D)) / np.sqrt(m_D)\n    A_D = np.hstack([A0_D, A0_D])\n    b_D = rng_D.standard_normal(m_D)\n    lambda_D = 0.05 * np.linalg.norm(A_D.T @ b_D, np.inf)\n\n    x_alg_D = fista_with_restart(A_D, b_D, lambda_D, MAX_ITER, TOL, TOL)\n    grad_f_D = A_D.T @ (A_D @ x_alg_D - b_D)\n    \n    tol_kkt_zero = 1e-12\n    is_zero = np.abs(x_alg_D) <= tol_kkt_zero\n    is_nonzero = ~is_zero\n    \n    res_nonzero = np.abs(grad_f_D[is_nonzero] + lambda_D * np.sign(x_alg_D[is_nonzero]))\n    res_zero = np.maximum(0, np.abs(grad_f_D[is_zero]) - lambda_D)\n    \n    max_res_nonzero = np.max(res_nonzero) if res_nonzero.size > 0 else 0.0\n    max_res_zero = np.max(res_zero) if res_zero.size > 0 else 0.0\n    \n    kkt_residual_D = np.max([max_res_nonzero, max_res_zero])\n    results.append(kkt_residual_D)\n\n    print(f\"[{results[0]},{results[1]},{results[2]},{results[3]}]\")\n\nsolve()\n```", "id": "3461192"}, {"introduction": "With a functional FISTA implementation in hand, we now turn to more advanced techniques for enhancing its performance. This final practice explores the design and testing of sophisticated adaptive restart strategies, moving beyond simple objective-based checks. You will learn to use the primal-dual gap as a convergence metric for LASSO and investigate how a specific gradient-based condition can trigger restarts, offering a deeper look into the heuristics that improve the practical speed and stability of accelerated methods [@problem_id:3461157].", "problem": "Consider the composite convex optimization problem known as the Least Absolute Shrinkage and Selection Operator (LASSO): minimize the function $F(x) \\triangleq f(x) + \\lambda \\lVert x \\rVert_{1}$ where $f(x) \\triangleq \\tfrac{1}{2} \\lVert A x - b \\rVert_{2}^{2}$, $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, and $\\lambda \\in \\mathbb{R}_{+}$. The gradient of $f$ is $\\nabla f(x) = A^{\\top} (A x - b)$, which is Lipschitz continuous with a constant $L \\geq \\lVert A \\rVert_{2}^{2}$. The Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) is an accelerated proximal gradient method using Nesterov acceleration for nonsmooth optimization of such composite objectives. A restart mechanism is often employed to recover a fast convergence regime when the acceleration becomes detrimental.\n\nYou will design and test an adaptive restart strategy for FISTA based on the primal-dual gap for LASSO, and evaluate whether additional restarts triggered by a gradient inner product condition correlate with faster support recovery under the Restricted Isometry Property (RIP). Use purely mathematical and algorithmic constructs; no physical units are involved.\n\nDefinitions to be used:\n- Least Absolute Shrinkage and Selection Operator (LASSO): minimize $F(x) = \\tfrac{1}{2} \\lVert A x - b \\rVert_{2}^{2} + \\lambda \\lVert x \\rVert_{1}$.\n- Fast Iterative Shrinkage-Thresholding Algorithm (FISTA): an accelerated proximal gradient method for composite convex minimization, using the proximal operator of the $\\ell_{1}$-norm (soft-thresholding) with step size $1/L$.\n- Restricted Isometry Property (RIP): a structural property of a sensing matrix $A$ such that, for sparse vectors $x$, the action of $A$ approximately preserves the $\\ell_{2}$-norm. You will not compute RIP constants; instead, you will generate matrices with properties commonly associated with approximate RIP (e.g., Gaussian with column normalization), and one case with increased mutual coherence to approximate a departure from RIP.\n- Primal-dual gap for LASSO: treat $f(x) = g(Ax)$ with $g(z) = \\tfrac{1}{2} \\lVert z - b \\rVert_{2}^{2}$ and $h(x) = \\lambda \\lVert x \\rVert_{1}$. The convex conjugates satisfy $g^{\\ast}(y) = \\tfrac{1}{2} \\lVert y \\rVert_{2}^{2} + b^{\\top} y$ and $h^{\\ast}(s) = \\iota_{\\lVert s \\rVert_{\\infty} \\leq \\lambda}(s)$, where $\\iota_{C}$ is the indicator of a convex set $C$. A dual feasible point $y$ must satisfy $\\lVert A^{\\top} y \\rVert_{\\infty} \\leq \\lambda$. Given a primal iterate $x$, one can construct a dual feasible point $y(x)$ by scaling the residual $r(x) \\triangleq A x - b$ as $y(x) \\triangleq \\theta(x) \\, r(x)$, where $\\theta(x) \\triangleq \\min\\!\\left\\{ 1, \\, \\dfrac{\\lambda}{\\lVert A^{\\top} r(x) \\rVert_{\\infty}} \\right\\}$. The primal-dual gap is then $G(x) \\triangleq \\left(\\tfrac{1}{2} \\lVert A x - b \\rVert_{2}^{2} + \\lambda \\lVert x \\rVert_{1}\\right) - \\left(-\\tfrac{1}{2} \\lVert y(x) \\rVert_{2}^{2} - b^{\\top} y(x)\\right)$, which is nonnegative.\n\nYour tasks:\n- Implement FISTA with two restart triggers:\n  - Gap-based adaptive restart: if the primal-dual gap $G(x_{k})$ strictly increases relative to $G(x_{k-1})$ (allowing a negligible numerical tolerance), reset the momentum by setting the acceleration parameter to its initial value and the extrapolated point to the current iterate.\n  - Gradient inner product restart: if the Nesterov acceleration induces misalignment as indicated by the condition $\\langle x_{k} - x_{k-1}, \\nabla f(x_{k}) - \\nabla f(x_{k-1}) \\rangle > 0$, reset the momentum as above.\n- Implement two configurations to solve each instance:\n  - Configuration A: only the gap-based adaptive restart is active.\n  - Configuration B: both the gap-based and the gradient inner product restarts are active simultaneously.\n- Define support recovery as recovering the exact support of the ground-truth sparse vector $x^{\\star}$, i.e., the index set equality $\\operatorname{supp}(x_{k}) = \\operatorname{supp}(x^{\\star})$, where $\\operatorname{supp}(x) \\triangleq \\{ i : |x_{i}| > \\tau \\}$ for a small threshold $\\tau$. Measure the number of iterations until the first exact support recovery. If exact recovery is not achieved within a maximum number of iterations $N_{\\max}$, report $N_{\\max} + 1$.\n\nAlgorithmic foundations to start from:\n- Proximal gradient step for $x_{k+1}$ from $y_{k}$ using step size $1/L$: $x_{k+1} = \\operatorname{soft}(y_{k} - \\tfrac{1}{L} \\nabla f(y_{k}), \\tfrac{\\lambda}{L})$, where $\\operatorname{soft}(v, \\tau)$ applies entrywise soft-thresholding $\\operatorname{soft}(v_{i}, \\tau) \\triangleq \\operatorname{sign}(v_{i}) \\max\\{ |v_{i}| - \\tau, 0 \\}$.\n- Nesterov extrapolation update with standard FISTA parameterization: $t_{k+1} = \\tfrac{1 + \\sqrt{1 + 4 t_{k}^{2}}}{2}$ and $y_{k+1} = x_{k+1} + \\tfrac{t_{k} - 1}{t_{k+1}} (x_{k+1} - x_{k})$ unless a restart is triggered, in which case set $t_{k+1} = 1$ and $y_{k+1} = x_{k+1}$.\n- Lipschitz constant selection: use $L = \\lVert A \\rVert_{2}^{2}$, where $\\lVert A \\rVert_{2}$ is the spectral norm.\n\nTest suite:\nImplement the following three deterministic test instances. For each, generate the sensing matrix $A$, sparse ground-truth $x^{\\star}$, and noisy observations $b = A x^{\\star} + e$ with a specified signal-to-noise ratio in decibels computed via $20 \\log_{10} \\left( \\dfrac{\\lVert A x^{\\star} \\rVert_{2}}{\\lVert e \\rVert_{2}} \\right)$. Normalize the columns of $A$ to unit $\\ell_{2}$-norm in all cases. Define $\\lambda = \\alpha \\, \\lVert A^{\\top} b \\rVert_{\\infty}$ with the specified $\\alpha \\in (0,1)$.\n\n- Case $1$ (approximate RIP, moderate difficulty):\n  - Dimensions: $m = 80$, $n = 200$.\n  - Sparsity: $s = 10$.\n  - Noise: signal-to-noise ratio $40$ dB.\n  - Regularization: $\\alpha = 0.10$.\n  - Random seed: $0$.\n  - Matrix model: independent Gaussian entries with variance $1/m$, then column normalization.\n\n- Case $2$ (increased mutual coherence, harder, approximate departure from RIP):\n  - Dimensions: $m = 60$, $n = 200$.\n  - Sparsity: $s = 20$.\n  - Noise: signal-to-noise ratio $30$ dB.\n  - Regularization: $\\alpha = 0.05$.\n  - Random seed: $1$.\n  - Matrix model: start with independent Gaussian entries with variance $1/m$, then for columns $j \\geq 1$, set $A_{\\cdot j} \\leftarrow \\operatorname{normalize}(A_{\\cdot j} + 0.5 \\, c)$ where $c$ is the first column before correlation, then renormalize all columns.\n\n- Case $3$ (taller system, easier, strong approximate RIP):\n  - Dimensions: $m = 120$, $n = 300$.\n  - Sparsity: $s = 20$.\n  - Noise: signal-to-noise ratio $50$ dB.\n  - Regularization: $\\alpha = 0.10$.\n  - Random seed: $2$.\n  - Matrix model: independent Gaussian entries with variance $1/m$, then column normalization.\n\nExecution details:\n- Use support threshold $\\tau = 10^{-6}$ and maximum iterations $N_{\\max} = 2000$.\n- Initialize $x_{0}$ to the zero vector and $t_{0} = 1$.\n- Use a numerical tolerance of $\\varepsilon = 10^{-12}$ when comparing consecutive primal-dual gaps to decide an increase.\n- For each test case, run Configuration A and Configuration B, and record the iteration counts until exact support recovery, denoted $K_{A}$ and $K_{B}$, respectively.\n- For each test case, output the boolean value $\\mathrm{True}$ if $K_{B} \\leq K_{A}$ and $\\mathrm{False}$ otherwise.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, e.g., $[\\mathrm{True},\\mathrm{False},\\mathrm{True}]$ corresponding to Cases $1$, $2$, and $3$, respectively.", "solution": "The user requires the implementation and evaluation of an adaptive restart strategy for the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) applied to the LASSO problem. The solution will involve generating test cases, implementing FISTA with two distinct restart configurations, and comparing their performance in terms of support recovery speed.\n\n### 1. Problem Formulation and Preliminaries\n\nThe optimization problem is the LASSO:\n$$ \\min_{x \\in \\mathbb{R}^n} F(x) \\triangleq \\underbrace{\\frac{1}{2} \\lVert Ax - b \\rVert_2^2}_{f(x)} + \\underbrace{\\lambda \\lVert x \\rVert_1}_{h(x)} $$\nwhere $f(x)$ is a smooth convex function and $h(x)$ is a nonsmooth convex function. The gradient of $f(x)$ is $\\nabla f(x) = A^\\top(Ax-b)$, which is Lipschitz continuous with constant $L$. We will use $L = \\lVert A \\rVert_2^2$, the squared spectral norm of $A$.\n\nThe FISTA algorithm solves this by generating a sequence of iterates $(x_k, y_k, t_k)$ starting from $x_0 = \\mathbf{0}$, $y_0 = x_0$, and $t_0 = 1$. The update rules for iteration $k \\geq 0$ are:\n1.  **Proximal Gradient Step**: Compute the next iterate $x_{k+1}$ from the extrapolated point $y_k$.\n    $$ x_{k+1} = \\operatorname{prox}_{h/L}(y_k - \\frac{1}{L}\\nabla f(y_k)) $$\n    For $h(x) = \\lambda \\lVert x \\rVert_1$, the proximal operator is the soft-thresholding function:\n    $$ x_{k+1} = \\operatorname{soft}\\left(y_k - \\frac{1}{L}A^\\top(Ay_k - b), \\frac{\\lambda}{L}\\right) $$\n    where $\\operatorname{soft}(v, \\tau)_i = \\operatorname{sign}(v_i)\\max(|v_i|-\\tau, 0)$.\n\n2.  **Restart Check**: After computing $x_{k+1}$, we check if a restart is warranted. A restart is triggered if either of the active conditions are met. The conditions are checked using the current iterate $x_{k+1}$ and the previous iterate $x_k$. The problem description uses indices $(k, k-1)$ generically to denote consecutive iterates, which we interpret in the context of a standard adaptive FISTA loop.\n    *   **Gap-based condition**: The primal-dual gap increases. $G(x_{k+1}) > G(x_k) + \\varepsilon$.\n    *   **Gradient-based condition**: The change in iterate direction aligns with the change in gradient direction. $\\langle x_{k+1} - x_k, \\nabla f(x_{k+1}) - \\nabla f(x_k) \\rangle > 0$.\n\n3.  **Extrapolation Step**: Based on the restart check, we compute the next momentum parameter $t_{k+1}$ and the next extrapolated point $y_{k+1}$.\n    *   **If Restart**: Reset momentum.\n        $$ t_{k+1} = 1 $$\n        $$ y_{k+1} = x_{k+1} $$\n    *   **If No Restart**: Apply Nesterov acceleration.\n        $$ t_{k+1} = \\frac{1 + \\sqrt{1 + 4 t_k^2}}{2} $$\n        $$ y_{k+1} = x_{k+1} + \\frac{t_k - 1}{t_{k+1}}(x_{k+1} - x_k) $$\n\n### 2. Primal-Dual Gap Calculation\n\nThe primal-dual gap provides a certificate of suboptimality. For a primal iterate $x$, we construct a dual-feasible variable $y(x)$:\n$$ y(x) = \\theta(x) (Ax - b) \\quad \\text{with} \\quad \\theta(x) = \\min\\left\\{1, \\frac{\\lambda}{\\lVert A^\\top(Ax-b) \\rVert_\\infty}\\right\\} $$\nThis construction ensures that the dual variable associated with $y(x)$, which is $s(x) = -A^\\top y(x)$, satisfies the dual feasibility condition $\\lVert s(x) \\rVert_\\infty \\le \\lambda$.\nThe primal objective is $P(x) = \\frac{1}{2}\\lVert Ax-b \\rVert_2^2 + \\lambda\\lVert x \\rVert_1$.\nThe dual objective, evaluated at $y(x)$, is $D(y(x)) = -\\frac{1}{2}\\lVert y(x) \\rVert_2^2 - b^\\top y(x)$.\nThe gap is the non-negative quantity $G(x) = P(x) - D(y(x))$.\n\n### 3. Test Data Generation\n\nFor each test case, we construct the matrix $A$, the sparse ground-truth vector $x^\\star$, and the observation vector $b$ according to the specifications.\n1.  A random number generator is seeded for reproducibility.\n2.  The matrix $A \\in \\mathbb{R}^{m \\times n}$ is generated based on the specified model (Gaussian or correlated Gaussian). For the correlated case (Case 2), we first generate a base Gaussian matrix $A_{\\text{base}}$. The first column of $A_{\\text{base}}$ becomes the correlating vector $c$. For each other column $j$, we add $0.5c$ to the $j$-th column of $A_{\\text{base}}$. Finally, all columns of the resulting matrix are normalized to have unit $\\ell_2$-norm. For the standard Gaussian cases, we generate an i.i.d. Gaussian matrix and then normalize its columns.\n3.  The ground-truth sparse vector $x^\\star \\in \\mathbb{R}^n$ is created with $s$ non-zero entries at randomly chosen locations. The non-zero values are drawn from a standard normal distribution.\n4.  The observation vector $b$ is computed as $b = Ax^\\star + e$. The noise vector $e$ is a Gaussian vector whose norm $\\lVert e \\rVert_2$ is scaled to achieve the desired Signal-to-Noise Ratio (SNR) in dB, defined as $\\text{SNR} = 20 \\log_{10}(\\lVert Ax^\\star \\rVert_2 / \\lVert e \\rVert_2)$. Thus, $\\lVert e \\rVert_2 = \\lVert Ax^\\star \\rVert_2 \\cdot 10^{-\\text{SNR}/20}$.\n5.  The regularization parameter $\\lambda$ is set relative to the data via $\\lambda = \\alpha \\lVert A^\\top b \\rVert_\\infty$. This is a common heuristic to ensure $\\lambda$ is in a meaningful range.\n\n### 4. Algorithm Implementation and Evaluation\n\nThe core of the solution is a function that implements the FISTA algorithm with configurable restarts. This function takes the problem data $(A, b, \\lambda, L)$ and the configuration parameters (which restart conditions to use). It iterates until the support of the current iterate $x_k$ matches the true support of $x^\\star$ or until the maximum number of iterations $N_{\\max}$ is reached. The support of an iterate is defined as $\\operatorname{supp}(x_k) = \\{ i : |x_{k,i}| > \\tau \\}$ with $\\tau = 10^{-6}$.\n\nThe main script will execute the following for each of the three test cases:\n1.  Generate the data $(A, b, x^\\star, \\lambda, L)$.\n2.  Run FISTA with Configuration A (gap-based restart only) and record the number of iterations for support recovery, $K_A$.\n3.  Run FISTA with Configuration B (both gap-based and gradient-based restarts) and record the iteration count, $K_B$.\n4.  Compare the results and determine if $K_B \\leq K_A$.\n\nThe final output will be a list of boolean values corresponding to the outcome of this comparison for each test case.", "answer": "```python\nimport numpy as np\nimport scipy  # Permitted, but not strictly necessary for this implementation.\n\ndef solve():\n    \"\"\"\n    Main function to run the complete test suite for FISTA with adaptive restarts.\n    \"\"\"\n    test_cases = [\n        # Case 1: Approximate RIP, moderate difficulty\n        {\n            \"m\": 80, \"n\": 200, \"s\": 10, \"snr_db\": 40, \"alpha\": 0.10, \"seed\": 0,\n            \"matrix_model\": \"gaussian\",\n        },\n        # Case 2: Increased mutual coherence, harder\n        {\n            \"m\": 60, \"n\": 200, \"s\": 20, \"snr_db\": 30, \"alpha\": 0.05, \"seed\": 1,\n            \"matrix_model\": \"correlated_gaussian\",\n        },\n        # Case 3: Taller system, easier\n        {\n            \"m\": 120, \"n\": 300, \"s\": 20, \"snr_db\": 50, \"alpha\": 0.10, \"seed\": 2,\n            \"matrix_model\": \"gaussian\"\n        },\n    ]\n\n    results = []\n    for case_params in test_cases:\n        data = generate_case_data(**case_params)\n\n        # Config A: Gap restart only\n        k_a = run_fista(data, use_gap_restart=True, use_grad_restart=False)\n\n        # Config B: Both restarts active\n        k_b = run_fista(data, use_gap_restart=True, use_grad_restart=True)\n\n        results.append(k_b <= k_a)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef generate_case_data(m, n, s, snr_db, alpha, seed, matrix_model):\n    \"\"\"\n    Generates a deterministic test case for the LASSO problem.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Generate sensing matrix A\n    if matrix_model == \"gaussian\":\n        A = rng.normal(0, 1.0 / np.sqrt(m), size=(m, n))\n    elif matrix_model == \"correlated_gaussian\":\n        A_base = rng.normal(0, 1.0 / np.sqrt(m), size=(m, n))\n        c = A_base[:, 0].copy()\n        A = A_base.copy()\n        for j in range(1, n):\n            A[:, j] += 0.5 * c\n    else:\n        raise ValueError(\"Unknown matrix model\")\n    \n    A /= np.linalg.norm(A, axis=0)\n\n    # Generate sparse ground-truth vector x_star\n    x_star = np.zeros(n)\n    support_indices = rng.choice(n, s, replace=False)\n    x_star[support_indices] = rng.standard_normal(s)\n    x_star_support = set(support_indices)\n\n    # Generate observation vector b\n    Ax_star = A @ x_star\n    signal_norm = np.linalg.norm(Ax_star)\n    noise_norm = signal_norm / (10**(snr_db / 20.0))\n\n    e = rng.standard_normal(m)\n    e *= noise_norm / np.linalg.norm(e)\n    b = Ax_star + e\n\n    # Calculate regularization parameter lambda\n    lambda_ = alpha * np.linalg.norm(A.T @ b, ord=np.inf)\n\n    # Calculate Lipschitz constant L\n    L = np.linalg.norm(A, 2)**2\n\n    return {\n        \"A\": A, \"b\": b, \"lambda_\": lambda_, \"L\": L,\n        \"x_star_support\": x_star_support\n    }\n\ndef soft_threshold(v, tau):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(v) * np.maximum(np.abs(v) - tau, 0)\n\ndef calculate_primal_dual_gap(A, b, lambda_, x):\n    \"\"\"Calculates the primal-dual gap for a LASSO iterate x.\"\"\"\n    residual = A @ x - b\n    primal_obj = 0.5 * np.dot(residual, residual) + lambda_ * np.linalg.norm(x, 1)\n\n    At_residual = A.T @ residual\n    At_residual_inf_norm = np.linalg.norm(At_residual, ord=np.inf)\n    \n    # Scaling factor for dual feasibility\n    theta = 1.0\n    if At_residual_inf_norm > lambda_:\n        theta = lambda_ / At_residual_inf_norm\n    \n    dual_var = theta * residual\n    dual_obj = -0.5 * np.dot(dual_var, dual_var) - np.dot(b, dual_var)\n    \n    return primal_obj - dual_obj\n\n\ndef run_fista(data, use_gap_restart, use_grad_restart):\n    \"\"\"\n    Runs FISTA with specified restart configuration.\n    \"\"\"\n    A, b, lambda_, L = data[\"A\"], data[\"b\"], data[\"lambda_\"], data[\"L\"]\n    x_star_support = data[\"x_star_support\"]\n    m, n = A.shape\n\n    # Algorithm parameters\n    N_MAX = 2000\n    SUPPORT_THRESH = 1e-6\n    GAP_TOL = 1e-12\n\n    # FISTA state variables\n    x_curr = np.zeros(n)\n    y_curr = x_curr.copy()\n    t_curr = 1.0\n    \n    # Variables for restart conditions\n    grad_fx_curr = A.T @ (A @ x_curr - b)\n    gap_curr = calculate_primal_dual_gap(A, b, lambda_, x_curr)\n    \n    for k in range(N_MAX):\n        # --- FISTA step to get x_{k+1} from y_k ---\n        grad_fy_curr = A.T @ (A @ y_curr - b)\n        x_next = soft_threshold(y_curr - grad_fy_curr / L, lambda_ / L)\n\n        # --- Check for exact support recovery ---\n        current_support = {i for i, val in enumerate(x_next) if abs(val) > SUPPORT_THRESH}\n        if current_support == x_star_support:\n            return k + 1\n\n        # --- Check restart conditions on x_{k+1} vs x_k ---\n        restart = False\n        grad_fx_next = A.T @ (A @ x_next - b)\n\n        # Gap-based restart\n        if use_gap_restart:\n            gap_next = calculate_primal_dual_gap(A, b, lambda_, x_next)\n            if gap_next > gap_curr + GAP_TOL:\n                restart = True\n        \n        # Gradient-based restart\n        # Condition: <x_{k+1}-x_k, grad f(x_{k+1})-grad f(x_k)> > 0\n        if not restart and use_grad_restart:\n            grad_diff = grad_fx_next - grad_fx_curr\n            x_diff = x_next - x_curr\n            if np.dot(x_diff, grad_diff) > 0:\n                restart = True\n        \n        # --- Update momentum parameter t and extrapolated point y ---\n        if restart:\n            t_next = 1.0\n            y_next = x_next.copy()\n        else:\n            t_next = (1.0 + np.sqrt(1.0 + 4.0 * t_curr**2)) / 2.0\n            y_next = x_next + ((t_curr - 1.0) / t_next) * (x_next - x_curr)\n\n        # --- Update state for next iteration ---\n        x_curr = x_next\n        t_curr = t_next\n        y_curr = y_next\n        grad_fx_curr = grad_fx_next\n        if use_gap_restart and 'gap_next' in locals():\n            gap_curr = gap_next\n        \n    return N_MAX + 1\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3461157"}]}