{"hands_on_practices": [{"introduction": "Before we can trust an algorithm, we must understand its goal. For the LASSO, the goal is to find a point that satisfies the Karush-Kuhn-Tucker (KKT) conditions, which generalize the concept of setting the gradient to zero for non-differentiable functions. This first exercise provides a direct, numerical check of these optimality conditions for a candidate solution, connecting the abstract theory to the concrete steps of the coordinate descent algorithm [@problem_id:3442175]. By manually computing the subgradient and identifying violations, you will gain a tangible understanding of what it means for a LASSO solution to be optimal.", "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) problem, which seeks to minimize the objective\n$$\nF(\\beta) \\;=\\; \\frac{1}{2}\\,\\|y - X\\beta\\|_{2}^{2} \\;+\\; \\lambda\\,\\|\\beta\\|_{1},\n$$\nwhere $X \\in \\mathbb{R}^{n \\times p}$ is a design matrix, $y \\in \\mathbb{R}^{n}$ is a response vector, $\\beta \\in \\mathbb{R}^{p}$ is the parameter vector, and $\\lambda  0$ is a regularization parameter. The subgradient of the $\\ell_{1}$ norm is defined componentwise by $s_{j} \\in \\partial |\\beta_{j}|$, where $s_{j} = \\operatorname{sign}(\\beta_{j})$ if $\\beta_{j} \\neq 0$ and $s_{j} \\in [-1,1]$ if $\\beta_{j} = 0$. The Karush–Kuhn–Tucker (KKT) conditions for convex composite optimization require stationarity of the sum of the gradient of the smooth term and a subgradient of the nonsmooth term, together with subgradient feasibility.\n\nWork from these core definitions to verify the KKT stationarity condition numerically for a small synthetic instance. Let\n$$\nX \\;=\\; \\begin{pmatrix}\n1  0  2 \\\\\n0  1  -1 \\\\\n1  1  0\n\\end{pmatrix}, \n\\qquad\ny \\;=\\; \\begin{pmatrix}\n3 \\\\ 0 \\\\ 1\n\\end{pmatrix},\n\\qquad\n\\lambda \\;=\\; 1,\n\\qquad\n\\hat{\\beta} \\;=\\; \\begin{pmatrix}\n1 \\\\ 0 \\\\ \\tfrac{1}{2}\n\\end{pmatrix}.\n$$\nUsing only the fundamental definitions above, do the following. First, derive the KKT stationarity condition specialized to this LASSO: express it in terms of the gradient of the quadratic loss and a subgradient of the $\\ell_{1}$ norm evaluated at $\\hat{\\beta}$. Second, compute the residual vector $r = X\\hat{\\beta} - y$, the gradient $g = X^{\\top} r$, and a subgradient vector $s \\in \\partial \\|\\hat{\\beta}\\|_{1}$ that minimizes the KKT stationarity residual componentwise subject to the subgradient feasibility constraints. Third, verify numerically whether the KKT stationarity condition holds at $\\hat{\\beta}$, and explain any violation in terms of a missing coordinate minimization one would perform in cyclic coordinate descent. In particular, relate any violating coordinate to its one-step coordinate update derived from first principles.\n\nFinally, let $t = g + \\lambda s$ denote the KKT stationarity residual vector at $\\hat{\\beta}$ for your optimally chosen $s$. Report the value of the infinity norm $\\|t\\|_{\\infty}$ as your final answer. Provide your final answer as an exact fraction (no rounding).", "solution": "The LASSO objective function is given by $F(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda\\|\\beta\\|_{1}$. The Karush–Kuhn–Tucker (KKT) conditions provide necessary and sufficient conditions for a point $\\hat{\\beta}$ to be a minimizer. The stationarity condition requires that the zero vector is an element of the subdifferential of $F(\\beta)$ at $\\hat{\\beta}$. The objective function is a sum of a differentiable term, $L(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2}$, and a non-differentiable convex term, $R(\\beta) = \\lambda\\|\\beta\\|_{1}$. The subdifferential of the sum is the sum of the gradient of the differentiable part and the subdifferential of the non-differentiable part.\n\nFirst, we derive the general KKT stationarity condition. The gradient of the least-squares loss term $L(\\beta)$ is $\\nabla L(\\beta) = X^{\\top}(X\\beta - y)$. The subdifferential of the $\\ell_{1}$-norm term $R(\\beta)$ is $\\partial R(\\beta) = \\lambda \\, \\partial \\|\\beta\\|_{1}$. The stationarity condition is $0 \\in \\nabla L(\\hat{\\beta}) + \\partial R(\\hat{\\beta})$, which can be written as:\n$$\n-\\nabla L(\\hat{\\beta}) \\in \\partial R(\\hat{\\beta})\n$$\nor equivalently, there must exist a subgradient vector $s \\in \\partial \\|\\hat{\\beta}\\|_{1}$ such that\n$$\n\\nabla L(\\hat{\\beta}) + \\lambda s = 0.\n$$\nComponent-wise, for each $j \\in \\{1, \\dots, p\\}$, we must have $(\\nabla L(\\hat{\\beta}))_j + \\lambda s_j = 0$, where $s_j = \\operatorname{sign}(\\hat{\\beta}_j)$ if $\\hat{\\beta}_j \\neq 0$, and $s_j \\in [-1, 1]$ if $\\hat{\\beta}_j = 0$.\n\nSecond, we compute the necessary quantities for the given numerical instance.\nThe data are:\n$$\nX = \\begin{pmatrix} 1  0  2 \\\\ 0  1  -1 \\\\ 1  1  0 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix}, \\quad \\lambda = 1, \\quad \\hat{\\beta} = \\begin{pmatrix} 1 \\\\ 0 \\\\ \\tfrac{1}{2} \\end{pmatrix}.\n$$\nWe first compute the predicted response $X\\hat{\\beta}$:\n$$\nX\\hat{\\beta} = \\begin{pmatrix} 1  0  2 \\\\ 0  1  -1 \\\\ 1  1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 0 \\cdot 0 + 2 \\cdot \\frac{1}{2} \\\\ 0 \\cdot 1 + 1 \\cdot 0 + (-1) \\cdot \\frac{1}{2} \\\\ 1 \\cdot 1 + 1 \\cdot 0 + 0 \\cdot \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -\\frac{1}{2} \\\\ 1 \\end{pmatrix}.\n$$\nNext, we compute the residual vector $r = X\\hat{\\beta} - y$:\n$$\nr = \\begin{pmatrix} 2 \\\\ -\\frac{1}{2} \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -\\frac{1}{2} \\\\ 0 \\end{pmatrix}.\n$$\nNow, we compute the gradient of the loss term, $g = \\nabla L(\\hat{\\beta}) = X^{\\top}r$:\n$$\ng = X^{\\top}r = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 2  -1  0 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -\\frac{1}{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1(-1) + 0(-\\frac{1}{2}) + 1(0) \\\\ 0(-1) + 1(-\\frac{1}{2}) + 1(0) \\\\ 2(-1) + (-1)(-\\frac{1}{2}) + 0(0) \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -\\frac{1}{2} \\\\ -\\frac{3}{2} \\end{pmatrix}.\n$$\nThird, we must find a subgradient vector $s \\in \\partial \\|\\hat{\\beta}\\|_{1}$ that minimizes the KKT stationarity residual $t = g + \\lambda s$. The components of $\\hat{\\beta}$ are $\\hat{\\beta}_1 = 1$, $\\hat{\\beta}_2 = 0$, and $\\hat{\\beta}_3 = \\frac{1}{2}$.\nFor $\\hat{\\beta}_1 = 1 \\neq 0$, the subgradient component $s_1$ is uniquely determined as $s_1 = \\operatorname{sign}(1) = 1$.\nFor $\\hat{\\beta}_3 = \\frac{1}{2} \\neq 0$, the subgradient component $s_3$ is uniquely determined as $s_3 = \\operatorname{sign}(\\frac{1}{2}) = 1$.\nFor $\\hat{\\beta}_2 = 0$, the subgradient component $s_2$ can be any value in the interval $[-1, 1]$. To minimize the KKT residual, we choose $s_2$ to make the second component of $t = g + \\lambda s$ as close to zero as possible. We want to minimize $|t_2| = |g_2 + \\lambda s_2| = |-\\frac{1}{2} + 1 \\cdot s_2|$. This is minimized when $s_2 = \\frac{1}{2}$. Since $\\frac{1}{2} \\in [-1, 1]$, this is a valid choice.\nThus, the optimally chosen subgradient vector is $s = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\\\ 1 \\end{pmatrix}$.\n\nWith this $s$, the KKT stationarity residual vector $t$ is:\n$$\nt = g + \\lambda s = \\begin{pmatrix} -1 \\\\ -\\frac{1}{2} \\\\ -\\frac{3}{2} \\end{pmatrix} + 1 \\cdot \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1+1 \\\\ -\\frac{1}{2}+\\frac{1}{2} \\\\ -\\frac{3}{2}+1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ -\\frac{1}{2} \\end{pmatrix}.\n$$\nThe KKT stationarity condition $t=0$ is not satisfied because $t_3 = -\\frac{1}{2} \\neq 0$. Therefore, $\\hat{\\beta}$ is not an optimal solution for the LASSO problem.\n\nThe violation occurs in the third coordinate. This means that if we were to perform cyclic coordinate descent, the value of $\\beta_3$ would be updated from its current value of $\\frac{1}{2}$. To verify this, we find the value of $\\beta_3$ that minimizes the objective function while keeping $\\beta_1=1$ and $\\beta_2=0$ fixed. The objective, viewed as a function of $\\beta_3$, is:\n$$\nF_3(\\beta_3) = \\frac{1}{2}\\left\\| y - X_{\\cdot 1}\\hat{\\beta}_1 - X_{\\cdot 2}\\hat{\\beta}_2 - X_{\\cdot 3}\\beta_3 \\right\\|_2^2 + \\lambda|\\hat{\\beta}_1| + \\lambda|\\hat{\\beta}_2| + \\lambda|\\beta_3|\n$$\nMinimizing this is equivalent to solving the 1D LASSO problem:\n$$\n\\min_{\\beta_3} \\frac{1}{2}\\left\\| (y - X_{\\cdot 1}\\hat{\\beta}_1 - X_{\\cdot 2}\\hat{\\beta}_2) - X_{\\cdot 3}\\beta_3 \\right\\|_2^2 + \\lambda|\\beta_3|\n$$\nThe residualized response is $y' = y - X_{\\cdot 1}(1) - X_{\\cdot 2}(0) = \\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\nThe predictor is $X_{\\cdot 3} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix}$. Its squared norm is $\\|X_{\\cdot 3}\\|_2^2 = 2^2 + (-1)^2 + 0^2 = 5$.\nThe coordinate descent update is given by the soft-thresholding operator:\n$$\n\\beta_3^{\\text{new}} = \\frac{1}{\\|X_{\\cdot 3}\\|_2^2} S_{\\lambda}\\left(X_{\\cdot 3}^{\\top}y'\\right)\n$$\nWe compute $X_{\\cdot 3}^{\\top}y' = \\begin{pmatrix} 2  -1  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\\\ 0 \\end{pmatrix} = 4$.\nWith $\\lambda=1$, the update is:\n$$\n\\beta_3^{\\text{new}} = \\frac{1}{5} S_{1}\\left(4\\right) = \\frac{1}{5} \\operatorname{sign}\\left(4\\right) \\max\\left(\\left|4\\right| - 1, 0\\right) = \\frac{1}{5} \\cdot 3 = \\frac{3}{5}.\n$$\nSince the updated value $\\beta_3^{\\text{new}} = \\frac{3}{5}$ is different from the current value $\\hat{\\beta}_3 = \\frac{1}{2}$, this confirms that the third coordinate is not optimal. The non-zero KKT residual $t_3 = -\\frac{1}{2}$ correctly flags this sub-optimality.\n\nFinally, we report the infinity norm of the KKT stationarity residual vector $t$:\n$$\n\\|t\\|_{\\infty} = \\max(|t_1|, |t_2|, |t_3|) = \\max\\left(|0|, |0|, \\left|-\\frac{1}{2}\\right|\\right) = \\frac{1}{2}.\n$$", "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$", "id": "3442175"}, {"introduction": "While cyclic coordinate descent is guaranteed to converge to a minimum for the LASSO problem, its efficiency can be surprisingly sensitive to the order in which coordinates are updated. This path-dependence arises from the interplay between feature correlations and the greedy, one-coordinate-at-a-time nature of the updates. This practice challenges you to trace the algorithm's path for two different update orders on a simple, illustrative dataset, revealing how a seemingly minor choice can impact the speed of convergence [@problem_id:3442192].", "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) problem with objective\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\;\\; \\frac{1}{2}\\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1,\n$$\nand suppose we use cyclic coordinate descent, defined as exact minimization with respect to one coordinate at a time while holding the others fixed, in a specified cyclic order. The Karush–Kuhn–Tucker (KKT) tolerance is measured by the maximum coordinate-wise KKT residual\n$$\nV(\\beta) \\equiv \\max_{1 \\leq j \\leq p} v_j(\\beta),\n$$\nwhere, writing $r \\equiv y - X\\beta$,\n$$\nv_j(\\beta) \\equiv \n\\begin{cases}\n\\left|x_j^{\\top} r - \\lambda \\,\\mathrm{sign}(\\beta_j)\\right|,  \\text{if } \\beta_j \\neq 0 \\\\[4pt]\n\\max\\!\\big(0, \\, |x_j^{\\top} r| - \\lambda\\big),  \\text{if } \\beta_j = 0\n\\end{cases}\n$$\nand $x_j$ denotes the $j$-th column of $X$. One iteration is defined as a full sweep through all coordinates in the given cyclic order. Start from $\\beta^{(0)} = (0, 0)^{\\top}$.\n\nWork with the specific data\n$$\nX \\;=\\; \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix}, \\qquad y \\;=\\; \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\qquad \\lambda \\;=\\; 1,\n$$\nso that $n = 2$ and $p = 2$. Consider two cyclic orders:\n- Order $\\mathcal{A}$: update coordinate $1$ then coordinate $2$ in each sweep.\n- Order $\\mathcal{B}$: update coordinate $2$ then coordinate $1$ in each sweep.\n\nUsing only the definition of the LASSO objective and the necessary optimality conditions, analyze cyclic coordinate descent under each order. At each inner update, exactly minimize the objective with respect to the chosen coordinate given the current values of the others. Define the stopping criterion as achieving a KKT tolerance at or below the fixed threshold\n$$\n\\tau \\;=\\; 0.2.\n$$\n\nDetermine the number of full iterations (full sweeps) required to reach $V(\\beta) \\leq \\tau$ under each order, and report the difference\n$$\nN_{\\mathcal{B}} - N_{\\mathcal{A}}.\n$$\nProvide your final answer as a single real-valued number. No rounding is required.", "solution": "The problem asks for the difference in the number of iterations required for cyclic coordinate descent to converge for the LASSO problem under two different update orders. The stopping criterion is based on the Karush-Kuhn-Tucker (KKT) tolerance $V(\\beta)$ falling at or below a threshold $\\tau = 0.2$.\n\nThe LASSO objective function is given by:\n$$\nL(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1\n$$\nwhere $\\beta \\in \\mathbb{R}^p$. We are given $p=2$ and the data:\n$$\nX = \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\quad \\lambda = 1\n$$\nThe columns of $X$ are $x_1 = (1, 0)^{\\top}$ and $x_2 = (1, 1)^{\\top}$. The initial estimate is $\\beta^{(0)} = (0, 0)^{\\top}$.\n\nFirst, we derive the general coordinate-wise update rule. To update coordinate $\\beta_j$, we fix all other coordinates $\\beta_k$ for $k \\ne j$. The objective, as a function of $\\beta_j$, is:\n$$\nL(\\beta_j) = \\frac{1}{2} \\|y - \\sum_{k \\ne j} x_k \\beta_k - x_j \\beta_j\\|_2^2 + \\lambda |\\beta_j| + C\n$$\nwhere $C$ is a constant with respect to $\\beta_j$. Let $r^{(j)} = y - \\sum_{k \\ne j} x_k \\beta_k$ be the partial residual. The objective simplifies to minimizing $\\frac{1}{2} \\|r^{(j)} - x_j \\beta_j\\|_2^2 + \\lambda |\\beta_j|$. Expanding the quadratic term and taking the subgradient with respect to $\\beta_j$ gives the optimality condition. The solution is obtained via soft-thresholding:\n$$\n\\beta_j \\leftarrow \\frac{1}{x_j^{\\top}x_j} S_{\\lambda}\\left(x_j^{\\top} r^{(j)}\\right)\n$$\nwhere $S_{\\alpha}(z) = \\mathrm{sign}(z) \\max(|z| - \\alpha, 0)$.\n\nLet's pre-compute some values:\n$x_1^{\\top}x_1 = 1^2 + 0^2 = 1$.\n$x_2^{\\top}x_2 = 1^2 + 1^2 = 2$.\n$x_1^{\\top}y = 1 \\cdot 2 + 0 \\cdot 0 = 2$.\n$x_2^{\\top}y = 1 \\cdot 2 + 1 \\cdot 0 = 2$.\n\nThe KKT tolerance is $V(\\beta) = \\max_j v_j(\\beta)$, where $r = y - X\\beta$ and\n$$\nv_j(\\beta) = \n\\begin{cases}\n\\left|x_j^{\\top} r - \\lambda \\,\\mathrm{sign}(\\beta_j)\\right|,  \\text{if } \\beta_j \\neq 0 \\\\\n\\max\\!\\big(0, \\, |x_j^{\\top} r| - \\lambda\\big),  \\text{if } \\beta_j = 0\n\\end{cases}\n$$\n\n### Analysis of Order $\\mathcal{A}$: Update $\\beta_1$ then $\\beta_2$\n\nWe start with $\\beta^{(0)} = (0, 0)^{\\top}$. An iteration is one full sweep.\n\n**Iteration 1 (Order $\\mathcal{A}$):**\n1.  **Update $\\beta_1$**: We fix $\\beta_2 = 0$. The partial residual is $r^{(1)} = y - x_2 \\beta_2 = y = (2, 0)^{\\top}$.\n    $$\n    \\beta_1 \\leftarrow \\frac{1}{x_1^{\\top}x_1} S_{\\lambda}(x_1^{\\top} y) = \\frac{1}{1} S_1(2) = \\mathrm{sign}(2) \\max(|2|-1, 0) = 1\n    $$\n    The state becomes $(\\beta_1, \\beta_2) = (1, 0)^{\\top}$.\n\n2.  **Update $\\beta_2$**: We fix $\\beta_1 = 1$. The partial residual is $r^{(2)} = y - x_1 \\beta_1 = (2, 0)^{\\top} - (1, 0)^{\\top} \\cdot 1 = (1, 0)^{\\top}$.\n    $$\n    x_2^{\\top} r^{(2)} = (1, 1)^{\\top} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1\n    $$\n    $$\n    \\beta_2 \\leftarrow \\frac{1}{x_2^{\\top}x_2} S_{\\lambda}(x_2^{\\top} r^{(2)}) = \\frac{1}{2} S_1(1) = \\frac{1}{2} \\mathrm{sign}(1) \\max(|1|-1, 0) = 0\n    $$\n    The state remains $(1, 0)^{\\top}$.\n\nEnd of Iteration 1: $\\beta^{(1)} = (1, 0)^{\\top}$.\n\n**Check stopping criterion for $\\beta^{(1)}$:**\nThe full residual is $r = y - X\\beta^{(1)} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n-   For $j=1$: $\\beta_1 = 1 \\neq 0$.\n    $x_1^{\\top}r = (1, 0)^{\\top} (1, 0)^{\\top} = 1$.\n    $v_1(\\beta^{(1)}) = |x_1^{\\top}r - \\lambda \\mathrm{sign}(\\beta_1)| = |1 - 1 \\cdot 1| = 0$.\n-   For $j=2$: $\\beta_2 = 0$.\n    $x_2^{\\top}r = (1, 1)^{\\top} (1, 0)^{\\top} = 1$.\n    $v_2(\\beta^{(1)}) = \\max(0, |x_2^{\\top}r| - \\lambda) = \\max(0, |1| - 1) = 0$.\n\nThe KKT tolerance is $V(\\beta^{(1)}) = \\max(0, 0) = 0$. Since $0 \\le \\tau = 0.2$, the algorithm stops.\nThe number of full iterations required for Order $\\mathcal{A}$ is $N_{\\mathcal{A}} = 1$.\n\n### Analysis of Order $\\mathcal{B}$: Update $\\beta_2$ then $\\beta_1$\n\nWe start with $\\beta^{(0)} = (0, 0)^{\\top}$.\n\n**Iteration 1 (Order $\\mathcal{B}$):**\n1.  **Update $\\beta_2$**: We fix $\\beta_1 = 0$. The partial residual is $r^{(2)} = y - x_1 \\beta_1 = y = (2, 0)^{\\top}$.\n    $$\n    \\beta_2 \\leftarrow \\frac{1}{x_2^{\\top}x_2} S_{\\lambda}(x_2^{\\top} y) = \\frac{1}{2} S_1(2) = \\frac{1}{2} \\cdot 1 = \\frac{1}{2}\n    $$\n    The state becomes $(0, 1/2)^{\\top}$.\n2.  **Update $\\beta_1$**: We fix $\\beta_2 = 1/2$. The partial residual is $r^{(1)} = y - x_2 \\beta_2 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\frac{1}{2} = \\begin{pmatrix} 3/2 \\\\ -1/2 \\end{pmatrix}$.\n    $$\n    x_1^{\\top} r^{(1)} = (1, 0)^{\\top} (3/2, -1/2)^{\\top} = 3/2\n    $$\n    $$\n    \\beta_1 \\leftarrow \\frac{1}{x_1^{\\top}x_1} S_{\\lambda}(x_1^{\\top} r^{(1)}) = \\frac{1}{1} S_1(3/2) = \\frac{1}{2}\n    $$\nEnd of Iteration 1: $\\beta^{(1)} = (1/2, 1/2)^{\\top}$.\nCheck KKT: $r = y - X\\beta^{(1)} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1/2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1/2 \\end{pmatrix}$.\n$v_1(\\beta^{(1)}) = |x_1^{\\top}r - \\lambda \\mathrm{sign}(\\beta_1)| = |1 - 1\\cdot 1| = 0$.\n$v_2(\\beta^{(1)}) = |x_2^{\\top}r - \\lambda \\mathrm{sign}(\\beta_2)| = |(1,1)^{\\top}(1, -1/2)^{\\top} - 1| = |1/2-1| = 1/2 = 0.5$.\n$V(\\beta^{(1)}) = 0.5  0.2$. We continue.\n\n**Iteration 2 (Order $\\mathcal{B}$):**\nStart with $\\beta = (1/2, 1/2)^{\\top}$.\n1.  **Update $\\beta_2$**: Fix $\\beta_1=1/2$. $r^{(2)} = y-x_1(1/2) = (3/2, 0)^\\top$.\n    $x_2^\\top r^{(2)} = 3/2$. $\\beta_2 \\leftarrow \\frac{1}{2}S_1(3/2) = \\frac{1}{4}$. State: $(1/2, 1/4)^{\\top}$.\n2.  **Update $\\beta_1$**: Fix $\\beta_2=1/4$. $r^{(1)} = y-x_2(1/4) = (7/4, -1/4)^\\top$.\n    $x_1^\\top r^{(1)} = 7/4$. $\\beta_1 \\leftarrow S_1(7/4) = 3/4$.\nEnd of Iteration 2: $\\beta^{(2)} = (3/4, 1/4)^{\\top}$.\nCheck KKT: $r = y-X\\beta^{(2)} = \\begin{pmatrix} 1 \\\\ -1/4 \\end{pmatrix}$.\n$v_1(\\beta^{(2)}) = |1-1| = 0$.\n$v_2(\\beta^{(2)}) = |x_2^\\top r - 1| = |3/4-1| = 1/4 = 0.25$.\n$V(\\beta^{(2)}) = 0.25  0.2$. We continue.\n\n**Iteration 3 (Order $\\mathcal{B}$):**\nStart with $\\beta = (3/4, 1/4)^{\\top}$.\n1.  **Update $\\beta_2$**: Fix $\\beta_1=3/4$. $r^{(2)} = y-x_1(3/4) = (5/4, 0)^\\top$.\n    $x_2^\\top r^{(2)} = 5/4$. $\\beta_2 \\leftarrow \\frac{1}{2}S_1(5/4) = \\frac{1}{8}$. State: $(3/4, 1/8)^{\\top}$.\n2.  **Update $\\beta_1$**: Fix $\\beta_2=1/8$. $r^{(1)} = y-x_2(1/8) = (15/8, -1/8)^\\top$.\n    $x_1^\\top r^{(1)} = 15/8$. $\\beta_1 \\leftarrow S_1(15/8) = 7/8$.\nEnd of Iteration 3: $\\beta^{(3)} = (7/8, 1/8)^{\\top}$.\nCheck KKT: $r = y-X\\beta^{(3)} = \\begin{pmatrix} 1 \\\\ -1/8 \\end{pmatrix}$.\n$v_1(\\beta^{(3)}) = |1-1| = 0$.\n$v_2(\\beta^{(3)}) = |x_2^\\top r - 1| = |7/8-1| = 1/8 = 0.125$.\n$V(\\beta^{(3)}) = 0.125 \\le 0.2$. The algorithm stops.\nThe number of full iterations required for Order $\\mathcal{B}$ is $N_{\\mathcal{B}} = 3$.\n\n### Final Calculation\n\nThe number of iterations for Order $\\mathcal{A}$ is $N_{\\mathcal{A}} = 1$.\nThe number of iterations for Order $\\mathcal{B}$ is $N_{\\mathcal{B}} = 3$.\nThe requested difference is:\n$$\nN_{\\mathcal{B}} - N_{\\mathcal{A}} = 3 - 1 = 2\n$$", "answer": "$$\\boxed{2}$$", "id": "3442192"}, {"introduction": "A fascinating property of the LASSO objective emerges when features are perfectly correlated: the optimal coefficient vector $\\beta$ becomes non-unique. While the objective value and model predictions remain unique, the individual coefficient values can be distributed in multiple, or even infinite, ways. This coding exercise allows you to investigate how cyclic coordinate descent navigates this ambiguity, demonstrating how its deterministic path leads it to select one specific solution from the set of optimal ones based on the update order, a phenomenon known as algorithmic symmetry-breaking [@problem_id:3111866].", "problem": "You are tasked with implementing a cyclic coordinate descent algorithm for the Least Absolute Shrinkage and Selection Operator (LASSO), which minimizes the convex objective\n$$\n\\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^p} \\; \\frac{1}{2n} \\left\\| \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} \\right\\|_2^2 \\; + \\; \\lambda \\left\\| \\boldsymbol{\\beta} \\right\\|_1,\n$$\nwhere $n$ is the number of samples, $p$ is the number of features, $\\boldsymbol{X} \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\boldsymbol{y} \\in \\mathbb{R}^{n}$ is the response vector, $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$ is the coefficient vector, $\\lambda \\in \\mathbb{R}_{\\ge 0}$ is the regularization parameter, $\\left\\| \\cdot \\right\\|_2$ denotes the Euclidean norm, and $\\left\\| \\cdot \\right\\|_1$ denotes the $\\ell_1$ norm. Starting from fundamental convex optimization principles and subgradient optimality conditions, derive and implement a coordinate-wise update rule that minimizes the objective along one coordinate while holding the others fixed. Your implementation must:\n- Perform cyclic updates in a specified coordinate order over passes through the coordinates.\n- Maintain and update the residual $\\boldsymbol{r} = \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}$ efficiently after each coordinate update.\n- Terminate when the maximum absolute change in coordinates across one full pass is less than a tolerance (use $10^{-12}$) or when a maximum number of passes is reached (use $100$).\n\nThe central phenomenon to study is solution non-uniqueness when two columns of $\\boldsymbol{X}$ are exactly identical. In such cases, the objective depends only on the sum of the duplicated coefficients, and different allocations across the duplicate coordinates can achieve the same objective value. You must empirically demonstrate coordinate descent path dependence and symmetry-breaking: changing the order of coordinate updates can lead to different coefficient allocations across duplicate features despite identical predictions.\n\nImplement a program that runs the coordinate descent algorithm twice per test case with two different coordinate orders: first with order $[0,1]$ and then with order $[1,0]$. Compute and report quantitative metrics that capture differences between the two runs.\n\nUse the following fixed data for all test cases:\n- Let $n = 10$ and define the vector $\\boldsymbol{x} = [1,2,3,4,5,6,7,8,9,10]^\\top$.\n- Construct $\\boldsymbol{X} \\in \\mathbb{R}^{10 \\times 2}$ with two identical columns $\\boldsymbol{X}_{:,0} = \\boldsymbol{X}_{:,1} = \\boldsymbol{x}$.\n- Define three test cases differing by the response vector $\\boldsymbol{y}$ and the regularization parameter $\\lambda$:\n  1. Test Case A (happy path): $\\boldsymbol{y} = 3 \\boldsymbol{x}$ and $\\lambda = 5$.\n  2. Test Case B (sign edge case): $\\boldsymbol{y} = -3 \\boldsymbol{x}$ and $\\lambda = 5$.\n  3. Test Case C (boundary case): $\\boldsymbol{y} = 3 \\boldsymbol{x}$ and $\\lambda = 200$.\n\nInitialization and stopping:\n- Initialize $\\boldsymbol{\\beta}$ to the zero vector for all runs.\n- Use tolerance $10^{-12}$ and maximum passes $100$ as specified.\n\nFor each test case, run the algorithm twice (once with coordinate update order $[0,1]$ and once with $[1,0]$) and compute the following four metrics:\n1. The Euclidean norm of the difference between the two coefficient vectors, i.e., $ \\left\\| \\boldsymbol{\\beta}^{(01)} - \\boldsymbol{\\beta}^{(10)} \\right\\|_2 $, as a floating-point number.\n2. The absolute difference in the sum of the duplicate coefficients, i.e., $ \\left| \\left( \\beta^{(01)}_0 + \\beta^{(01)}_1 \\right) - \\left( \\beta^{(10)}_0 + \\beta^{(10)}_1 \\right) \\right| $, as a floating-point number.\n3. The Euclidean norm of the difference between the two prediction vectors, i.e., $ \\left\\| \\boldsymbol{X}\\boldsymbol{\\beta}^{(01)} - \\boldsymbol{X}\\boldsymbol{\\beta}^{(10)} \\right\\|_2 $, as a floating-point number.\n4. A boolean indicating symmetry-breaking, defined as whether the duplicate coefficients differ in at least one run, i.e., whether $ \\left| \\beta^{(01)}_0 - \\beta^{(01)}_1 \\right|  10^{-9} $ or $ \\left| \\beta^{(10)}_0 - \\beta^{(10)}_1 \\right|  10^{-9} $.\n\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets. Each test case’s result must itself be a list of the four metrics in the order specified above. For example, the output must have the form\n`[[m_{A,1},m_{A,2},m_{A,3},b_A],[m_{B,1},m_{B,2},m_{B,3},b_B],[m_{C,1},m_{C,2},m_{C,3},b_C]]`\nwhere $m_{\\cdot,\\cdot}$ are floating-point numbers and $b_{\\cdot}$ are booleans. No physical units or angles are involved; all quantities are dimensionless real numbers. Ensure scientific realism by adhering to the convexity of the objective and the definition of subgradient-based coordinate-wise minimization without relying on ad hoc heuristics.", "solution": "The user-provided problem is valid. It is a well-posed and scientifically grounded task in computational statistics, specifically focusing on the coordinate descent algorithm for LASSO regression. All necessary parameters and conditions are provided, and the objective is to explore the known phenomenon of path-dependent solutions in the presence of collinear features, which is a standard topic in optimization.\n\n### 1. The LASSO Objective Function\n\nThe problem asks for the minimization of the LASSO objective function, which is composed of a least-squares data fidelity term and an $\\ell_1$-norm regularization term. The objective function $L(\\boldsymbol{\\beta})$ for a coefficient vector $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ is given by:\n$$\nL(\\boldsymbol{\\beta}) = \\frac{1}{2n} \\left\\| \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} \\right\\|_2^2 + \\lambda \\left\\| \\boldsymbol{\\beta} \\right\\|_1\n$$\nHere, $\\boldsymbol{y} \\in \\mathbb{R}^n$ is the response vector, $\\boldsymbol{X} \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $n$ is the number of samples, $p$ is the number of features, and $\\lambda \\ge 0$ is the regularization parameter. The $\\ell_1$ norm is defined as $\\left\\| \\boldsymbol{\\beta} \\right\\|_1 = \\sum_{j=1}^p |\\beta_j|$. This objective function is convex, which guarantees the existence of a global minimum.\n\n### 2. Derivation of the Coordinate-wise Update Rule\n\nCoordinate descent optimizes the objective function with respect to a single coordinate at a time, holding all other coordinates fixed. To derive the update for the $k$-th coefficient, $\\beta_k$, we consider all other coefficients $\\beta_j$ (for $j \\neq k$) to be constant.\n\nThe objective function can be written to isolate the terms involving $\\beta_k$:\n$$\nL(\\boldsymbol{\\beta}) = \\frac{1}{2n} \\left\\| \\boldsymbol{y} - \\sum_{j \\neq k} \\boldsymbol{X}_{:,j}\\beta_j - \\boldsymbol{X}_{:,k}\\beta_k \\right\\|_2^2 + \\lambda \\sum_{j \\neq k} |\\beta_j| + \\lambda |\\beta_k|\n$$\nLet's define the partial residual $\\boldsymbol{r}_k = \\boldsymbol{y} - \\sum_{j \\neq k} \\boldsymbol{X}_{:,j}\\beta_j$. These are the residuals if feature $k$ were removed from the model. The terms not involving $\\beta_k$ are constant with respect to the minimization over $\\beta_k$. The objective to minimize for $\\beta_k$ is thus:\n$$\nL_k(\\beta_k) = \\frac{1}{2n} \\left\\| \\boldsymbol{r}_k - \\boldsymbol{X}_{:,k}\\beta_k \\right\\|_2^2 + \\lambda |\\beta_k|\n$$\nExpanding the quadratic term gives:\n$$\nL_k(\\beta_k) = \\frac{1}{2n} \\left( \\boldsymbol{r}_k^\\top \\boldsymbol{r}_k - 2\\beta_k \\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k + \\beta_k^2 \\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k} \\right) + \\lambda |\\beta_k|\n$$\nTo find the minimum of this convex function, we use the subgradient optimality condition, which states that $0$ must be in the subgradient of $L_k(\\beta_k)$ at the minimizer. The subgradient of the absolute value function $|\\cdot|$ is the sign function `sgn`, which is a set-valued function at $0$.\n$$\n\\frac{\\partial L_k(\\beta_k)}{\\partial \\beta_k} = \\frac{1}{n} \\left( -\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k + \\beta_k \\left(\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k}\\right) \\right) + \\lambda \\cdot \\text{sgn}(\\beta_k)\n$$\nSetting the subgradient to contain $0$:\n$$\n0 \\in \\frac{1}{n} \\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k - \\beta_k \\frac{1}{n} \\left(\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k}\\right) - \\lambda \\cdot \\text{sgn}(\\beta_k)\n$$\nLet's define $\\rho_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k$ and $z_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k}$. The condition becomes:\n$$\n\\beta_k z_k \\in \\rho_k - \\lambda \\cdot \\text{sgn}(\\beta_k)\n$$\nThis leads to the soft-thresholding function $S(a, \\delta) = \\text{sgn}(a)\\max(|a|-\\delta, 0)$. The solution for $\\beta_k$ is:\n$$\n\\beta_k = \\frac{S(\\rho_k, \\lambda)}{z_k}\n$$\nThis update rule minimizes the objective aong the $k$-th coordinate.\n\n### 3. The Coordinate Descent Algorithm and Efficient Updates\n\nThe cyclic coordinate descent algorithm iterates through all coordinates $k=0, 1, \\dots, p-1$ repeatedly until convergence. For computational efficiency, we avoid recomputing the partial residual $\\boldsymbol{r}_k$ at each step. Instead, we maintain the full residual $\\boldsymbol{r} = \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}$. The partial residual can be expressed in terms of the full residual and the current value of $\\beta_k$ before the update:\n$$\n\\boldsymbol{r}_k = \\boldsymbol{y} - \\sum_{j \\neq k} \\boldsymbol{X}_{:,j}\\beta_j = \\left(\\boldsymbol{y} - \\sum_{j=1}^p \\boldsymbol{X}_{:,j}\\beta_j \\right) + \\boldsymbol{X}_{:,k}\\beta_k = \\boldsymbol{r} + \\boldsymbol{X}_{:,k}\\beta_k\n$$\nSo, the term $\\rho_k$ can be calculated as $\\rho_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top(\\boldsymbol{r} + \\boldsymbol{X}_{:,k}\\beta_k)$, where $\\boldsymbol{r}$ and $\\beta_k$ are the values before the update for coordinate $k$.\n\nAfter updating $\\beta_k$ from $\\beta_k^{\\text{old}}$ to $\\beta_k^{\\text{new}}$, the full residual $\\boldsymbol{r}$ must also be updated. The change in the prediction is $\\boldsymbol{X}_{:,k}(\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}})$. The new residual is:\n$$\n\\boldsymbol{r}^{\\text{new}} = \\boldsymbol{r}^{\\text{old}} - \\boldsymbol{X}_{:,k}(\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}})\n$$\nThis is an efficient $O(n)$ update. The algorithm is as follows:\n\n1.  Initialize $\\boldsymbol{\\beta} = \\boldsymbol{0}$ and $\\boldsymbol{r} = \\boldsymbol{y}$. Precompute $z_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top\\boldsymbol{X}_{:,k}$ for all $k$.\n2.  For each pass up to a maximum number of passes:\n    a. Initialize `max_abs_change` to $0$.\n    b. For each coordinate $k$ in a specified order:\n        i.   Store $\\beta_k^{\\text{old}} = \\beta_k$.\n        ii.  Calculate $\\rho_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top(\\boldsymbol{r} + \\boldsymbol{X}_{:,k}\\beta_k^{\\text{old}})$.\n        iii. Calculate $\\beta_k^{\\text{new}} = S(\\rho_k, \\lambda) / z_k$.\n        iv.  Update the residual: $\\boldsymbol{r} \\leftarrow \\boldsymbol{r} - \\boldsymbol{X}_{:,k}(\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}})$.\n        v.   Update the coefficient: $\\beta_k \\leftarrow \\beta_k^{\\text{new}}$.\n        vi.  Update `max_abs_change` with $|\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}}|$.\n    c. If `max_abs_change` is less than a tolerance (e.g., $10^{-12}$), terminate.\n3.  Return the final coefficient vector $\\boldsymbol{\\beta}$.\n\n### 4. Non-uniqueness with Identical Columns\n\nThe problem sets up a scenario where two columns of $\\boldsymbol{X}$ are identical, i.e., $\\boldsymbol{X}_{:,0} = \\boldsymbol{X}_{:,1} = \\boldsymbol{x}$. In this case, the model prediction is:\n$$\n\\boldsymbol{X}\\boldsymbol{\\beta} = \\boldsymbol{X}_{:,0}\\beta_0 + \\boldsymbol{X}_{:,1}\\beta_1 = \\boldsymbol{x}\\beta_0 + \\boldsymbol{x}\\beta_1 = (\\beta_0 + \\beta_1)\\boldsymbol{x}\n$$\nThe objective function's data fidelity term depends only on the sum of the coefficients, $\\beta_{\\text{sum}} = \\beta_0 + \\beta_1$. The $\\ell_1$ penalty term is $\\lambda(|\\beta_0| + |\\beta_1|)$. The optimization problem becomes:\n$$\n\\min_{\\beta_0, \\beta_1} \\; \\frac{1}{2n} \\left\\| \\boldsymbol{y} - (\\beta_0 + \\beta_1)\\boldsymbol{x} \\right\\|_2^2 + \\lambda(|\\beta_0| + |\\beta_1|)\n$$\nWhile the optimal value of the sum $\\beta_{\\text{sum}}$ is unique, there can be infinitely many pairs $(\\beta_0, \\beta_1)$ that produce this sum. For any given optimal $\\beta_{\\text{sum}}$, the $\\ell_1$ penalty term $\\lambda(|\\beta_0| + |\\beta_1|)$ is minimized when the entire magnitude of $\\beta_{\\text{sum}}$ is allocated to a single coefficient (e.g., $\\beta_0 = \\beta_{\\text{sum}}, \\beta_1 = 0$ or vice-versa), assuming $\\beta_{\\text{sum}} \\neq 0$.\n\nThe cyclic coordinate descent algorithm breaks this symmetry based on the update order. When updating coordinate $0$ first, it will absorb as much of the signal as possible, potentially leaving a small residual for coordinate $1$. If the residual signal is below the threshold set by $\\lambda$, the update for $\\beta_1$ will be zero. Reversing the order to $[1, 0]$ will cause $\\beta_1$ to absorb the signal first, resulting in a different final coefficient vector $\\boldsymbol{\\beta}$ even though the sum $\\beta_0+\\beta_1$ and the predictions $\\boldsymbol{X}\\boldsymbol{\\beta}$ will be identical (up to numerical precision). This path dependence is a key characteristic of coordinate descent on non-strictly convex objectives.", "answer": "[[3.733766233766233, 0.0, 0.0, True],[3.733766233766233, 0.0, 0.0, True],[0.0, 0.0, 0.0, False]]", "id": "3111866"}]}