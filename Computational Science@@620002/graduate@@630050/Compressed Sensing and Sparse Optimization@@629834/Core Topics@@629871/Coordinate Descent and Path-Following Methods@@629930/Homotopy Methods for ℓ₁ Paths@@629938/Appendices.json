{"hands_on_practices": [{"introduction": "This practice is about building the complete LASSO solution path from the ground up. By implementing the homotopy algorithm, you will trace how the coefficient vector $x(\\lambda)$ evolves as the regularization parameter $\\lambda$ changes, observing variables enter and exit the active set. This hands-on implementation is the most effective way to gain a deep, practical understanding of the path's structure and the crucial role of the KKT conditions [@problem_id:3451768].", "problem": "You are tasked with constructing and analyzing a homotopy path for the Least Absolute Shrinkage and Selection Operator (LASSO) problem in compressed sensing and sparse optimization. The focus is on explicitly tracking support changes and verifying piecewise linearity of the LASSO solution path with respect to the regularization parameter. The path should be traced by applying the Karush-Kuhn-Tucker (KKT) conditions and the homotopy method (also known as Least Angle Regression with LASSO modification).\n\nConsider the LASSO objective\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^m$, and $\\lambda \\ge 0$ is the regularization parameter. The KKT conditions for optimality state that\n$$\nA^\\top \\left(b - A x(\\lambda)\\right) \\in \\lambda \\, \\partial \\|x(\\lambda)\\|_1,\n$$\nwhich is equivalent to\n$$\n\\begin{cases}\nA_i^\\top \\left(b - A x(\\lambda)\\right) = \\lambda \\, \\operatorname{sign}\\left(x_i(\\lambda)\\right), & \\text{if } x_i(\\lambda) \\ne 0, \\\\\n\\left|A_i^\\top \\left(b - A x(\\lambda)\\right)\\right| \\le \\lambda, & \\text{if } x_i(\\lambda) = 0,\n\\end{cases}\n$$\nfor each index $i \\in \\{1,\\ldots,n\\}$. The homotopy method for $\\ell_1$ paths begins at $\\lambda_{\\max} = \\|A^\\top b\\|_\\infty$ with $x(\\lambda_{\\max}) = 0$ and decreases $\\lambda$, tracking changes in the active support and signs while maintaining the KKT conditions. Between support change events, the solution $x(\\lambda)$ evolves linearly in $\\lambda$.\n\nYour program must:\n\n1. Construct synthetic matrices $A$ and vectors $b$ that induce predictable and nontrivial LASSO paths, including cases with multiple coefficient dropouts (indices leaving the active set), and cases without dropouts.\n\n2. Starting at $\\lambda_{\\max} = \\|A^\\top b\\|_\\infty$, trace the homotopy path by iteratively:\n   - Selecting entries into the active set when a non-active correlation hits $\\pm \\lambda$,\n   - Computing the equiangular descent direction that maintains equicorrelations on the active set,\n   - Detecting and applying dropouts when an active coefficient reaches zero,\n   - Updating $x(\\lambda)$ and $\\lambda$ linearly along the chosen direction between events,\n   - Enforcing the KKT conditions at each breakpoint.\n\n3. Predict and output the exact sequence of support changes as the list of events along the path, coded as integer pairs $[e,i]$ with $e=1$ indicating an entry of index $i$ into the active set and $e=-1$ indicating a dropout of index $i$ from the active set. Indices must be reported using one-based indexing.\n\n4. Verify, for each segment between consecutive support change events:\n   - KKT feasibility at the segment endpoints by checking $A^\\top(b - A x(\\lambda))$ against $\\lambda \\, \\partial \\|x\\|_1$,\n   - Piecewise linearity by confirming that the per-segment slope of $x(\\lambda)$ with respect to $\\lambda$ is constant and matches the computed equiangular direction.\n\nAll computations are purely mathematical and dimensionless; there are no physical units. Angles are not involved.\n\nTest Suite:\nProvide three test cases that cover diverse behaviors:\n- Case 1 (multiple dropouts): $m = 4$, $n = 5$, columns of $A$ defined via combinations of the canonical basis $\\{e_1,e_2,e_3,e_4\\}$ and then normalized to unit $\\ell_2$ norm:\n  $$\n  \\begin{aligned}\n  &a_1 = e_1, \\\\\n  &a_2 = 0.98\\, e_1 + 0.20\\, e_2, \\\\\n  &a_3 = -0.95\\, e_1 + 0.30\\, e_3, \\\\\n  &a_4 = 0.60\\, e_2 + 0.20\\, e_3, \\\\\n  &a_5 = 0.60\\, e_3 + 0.20\\, e_1, \\\\\n  &b = 1.00\\, e_1 + 0.45\\, e_2 - 0.50\\, e_3 + 0.05\\, e_4.\n  \\end{aligned}\n  $$\n  After column normalization of $A$, track the LASSO path. This case is designed to exhibit multiple dropouts due to competing positively and negatively correlated predictors.\n\n- Case 2 (no dropouts, orthonormal design): $m = 5$, $n = 5$, $A = I_5$ and\n  $$\n  b = \\begin{bmatrix} 1.0 \\\\ 0.8 \\\\ 0.6 \\\\ 0.4 \\\\ 0.2 \\end{bmatrix}.\n  $$\n  The path should be soft-thresholding with monotonic entries and no dropouts.\n\n- Case 3 (near-collinear predictors): $m = 6$, $n = 6$, columns constructed from $\\{e_1,\\ldots,e_6\\}$ and normalized:\n  $$\n  \\begin{aligned}\n  &a_1 = e_1, \\\\\n  &a_2 = 0.99\\, e_1 + 0.10\\, e_2, \\\\\n  &a_3 = 0.99\\, e_1 - 0.10\\, e_2, \\\\\n  &a_4 = e_2, \\\\\n  &a_5 = e_3, \\\\\n  &a_6 = 0.50\\, e_3 + 0.40\\, e_4, \\\\\n  &b = 1.00\\, e_1 + 0.70\\, e_2 + 0.25\\, e_3 - 0.20\\, e_4 + 0.10\\, e_6.\n  \\end{aligned}\n  $$\n\nRequired output format:\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets. Each test case result must be a list with the following three components:\n- The list of support change events as integer pairs $[e,i]$ with $e \\in \\{1,-1\\}$ and one-based index $i$.\n- A boolean indicating whether all KKT checks at breakpoints succeeded.\n- A boolean indicating whether all per-segment piecewise linearity checks succeeded.\n\nFor example, the overall output should look like:\n$$\n[\\,[\\,[e_1,i_1],\\ldots,[e_k,i_k]\\, ,\\, \\text{True}\\, ,\\, \\text{True}\\, ]\\, ,\\, \\ldots\\, ].\n$$\nYour program must implement the homotopy tracking and verification from first principles and produce the exact single-line output in the specified format.", "solution": "The user has provided a valid problem statement. The task is to implement the homotopy method for the LASSO optimization problem, trace the solution path as a function of the regularization parameter $\\lambda$, identify support changes (entries and dropouts of coefficients), and verify the correctness of the path against the Karush-Kuhn-Tucker (KKT) conditions and its piecewise linear nature.\n\nThe LASSO problem is defined as:\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1\n$$\nThe solution, $x(\\lambda)$, traces a piecewise linear path as $\\lambda$ varies. The homotopy method, also known as the LARS-LASSO algorithm, provides a way to compute this entire path. We start from a large $\\lambda$ where the solution is $x=0$ and incrementally decrease $\\lambda$, tracking the changes in the active set of non-zero coefficients.\n\nThe algorithm proceeds as follows:\n\n**1. Initialization**\nThe path starts at $\\lambda = \\lambda_{\\max}$ where the solution is trivial, $x(\\lambda_{\\max}) = 0$. At this point, the residual is $r = b - A x = b$. The KKT conditions require that for all $i$, $|A_i^\\top(b-Ax)| \\le \\lambda$. To find the largest $\\lambda$ for which a non-zero solution might first appear, we find the point where this inequality becomes an equality for at least one index. This occurs at:\n$$\n\\lambda_{\\max} = \\|A^\\top b\\|_{\\infty}\n$$\nLet $j_0$ be an index for which $|A_{j_0}^\\top b| = \\lambda_{\\max}$. This index is the first to enter the active set, $\\mathcal{A}$.\n- Initial solution: $x_0 = 0 \\in \\mathbb{R}^n$.\n- Initial $\\lambda$: $\\lambda_0 = \\lambda_{\\max}$.\n- Initial active set: $\\mathcal{A} = \\{j_0\\}$.\n- Initial signs: $s_{\\mathcal{A}} = [\\operatorname{sign}((A^\\top b)_{j_0})]$.\n\n**2. Path Segment Construction**\nBetween two consecutive events (changes in the active set), the solution $x(\\lambda)$ is linear in $\\lambda$. For a given active set $\\mathcal{A}$ and corresponding signs $s_{\\mathcal{A}}$, the active KKT conditions are:\n$$\nA_{\\mathcal{A}}^\\top (b - A x(\\lambda)) = \\lambda s_{\\mathcal{A}}\n$$\nwhere $A_{\\mathcal{A}}$ is the submatrix of $A$ containing columns indexed by $\\mathcal{A}$, and $x_i(\\lambda) = 0$ for $i \\notin \\mathcal{A}$. This simplifies to:\n$$\nA_{\\mathcal{A}}^\\top (b - A_{\\mathcal{A}} x_{\\mathcal{A}}(\\lambda)) = \\lambda s_{\\mathcal{A}}\n$$\nDifferentiating with respect to $\\lambda$, we obtain the direction of the path for the active coefficients:\n$$\n-A_{\\mathcal{A}}^\\top A_{\\mathcal{A}} \\frac{dx_{\\mathcal{A}}}{d\\lambda} = s_{\\mathcal{A}}\n$$\nSolving for the derivative, which represents the slope of the path segment:\n$$\n\\frac{dx_{\\mathcal{A}}}{d\\lambda} = -(A_{\\mathcal{A}}^\\top A_{\\mathcal{A}})^{-1} s_{\\mathcal{A}}\n$$\nLet's define the path update direction as $w_{\\mathcal{A}} = (A_{\\mathcal{A}}^\\top A_{\\mathcal{A}})^{-1} s_{\\mathcal{A}}$. The full direction vector $w \\in \\mathbb{R}^n$ has non-zero entries only for indices in $\\mathcal{A}$. As we decrease $\\lambda$ from a current value $\\lambda_{\\text{curr}}$ by an amount $\\delta_{\\lambda} > 0$, the solution updates as:\n$$\nx_{\\text{new}} = x_{\\text{curr}} + \\delta_{\\lambda} w\n$$\nThe new $\\lambda$ will be $\\lambda_{\\text{new}} = \\lambda_{\\text{curr}} - \\delta_{\\lambda}$.\n\n**3. Event Detection**\nThe linear evolution of the path segment is valid until the active set must change. This is an \"event\". We must find the smallest step $\\delta_{\\lambda} > 0$ that triggers such an event. There are two types of events:\n\n- **Entry Event**: A coefficient $x_j$ for $j \\notin \\mathcal{A}$ becomes non-zero. This happens when its corresponding correlation $c_j = A_j^\\top(b - Ax)$ reaches the boundary $\\pm\\lambda$. The correlation evolves as $c_j(\\lambda_{\\text{new}}) = c_j(\\lambda_{\\text{curr}}) - \\delta_{\\lambda} A_j^\\top A w$. We need to find the smallest $\\delta_{\\lambda} > 0$ that satisfies $|c_j(\\lambda_{\\text{curr}}) - \\delta_{\\lambda} A_j^\\top A w| = \\lambda_{\\text{curr}} - \\delta_{\\lambda}$ for some $j \\notin \\mathcal{A}$. This leads to two linear equations for $\\delta_{\\lambda}$ for each inactive index $j$:\n    $$\n    \\delta_{\\lambda} = \\frac{\\lambda_{\\text{curr}} - c_j(\\lambda_{\\text{curr}})}{1 - A_j^\\top A w} \\quad \\text{and} \\quad \\delta_{\\lambda} = \\frac{\\lambda_{\\text{curr}} + c_j(\\lambda_{\\text{curr}})}{1 + A_j^\\top A w}\n    $$\n    We consider the minimum positive $\\delta_{\\lambda}$ found across all $j \\notin \\mathcal{A}$.\n\n- **Dropout Event**: An active coefficient $x_k(\\lambda)$ for $k \\in \\mathcal{A}$ passes through zero. The LASSO modification to LARS requires that such a coefficient be removed from the active set. This occurs when $x_k(\\lambda_{\\text{new}}) = x_k(\\lambda_{\\text{curr}}) + \\delta_{\\lambda} w_k = 0$. This gives the step size:\n    $$\n    \\delta_{\\lambda} = -x_k(\\lambda_{\\text{curr}}) / w_k\n    $$\n    This is only a candidate for a future event if $\\delta_{\\lambda} > 0$, which occurs when $x_k$ and $w_k$ have opposite signs. We find the minimum positive such $\\delta_{\\lambda}$ over all $k \\in \\mathcal{A}$.\n\nThe next event occurs at $\\delta_{\\lambda} = \\min(\\delta_{\\lambda, \\text{entry}}, \\delta_{\\lambda, \\text{dropout}})$.\n\n**4. Iteration and Verification**\nThe algorithm iteratively computes the direction $w$, finds the smallest step $\\delta_{\\lambda}$, updates the state $(x, \\lambda)$, and modifies the active set $\\mathcal{A}$ and signs $s_{\\mathcal{A}}$ according to the event type. The process continues until $\\lambda$ reaches zero or another termination condition is met.\n\nAt each breakpoint (i.e., after each step), we perform two verifications:\n- **KKT Verification**: We check if the KKT conditions hold at the new point $(x_{\\text{new}}, \\lambda_{\\text{new}})$ for the *new* active set. That is, $|A_i^\\top(b-Ax_{\\text{new}})| = \\lambda_{\\text{new}}$ for active indices $i$ and $|A_j^\\top(b-Ax_{\\text{new}})| \\le \\lambda_{\\text{new}}$ for inactive indices $j$.\n- **Piecewise Linearity (PWL) Verification**: We confirm that the change in $x$ over the segment is consistent with the computed path direction. The empirical slope is $(\\Delta x) / (\\Delta \\lambda) = (x_{\\text{new}} - x_{\\text{curr}}) / (\\lambda_{\\text{new}} - \\lambda_{\\text{curr}})$. This must match the theoretical slope, $dx/d\\lambda = -w$.\n\nThe implementation will handle numerical precision using a small tolerance, $\\epsilon$, for comparisons. The test cases are designed to exercise different aspects of the algorithm, including dropouts (Case 1), a simple monotonic path (Case 2), and near-collinearity (Case 3).", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the LASSO path algorithm.\n    It formats and prints the results as specified.\n    \"\"\"\n    \n    TOL = 1e-9\n\n    def format_result(result_list):\n        \"\"\"Custom formatter to match the output string format [item1,item2,...].\"\"\"\n        events_str = '[' + ','.join([f'[{e},{i}]' for e, i in result_list[0]]) + ']'\n        kkt_str = 'True' if result_list[1] else 'False'\n        pwl_str = 'True' if result_list[2] else 'False'\n        return f\"[{events_str},{kkt_str},{pwl_str}]\"\n    \n    # Case 1: Multiple dropouts\n    m1, n1 = 4, 5\n    A1_unnormalized = np.zeros((m1, n1))\n    e = np.eye(m1)\n    A1_unnormalized[:, 0] = e[:, 0]\n    A1_unnormalized[:, 1] = 0.98 * e[:, 0] + 0.20 * e[:, 1]\n    A1_unnormalized[:, 2] = -0.95 * e[:, 0] + 0.30 * e[:, 2]\n    A1_unnormalized[:, 3] = 0.60 * e[:, 1] + 0.20 * e[:, 2]\n    A1_unnormalized[:, 4] = 0.60 * e[:, 2] + 0.20 * e[:, 0]\n    b1 = 1.00 * e[:, 0] + 0.45 * e[:, 1] - 0.50 * e[:, 2] + 0.05 * e[:, 3]\n    \n    # Case 2: Orthonormal design\n    m2, n2 = 5, 5\n    A2_unnormalized = np.eye(m2)\n    b2 = np.array([1.0, 0.8, 0.6, 0.4, 0.2])\n\n    # Case 3: Near-collinear predictors\n    m3, n3 = 6, 6\n    A3_unnormalized = np.zeros((m3, n3))\n    e = np.eye(m3)\n    A3_unnormalized[:, 0] = e[:, 0]\n    A3_unnormalized[:, 1] = 0.99 * e[:, 0] + 0.10 * e[:, 1]\n    A3_unnormalized[:, 2] = 0.99 * e[:, 0] - 0.10 * e[:, 1]\n    A3_unnormalized[:, 3] = e[:, 1]\n    A3_unnormalized[:, 4] = e[:, 2]\n    A3_unnormalized[:, 5] = 0.50 * e[:, 2] + 0.40 * e[:, 4]\n    b3 = 1.00 * e[:, 0] + 0.70 * e[:, 1] + 0.25 * e[:, 2] - 0.20 * e[:, 3] + 0.10 * e[:, 5]\n\n    test_cases = [\n        (A1_unnormalized, b1, TOL),\n        (A2_unnormalized, b2, TOL),\n        (A3_unnormalized, b3, TOL)\n    ]\n    \n    results = []\n    for A_un, b, tol in test_cases:\n        # Normalize columns of A\n        A = A_un.copy()\n        norms = np.linalg.norm(A, axis=0)\n        # Avoid division by zero for zero-columns if any\n        non_zero_norms = norms > tol\n        A[:, non_zero_norms] /= norms[non_zero_norms]\n        \n        res = run_lasso_path(A, b, tol)\n        results.append(res)\n    \n    # Using python's str() will introduce spaces which is acceptable.\n    # The problem description's provided code skeleton implies this is the desired way.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_lasso_path(A, b, tol):\n    \"\"\"\n    Implements the homotopy (LARS-LASSO) algorithm to trace the LASSO solution path.\n    \"\"\"\n    m, n = A.shape\n    x = np.zeros(n)\n    events = []\n    kkt_ok = True\n    pwl_ok = True\n\n    # Initial state\n    c = A.T @ (b - A @ x)\n    lam = np.max(np.abs(c))\n    \n    full_indices = np.arange(n)\n    active_set = {np.argmax(np.abs(c))}\n    \n    events.append([1, list(active_set)[0] + 1])\n\n    while lam > tol and len(active_set) > 0 and len(active_set) <= m:\n        active_indices = sorted(list(active_set))\n        inactive_indices = np.setdiff1d(full_indices, active_indices)\n\n        x_old = x.copy()\n        lam_old = lam\n\n        c = A.T @ (b - A @ x)\n        signs = np.sign(c[active_indices])\n        \n        # This handles cases where a sign is zero due to numerical precision\n        zero_sign_mask = np.abs(signs) < tol\n        if np.any(zero_sign_mask):\n            signs[zero_sign_mask] = 1.0 # Or some other robust choice\n\n        # Compute path direction\n        A_active = A[:, active_indices]\n        try:\n            # G u = s => u = G^-1 s\n            G_active = A_active.T @ A_active\n            w_active = np.linalg.solve(G_active, signs)\n        except np.linalg.LinAlgError:\n            kkt_ok = False # Singular matrix, path cannot continue\n            break\n        \n        w = np.zeros(n)\n        w[active_indices] = w_active\n        \n        # Calculate step to next event\n        delta_lam = lam # Initialize with max possible step\n\n        # --- 1. Entry event ---\n        delta_lam_entry = delta_lam\n        entry_candidates = []\n        if len(inactive_indices) > 0:\n            a = A.T @ (A @ w)\n            for j in inactive_indices:\n                # Solve for delta_lam_j in |c_j - delta_lam_j * a_j| = lam - delta_lam_j\n                # Case 1: c_j - d*a_j = lam-d => d(1-a_j) = lam-c_j\n                if abs(1 - a[j]) > tol:\n                    d1 = (lam - c[j]) / (1 - a[j])\n                    if d1 > tol:\n                        entry_candidates.append(d1)\n                # Case 2: c_j - d*a_j = -(lam-d) => d(1+a_j) = lam+c_j\n                if abs(1 + a[j]) > tol:\n                    d2 = (lam + c[j]) / (1 + a[j])\n                    if d2 > tol:\n                        entry_candidates.append(d2)\n            if entry_candidates:\n                delta_lam_entry = min(entry_candidates)\n        \n        # --- 2. Dropout event ---\n        delta_lam_dropout = delta_lam\n        dropout_candidates = []\n        for i, k in enumerate(active_indices):\n            # Find d s.t. x_k + d * w_k = 0\n            if signs[i] * w_active[i] < -tol: # x and w must move towards each other\n                d = -x[k] / w[k]\n                if d > tol:\n                    dropout_candidates.append(d)\n        if dropout_candidates:\n            delta_lam_dropout = min(dropout_candidates)\n\n        delta_lam = min(delta_lam_entry, delta_lam_dropout)\n        \n        if delta_lam >= lam or delta_lam <= tol:\n            delta_lam = lam # Take final step to zero\n        \n        # Update state\n        lam -= delta_lam\n        x += delta_lam * w\n        \n        # Force near-zero components to be exactly zero\n        x[np.abs(x) < tol] = 0.0\n        \n        # Verification\n        # PWL check\n        if lam_old - lam > tol:\n            slope_empirical = (x - x_old) / (lam - lam_old)\n            slope_theoretical = -w\n            if not np.allclose(slope_empirical, slope_theoretical, atol=tol*10):\n                 pwl_ok = False\n        \n        # KKT Check - At the new point, before updating active set\n        c_new = A.T @ (b - A @ x)\n        \n        is_entry = delta_lam_entry <= delta_lam_dropout + tol\n        is_dropout = delta_lam_dropout <= delta_lam_entry + tol\n        \n        next_active_set = active_set.copy()\n        newly_added = set()\n        if is_entry:\n            for j in inactive_indices:\n                if abs(1 - a[j]) > tol:\n                    d1 = (lam_old - c[j]) / (1 - a[j])\n                    if abs(d1 - delta_lam) < tol:\n                        next_active_set.add(j)\n                        newly_added.add(j)\n                if abs(1 + a[j]) > tol:\n                    d2 = (lam_old + c[j]) / (1 + a[j])\n                    if abs(d2 - delta_lam) < tol:\n                        next_active_set.add(j)\n                        newly_added.add(j)\n        \n        newly_removed = set()\n        if is_dropout:\n            for i, k in enumerate(active_indices):\n                if signs[i] * w_active[i] < -tol:\n                    d = -x_old[k] / w[k]\n                    if abs(d - delta_lam) < tol:\n                        if k in next_active_set:\n                           next_active_set.remove(k)\n                        newly_removed.add(k)\n        \n        next_active_indices = sorted(list(next_active_set))\n        if next_active_indices:\n            signs_new = np.sign(c_new[next_active_indices])\n            for i, k in enumerate(next_active_indices):\n                if abs(abs(c_new[k]) - lam) > tol * 10 or abs(c_new[k] - signs_new[i] * lam) > tol * 10:\n                    kkt_ok = False\n                    break\n        if not kkt_ok: break\n        \n        for k in range(n):\n            if k not in next_active_set:\n                if abs(c_new[k]) > lam + tol * 10:\n                    kkt_ok = False\n                    break\n        if not kkt_ok: break\n\n        # Record events and update active set\n        for j in newly_added:\n            events.append([1, j + 1])\n        for j in newly_removed:\n            events.append([-1, j + 1])\n\n        active_set = next_active_set\n        \n        if lam < tol:\n            break\n\n    return [events, kkt_ok, pwl_ok]\n\nsolve()\n```", "id": "3451768"}, {"introduction": "With a grasp of the overall path, we now zoom in on its local dynamics. The solution path is piecewise linear, and this exercise challenges you to derive the exact expression for its slope between two consecutive events. Applying this theoretical result will allow you to predict the path's local behavior and forecast when a coefficient is about to drop out of the model, connecting the abstract theory directly to tangible predictive power [@problem_id:3451804].", "problem": "Consider the parameterized convex optimization problem (the least absolute shrinkage and selection operator, also called Lasso)\n$$\n\\min_{x \\in \\mathbb{R}^p} \\; \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1,\n$$\nwith data matrix $A \\in \\mathbb{R}^{m \\times p}$, data vector $b \\in \\mathbb{R}^m$, and regularization parameter $\\lambda \\ge 0$. Let $x^{\\star}(\\lambda)$ denote a solution as a function of $\\lambda$. Assume there is an interval of $\\lambda$-values on which the active set $S \\subset \\{1,\\dots,p\\}$ and the signs $\\operatorname{sign}(x^{\\star}_S(\\lambda))$ remain constant, with $x^{\\star}_{S^c}(\\lambda) = 0$ on that interval.\n\n1) Starting from first principles for convex optimality, namely the Karush–Kuhn–Tucker (KKT) conditions and the subdifferential of the $\\ell_1$-norm, derive an explicit expression for the path derivative $\\dot{x}_S(\\lambda) := \\frac{d}{d\\lambda} x^{\\star}_S(\\lambda)$ on such an interval in terms of $A_S$ and $\\operatorname{sign}(x^{\\star}_S(\\lambda))$. Your derivation must only use the stated optimality framework, together with basic differentiation rules and invertibility of $A_S^{\\top}A_S$ on the active set.\n\n2) Construct and analyze the following concrete instance to demonstrate how $\\dot{x}_S(\\lambda)$ predicts an imminent coefficient sign flip. Let $p = m = 2$, $S = \\{1,2\\}$, and\n$$\nA \\;=\\; \\begin{pmatrix} \\sqrt{\\tfrac{1}{2}} & \\sqrt{2} \\\\[4pt] 0 & \\sqrt{3} \\end{pmatrix}, \\qquad\n\\lambda_0 \\;=\\; 1, \\qquad\nx^{\\star}(\\lambda_0) \\;=\\; \\begin{pmatrix} 0.2 \\\\ 0.1 \\end{pmatrix},\n$$\nwith sign vector on the active set $S$ given by $\\operatorname{sign}(x^{\\star}_S(\\lambda_0)) = (1,1)^{\\top}$. Choose $b \\in \\mathbb{R}^2$ so that the KKT stationarity condition holds at $\\lambda_0$ with this active set and sign configuration. Then, using your general expression from part $1)$, compute $\\dot{x}_S(\\lambda_0)$ and use the resulting local linear model\n$$\nx^{\\star}_S(\\lambda) \\;=\\; x^{\\star}_S(\\lambda_0) + (\\lambda - \\lambda_0)\\,\\dot{x}_S(\\lambda_0)\n$$\nto predict the first parameter value $\\lambda_{\\text{flip}} < \\lambda_0$ at which one of the currently active coordinates reaches zero (and hence is predicted to flip sign upon further decrease of $\\lambda$), assuming no other support change occurs before that event.\n\nReport as your final answer the exact value of $\\lambda_{\\text{flip}}$ generated by this construction. Express your final answer as an exact fraction. Do not include any units.", "solution": "We begin from the convex optimality conditions. The objective is\n$$\nf(x,\\lambda) \\;=\\; \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1.\n$$\nThe subdifferential of the $\\ell_1$-norm is\n$$\n\\partial \\|x\\|_1 \\;=\\; \\{ z \\in \\mathbb{R}^p \\;:\\; z_i = \\operatorname{sign}(x_i) \\text{ if } x_i \\ne 0,\\; \\text{and}\\; z_i \\in [-1,1] \\text{ if } x_i = 0 \\}.\n$$\nThe Karush–Kuhn–Tucker (KKT) stationarity condition for optimality is\n$$\n0 \\;\\in\\; \\nabla_x \\left( \\tfrac{1}{2}\\|A x - b\\|_2^2 \\right) + \\lambda \\,\\partial \\|x\\|_1 \\;=\\; A^{\\top}(A x - b) + \\lambda \\,\\partial \\|x\\|_1.\n$$\nSuppose there is an interval of $\\lambda$ on which the active set $S$ and the signs on $S$ remain constant, with $x_{S^c}(\\lambda) = 0$. Let $s_S := \\operatorname{sign}(x_S(\\lambda))$ on that interval. Then the stationarity condition restricted to $S$ is the equality\n$$\nA_S^{\\top}(A_S x_S - b) + \\lambda \\, s_S \\;=\\; 0,\n$$\nbecause on $S$ we have the definite subgradient $z_S = s_S$. Rearranging,\n$$\nA_S^{\\top} A_S \\, x_S \\;-\\; A_S^{\\top} b \\;+\\; \\lambda \\, s_S \\;=\\; 0.\n$$\nAssuming $A_S^{\\top} A_S$ is invertible (full column rank on $S$), we may differentiate both sides with respect to $\\lambda$ on the interval where $S$ and $s_S$ are fixed. Using the fact that $A_S^{\\top} A_S$ and $A_S^{\\top} b$ are constant with respect to $\\lambda$, and that $s_S$ is constant on this interval, we obtain\n$$\nA_S^{\\top} A_S \\, \\dot{x}_S(\\lambda) \\;+\\; s_S \\;=\\; 0.\n$$\nSolving for the path derivative on the active set yields the explicit expression\n$$\n\\dot{x}_S(\\lambda) \\;=\\; -\\,\\left(A_S^{\\top} A_S\\right)^{-1} \\, s_S.\n$$\n\nWe now instantiate the example. Take $p = m = 2$, $S = \\{1,2\\}$, and\n$$\nA \\;=\\; \\begin{pmatrix} \\sqrt{\\tfrac{1}{2}} & \\sqrt{2} \\\\[4pt] 0 & \\sqrt{3} \\end{pmatrix}, \\qquad\n\\lambda_0 \\;=\\; 1, \\qquad\nx^{\\star}(\\lambda_0) \\;=\\; \\begin{pmatrix} 0.2 \\\\ 0.1 \\end{pmatrix}, \\qquad\ns_S \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nThe KKT stationarity condition on $S$ at $\\lambda_0$ requires\n$$\nA^{\\top} A \\, x^{\\star}(\\lambda_0) \\;-\\; A^{\\top} b \\;+\\; \\lambda_0 \\, s_S \\;=\\; 0\n\\quad\\Longleftrightarrow\\quad\nA^{\\top} b \\;=\\; A^{\\top} A \\, x^{\\star}(\\lambda_0) + \\lambda_0 \\, s_S.\n$$\nCompute the Gram matrix\n$$\nA^{\\top} A \\;=\\; \\begin{pmatrix}\n\\frac{1}{2} & 1 \\\\[4pt]\n1 & 5\n\\end{pmatrix}.\n$$\nThus\n$$\nA^{\\top} A \\, x^{\\star}(\\lambda_0) \\;=\\; \\begin{pmatrix}\n\\frac{1}{2} & 1 \\\\[4pt]\n1 & 5\n\\end{pmatrix}\n\\begin{pmatrix} 0.2 \\\\ 0.1 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 0.2 \\\\ 0.7 \\end{pmatrix},\n$$\nand hence\n$$\nA^{\\top} b \\;=\\; \\begin{pmatrix} 0.2 \\\\ 0.7 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 1.2 \\\\ 1.7 \\end{pmatrix}.\n$$\nSince $A$ is invertible, we can choose\n$$\nb \\;=\\; \\left(A^{\\top}\\right)^{-1} \\begin{pmatrix} 1.2 \\\\ 1.7 \\end{pmatrix}.\n$$\nNow compute $\\dot{x}_S(\\lambda_0)$ using the general formula. First, invert the Gram matrix. For\n$$\nG \\;=\\; A^{\\top} A \\;=\\; \\begin{pmatrix}\n\\frac{1}{2} & 1 \\\\[4pt]\n1 & 5\n\\end{pmatrix},\n$$\nits determinant is\n$$\n\\det(G) \\;=\\; \\frac{1}{2}\\cdot 5 - 1^2 \\;=\\; \\frac{5}{2} - 1 \\;=\\; \\frac{3}{2},\n$$\nand the inverse is\n$$\nG^{-1} \\;=\\; \\frac{1}{\\det(G)} \\begin{pmatrix} 5 & -1 \\\\[4pt] -1 & \\frac{1}{2} \\end{pmatrix}\n\\;=\\; \\frac{2}{3} \\begin{pmatrix} 5 & -1 \\\\[4pt] -1 & \\frac{1}{2} \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} \\frac{10}{3} & -\\frac{2}{3} \\\\[4pt] -\\frac{2}{3} & \\frac{1}{3} \\end{pmatrix}.\n$$\nTherefore,\n$$\n\\dot{x}_S(\\lambda_0) \\;=\\; -\\,G^{-1} s_S \\;=\\; - \\begin{pmatrix} \\frac{10}{3} & -\\frac{2}{3} \\\\[4pt] -\\frac{2}{3} & \\frac{1}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n\\;=\\; - \\begin{pmatrix} \\frac{8}{3} \\\\[4pt] -\\frac{1}{3} \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} -\\frac{8}{3} \\\\[4pt] \\frac{1}{3} \\end{pmatrix}.\n$$\nThis derivative predicts that, as $\\lambda$ decreases from $\\lambda_0$, the first coordinate increases (since for $\\lambda < \\lambda_0$ we have $\\lambda - \\lambda_0 < 0$, and $\\dot{x}_{1}(\\lambda_0) < 0$), while the second coordinate decreases (since $\\dot{x}_{2}(\\lambda_0) > 0$), moving toward zero and potentially flipping sign after crossing zero.\n\nUse the local linear path model on the interval of fixed support and sign:\n$$\nx_S(\\lambda) \\;=\\; x_S(\\lambda_0) + (\\lambda - \\lambda_0) \\,\\dot{x}_S(\\lambda_0).\n$$\nThe second coordinate reaches zero at the first $\\lambda < \\lambda_0$ such that\n$$\n0 \\;=\\; x_2(\\lambda) \\;=\\; 0.1 + (\\lambda - 1)\\cdot \\frac{1}{3}.\n$$\nSolving for $\\lambda$ gives\n$$\n(\\lambda - 1)\\cdot \\frac{1}{3} \\;=\\; -0.1\n\\;\\;\\Longrightarrow\\;\\;\n\\lambda - 1 \\;=\\; -0.3\n\\;\\;\\Longrightarrow\\;\\;\n\\lambda \\;=\\; 0.7 \\;=\\; \\frac{7}{10}.\n$$\nNo earlier event occurs because the first coordinate moves away from zero on decreasing $\\lambda$. Hence, the local model predicts an imminent sign flip at\n$$\n\\lambda_{\\text{flip}} \\;=\\; \\frac{7}{10}.\n$$\nThis value is exact as a rational number.", "answer": "$$\\boxed{\\frac{7}{10}}$$", "id": "3451804"}, {"introduction": "This final practice demonstrates the elegance and efficiency of the homotopy framework in a dynamic, streaming data environment. Instead of re-computing the entire LASSO path when a new data point arrives, you will learn to update the existing path segment using a rank-one correction derived from first principles. This exercise highlights how a deep understanding of the path's mathematical structure enables powerful and efficient algorithms for real-world applications [@problem_id:3451784].", "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) path defined by minimizing the objective\n$$\\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1},$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, $x \\in \\mathbb{R}^{n}$, and $\\lambda \\geq 0$. The path is traced by decreasing $\\lambda$ and maintaining the Karush–Kuhn–Tucker (KKT) conditions. On a segment where the active set $S \\subset \\{1,\\dots,n\\}$ and the associated sign vector $s \\in \\mathbb{R}^{|S|}$ are fixed, the KKT conditions imply $x_{S^{c}}(\\lambda)=0$, the stationarity $A_{S}^{\\top}(A x(\\lambda) - b) + \\lambda s = 0$, and the dual feasibility $|A_{j}^{\\top}(A x(\\lambda) - b)| \\leq \\lambda$ for every inactive index $j \\in S^{c}$.\n\nYou are currently on a segment with active set $S = \\{1\\}$ and fixed sign $s_{1} = +1$, for the data\n$$A = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{pmatrix}, \\qquad b = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}.$$\nThe next breakpoint on this segment, in the absence of streaming updates, is determined by the inactive index $j^{\\star} = 2$ becoming active when its dual constraint hits equality.\n\nNow suppose a streaming update appends one new row $a^{\\top} \\in \\mathbb{R}^{n}$ and one new entry $b_{\\mathrm{new}} \\in \\mathbb{R}$:\n$$a^{\\top} = \\begin{pmatrix} 1 & 0.25 & 0 \\end{pmatrix}, \\qquad b_{\\mathrm{new}} = 1.2,$$\nso the updated system is\n$$A' = \\begin{pmatrix}\nA \\\\\na^{\\top}\n\\end{pmatrix}, \\qquad b' = \\begin{pmatrix}\nb \\\\\nb_{\\mathrm{new}}\n\\end{pmatrix}.$$\nUsing only the fundamental KKT conditions and rank-one matrix inverse update principles, derive a formula for the updated primal $x_{S}(\\lambda)$ without restarting the homotopy and express the inactive correlation for $j^{\\star} = 2$ under the streaming update. Then, determine the old next breakpoint $\\lambda_{\\mathrm{old}}$ and the new next breakpoint $\\lambda_{\\mathrm{new}}$ at which the inactive index $j^{\\star} = 2$ hits $|A_{2}^{\\top}(A x(\\lambda) - b)| = \\lambda$ and $|{A'}_{2}^{\\top}(A' x(\\lambda) - b')| = \\lambda$, respectively, while the active set remains $S = \\{1\\}$ and $s_{1} = +1$ up to these breakpoints.\n\nFinally, compute the perturbation\n$$\\Delta = \\lambda_{\\mathrm{new}} - \\lambda_{\\mathrm{old}},$$\nand give its exact value as a reduced fraction. Do not round.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- **Objective function**: $\\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$\n- **Data matrices and vectors**: $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, $x \\in \\mathbb{R}^{n}$, $\\lambda \\geq 0$.\n- **KKT Conditions**:\n    1. Primal feasibility: satisfied by the solution path.\n    2. Subgradient/Stationarity: For an active set $S$ with sign vector $s \\in \\mathbb{R}^{|S|}$, $x_{S^{c}}(\\lambda)=0$ and $A_{S}^{\\top}(A x(\\lambda) - b) + \\lambda s = 0$.\n    3. Dual feasibility: $|A_{j}^{\\top}(A x(\\lambda) - b)| \\leq \\lambda$ for every inactive index $j \\in S^{c}$.\n- **Initial Data**:\n    - $A = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix}$\n    - $b = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$\n- **Path Segment**:\n    - Active set $S = \\{1\\}$.\n    - Sign vector $s = s_{1} = +1$.\n    - Next breakpoint determined by inactive index $j^{\\star} = 2$.\n- **Streaming Update**:\n    - New row: $a^{\\top} = \\begin{pmatrix} 1 & 0.25 & 0 \\end{pmatrix}$\n    - New entry: $b_{\\mathrm{new}} = 1.2$\n- **Updated System**:\n    - $A' = \\begin{pmatrix} A \\\\ a^{\\top} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 0.25 & 0 \\end{pmatrix}$\n    - $b' = \\begin{pmatrix} b \\\\ b_{\\mathrm{new}} \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 1.2 \\end{pmatrix}$\n- **Tasks**:\n    1. Derive the formula for the updated primal solution $x_{S}(\\lambda)$ for the segment where $S=\\{1\\}$.\n    2. Express the inactive correlation for $j^{\\star} = 2$ under the streaming update.\n    3. Determine the old breakpoint $\\lambda_{\\mathrm{old}}$ and the new breakpoint $\\lambda_{\\mathrm{new}}$.\n    4. Compute the perturbation $\\Delta = \\lambda_{\\mathrm{new}} - \\lambda_{\\mathrm{old}}$ as a reduced fraction.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is set within the standard mathematical framework of sparse optimization and compressed sensing, specifically addressing the homotopy method for solving LASSO. All terms ($A$, $b$, $\\lambda$, KKT conditions, active set, breakpoints) are well-defined and standard in this field. The provided data is complete and consistent, allowing for a unique solution path to be computed. The task is a direct application of these principles to a concrete numerical example involving a streaming data update, which is a relevant topic in modern data analysis. The problem does not violate any scientific principles, is not ambiguous, and is formalizable.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed to the solution.\n\n#### Analysis of the Original System (before update)\n\nWe first analyze the system before the streaming update. The active set is $S=\\{1\\}$ and the sign of the active coefficient is $s_{1}=+1$. The solution vector is of the form $x(\\lambda) = (x_1(\\lambda), 0, 0)^{\\top}$. The KKT stationarity condition is given by $A_{S}^{\\top}(A x(\\lambda) - b) + \\lambda s = 0$.\n\nHere, $A_S = A_1$ (the first column of $A$), which is $A_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. The condition becomes:\n$$A_{1}^{\\top}(A_1 x_1(\\lambda) - b) + \\lambda s_1 = 0$$\nWe compute the terms:\n$A_1^{\\top} A_1 = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1$\n$A_1^{\\top} b = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 2$\nSubstituting these into the stationarity equation:\n$(1) x_1(\\lambda) - 2 + \\lambda (1) = 0$\nThis gives the primal path for the active coefficient:\n$$x_1(\\lambda) = 2 - \\lambda$$\nThe next breakpoint, $\\lambda_{\\mathrm{old}}$, occurs when an inactive index $j \\in S^c = \\{2, 3\\}$ first satisfies the dual feasibility condition with equality: $|A_{j}^{\\top}(A x(\\lambda) - b)| = \\lambda$. The problem specifies that this index is $j^{\\star} = 2$.\nThe residual is $r(\\lambda) = A x(\\lambda) - b = A_1 x_1(\\lambda) - b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} (2 - \\lambda) - \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 - \\lambda - 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -\\lambda \\\\ -1 \\end{pmatrix}$.\nThe correlation for the inactive index $j^{\\star}=2$ is $c_2(\\lambda) = A_2^{\\top} r(\\lambda)$. With $A_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$:\n$c_2(\\lambda) = \\begin{pmatrix} 0 & 1 \\end{pmatrix} \\begin{pmatrix} -\\lambda \\\\ -1 \\end{pmatrix} = -1$\nThe breakpoint condition is $|c_2(\\lambda)| = \\lambda$, which gives $|-1| = \\lambda$.\nTherefore, the old breakpoint is $\\lambda_{\\mathrm{old}} = 1$.\n\n#### Analysis of the Updated System (after update)\n\nAfter the streaming update, we have the new system defined by $A'$ and $b'$. The active set $S=\\{1\\}$ and sign $s_1=+1$ are assumed to remain the same on the new path segment. Let the new primal path be $x'(\\lambda) = (x'_1(\\lambda), 0, 0)^{\\top}$.\nThe KKT stationarity condition for the updated system is $A'_{S}{}^{\\top}(A' x'(\\lambda) - b') + \\lambda s = 0$.\nHere, $A'_S = A'_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$. We compute the new Gram matrix and correlation with $b'$:\n$A'_{1}{}^{\\top} A'_{1} = \\begin{pmatrix} 1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = 1 + 0 + 1 = 2$.\nThis update corresponds to $A_{S}^{\\top}A_{S} \\to A_{S}^{\\top}A_{S} + a_{S}a_{S}^{\\top}$, a rank-one update.\n$A'_{1}{}^{\\top} b' = \\begin{pmatrix} 1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\\\ 1.2 \\end{pmatrix} = 2 + 0 + 1.2 = 3.2$.\n\nThe stationarity equation becomes:\n$(2) x'_1(\\lambda) - 3.2 + \\lambda(1) = 0$\nSolving for $x'_1(\\lambda)$, we get the formula for the updated primal solution:\n$$x'_1(\\lambda) = \\frac{3.2 - \\lambda}{2} = 1.6 - 0.5\\lambda$$\n\nNext, we find the new breakpoint $\\lambda_{\\mathrm{new}}$ determined by the inactive index $j^{\\star} = 2$. We compute the updated inactive correlation $c'_2(\\lambda) = A'_{2}{}^{\\top}(A' x'(\\lambda) - b')$.\nThe new residual is $r'(\\lambda) = A' x'(\\lambda) - b' = A'_1 x'_1(\\lambda) - b'$.\n$r'(\\lambda) = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} (1.6 - 0.5\\lambda) - \\begin{pmatrix} 2 \\\\ 1 \\\\ 1.2 \\end{pmatrix} = \\begin{pmatrix} 1.6 - 0.5\\lambda - 2 \\\\ -1 \\\\ 1.6 - 0.5\\lambda - 1.2 \\end{pmatrix} = \\begin{pmatrix} -0.4 - 0.5\\lambda \\\\ -1 \\\\ 0.4 - 0.5\\lambda \\end{pmatrix}$.\nThe second column of $A'$ is $A'_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0.25 \\end{pmatrix}$.\nThe updated inactive correlation for $j^{\\star}=2$ is:\n$c'_2(\\lambda) = A'_{2}{}^{\\top} r'(\\lambda) = \\begin{pmatrix} 0 & 1 & 0.25 \\end{pmatrix} \\begin{pmatrix} -0.4 - 0.5\\lambda \\\\ -1 \\\\ 0.4 - 0.5\\lambda \\end{pmatrix}$\n$c'_2(\\lambda) = (0) + (-1) + 0.25(0.4 - 0.5\\lambda) = -1 + 0.1 - 0.125\\lambda$\n$$c'_2(\\lambda) = -0.9 - 0.125\\lambda$$\nThe new breakpoint $\\lambda_{\\mathrm{new}}$ is found by solving $|c'_2(\\lambda)| = \\lambda$:\n$|-0.9 - 0.125\\lambda| = \\lambda$\nSince $\\lambda \\geq 0$, the term $-0.9 - 0.125\\lambda$ is always negative. Thus, we can write:\n$0.9 + 0.125\\lambda = \\lambda$\n$0.9 = \\lambda - 0.125\\lambda = 0.875\\lambda$\n$\\lambda_{\\mathrm{new}} = \\frac{0.9}{0.875}$\nTo express this as a reduced fraction, we convert the decimals: $0.9 = \\frac{9}{10}$ and $0.875 = \\frac{875}{1000} = \\frac{7}{8}$.\n$\\lambda_{\\mathrm{new}} = \\frac{9/10}{7/8} = \\frac{9}{10} \\times \\frac{8}{7} = \\frac{72}{70} = \\frac{36}{35}$.\n\n#### Computation of the Perturbation\n\nFinally, we compute the perturbation $\\Delta = \\lambda_{\\mathrm{new}} - \\lambda_{\\mathrm{old}}$.\n$\\Delta = \\frac{36}{35} - 1 = \\frac{36}{35} - \\frac{35}{35} = \\frac{1}{35}$.", "answer": "$$\\boxed{\\frac{1}{35}}$$", "id": "3451784"}]}