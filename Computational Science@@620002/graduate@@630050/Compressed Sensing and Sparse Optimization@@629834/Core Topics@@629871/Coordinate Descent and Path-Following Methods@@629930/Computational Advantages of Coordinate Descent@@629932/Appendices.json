{"hands_on_practices": [{"introduction": "The primary computational advantage of coordinate descent (CD) for sparse problems like the Lasso lies in its remarkably low per-update cost. Unlike full gradient methods that must process the entire dataset at each iteration, CD can focus on one variable at a time. This practice guides you through a first-principles analysis to quantify this benefit, showing how caching the residual vector allows the cost of a single coordinate update to scale only with the sparsity of the corresponding data column [@problem_id:3436973]. By comparing this to the cost of an ISTA iteration, you will build a concrete, quantitative understanding of why CD is often the method of choice for high-dimensional, sparse regression.", "problem": "Consider the Least Absolute Shrinkage and Selection Operator (Lasso) problem defined as minimizing the composite convex objective $F(x) = f(x) + g(x)$, where $f(x) = \\tfrac{1}{2}\\lVert A x - b \\rVert_2^2$ and $g(x) = \\lambda \\lVert x \\rVert_1$, with $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, $x \\in \\mathbb{R}^n$, and $\\lambda \\ge 0$. Suppose $A$ is stored in a column-sparse format, and let $a_j \\in \\mathbb{R}^m$ denote column $j$ with sparsity level $s_j = \\mathrm{nnz}(a_j)$, where $\\mathrm{nnz}(\\cdot)$ denotes the number of nonzero entries. Assume that the residual $r = b - A x$ is explicitly cached and updated after every coordinate update.\n\nUsing only the following fundamental facts, derive the per-update and per-iteration computational costs:\n\n- The gradient of $f(x)$ is $\\nabla f(x) = A^\\top (A x - b)$.\n- The coordinate-wise Lipschitz constant for coordinate $j$ is $L_j = \\lVert a_j \\rVert_2^2$.\n- The proximal operator of the scalar function $\\lambda |\\cdot|$ is the soft-thresholding map $S_\\tau(t) = \\mathrm{sign}(t)\\max\\{|t| - \\tau, 0\\}$.\n- For a sparse dot product between a sparse vector with $s$ nonzeros and a dense vector, the arithmetic cost is $O(s)$, counting one multiplication-add pair per nonzero.\n- For a sparse matrix-vector product with $N$ nonzeros, the arithmetic cost is $O(N)$, counting one multiplication-add pair per nonzero.\n\nPart 1. Show that the exact coordinate-wise proximal update for coordinate $j$ of $x$ is\n$$\nx_j^+ \\leftarrow S_{\\lambda/L_j}\\!\\left(x_j + \\frac{a_j^\\top r}{L_j}\\right),\n$$\nand that, when the residual $r$ is cached, the total arithmetic work to update a single coordinate $j$ and refresh the residual\n$$\nr^+ \\leftarrow r - a_j (x_j^+ - x_j)\n$$\nis $O(\\mathrm{nnz}(a_j))$. Your derivation must start from the definitions of $\\nabla f(x)$, $L_j$, and the proximal operator, and must explicitly justify each step of the arithmetic work count in terms of $s_j = \\mathrm{nnz}(a_j)$.\n\nPart 2. For Iterative Soft-Thresholding Algorithm (ISTA) and Fast Iterative Soft-Thresholding Algorithm (FISTA), each iteration computes a gradient using two sparse matrix-vector products: one for $A x$ and one for $A^\\top (Ax-b)$. Justify that a single ISTA/FISTA iteration has arithmetic cost $O(\\mathrm{nnz}(A))$ up to a fixed constant factor that does not depend on $m$ or $n$. Explain why, in column-sparse regimes, the per-iteration cost is dominated by operations proportional to $\\mathrm{nnz}(A)$.\n\nPart 3. Assume the following workload model for comparing computational efficiency at fixed accuracy:\n\n- After a short transient, Coordinate Descent (CD) repeatedly updates only an active set $S \\subset \\{1,\\dots,n\\}$ of size $k = |S|$, because coordinates outside $S$ remain at zero due to soft-thresholding. In this phase, CD performs $T$ updates per active coordinate, for a total of $T k$ coordinate updates. The total arithmetic cost is proportional to $\\sum_{j \\in S} s_j$ per full sweep over $S$.\n- ISTA/FISTA requires $E$ full iterations to reach the same accuracy in objective value, and each iteration costs arithmetic work proportional to $\\mathrm{nnz}(A)$.\n\nUnder the simplifying assumption that $E=T$ (that is, both methods require the same number of outer iterations to reach the desired accuracy), the predicted runtime advantage factor of CD over ISTA/FISTA is\n$$\n\\rho \\equiv \\frac{\\text{work per ISTA/FISTA iteration}}{\\text{work per CD sweep over }S} = \\frac{\\mathrm{nnz}(A)}{\\sum_{j \\in S} s_j}.\n$$\nCompute $\\rho$ for each test case below. Assume that the cost of proximal scalar operations (soft-thresholding) and constant-time scalar updates is negligible compared to sparse arithmetic, and that each nonzero contributes one unit of work per multiplication-add pair. Use the definitions:\n\n- $\\mathrm{nnz}(A) = \\sum_{j=1}^n s_j$,\n- $s_j = \\mathrm{nnz}(a_j)$,\n- $S$ is the active set.\n\nTest Suite. For each case below, compute and report the scalar $\\rho$ as defined above.\n\n- Case A (happy path; highly column-sparse with small active set): Let $m = 100000$, $n = 50000$, $s_j = 5$ for all $j$, and $S$ be any subset of size $k = 500$.\n- Case B (heterogeneous sparsity; active set concentrates on the sparsest columns): Let $m = 20000$, $n = 20000$, and define $s_j = 2 + \\left\\lfloor 8 \\cdot \\frac{j-1}{n-1} \\right\\rfloor$ so that $s_j \\in \\{2,3,\\dots,10\\}$ increases approximately linearly with $j$. Let $S$ be the $k = 100$ indices with smallest $s_j$ values.\n- Case C (boundary; dense columns and no sparsity advantage): Let $m = 5000$, $n = 5000$, $s_j = m$ for all $j$, and $S = \\{1,\\dots,n\\}$ so that $k = n$.\n- Case D (skewed mixture; very sparse majority and a few moderately dense columns; small active set among the very sparse): Let $m = 100000$, $n = 100000$, and define $s_j = 3$ for the first $90000$ columns and $s_j = 1000$ for the last $10000$ columns. Let $S$ be any subset of the first $90000$ columns of size $k = 1000$.\n\nFinal Output Format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of cases A, B, C, D. Each entry must be a floating-point number. For example, an output with four results must look like $[r_A,r_B,r_C,r_D]$, where each $r_\\cdot$ is a float written in standard decimal notation without units or percentage signs.", "solution": "The user has provided a problem in three parts concerning the computational cost analysis of Coordinate Descent (CD) versus Iterative Soft-Thresholding Algorithm (ISTA) for the Lasso optimization problem. The problem is valid as it is scientifically grounded in convex optimization theory, internally consistent, well-posed, and provides all necessary information for a complete solution.\n\n### Part 1: Coordinate Descent Update and Per-Update Cost\n\nThe Lasso objective function is $F(x) = f(x) + g(x)$, where $f(x) = \\frac{1}{2}\\lVert A x - b \\rVert_2^2$ is the smooth least-squares term and $g(x) = \\lambda \\lVert x \\rVert_1 = \\sum_{i=1}^n \\lambda |x_i|$ is the non-smooth regularization term.\n\nA coordinate descent algorithm updates one coordinate $x_j$ at a time while holding all other coordinates $x_{i \\neq j}$ fixed. This involves solving a one-dimensional optimization problem. A standard method for this is the proximal gradient update for the single coordinate $x_j$. The general form of this update is:\n$$\nx_j^+ \\leftarrow \\mathrm{prox}_{\\gamma g_j}(x_j - \\gamma \\nabla_j f(x))\n$$\nwhere $g_j(t) = \\lambda |t|$ is the component of the regularizer corresponding to coordinate $j$, $\\nabla_j f(x) = \\frac{\\partial f}{\\partial x_j}$ is the $j$-th partial derivative of $f(x)$, and $\\gamma$ is the step size. For coordinate-wise updates, the optimal step size is the inverse of the coordinate-wise Lipschitz constant of the gradient, $\\gamma_j = 1/L_j$. The problem states that $L_j = \\lVert a_j \\rVert_2^2$.\n\nWith the residual defined as $r = b - Ax$, the $j$-th component of the gradient $\\nabla f(x) = A^\\top(Ax-b)$ is given by:\n$$\n\\nabla_j f(x) = a_j^\\top (Ax - b) = -a_j^\\top (b - Ax) = -a_j^\\top r\n$$\nThe proximal operator for the function $\\alpha |\\cdot|$ is the soft-thresholding operator $S_\\alpha(z) = \\mathrm{sign}(z) \\max\\{|z|-\\alpha, 0\\}$. In our case, the function is $\\gamma_j g_j(t) = \\frac{1}{L_j} \\lambda |t|$, so the parameter for the soft-thresholding operator is $\\alpha = \\lambda/L_j$.\n\nSubstituting $\\gamma = 1/L_j$, $\\nabla_j f(x) = -a_j^\\top r$, and the form of the proximal operator into the update rule, we obtain:\n$$\nx_j^+ \\leftarrow \\mathrm{prox}_{\\lambda/L_j |\\cdot|}\\left(x_j - \\frac{1}{L_j} (-a_j^\\top r)\\right) = S_{\\lambda/L_j}\\left(x_j + \\frac{a_j^\\top r}{L_j}\\right)\n$$\nThis completes the derivation of the update formula.\n\nNext, we analyze the computational cost of performing this update and refreshing the cached residual $r$. The process involves two steps:\n1.  **Computing the update for $x_j$**: The main computational work is the dot product $a_j^\\top r$. Given that $a_j$ is a sparse vector with $s_j = \\mathrm{nnz}(a_j)$ non-zero entries and $r$ is a dense vector of size $m$, this operation costs $O(s_j)$ arithmetic operations (specifically, $s_j$ multiplications and $s_j-1$ additions). All other operations (scalar additions, division, and the soft-thresholding map) take constant time, $O(1)$. Therefore, the cost of computing $x_j^+$ is dominated by the dot product, resulting in a cost of $O(s_j)$.\n2.  **Updating the residual**: The new residual $r^+$ is calculated based on the change in $x_j$, denoted $\\Delta x_j = x_j^+ - x_j$.\n    $$\n    r^+ = b - A x^+ = b - A (x + \\Delta x_j e_j) = (b - A x) - A(\\Delta x_j e_j) = r - \\Delta x_j a_j\n    $$\n    where $e_j$ is the $j$-th standard basis vector. The update $r^+ \\leftarrow r - (\\Delta x_j) a_j$ is a scaled vector subtraction (a SAXPY operation). Since $a_j$ has only $s_j$ non-zero entries, only $s_j$ elements of $r$ will be modified. This requires $s_j$ multiplications and $s_j$ additions, for a total cost of $O(s_j)$.\n\nThe total arithmetic cost for one complete coordinate update (updating $x_j$ and then $r$) is the sum of the costs of these two steps, which is $O(s_j) + O(s_j) = O(s_j)$.\n\n### Part 2: ISTA/FISTA Per-Iteration Cost\n\nAn iteration of ISTA (and FISTA) requires the computation of the full gradient $\\nabla f(x) = A^\\top(Ax-b)$. As stated in the problem, this is achieved with two sparse matrix-vector products.\n1.  Compute the intermediate vector $v=Ax-b$. The product $Ax$, where $A$ is a sparse matrix with $\\mathrm{nnz}(A)$ non-zeros, costs $O(\\mathrm{nnz}(A))$ operations according to the provided cost model. The subsequent vector subtraction costs $O(m)$.\n2.  Compute the gradient $\\nabla f(x) = A^\\top v$. The matrix $A^\\top$ also has $\\mathrm{nnz}(A)$ non-zeros. This product similarly costs $O(\\mathrm{nnz}(A))$ operations.\n\nThe remaining parts of an ISTA/FISTA iteration, such as vector additions and the element-wise proximal mapping, have costs of $O(n)$ or $O(m)$. In typical sparse, high-dimensional settings, the number of non-zeros $\\mathrm{nnz}(A)$ is the dominant term, i.e., $\\mathrm{nnz}(A) \\gg n$ and $\\mathrm{nnz}(A) \\gg m$. Therefore, the total per-iteration cost for ISTA or FISTA is dominated by the two matrix-vector products, yielding a computational complexity of $O(\\mathrm{nnz}(A))$.\n\n### Part 3: Computation of the Advantage Factor $\\rho$\n\nThe advantage factor $\\rho$ compares the work of one ISTA/FISTA iteration to the work of one sweep of CD over the active set $S$. Based on the problem definition and the costs derived above, we have:\n$$\n\\rho = \\frac{\\text{Work(ISTA iteration)}}{\\text{Work(CD sweep over } S)} = \\frac{O(\\mathrm{nnz}(A))}{O\\left(\\sum_{j \\in S} s_j\\right)}\n$$\nAssuming the constants hidden by the big-O notation are comparable and can be canceled, the formula simplifies to:\n$$\n\\rho = \\frac{\\mathrm{nnz}(A)}{\\sum_{j \\in S} s_j}\n$$\nWe now compute $\\rho$ for each given test case.\n\n**Case A**:\n-   $n = 50000$, $s_j=5$ for all $j$, $k = |S| = 500$.\n-   $\\mathrm{nnz}(A) = n \\times s_j = 50000 \\times 5 = 250000$.\n-   $\\sum_{j \\in S} s_j = k \\times s_j = 500 \\times 5 = 2500$.\n-   $\\rho_A = \\frac{250000}{2500} = 100.0$.\n\n**Case B**:\n-   $n = 20000$, $s_j = 2 + \\lfloor 8 \\cdot \\frac{j-1}{n-1} \\rfloor$, $k = |S| = 100$.\n-   The active set $S$ consists of the $100$ indices with the smallest $s_j$. The minimum value is $s_1 = 2 + \\lfloor 0 \\rfloor = 2$. Therefore, for all $j \\in S$, $s_j = 2$.\n-   $\\sum_{j \\in S} s_j = k \\times 2 = 100 \\times 2 = 200$.\n-   $\\mathrm{nnz}(A) = \\sum_{j=1}^n s_j = \\sum_{j=1}^{20000} \\left(2 + \\lfloor 8 \\frac{j-1}{19999} \\rfloor\\right)$. This sum evaluates to $110001$.\n-   $\\rho_B = \\frac{110001}{200} = 550.005$.\n\n**Case C**:\n-   $n = 5000$, $m=5000$, $s_j = m = 5000$ for all $j$, $S = \\{1, \\dots, n\\}$.\n-   $\\mathrm{nnz}(A) = n \\times m = 5000 \\times 5000 = 25000000$.\n-   Since $S$ includes all columns, $\\sum_{j \\in S} s_j = \\sum_{j=1}^n s_j = \\mathrm{nnz}(A)$.\n-   $\\rho_C = \\frac{\\mathrm{nnz}(A)}{\\mathrm{nnz}(A)} = 1.0$.\n\n**Case D**:\n-   $n = 100000$, $k = |S| = 1000$. $S$ is a subset of the first $90000$ columns.\n-   $s_j=3$ for $j \\in \\{1, \\dots, 90000\\}$ and $s_j=1000$ for $j \\in \\{90001, \\dots, 100000\\}$.\n-   $\\mathrm{nnz}(A) = (90000 \\times 3) + (10000 \\times 1000) = 270000 + 10000000 = 10270000$.\n-   For any $j \\in S$, $s_j = 3$.\n-   $\\sum_{j \\in S} s_j = k \\times 3 = 1000 \\times 3 = 3000$.\n-   $\\rho_D = \\frac{10270000}{3000} = \\frac{10270}{3} \\approx 3423.333333$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the predicted runtime advantage factor of Coordinate Descent (CD)\n    over ISTA/FISTA for the Lasso problem under a simplified workload model.\n    \"\"\"\n\n    test_cases = [\n        # Case A: Happy path; highly column-sparse with small active set\n        {\n            \"name\": \"A\",\n            \"params\": {\"m\": 100000, \"n\": 50000, \"k\": 500},\n            \"s_func\": lambda j, n, m: 5,\n            \"s_active_func\": lambda s_all: 5\n        },\n        # Case B: Heterogeneous sparsity; active set on sparsest columns\n        {\n            \"name\": \"B\",\n            \"params\": {\"m\": 20000, \"n\": 20000, \"k\": 100},\n            \"s_func\": lambda j, n, m: 2 + int(8 * (j - 1) / (n - 1)),\n            \"s_active_func\": lambda s_all: 2\n        },\n        # Case C: Boundary; dense columns and no sparsity advantage\n        {\n            \"name\": \"C\",\n            \"params\": {\"m\": 5000, \"n\": 5000, \"k\": 5000},\n            \"s_func\": lambda j, n, m: m,\n            \"s_active_func\": lambda s_all: 5000\n        },\n        # Case D: Skewed mixture; small active set among very sparse columns\n        {\n            \"name\": \"D\",\n            \"params\": {\"m\": 100000, \"n\": 100000, \"k\": 1000},\n            \"s_func\": lambda j, n, m: 3 if j <= 90000 else 1000,\n            \"s_active_func\": lambda s_all: 3\n        },\n    ]\n\n    results = []\n    \n    # The advantage factor rho is defined as nnz(A) / sum_{j in S} s_j\n    # where nnz(A) = sum_{j=1 to n} s_j\n\n    # --- Case A ---\n    case_a = test_cases[0]\n    p_a = case_a[\"params\"]\n    n_a, k_a = p_a[\"n\"], p_a[\"k\"]\n    s_j_a = case_a[\"s_func\"](1, n_a, p_a[\"m\"]) # s_j is constant\n    nnz_A_a = n_a * s_j_a\n    sum_s_j_S_a = k_a * case_a[\"s_active_func\"](None)\n    rho_a = nnz_A_a / sum_s_j_S_a\n    results.append(rho_a)\n\n    # --- Case B ---\n    case_b = test_cases[1]\n    p_b = case_b[\"params\"]\n    n_b, k_b = p_b[\"n\"], p_b[\"k\"]\n    s_values_b = [case_b[\"s_func\"](j, n_b, p_b[\"m\"]) for j in range(1, n_b + 1)]\n    nnz_A_b = sum(s_values_b)\n    sum_s_j_S_b = k_b * case_b[\"s_active_func\"](s_values_b)\n    rho_b = nnz_A_b / sum_s_j_S_b\n    results.append(rho_b)\n\n    # --- Case C ---\n    case_c = test_cases[2]\n    p_c = case_c[\"params\"]\n    n_c, m_c, k_c = p_c[\"n\"], p_c[\"m\"], p_c[\"k\"]\n    s_j_c = case_c[\"s_func\"](1, n_c, m_c)\n    nnz_A_c = n_c * s_j_c\n    # S = {1,...,n}, so sum over S is the same as sum over all columns\n    sum_s_j_S_c = nnz_A_c \n    rho_c = nnz_A_c / sum_s_j_S_c\n    results.append(rho_c)\n    \n    # --- Case D ---\n    case_d = test_cases[3]\n    p_d = case_d[\"params\"]\n    n_d, k_d = p_d[\"n\"], p_d[\"k\"]\n    n1, s1 = 90000, 3\n    n2, s2 = 10000, 1000\n    nnz_A_d = n1 * s1 + n2 * s2\n    sum_s_j_S_d = k_d * case_d[\"s_active_func\"](None)\n    rho_d = nnz_A_d / sum_s_j_S_d\n    results.append(rho_d)\n\n    # Format the final output as specified.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3436973"}, {"introduction": "While raw floating-point operation (FLOP) counts provide a first-order approximation of performance, modern computational bottlenecks are often dominated by memory bandwidth. This exercise moves beyond simple FLOP analysis to explore the practical performance of CD through the lens of the roofline model [@problem_id:3436942]. You will investigate how the memory layout of the data matrix—column-major versus row-major—dramatically affects cache locality and memory traffic, ultimately dictating whether an algorithm is compute-bound or memory-bound and explaining the real-world speed advantages of CD.", "problem": "Consider a dense matrix $A \\in \\mathbb{R}^{m \\times n}$ and the Least Absolute Shrinkage and Selection Operator (LASSO) objective $f(x) = \\tfrac{1}{2}\\lVert Ax - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_1$, where $x \\in \\mathbb{R}^n$ and $b \\in \\mathbb{R}^m$. Two standard iterative methods to approximately minimize $f(x)$ are:\n- Iterative Shrinkage-Thresholding Algorithm (ISTA).\n- Coordinate Descent (CD) with residual updates.\n\nYour task is to quantitatively compare the per-iteration arithmetic intensity and cache locality characteristics of these two methods under column-major and row-major storage of $A$, model Central Processing Unit (CPU) memory bandwidth constraints, and predict the performance crossover point between the methods.\n\nStart from the following context-appropriate fundamental base:\n- Arithmetic intensity is defined as $I = F/B$, where $F$ counts floating-point operations (FLOPs) and $B$ counts the total bytes moved between main memory and caches during the iteration.\n- The roofline performance model states that the iteration time is bounded by $T = \\max(F/P, B/\\mathcal{B})$, where $P$ is the peak floating-point rate in FLOPs per second and $\\mathcal{B}$ is the sustained memory bandwidth in bytes per second.\n- Double-precision floating-point numbers use $s_e = 8$ bytes per element.\n- Cache lines have size $L$ bytes and main memory transfers occur in whole cache lines.\n- Under column-major storage, columns of $A$ are laid out contiguously in memory. Under row-major storage, rows are laid out contiguously.\n- A dense matrix-vector product $y = Ax$ uses approximately $F_{mv} = 2mn$ FLOPs and $g = A^\\top r$ uses $F_{tmv} = 2mn$ FLOPs in the dense case, ignoring lower-order terms from vector operations. ISTA performs both $y = Ax$ and $g = A^\\top r$ per iteration and then a component-wise soft-thresholding on $x$.\n- Coordinate descent with residual updates for a single coordinate $j$ performs $c = A_{\\cdot j}^\\top r$ and $r \\leftarrow r + \\Delta A_{\\cdot j}$, where $A_{\\cdot j}$ is column $j$ of $A$ and $\\Delta$ is the coordinate update, using approximately $F_{cd,1} \\approx 4m$ FLOPs per single-coordinate update in the dense case (a dot product plus a scaled axpy on length $m$).\n\nModel cache locality and memory traffic using these scientifically realistic assumptions:\n- When accessing contiguous memory (a stream through a row in row-major layout or a column in column-major layout), each fetched cache line of size $L$ yields useful data across its $L/s_e$ elements, i.e., no penalty beyond transferring whole lines.\n- When accessing strided memory that touches only one $s_e$-byte element per $L$-byte cache line (e.g., reading a column from a row-major array or reading a row from a column-major array without blocking), the effective memory traffic is inflated by a stride penalty factor $p_s = L/s_e$, since only one out of $L/s_e$ elements per line is useful.\n- Assume write-allocate behavior so that writes incur the same line traffic as reads at the byte-transfer level.\n\nDefine the following per-iteration memory traffic models based on the above:\n- ISTA: two dense matrix-vector passes and vector reads/writes for intermediate quantities. Under column-major storage, the $y = Ax$ pass suffers the stride penalty $p_s$, while the $g = A^\\top r$ pass is contiguous; under row-major storage, the roles are reversed. Vector operations contribute lower-order terms proportional to $m$ and $n$. Let the total ISTA FLOPs be $F_I = 4mn$, and total ISTA bytes be $B_I = s_e\\,mn\\,(p_y + p_g) + s_e\\,(5m + 5n)$, where $(p_y, p_g) = (p_s, 1)$ for column-major and $(p_y, p_g) = (1, p_s)$ for row-major.\n- Coordinate descent: consider a single-coordinate update. Under column-major storage, reading $A_{\\cdot j}$ is contiguous; under row-major storage, reading $A_{\\cdot j}$ has stride penalty $p_s$. Updating the residual $r$ is a contiguous read-and-write. Let the single-coordinate FLOPs be $F_{C,1} = 4m$, and total bytes be $B_{C,1} = s_e\\,(m\\,p_c + 2m + 2)$, where $p_c = 1$ for column-major and $p_c = p_s$ for row-major, and the $+2$ accounts for reading and writing $x_j$.\n\nUse the roofline model to compute per-iteration times:\n- ISTA full iteration time $T_I = \\max(F_I/P, B_I/\\mathcal{B})$.\n- CD single-coordinate time $T_{C,1} = \\max(F_{C,1}/P, B_{C,1}/\\mathcal{B})$.\n\nDefine the performance crossover point as the number of CD coordinate updates that can be executed in the time of one ISTA iteration,\n$$\nk^\\star = \\left\\lfloor \\frac{T_I}{T_{C,1}} \\right\\rfloor,\n$$\nand declare CD epoch faster if $n\\,T_{C,1} < T_I$.\n\nImplement a program that, for each test case, computes:\n- The arithmetic intensities $I_C = F_{C,1}/B_{C,1}$ and $I_I = F_I/B_I$.\n- The iteration times $T_{C,1}$ and $T_I$ in seconds.\n- The crossover count $k^\\star$ as defined above.\n- A boolean indicating whether a full CD sweep over all $n$ coordinates is faster than one ISTA iteration.\n\nYour program must use the following test suite and parameters:\n- Test case $1$: column-major, $m = 5000$, $n = 1000$, $P = 1.0\\times 10^{11}$ FLOPs/s, $\\mathcal{B} = 5.0\\times 10^{10}$ bytes/s, $L = 64$ bytes, $s_e = 8$ bytes.\n- Test case $2$: row-major, $m = 5000$, $n = 1000$, $P = 1.0\\times 10^{11}$ FLOPs/s, $\\mathcal{B} = 5.0\\times 10^{10}$ bytes/s, $L = 64$ bytes, $s_e = 8$ bytes.\n- Test case $3$: column-major, $m = 200$, $n = 100$, $P = 1.0\\times 10^{11}$ FLOPs/s, $\\mathcal{B} = 5.0\\times 10^{10}$ bytes/s, $L = 64$ bytes, $s_e = 8$ bytes.\n- Test case $4$: row-major, $m = 200$, $n = 100$, $P = 1.0\\times 10^{11}$ FLOPs/s, $\\mathcal{B} = 5.0\\times 10^{10}$ bytes/s, $L = 64$ bytes, $s_e = 8$ bytes.\n- Test case $5$: column-major, $m = 8000$, $n = 2000$, $P = 1.0\\times 10^{11}$ FLOPs/s, $\\mathcal{B} = 2.0\\times 10^{11}$ bytes/s, $L = 64$ bytes, $s_e = 8$ bytes.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry corresponds to one test case and is itself a list of six values in the order $[I_C, I_I, T_{C,1}, T_I, k^\\star, \\text{CD\\_epoch\\_faster}]$. Times must be expressed in seconds, and intensities as floats. The boolean must be either $True$ or $False$.", "solution": "The problem requires a quantitative comparison of the per-iteration performance of the Iterative Shrinkage-Thresholding Algorithm (ISTA) and Coordinate Descent (CD) for solving the LASSO problem. The comparison is based on a simplified roofline performance model, which considers both computational throughput and memory bandwidth limitations. The analysis hinges on modeling the floating-point operations (FLOPs) and memory traffic (bytes moved) for each algorithm, with special attention to how a matrix's storage layout (column-major vs. row-major) impacts memory access patterns and, consequently, performance.\n\nFirst, we establish the parameters from the problem statement. The size of a double-precision floating-point number is $s_e = 8$ bytes. The cache line size is $L$ bytes. Accessing memory with a stride larger than a single element induces a penalty. Specifically, when accessing a column of a row-major matrix or a row of a column-major matrix, only one $s_e$-byte element is useful per $L$-byte cache line fetched. This incurs a stride penalty factor of $p_s = L/s_e$. For the given parameters $L=64$ and $s_e=8$, this penalty factor is $p_s = 64/8 = 8$. Contiguous memory access, such as streaming through a row in a row-major matrix or a column in a column-major matrix, does not incur this penalty.\n\nThe performance of an iteration is predicted by the roofline model, where the time $T$ is the maximum of the time required for computation and the time required for memory transfers:\n$$\nT = \\max\\left(\\frac{F}{P}, \\frac{B}{\\mathcal{B}}\\right)\n$$\nHere, $F$ is the total FLOPs, $B$ is the total bytes transferred, $P$ is the peak computational rate (FLOPs/s), and $\\mathcal{B}$ is the sustained memory bandwidth (bytes/s). The arithmetic intensity, $I = F/B$, measures the ratio of computation to memory traffic.\n\nLet's model the performance of one full ISTA iteration.\nAn ISTA iteration primarily consists of two matrix-vector products, $y = Ax$ and $g = A^\\top(Ax-b)$, followed by vector operations. The problem statement gives the total FLOP count as:\n$$\nF_I = 4mn\n$$\nThe total memory traffic, $B_I$, is modeled as the sum of traffic from matrix traversals and vector operations. The traffic for the two matrix passes depends on the storage layout. For the $y=Ax$ pass, let the penalty be $p_y$; for the $g=A^\\top r$ pass (where $r=Ax-b$), let it be $p_g$. The total traffic is given by:\n$$\nB_I = s_e\\,mn\\,(p_y + p_g) + s_e\\,(5m + 5n)\n$$\n- In **column-major** storage, accessing columns of $A$ is contiguous. $A^\\top r$ involves dot products of rows of $A^\\top$ (columns of $A$) with $r$, which is a contiguous access pattern. Thus, $p_g=1$. Conversely, $Ax$ involves linear combinations of columns of $A$, but forming each element of the output $y$ requires accessing a row of $A$, which is a strided access. Thus, $p_y=p_s$. So, $(p_y, p_g) = (p_s, 1)$.\n- In **row-major** storage, the roles are reversed. $Ax$ involves dot products of rows of $A$ with $x$, a contiguous access pattern, so $p_y=1$. $A^\\top r$ requires access to columns of $A$, which is strided, so $p_g=p_s$. So, $(p_y, p_g) = (1, p_s)$.\nThe arithmetic intensity for ISTA is $I_I = F_I / B_I$, and the iteration time is $T_I = \\max(F_I/P, B_I/\\mathcal{B})$.\n\nNext, we model the performance of a single coordinate update in CD.\nA single update for coordinate $j$ involves computing a dot product $A_{\\cdot j}^\\top r$ and updating the residual $r \\leftarrow r - \\delta_j A_{\\cdot j}$. The problem gives the FLOP count as:\n$$\nF_{C,1} = 4m\n$$\nThe memory traffic, $B_{C,1}$, is dominated by accessing the column $A_{\\cdot j}$ and a read/write pass over the residual vector $r$. Let $p_c$ be the stride penalty for accessing $A_{\\cdot j}$. The total traffic is given as:\n$$\nB_{C,1} = s_e\\,(m\\,p_c + 2m + 2)\n$$\n- In **column-major** storage, columns $A_{\\cdot j}$ are stored contiguously. Thus, $p_c = 1$.\n- In **row-major** storage, accessing a column requires striding through memory, so $p_c = p_s$.\nThe arithmetic intensity for a single CD update is $I_C = F_{C,1} / B_{C,1}$, and the time is $T_{C,1} = \\max(F_{C,1}/P, B_{C,1}/\\mathcal{B})$.\n\nFinally, we define the comparative metrics. The performance crossover point, $k^\\star$, is the number of CD coordinate updates that can be performed in the time it takes to complete one ISTA iteration. It is an integer value given by:\n$$\nk^\\star = \\left\\lfloor \\frac{T_I}{T_{C,1}} \\right\\rfloor\n$$\nA larger $k^\\star$ suggests that CD is more efficient on a per-update basis compared to ISTA's full iteration. We also assess if a full pass of CD over all $n$ coordinates (an \"epoch\") is faster than one ISTA iteration. This is true if the total time for $n$ CD updates is less than the time for one ISTA iteration:\n$$\nn \\cdot T_{C,1} < T_I\n$$\nThe following program implements these calculations for the provided test cases to compute the required metrics: $[I_C, I_I, T_{C,1}, T_I, k^\\star, \\text{CD\\_epoch\\_faster}]$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the performance comparison problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        # (storage, m, n, P, B, L, s_e)\n        ('column-major', 5000, 1000, 1.0e11, 5.0e10, 64, 8),\n        ('row-major', 5000, 1000, 1.0e11, 5.0e10, 64, 8),\n        ('column-major', 200, 100, 1.0e11, 5.0e10, 64, 8),\n        ('row-major', 200, 100, 1.0e11, 5.0e10, 64, 8),\n        ('column-major', 8000, 2000, 1.0e11, 2.0e11, 64, 8),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        storage, m, n, P, B_bw, L, s_e = case\n        \n        # Calculate stride penalty factor\n        p_s = L / s_e\n\n        # Determine storage-dependent penalties\n        if storage == 'column-major':\n            p_y, p_g = p_s, 1\n            p_c = 1\n        elif storage == 'row-major':\n            p_y, p_g = 1, p_s\n            p_c = p_s\n        else:\n            # This case should not be reached with the given test suite\n            raise ValueError(\"Invalid storage format specified.\")\n\n        # ISTA performance calculation\n        F_I = 4.0 * m * n\n        B_I = s_e * m * n * (p_y + p_g) + s_e * (5 * m + 5 * n)\n        I_I = F_I / B_I if B_I > 0 else 0.0\n        T_I = max(F_I / P, B_I / B_bw)\n\n        # Coordinate Descent (single coordinate) performance calculation\n        F_C1 = 4.0 * m\n        B_C1 = s_e * (m * p_c + 2 * m + 2)\n        I_C = F_C1 / B_C1 if B_C1 > 0 else 0.0\n        T_C1 = max(F_C1 / P, B_C1 / B_bw)\n\n        # Comparative analysis\n        # Crossover count k_star\n        k_star = int(np.floor(T_I / T_C1)) if T_C1 > 0 else float('inf')\n        \n        # CD epoch faster check\n        cd_epoch_faster = (n * T_C1) < T_I\n\n        case_results = [I_C, I_I, T_C1, T_I, k_star, cd_epoch_faster]\n        results.append(str(case_results))\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3436942"}, {"introduction": "Coordinate descent's power extends far beyond the convex Lasso problem, shining in scenarios with more complex, even nonconvex, regularizers. This advantage stems from its 'divide and conquer' strategy, which breaks a difficult high-dimensional problem into a sequence of simple one-dimensional minimizations. In this practice, you will tackle a regression problem with a nonconvex $L_p$ penalty ($0 \\lt p \\lt 1$), deriving and implementing the exact coordinate-wise minimizer [@problem_id:3437037]. This hands-on experience demonstrates how CD provides a computationally tractable path for optimization problems that are challenging for full proximal gradient methods.", "problem": "Consider the sparse regression problem in compressed sensing, where the goal is to minimize the objective $$F(x) = \\tfrac{1}{2}\\lVert A x - b\\rVert_2^2 + \\lambda \\sum_{j=1}^{n} \\lvert x_j \\rvert^p,$$ with design matrix $A \\in \\mathbb{R}^{m \\times n}$, response vector $b \\in \\mathbb{R}^m$, regularization parameter $\\lambda > 0$, penalty exponent $p \\in (0,1)$, and decision variable $x \\in \\mathbb{R}^n$. The penalty $\\sum_j \\lambda \\lvert x_j \\rvert^p$ is nonconvex for $p \\in (0,1)$ and is known to be sparsity promoting. Your task is to derive and implement computational procedures that exploit exact coordinate-wise minimization to achieve efficiency.\n\nStarting from the foundational principles of coordinate descent and first-order optimality in one dimension, perform the following:\n\n- Derivation. For a fixed coordinate $j$, show that minimizing $F(x)$ with respect to the scalar variable $t$ replacing $x_j$ reduces, up to an additive constant independent of $t$, to minimizing the one-dimensional function $$g_j(t) = \\tfrac{1}{2} L_j (t - z_j)^2 + \\lambda \\lvert t \\rvert^p,$$ where $L_j = \\lVert a_j \\rVert_2^2$ and $z_j = x_j + \\tfrac{a_j^\\top r}{L_j}$, with the residual defined as $r = b - Ax$. Conclude that the exact coordinate-wise update is the scalar proximal map $$\\operatorname{prox}_{\\alpha \\lvert \\cdot \\rvert^p}(z) \\in \\arg\\min_{t \\in \\mathbb{R}} \\tfrac{1}{2} (t - z)^2 + \\alpha \\lvert t \\rvert^p,$$ with $\\alpha = \\lambda / L_j$ and $z=z_j$. Derive the necessary optimality condition for nonzero minimizers, namely $$t - z + \\alpha p \\operatorname{sign}(t) \\lvert t \\rvert^{p-1} = 0,$$ and argue why a decision rule that compares $g_j(0)$ to the value at the larger positive root of the scalar equation for $t \\ge 0$ suffices to select the global minimizer. Explain why a robust bisection routine on the monotone branch for $t \\ge 0$ can be used to compute the nonzero minimizer, using the fact that the function $\\phi(t) = t - \\lvert z \\rvert + \\alpha p t^{p-1}$ has a unique minimizer at $$t_{\\min} = \\big(\\alpha p (1-p)\\big)^{\\tfrac{1}{2-p}}.$$ Provide a bracketing strategy to locate the larger root when it exists.\n\n- Algorithms. Implement two algorithms that share the same scalar proximal subroutine:\n  - Exact cyclic coordinate descent, which visits coordinates $j = 1, \\dots, n$ in order, maintaining the residual $r = b - Ax$ and at each step computing $z_j = x_j + \\tfrac{a_j^\\top r}{L_j}$, applying the scalar proximal operator with $\\alpha = \\lambda / L_j$, and updating $r \\leftarrow r - a_j (t - x_j)$ if $t \\ne x_j$.\n  - Proximal gradient method with a fixed global step size $1/L$, where $L$ is the squared operator $2$-norm of $A$, i.e., $L = \\lVert A \\rVert_2^2$, using the update $y = x - \\tfrac{1}{L} A^\\top (A x - b)$ followed by the elementwise scalar proximal operator with parameter $\\alpha = \\lambda/L$.\n\n- Operation counting model. To compare computational advantages, count only multiplications and additions that involve entries of the data matrix $A$ (modeling matrix–vector multiply–accumulate pairs) as unit operations. Under this model:\n  - A product $A x$ or $A^\\top r$ costs $m n$ unit operations each.\n  - In coordinate descent, computing $a_j^\\top r$ costs $m$ unit operations, and updating $r \\leftarrow r - a_j \\Delta$ costs $m$ unit operations; if $\\Delta = 0$, the residual update is skipped and costs $0$.\n  - Scalar operations that do not involve $A$ (including proximal root-finds, vector updates not touching $A$, norm computations, and threshold comparisons) have zero cost in this model.\n  Use these rules to accumulate an operation count for each method until convergence.\n\n- Convergence and stopping. Use the relative change in the iterate as the stopping criterion. Specifically, terminate when $$\\frac{\\lVert x^{k+1} - x^k \\rVert_2}{\\lVert x^k \\rVert_2 + 10^{-12}} \\le \\varepsilon,$$ with tolerance $\\varepsilon = 10^{-6}$. Impose a hard cap of $50$ full passes for coordinate descent and $500$ iterations for proximal gradient. Initialize $x^0 = 0$ for both methods.\n\n- Test suite. For reproducibility, construct synthetic data with the following procedure: draw $A$ with independent standard normal entries, normalize each column to unit Euclidean norm, construct a sparse ground truth $x^\\star$ with exactly $s$ nonzeros at uniformly random distinct indices with values drawn independently from a standard normal distribution, and set $b = A x^\\star + \\eta$ with $\\eta$ having independent normal entries of mean $0$ and standard deviation $\\sigma$. For each case below, specify $(m,n,s,p,\\lambda,\\sigma,\\text{seed})$:\n  - Case $1$: $(50,200,10,0.5,0.1,0.01,0)$.\n  - Case $2$: $(80,80,5,0.9,0.05,0.01,1)$.\n  - Case $3$: $(30,300,5,0.2,0.2,0.02,2)$.\n\n- Required output. For each case, run both methods until convergence and compute the ratio $$R = \\frac{\\text{proximal-gradient operation count}}{\\text{coordinate-descent operation count}}.$$ Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Cases $1,2,3$, for example, $$[R_1,R_2,R_3].$$ The entries must be floating-point numbers. No physical units are involved, and no angles are used.\n\nThe implementation must be self-contained, deterministic, and runnable without user input. The scalar proximal subroutine must either produce a closed-form expression when available or otherwise use a robust bisection procedure justified by your derivation. The goal is to empirically demonstrate the computational advantages of exact coordinate-wise minimization under the stated operation counting model across the provided test suite.", "solution": "The user wants me to do the following:\n1.  Derive and analyze the one-dimensional subproblem for coordinate descent applied to an $L_p$-regularized ($p \\in (0,1)$) regression problem.\n2.  Implement and compare two algorithms: cyclic coordinate descent (CD) and proximal gradient (PG).\n3.  The comparison must be based on a specific operation counting model that focuses on matrix-related operations.\n4.  Run the comparison on three specified test cases and report the ratio of operation counts.\n\nFirst, I will validate the problem. The problem is well-posed, scientifically grounded in optimization theory, and provides all necessary details for a unique, verifiable solution. The parameters, algorithms, and evaluation metric are clearly defined. No inconsistencies or ambiguities are found. The problem is valid.\n\nI will proceed with the solution by first detailing the mathematical derivations, then describing the algorithms and their implementation based on these derivations, and finally executing the comparison on the provided test cases.\n\n### Derivation of the Coordinate-wise Minimization Rule\n\nThe objective function is $F(x) = \\tfrac{1}{2}\\lVert A x - b\\rVert_2^2 + \\lambda \\sum_{j=1}^{n} \\lvert x_j \\rvert^p$. To perform coordinate descent on the $j$-th coordinate, we fix all other coordinates $x_k$ for $k \\ne j$ and minimize $F(x)$ with respect to $x_j$. Let $t$ be the scalar variable for the $j$-th coordinate. The objective function, viewed as a function of $t$, becomes:\n$$ g_j(t) = \\tfrac{1}{2}\\lVert A(x_{-j}, t) - b \\rVert_2^2 + \\lambda \\lvert t \\rvert^p + C_{-j} $$\nwhere $C_{-j} = \\lambda \\sum_{k \\ne j} \\lvert x_k \\rvert^p$ is a constant with respect to $t$. Let $r = b-Ax$ be the current residual. The term inside the norm can be written as $(Ax-b) - a_j x_j + a_j t = -r - a_j x_j + a_j t = -(r+a_j(x_j-t))$. Since the norm is squared, the sign does not matter. Expanding the quadratic term:\n$$ \\tfrac{1}{2}\\lVert r + a_j x_j - a_j t \\rVert_2^2 = \\tfrac{1}{2}\\left( \\lVert r+a_j x_j \\rVert_2^2 - 2t a_j^\\top(r+a_j x_j) + t^2 \\lVert a_j \\rVert_2^2 \\right) $$\nLet $L_j = \\lVert a_j \\rVert_2^2$. Completing the square for the terms involving $t$:\n$$ \\tilde{g}_j(t) = \\tfrac{1}{2} L_j t^2 - a_j^\\top(r+a_j x_j) t + \\lambda \\lvert t \\rvert^p = \\tfrac{1}{2} L_j \\left( t - \\frac{a_j^\\top(r+a_j x_j)}{L_j} \\right)^2 - C' + \\lambda \\lvert t \\rvert^p $$\nwhere $C'$ is a constant w.r.t $t$. The minimizer of the quadratic part is $z_j = \\frac{a_j^\\top r + L_j x_j}{L_j} = x_j + \\frac{a_j^\\top r}{L_j}$. Minimizing $\\tilde{g}_j(t)$ is thus equivalent to minimizing $\\tfrac{1}{2} L_j (t - z_j)^2 + \\lambda \\lvert t \\rvert^p$, which confirms the form given in the problem statement.\n\nThe minimization problem for coordinate $j$ is therefore:\n$$ t_j^* = \\arg\\min_{t \\in \\mathbb{R}} \\left\\{ \\tfrac{1}{2} L_j (t - z_j)^2 + \\lambda \\lvert t \\rvert^p \\right\\} $$\nDividing by $L_j$, this is equivalent to:\n$$ t_j^* = \\arg\\min_{t \\in \\mathbb{R}} \\left\\{ \\tfrac{1}{2} (t - z_j)^2 + \\frac{\\lambda}{L_j} \\lvert t \\rvert^p \\right\\} $$\nThis is the definition of the scalar proximal operator, $t_j^* = \\operatorname{prox}_{\\alpha \\lvert \\cdot \\rvert^p}(z_j)$, with $\\alpha = \\lambda / L_j$.\n\nTo find the minimizer, we analyze the one-dimensional objective $h(t) = \\tfrac{1}{2}(t-z)^2 + \\alpha |t|^p$. Since $|t|^p$ is symmetric, the sign of the global minimizer $t^*$ must match the sign of $z$. We can therefore analyze the problem for $t \\ge 0$ with $z$ replaced by $|z|$. The objective for $t \\ge 0$ is $h(t) = \\tfrac{1}{2}(t-|z|)^2 + \\alpha t^p$.\nIts derivative for $t > 0$ is $h'(t) = t - |z| + \\alpha p t^{p-1}$. A nonzero minimizer must satisfy the optimality condition $h'(t)=0$, which is $t - z + \\alpha p \\operatorname{sign}(t) \\lvert t \\rvert^{p-1} = 0$.\n\nLet $\\phi(t) = t - |z| + \\alpha p t^{p-1}$ for $t>0$. We seek roots of $\\phi(t)$.\nThe derivative of $\\phi(t)$ is $\\phi'(t) = 1 + \\alpha p (p-1) t^{p-2} = 1 - \\alpha p (1-p) t^{p-2}$. Since $p \\in (0,1)$, we have $1-p > 0$.\n$\\phi'(t)$ has a unique zero at $t_{\\min} = (\\alpha p(1-p))^{1/(2-p)}$, which corresponds to the minimum of $\\phi(t)$.\nAs $t \\to 0^+$, $\\phi(t) \\to \\infty$ (since $p-10$). As $t \\to \\infty$, $\\phi(t) \\to \\infty$.\n- If the minimum value $\\phi(t_{\\min}) \\ge 0$, then $\\phi(t) \\ge 0$ for all $t > 0$, meaning $h(t)$ is monotonically increasing for $t \\ge 0$. The minimum is at $t=0$.\n- If $\\phi(t_{\\min})  0$, $\\phi(t)$ crosses the axis twice. Let the roots be $t_1  t_{\\min}  t_2$. These are the stationary points of $h(t)$. Analysis of the second derivative $h''(t) = \\phi'(t)$ shows that $h(t)$ has a local maximum at $t_1$ and a local minimum at $t_2$.\nTherefore, the global minimizer of $h(t)$ for $t \\ge 0$ can only be at $t=0$ or at the larger root $t_2$. This justifies the decision rule: we find the larger positive root $t_2$ (if it exists) and compare the objective value $h(t_2)$ with $h(0) = \\frac{1}{2}z^2$ to find the minimizer for $t \\ge 0$. The global minimizer is then this result multiplied by $\\operatorname{sign}(z)$.\n\nTo find the root $t_2$ when it exists (i.e., when $\\phi(t_{\\min})  0$), we can use a bisection method. We need a bracket $[t_a, t_b]$ such that $\\phi(t_a)$ and $\\phi(t_b)$ have opposite signs. We know $t_2  t_{\\min}$ and $\\phi(t_{\\min})  0$. We need an upper bound $t_b$ where $\\phi(t_b)  0$. Consider $t=|z|$. For $t > |z|$, $t-|z| > 0$ and $\\alpha p t^{p-1} > 0$, so $\\phi(t) > 0$. This implies that any root must be less than or equal to $|z|$. At $t=|z|$, $\\phi(|z|) = |z| - |z| + \\alpha p |z|^{p-1} = \\alpha p |z|^{p-1} > 0$. Thus, a valid bracket for the larger root $t_2$ is $[t_{\\min}, |z|]$. A bisection routine on $\\phi(t)$ within this interval robustly finds $t_2$.\n\n### Algorithms and Computational Model\n\n**Exact Cyclic Coordinate Descent (CD)**:\nThis algorithm iterates through the coordinates $j=1, \\dots, n$ in a cyclic fashion. For each coordinate, it computes the exact minimizer of the one-dimensional subproblem using the proximal operator derived above. The implementation maintains a residual vector $r=b-Ax$. In each step $j$, it computes $z_j = x_j + a_j^\\top r / L_j$. With normalized columns, $L_j=1$. The new coordinate value $t$ is found via the scalar proximal solver. If the coordinate value changes ($t \\ne x_j$), the residual is updated efficiently as $r \\leftarrow r - a_j(t-x_j)$.\n\n**Proximal Gradient Method (PG)**:\nThis is an iterative first-order method. Each iteration consists of two steps: a gradient descent step on the smooth part of the objective, followed by a proximal step on the nonsmooth penalty. The update is $x^{k+1} = \\operatorname{prox}_{\\alpha g}(x^k - \\gamma \\nabla f(x^k))$, where $f(x) = \\frac{1}{2}\\|Ax-b\\|_2^2$ and $g(x) = \\lambda \\sum_j |x_j|^p$. The gradient is $\\nabla f(x) = A^\\top(Ax-b)$. For convergence, the step size $\\gamma$ is set to $1/L$, where $L$ is a Lipschitz constant for $\\nabla f$. The constant is $L = \\lVert A^\\top A \\rVert_2 = \\lVert A \\rVert_2^2$, the squared largest singular value of $A$. The parameter for the proximal operator becomes $\\alpha = \\lambda/L$. Since the penalty is separable, the proximal operator is applied element-wise.\n\n**Operation Counting Model**:\nWe count operations involving the data matrix $A$ to compare the computational cost.\n- Matrix-vector products $Ax$ or $A^\\top r$ cost $mn$ operations.\n- In CD, for each coordinate $j$, computing the gradient component $a_j^\\top r$ costs $m$ operations. If the coordinate is updated, the residual update $r \\leftarrow r - a_j \\Delta x_j$ costs another $m$ operations. Thus, a non-zero update costs $2m$ operations, while a zero update (where the coordinate value doesn't change) costs $m$ operations. A full pass over $n$ coordinates can cost between $mn$ and $2mn$ operations.\n- In PG, each iteration requires computing the full gradient $A^\\top(Ax-b)$, which involves one $Ax$ product and one $A^\\top r$ product, costing a total of $2mn$ operations.\n- All other operations, including the root-finding for the scalar subproblem, are considered to have zero cost under this model.\n\n### Test Procedure and Results\n\nBoth algorithms are initialized with $x^0 = 0$ and run until the relative change in the iterates $\\lVert x^{k+1} - x^k \\rVert_2 / (\\lVert x^k \\rVert_2 + 10^{-12})$ is less than or equal to the tolerance $\\varepsilon = 10^{-6}$. A maximum of $50$ full passes is set for CD, and $500$ iterations for PG. The comparison is performed on three synthetic datasets as specified in the problem, and the ratio of total operations $R = (\\text{PG count}) / (\\text{CD count})$ is computed for each case. The expectation is that CD will be more efficient, especially for sparse solutions, as it can skip expensive residual updates when coordinates remain zero, leading to $R > 1$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import svdvals\n\ndef solve_prox_lp(z, alpha, p, bisection_iters=100):\n    \"\"\"\n    Computes the proximal operator for the Lp penalty.\n    argmin_t { 1/2 * (t - z)^2 + alpha * |t|^p } for p in (0,1).\n    \"\"\"\n    if p  0 or p = 1:\n        raise ValueError(\"p must be in [0, 1)\")\n\n    abs_z = abs(z)\n    if abs_z == 0.0:\n        return 0.0\n\n    # The derivative of the objective (for t0) is phi(t) = t - |z| + alpha*p*t^(p-1)\n    # The minimum of phi(t) occurs at t_min.\n    try:\n        t_min = (alpha * p * (1 - p))**(1.0 / (2 - p))\n    except (ValueError, OverflowError): # Handles large exponents or negative bases\n        return 0.0\n\n    def phi(t, abs_z_val, alpha_val, p_val):\n        # handle t=0 case gracefully, although we call it on t0\n        if t = 1e-15: return np.inf\n        return t - abs_z_val + alpha_val * p_val * t**(p_val - 1)\n\n    phi_at_t_min = phi(t_min, abs_z, alpha, p)\n\n    if phi_at_t_min = 0:\n        # phi(t) = 0 for all t0, so objective is increasing. Minimum is at t=0.\n        return 0.0\n\n    # If phi(t_min)  0, there are two positive roots. We need the larger one, t2.\n    # The bracket for t2 is [t_min, abs_z]\n    low = t_min\n    high = abs_z\n    \n    for _ in range(bisection_iters):\n        mid = (low + high) / 2.0\n        if mid == low or mid == high:\n            break\n        if phi(mid, abs_z, alpha, p)  0:\n            high = mid\n        else:\n            low = mid\n    \n    t2 = (low + high) / 2.0\n\n    # Compare objective value at 0 and t2 to find the global minimum for t=0\n    val_at_0 = 0.5 * abs_z**2\n    val_at_t2 = 0.5 * (t2 - abs_z)**2 + alpha * t2**p\n    \n    if val_at_0 = val_at_t2:\n        return 0.0\n    else:\n        return np.sign(z) * t2\n\ndef solve_prox_lp_vec(z_vec, alpha, p):\n    \"\"\"Vectorized version of the Lp proximal operator.\"\"\"\n    return np.array([solve_prox_lp(z_i, alpha, p) for z_i in z_vec])\n\ndef generate_data(m, n, s, sigma, seed):\n    \"\"\"Generates synthetic data for the sparse regression problem.\"\"\"\n    rng = np.random.default_rng(seed)\n    A = rng.standard_normal((m, n))\n    A /= np.linalg.norm(A, axis=0, keepdims=True)\n    \n    x_star = np.zeros(n)\n    indices = rng.choice(n, s, replace=False)\n    x_star[indices] = rng.standard_normal(s)\n    \n    eta = rng.standard_normal(m) * sigma\n    b = A @ x_star + eta\n    return A, b\n\ndef cd(A, b, p, lambda_, max_passes, tol):\n    \"\"\"Cyclic Coordinate Descent.\"\"\"\n    m, n = A.shape\n    x = np.zeros(n)\n    r = b.copy()  # Initial residual r = b - A@x\n    op_count = 0.0\n\n    # Per problem, columns of A are normalized, so L_j = 1 for all j.\n    L_j_val = 1.0 \n    alpha = lambda_ / L_j_val\n\n    for k in range(max_passes):\n        x_old = x.copy()\n        \n        for j in range(n):\n            aj = A[:, j]\n            \n            op_count += m  # For dot product a_j.T @ r\n            z_j = x[j] + np.dot(aj, r) / L_j_val\n\n            t = solve_prox_lp(z_j, alpha, p)\n            \n            delta_xj = t - x[j]\n            \n            if abs(delta_xj)  1e-15:\n                x[j] = t\n                r -= aj * delta_xj\n                op_count += m  # For residual update\n        \n        rel_change = np.linalg.norm(x - x_old) / (np.linalg.norm(x_old) + 1e-12)\n        if rel_change = tol:\n            break\n            \n    return x, op_count\n\ndef pg(A, b, p, lambda_, max_iters, tol):\n    \"\"\"Proximal Gradient Method.\"\"\"\n    m, n = A.shape\n    x = np.zeros(n)\n    op_count = 0.0\n\n    try:\n        L = svdvals(A)[0]**2\n    except np.linalg.LinAlgError:\n        # Fallback for matrices where SVD might fail, though unlikely with this data\n        U, s, Vh = np.linalg.svd(A, full_matrices=False)\n        L = s[0]**2\n        \n    alpha = lambda_ / L\n    \n    for k in range(max_iters):\n        x_old = x.copy()\n        \n        # Gradient step\n        r = A @ x - b\n        op_count += m * n  # For A @ x\n        grad = A.T @ r\n        op_count += m * n  # For A.T @ r\n        \n        y = x - (1/L) * grad\n        \n        # Proximal step\n        x = solve_prox_lp_vec(y, alpha, p)\n        \n        rel_change = np.linalg.norm(x - x_old) / (np.linalg.norm(x_old) + 1e-12)\n        if rel_change = tol:\n            break\n            \n    return x, op_count\n\ndef solve():\n    \"\"\"Main function to run test cases and print results.\"\"\"\n    test_cases = [\n        # (m, n, s, p, lambda, sigma, seed)\n        (50, 200, 10, 0.5, 0.1, 0.01, 0),\n        (80, 80, 5, 0.9, 0.05, 0.01, 1),\n        (30, 300, 5, 0.2, 0.2, 0.02, 2),\n    ]\n\n    results = []\n    tol = 1e-6\n\n    for i, params in enumerate(test_cases):\n        m, n, s, p, lambda_, sigma, seed = params\n        \n        A, b = generate_data(m, n, s, sigma, seed)\n        \n        _, count_cd = cd(A, b, p, lambda_, max_passes=50, tol=tol)\n        _, count_pg = pg(A, b, p, lambda_, max_iters=500, tol=tol)\n        \n        if count_cd > 0:\n            ratio = count_pg / count_cd\n        else:\n            ratio = float('inf')\n        \n        results.append(ratio)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n\n```", "id": "3437037"}]}