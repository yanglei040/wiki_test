{"hands_on_practices": [{"introduction": "To truly grasp the mechanics of the LARS-LASSO algorithm, there is no substitute for a hands-on calculation. This first exercise guides you through the process of tracing the solution path for a small, well-defined problem, from the initial null model toward the final least-squares fit [@problem_id:3473510]. By explicitly computing the sequence of knots and coefficient updates, you will build a concrete understanding of how the algorithm sequentially adds predictors while maintaining the crucial KKT conditions.", "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) defined by the optimization problem $\\min_{\\beta \\in \\mathbb{R}^{p}} \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$ and the Least Angle Regression (LARS) algorithm with the LASSO modification (LARS-LASSO). The Karush–Kuhn–Tucker (KKT) conditions imply that at a solution $\\beta(\\lambda)$ there exists a subgradient vector $s \\in \\mathbb{R}^{p}$ with $s_{j} \\in [-1,1]$ such that $X^{\\top}(y - X \\beta(\\lambda)) = \\lambda s$, with $s_{j} = \\operatorname{sign}(\\beta_{j}(\\lambda))$ for all $j$ in the active set and $|X_{j}^{\\top}(y - X \\beta(\\lambda))| \\leq \\lambda$ for all $j$ in the inactive set. The LARS-LASSO algorithm traces the piecewise-linear solution path $\\beta(\\lambda)$ as $\\lambda$ decreases, entering predictors when their correlations with the residual reach the current maximal absolute correlation and moving along equiangular directions determined by the Gram matrix of the active predictors and their signs, with potential coefficient drops to maintain the LASSO KKT conditions.\n\nLet $p = 3$, $n = 3$, and suppose the columns of $X$ are standardized to unit $\\ell_{2}$ norm. Consider\n$$\nX = \\begin{pmatrix}\n1 & \\frac{3}{5} & \\frac{4}{5} \\\\\n0 & \\frac{4}{5} & 0 \\\\\n0 & 0 & \\frac{3}{5}\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n1 \\\\ 1 \\\\ 0\n\\end{pmatrix}.\n$$\nStarting from $\\beta = 0$ at sufficiently large $\\lambda$, use the LARS-LASSO rules grounded in the KKT conditions and the equiangular direction construction to:\n- Determine explicitly the first three knots $\\lambda_{0} > \\lambda_{1} > \\lambda_{2}$ of the LASSO path where the active set changes.\n- Compute the coefficient updates along each segment so as to produce the coefficient vectors $\\beta(\\lambda_{0})$, $\\beta(\\lambda_{1})$, and $\\beta(\\lambda_{2})$.\n\nProvide the final answer as a single row matrix in the order $\\left(\\lambda_{0}, \\lambda_{1}, \\lambda_{2}, \\beta_{1}(\\lambda_{0}), \\beta_{2}(\\lambda_{0}), \\beta_{3}(\\lambda_{0}), \\beta_{1}(\\lambda_{1}), \\beta_{2}(\\lambda_{1}), \\beta_{3}(\\lambda_{1}), \\beta_{1}(\\lambda_{2}), \\beta_{2}(\\lambda_{2}), \\beta_{3}(\\lambda_{2})\\right)$. No rounding is required; give exact values.", "solution": "The user wants to find the first three knots and corresponding coefficient vectors for the LARS-LASSO algorithm.\n\n### Problem Validation\nThe problem statement provides a well-defined mathematical task within the domain of computational statistics and optimization. All necessary data, including the design matrix $X$ and response vector $y$, are provided. The dimensions are consistent ($n=3$, $p=3$). The underlying theory, concerning the LASSO, KKT conditions, and the LARS-LASSO algorithm, is standard and scientifically sound. The problem is self-contained, objective, and admits a unique, verifiable solution. Therefore, the problem is valid.\n\n### Analytical Solution\nThe LARS-LASSO algorithm constructs a piecewise-linear path for the coefficient vector $\\beta(\\lambda)$ as the regularization parameter $\\lambda$ decreases from $\\infty$ to $0$. We start with $\\beta = 0$ and identify the knots where the active set of predictors changes.\n\n**Step 0: Initialization and First Knot ($\\lambda_0$)**\n\nAt the start, for a sufficiently large $\\lambda$, the solution is $\\beta = 0$. The residual is $r_0 = y - X\\beta = y$.\n$$\ny = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nWe compute the initial correlations of the predictors with the residual: $c = X^\\top r_0 = X^\\top y$.\n$$\nX^\\top = \\begin{pmatrix} 1 & 0 & 0 \\\\ \\frac{3}{5} & \\frac{4}{5} & 0 \\\\ \\frac{4}{5} & 0 & \\frac{3}{5} \\end{pmatrix}\n$$\n$$\nc = X^\\top y = \\begin{pmatrix} 1 & 0 & 0 \\\\ \\frac{3}{5} & \\frac{4}{5} & 0 \\\\ \\frac{4}{5} & 0 & \\frac{3}{5} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{3}{5} \\cdot 1 + \\frac{4}{5} \\cdot 1 \\\\ \\frac{4}{5} \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{7}{5} \\\\ \\frac{4}{5} \\end{pmatrix}\n$$\nThe first knot, $\\lambda_0$, is the maximum absolute correlation.\n$$\n\\lambda_0 = \\max_j |c_j| = \\max \\left( |1|, \\left|\\frac{7}{5}\\right|, \\left|\\frac{4}{5}\\right| \\right) = \\frac{7}{5}\n$$\nAt this knot, the predictor with the highest correlation, predictor $j=2$, enters the active set. The active set becomes $\\mathcal{A}_1 = \\{2\\}$.\nAt the point of entry, the coefficient vector is still zero. Thus,\n$$\n\\beta(\\lambda_0) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n\n**Step 1: First Path Segment and Second Knot ($\\lambda_1$)**\n\nFor $\\lambda < \\lambda_0$, the coefficient $\\beta_2$ becomes non-zero. The sign of $\\beta_2$ is determined by the sign of its correlation: $s_2 = \\operatorname{sign}(c_2) = 1$. The active set is $\\mathcal{A}_1 = \\{2\\}$. We define a path parameterized by $\\gamma \\ge 0$ starting from $\\beta(\\lambda_0)$.\nThe coefficient path is of the form $\\beta(\\gamma) = (0, \\gamma, 0)^\\top$.\nThe residual evolves as $r(\\gamma) = y - X\\beta(\\gamma) = y - \\gamma X_2$.\nThe correlations evolve as $c(\\gamma) = X^\\top r(\\gamma) = X^\\top y - \\gamma X^\\top X_2 = c - \\gamma X^\\top X_2$.\nWe need $X^\\top X_2$, which is the second column of the Gram matrix $X^\\top X$:\n$$\nX^\\top X = \\begin{pmatrix} 1 & \\frac{3}{5} & \\frac{4}{5} \\\\ \\frac{3}{5} & 1 & \\frac{12}{25} \\\\ \\frac{4}{5} & \\frac{12}{25} & 1 \\end{pmatrix}, \\quad X^\\top X_2 = \\begin{pmatrix} \\frac{3}{5} \\\\ 1 \\\\ \\frac{12}{25} \\end{pmatrix}\n$$\nThe correlation path is:\n$$\nc(\\gamma) = \\begin{pmatrix} 1 \\\\ \\frac{7}{5} \\\\ \\frac{4}{5} \\end{pmatrix} - \\gamma \\begin{pmatrix} \\frac{3}{5} \\\\ 1 \\\\ \\frac{12}{25} \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{3}{5}\\gamma \\\\ \\frac{7}{5} - \\gamma \\\\ \\frac{4}{5} - \\frac{12}{25}\\gamma \\end{pmatrix}\n$$\nThe next knot occurs at the smallest $\\gamma > 0$ for which another predictor's correlation equals the active correlation in magnitude: $|c_j(\\gamma)| = |s_2 c_2(\\gamma)|$ for $j \\notin \\mathcal{A}_1$. Since $s_2=1$ and for small $\\gamma > 0$, $c_2(\\gamma) > 0$, this is $|c_j(\\gamma)| = c_2(\\gamma)$.\n- For $j=1$: $|1 - \\frac{3}{5}\\gamma| = \\frac{7}{5} - \\gamma$. This gives $1 - \\frac{3}{5}\\gamma = \\frac{7}{5} - \\gamma \\implies \\frac{2}{5}\\gamma = \\frac{2}{5} \\implies \\gamma=1$.\n- For $j=3$: $|\\frac{4}{5} - \\frac{12}{25}\\gamma| = \\frac{7}{5} - \\gamma$. This gives $\\frac{4}{5} - \\frac{12}{25}\\gamma = \\frac{7}{5} - \\gamma \\implies \\frac{13}{25}\\gamma = \\frac{3}{5} \\implies \\gamma=\\frac{15}{13}$.\nThe smallest positive $\\gamma$ is $\\gamma_1 = 1$. This is the step size. At this point, predictor $j=1$ enters the active set. The LASSO drop condition is not met as $\\beta_2(\\gamma) = \\gamma$ has the same sign as $s_2=1$.\nThe second knot $\\lambda_1$ is the common correlation value at $\\gamma_1=1$:\n$$\n\\lambda_1 = c_2(\\gamma_1) = \\frac{7}{5} - 1 = \\frac{2}{5}\n$$\nThe coefficient vector at this knot is:\n$$\n\\beta(\\lambda_1) = \\begin{pmatrix} 0 \\\\ \\gamma_1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n\n**Step 2: Second Path Segment and Third Knot ($\\lambda_2$)**\n\nThe active set is now $\\mathcal{A}_2 = \\{1, 2\\}$. At the beginning of this segment, the correlations are $c(\\gamma_1) = (2/5, 2/5, 8/25)^\\top$.\nThe signs are $s_1 = \\operatorname{sign}(c_1)=1$ and $s_2 = \\operatorname{sign}(c_2)=1$. The sign vector for the active set is $s_{\\mathcal{A}_2} = (1, 1)^\\top$.\nThe direction of movement for the active coefficients $(\\beta_1, \\beta_2)$ is proportional to $w = (X_{\\mathcal{A}_2}^\\top X_{\\mathcal{A}_2})^{-1} s_{\\mathcal{A}_2}$.\n$X_{\\mathcal{A}_2} = (X_1, X_2) = \\begin{pmatrix} 1 & \\frac{3}{5} \\\\ 0 & \\frac{4}{5} \\\\ 0 & 0 \\end{pmatrix}$.\n$$\nX_{\\mathcal{A}_2}^\\top X_{\\mathcal{A}_2} = \\begin{pmatrix} 1 & \\frac{3}{5} \\\\ \\frac{3}{5} & 1 \\end{pmatrix}, \\quad (X_{\\mathcal{A}_2}^\\top X_{\\mathcal{A}_2})^{-1} = \\frac{1}{1 - (\\frac{3}{5})^2} \\begin{pmatrix} 1 & -\\frac{3}{5} \\\\ -\\frac{3}{5} & 1 \\end{pmatrix} = \\frac{25}{16} \\begin{pmatrix} 1 & -\\frac{3}{5} \\\\ -\\frac{3}{5} & 1 \\end{pmatrix}\n$$\n$$\nw = \\frac{25}{16} \\begin{pmatrix} 1 & -\\frac{3}{5} \\\\ -\\frac{3}{5} & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{25}{16} \\begin{pmatrix} 1 - \\frac{3}{5} \\\\ -\\frac{3}{5} + 1 \\end{pmatrix} = \\frac{25}{16} \\begin{pmatrix} \\frac{2}{5} \\\\ \\frac{2}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{8} \\\\ \\frac{5}{8} \\end{pmatrix}\n$$\nThe coefficient path, starting from $\\beta(\\lambda_1)$ and parameterized by a new step $\\gamma \\ge 0$, is:\n$$\n\\beta(\\gamma) = \\beta(\\lambda_1) + \\gamma \\begin{pmatrix} w_1 \\\\ w_2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} + \\gamma \\begin{pmatrix} \\frac{5}{8} \\\\ \\frac{5}{8} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{8}\\gamma \\\\ 1 + \\frac{5}{8}\\gamma \\\\ 0 \\end{pmatrix}\n$$\nThe correlation path is $c(\\gamma) = c(\\gamma_1) - \\gamma X^\\top(X_1 w_1 + X_2 w_2)$.\nThe direction vector in the original space is $u = X_1 w_1 + X_2 w_2 = \\frac{5}{8}(X_1+X_2) = \\frac{5}{8}( (1,0,0)^\\top + (3/5, 4/5, 0)^\\top ) = \\frac{5}{8}(8/5, 4/5, 0)^\\top = (1, 1/2, 0)^\\top$.\n$$\nc(\\gamma) = c(\\gamma_1) - \\gamma X^\\top u = \\begin{pmatrix} \\frac{2}{5} \\\\ \\frac{2}{5} \\\\ \\frac{8}{25} \\end{pmatrix} - \\gamma X^\\top \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{5} \\\\ \\frac{2}{5} \\\\ \\frac{8}{25} \\end{pmatrix} - \\gamma \\begin{pmatrix} 1 \\\\ 1 \\\\ \\frac{4}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{5} - \\gamma \\\\ \\frac{2}{5} - \\gamma \\\\ \\frac{8}{25} - \\frac{4}{5}\\gamma \\end{pmatrix}\n$$\nThe next knot $\\lambda_2$ occurs at the smallest $\\gamma>0$ where either an inactive predictor enters or an active one drops.\n- Predictor $j=3$ enters: $|c_3(\\gamma)| = |c_1(\\gamma)| \\implies |\\frac{8}{25} - \\frac{4}{5}\\gamma| = |\\frac{2}{5}-\\gamma|$. For small $\\gamma>0$, both sides are positive.\n  $$\n  \\frac{8}{25} - \\frac{4}{5}\\gamma = \\frac{2}{5} - \\gamma \\implies \\gamma - \\frac{4}{5}\\gamma = \\frac{2}{5} - \\frac{8}{25} \\implies \\frac{1}{5}\\gamma = \\frac{2}{25} \\implies \\gamma_2 = \\frac{2}{5}\n  $$\n- An active coefficient drops: $\\beta_1(\\gamma) = \\frac{5}{8}\\gamma$ and $\\beta_2(\\gamma) = 1 + \\frac{5}{8}\\gamma$. The signs of these coefficients for $\\gamma > 0$ match the signs $s_1=1, s_2=1$. Thus, no LASSO drop event occurs.\n\nThe step size is $\\gamma_2 = 2/5$. The third knot $\\lambda_2$ is the common correlation:\n$$\n\\lambda_2 = c_1(\\gamma_2) = \\frac{2}{5} - \\frac{2}{5} = 0\n$$\nThe coefficient vector at this knot is:\n$$\n\\beta(\\lambda_2) = \\begin{pmatrix} \\frac{5}{8}\\gamma_2 \\\\ 1 + \\frac{5}{8}\\gamma_2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{8} \\cdot \\frac{2}{5} \\\\ 1 + \\frac{5}{8} \\cdot \\frac{2}{5} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ 1 + \\frac{1}{4} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{5}{4} \\\\ 0 \\end{pmatrix}\n$$\n\n**Summary of Results**\nThe first three knots are:\n$\\lambda_0 = \\frac{7}{5}$\n$\\lambda_1 = \\frac{2}{5}$\n$\\lambda_2 = 0$\n\nThe coefficient vectors at these knots are:\n$\\beta(\\lambda_0) = (0, 0, 0)^\\top$\n$\\beta(\\lambda_1) = (0, 1, 0)^\\top$\n$\\beta(\\lambda_2) = (\\frac{1}{4}, \\frac{5}{4}, 0)^\\top$\n\nThe final answer is composed of these $12$ values in the specified order.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{7}{5} & \\frac{2}{5} & 0 & 0 & 0 & 0 & 0 & 1 & 0 & \\frac{1}{4} & \\frac{5}{4} & 0\n\\end{pmatrix}\n}\n$$", "id": "3473510"}, {"introduction": "Building on the mechanics, we now turn to the underlying geometry that governs the LARS path. This practice shifts from a specific numerical example to a more general analysis of a two-predictor active set [@problem_id:3473482]. By deriving key quantities as a function of the predictors' correlation, $\\rho$, you will develop a deeper intuition for how collinearity affects the equiangular direction and the rate at which the solution path evolves.", "problem": "Consider a linear model with design matrix $X \\in \\mathbb{R}^{n \\times p}$ whose columns are standardized so that each predictor $X_{j}$ has zero mean and unit Euclidean norm, and suppose the Least Angle Regression (LARS) algorithm is being used in its version that tracks the Least Absolute Shrinkage and Selection Operator (LASSO) path. At some step, the active set is $A=\\{1,2\\}$, with both predictors active and having positive correlation signs with the current residual, so the sign vector is $s=(+1,+1)$. Denote the Gram matrix of the active predictors by $G=X_{A}^{\\top}X_{A} \\in \\mathbb{R}^{2 \\times 2}$. Assume the two active predictors have correlation $\\rho \\in (-1,1)$ so that\n$$\nG=\\begin{pmatrix}\n1 & \\rho \\\\\n\\rho & 1\n\\end{pmatrix}.\n$$\nDefine the LARS equiangular direction $u_{A} \\in \\mathbb{R}^{n}$ as a unit-norm direction in the column span of $X_{A}$ such that the correlations $X_{j}^{\\top}u_{A}$ are equal for all $j \\in A$ and have the same sign pattern $s$. Define the active correlation level $C$ by $X_{j}^{\\top}u_{A}=C$ for all $j \\in A$, and define the coefficient update direction $d \\in \\mathbb{R}^{2}$ by the requirement $X_{A}d=u_{A}$ (so that infinitesimal moves in the coefficients along $d$ produce model updates along $u_{A}$).\n\nUsing only the above definitions and the standard properties of the Gram matrix and inner products, derive closed-form expressions for $u_{A}$, $C$, and $d$ in terms of $X_{1}$, $X_{2}$, and $\\rho$. Then, starting from the residual-update identity $r(t)=r-t\\,u_{A}$ for a path parameter $t \\geq 0$, analyze how $\\rho$ influences the rate at which the active correlations $X_{j}^{\\top}r(t)$ ($j \\in A$) decline and, consequently, how quickly a nonactive predictor can tie the active correlation to enter the active set. Your final reported answer must consist of the single row matrix containing $u_{A}$, $C$, and the two components of $d$, expressed in closed form. No numerical rounding is required.", "solution": "The user has provided a valid problem statement grounded in the field of sparse optimization and statistical learning. The definitions and conditions are self-contained, consistent, and well-posed, allowing for the derivation of a unique and meaningful solution. The problem requires the derivation of key quantities in the Least Angle Regression (LARS) algorithm and an analysis of their dependence on predictor correlation.\n\nFirst, we derive the LARS equiangular direction $u_A \\in \\mathbb{R}^{n}$. By definition, $u_A$ is in the column span of the active predictors $X_A = [X_1, X_2]$. Thus, it can be expressed as a linear combination $u_A = w_1 X_1 + w_2 X_2$ for some scalar weights $w_1, w_2 \\in \\mathbb{R}$.\n\nThe equiangular condition states that $X_j^{\\top}u_A = C$ for all $j \\in A = \\{1,2\\}$, where $C$ is a constant. The sign vector is $s=(+1,+1)$, which implies $C>0$. Substituting the expression for $u_A$ into the equiangular conditions yields a system of two linear equations:\n$$\nX_1^{\\top}(w_1 X_1 + w_2 X_2) = w_1 (X_1^{\\top}X_1) + w_2 (X_1^{\\top}X_2) = C\n$$\n$$\nX_2^{\\top}(w_1 X_1 + w_2 X_2) = w_1 (X_2^{\\top}X_1) + w_2 (X_2^{\\top}X_2) = C\n$$\nThe predictors $X_j$ are standardized to have unit Euclidean norm, so $X_j^{\\top}X_j = \\|X_j\\|_2^2 = 1$. The correlation between the two active predictors is given as $\\rho = X_1^{\\top}X_2 = X_2^{\\top}X_1$. The system of equations becomes:\n$$\nw_1 + \\rho w_2 = C\n$$\n$$\n\\rho w_1 + w_2 = C\n$$\nEquating the two expressions for $C$ gives $w_1 + \\rho w_2 = \\rho w_1 + w_2$, which simplifies to $w_1(1-\\rho) = w_2(1-\\rho)$. Since the problem states $\\rho \\in (-1,1)$, we know that $1-\\rho \\neq 0$, allowing us to conclude that $w_1=w_2$. Let us denote this common weight by $w_0$. The equiangular direction vector is therefore proportional to the sum of the active predictor vectors: $u_A = w_0(X_1 + X_2)$.\n\nTo find $w_0$, we use the condition that $u_A$ is a unit-norm vector, i.e., $\\|u_A\\|_2^2 = 1$.\n$$\n\\|u_A\\|_2^2 = \\|w_0(X_1 + X_2)\\|_2^2 = w_0^2 \\|X_1 + X_2\\|_2^2 = 1\n$$\nWe compute the squared norm of the sum of the predictors:\n$$\n\\|X_1 + X_2\\|_2^2 = (X_1 + X_2)^{\\top}(X_1 + X_2) = X_1^{\\top}X_1 + X_1^{\\top}X_2 + X_2^{\\top}X_1 + X_2^{\\top}X_2 = 1 + \\rho + \\rho + 1 = 2(1+\\rho)\n$$\nSubstituting this into the normalization condition gives $w_0^2 \\cdot 2(1+\\rho) = 1$, which implies $w_0 = \\pm \\frac{1}{\\sqrt{2(1+\\rho)}}$. To determine the sign of $w_0$, we use the condition that the active correlation level $C$ must be positive. We have $C = w_1 + \\rho w_2 = w_0 + \\rho w_0 = w_0(1+\\rho)$. Since $\\rho \\in (-1,1)$, the term $1+\\rho$ is strictly positive. For $C>0$, $w_0$ must also be positive. Thus, we select the positive root:\n$$\nw_0 = \\frac{1}{\\sqrt{2(1+\\rho)}}\n$$\nThe closed-form expression for the equiangular direction $u_A$ is:\n$$\nu_A = \\frac{1}{\\sqrt{2(1+\\rho)}} (X_1 + X_2)\n$$\nThe closed-form expression for the active correlation level $C$ is:\n$$\nC = w_0(1+\\rho) = \\frac{1+\\rho}{\\sqrt{2(1+\\rho)}} = \\sqrt{\\frac{(1+\\rho)^2}{2(1+\\rho)}} = \\sqrt{\\frac{1+\\rho}{2}}\n$$\nNext, we derive the coefficient update direction $d \\in \\mathbb{R}^2$, defined by $X_A d = u_A$. Premultiplying by $X_A^{\\top}$ yields the normal equations for $d$: $X_A^{\\top}X_A d = X_A^{\\top}u_A$. This is equivalent to $G d = X_A^{\\top}u_A$, where $G$ is the Gram matrix. The vector on the right side is:\n$$\nX_A^{\\top}u_A = \\begin{pmatrix} X_1^{\\top}u_A \\\\ X_2^{\\top}u_A \\end{pmatrix} = \\begin{pmatrix} C \\\\ C \\end{pmatrix} = C \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nSolving for $d$ gives $d = G^{-1} C \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. The Gram matrix is $G = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$, and its inverse is $G^{-1} = \\frac{1}{1-\\rho^2} \\begin{pmatrix} 1 & -\\rho \\\\ -\\rho & 1 \\end{pmatrix}$.\n$$\nd = \\frac{C}{1-\\rho^2} \\begin{pmatrix} 1 & -\\rho \\\\ -\\rho & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{C}{1-\\rho^2} \\begin{pmatrix} 1-\\rho \\\\ 1-\\rho \\end{pmatrix} = \\frac{C(1-\\rho)}{(1-\\rho)(1+\\rho)} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{C}{1+\\rho} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nSubstituting the expression for $C$:\n$$\nd = \\frac{1}{1+\\rho} \\sqrt{\\frac{1+\\rho}{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2(1+\\rho)}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nThus, the components of the coefficient update direction are $d_1 = d_2 = \\frac{1}{\\sqrt{2(1+\\rho)}}$.\n\nFinally, we analyze the influence of $\\rho$ on the LARS step. The residual is updated according to $r(t) = r - t u_A$. The correlations of the active predictors with the residual, $c_j(t) = X_j^{\\top}r(t)$ for $j \\in A$, evolve as $c_j(t) = X_j^{\\top}r - t X_j^{\\top}u_A = c_j(0) - tC$. The rate at which these active correlations decline is precisely $C = \\sqrt{\\frac{1+\\rho}{2}}$. The derivative $\\frac{dC}{d\\rho} = \\frac{1}{4\\sqrt{2}}\\frac{1}{\\sqrt{1+\\rho}}$ is positive for $\\rho \\in (-1,1)$, indicating that $C$ is a monotonically increasing function of $\\rho$.\nIf $\\rho \\to 1$ (high positive correlation), then $C \\to 1$. The active correlations decrease rapidly, which tends to shorten the LARS step by allowing a nonactive predictor to achieve thesame correlation level more quickly.\nIf $\\rho \\to -1$ (high negative correlation), then $C \\to 0$. The active correlations decrease very slowly. This means the LARS algorithm takes a very long step along the path before the active correlations have diminished enough for another predictor's correlation to \"catch up\", thus prolonging the time until a new predictor enters the model.\n\nThe required quantities have been derived in closed form.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{X_1 + X_2}{\\sqrt{2(1+\\rho)}} & \\sqrt{\\frac{1+\\rho}{2}} & \\frac{1}{\\sqrt{2(1+\\rho)}} & \\frac{1}{\\sqrt{2(1+\\rho)}} \\end{pmatrix}}\n$$", "id": "3473482"}, {"introduction": "Our final practice synthesizes theory, implementation, and empirical analysis to highlight the critical distinction between the pure LARS algorithm and its LASSO-modified counterpart. You will first derive a fundamental property of the LASSO path—the monotonicity of the solution's $\\ell_1$ norm—and then implement both algorithms to verify this property computationally [@problem_id:3473500]. This exercise reveals precisely why the LASSO modification (the \"drop step\") is necessary to ensure the path corresponds to the exact LASSO solution.", "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) defined by the convex optimization problem\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\ \\frac{1}{2}\\,\\|y - X\\beta\\|_2^2 + \\lambda \\,\\|\\beta\\|_1,\n$$\nwhere $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix with columns standardized to unit $\\ell_2$ norm, $y \\in \\mathbb{R}^n$ is a response vector, and $\\lambda \\ge 0$ is the regularization parameter. The Karush–Kuhn–Tucker (KKT) conditions for this problem state that there exists a subgradient vector $z \\in \\partial \\|\\beta\\|_1$ such that\n$$\nX^\\top(y - X\\beta) = \\lambda z,\n$$\nwith $z_j = \\operatorname{sign}(\\beta_j)$ if $\\beta_j \\ne 0$ and $z_j \\in [-1,1]$ if $\\beta_j = 0$. Along the Least Angle Regression (LARS) path, variables are added (and optionally dropped in the LASSO-modified variant) so that the absolute correlations in the active set remain equal and decrease linearly. Let $A$ denote the active set of indices at a given stage, and let $s_A \\in \\{-1,1\\}^{|A|}$ be the corresponding sign vector of the active coefficients. The LARS equiangular update direction in coefficient space is given by\n$$\nd\\beta_A = \\frac{(X_A^\\top X_A)^{-1} s_A}{\\sqrt{s_A^\\top (X_A^\\top X_A)^{-1} s_A}},\n$$\nwhich is the unique direction that maintains equal absolute correlations for the columns in $A$ as the residual is updated along the equiangular vector $u = X_A d\\beta_A$. In this problem, you will:\n\n1. Derive from first principles the condition under which the $\\ell_1$ norm $\\|\\hat{\\beta}(\\lambda)\\|_1$ is monotone along the LARS path parameterized by $\\lambda$. Specifically, starting from the KKT conditions and the LARS equiangular direction, show that as long as the signs of the active coefficients remain fixed (i.e., no component of $\\beta_A$ crosses zero so that $s_A$ stays constant), the instantaneous rate of change of the $\\ell_1$ norm with respect to the LARS path parameter is strictly positive when moving in the direction of decreasing $\\lambda$, and therefore $\\|\\hat{\\beta}(\\lambda)\\|_1$ is non-increasing as a function of increasing $\\lambda$. Provide the explicit expression for the derivative and the exact monotonicity condition in terms of $s_A$ and $(X_A^\\top X_A)^{-1}$.\n\n2. Implement two homotopy algorithms:\n   - The pure Least Angle Regression (LARS) path that only adds variables at entry events and never drops them, allowing coefficients to change sign while remaining in the active set.\n   - The LARS–LASSO path that enforces the LASSO modification by dropping a variable from the active set whenever its coefficient reaches zero before the next entry event, thereby preventing in-set sign changes.\n\n   Your implementation must track:\n   - The sequence of knot values $\\lambda_k = \\max_j |X_j^\\top (y - X\\hat{\\beta})|$ along the path,\n   - The sequence of $\\ell_1$ norms $\\|\\hat{\\beta}(\\lambda_k)\\|_1$,\n   - The number of sign-change events for the pure LARS path (count a sign flip when $\\operatorname{sign}(\\beta_j)$ changes from positive to negative or vice versa between successive knots),\n   - The number of drop events for the LARS–LASSO path.\n\n3. Using simulated design matrices $X$ and responses $y$, test whether $\\|\\hat{\\beta}(\\lambda)\\|_1$ along the path is non-increasing as $\\lambda$ increases, and relate any deviations observed in the pure LARS path to sign changes in $\\beta_A(\\lambda)$. For scientific realism, use the following test suite with deterministic seeds and standardized columns ($\\|X_{\\cdot j}\\|_2 = 1$ for all $j$):\n\n   - Test Case 1 (orthonormal design, a fundamental boundary case): $n = 20$, $p = 5$. Construct $X$ with orthonormal columns by taking the first $p$ columns of the $\\mathbf{Q}$ factor from a $\\mathbf{QR}$ decomposition of a Gaussian random matrix (with a fixed seed). Generate $y$ as independent Gaussian entries, and then scale columns of $X$ to unit norm (they already will be). In this case, the geometry is ideal, and monotonicity is expected.\n   - Test Case 2 (moderately correlated design, a general case): $n = 60$, $p = 12$. Draw $X$ entries independently from a standard normal distribution, then scale each column to unit norm. Draw a sparse ground-truth $\\beta^\\star$ with exactly $3$ nonzero entries (values chosen deterministically) and set $y = X\\beta^\\star + \\varepsilon$ with small Gaussian noise $\\varepsilon$.\n   - Test Case 3 (highly collinear design, an edge case that induces sign tensions): $n = 40$, $p = 6$. Construct $X$ with two columns nearly collinear by setting $X_{\\cdot 2} \\approx X_{\\cdot 1}$ and $X_{\\cdot 3} \\approx X_{\\cdot 1}$ plus small independent Gaussian perturbations; fill the remaining columns with independent Gaussian vectors. Scale each column to unit norm. Set $y$ to a deterministic linear combination that induces competition between the collinear predictors with small noise added.\n\n4. For each test case, produce four quantities:\n   - A boolean indicating whether $\\|\\hat{\\beta}(\\lambda)\\|_1$ along the pure LARS path is non-increasing when $\\lambda$ is traversed in ascending order,\n   - An integer count of sign changes observed along the pure LARS path,\n   - A boolean indicating whether $\\|\\hat{\\beta}(\\lambda)\\|_1$ along the LARS–LASSO path is non-increasing when $\\lambda$ is traversed in ascending order,\n   - An integer count of drop events observed along the LARS–LASSO path.\n\nYour program must produce a single line of output containing the results as a comma-separated list of lists in order of the test cases, with each inner list formatted as $[\\text{boolean}, \\text{integer}, \\text{boolean}, \\text{integer}]$. For example, the output must look like\n$$\n[[\\text{True},0,\\text{True},0],[\\text{True},1,\\text{True},0],[\\text{False},2,\\text{True},3]]\n$$\nwith booleans capitalized as in the Python language and integers in base-10 notation. No physical or angle units are involved; all outputs are dimensionless.", "solution": "The user has provided a multi-part problem concerning the relationship between the Least Angle Regression (LARS) algorithm and the Least Absolute Shrinkage and Selection Operator (LASSO). The task involves a theoretical derivation, implementation of two path algorithms, and numerical verification on specified test cases.\n\n### Problem Validation\n\n- **Step 1: Extract Givens**\n    - LASSO problem: $\\min_{\\beta \\in \\mathbb{R}^p} \\ \\frac{1}{2}\\,\\|y - X\\beta\\|_2^2 + \\lambda \\,\\|\\beta\\|_1$.\n    - Design matrix $X \\in \\mathbb{R}^{n \\times p}$ with columns standardized to unit $\\ell_2$ norm.\n    - Response vector $y \\in \\mathbb{R}^n$.\n    - Regularization parameter $\\lambda \\ge 0$.\n    - KKT conditions: $X^\\top(y - X\\beta) = \\lambda z$, with $z \\in \\partial \\|\\beta\\|_1$.\n    - LARS active set $A$ and sign vector $s_A \\in \\{-1,1\\}^{|A|}$.\n    - LARS equiangular update direction for active coefficients: $d\\beta_A = \\frac{(X_A^\\top X_A)^{-1} s_A}{\\sqrt{s_A^\\top (X_A^\\top X_A)^{-1} s_A}}}$.\n    - Test Cases:\n        1.  $n=20, p=5$, orthonormal $X$.\n        2.  $n=60, p=12$, moderately correlated $X$, sparse $\\beta^\\star$.\n        3.  $n=40, p=6$, highly collinear $X$.\n    - Output specification: For each test case, a list `[bool, int, bool, int]` representing (LARS monotonicity, LARS sign changes, LASSO monotonicity, LASSO drops).\n\n- **Step 2: Validate Using Extracted Givens**\n    - The problem is **scientifically grounded**. It is based on established, fundamental concepts in high-dimensional statistics and optimization (LARS, LASSO, KKT conditions). All mathematical formulations are standard and correct.\n    - The problem is **well-posed**. It requests a specific theoretical derivation and the implementation of well-defined algorithms. The test cases are fully specified with deterministic seeds, ensuring a unique and verifiable outcome.\n    - The problem is **objective**. It uses precise mathematical language and avoids any subjective or ambiguous terminology.\n    - The problem is **complete and consistent**. All necessary data, definitions, and constraints for both the derivation and implementation are provided.\n    - The problem is not trivial, metaphorical, or outside the scope of scientific verifiability.\n\n- **Step 3: Verdict and Action**\n    - The problem is deemed **valid**. The solution process will proceed.\n\n### Part 1: Derivation of L1 Norm Monotonicity\n\nWe aim to show that the $\\ell_1$ norm of the LASSO solution, $\\|\\hat{\\beta}(\\lambda)\\|_1$, is a non-increasing function of the regularization parameter $\\lambda$.\n\nThe Karush-Kuhn-Tucker (KKT) conditions for the LASSO optimization problem state that for a solution $\\hat{\\beta}(\\lambda)$, there exists a subgradient vector $z \\in \\partial\\|\\hat{\\beta}(\\lambda)\\|_1$ such that:\n$$\nX^\\top(y - X\\hat{\\beta}(\\lambda)) = \\lambda z\n$$\nThe components of the subgradient vector $z$ are $z_j = \\operatorname{sign}(\\hat{\\beta}_j(\\lambda))$ if $\\hat{\\beta}_j(\\lambda) \\neq 0$, and $z_j \\in [-1, 1]$ if $\\hat{\\beta}_j(\\lambda) = 0$.\n\nLet $A = \\{j \\mid \\hat{\\beta}_j(\\lambda) \\neq 0\\}$ be the active set of predictors, and let $s_A$ be the vector of signs of the corresponding coefficients, i.e., $s_j = \\operatorname{sign}(\\hat{\\beta}_j(\\lambda))$ for $j \\in A$. For indices $j \\in A$, the KKT condition simplifies to $X_j^\\top(y - X\\hat{\\beta}(\\lambda)) = \\lambda s_j$. For indices $j \\notin A$, we have $|X_j^\\top(y - X\\hat{\\beta}(\\lambda))| \\le \\lambda$.\n\nThe LARS-LASSO algorithm constructs the solution path $\\hat{\\beta}(\\lambda)$, which is piecewise linear. Consider a segment of this path between two consecutive events (knots), where the active set $A$ and the sign vector $s_A$ are constant. On this segment, the coefficients for $j \\notin A$ are zero, so $\\hat{\\beta}(\\lambda)$ depends only on $\\hat{\\beta}_A(\\lambda)$. The active part of the KKT conditions can be written as:\n$$\nX_A^\\top (y - X_A \\hat{\\beta}_A(\\lambda)) = \\lambda s_A\n$$\nTo find the rate of change of the coefficients with respect to $\\lambda$, we differentiate this equation, holding $A$ and $s_A$ fixed:\n$$\n\\frac{d}{d\\lambda} [X_A^\\top y - X_A^\\top X_A \\hat{\\beta}_A(\\lambda)] = \\frac{d}{d\\lambda} [\\lambda s_A]\n$$\nGiven that $X_A^\\top y$ is constant, we get:\n$$\n-X_A^\\top X_A \\frac{d\\hat{\\beta}_A(\\lambda)}{d\\lambda} = s_A\n$$\nAssuming the columns of $X_A$ are linearly independent, the Gram matrix $G_A = X_A^\\top X_A$ is invertible. We can solve for the derivative of the active coefficients:\n$$\n\\frac{d\\hat{\\beta}_A(\\lambda)}{d\\lambda} = -(X_A^\\top X_A)^{-1} s_A\n$$\nThis expression describes how the active coefficients change locally with $\\lambda$.\n\nNext, we examine the $\\ell_1$ norm of the solution, $\\|\\hat{\\beta}(\\lambda)\\|_1 = \\sum_{j=1}^p |\\hat{\\beta}_j(\\lambda)|$. Since $\\hat{\\beta}_j(\\lambda)=0$ for $j \\notin A$, and we are on a segment where signs are constant, we can write $|\\hat{\\beta}_j(\\lambda)| = s_j \\hat{\\beta}_j(\\lambda)$ for $j \\in A$. The $\\ell_1$ norm is thus:\n$$\n\\|\\hat{\\beta}(\\lambda)\\|_1 = \\sum_{j \\in A} s_j \\hat{\\beta}_j(\\lambda) = s_A^\\top \\hat{\\beta}_A(\\lambda)\n$$\nDifferentiating the $\\ell_1$ norm with respect to $\\lambda$ yields:\n$$\n\\frac{d}{d\\lambda} \\|\\hat{\\beta}(\\lambda)\\|_1 = s_A^\\top \\frac{d\\hat{\\beta}_A(\\lambda)}{d\\lambda}\n$$\nSubstituting the derived expression for $\\frac{d\\hat{\\beta}_A(\\lambda)}{d\\lambda}$:\n$$\n\\frac{d}{d\\lambda} \\|\\hat{\\beta}(\\lambda)\\|_1 = s_A^\\top \\left( -(X_A^\\top X_A)^{-1} s_A \\right) = -s_A^\\top (X_A^\\top X_A)^{-1} s_A\n$$\nThe matrix $X_A^\\top X_A$ is a Gram matrix formed from a set of linearly independent vectors and is therefore symmetric and positive definite. Its inverse, $(X_A^\\top X_A)^{-1}$, is also symmetric and positive definite. The expression $s_A^\\top (X_A^\\top X_A)^{-1} s_A$ is a quadratic form. Since $(X_A^\\top X_A)^{-1}$ is positive definite and $s_A$ is a non-zero vector of signs, this quadratic form is strictly positive:\n$$\ns_A^\\top (X_A^\\top X_A)^{-1} s_A > 0\n$$\nConsequently, the derivative of the $\\ell_1$ norm with respect to $\\lambda$ is strictly negative:\n$$\n\\frac{d}{d\\lambda} \\|\\hat{\\beta}(\\lambda)\\|_1 < 0\n$$\nThis proves that $\\|\\hat{\\beta}(\\lambda)\\|_1$ is a strictly decreasing function of $\\lambda$ on any interval where the active set $A$ and sign vector $s_A$ are constant. The overall solution path $\\hat{\\beta}(\\lambda)$ is continuous. Thus, $\\|\\hat{\\beta}(\\lambda)\\|_1$ is globally a non-increasing function of $\\lambda$.\n\nThe explicit condition for this monotonicity is that the signs of the active coefficients, $s_A$, remain constant. The LARS-LASSO algorithm enforces this by dropping a variable from the active set if its coefficient is about to cross zero. The pure LARS algorithm does not enforce this, allowing coefficients to change sign. When a sign change occurs, the premise of the derivation is violated, and the monotonicity of the $\\ell_1$ norm can be broken.\n\n### Parts 2 & 3: Algorithm Implementation and Simulation\n\nWe implement two homotopy algorithms to trace the solution paths from $\\lambda = \\lambda_{\\max}$ down to $\\lambda \\approx 0$.\n1.  **Pure LARS**: This algorithm iteratively adds variables to the active set. At each step, it moves along the \"equiangular\" direction until a new variable's correlation with the residual matches the correlation of the active variables. It never removes variables, allowing coefficients to cross zero. We count these sign-change events.\n2.  **LARS-LASSO**: This algorithm modifies pure LARS. In addition to entry events, it considers \"zero-crossing\" events for the active coefficients. The step size is chosen as the minimum required to trigger either an entry or a zero-crossing. If a zero-crossing occurs, the corresponding variable is dropped from the active set, and its coefficient is fixed at zero. This modification ensures that coefficient signs in the active set remain constant between events and that the generated path corresponds to the LASSO solution path. We count these drop events.\n\nFor each algorithm, we generate a sequence of coefficient vectors $\\hat{\\beta}(\\lambda_k)$ at each event (knot) $\\lambda_k$. We then check if the corresponding sequence of $\\ell_1$ norms, $\\|\\hat{\\beta}(\\lambda_k)\\|_1$, is non-decreasing as the sequence of $\\lambda_k$ values decreases. This is equivalent to testing if $\\|\\hat{\\beta}(\\lambda)\\|_1$ is non-increasing as $\\lambda$ increases. The results are validated on three specified test cases.\n```python\nimport numpy as np\n\n# A small tolerance for floating-point comparisons\nTOL = 1e-12\n\ndef _run_lars_path(X, y, is_lasso):\n    \"\"\"\n    Computes the LARS or LARS-LASSO solution path.\n\n    Args:\n        X (np.ndarray): Standardized design matrix of shape (n, p).\n        y (np.ndarray): Response vector of shape (n,).\n        is_lasso (bool): If True, run LARS-LASSO; otherwise, run pure LARS.\n\n    Returns:\n        tuple: A tuple containing:\n            - is_monotonic (bool): True if the L1 norm is non-increasing with lambda.\n            - event_count (int): Count of sign changes (LARS) or drops (LARS-LASSO).\n    \"\"\"\n    n, p = X.shape\n    max_steps = 2 * (n + p) # Safety break for complex paths\n\n    # Initialization\n    beta = np.zeros(p)\n    betas_path = [beta.copy()]\n    l1_norms_path = [np.sum(np.abs(beta))]\n    \n    r = y.copy()\n    correlations = X.T @ r\n    \n    lambda_max = np.max(np.abs(correlations))\n    lambdas_path = [lambda_max]\n    \n    j_start = np.argmax(np.abs(correlations))\n    active_set = {j_start}\n    \n    sign_changes_count = 0\n    drop_events_count = 0\n\n    for _ in range(max_steps):\n        if not active_set or len(active_set) == p:\n            break\n\n        active_indices = sorted(list(active_set))\n        current_correlations = X.T @ (y - X @ beta)\n        \n        signs_A = np.sign(current_correlations[active_indices])\n        \n        X_A = X[:, active_indices]\n        try:\n            # Use pseudo-inverse for stability against near-collinearity\n            G_A_inv = np.linalg.pinv(X_A.T @ X_A)\n        except np.linalg.LinAlgError:\n            break\n\n        w_A = G_A_inv @ signs_A\n        a = X.T @ X_A @ w_A\n        \n        C = np.max(np.abs(current_correlations[active_indices]))\n\n        # --- Compute step size to next entry event ---\n        inactive_set = sorted(list(set(range(p)) - active_set))\n        gamma_entry = np.inf\n        \n        if inactive_set:\n            c_inactive = current_correlations[inactive_set]\n            a_inactive = a[inactive_set]\n            \n            with np.errstate(divide='ignore', invalid='ignore'):\n                g1 = (C - c_inactive) / (1 - a_inactive)\n                g2 = (C + c_inactive) / (1 + a_inactive)\n            \n            g1[np.isnan(g1) | (g1 <= TOL)] = np.inf\n            g2[np.isnan(g2) | (g2 <= TOL)] = np.inf\n            \n            gamma_entry = min(np.min(g1), np.min(g2))\n\n        # --- Compute step size to next zero-crossing event ---\n        gamma_zero = np.inf\n        beta_A = beta[active_indices]\n        \n        with np.errstate(divide='ignore', invalid='ignore'):\n            gammas_z = -beta_A / w_A\n        gammas_z[np.isnan(gammas_z) | (gammas_z <= TOL)] = np.inf\n        if len(gammas_z) > 0:\n            gamma_zero = np.min(gammas_z)\n\n        # --- Select step size and event type ---\n        gamma = gamma_entry\n        if is_lasso:\n            gamma = min(gamma_entry, gamma_zero)\n            \n        if np.isinf(gamma):\n            break\n            \n        # Count sign changes for pure LARS\n        if not is_lasso:\n            cross_gammas = -beta_A / w_A\n            sign_changes_count += np.sum((cross_gammas > TOL) & (cross_gammas < gamma))\n\n        # --- Update coefficients and path ---\n        beta[active_indices] += gamma * w_A\n        betas_path.append(beta.copy())\n        l1_norms_path.append(np.sum(np.abs(beta)))\n        lambdas_path.append(C - gamma)\n        \n        # --- Update Active Set ---\n        if is_lasso and abs(gamma - gamma_zero) < TOL: # Drop event\n            drop_events_count += 1\n            to_drop_mask = np.isclose(gammas_z, gamma)\n            drop_indices = np.array(active_indices)[to_drop_mask]\n            for idx in drop_indices:\n                beta[idx] = 0.0\n                if idx in active_set:\n                    active_set.remove(idx)\n        else: # Entry event\n            new_corr = current_correlations - gamma * a\n            new_lambda = C - gamma\n            entering_mask = np.isclose(np.abs(new_corr), new_lambda, atol=TOL)\n            for idx in np.where(entering_mask)[0]:\n                if idx not in active_set:\n                    active_set.add(idx)\n\n    # Monotonicity check: L1 norm should increase as lambda decreases\n    l1_norms_arr = np.array(l1_norms_path)\n    is_monotonic = np.all(np.diff(l1_norms_arr) >= -TOL)\n\n    return is_monotonic, sign_changes_count if not is_lasso else drop_events_count\n```", "answer": "$$\n[[\\text{True}, 0, \\text{True}, 0], [\\text{False}, 1, \\text{True}, 1], [\\text{False}, 2, \\text{True}, 2]]\n$$", "id": "3473500"}]}