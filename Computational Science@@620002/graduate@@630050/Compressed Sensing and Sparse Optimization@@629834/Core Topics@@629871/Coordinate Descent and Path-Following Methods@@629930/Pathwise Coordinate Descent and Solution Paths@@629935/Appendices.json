{"hands_on_practices": [{"introduction": "Pathwise coordinate descent tackles the complex, high-dimensional LASSO problem by breaking it down into a sequence of simple, one-dimensional optimizations. This first practice drills down into the core mechanism of the algorithm. By deriving and applying the update for a single coordinate, you will gain a concrete understanding of the soft-thresholding operator, which is the fundamental building block for all subsequent analysis [@problem_id:3465818].", "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) problem of minimizing the objective $$\\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\|x\\|_{1},$$ where $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, $x \\in \\mathbb{R}^{n}$, and $\\lambda > 0$. The pathwise coordinate descent method updates one coordinate $x_{j}$ at a time while keeping the others fixed. Let the current iterate be $x$ with residual $r = y - A x$. For a fixed coordinate index $j$, the coordinate descent update replaces $x_{j}$ by the unique minimizer of the one-dimensional subproblem obtained by varying only $x_{j}$ in the LASSO objective, and then updates the residual accordingly.\n\nStarting from first principles of convex optimization and subdifferential optimality for separable one-dimensional convex functions, derive the coordinate-wise minimizer for $x_{j}$ in closed form and perform a single coordinate update for the specific instance\n$$A = \\begin{pmatrix}\n1 & 0 & 2 & -1 \\\\\n0 & 1 & 1 & 1 \\\\\n1 & -1 & 0 & 2\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n2 \\\\ -1 \\\\ 3\n\\end{pmatrix}, \\quad\nx = \\begin{pmatrix}\n1 \\\\ 0 \\\\ -1 \\\\ \\frac{1}{2}\n\\end{pmatrix}, \\quad\n\\lambda = 1,$$\nupdating the coordinate with index $j = 2$ (second coordinate). Compute the updated residual $r$ after this single-coordinate update and the coordinate-wise Karush-Kuhn-Tucker (KKT) residual for index $j$. The coordinate-wise KKT residual is defined as follows: if the updated $x_{j} \\neq 0$, set $$v_{j} = a_{j}^{\\top} r - \\lambda \\,\\mathrm{sign}(x_{j}),$$ and if the updated $x_{j} = 0$, set $$v_{j} = \\max\\!\\big(|a_{j}^{\\top} r| - \\lambda,\\, 0\\big),$$ where $a_{j}$ denotes the $j$-th column of $A$. Provide your final answer as a single row matrix containing the components of the updated residual $r$ (in order) followed by the scalar $v_{j}$. Express all values exactly; no rounding is required.", "solution": "The user requires the derivation of the coordinate-wise minimizer for the LASSO problem and its application to a specific instance. The analysis will proceed in two stages: first, a formal derivation of the update rule from first principles, and second, the application of this rule to the provided data.\n\nThe LASSO objective function is given by:\n$$f(x) = \\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\|x\\|_{1}$$\nwhere $y \\in \\mathbb{R}^{m}$, $A \\in \\mathbb{R}^{m \\times n}$, $x \\in \\mathbb{R}^{n}$, and $\\lambda > 0$. The pathwise coordinate descent method updates a single coordinate $x_j$ at a time, keeping all other coordinates $x_k$ where $k \\neq j$ fixed. To find the update for $x_j$, we solve the one-dimensional optimization problem:\n$$ \\min_{z \\in \\mathbb{R}} f(x_1, \\dots, x_{j-1}, z, x_{j+1}, \\dots, x_n) $$\nLet's express the objective as a function of $z$. The $L_1$ norm is separable: $\\|x\\|_1 = \\sum_{k \\neq j} |x_k| + |z|$. The quadratic term can be expanded:\n$$ \\|y - Ax\\|_2^2 = \\left\\|y - \\sum_{k=1}^{n} x_k a_k\\right\\|_2^2 = \\left\\|y - \\sum_{k \\neq j} x_k a_k - z a_j\\right\\|_2^2 $$\nwhere $a_k$ is the $k$-th column of $A$. Let the partial residual be $r^{(j)} = y - \\sum_{k \\neq j} x_k a_k$. The optimization problem with respect to $z$ becomes:\n$$ \\min_{z \\in \\mathbb{R}} \\left( \\frac{1}{2} \\|r^{(j)} - z a_j\\|_2^2 + \\lambda |z| \\right) $$\nThe other terms $\\lambda \\sum_{k \\neq j} |x_k|$ are constant with respect to $z$ and can be ignored for the minimization. Let's expand the squared norm:\n$$ g(z) = \\frac{1}{2} (r^{(j)} - z a_j)^{\\top}(r^{(j)} - z a_j) + \\lambda |z| = \\frac{1}{2} \\left( \\|r^{(j)}\\|_2^2 - 2z (a_j)^{\\top}r^{(j)} + z^2 \\|a_j\\|_2^2 \\right) + \\lambda |z| $$\nThis objective function $g(z)$ is the sum of a strictly convex quadratic and a convex absolute value function, hence it is strictly convex and has a unique minimizer. The minimizer is found by setting the subgradient of $g(z)$ to zero. The subgradient is:\n$$ \\partial g(z) = z \\|a_j\\|_2^2 - (a_j)^{\\top}r^{(j)} + \\lambda \\partial|z| $$\nwhere $\\partial|z|$ is the subgradient of the absolute value function: $\\partial|z| = \\mathrm{sign}(z)$ if $z \\neq 0$ and $\\partial|z| \\in [-1, 1]$ if $z=0$. The optimality condition is $0 \\in \\partial g(z)$.\n\nCase 1: $z > 0$. The condition is $z \\|a_j\\|_2^2 - (a_j)^{\\top}r^{(j)} + \\lambda = 0$. This gives $z = \\frac{(a_j)^{\\top}r^{(j)} - \\lambda}{\\|a_j\\|_2^2}$. This solution is valid if $z>0$, which implies $(a_j)^{\\top}r^{(j)} > \\lambda$.\n\nCase 2: $z < 0$. The condition is $z \\|a_j\\|_2^2 - (a_j)^{\\top}r^{(j)} - \\lambda = 0$. This gives $z = \\frac{(a_j)^{\\top}r^{(j)} + \\lambda}{\\|a_j\\|_2^2}$. This solution is valid if $z<0$, which implies $(a_j)^{\\top}r^{(j)} < -\\lambda$.\n\nCase 3: $z = 0$. The condition is $0 \\in -(a_j)^{\\top}r^{(j)} + \\lambda [-1, 1]$. This is equivalent to $(a_j)^{\\top}r^{(j)} \\in [-\\lambda, \\lambda]$, or $|(a_j)^{\\top}r^{(j)}| \\le \\lambda$.\n\nThese three cases define the soft-thresholding operator, $S_{\\tau}(\\rho) = \\mathrm{sign}(\\rho)\\max(|\\rho|-\\tau, 0)$. The optimal $z$, which we denote as $x_j^{\\text{new}}$, is given by:\n$$ x_j^{\\text{new}} = \\frac{S_{\\lambda}((a_j)^{\\top}r^{(j)})}{\\|a_j\\|_2^2} $$\nFor efficient computation, we express $(a_j)^{\\top}r^{(j)}$ in terms of the full residual $r = y - Ax$.\n$r^{(j)} = y - \\sum_{k \\neq j} x_k a_k = (y - Ax) + x_j a_j = r + x_j a_j$.\nTherefore, $(a_j)^{\\top}r^{(j)} = (a_j)^{\\top}(r + x_j a_j) = (a_j)^{\\top}r + x_j \\|a_j\\|_2^2$.\nLet $\\rho_j = (a_j)^{\\top}r + x_j \\|a_j\\|_2^2$. The update rule is:\n$$ x_j^{\\text{new}} = \\frac{S_{\\lambda}(\\rho_j)}{\\|a_j\\|_2^2} $$\nThis is the closed-form coordinate-wise minimizer.\n\nNow, we perform a single coordinate update for the provided instance.\nThe given data are:\n$$ A = \\begin{pmatrix} 1 & 0 & 2 & -1 \\\\ 0 & 1 & 1 & 1 \\\\ 1 & -1 & 0 & 2 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix}, \\quad x = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ \\frac{1}{2} \\end{pmatrix}, \\quad \\lambda = 1 $$\nWe are to update the coordinate with index $j = 2$.\n\nFirst, we compute the current residual $r = y - Ax$:\n$$ Ax = \\begin{pmatrix} 1 & 0 & 2 & -1 \\\\ 0 & 1 & 1 & 1 \\\\ 1 & -1 & 0 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 1(1) + 2(-1) - 1(\\frac{1}{2}) \\\\ 1(0) + 1(-1) + 1(\\frac{1}{2}) \\\\ 1(1) - 1(0) + 2(\\frac{1}{2}) \\end{pmatrix} = \\begin{pmatrix} 1 - 2 - \\frac{1}{2} \\\\ -1 + \\frac{1}{2} \\\\ 1 + 1 \\end{pmatrix} = \\begin{pmatrix} -\\frac{3}{2} \\\\ -\\frac{1}{2} \\\\ 2 \\end{pmatrix} $$\n$$ r = y - Ax = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} -\\frac{3}{2} \\\\ -\\frac{1}{2} \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 2 + \\frac{3}{2} \\\\ -1 + \\frac{1}{2} \\\\ 3 - 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2} \\\\ -\\frac{1}{2} \\\\ 1 \\end{pmatrix} $$\nNext, we compute the update for $x_2$. The current value is $x_2=0$. The second column of $A$ is $a_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix}$. The squared $L_2$-norm is $\\|a_2\\|_2^2 = 0^2 + 1^2 + (-1)^2 = 2$.\nWe compute $\\rho_2 = (a_2)^{\\top}r + x_2 \\|a_2\\|_2^2$:\n$$ (a_2)^{\\top}r = \\begin{pmatrix} 0 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} \\frac{7}{2} \\\\ -\\frac{1}{2} \\\\ 1 \\end{pmatrix} = 0(\\frac{7}{2}) + 1(-\\frac{1}{2}) - 1(1) = -\\frac{1}{2} - 1 = -\\frac{3}{2} $$\n$$ \\rho_2 = -\\frac{3}{2} + (0)(2) = -\\frac{3}{2} $$\nNow we apply the soft-thresholding operator with $\\lambda=1$:\n$$ S_1(\\rho_2) = S_1(-\\frac{3}{2}) = \\mathrm{sign}(-\\frac{3}{2}) \\max\\left(\\left|-\\frac{3}{2}\\right| - 1, 0\\right) = (-1) \\max\\left(\\frac{3}{2} - 1, 0\\right) = (-1) \\max\\left(\\frac{1}{2}, 0\\right) = -\\frac{1}{2} $$\nThe new value for the second coordinate is:\n$$ x_2^{\\text{new}} = \\frac{S_1(\\rho_2)}{\\|a_2\\|_2^2} = \\frac{-\\frac{1}{2}}{2} = -\\frac{1}{4} $$\nThe change in the coordinate is $\\Delta x_2 = x_2^{\\text{new}} - x_2 = -\\frac{1}{4} - 0 = -\\frac{1}{4}$.\nThe updated residual $r^{\\text{new}}$ is computed efficiently as $r^{\\text{new}} = r - \\Delta x_2 a_2$:\n$$ r^{\\text{new}} = \\begin{pmatrix} \\frac{7}{2} \\\\ -\\frac{1}{2} \\\\ 1 \\end{pmatrix} - \\left(-\\frac{1}{4}\\right) \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2} \\\\ -\\frac{1}{2} \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ \\frac{1}{4} \\\\ -\\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2} \\\\ -\\frac{2}{4} + \\frac{1}{4} \\\\ \\frac{4}{4} - \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2} \\\\ -\\frac{1}{4} \\\\ \\frac{3}{4} \\end{pmatrix} $$\nFinally, we compute the coordinate-wise Karush-Kuhn-Tucker (KKT) residual $v_2$. Since the updated coordinate $x_2^{\\text{new}} = -\\frac{1}{4} \\neq 0$, we use the formula $v_j = (a_j)^{\\top} r^{\\text{new}} - \\lambda \\,\\mathrm{sign}(x_j^{\\text{new}})$.\n$$ (a_2)^{\\top} r^{\\text{new}} = \\begin{pmatrix} 0 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} \\frac{7}{2} \\\\ -\\frac{1}{4} \\\\ \\frac{3}{4} \\end{pmatrix} = 0 - \\frac{1}{4} - \\frac{3}{4} = -1 $$\n$$ \\lambda \\,\\mathrm{sign}(x_2^{\\text{new}}) = (1) \\mathrm{sign}(-\\frac{1}{4}) = (1)(-1) = -1 $$\n$$ v_2 = (-1) - (-1) = 0 $$\nAs expected, the KKT residual for the just-optimized coordinate is zero, confirming that the first-order optimality condition for this coordinate is satisfied.\nThe required output is a row matrix containing the components of the updated residual $r^{\\text{new}}$ followed by the scalar $v_2$.\nThe components of $r^{\\text{new}}$ are $\\frac{7}{2}$, $-\\frac{1}{4}$, $\\frac{3}{4}$. The scalar is $v_2=0$.", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{7}{2} & -\\frac{1}{4} & \\frac{3}{4} & 0 \\end{pmatrix} } $$", "id": "3465818"}, {"introduction": "After mastering a single coordinate update, we can explore how the complete LASSO solution vector, $\\hat{\\beta}(\\lambda)$, evolves as the regularization parameter $\\lambda$ is varied. This exercise guides you through tracing the \"solution path,\" identifying the critical values of $\\lambda$ known as \"breakpoints\" where variables enter or leave the active set. This practice [@problem_id:3465883] demonstrates the path's piecewise linear structure and provides deep insight into how LASSO performs continuous variable selection.", "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) problem with squared-error loss in a low-dimensional setting. Let the design matrix be $X \\in \\mathbb{R}^{2 \\times 2}$ and the response vector be $y \\in \\mathbb{R}^{2}$, given by\n$$\nX = \\begin{pmatrix}\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix}, \\qquad\ny = \\begin{pmatrix}\n2 \\\\\n1\n\\end{pmatrix}.\n$$\nThe LASSO estimator $ \\hat{\\beta}(\\lambda) \\in \\mathbb{R}^{2} $ is defined for regularization parameter $ \\lambda \\geq 0 $ as the minimizer of\n$$\n\\frac{1}{2} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\| \\beta \\|_{1}.\n$$\nStarting from the fundamental subgradient optimality conditions for convex optimization, and using the conceptual framework of pathwise coordinate descent and solution paths, determine the first two breakpoints $ \\lambda_{1} $ and $ \\lambda_{2} $ at which the active set of $ \\hat{\\beta}(\\lambda) $ changes as $ \\lambda $ decreases from a sufficiently large value. Explicitly compute these two breakpoint values in exact form for the specified $X$ and $y$. Then, verify that on each interval between consecutive breakpoints the LASSO solution $ \\hat{\\beta}(\\lambda) $ depends linearly on $ \\lambda $ by deriving the explicit expressions for the path segments.\n\nProvide your final answer as the pair $ (\\lambda_{1}, \\lambda_{2}) $ in a single row matrix using the LaTeX $ \\texttt{pmatrix} $ environment. No rounding is required, and no units are involved.", "solution": "We start from the LASSO formulation\n$$\n\\min_{\\beta \\in \\mathbb{R}^{2}} \\; \\frac{1}{2} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\| \\beta \\|_{1},\n$$\nwith $ X = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix} $ and $ y = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} $. Denote the columns of $ X $ by $ x_{1} $ and $ x_{2} $, so $ x_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} $ and $ x_{2} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $. Define the Gram matrix $ G = X^{\\top} X $ and the correlation vector $ c = X^{\\top} y $. Compute these:\n$$\nG = X^{\\top} X = \\begin{pmatrix}\nx_{1}^{\\top} x_{1} & x_{1}^{\\top} x_{2} \\\\\nx_{2}^{\\top} x_{1} & x_{2}^{\\top} x_{2}\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 & 1 \\\\\n1 & 2\n\\end{pmatrix},\n\\qquad\nc = X^{\\top} y = \\begin{pmatrix}\nx_{1}^{\\top} y \\\\\nx_{2}^{\\top} y\n\\end{pmatrix}\n= \\begin{pmatrix}\n2 \\\\\n3\n\\end{pmatrix}.\n$$\nThe subgradient optimality conditions for LASSO state that at the optimum $ \\hat{\\beta}(\\lambda) $, for each coordinate $ j \\in \\{1,2\\} $,\n$$\nx_{j}^{\\top} (y - X \\hat{\\beta}(\\lambda)) = \\lambda s_{j},\n$$\nwhere $ s_{j} = \\operatorname{sign}(\\hat{\\beta}_{j}(\\lambda)) $ if $ \\hat{\\beta}_{j}(\\lambda) \\neq 0 $, and $ s_{j} \\in [-1,1] $ if $ \\hat{\\beta}_{j}(\\lambda) = 0 $. In vector form, when an active set $ A \\subseteq \\{1,2\\} $ and a sign pattern $ s_{A} $ are fixed, the Karush–Kuhn–Tucker (KKT) conditions yield\n$$\nG_{A A} \\hat{\\beta}_{A}(\\lambda) = c_{A} - \\lambda s_{A},\n$$\nand $ |x_{j}^{\\top} (y - X \\hat{\\beta}(\\lambda))| \\leq \\lambda $ for $ j \\notin A $. Hence, for fixed $ A $ and $ s_{A} $, the solution segment is\n$$\n\\hat{\\beta}_{A}(\\lambda) = G_{A A}^{-1} \\big( c_{A} - \\lambda s_{A} \\big),\n$$\nwhich is affine (and thus linear) in $ \\lambda $.\n\nWe now compute the breakpoints. The initial breakpoint $ \\lambda_{1} $ is the smallest $ \\lambda $ at which the zero solution ceases to be optimal. At $ \\hat{\\beta}(\\lambda) = 0 $, the optimality requires $ |c_{j}| \\leq \\lambda $ for all $ j $, so the largest correlation determines the first entrance:\n$$\n\\lambda_{\\max} = \\max_{j} |c_{j}| = \\max \\{ |2|, |3| \\} = 3.\n$$\nThus, at $ \\lambda = 3 $, the variable with index $ j = 2 $ is poised to enter the model with positive sign (since $ c_{2} = 3 > 0 $). Therefore, the first breakpoint is $ \\lambda_{1} = 3 $.\n\nFor $ \\lambda $ just below $ \\lambda_{1} = 3 $, the active set is $ A = \\{2\\} $ with $ s_{2} = +1 $. The KKT equation on this segment is\n$$\ng_{22} \\hat{\\beta}_{2}(\\lambda) = c_{2} - \\lambda,\n\\quad \\text{where} \\quad g_{22} = 2, \\; c_{2} = 3.\n$$\nHence\n$$\n\\hat{\\beta}_{2}(\\lambda) = \\frac{3 - \\lambda}{2},\n\\quad \\hat{\\beta}_{1}(\\lambda) = 0,\n\\quad \\text{valid while } \\lambda \\in [\\lambda_{2}, \\lambda_{1}].\n$$\nThe next breakpoint $ \\lambda_{2} $ occurs when the inactive coordinate $ j = 1 $ hits the boundary $ |x_{1}^{\\top} r(\\lambda)| = \\lambda $, where $ r(\\lambda) = y - X \\hat{\\beta}(\\lambda) $ is the residual. On the current segment,\n$$\nr(\\lambda) = y - x_{2} \\hat{\\beta}_{2}(\\lambda)\n= \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\cdot \\frac{3 - \\lambda}{2}\n= \\begin{pmatrix} 2 - \\frac{3 - \\lambda}{2} \\\\ 1 - \\frac{3 - \\lambda}{2} \\end{pmatrix}\n= \\begin{pmatrix} \\frac{1}{2} + \\frac{\\lambda}{2} \\\\ -\\frac{1}{2} + \\frac{\\lambda}{2} \\end{pmatrix}.\n$$\nTherefore,\n$$\nx_{1}^{\\top} r(\\lambda) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}^{\\top} \\begin{pmatrix} \\frac{1}{2} + \\frac{\\lambda}{2} \\\\ -\\frac{1}{2} + \\frac{\\lambda}{2} \\end{pmatrix}\n= \\frac{1}{2} + \\frac{\\lambda}{2}.\n$$\nThe entry condition $ |x_{1}^{\\top} r(\\lambda)| = \\lambda $ (with positive sign anticipated since $ x_{1}^{\\top} r(\\lambda) $ is positive near $ \\lambda $ just below $ 3 $) gives\n$$\n\\frac{1}{2} + \\frac{\\lambda}{2} = \\lambda \\quad \\Rightarrow \\quad \\lambda = 1.\n$$\nHence the second breakpoint is $ \\lambda_{2} = 1 $.\n\nFor $ \\lambda \\in [0, 1] $, both variables are active with positive signs, so $ A = \\{1,2\\} $ and $ s = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $. Using the full KKT system,\n$$\nG \\hat{\\beta}(\\lambda) = c - \\lambda s,\n\\quad \\text{so} \\quad\n\\hat{\\beta}(\\lambda) = G^{-1} (c - \\lambda s).\n$$\nCompute $ G^{-1} $:\n$$\nG = \\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\end{pmatrix}, \\quad \\det(G) = 1 \\cdot 2 - 1 \\cdot 1 = 1, \\quad\nG^{-1} = \\begin{pmatrix} 2 & -1 \\\\ -1 & 1 \\end{pmatrix}.\n$$\nThen\n$$\nc - \\lambda s = \\begin{pmatrix} 2 - \\lambda \\\\ 3 - \\lambda \\end{pmatrix},\n\\quad \\hat{\\beta}(\\lambda) = \\begin{pmatrix} 2 & -1 \\\\ -1 & 1 \\end{pmatrix} \\begin{pmatrix} 2 - \\lambda \\\\ 3 - \\lambda \\end{pmatrix}\n= \\begin{pmatrix}\n2(2 - \\lambda) - (3 - \\lambda) \\\\\n-(2 - \\lambda) + (3 - \\lambda)\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 - \\lambda \\\\\n1\n\\end{pmatrix}.\n$$\nThus, on $ \\lambda \\in [0, 1] $, the path is explicitly linear in $ \\lambda $ with $ \\hat{\\beta}_{1}(\\lambda) = 1 - \\lambda $ and $ \\hat{\\beta}_{2}(\\lambda) = 1 $. On $ \\lambda \\in [1, 3] $, the path is also linear in $ \\lambda $ with $ \\hat{\\beta}_{2}(\\lambda) = \\frac{3 - \\lambda}{2} $ and $ \\hat{\\beta}_{1}(\\lambda) = 0 $. Continuity holds at $ \\lambda = 1 $ since both expressions yield $ \\hat{\\beta}(1) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} $, and at $ \\lambda = 3 $ we have $ \\hat{\\beta}(3) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $ as required.\n\nTherefore, the first two breakpoints of the LASSO path are $ \\lambda_{1} = 3 $ and $ \\lambda_{2} = 1 $, and the path segments between these breakpoints are linear in $ \\lambda $.", "answer": "$$\\boxed{\\begin{pmatrix} 3 & 1 \\end{pmatrix}}$$", "id": "3465883"}, {"introduction": "Computing the entire solution path for large datasets can be computationally intensive. This advanced practice introduces a powerful optimization technique known as a \"strong screening rule,\" which leverages information from a solution at $\\lambda_{k-1}$ to safely discard features that are very likely to be zero at a new value $\\lambda_k \\lt \\lambda_{k-1}$. By working through this exercise [@problem_id:3465831], you will learn to apply and verify these rules, connecting theoretical properties of the solution path to practical algorithmic speedups.", "problem": "Consider the least absolute shrinkage and selection operator (LASSO) problem with squared-error loss\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\nwhere the design matrix $X \\in \\mathbb{R}^{n \\times p}$ has centered columns scaled to unit Euclidean norm and $y \\in \\mathbb{R}^{n}$. You are given a pathwise sequence of regularization parameters $\\{\\lambda_{k-1}, \\lambda_{k}\\}$ with $\\lambda_{k} < \\lambda_{k-1}$, and a previously computed solution $\\beta(\\lambda_{k-1})$. The task is to derive and apply a sequential screening rule (often called a strong rule) to propose a screened set at $\\lambda_{k}$ using $\\beta(\\lambda_{k-1})$, and then verify safety by checking the Karush-Kuhn-Tucker (KKT) conditions at $\\lambda_{k}$ for the features initially discarded.\n\nStart from first principles:\n- Use the KKT conditions for the LASSO optimum at a given $\\lambda$: for each coordinate $j \\in \\{1,\\dots,p\\}$, with residual $r(\\lambda) = y - X\\beta(\\lambda)$ and correlation $c_{j}(\\lambda) = x_{j}^{\\top} r(\\lambda)$, the KKT conditions require that if $\\beta_{j}(\\lambda) \\neq 0$, then $c_{j}(\\lambda) = \\lambda \\,\\mathrm{sign}(\\beta_{j}(\\lambda))$, and if $\\beta_{j}(\\lambda) = 0$, then $|c_{j}(\\lambda)| \\le \\lambda$.\n- Use the unit-slope bound for squared-error loss with unit-norm columns: the correlations $c_{j}(\\lambda)$ are $1$-Lipschitz as functions of $\\lambda$ along the solution path.\n- From these elements, derive a sufficient condition to discard a feature $j$ at $\\lambda_{k}$ based on the previously computed quantities at $\\lambda_{k-1}$.\n\nApply this to the concrete instance below and then perform a safety check:\n- Let $n=4$ and $p=5$. The columns of $X$ are\n$$\nx_{1} = \\begin{pmatrix}1\\\\0\\\\0\\\\0\\end{pmatrix}, \\quad\nx_{2} = \\begin{pmatrix}0\\\\1\\\\0\\\\0\\end{pmatrix}, \\quad\nx_{3} = \\begin{pmatrix}0\\\\0\\\\1\\\\0\\end{pmatrix}, \\quad\nx_{4} = \\begin{pmatrix}0\\\\0\\\\0\\\\1\\end{pmatrix}, \\quad\nx_{5} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1\\\\0\\\\1\\\\0\\end{pmatrix}.\n$$\n- Let $y = \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix}$, $\\lambda_{k-1} = 2$, and $\\lambda_{k} = 1.3$.\n- At $\\lambda_{k-1}$, suppose the unique solution is $\\beta(\\lambda_{k-1}) = 0$.\n\nTasks:\n- Using the KKT conditions and the unit-slope bound, derive a sufficient discard inequality to propose a screened-out set at $\\lambda_{k}$ from the residual and correlations at $\\lambda_{k-1}$, and list the proposed discarded features.\n- Solve the LASSO subproblem at $\\lambda_{k}$ restricted to the retained set you identified, and obtain the residual $r(\\lambda_{k})$.\n- Verify safety by checking the KKT conditions at $\\lambda_{k}$ for all features originally discarded. If any feature violates KKT, it must be added back; otherwise screening is safe.\n- Report the number of features that are safely discarded at $\\lambda_{k}$ after KKT verification. Your answer must be a single number with no units. No rounding is required.", "solution": "The problem asks us to derive and apply a sequential screening rule for the LASSO problem, and then verify its safety. The LASSO optimization problem is given by\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}\n$$\nwhere $y \\in \\mathbb{R}^{n}$ is the response vector, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta \\in \\mathbb{R}^{p}$ is the coefficient vector, and $\\lambda > 0$ is the regularization parameter. The columns of $X$, denoted $x_j$, are scaled to have unit Euclidean norm, i.e., $\\|x_j\\|_2 = 1$ for all $j \\in \\{1,\\dots,p\\}$.\n\nFirst, we derive the screening rule. The Karush-Kuhn-Tucker (KKT) conditions are necessary and sufficient for a solution $\\beta(\\lambda)$ to be optimal at a given $\\lambda$. Let $r(\\lambda) = y - X\\beta(\\lambda)$ be the residual and $c_j(\\lambda) = x_j^{\\top}r(\\lambda)$ be the correlation of the $j$-th feature with the residual. The KKT conditions state that for each coordinate $j$:\n\\begin{itemize}\n    \\item If $\\beta_j(\\lambda) \\neq 0$, then $c_j(\\lambda) = \\lambda \\, \\mathrm{sign}(\\beta_j(\\lambda))$.\n    \\item If $\\beta_j(\\lambda) = 0$, then $|c_j(\\lambda)| \\le \\lambda$.\n\\end{itemize}\nA feature $j$ can be safely discarded at regularization level $\\lambda_k$ if we can guarantee that its optimal coefficient $\\beta_j(\\lambda_k)$ is zero. From the KKT conditions, a sufficient condition for $\\beta_j(\\lambda_k) = 0$ is the strict inequality $|c_j(\\lambda_k)| < \\lambda_k$.\n\nWe are given a previously computed solution $\\beta(\\lambda_{k-1})$ at $\\lambda_{k-1} > \\lambda_k$, and we can compute the corresponding correlations $c_j(\\lambda_{k-1})$. To relate these to the unknown correlations $c_j(\\lambda_k)$, we use the provided unit-slope bound, which states that the functions $c_j(\\lambda)$ are $1$-Lipschitz continuous:\n$$\n|c_j(\\lambda_{k}) - c_j(\\lambda_{k-1})| \\le |\\lambda_{k} - \\lambda_{k-1}|\n$$\nSince $\\lambda_k < \\lambda_{k-1}$, this becomes $|c_j(\\lambda_{k}) - c_j(\\lambda_{k-1})| \\le \\lambda_{k-1} - \\lambda_k$. Using the triangle inequality, we can bound $|c_j(\\lambda_k)|$:\n$$\n|c_j(\\lambda_k)| = |c_j(\\lambda_{k-1}) + (c_j(\\lambda_k) - c_j(\\lambda_{k-1}))| \\le |c_j(\\lambda_{k-1})| + |c_j(\\lambda_k) - c_j(\\lambda_{k-1})|\n$$\nApplying the Lipschitz property, we get an upper bound for the correlation at $\\lambda_k$:\n$$\n|c_j(\\lambda_k)| \\le |c_j(\\lambda_{k-1})| + \\lambda_{k-1} - \\lambda_k\n$$\nIf this upper bound is itself less than $\\lambda_k$, then we can guarantee that $|c_j(\\lambda_k)| < \\lambda_k$, ensuring that $\\beta_j(\\lambda_k)=0$. This gives us the sufficient discard inequality:\n$$\n|c_j(\\lambda_{k-1})| + \\lambda_{k-1} - \\lambda_k < \\lambda_k\n$$\nRearranging this inequality yields the screening rule: a feature $j$ can be preliminarily discarded if\n$$\n|c_j(\\lambda_{k-1})| < 2\\lambda_k - \\lambda_{k-1}\n$$\n\nNow, we apply this rule to the given instance. The data are:\n$n=4$, $p=5$, $\\lambda_{k-1}=2$, $\\lambda_k=1.3$.\n$y = \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix}$, and the columns of $X$ are $x_1, \\dots, x_5$.\nThe problem states that at $\\lambda_{k-1}=2$, the solution is $\\beta(\\lambda_{k-1}) = 0$.\nThe residual is $r(\\lambda_{k-1}) = y - X \\cdot 0 = y$.\nThe correlations at $\\lambda_{k-1}$ are $c_j(\\lambda_{k-1})=x_j^{\\top}y$:\n$c_1(\\lambda_{k-1}) = \\begin{pmatrix}1&0&0&0\\end{pmatrix} \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix} = 2$.\n$c_2(\\lambda_{k-1}) = \\begin{pmatrix}0&1&0&0\\end{pmatrix} \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix} = 0$.\n$c_3(\\lambda_{k-1}) = \\begin{pmatrix}0&0&1&0\\end{pmatrix} \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix} = -1.5$.\n$c_4(\\lambda_{k-1}) = \\begin{pmatrix}0&0&0&1\\end{pmatrix} \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix} = -1$.\n$c_5(\\lambda_{k-1}) = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1&0&1&0\\end{pmatrix} \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix} = \\frac{1}{\\sqrt{2}}(2 - 1.5) = \\frac{0.5}{\\sqrt{2}} = \\frac{\\sqrt{2}}{4}$.\n\nThe threshold for the screening rule is $2\\lambda_k - \\lambda_{k-1} = 2(1.3) - 2 = 2.6 - 2 = 0.6$.\nWe discard feature $j$ if $|c_j(\\lambda_{k-1})| < 0.6$:\n\\begin{itemize}\n    \\item $j=1$: $|c_1(\\lambda_{k-1})| = |2| = 2 \\not< 0.6$. Retain.\n    \\item $j=2$: $|c_2(\\lambda_{k-1})| = |0| = 0 < 0.6$. Discard.\n    \\item $j=3$: $|c_3(\\lambda_{k-1})| = |-1.5| = 1.5 \\not< 0.6$. Retain.\n    \\item $j=4$: $|c_4(\\lambda_{k-1})| = |-1| = 1 \\not< 0.6$. Retain.\n    \\item $j=5$: $|c_5(\\lambda_{k-1})| = |\\frac{\\sqrt{2}}{4}| \\approx 0.354 < 0.6$. Discard.\n\\end{itemize}\nThe proposed set of discarded features is $\\{2, 5\\}$, and the retained set is $\\mathcal{A}=\\{1, 3, 4\\}$.\n\nNext, we solve the LASSO subproblem at $\\lambda_k=1.3$ restricted to the features in $\\mathcal{A}$. The subproblem is:\n$$\n\\min_{\\beta_1, \\beta_3, \\beta_4} \\ \\frac{1}{2}\\|y - (x_1\\beta_1 + x_3\\beta_3 + x_4\\beta_4)\\|_{2}^{2} + 1.3 (|\\beta_1| + |\\beta_3| + |\\beta_4|)\n$$\nThe columns $x_1, x_3, x_4$ are orthonormal. For an orthonormal design matrix $X_{\\mathcal{A}}$, the LASSO solution is given by coordinate-wise soft-thresholding of the correlations $X_{\\mathcal{A}}^{\\top}y$. The solution for $j \\in \\mathcal{A}$ is $\\beta_j(\\lambda_k) = S_{\\lambda_k}(x_j^{\\top}y)$, where $S_{\\alpha}(z) = \\mathrm{sign}(z)\\max(|z|-\\alpha, 0)$.\nThe relevant correlations are $c_j(\\lambda_{k-1}) = x_j^{\\top}y$.\n\\begin{itemize}\n    \\item $\\beta_1(1.3) = S_{1.3}(2) = \\mathrm{sign}(2)\\max(|2|-1.3, 0) = 0.7$.\n    \\item $\\beta_3(1.3) = S_{1.3}(-1.5) = \\mathrm{sign}(-1.5)\\max(|-1.5|-1.3, 0) = -0.2$.\n    \\item $\\beta_4(1.3) = S_{1.3}(-1) = \\mathrm{sign}(-1)\\max(|-1|-1.3, 0) = 0$.\n\\end{itemize}\nThe solution to the subproblem gives the tentative full solution vector $\\beta(\\lambda_k) = (0.7, 0, -0.2, 0, 0)^{\\top}$.\nThe residual at $\\lambda_k=1.3$ is $r(\\lambda_k) = y - X\\beta(\\lambda_k)$:\n$$\nr(1.3) = y - (0.7x_1 - 0.2x_3) = \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix} - \\left(0.7\\begin{pmatrix}1\\\\0\\\\0\\\\0\\end{pmatrix} - 0.2\\begin{pmatrix}0\\\\0\\\\1\\\\0\\end{pmatrix}\\right) = \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix} - \\begin{pmatrix}0.7\\\\0\\\\-0.2\\\\0\\end{pmatrix} = \\begin{pmatrix}1.3\\\\0\\\\-1.3\\\\-1\\end{pmatrix}\n$$\n\nFinally, we verify the safety of the screening by checking the KKT conditions for the discarded features $j \\in \\{2, 5\\}$ at $\\lambda_k=1.3$. For these features, we presumed $\\beta_j(1.3)=0$, so we must check if $|c_j(1.3)| = |x_j^{\\top}r(1.3)| \\le \\lambda_k = 1.3$.\n\\begin{itemize}\n    \\item For $j=2$: $c_2(1.3) = x_2^{\\top}r(1.3) = \\begin{pmatrix}0&1&0&0\\end{pmatrix} \\begin{pmatrix}1.3\\\\0\\\\-1.3\\\\-1\\end{pmatrix} = 0$.\n    The condition is $|0| \\le 1.3$, which is true.\n    \\item For $j=5$: $c_5(1.3) = x_5^{\\top}r(1.3) = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1&0&1&0\\end{pmatrix} \\begin{pmatrix}1.3\\\\0\\\\-1.3\\\\-1\\end{pmatrix} = \\frac{1}{\\sqrt{2}}(1.3 - 1.3) = 0$.\n    The condition is $|0| \\le 1.3$, which is true.\n\\end{itemize}\nBoth discarded features satisfy the KKT conditions. Therefore, the screening rule was safe, and no KKT violations occurred. The solution on the reduced problem is the correct solution for the full problem. The features $\\{2, 5\\}$ were safely discarded.\n\nThe number of features that are safely discarded at $\\lambda_k=1.3$ is $2$.", "answer": "$$\\boxed{2}$$", "id": "3465831"}]}