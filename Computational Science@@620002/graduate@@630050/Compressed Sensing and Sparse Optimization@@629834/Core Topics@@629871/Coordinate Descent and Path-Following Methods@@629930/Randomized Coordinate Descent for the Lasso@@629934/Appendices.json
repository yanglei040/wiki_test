{"hands_on_practices": [{"introduction": "To build a solid theoretical foundation, we first analyze randomized coordinate descent in an idealized setting. By assuming the columns of the matrix $A$ are orthonormal, the LASSO objective function conveniently separates, allowing us to find the global minimum coordinate by coordinate. This exercise [@problem_id:3472578] reveals a beautiful connection between the optimization algorithm's runtime and the classic Coupon Collector's Problem, providing an exact analytical expression for the expected number of iterations to convergence.", "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO), which minimizes the objective $F(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$ over $x \\in \\mathbb{R}^{n}$, where $A \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns so that $A^{\\top}A = I_{n}$, $b \\in \\mathbb{R}^{m}$, and $\\lambda > 0$. Let $a_{j} \\in \\mathbb{R}^{m}$ denote the $j$-th column of $A$. Define the soft-threshold (shrinkage) operator $S_{\\lambda}:\\mathbb{R}\\to\\mathbb{R}$ by $S_{\\lambda}(t) = \\mathrm{sgn}(t)\\,\\max(|t| - \\lambda, 0)$.\n\nConsider the following randomized coordinate descent procedure: starting from any initial vector $x^{(0)} \\in \\mathbb{R}^{n}$, at each iteration $t = 1,2,\\dots$, select a coordinate $j_{t} \\in \\{1,\\dots,n\\}$ uniformly at random, independently across iterations, and update only that coordinate via $x_{j_{t}} \\leftarrow S_{\\lambda}(a_{j_{t}}^{\\top} b)$, keeping all other coordinates unchanged at iteration $t$.\n\nAssume the algorithm terminates at the first iteration $T$ for which $x^{(T)}$ equals the unique minimizer of $F$. Compute the exact expected number of iterations until termination as a closed-form analytic expression in $n$. No rounding is required, and no physical units apply. Your final answer must be a single expression.", "solution": "The problem asks for the expected number of iterations for a specific randomized coordinate descent algorithm to find the unique minimizer of the LASSO objective function $F(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$, under the condition that the columns of $A$ are orthonormal.\n\nFirst, let's analyze the objective function $F(x)$. The squared $\\ell_2$-norm term can be expanded as:\n$$ \\|A x - b\\|_{2}^{2} = (A x - b)^{\\top}(A x - b) = x^{\\top}A^{\\top}Ax - 2x^{\\top}A^{\\top}b + b^{\\top}b $$\nThe problem states that the matrix $A \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns, which is formally expressed as $A^{\\top}A = I_{n}$, where $I_n$ is the $n \\times n$ identity matrix. Substituting this into the expression for $F(x)$:\n$$ F(x) = \\frac{1}{2}(x^{\\top}I_n x - 2x^{\\top}A^{\\top}b + b^{\\top}b) + \\lambda \\|x\\|_{1} $$\nLet's express this in terms of the components $x_j$ of the vector $x$. The term $x^{\\top}x = \\sum_{j=1}^{n} x_j^2$. The term $x^{\\top}A^{\\top}b = \\sum_{j=1}^{n} x_j (A^{\\top}b)_j$. The $j$-th component of the vector $A^{\\top}b$ is $a_j^{\\top}b$, where $a_j$ is the $j$-th column of $A$. The $\\ell_1$-norm is $\\|x\\|_1 = \\sum_{j=1}^{n} |x_j|$.\nPutting this all together, the objective function becomes:\n$$ F(x) = \\frac{1}{2}\\left(\\sum_{j=1}^{n} x_j^2 - 2\\sum_{j=1}^{n} x_j (a_j^{\\top}b) + \\|b\\|_2^2\\right) + \\lambda \\sum_{j=1}^{n} |x_j| $$\nWe can rearrange the summation:\n$$ F(x) = \\sum_{j=1}^{n} \\left( \\frac{1}{2}x_j^2 - (a_j^{\\top}b)x_j + \\lambda|x_j| \\right) + \\frac{1}{2}\\|b\\|_2^2 $$\nThe crucial consequence of the orthonormality condition $A^{\\top}A=I_n$ is that the objective function $F(x)$ is separable. It is a sum of $n$ independent functions, one for each coordinate $x_j$, plus a constant term $\\frac{1}{2}\\|b\\|_2^2$ that does not affect the location of the minimum.\n\nTo find the unique minimizer $x^*$ of $F(x)$, we can minimize each term in the sum independently. For each $j \\in \\{1, \\dots, n\\}$, we must find the value $x_j^*$ that minimizes the one-dimensional function:\n$$ f_j(z) = \\frac{1}{2}z^2 - (a_j^{\\top}b)z + \\lambda|z| $$\nThe function $f_j(z)$ is convex. The optimality condition is that $0$ must be in the subgradient of $f_j(z)$ at $z=x_j^*$. The subgradient is:\n$$ \\partial f_j(z) = z - a_j^{\\top}b + \\lambda \\partial|z| $$\nwhere $\\partial|z|$ is the subgradient of the absolute value function, which is $\\mathrm{sgn}(z)$ for $z \\neq 0$ and $[-1, 1]$ for $z=0$. Setting the subgradient to contain $0$:\n$$ a_j^{\\top}b - x_j^* \\in \\lambda \\partial|x_j^*| \\quad \\iff \\quad x_j^* = \\mathrm{prox}_{\\lambda|\\cdot|}(a_j^{\\top}b) $$\nThis proximal operator corresponds to the soft-thresholding function $S_{\\lambda}(t) = \\mathrm{sgn}(t)\\max(|t|-\\lambda, 0)$. Therefore, the $j$-th component of the unique global minimizer $x^*$ is:\n$$ x_j^* = S_{\\lambda}(a_j^{\\top}b) $$\nThis holds for all $j=1, \\dots, n$.\n\nNow, let's analyze the given randomized coordinate descent algorithm. At each iteration $t=1, 2, \\dots$, a coordinate $j_t \\in \\{1, \\dots, n\\}$ is selected uniformly at random. The update rule for this coordinate is specified as:\n$$ x_{j_t} \\leftarrow S_{\\lambda}(a_{j_t}^{\\top} b) $$\nFrom our analysis above, this update rule sets the selected coordinate $x_{j_t}$ to its globally optimal value $x_{j_t}^*$. The update for any given coordinate is independent of all other coordinates and also independent of its own previous value.\n\nThe algorithm is defined to terminate at the first iteration $T$ for which the iterate $x^{(T)}$ is equal to the unique minimizer $x^*$. This condition $x^{(T)} = x^*$ is met if and only if every coordinate $j \\in \\{1, \\dots, n\\}$ has been updated to its optimal value. Since the update rule sets the chosen coordinate to its final value, this termination condition is equivalent to stating that every coordinate index in the set $\\{1, \\dots, n\\}$ must have been selected at least once. The initial vector $x^{(0)}$ is irrelevant, as any coordinate, once selected, is immediately \"corrected\" to its final optimal value.\n\nThe problem is thus transformed into a classic probability puzzle: the Coupon Collector's Problem. We are repeatedly drawing from a set of $n$ \"coupons\" (the coordinate indices) with uniform probability and with replacement. We seek the expected number of draws required to collect all $n$ distinct coupons.\n\nLet $T$ be the total number of iterations required. We can express $T$ as the sum of random variables: $T = T_1 + T_2 + \\dots + T_n$, where $T_k$ is the number of additional iterations required to select the $k$-th new coordinate, given that $k-1$ distinct coordinates have already been selected. By the linearity of expectation, $E[T] = \\sum_{k=1}^{n} E[T_k]$.\n\nLet's compute the expectation of each $T_k$:\n- For $k=1$: The first iteration always selects a \"new\" coordinate. So, $T_1=1$ and $E[T_1]=1$.\n- For $k=2$: After $1$ distinct coordinate has been selected, there are $n-1$ unselected coordinates. The probability of selecting a new coordinate in any given iteration is $p_2 = \\frac{n-1}{n}$. The number of trials $T_2$ to achieve the first success follows a geometric distribution with success probability $p_2$. The expectation is $E[T_2] = \\frac{1}{p_2} = \\frac{n}{n-1}$.\n- In general, for any $k \\in \\{1, \\dots, n\\}$, suppose $k-1$ distinct coordinates have already been selected. The number of remaining unselected coordinates is $n-(k-1)$. The probability of selecting a new coordinate in the next iteration is $p_k = \\frac{n-(k-1)}{n} = \\frac{n-k+1}{n}$. The number of iterations $T_k$ to select this $k$-th new coordinate follows a geometric distribution with success probability $p_k$. Its expectation is $E[T_k] = \\frac{1}{p_k} = \\frac{n}{n-k+1}$.\n\nThe total expected number of iterations is the sum of these expectations:\n$$ E[T] = \\sum_{k=1}^{n} E[T_k] = \\sum_{k=1}^{n} \\frac{n}{n-k+1} $$\nTo simplify the appearance of this sum, let's perform a change of index. Let $j = n-k+1$. When $k=1$, $j=n$. When $k=n$, $j=1$. The sum becomes:\n$$ E[T] = \\sum_{j=1}^{n} \\frac{n}{j} = n \\sum_{j=1}^{n} \\frac{1}{j} $$\nThis is the final expression for the expected number of iterations. The sum $\\sum_{j=1}^{n} \\frac{1}{j}$ is known as the $n$-th harmonic number, $H_n$.", "answer": "$$\n\\boxed{n \\sum_{k=1}^{n} \\frac{1}{k}}\n$$", "id": "3472578"}, {"introduction": "Theory provides a baseline, but practical performance often depends on the structure of the data. This hands-on exercise [@problem_id:3441210] explores the crucial difference between cyclic and randomized coordinate selection, particularly in the challenging case of highly correlated features. By constructing a \"worst-case\" scenario, you will implement both methods and quantitatively observe why the stochastic nature of randomized descent can prevent the \"zig-zagging\" that plagues its deterministic counterpart, leading to more robust and efficient convergence.", "problem": "Consider the least absolute shrinkage and selection operator (Lasso) objective in compressed sensing and sparse optimization, defined for a sensing matrix $A \\in \\mathbb{R}^{m \\times p}$ and a response vector $y \\in \\mathbb{R}^{m}$ as\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\,\\|A x - y\\|_2^2 \\;+\\; \\lambda\\,\\|x\\|_1,\n$$\nwhere $\\lambda \\ge 0$ is a regularization parameter and $\\|\\cdot\\|_1$ denotes the $\\ell_1$ norm. Coordinate descent algorithms optimize $f(x)$ by successively minimizing $f$ with respect to one coordinate of $x$ at a time while holding the others fixed. Two selection rules are considered:\n- Cyclic coordinate descent (CCD): repeatedly visit coordinates in the fixed order $1,2,\\dots,p$.\n- Randomized coordinate descent (RCD): at each update, select a coordinate uniformly at random with replacement.\n\nAn epoch is defined as exactly $p$ single-coordinate updates. Both methods start from $x^{(0)} = 0$ and use exact minimization along the chosen coordinate at each update.\n\nDesign a worst-case sensing matrix $A$ and response $y$ that induce zig-zagging for the cyclic rule by constructing highly correlated columns. Specifically, for given $m$, $p$, and correlation parameter $\\rho \\in [0,1)$, construct the first two columns of $A$ to have inner product approximately $\\rho$ and unit norm, and construct any remaining columns to be approximately uncorrelated with the first two and with each other. Set $y = A x^\\star$ for a sparse ground-truth vector $x^\\star$ with only the first two entries nonzero and equal in magnitude. The program must then compare the average objective decrease per epoch between CCD and RCD, where the average is taken over a fixed number of epochs $E$, for each test case. Randomized selection must use a fixed pseudorandom seed $2025$ so that results are reproducible.\n\nYour program must implement both selection rules using exact single-coordinate minimization at each update, based solely on the definition of $f(x)$ and the per-coordinate one-dimensional optimization subproblem. It must compute and report, for each test case, the ratio\n$$\nr \\;=\\; \\frac{\\text{average per-epoch objective decrease under RCD}}{\\text{average per-epoch objective decrease under CCD}},\n$$\nexpressed as a floating-point number.\n\nTest Suite. Use the following four test cases, all with $E = 50$ epochs and no observation noise:\n- Case $1$: $m = 200$, $p = 2$, $\\rho = 0.999$, $\\lambda = 0.05$, $x^\\star = [1,\\,1]^T$.\n- Case $2$: $m = 200$, $p = 2$, $\\rho = 0.9$, $\\lambda = 0.05$, $x^\\star = [1,\\,1]^T$.\n- Case $3$: $m = 200$, $p = 2$, $\\rho = 0.0$, $\\lambda = 0.05$, $x^\\star = [1,\\,1]^T$.\n- Case $4$: $m = 200$, $p = 10$, where columns $1$ and $2$ have correlation $\\rho = 0.999$ and unit norm, columns $3$ through $10$ are random with unit norm and approximately uncorrelated with the first two and with each other, $\\lambda = 0.05$, $x^\\star = [1,\\,1,\\,0,\\,\\dots,\\,0]^T$ (length $p$).\n\nConstruction details. For $p \\ge 2$, generate the first column $a_1$ by sampling i.i.d. standard normal entries and normalizing to unit Euclidean norm. Generate $a_2$ as\n$$\na_2 \\;=\\; \\rho\\,a_1 \\;+\\; \\sqrt{1-\\rho^2}\\,w,\n$$\nwhere $w$ has i.i.d. standard normal entries and $a_2$ is then normalized to unit norm. For $p > 2$, generate columns $a_j$ for $j \\ge 3$ as independent standard normal vectors normalized to unit norm. Set $A = [a_1,\\dots,a_p]$ and $y = A x^\\star$. For both CCD and RCD, initialize $x^{(0)} = 0$, define an epoch as $p$ updates, and after each epoch record $f(x)$.\n\nAverage per-epoch objective decrease. For each method, compute the average per-epoch decrease as\n$$\n\\Delta_{\\mathrm{avg}} \\;=\\; \\frac{1}{E}\\,\\sum_{t=0}^{E-1} \\left(f\\left(x^{(t)}\\right) - f\\left(x^{(t+1)}\\right)\\right),\n$$\nwhere $x^{(t)}$ is the iterate after $t$ epochs.\n\nFinal Output Format. Your program should produce a single line of output containing a comma-separated list of four floating-point ratios $[r_1,r_2,r_3,r_4]$, in the order of the test cases above, enclosed in square brackets. Use the pseudorandom seed $2025$ for all random draws. No physical units, angle units, or percentages are involved; all outputs are dimensionless floating-point numbers.", "solution": "The core of the problem is to implement and compare Cyclic Coordinate Descent (CCD) and Randomized Coordinate Descent (RCD) for the Lasso objective function, which is given by:\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\,\\|A x - y\\|_2^2 \\;+\\; \\lambda\\,\\|x\\|_1\n$$\nHere, $x \\in \\mathbb{R}^p$ is the vector of parameters to be optimized, $A \\in \\mathbb{R}^{m \\times p}$ is the sensing matrix, $y \\in \\mathbb{R}^m$ is the response vector, and $\\lambda \\ge 0$ is the regularization parameter.\n\nCoordinate descent algorithms optimize this function by iteratively minimizing it with respect to a single coordinate $x_j$ while keeping all other coordinates $x_k$ (for $k \\neq j$) fixed. The one-dimensional subproblem for coordinate $x_j$ is to minimize:\n$$\ng(z) \\;=\\; f(x_1, \\dots, x_{j-1}, z, x_{j+1}, \\dots, x_p)\n$$\nExpanding the objective function, we separate terms that depend on $x_j$:\n$$\n\\begin{aligned}\nf(x) &= \\tfrac{1}{2} \\left\\| \\sum_{k=1}^p a_k x_k - y \\right\\|_2^2 + \\lambda \\sum_{k=1}^p |x_k| \\\\\n&= \\tfrac{1}{2} \\left\\| a_j x_j + \\sum_{k \\neq j} a_k x_k - y \\right\\|_2^2 + \\lambda |x_j| + \\lambda \\sum_{k \\neq j} |x_k|\n\\end{aligned}\n$$\nwhere $a_k$ is the $k$-th column of $A$. To minimize with respect to $x_j$, we can ignore terms that do not depend on it. The subproblem becomes minimizing:\n$$\n\\arg\\min_{x_j} \\left( \\tfrac{1}{2} \\|a_j x_j + \\sum_{k \\neq j} a_k x_k - y\\|_2^2 + \\lambda|x_j| \\right)\n$$\nExpanding the squared norm term:\n$$\n\\tfrac{1}{2} \\left( x_j^2 \\|a_j\\|_2^2 + 2x_j a_j^T\\left(\\sum_{k \\neq j} a_k x_k - y\\right) \\right) + \\lambda|x_j| + \\text{const}\n$$\nThe problem specifies that all columns $a_j$ are normalized to have unit Euclidean norm, i.e., $\\|a_j\\|_2^2 = a_j^T a_j = 1$. This simplifies the subproblem to:\n$$\n\\arg\\min_{x_j} \\left( \\tfrac{1}{2} x_j^2 + x_j a_j^T\\left(\\sum_{k \\neq j} a_k x_k - y\\right) + \\lambda|x_j| \\right)\n$$\nThis is a quadratic function of $x_j$ plus an $\\ell_1$-norm penalty. The solution to $\\arg\\min_z (\\frac{1}{2}z^2 - c z + \\lambda|z|)$ is given by the soft-thresholding operator, $z^* = S_\\lambda(c)$. In our case, $c = -a_j^T\\left(\\sum_{k \\neq j} a_k x_k - y\\right) = a_j^T y - \\sum_{k \\neq j} (a_j^T a_k) x_k$.\nThus, the update rule for coordinate $x_j$ is:\n$$\nx_j^{\\text{new}} \\leftarrow S_\\lambda\\left( a_j^T y - \\sum_{k \\neq j} (a_j^T a_k) x_k \\right)\n$$\nwhere $S_\\lambda(z) = \\text{sgn}(z) \\max(|z| - \\lambda, 0)$. For computational efficiency, we can pre-compute the Gram matrix $A^T A$ and the vector $A^T y$. Let $G = A^T A$ and $c_y = A^T y$. The update becomes:\n$$\nx_j^{\\text{new}} \\leftarrow S_\\lambda\\left( (c_y)_j - \\sum_{k \\neq j} G_{jk} x_k \\right)\n$$\nThe summation term can be written as $(G x)_j - G_{jj} x_j$. Since $G_{jj}=a_j^T a_j=1$, the argument for the soft-thresholding function is $(c_y)_j - ((G x)_j - x_j)$. The vector $x$ contains the most recently updated values of the coordinates.\n\nThe algorithm proceeds as follows:\n1.  **Initialization**: Set the pseudorandom seed to $2025$ for reproducibility. For each test case, construct the matrix $A$ and vector $y$ as specified. The columns $a_j$ are generated from i.i.d. standard normal distributions and normalized. The first two columns, $a_1$ and $a_2$, are constructed to have a specified correlation structure. The response is noiseless, $y=Ax^\\star$. Pre-compute $A^TA$ and $A^Ty$. Initialize the solution estimate $x^{(0)} = 0$.\n\n2.  **Cyclic Coordinate Descent (CCD)**: Iterate for $E$ epochs. In each epoch, update coordinates sequentially from $j=1, \\dots, p$.\n    $$\n    x_j \\leftarrow S_\\lambda\\left( (A^Ty)_j - \\left( \\sum_{k=1}^p (A^TA)_{jk} x_k - (A^TA)_{jj} x_j \\right) \\right)\n    $$\n    The values of $x_k$ used in the update for $x_j$ are the most current ones available.\n\n3.  **Randomized Coordinate Descent (RCD)**: Iterate for $E$ epochs. In each epoch, perform $p$ updates. For each update, select a coordinate $j \\in \\{1, \\dots, p\\}$ uniformly at random with replacement. Apply the same update rule as in CCD.\n\n4.  **Evaluation**: For both CCD and RCD, the vector of iterates after $t$ epochs is denoted $x^{(t)}$. The objective function values $f(x^{(t)})$ are recorded for $t=0, \\dots, E$. The average per-epoch objective decrease is calculated as:\n    $$\n    \\Delta_{\\mathrm{avg}} = \\frac{1}{E} \\sum_{t=0}^{E-1} \\left(f(x^{(t)}) - f(x^{(t+1)})\\right)\n    $$\n    The final reported value for each test case is the ratio $r = \\Delta_{\\mathrm{avg, RCD}} / \\Delta_{\\mathrm{avg, CCD}}$.\n\nThe code implements this logic, carefully following the construction details for $A$, the iterative update schemes for CCD and RCD, and the final calculation of the performance ratio.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the coordinate descent comparison problem for Lasso.\n    It implements and compares Cyclic Coordinate Descent (CCD) and\n    Randomized Coordinate Descent (RCD) on constructed test cases,\n    reporting the ratio of their average per-epoch objective decrease.\n    \"\"\"\n    \n    # Per the problem statement, a single seed is used for all random draws.\n    np.random.seed(2025)\n\n    test_cases = [\n        # Case 1: High correlation\n        {'m': 200, 'p': 2, 'rho': 0.999, 'lam': 0.05, 'x_star': [1.0, 1.0], 'E': 50},\n        # Case 2: Moderate correlation\n        {'m': 200, 'p': 2, 'rho': 0.9, 'lam': 0.05, 'x_star': [1.0, 1.0], 'E': 50},\n        # Case 3: No correlation\n        {'m': 200, 'p': 2, 'rho': 0.0, 'lam': 0.05, 'x_star': [1.0, 1.0], 'E': 50},\n        # Case 4: High correlation in a larger-p setting\n        {'m': 200, 'p': 10, 'rho': 0.999, 'lam': 0.05, 'x_star': [1.0, 1.0] + [0.0] * 8, 'E': 50}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        m, p, rho, lam, x_star_list, E = case['m'], case['p'], case['rho'], case['lam'], case['x_star'], case['E']\n        x_star = np.array(x_star_list, dtype=float)\n\n        # 1. Construct sensing matrix A and response vector y\n        A = np.zeros((m, p))\n        \n        # First column a_1\n        a1 = np.random.randn(m)\n        a1 /= np.linalg.norm(a1)\n        A[:, 0] = a1\n\n        # Second column a_2, constructed to be correlated with a_1\n        if p >= 2:\n            w = np.random.randn(m)\n            # The construction follows the problem statement verbatim.\n            # Adding max(0,...) ensures the argument to sqrt is non-negative.\n            a2_unnormalized = rho * a1 + np.sqrt(max(0, 1 - rho**2)) * w\n            A[:, 1] = a2_unnormalized / np.linalg.norm(a2_unnormalized)\n\n        # Remaining columns a_j for j >= 3\n        for j in range(2, p):\n            aj = np.random.randn(m)\n            aj /= np.linalg.norm(aj)\n            A[:, j] = aj\n            \n        # Noiseless response vector\n        y = A @ x_star\n        \n        # Precompute matrices for efficiency\n        AtA = A.T @ A\n        Aty = A.T @ y\n\n        # Helper functions defined within the loop to capture A, y, lam etc.\n        def objective_function(x):\n            residual = A @ x - y\n            l2_term = 0.5 * np.sum(residual**2)\n            l1_term = lam * np.sum(np.abs(x))\n            return l2_term + l1_term\n\n        def soft_threshold(z, l):\n            return np.sign(z) * np.maximum(np.abs(z) - l, 0.0)\n\n        # 2. Cyclic Coordinate Descent (CCD)\n        x_ccd = np.zeros(p)\n        f_values_ccd = [objective_function(x_ccd)]\n        for _ in range(E):\n            for j in range(p):\n                # Update rule for coordinate j\n                val = Aty[j] - (np.dot(AtA[j, :], x_ccd) - AtA[j,j] * x_ccd[j])\n                x_ccd[j] = soft_threshold(val, lam)\n            f_values_ccd.append(objective_function(x_ccd))\n        \n        decreases_ccd = [f_values_ccd[t] - f_values_ccd[t+1] for t in range(E)]\n        delta_ccd = np.mean(decreases_ccd)\n\n        # 3. Randomized Coordinate Descent (RCD)\n        x_rcd = np.zeros(p)\n        f_values_rcd = [objective_function(x_rcd)]\n        for _ in range(E):\n            # An epoch consists of p updates at random coordinates\n            for _ in range(p):\n                j = np.random.randint(0, p)\n                val = Aty[j] - (np.dot(AtA[j, :], x_rcd) - AtA[j,j] * x_rcd[j])\n                x_rcd[j] = soft_threshold(val, lam)\n            f_values_rcd.append(objective_function(x_rcd))\n\n        decreases_rcd = [f_values_rcd[t] - f_values_rcd[t+1] for t in range(E)]\n        delta_rcd = np.mean(decreases_rcd)\n\n        # 4. Compute and store the ratio\n        if delta_ccd == 0.0:\n            ratio = np.inf if delta_rcd > 0 else 1.0\n        else:\n            ratio = delta_rcd / delta_ccd\n            \n        results.append(ratio)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3441210"}, {"introduction": "After establishing the benefits of randomization, we can seek further acceleration by optimizing the step size at each iteration. Instead of using a conservative, fixed step size based on the worst-case curvature, this practice [@problem_id:3472576] introduces the adaptive Barzilai–Borwein (BB) method, which uses information from recent updates to estimate local curvature. You will implement this quasi-Newton technique in a stochastic, coordinate-wise setting to see how it can speed up convergence on ill-conditioned problems, while also learning about the necessity of safeguards to ensure stability.", "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) problem, which seeks to minimize the convex objective $$F(x) = \\tfrac{1}{2}\\lVert A x - b\\rVert_2^2 + \\lambda \\lVert x\\rVert_1$$ over $x \\in \\mathbb{R}^n$, where $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, and $\\lambda > 0$. The smooth part of $F(x)$ is $$f(x) = \\tfrac{1}{2}\\lVert A x - b\\rVert_2^2,$$ whose gradient is $$\\nabla f(x) = A^\\top (A x - b).$$ For a single coordinate $j$, define the coordinate-wise Lipschitz constant $$L_j = \\lVert A_{:,j} \\rVert_2^2,$$ where $A_{:,j}$ denotes the $j$-th column of $A$. The coordinate-wise proximal gradient update with step size $\\alpha_j$ is $$x_j^{\\text{new}} = S_{\\lambda \\alpha_j}\\!\\left(x_j - \\alpha_j\\, [\\nabla f(x)]_j\\right),$$ where $S_{\\tau}(z) = \\operatorname{sign}(z)\\max\\{|z| - \\tau, 0\\}$ is the soft-thresholding operator at level $\\tau$.\n\nIn randomized coordinate descent, one selects $j$ uniformly at random at each iteration and applies the above update. The baseline choice $\\alpha_j = 1/L_j$ yields a safe step based on curvature. A different choice uses a coordinate-wise Barzilai–Borwein (BB) scaling, which estimates curvature via successive differences in a stochastic manner induced by randomized coordinate visits. The BB step for coordinate $j$ is conceptually given by $$\\eta_j \\approx \\frac{\\Delta x_j}{\\Delta g_j},$$ where $\\Delta x_j$ is the change in the $j$-th coordinate between two successive randomized visits and $\\Delta g_j$ is the corresponding change in the $j$-th component of the gradient of $f(x)$ at those visits. Because the visit times are random, these differences are stochastic measurements of local curvature. To ensure stability, one clips the BB step into a window determined by coordinate curvature, $$\\alpha_j \\in \\left[\\frac{c_{\\min}}{L_j}, \\frac{c_{\\max}}{L_j}\\right],$$ where $c_{\\min}$ and $c_{\\max}$ are positive constants. The question is whether this randomized BB scaling improves convergence relative to the baseline step for ill-conditioned matrices and, if so, under what curvature window it remains stable.\n\nStarting from the fundamental definitions above, implement a program that:\n\n1. Constructs synthetic matrices $A$ with prescribed condition numbers by composing random orthonormal factors with singular values spanning a geometric progression. Let $m<n$ and define the singular values $\\sigma_i$ to satisfy $\\sigma_1/\\sigma_m = K$, where $K$ is the target condition number. Use a randomized but reproducible construction.\n\n2. Generates a sparse ground-truth vector $x^\\star$ with $s$ nonzeros and forms noisy measurements $b = A x^\\star + \\varepsilon$, where $\\varepsilon$ is small Gaussian noise.\n\n3. Runs two randomized coordinate descent solvers for the LASSO for a fixed number of iterations $T$ using the same randomized coordinate sequence:\n   - Baseline solver with $\\alpha_j = 1/L_j$.\n   - Randomized BB solver that, upon each visit to coordinate $j$, computes a BB estimate $\\eta_j$ from successive visit differences, ensures positivity, and then clips it to $\\alpha_j \\in [c_{\\min}/L_j, c_{\\max}/L_j]$ before applying the proximal update.\n\n4. Tracks the objective $F(x)$ after each update in the randomized BB solver and reports the stability of the randomized BB scheme in terms of monotonic decrease frequency, defined as the fraction of iterations with $$F(x^{(t)}) \\le F(x^{(t-1)}).$$ Declare stability as a boolean if this fraction is at least $0.95$.\n\n5. Reports the improvement of randomized BB relative to baseline as the normalized difference $$I = \\frac{F_{\\text{baseline}} - F_{\\text{BB}}}{\\max\\{F_{\\text{baseline}}, 10^{-12}\\}}$$ after $T$ iterations, where $F_{\\text{baseline}}$ and $F_{\\text{BB}}$ are the final objective values for the baseline and randomized BB solvers, respectively.\n\nUse the following test suite that covers well-conditioned and ill-conditioned cases and explores curvature windows via $c_{\\min}$ and $c_{\\max}$:\n\n- Case 1 (happy path, well-conditioned): $m=120$, $n=300$, $K=10$, $s=20$, $\\lambda=0.05$, $T=15000$, $c_{\\min}=0.2$, $c_{\\max}=1.0$.\n- Case 2 (ill-conditioned, stability window): $m=120$, $n=300$, $K=10^4$, $s=20$, $\\lambda=0.05$, $T=15000$, $c_{\\min}=0.2$, $c_{\\max}=1.0$.\n- Case 3 (ill-conditioned, overshoot window): $m=120$, $n=300$, $K=10^4$, $s=20$, $\\lambda=0.05$, $T=15000$, $c_{\\min}=0.2$, $c_{\\max}=1.5$.\n- Case 4 (ill-conditioned, high regularization): $m=120$, $n=300$, $K=10^3$, $s=10$, $\\lambda=0.5$, $T=15000$, $c_{\\min}=0.2$, $c_{\\max}=1.0$.\n\nAll random draws must be reproducible with fixed seeds inside the program. The final output format is a single line containing the results for all cases as a comma-separated list of lists, where each inner list is of the form $[I, S]$ with $I$ a float and $S$ a boolean. For example, the program should print something like $$[[I_1,S_1],[I_2,S_2],[I_3,S_3],[I_4,S_4]].$$ There are no physical units involved, and angles do not appear. Ensure scientific realism by using convex objectives and mathematically sound update rules derived from the provided definitions.", "solution": "### 1. Mathematical Formulation and Preliminaries\n\nThe problem centers on the LASSO objective function:\n$$F(x) = \\frac{1}{2}\\lVert A x - b\\rVert_2^2 + \\lambda \\lVert x\\rVert_1$$\nwhere $x \\in \\mathbb{R}^n$ is the optimization variable, $A \\in \\mathbb{R}^{m \\times n}$ is the data matrix, $b \\in \\mathbb{R}^m$ is the observation vector, and $\\lambda > 0$ is the regularization parameter. The objective is a sum of a smooth, differentiable quadratic term, $f(x) = \\frac{1}{2}\\lVert A x - b\\rVert_2^2$, and a non-smooth but convex regularization term, $g(x) = \\lambda \\lVert x\\rVert_1$.\n\nCoordinate descent methods iteratively minimize the objective with respect to a single coordinate $j$ at a time, while keeping all other coordinates fixed. The update for coordinate $j$ is derived from the proximal gradient method applied to the one-dimensional subproblem. This yields the update rule:\n$$x_j \\leftarrow S_{\\lambda \\alpha_j}\\!\\left(x_j - \\alpha_j\\, [\\nabla f(x)]_j\\right)$$\nwhere $S_{\\tau}(z)$ is the soft-thresholding operator, $\\alpha_j$ is the step size, and $[\\nabla f(x)]_j$ is the $j$-th component of the gradient of the smooth part:\n$$[\\nabla f(x)]_j = A_{:,j}^\\top (A x - b)$$\nwhere $A_{:,j}$ is the $j$-th column of $A$. To improve efficiency, we maintain the residual $r = Ax - b$. The gradient component is then $[\\nabla f(x)]_j = A_{:,j}^\\top r$. When $x_j$ is updated by $\\Delta x_j$, the residual is updated efficiently via $r \\leftarrow r + A_{:,j} \\Delta x_j$.\n\n### 2. Step Size Strategies\n\nThe core of the problem lies in the choice of the step size $\\alpha_j$.\n\n**Baseline (Inverse Lipschitz) Step:** A guaranteed-convergent choice for the step size is the inverse of the coordinate-wise Lipschitz constant of the gradient, $L_j$. This constant is an upper bound on the curvature of $f(x)$ along coordinate $j$:\n$$L_j = \\lVert A_{:,j} \\rVert_2^2$$\nThe baseline step size is thus $\\alpha_j = 1/L_j$. This choice is conservative but ensures monotonic decrease of the objective function.\n\n**Randomized Barzilai-Borwein (BB) Step:** The BB method approximates the Hessian using finite differences of the gradient and variable, forming a secant-like approximation. In the context of randomized coordinate descent, the step size for coordinate $j$ is based on information from successive visits to this coordinate.\n\nLet $k_1$ and $k_2$ be two successive iteration indices where coordinate $j$ is selected. The state of the system (the vector $x$) evolves between these visits due to updates at other coordinates. The BB step size $\\eta_j$ is conceptually $\\eta_j \\approx \\Delta x_j / \\Delta g_j$. Following the problem description, we define:\n-   $\\Delta x_j$: The change in $x_j$ that occurred *during* the update at the first visit, $k_1$. Let this be $\\delta_j^{(k_1)} = x_j^{(k_1)} - x_j^{(k_1-1)}$.\n-   $\\Delta g_j$: The change in the $j$-th gradient component between the *start* of the first visit ($k_1$) and the *start* of the second visit ($k_2$). That is, $\\Delta g_j = [\\nabla f(x^{(k_2-1)})]_j - [\\nabla f(x^{(k_1-1)})]_j$.\n\nThis formulation captures how updates to other coordinates between visits influence the curvature perceived at coordinate $j$. The resulting BB step estimate is then:\n$$\\eta_j = \\frac{\\delta_j^{(k_1)}}{[\\nabla f(x^{(k_2-1)})]_j - [\\nabla f(x^{(k_1-1)})]_j}$$\nTo ensure stability, this step size is used only if it's positive (indicating correct curvature sense) and is clipped to a window:\n$$\\alpha_j = \\operatorname{clip}\\left(\\eta_j, \\frac{c_{\\min}}{L_j}, \\frac{c_{\\max}}{L_j}\\right)$$\nIf $\\eta_j \\le 0$, we revert to the safe baseline step $\\alpha_j = 1/L_j$. For the very first visit to any coordinate, we also use the baseline step.\n\n### 3. Algorithm Implementation\n\nThe implementation will consist of the following components:\n\n-   **Problem Generation:** A function to construct the matrix $A$ with a specified condition number $K$ by forming $A = U \\Sigma V_m^\\top$. Here, $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are random orthonormal matrices (from QR decomposition of random matrices), and $\\Sigma \\in \\mathbb{R}^{m \\times m}$ is a diagonal matrix of singular values $\\sigma_i$ forming a geometric progression from $1$ to $1/K$. The ground-truth sparse vector $x^\\star$ and measurement vector $b = A x^\\star + \\varepsilon$ are generated using a reproducible random number generator.\n\n-   **Solver Function:** A single function `run_solver` will implement both the baseline and randomized BB versions of the randomized coordinate descent algorithm. It will accept a `method` parameter to switch between the two behaviors. The function will use the efficient residual update strategy. For the 'bb' method, it will maintain the necessary state for each coordinate (gradient at last visit, change in coordinate at last visit) to compute the BB step size.\n\n-   **Main Loop:** The main `solve` function will iterate through the four test cases. For each case, it will:\n    1.  Generate the specific problem instance ($A, b, x^\\star$) using a fixed seed for reproducibility.\n    2.  Generate a single, shared sequence of random coordinates to be used by both solvers for a fair comparison.\n    3.  Run the baseline solver to obtain the final objective value $F_{\\text{baseline}}$.\n    4.  Run the randomized BB solver, which returns its final objective value $F_{\\text{BB}}$ and the history of objective values at each iteration.\n    5.  Calculate the stability metric $S$ by checking if the monotonic decrease frequency in the BB solver's objective history is at least $0.95$. A small tolerance ($10^{-12}$) is used to account for floating-point inaccuracies.\n    6.  Calculate the relative improvement metric $I = (F_{\\text{baseline}} - F_{\\text{BB}}) / \\max\\{F_{\\text{baseline}}, 10^{-12}\\}$.\n    7.  Store and format the pair $[I, S]$ for final output.\n\nAll random processes are seeded to ensure full reproducibility of the experiment as required.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the full analysis as described in the problem.\n    It sets up test cases, generates data, runs solvers, and reports results.\n    \"\"\"\n\n    def soft_threshold(z, tau):\n        \"\"\"Soft-thresholding operator S_tau(z).\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - tau, 0)\n\n    def run_solver(A, b, lambda_, T, j_sequence, L, method, c_min=None, c_max=None):\n        \"\"\"\n        Runs Randomized Coordinate Descent for LASSO.\n\n        Args:\n            A (np.ndarray): The design matrix.\n            b (np.ndarray): The measurement vector.\n            lambda_ (float): The regularization parameter.\n            T (int): The number of iterations.\n            j_sequence (np.ndarray): The pre-generated sequence of coordinates.\n            L (np.ndarray): The coordinate-wise Lipschitz constants.\n            method (str): 'baseline' or 'bb'.\n            c_min (float, optional): Min clipping factor for BB step.\n            c_max (float, optional): Max clipping factor for BB step.\n\n        Returns:\n            A tuple containing:\n            - float: The final objective value.\n            - list or None: History of objective values (only for 'bb' method).\n        \"\"\"\n        m, n = A.shape\n        x = np.zeros(n)\n        r = -b.copy()\n\n        # Method-specific initializations for the BB variant\n        if method == 'bb':\n            obj_history = []\n            grad_last_visit = np.zeros(n)\n            delta_x_last_visit = np.zeros(n)\n            first_visit = np.ones(n, dtype=bool)\n\n        # Main RCD loop\n        for t in range(T):\n            j = j_sequence[t]\n            \n            # The gradient component is computed using the current residual\n            grad_j_current = A[:, j].T @ r\n\n            # --- Step size calculation ---\n            if method == 'baseline':\n                alpha_j = 1.0 / L[j]\n            elif method == 'bb':\n                if first_visit[j]:\n                    alpha_j = 1.0 / L[j]\n                else:\n                    delta_x_prev = delta_x_last_visit[j]\n                    grad_j_prev = grad_last_visit[j]\n                    delta_g = grad_j_current - grad_j_prev\n                    \n                    # Compute BB step, ensuring valid numerator and denominator\n                    if np.abs(delta_x_prev) > 1e-12 and np.abs(delta_g) > 1e-12:\n                        eta_j = delta_x_prev / delta_g\n                        # Enforce positivity and clipping as per problem description\n                        if eta_j > 0:\n                            alpha_j = np.clip(eta_j, c_min / L[j], c_max / L[j])\n                        else:\n                            alpha_j = 1.0 / L[j]  # Fallback to safe step\n                    else:\n                        alpha_j = 1.0 / L[j]  # Fallback if no information\n            \n            # --- Store information for the next BB step for this coordinate ---\n            if method == 'bb':\n                grad_last_visit[j] = grad_j_current\n                first_visit[j] = False\n                \n            # --- Proximal gradient update ---\n            x_j_old = x[j]\n            z = x_j_old - alpha_j * grad_j_current\n            x_j_new = soft_threshold(z, lambda_ * alpha_j)\n            \n            delta_x_j = x_j_new - x_j_old\n            \n            # --- Update state (vector x and residual r) ---\n            x[j] = x_j_new\n            r += A[:, j] * delta_x_j\n            \n            if method == 'bb':\n                delta_x_last_visit[j] = delta_x_j\n                # Track objective value for stability analysis\n                F_current = 0.5 * (r @ r) + lambda_ * np.sum(np.abs(x))\n                obj_history.append(F_current)\n\n        # Calculate final objective value after T iterations\n        final_obj = 0.5 * (r @ r) + lambda_ * np.sum(np.abs(x))\n        \n        if method == 'bb':\n            return final_obj, obj_history\n        else:  # baseline\n            return final_obj, None\n\n    # Test cases as defined in the problem statement\n    test_cases = [\n        {'m': 120, 'n': 300, 'K': 10, 's': 20, 'lambda': 0.05, 'T': 15000, 'c_min': 0.2, 'c_max': 1.0},\n        {'m': 120, 'n': 300, 'K': 10**4, 's': 20, 'lambda': 0.05, 'T': 15000, 'c_min': 0.2, 'c_max': 1.0},\n        {'m': 120, 'n': 300, 'K': 10**4, 's': 20, 'lambda': 0.05, 'T': 15000, 'c_min': 0.2, 'c_max': 1.5},\n        {'m': 120, 'n': 300, 'K': 10**3, 's': 10, 'lambda': 0.5, 'T': 15000, 'c_min': 0.2, 'c_max': 1.0},\n    ]\n\n    results = []\n    master_seed = 42  # For full reproducibility\n\n    for case_idx, params in enumerate(test_cases):\n        # Use a reproducible RNG for each distinct test case\n        case_rng = np.random.default_rng(master_seed + case_idx)\n        \n        m, n, K = params['m'], params['n'], params['K']\n        s, lambda_ = params['s'], params['lambda']\n        T, c_min, c_max = params['T'], params['c_min'], params['c_max']\n\n        # 1. Construct matrix A with prescribed condition number K\n        U, _ = np.linalg.qr(case_rng.standard_normal(size=(m, m)))\n        V, _ = np.linalg.qr(case_rng.standard_normal(size=(n, n)))\n        s_vals = np.logspace(0, -np.log10(K), num=m)\n        Sigma = np.diag(s_vals)\n        A = U @ Sigma @ V[:, :m].T\n\n        # 2. Generate sparse ground-truth x_star and noisy measurements b\n        x_star = np.zeros(n)\n        indices = case_rng.choice(n, s, replace=False)\n        x_star[indices] = case_rng.standard_normal(s)\n        \n        b_clean = A @ x_star\n        noise = case_rng.standard_normal(m)\n        noise_std = 0.01 * np.linalg.norm(b_clean) / np.sqrt(m)\n        b = b_clean + noise_std * noise\n\n        # Precompute coordinate-wise Lipschitz constants L_j = ||A_j||^2\n        L = np.sum(A**2, axis=0)\n        # Add a small epsilon to avoid division by zero for null columns\n        L[L == 0] = 1e-12\n\n        # 3. Generate shared coordinate sequence for a fair comparison\n        j_sequence = case_rng.integers(0, n, size=T)\n\n        # Run baseline solver\n        F_baseline, _ = run_solver(A, b, lambda_, T, j_sequence, L, 'baseline')\n\n        # Run randomized BB solver\n        F_bb, obj_history = run_solver(A, b, lambda_, T, j_sequence, L, 'bb', c_min, c_max)\n        \n        # 4. Evaluate stability S of the BB solver\n        monotonic_decreases = 0\n        if T > 1:\n            for i in range(1, len(obj_history)):\n                if obj_history[i] = obj_history[i-1] + 1e-12:\n                    monotonic_decreases += 1\n            stability_fraction = monotonic_decreases / (T - 1)\n        else:\n            stability_fraction = 1.0\n        S = stability_fraction >= 0.95\n\n        # 5. Evaluate relative improvement I\n        I = (F_baseline - F_bb) / np.maximum(F_baseline, 1e-12)\n        \n        results.append([I, S])\n    \n    # Format the final output string exactly as required\n    formatted_results = [f\"[{i:.6f},{str(s).lower()}]\" for i, s in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3472576"}]}