{"hands_on_practices": [{"introduction": "The effectiveness of Basis Pursuit hinges on the unique, non-smooth geometry of the $\\ell_1$-norm, which promotes sparsity in solutions. To rigorously analyze this property and establish the conditions for optimality, we must move beyond classical calculus and employ the tools of convex analysis. This exercise guides you through the fundamental derivation of the subdifferential of the $\\|x\\|_1$ function, a concept that replaces the gradient for non-differentiable convex functions and provides the key to unlocking the mechanism of sparse recovery [@problem_id:3433124].", "problem": "In the context of basis pursuit as an optimization model in compressed sensing, consider the convex function given by the one-norm on $\\mathbb{R}^{n}$, namely $x \\mapsto \\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$. This function is used as the regularizer in basis pursuit, which seeks to minimize $\\|x\\|_{1}$ subject to linear equality constraints. Working from first principles of convex analysis and without invoking any pre-memorized subdifferential rules beyond core definitions, compute the subdifferential $\\partial \\|x\\|_{1}$ at an arbitrary point $x \\in \\mathbb{R}^{n}$. Your derivation must begin from the definition of the subdifferential of a proper, convex function, namely\n$$\n\\partial f(x) \\;=\\; \\{\\, g \\in \\mathbb{R}^{n} \\;:\\; f(y) \\,\\ge\\, f(x) + \\langle g,\\, y - x \\rangle \\;\\text{ for all }\\; y \\in \\mathbb{R}^{n} \\,\\}.\n$$\nUse separability to reduce to the scalar absolute value map $t \\mapsto |t|$ and characterize precisely the set-valued behavior of the subdifferential at coordinates where $x_{i} \\neq 0$ versus where $x_{i} = 0$. State the final answer as a closed-form expression for the entire set $\\partial \\|x\\|_{1}$ in terms of the components of $x$. No numerical rounding is required, and no physical units are involved. The final answer must be a single analytic expression.", "solution": "The problem is to compute the subdifferential of the $L_1$-norm function $f: \\mathbb{R}^{n} \\to \\mathbb{R}$ defined by $f(x) = \\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$ at an arbitrary point $x \\in \\mathbb{R}^{n}$. The derivation must proceed from the first principles of convex analysis.\n\nThe problem statement is first validated.\n\\\n**Step 1: Extract Givens**\n-   **Function:** $f(x) = \\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$ for $x \\in \\mathbb{R}^{n}$.\n-   **Domain:** The function is defined on $\\mathbb{R}^{n}$.\n-   **Definition of Subdifferential:** For a proper, convex function $f$, the subdifferential at $x$ is defined as $\\partial f(x) = \\{ g \\in \\mathbb{R}^{n} : f(y) \\ge f(x) + \\langle g, y - x \\rangle \\text{ for all } y \\in \\mathbb{R}^{n} \\}$.\n-   **Constraint/Hint:** The derivation should use the separability of the function to reduce the problem to the scalar case.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is a standard exercise in convex analysis, a core topic in optimization and signal processing. All definitions and concepts are mathematically sound.\n-   **Well-Posed:** The problem is well-posed. The subdifferential of a proper, convex function is a well-defined mathematical object, and its characterization is a solvable problem with a unique answer.\n-   **Objective:** The language is formal and objective.\n-   **Completeness:** The problem is self-contained, providing the function and the definition of the subdifferential.\n-   **Other Flaws:** The problem is free from the other listed flaws (unrealistic, ill-posed, trivial, etc.). It is a legitimate and substantive mathematical problem.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A full solution will be provided.\n\n**Derivation**\n\nThe function in question is $f(x) = \\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$. This is a proper, convex function on $\\mathbb{R}^{n}$. We are tasked with finding its subdifferential, $\\partial f(x)$.\n\nThe function $f(x)$ is separable, meaning it can be written as a sum of functions of its individual components:\n$$\nf(x) = f(x_1, x_2, \\ldots, x_n) = \\sum_{i=1}^{n} \\phi(x_i)\n$$\nwhere $\\phi: \\mathbb{R} \\to \\mathbb{R}$ is the absolute value function, $\\phi(t) = |t|$.\n\nFor a separable convex function of this form, the subdifferential $\\partial f(x)$ is the Cartesian product of the subdifferentials of the individual component functions $\\phi$ evaluated at each component $x_i$:\n$$\n\\partial f(x) = \\partial \\phi(x_1) \\times \\partial \\phi(x_2) \\times \\cdots \\times \\partial \\phi(x_n)\n$$\nThis means that a vector $g = (g_1, g_2, \\ldots, g_n) \\in \\mathbb{R}^{n}$ is an element of $\\partial \\|x\\|_1$ if and only if $g_i \\in \\partial |\\cdot|(x_i)$ for every $i \\in \\{1, 2, \\ldots, n\\}$.\n\nThe problem is thus reduced to finding the subdifferential of the scalar absolute value function $\\phi(t) = |t|$ at a point $t \\in \\mathbb{R}$. The definition of the subdifferential for a scalar function is the set of all subgradients $s \\in \\mathbb{R}$ such that:\n$$\n\\partial \\phi(t) = \\{ s \\in \\mathbb{R} \\mid \\phi(u) \\ge \\phi(t) + s(u - t) \\text{ for all } u \\in \\mathbb{R} \\}\n$$\nSubstituting $\\phi(t)=|t|$, we seek $s$ such that:\n$$\n|u| \\ge |t| + s(u - t) \\quad \\text{for all } u \\in \\mathbb{R}\n$$\n\nWe analyze this condition by considering three cases for the point $t$.\n\n**Case 1: $t  0$**\nFor $t  0$, the function $\\phi(t) = |t|$ is differentiable in a neighborhood of $t$, with $\\phi(t) = t$. The derivative is $\\phi'(t) = 1$. For a convex function, if it is differentiable at a point, its subdifferential at that point is a singleton set containing just the derivative. Thus, for $t  0$:\n$$\n\\partial |t| = \\{1\\}\n$$\n\n**Case 2: $t  0$**\nFor $t  0$, the function $\\phi(t) = |t|$ is differentiable in a neighborhood of $t$, with $\\phi(t) = -t$. The derivative is $\\phi'(t) = -1$. As in the previous case, the subdifferential is the singleton set containing the derivative. Thus, for $t  0$:\n$$\n\\partial |t| = \\{-1\\}\n$$\nWe can combine Case 1 and Case 2 using the signum function, $\\mathrm{sgn}(t)$, defined for non-zero arguments as $\\mathrm{sgn}(t) = 1$ if $t  0$ and $\\mathrm{sgn}(t) = -1$ if $t  0$. For $t \\ne 0$, we have:\n$$\n\\partial |t| = \\{\\mathrm{sgn}(t)\\}\n$$\n\n**Case 3: $t = 0$**\nAt $t = 0$, the function $\\phi(t) = |t|$ is not differentiable. We must use the fundamental definition of the subdifferential. We are looking for all $s \\in \\mathbb{R}$ such that for all $u \\in \\mathbb{R}$:\n$$\n|u| \\ge |0| + s(u - 0)\n$$\nThis inequality simplifies to:\n$$\n|u| \\ge s \\cdot u\n$$\nTo find the valid range for $s$, we test this inequality:\n- If we choose $u  0$, the inequality becomes $u \\ge s \\cdot u$. Dividing by a positive $u$ gives $1 \\ge s$.\n- If we choose $u  0$, the inequality becomes $-u \\ge s \\cdot u$. Dividing by a negative $u$ reverses the inequality sign, giving $ -1 \\le s$.\n- If we choose $u = 0$, the inequality becomes $0 \\ge 0$, which is true for any $s$.\n\nCombining the conditions on $s$, we must have $-1 \\le s \\le 1$. Therefore, the subdifferential at $t = 0$ is the closed interval $[-1, 1]$:\n$$\n\\partial |0| = [-1, 1]\n$$\n\nNow, we assemble these results to characterize the subdifferential $\\partial \\|x\\|_1$ for a vector $x \\in \\mathbb{R}^n$. A vector $g \\in \\mathbb{R}^n$ lies in $\\partial \\|x\\|_1$ if and only if each of its components $g_i$ is a member of the subdifferential $\\partial |x_i|$.\n\nBased on our scalar analysis, the condition on each component $g_i$ is:\n1.  If $x_i \\ne 0$, then $g_i$ must be the unique element of $\\partial |x_i|$, which is $g_i = \\mathrm{sgn}(x_i)$.\n2.  If $x_i = 0$, then $g_i$ can be any element of $\\partial |0|$, which means $g_i \\in [-1, 1]$.\n\nWe can express the entire set $\\partial \\|x\\|_{1}$ using set-builder notation. This provides a complete and formal description of all subgradients $g$ at the point $x$. The subdifferential $\\partial \\|x\\|_1$ is the set of vectors $g \\in \\mathbb{R}^n$ whose components $g_i$ satisfy the conditions derived above.", "answer": "$$\n\\boxed{\\partial \\|x\\|_{1} = \\left\\{ g \\in \\mathbb{R}^{n} \\mid g_i = \\mathrm{sgn}(x_i) \\text{ if } x_i \\neq 0, \\text{ and } g_i \\in [-1, 1] \\text{ if } x_i = 0 \\right\\}}\n$$", "id": "3433124"}, {"introduction": "While Basis Pursuit is remarkably effective, it is a convex proxy for the true, intractable problem of finding the sparsest solution. It is crucial to understand the precise conditions under which this proxy is exact. This practice challenges you to explore a scenario where $\\ell_1$-minimization fails to recover the sparsest vector, and to connect this failure directly to the geometric properties of the sensing matrix by analyzing its null space, thereby illustrating the practical importance of the Null Space Property (NSP) [@problem_id:3433126].", "problem": "Consider the Basis Pursuit formulation of sparse recovery, which seeks a vector $x \\in \\mathbb{R}^{n}$ minimizing the $\\ell_{1}$-norm subject to linear equality constraints $A x = y$, where $A \\in \\mathbb{R}^{m \\times n}$ and $y \\in \\mathbb{R}^{m}$. Start from the core definitions of sparsity, the $\\ell_{1}$-norm, and the feasible set described by a linear system. Additionally, use the definition of the Null Space Property (NSP): a matrix $A$ is said to satisfy the NSP of order $s$ if for every nonzero $h \\in \\ker(A)$ and every index set $S \\subset \\{1,\\dots,n\\}$ with $|S| \\leq s$, one has $\\|h_{S}\\|_{1}  \\|h_{S^{c}}\\|_{1}$.\n\nLet\n$$\nA \\;=\\;\n\\begin{pmatrix}\n1  0  \\frac{1}{100} \\\\\n0  1  \\frac{1}{100}\n\\end{pmatrix},\n\\qquad\ny \\;=\\;\n\\begin{pmatrix}\n\\frac{1}{100} \\\\\n\\frac{1}{100}\n\\end{pmatrix}.\n$$\n\nYour tasks are:\n- Determine the sparsest solution $x_{0}$ to $A x = y$ and its support size.\n- Characterize the entire feasible set $\\{x \\in \\mathbb{R}^{3} : A x = y\\}$ through an explicit parameterization derived from the linear constraints.\n- Using only the definition of the $\\ell_{1}$-norm and the piecewise-linear structure induced by absolute values, determine the exact minimizer of the Basis Pursuit problem $\\min \\|x\\|_{1}$ subject to $A x = y$.\n- Conclude whether the $\\ell_{1}$ minimizer coincides with the sparsest solution and justify your conclusion.\n- Analyze the matrix property responsible for any failure by computing, for the support $S$ of the sparsest solution $x_{0}$, the Null Space Property ratio\n$$\nR_{S} \\;=\\; \\frac{\\|h_{S}\\|_{1}}{\\|h_{S^{c}}\\|_{1}},\n$$\nwhere $h$ is a nonzero generator of $\\ker(A)$. Express the final value of $R_{S}$ exactly, with no rounding.", "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- The problem is to find a vector $x \\in \\mathbb{R}^{n}$ that minimizes the $\\ell_{1}$-norm, $\\|x\\|_{1}$, subject to the linear constraints $A x = y$.\n- The matrix $A \\in \\mathbb{R}^{m \\times n}$ is given by\n$$A = \\begin{pmatrix} 1  0  \\frac{1}{100} \\\\ 0  1  \\frac{1}{100} \\end{pmatrix}$$\nwhere $m=2$ and $n=3$.\n- The vector $y \\in \\mathbb{R}^{m}$ is given by\n$$y = \\begin{pmatrix} \\frac{1}{100} \\\\ \\frac{1}{100} \\end{pmatrix}$$\n- The Null Space Property (NSP) of order $s$ is defined: a matrix $A$ satisfies it if for every nonzero $h \\in \\ker(A)$ and every index set $S \\subset \\{1,\\dots,n\\}$ with $|S| \\leq s$, the inequality $\\|h_{S}\\|_{1}  \\|h_{S^{c}}\\|_{1}$ holds. Here $h_S$ is the vector $h$ with entries not in $S$ set to zero.\n- The tasks are:\n    1. Determine the sparsest solution $x_{0}$ to $A x = y$ and its support size.\n    2. Characterize the feasible set $\\{x \\in \\mathbb{R}^{3} : A x = y\\}$.\n    3. Determine the exact minimizer of the Basis Pursuit problem $\\min \\|x\\|_{1}$ subject to $A x = y$.\n    4. Conclude whether the $\\ell_{1}$ minimizer coincides with the sparsest solution.\n    5. Compute the NSP ratio $R_{S} = \\frac{\\|h_{S}\\|_{1}}{\\|h_{S^{c}}\\|_{1}}$ for the support $S$ of $x_0$ and a generator $h$ of $\\ker(A)$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is a standard exercise in the field of compressed sensing and sparse optimization. The concepts of Basis Pursuit, $\\ell_1$-minimization, and the Null Space Property are fundamental to the theory.\n- **Well-Posed**: The problem is clearly stated with all necessary data provided. The matrix and vector dimensions ($A$ is $2 \\times 3$, $x$ is $3 \\times 1$, $y$ is $2 \\times 1$) are consistent. Each task is a well-defined mathematical objective.\n- **Objective**: The language is precise, mathematical, and free of any subjective claims.\n\nThe problem is self-contained, consistent, and scientifically sound. No flaws are detected.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. Proceeding to the solution.\n\n***\n\n### Solution\nThe problem asks for a multi-part analysis of a specific Basis Pursuit instance. We will address each task in sequence.\n\n**1. Determine the sparsest solution $x_{0}$ and its support size.**\nThe sparsest solution to $A x = y$ is the solution with the minimum number of non-zero components, i.e., the one that minimizes the $\\ell_0$-\"norm\" $\\|x\\|_0 = |\\{i : x_i \\neq 0\\}|$. We look for solutions of increasing sparsity.\n\n- **Sparsity 1**: A 1-sparse solution would have only one non-zero entry. Let's test the three possibilities.\n    - If $x = (x_1, 0, 0)^T$, then $A x = \\begin{pmatrix} 1  0  1/100 \\\\ 0  1  1/100 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} x_1 \\\\ 0 \\end{pmatrix}$. This cannot equal $y = (1/100, 1/100)^T$.\n    - If $x = (0, x_2, 0)^T$, then $A x = \\begin{pmatrix} 1  0  1/100 \\\\ 0  1  1/100 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ x_2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ x_2 \\end{pmatrix}$. This cannot equal $y$.\n    - If $x = (0, 0, x_3)^T$, then $A x = \\begin{pmatrix} 1  0  1/100 \\\\ 0  1  1/100 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} x_3/100 \\\\ x_3/100 \\end{pmatrix}$. Setting this equal to $y$ gives $x_3/100 = 1/100$, which implies $x_3 = 1$.\nThus, a 1-sparse solution exists: $x_0 = (0, 0, 1)^T$. Since no solution can be sparser than 1-sparse (the zero vector is not a solution as $y \\neq 0$), this is the sparsest solution.\nThe sparsest solution is $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$. Its support is $S_0 = \\{3\\}$, and its support size (sparsity) is $|S_0|=1$.\n\n**2. Characterize the entire feasible set.**\nThe set of all solutions to $A x = y$ is an affine subspace, which can be written as $x = x_p + h$, where $x_p$ is any particular solution to $A x = y$ and $h$ is any vector in the null space of $A$, $\\ker(A)$. We can use $x_p = x_0 = (0, 0, 1)^T$.\n\nTo find $\\ker(A)$, we solve $A h = 0$ for $h = (h_1, h_2, h_3)^T$:\n$$\n\\begin{pmatrix} 1  0  \\frac{1}{100} \\\\ 0  1  \\frac{1}{100} \\end{pmatrix} \\begin{pmatrix} h_1 \\\\ h_2 \\\\ h_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis gives the equations:\n$h_1 + \\frac{1}{100} h_3 = 0 \\implies h_1 = -\\frac{1}{100} h_3$\n$h_2 + \\frac{1}{100} h_3 = 0 \\implies h_2 = -\\frac{1}{100} h_3$\nThe null space consists of all vectors of the form $h = c \\begin{pmatrix} -1/100 \\\\ -1/100 \\\\ 1 \\end{pmatrix}$ for any scalar $c \\in \\mathbb{R}$. A generator for this 1-dimensional null space is $h_{gen} = \\begin{pmatrix} -1/100 \\\\ -1/100 \\\\ 1 \\end{pmatrix}$.\n\nThe feasible set is parameterized by $t \\in \\mathbb{R}$:\n$$x(t) = x_0 + t \\, h_{gen} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} + t \\begin{pmatrix} -1/100 \\\\ -1/100 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -t/100 \\\\ -t/100 \\\\ 1+t \\end{pmatrix}$$\n\n**3. Determine the exact minimizer of the Basis Pursuit problem.**\nWe need to find the solution $x$ that minimizes the $\\ell_1$-norm, $\\|x\\|_{1} = \\sum_i |x_i|$. Using the parameterization of the feasible set, we minimize the function $f(t) = \\|x(t)\\|_{1}$ with respect to $t$:\n$$f(t) = \\left\\| \\begin{pmatrix} -t/100 \\\\ -t/100 \\\\ 1+t \\end{pmatrix} \\right\\|_1 = \\left|-\\frac{t}{100}\\right| + \\left|-\\frac{t}{100}\\right| + |1+t| = \\frac{2}{100}|t| + |1+t|$$\nThis function is convex and piecewise-linear, with non-differentiable points at $t=0$ and $t=-1$. The minimum must occur at one of these points.\nLet's analyze the function based on intervals of $t$:\n- For $t \\ge 0$: $f(t) = \\frac{2}{100}t + (1+t) = 1 + (1 + \\frac{2}{100})t = 1 + \\frac{102}{100}t$. The slope is positive, so the function is increasing.\n- For $-1 \\le t  0$: $f(t) = \\frac{2}{100}(-t) + (1+t) = 1 + (1 - \\frac{2}{100})t = 1 + \\frac{98}{100}t$. The slope is positive, so the function is increasing.\n- For $t  -1$: $f(t) = \\frac{2}{100}(-t) - (1+t) = -1 + (-\\frac{2}{100} - 1)t = -1 - \\frac{102}{100}t$. The slope is negative, so the function is decreasing.\n\nThe function decreases for $t  -1$ and increases for $t  -1$. Therefore, the global minimum is at $t = -1$.\nLet's find the minimizer $x_{\\ell_1}$ by substituting $t = -1$ into our parameterization:\n$$x_{\\ell_1} = x(-1) = \\begin{pmatrix} -(-1)/100 \\\\ -(-1)/100 \\\\ 1+(-1) \\end{pmatrix} = \\begin{pmatrix} 1/100 \\\\ 1/100 \\\\ 0 \\end{pmatrix}$$\n\n**4. Conclude whether the $\\ell_{1}$ minimizer coincides with the sparsest solution.**\nThe sparsest solution is $x_0 = (0, 0, 1)^T$.\nThe $\\ell_1$-minimizer is $x_{\\ell_1} = (1/100, 1/100, 0)^T$.\nThese two vectors are not the same. The sparsest solution is 1-sparse, while the $\\ell_1$-minimizer is 2-sparse. Basis Pursuit fails to recover the sparsest solution in this case.\nWe can check their respective $\\ell_1$-norms:\n$\\|x_0\\|_1 = |0| + |0| + |1| = 1$.\n$\\|x_{\\ell_1}\\|_1 = |1/100| + |1/100| + |0| = 2/100 = 1/50$.\nIndeed, $\\|x_{\\ell_1}\\|_1  \\|x_0\\|_1$.\n\n**5. Analyze the Null Space Property and compute the ratio $R_S$.**\nThe failure of Basis Pursuit is explained by the violation of the Null Space Property (NSP). For guaranteed recovery of any $s$-sparse vector, the matrix $A$ must satisfy the NSP of order $s$. Here, the sparsest solution $x_0$ is 1-sparse ($s=1$), so we check the NSP of order $1$. The condition is that for any non-zero $h \\in \\ker(A)$ and any index set $S \\subset \\{1, 2, 3\\}$ with $|S| \\le 1$, we must have $\\|h_S\\|_1  \\|h_{S^c}\\|_1$.\nLet's check this condition for the support of the sparsest solution, $S = S_0 = \\{3\\}$, and the null space generator $h = \\begin{pmatrix} -1/100 \\\\ -1/100 \\\\ 1 \\end{pmatrix}$. The complement of the support is $S^c = \\{1, 2\\}$.\n\nWe compute the restricted norms:\n- The part of $h$ on the support $S$ is $h_S = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$. Its $\\ell_1$-norm is $\\|h_S\\|_1 = |1| = 1$.\n- The part of $h$ off the support $S$ is $h_{S^c} = \\begin{pmatrix} -1/100 \\\\ -1/100 \\\\ 0 \\end{pmatrix}$. Its $\\ell_1$-norm is $\\|h_{S^c}\\|_1 = |-1/100| + |-1/100| = 2/100 = 1/50$.\n\nThe NSP condition for this support set would be $\\|h_S\\|_1  \\|h_{S^c}\\|_1$, which is $1  1/50$. This is false.\nThe problem asks for the ratio $R_S$:\n$$ R_S = \\frac{\\|h_S\\|_1}{\\|h_{S^c}\\|_1} = \\frac{1}{1/50} = 50 $$\nSince $R_S = 50  1$, the Null Space Property of order $1$ is violated for the support set $S=\\{3\\}$. This violation is precisely the reason why the $\\ell_1$-minimization did not recover the sparsest solution $x_0$. It was \"cheaper\" in the $\\ell_1$ sense to move signal energy from the support of $x_0$ to its complement, guided by the structure of the null space vector $h$.", "answer": "$$\n\\boxed{50}\n$$", "id": "3433126"}, {"introduction": "Theoretical analyses like the Null Space Property provide worst-case guarantees, but a full understanding of Basis Pursuit also requires investigating its typical-case performance. This computational exercise tasks you with empirically measuring the success of sparse recovery and identifying the \"phase transition\" phenomenon, a sharp boundary between success and failure. By comparing matrices with independent columns to those with structured correlations, you will gain practical insight into how matrix coherence degrades recovery performance, a key consideration in designing real-world sensing systems [@problem_id:3433151].", "problem": "Consider the noiseless compressed sensing model where a signal $x_0 \\in \\mathbb{R}^n$ is measured through a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ to produce measurements $y \\in \\mathbb{R}^m$ obeying $y = A x_0$. The Basis Pursuit program seeks to recover $x_0$ by solving the convex optimization problem that minimizes the $\\ell_1$ norm of the reconstruction subject to exact data consistency. The central scientific question is whether column correlations in $A$ degrade sparse recovery for Basis Pursuit compared to independent and identically distributed (i.i.d.) designs. Your investigation must be framed from first principles, using core definitions of sparsity, convex optimization, and linear algebra.\n\nYou must implement a complete program that:\n- Constructs multiple random ensembles of sensing matrices $A$, including an i.i.d. Gaussian ensemble and a block-correlated Gaussian ensemble with controlled intra-block column correlations.\n- Generates random $k$-sparse signals $x_0$ and their corresponding measurements $y = A x_0$.\n- Solves the Basis Pursuit problem $\\min \\|x\\|_1$ subject to $A x = y$ for each instance to assess exact recovery.\n- Empirically determines a phase transition index $k^\\star$ defined as the largest sparsity level $k$ for which the recovery success rate is at least $0.5$ (expressed as a decimal) over a small number of trials, for each matrix ensemble.\n- Reports the phase transition indices for the i.i.d. and block-correlated ensembles to reveal the shift due to column correlations.\n\nFoundational base (to be used, not derived):\n- A vector $x_0 \\in \\mathbb{R}^n$ is $k$-sparse if it has at most $k$ nonzero entries.\n- The Basis Pursuit recovery program is the convex optimization problem: minimize $\\|x\\|_1$ subject to $A x = y$.\n- Linear programming can be used to solve the $\\ell_1$ minimization problem by splitting $x$ into positive and negative components under nonnegativity constraints.\n- Standard Gaussian vectors have independent entries with distribution $\\mathcal{N}(0,1)$.\n\nDesign of ensembles:\n- I.i.d. Gaussian ensemble: each column of $A$ is drawn with entries independently from $\\mathcal{N}(0,1)$ and then normalized to unit $\\ell_2$ norm.\n- Block-correlated Gaussian ensemble: fix a block size $b$ and a correlation parameter $\\rho \\in [0,1)$. Partition the $n$ columns into contiguous blocks of size $b$ (the last block may be smaller). For each block, draw a latent vector $g \\in \\mathbb{R}^m$ with entries independently from $\\mathcal{N}(0,1)$. For each column in that block, draw an independent $z \\in \\mathbb{R}^m$ with entries independently from $\\mathcal{N}(0,1)$, and set the column to $\\sqrt{\\rho}\\, g + \\sqrt{1-\\rho}\\, z$. Normalize each column to unit $\\ell_2$ norm. This construction induces intra-block correlations while preserving per-column variance.\n\nRecovery success criterion:\n- Let $\\hat{x}$ denote the Basis Pursuit solution for $y = A x_0$.\n- Declare success if the relative $\\ell_2$ error $\\| \\hat{x} - x_0 \\|_2 / \\| x_0 \\|_2$ is less than $10^{-3}$ (expressed as a decimal), ensuring the decision is unitless.\n\nPhase transition index:\n- For a fixed $m$ and $n$, define a grid of sparsity levels $k \\in \\{2,4,6,8,10,12,14\\}$.\n- For each $k$ and each ensemble, perform $T = 6$ independent trials, each drawing a new $k$-sparse $x_0$ with randomly chosen support and nonzero amplitudes drawn as independent products of a random sign from $\\{-1, +1\\}$ and a magnitude drawn uniformly from $[0.5, 1.5]$. For each trial, compute $y = A x_0$, solve Basis Pursuit, and test for success.\n- The empirical success rate at sparsity $k$ is the fraction of successful trials out of $T$.\n- Define $k^\\star$ for an ensemble as the largest $k$ in the grid such that the empirical success rate is at least $0.5$ (expressed as a decimal). If no $k$ achieves a success rate of at least $0.5$, then set $k^\\star = 0$.\n\nTest suite:\n- Use $m = 48$ and $n = 128$.\n- Use block size $b = 8$.\n- Evaluate three ensembles:\n  1. I.i.d. Gaussian design.\n  2. Block-correlated design with $\\rho = 0.3$.\n  3. Block-correlated design with $\\rho = 0.6$.\n- For all ensembles, normalize columns to have unit $\\ell_2$ norm.\n\nFinal output format:\n- Your program should produce a single line of output containing the three phase transition indices as a comma-separated list enclosed in square brackets, in the following order: i.i.d., $\\rho = 0.3$, $\\rho = 0.6$. For example, an output line might look like $[12,10,8]$.\n- No physical units appear in this problem; all quantities are dimensionless.\n- The program must be self-contained and require no user input or external files. It must fix its random seed to ensure reproducible output.\n\nYour implementation must adhere to the runtime environment specified later and must solve the Basis Pursuit program via a correct linear programming formulation without relying on external libraries beyond those specified.", "solution": "We begin from core definitions. A vector $x_0 \\in \\mathbb{R}^n$ is $k$-sparse if it has at most $k$ nonzero entries. The noiseless measurement model is $y = A x_0$ for a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ and measurements $y \\in \\mathbb{R}^m$. Basis Pursuit is the convex optimization program $\\min \\|x\\|_1$ subject to $A x = y$, which promotes sparsity under linear constraints.\n\nPrinciple-based justification for the algorithmic approach:\n- The $\\ell_1$ minimization program is a convex optimization problem over a polyhedral feasible set, and can be solved as a linear program by variable splitting. Introduce nonnegative variables $u \\in \\mathbb{R}^n$ and $v \\in \\mathbb{R}^n$ such that $x = u - v$ and $u \\ge 0$, $v \\ge 0$. The $\\ell_1$ norm becomes $\\|x\\|_1 = \\sum_{i=1}^n |x_i| = \\sum_{i=1}^n (u_i + v_i)$ when $u$ and $v$ are constrained as above. The equality constraint becomes $A(u - v) = y$. This yields a linear program:\n$$\n\\min_{u,v \\in \\mathbb{R}^n} \\;\\; \\sum_{i=1}^n (u_i + v_i) \\quad \\text{subject to} \\quad A u - A v = y, \\quad u \\ge 0, \\quad v \\ge 0.\n$$\nA solution $(u^\\star, v^\\star)$ provides a Basis Pursuit solution $x^\\star = u^\\star - v^\\star$. This formulation adheres to linear programming theory and allows solution via standard algorithms.\n\n- Recovery success for Basis Pursuit in noiseless settings is governed by geometric properties of the nullspace of $A$ and the polytope defined by the $\\ell_1$ ball, which are affected by column correlations. Increased intra-block correlations increase mutual coherence, which lowers the recoverable sparsity. Although we do not assume or use explicit phase transition formulas, we can empirically estimate the phase transition index $k^\\star$ by evaluating success rates across sparsity levels.\n\nDesign of ensembles:\n- For the i.i.d. Gaussian ensemble, we draw each entry of $A$ independently from $\\mathcal{N}(0,1)$ and normalize columns to unit $\\ell_2$ norm to fix scale. This normalization preserves the isotropy while standardizing column norms, which helps control mutual coherence measurements and stabilizes the linear program numerically.\n\n- For the block-correlated ensemble, we partition columns into contiguous blocks of size $b$. For each block, we draw a latent $g \\in \\mathbb{R}^m$ with entries independently from $\\mathcal{N}(0,1)$. For each column in that block, we draw an independent $z \\in \\mathbb{R}^m$ with entries independently from $\\mathcal{N}(0,1)$ and form the column as $\\sqrt{\\rho}\\, g + \\sqrt{1-\\rho}\\, z$. This construction yields, for two columns in the same block, a cross-covariance approximately $\\rho I_m$ prior to normalization, embedding structured correlations. After generating all columns, we normalize each to unit $\\ell_2$ norm. Larger $\\rho$ increases intra-block alignment, which tends to hamper Basis Pursuit recovery for larger sparsities.\n\nGeneration of sparse signals and success criterion:\n- For each sparsity level $k$, we produce random $x_0$ by uniformly drawing a support set $S \\subset \\{1,\\dots,n\\}$ of size $k$. For $i \\in S$, we set $x_{0,i} = s_i \\cdot a_i$, where $s_i \\in \\{-1,+1\\}$ is a random sign and $a_i$ is a random magnitude drawn uniformly from $[0.5, 1.5]$. For $i \\notin S$, we set $x_{0,i} = 0$. This avoids numerically tiny entries and balances signs.\n\n- We compute $y = A x_0$ and solve the linear program for Basis Pursuit as described. Let $\\hat{x}$ denote the recovered vector. We declare success if $\\|\\hat{x} - x_0\\|_2 / \\|x_0\\|_2  10^{-3}$, a stringent relative accuracy threshold.\n\nEmpirical phase transition estimation:\n- For each ensemble and each $k \\in \\{2,4,6,8,10,12,14\\}$, we perform $T = 6$ independent trials and compute the empirical success rate as the fraction of successful recoveries. We define the phase transition index $k^\\star$ as the largest $k$ whose success rate is at least $0.5$. If no $k$ meets this criterion, we set $k^\\star = 0$. This definition captures the onset of failure as sparsity increases and provides a simple, quantifiable boundary.\n\nNumerical considerations:\n- Column normalization of $A$ reduces the variability in column scales and makes the optimization more stable.\n- The linear program involves $2n$ variables and $m$ equality constraints, which is tractable for $m = 48$ and $n = 128$.\n- A fixed random seed ensures reproducibility of the empirical results.\n\nExpected qualitative outcome:\n- The i.i.d. ensemble typically admits a higher $k^\\star$ than correlated ensembles because column correlations increase mutual coherence and reduce the effective dimensionality available for sparse support separation.\n- As $\\rho$ increases from $0.3$ to $0.6$, we expect a nonincreasing sequence of phase transition indices, reflecting the detrimental effect of stronger correlations on Basis Pursuit recovery.\n\nThe program will compute the three phase transition indices for the specified test suite and print them in the required single-line format $[k_{\\text{iid}}, k_{\\rho=0.3}, k_{\\rho=0.6}]$, where each item is an integer.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef normalize_columns(A: np.ndarray) - np.ndarray:\n    \"\"\"\n    Normalize columns of A to have unit l2 norm.\n    \"\"\"\n    col_norms = np.linalg.norm(A, axis=0)\n    # Avoid division by zero by replacing zero norms with one (though unlikely with Gaussian draws)\n    col_norms[col_norms == 0] = 1.0\n    return A / col_norms\n\ndef generate_iid_gaussian_matrix(m: int, n: int, rng: np.random.Generator) - np.ndarray:\n    \"\"\"\n    Generate an i.i.d. Gaussian matrix with unit-norm columns.\n    \"\"\"\n    A = rng.standard_normal((m, n))\n    return normalize_columns(A)\n\ndef generate_block_correlated_gaussian_matrix(m: int, n: int, block_size: int, rho: float, rng: np.random.Generator) - np.ndarray:\n    \"\"\"\n    Generate a block-correlated Gaussian matrix with unit-norm columns.\n    Each block shares a latent vector g, and each column is sqrt(rho)*g + sqrt(1-rho)*z,\n    then columns are normalized to unit l2 norm.\n    \"\"\"\n    A = np.zeros((m, n), dtype=float)\n    num_blocks = (n + block_size - 1) // block_size\n    idx = 0\n    sqrt_rho = np.sqrt(max(0.0, min(1.0, rho)))\n    sqrt_1mr = np.sqrt(max(0.0, 1.0 - rho))\n    for b in range(num_blocks):\n        start = b * block_size\n        end = min((b + 1) * block_size, n)\n        size = end - start\n        g = rng.standard_normal(m)\n        for j in range(size):\n            z = rng.standard_normal(m)\n            col = sqrt_rho * g + sqrt_1mr * z\n            A[:, start + j] = col\n    return normalize_columns(A)\n\ndef generate_sparse_signal(n: int, k: int, rng: np.random.Generator) - np.ndarray:\n    \"\"\"\n    Generate a k-sparse signal of dimension n.\n    Nonzero entries are random signs multiplied by magnitudes uniformly in [0.5, 1.5].\n    \"\"\"\n    x0 = np.zeros(n, dtype=float)\n    if k == 0:\n        return x0\n    support = rng.choice(n, size=k, replace=False)\n    signs = rng.choice([-1.0, 1.0], size=k)\n    magnitudes = rng.uniform(0.5, 1.5, size=k)\n    x0[support] = signs * magnitudes\n    return x0\n\ndef basis_pursuit_via_lp(A: np.ndarray, y: np.ndarray) - np.ndarray:\n    \"\"\"\n    Solve Basis Pursuit: min ||x||_1 s.t. A x = y\n    via LP on variables (u, v) = 0 with x = u - v.\n    Objective: minimize sum(u + v)\n    Constraints: A u - A v = y\n    \"\"\"\n    m, n = A.shape\n    # Objective: c = [1,...,1, 1,...,1] for u and v\n    c = np.ones(2 * n, dtype=float)\n    # Equality constraints: A_eq @ [u; v] = y, where A_eq = [A, -A]\n    A_eq = np.hstack((A, -A))\n    b_eq = y.copy()\n    bounds = [(0.0, None)] * (2 * n)\n\n    # Solve LP\n    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n    if not res.success:\n        # If solver fails, return NaNs to signify failure.\n        return np.full(n, np.nan)\n    x_hat = res.x[:n] - res.x[n:]\n    return x_hat\n\ndef exact_recovery(x_hat: np.ndarray, x0: np.ndarray, tol: float = 1e-3) - bool:\n    \"\"\"\n    Decide exact recovery by relative l2 error threshold.\n    \"\"\"\n    if np.any(np.isnan(x_hat)):\n        return False\n    norm_x0 = np.linalg.norm(x0)\n    if norm_x0 == 0.0:\n        # Treat zero signal as recovered if estimate is numerically zero\n        return np.linalg.norm(x_hat)  tol\n    rel_err = np.linalg.norm(x_hat - x0) / norm_x0\n    return rel_err  tol\n\ndef evaluate_phase_transition(m: int, n: int, block_size: int, ensemble_type: str, rho: float,\n                              k_grid: list, trials: int, rng: np.random.Generator) - int:\n    \"\"\"\n    Evaluate empirical phase transition index k* as the largest k in k_grid\n    with success rate = 0.5, for the specified ensemble.\n    \"\"\"\n    # Generate a new matrix A for each trial? We choose fixed A per ensemble to isolate sparsity effects.\n    if ensemble_type == \"iid\":\n        A = generate_iid_gaussian_matrix(m, n, rng)\n    elif ensemble_type == \"corr\":\n        A = generate_block_correlated_gaussian_matrix(m, n, block_size, rho, rng)\n    else:\n        raise ValueError(\"Unknown ensemble type.\")\n\n    k_star = 0\n    for k in k_grid:\n        success_count = 0\n        for _ in range(trials):\n            x0 = generate_sparse_signal(n, k, rng)\n            y = A @ x0\n            x_hat = basis_pursuit_via_lp(A, y)\n            if exact_recovery(x_hat, x0):\n                success_count += 1\n        success_rate = success_count / trials\n        if success_rate = 0.5:\n            # Update k_star to this k if it meets the criterion\n            k_star = k\n    return k_star\n\ndef solve():\n    # Define the test cases from the problem statement.\n    m = 48\n    n = 128\n    block_size = 8\n    k_grid = [2, 4, 6, 8, 10, 12, 14]\n    trials = 6\n\n    # Fixed random seed for reproducibility\n    rng = np.random.default_rng(0)\n\n    test_cases = [\n        (\"iid\", 0.0),\n        (\"corr\", 0.3),\n        (\"corr\", 0.6),\n    ]\n\n    results = []\n    for ensemble_type, rho in test_cases:\n        k_star = evaluate_phase_transition(m, n, block_size, ensemble_type, rho, k_grid, trials, rng)\n        results.append(k_star)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3433151"}]}