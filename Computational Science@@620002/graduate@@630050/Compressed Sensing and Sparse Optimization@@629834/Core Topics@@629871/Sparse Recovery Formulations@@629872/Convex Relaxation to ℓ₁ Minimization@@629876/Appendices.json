{"hands_on_practices": [{"introduction": "The effectiveness of minimizing the $\\ell_1$-norm, $\\|x\\|_1$, to find sparse solutions is not magic; it stems directly from the geometric properties of this function. This exercise guides you through the derivation of the subdifferential of the $\\ell_1$-norm, a key concept from convex analysis that replaces the gradient for non-differentiable functions. By applying this to the optimality conditions of the basis pursuit problem, you will see precisely how the mathematical structure enforces that many components of the optimal solution must be zero. [@problem_id:3440299]", "problem": "Let $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ be the convex function $f(x)=\\|x\\|_{1}=\\sum_{i=1}^{n}|x_{i}|$. Using only the definition of the subgradient of a convex function, namely that a vector $v\\in\\mathbb{R}^{n}$ is a subgradient of $f$ at $x\\in\\mathbb{R}^{n}$ if and only if\n$$\nf(y)\\geq f(x)+v^{\\top}(y-x)\\quad\\text{for all }y\\in\\mathbb{R}^{n},\n$$\nderive the subdifferential $\\partial\\|x\\|_{1}$ at a general vector $x\\in\\mathbb{R}^{n}$ as an explicit set in $\\mathbb{R}^{n}$. Then, consider the equality-constrained convex program\n$$\n\\min_{x\\in\\mathbb{R}^{n}}\\ \\|x\\|_{1}\\quad\\text{subject to}\\quad A x=b,\n$$\nwith $A\\in\\mathbb{R}^{m\\times n}$ and $b\\in\\mathbb{R}^{m}$. Write the Karush–Kuhn–Tucker (KKT) stationarity condition for this problem and explain, in terms of the set $\\partial\\|x\\|_{1}$ you derived, how stationarity encodes the support and sign of the optimal solution. Your final answer must be the explicit set expression for $\\partial\\|x\\|_{1}$ at a general $x$. No numerical rounding is required.", "solution": "The user has provided a multipart problem concerning the subdifferential of the $\\ell_1$-norm and its application in a constrained optimization problem. The problem is valid as it is mathematically sound, well-posed, and based on established principles of convex analysis and optimization.\n\n### Part 1: Derivation of the Subdifferential $\\partial\\|x\\|_{1}$\n\nLet the function be $f(x) = \\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$. A vector $v \\in \\mathbb{R}^{n}$ is a subgradient of $f$ at a point $x \\in \\mathbb{R}^{n}$ if the following inequality holds for all $y \\in \\mathbb{R}^{n}$:\n$$\nf(y) \\geq f(x) + v^{\\top}(y-x)\n$$\nSubstituting the expression for $f(x)$, we have:\n$$\n\\sum_{i=1}^{n} |y_{i}| \\geq \\sum_{i=1}^{n} |x_{i}| + \\sum_{i=1}^{n} v_{i}(y_{i} - x_{i})\n$$\nThis inequality can be rearranged as:\n$$\n\\sum_{i=1}^{n} \\left( |y_{i}| - |x_{i}| - v_{i}(y_{i} - x_{i}) \\right) \\geq 0\n$$\nThis inequality must hold for any vector $y \\in \\mathbb{R}^{n}$. Let us investigate the conditions on the components $v_{i}$ of the vector $v$.\n\nConsider a specific vector $y$ which differs from $x$ only in the $j$-th component, i.e., $y_{j} \\in \\mathbb{R}$ is arbitrary, and $y_{i} = x_{i}$ for all $i \\neq j$. For this choice of $y$, the terms in the sum are zero for all $i \\neq j$. The inequality reduces to:\n$$\n|y_{j}| - |x_{j}| - v_{j}(y_{j} - x_{j}) \\geq 0\n$$\nThis must hold for any choice of $y_{j} \\in \\mathbb{R}$. This implies that for the original inequality to hold for all $y \\in \\mathbb{R}^n$, it is necessary that for each component $j \\in \\{1, \\dots, n\\}$, the inequality $|z| \\geq |x_{j}| + v_{j}(z - x_{j})$ must hold for all $z \\in \\mathbb{R}$.\n\nConversely, if $|y_{i}| \\geq |x_{i}| + v_{i}(y_{i} - x_{i})$ holds for each $i \\in \\{1, \\dots, n\\}$ and for any choice of $y_i$, then summing these non-negative terms over $i$ ensures that $\\sum_{i=1}^{n} \\left( |y_{i}| - |x_{i}| - v_{i}(y_{i} - x_{i}) \\right) \\geq 0$ for any $y \\in \\mathbb{R}^n$.\n\nTherefore, the condition for $v \\in \\partial\\|x\\|_{1}$ decouples into $n$ independent one-dimensional subgradient problems. A vector $v = (v_{1}, \\dots, v_{n})^{\\top}$ is a subgradient of $\\|x\\|_1$ at $x$ if and only if for each $i \\in \\{1, \\dots, n\\}$, $v_{i}$ is a subgradient of the absolute value function $|\\cdot|$ at $x_{i}$. Thus, the subdifferential $\\partial\\|x\\|_1$ is the Cartesian product of the subdifferentials of the component functions: $\\partial\\|x\\|_{1} = \\partial|x_{1}| \\times \\partial|x_{2}| \\times \\dots \\times \\partial|x_{n}|$.\n\nLet's find the subdifferential of the scalar function $g(z) = |z|$ at a point $z \\in \\mathbb{R}$. We need to find all $w \\in \\mathbb{R}$ such that $|y| \\geq |z| + w(y-z)$ for all $y \\in \\mathbb{R}$.\n\n**Case 1: $z > 0$**\nThe function $g(z) = |z| = z$ is differentiable at $z$, with derivative $g'(z) = 1$. The subdifferential is the singleton set containing the gradient: $\\partial|z| = \\{1\\}$. To formally verify using the definition, the inequality is $|y| \\geq z + 1(y-z)$, which simplifies to $|y| \\geq y$. This is true for all $y \\in \\mathbb{R}$. Thus, $w=1$ is a subgradient. Any other value $w \\neq 1$ would violate the local conditions for subgradients that must match the derivative where it exists.\n\n**Case 2: $z  0$**\nThe function $g(z) = |z| = -z$ is differentiable at $z$, with derivative $g'(z) = -1$. The subdifferential is $\\partial|z| = \\{-1\\}$. The inequality becomes $|y| \\geq -z - 1(y-z)$, which simplifies to $|y| \\geq -y$. This is true for all $y \\in \\mathbb{R}$. So, $w = -1$ is a subgradient.\n\n**Case 3: $z=0$**\nThe function is not differentiable at $z=0$. The subgradient inequality is $|y| \\geq |0| + w(y-0)$, which simplifies to $|y| \\geq wy$ for all $y \\in \\mathbb{R}$.\n- If we choose $y=1$, we get $1 \\geq w$.\n- If we choose $y=-1$, we get $1 \\geq -w$, or $w \\geq -1$.\nCombining these, we see that $w$ must be in the interval $[-1, 1]$. Let's check if any $w \\in [-1, 1]$ works.\n- If $y > 0$, the inequality is $y \\geq wy$, which means $1 \\geq w$. This holds for $w \\in [-1, 1]$.\n- If $y  0$, the inequality is $-y \\geq wy$. Dividing by $y0$ reverses the inequality sign, giving $-1 \\leq w$. This also holds for $w \\in [-1, 1]$.\n- If $y=0$, the inequality is $0 \\geq 0$, which is always true.\nTherefore, for $z=0$, the set of all subgradients is the interval $\\partial|0| = [-1, 1]$.\n\nWe can summarize the subdifferential of the absolute value function as:\n$$\n\\partial|z| = \n\\begin{cases}\n\\{\\mathrm{sgn}(z)\\}  \\text{if } z \\neq 0 \\\\\n[-1, 1]  \\text{if } z = 0\n\\end{cases}\n$$\nwhere $\\mathrm{sgn}(z)$ is $1$ if $z>0$ and $-1$ if $z0$.\n\nCombining these results for the $n$-dimensional case, a vector $v \\in \\mathbb{R}^{n}$ is in the subdifferential $\\partial\\|x\\|_{1}$ if and only if its components $v_{i}$ satisfy:\n- $v_{i} = \\mathrm{sgn}(x_{i})$ if $x_{i} \\neq 0$.\n- $v_{i} \\in [-1, 1]$ if $x_{i} = 0$.\n\nThis gives the explicit set expression for the subdifferential of the $\\ell_1$-norm.\n\n### Part 2: KKT Conditions and Interpretation\n\nConsider the convex program:\n$$\n\\min_{x\\in\\mathbb{R}^{n}}\\ \\|x\\|_{1} \\quad \\text{subject to} \\quad A x=b\n$$\nThis is a convex optimization problem with a non-differentiable objective and linear equality constraints. We form the Lagrangian:\n$$\nL(x, \\lambda) = \\|x\\|_{1} + \\lambda^{\\top}(Ax - b)\n$$\nwhere $\\lambda \\in \\mathbb{R}^{m}$ is the vector of Lagrange multipliers (dual variables). The Karush–Kuhn–Tucker (KKT) conditions for optimality are:\n1.  **Primal feasibility:** $Ax = b$\n2.  **Stationarity:** $0 \\in \\partial_{x} L(x, \\lambda)$\n\nThe subdifferential of the Lagrangian with respect to $x$ is given by the sum rule:\n$$\n\\partial_{x} L(x, \\lambda) = \\partial_{x} (\\|x\\|_{1} + \\lambda^{\\top}Ax - \\lambda^{\\top}b) = \\partial\\|x\\|_{1} + \\nabla_{x}(\\lambda^{\\top}Ax) = \\partial\\|x\\|_{1} + A^{\\top}\\lambda\n$$\nThe stationarity condition is therefore:\n$$\n0 \\in \\partial\\|x\\|_{1} + A^{\\top}\\lambda\n$$\nThis is equivalent to the condition $-A^{\\top}\\lambda \\in \\partial\\|x\\|_{1}$.\n\nLet $x^*$ be an optimal solution. The stationarity condition means there must exist a dual vector $\\lambda^*$ such that $-A^{\\top}\\lambda^* \\in \\partial\\|x^*\\|_{1}$. Using the characterization of $\\partial\\|x\\|_{1}$ derived above, this condition on $x^*$ and $\\lambda^*$ can be broken down component-wise. Let $v = -A^{\\top}\\lambda^*$. Then $v \\in \\partial\\|x^*\\|_{1}$, which means:\n- For any index $i$ such that $x_{i}^* \\neq 0$: $v_i = \\mathrm{sgn}(x_{i}^*)$.\n- For any index $i$ such that $x_{i}^* = 0$: $v_i \\in [-1, 1]$, which is $|v_i| \\leq 1$.\n\nThe $i$-th component of $v = -A^{\\top}\\lambda^*$ is $v_i = -a_{i}^{\\top}\\lambda^*$, where $a_{i}$ is the $i$-th column of the matrix $A$. The KKT stationarity condition thus explicitly encodes the properties of the optimal solution $x^*$ as follows:\n\n- **Support:** An index $i$ can be in the support of the optimal solution (i.e., $x_{i}^* \\neq 0$) only if the corresponding column $a_{i}$ of the measurement matrix $A$ satisfies $|a_{i}^{\\top}\\lambda^*|=1$. In other words, non-zero components of the optimal solution can only occur at indices where the vector $-A^\\top \\lambda^*$ has components equal to $\\pm 1$.\n\n- **Sign:** For an index $i$ in the support of $x^*$, the sign of $x_{i}^*$ is determined by the sign of $a_{i}^{\\top}\\lambda^*$. Specifically, $\\mathrm{sgn}(x_{i}^*) = -a_{i}^{\\top}\\lambda^*$. This means:\n    - If $x_{i}^* > 0$, then $a_{i}^{\\top}\\lambda^* = -1$.\n    - If $x_{i}^*  0$, then $a_{i}^{\\top}\\lambda^* = 1$.\n\n- **Zero components:** For any index $i$ not in the support of the optimal solution (i.e., $x_{i}^*=0$), the corresponding column $a_{i}$ must satisfy $|a_{i}^{\\top}\\lambda^*| \\leq 1$.\n\nIn summary, the stationarity condition provides a powerful link between the primal solution $x^*$, the dual solution $\\lambda^*$, and the problem data $A$. It partitions the indices $\\{1, \\dots, n\\}$ into two sets: the support set, where $|a_{i}^{\\top}\\lambda^*|=1$ and the signs are fixed, and the zero set, where $|a_{i}^{\\top}\\lambda^*| \\leq 1$. This is a fundamental result in the theory of sparse recovery and compressed sensing.", "answer": "$$\n\\boxed{\\partial\\|x\\|_{1} = \\left\\{v\\in\\mathbb{R}^{n} \\mid v_{i} = \\mathrm{sgn}(x_i) \\text{ if } x_{i}\\neq 0, \\text{ and } v_{i} \\in [-1, 1] \\text{ if } x_{i}=0 \\right\\}}\n$$", "id": "3440299"}, {"introduction": "While the $\\ell_1$-norm provides the driving force for sparsity, successful recovery also depends critically on the properties of the measurement matrix $A$. This practice introduces mutual coherence, an intuitive measure of the \"dissimilarity\" between the columns of $A$. By calculating this value for a given matrix, you will apply a foundational theorem to determine the maximum level of sparsity for which exact recovery is guaranteed, making the abstract theory of recovery guarantees concrete. [@problem_id:3440270]", "problem": "Consider the noiseless linear measurement model $y = A x$ in compressed sensing, where $A \\in \\mathbb{R}^{3 \\times 4}$ is a dictionary with unit-norm columns (atoms). Let the columns of $A$ be\n$$\na_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix},\\quad\na_{2} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2} \\\\ 0 \\end{pmatrix},\\quad\na_{3} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix},\\quad\na_{4} = \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix},\n$$\nand $A = [\\,a_{1}\\ a_{2}\\ a_{3}\\ a_{4}\\,]$. The recovery method is Basis Pursuit (BP), that is, $\\ell_{1}$ minimization.\n\nUsing only foundational definitions and well-tested sufficient conditions from the $\\ell_{1}$-based theory of exact recovery, perform the following:\n- Compute the mutual coherence $\\mu(A)$ of the dictionary $A$, defined as the maximum absolute inner product between distinct normalized columns.\n- Determine the largest integer sparsity level $s_{\\max}$ such that any $s$-sparse vector $x$ with $s \\leq s_{\\max}$ is guaranteed to be exactly recovered by $\\ell_{1}$ minimization for all measurements $y = A x$ via the standard coherence-based sufficient condition.\n\nExpress your final answer as a row matrix $(\\mu(A), s_{\\max})$. Use exact values; no rounding is required.", "solution": "The problem is valid as it is scientifically grounded in the theory of compressed sensing, is well-posed with all necessary information provided, and is formulated objectively using standard mathematical definitions.\n\nThe solution proceeds in two parts. First, we compute the mutual coherence $\\mu(A)$ of the dictionary $A$. Second, we use this value in the standard coherence-based sufficient condition for exact recovery to determine the maximum guaranteed sparsity level $s_{\\max}$.\n\nThe dictionary $A$ is given by its columns $a_1, a_2, a_3, a_4$:\n$$\nA = \\begin{pmatrix} 1  \\frac{1}{2}  \\frac{1}{2}  0 \\\\ 0  \\frac{\\sqrt{3}}{2}  0  \\frac{1}{2} \\\\ 0  0  \\frac{\\sqrt{3}}{2}  \\frac{\\sqrt{3}}{2} \\end{pmatrix}\n$$\nThe problem states, and it is easily verified, that the columns are of unit $\\ell_2$-norm. For example, for $a_2$:\n$$\n\\|a_2\\|_2 = \\sqrt{\\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{\\sqrt{3}}{2}\\right)^2 + 0^2} = \\sqrt{\\frac{1}{4} + \\frac{3}{4}} = \\sqrt{1} = 1\n$$\nSimilar calculations confirm $\\|a_1\\|_2 = 1$, $\\|a_3\\|_2 = 1$, and $\\|a_4\\|_2 = 1$.\n\n**Part 1: Computation of Mutual Coherence $\\mu(A)$**\n\nThe mutual coherence $\\mu(A)$ of a dictionary $A$ with unit-norm columns is defined as the maximum absolute value of the inner products between distinct columns:\n$$\n\\mu(A) = \\max_{i \\neq j} |a_i^T a_j|\n$$\nWe need to compute the $\\binom{4}{2} = 6$ distinct inner products:\n\n1.  $a_1^T a_2 = \\begin{pmatrix} 1  0  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2} \\\\ 0 \\end{pmatrix} = 1 \\cdot \\frac{1}{2} + 0 \\cdot \\frac{\\sqrt{3}}{2} + 0 \\cdot 0 = \\frac{1}{2}$\n\n2.  $a_1^T a_3 = \\begin{pmatrix} 1  0  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix} = 1 \\cdot \\frac{1}{2} + 0 \\cdot 0 + 0 \\cdot \\frac{\\sqrt{3}}{2} = \\frac{1}{2}$\n\n3.  $a_1^T a_4 = \\begin{pmatrix} 1  0  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix} = 1 \\cdot 0 + 0 \\cdot \\frac{1}{2} + 0 \\cdot \\frac{\\sqrt{3}}{2} = 0$\n\n4.  $a_2^T a_3 = \\begin{pmatrix} \\frac{1}{2}  \\frac{\\sqrt{3}}{2}  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix} = \\frac{1}{2} \\cdot \\frac{1}{2} + \\frac{\\sqrt{3}}{2} \\cdot 0 + 0 \\cdot \\frac{\\sqrt{3}}{2} = \\frac{1}{4}$\n\n5.  $a_2^T a_4 = \\begin{pmatrix} \\frac{1}{2}  \\frac{\\sqrt{3}}{2}  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix} = \\frac{1}{2} \\cdot 0 + \\frac{\\sqrt{3}}{2} \\cdot \\frac{1}{2} + 0 \\cdot \\frac{\\sqrt{3}}{2} = \\frac{\\sqrt{3}}{4}$\n\n6.  $a_3^T a_4 = \\begin{pmatrix} \\frac{1}{2}  0  \\frac{\\sqrt{3}}{2} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix} = \\frac{1}{2} \\cdot 0 + 0 \\cdot \\frac{1}{2} + \\frac{\\sqrt{3}}{2} \\cdot \\frac{\\sqrt{3}}{2} = \\frac{3}{4}$\n\nThe absolute values of these inner products are $\\{ \\frac{1}{2}, \\frac{1}{2}, 0, \\frac{1}{4}, \\frac{\\sqrt{3}}{4}, \\frac{3}{4} \\}$. We must find the maximum value in this set.\nLet's compare the values:\n$\\frac{1}{2} = \\frac{2}{4} = 0.5$\n$\\frac{1}{4} = 0.25$\n$\\frac{\\sqrt{3}}{4} \\approx \\frac{1.732}{4} = 0.433$\n$\\frac{3}{4} = 0.75$\n\nThe maximum value is $\\frac{3}{4}$.\nTherefore, the mutual coherence is $\\mu(A) = \\frac{3}{4}$.\n\n**Part 2: Determination of the Maximum Sparsity Level $s_{\\max}$**\n\nThe problem asks for the largest integer sparsity level $s_{\\max}$ for which exact recovery of any $s$-sparse vector $x$ (where $s \\le s_{\\max}$) from measurements $y = Ax$ is guaranteed via $\\ell_1$ minimization (Basis Pursuit).\n\nThe standard sufficient condition for exact recovery based on mutual coherence states that if a vector $x$ is $s$-sparse, it is guaranteed to be the unique solution of the $\\ell_1$ minimization problem if its sparsity $s$ satisfies the strict inequality:\n$$\ns  \\frac{1}{2} \\left( 1 + \\frac{1}{\\mu(A)} \\right)\n$$\nSubstituting the value $\\mu(A) = \\frac{3}{4}$ we found:\n$$\ns  \\frac{1}{2} \\left( 1 + \\frac{1}{3/4} \\right)\n$$\n$$\ns  \\frac{1}{2} \\left( 1 + \\frac{4}{3} \\right)\n$$\n$$\ns  \\frac{1}{2} \\left( \\frac{3}{3} + \\frac{4}{3} \\right)\n$$\n$$\ns  \\frac{1}{2} \\left( \\frac{7}{3} \\right)\n$$\n$$\ns  \\frac{7}{6}\n$$\nSince the sparsity $s = \\|x\\|_0$ must be a non-negative integer, we need to find the largest integer $s$ that satisfies $s  \\frac{7}{6}$.\nAs a decimal, $\\frac{7}{6} \\approx 1.167$. The integers less than this value are $1, 0, -1, \\ldots$. Since sparsity cannot be negative, the only applicable integers are $0$ and $1$. The case $s=0$ corresponds to the zero vector $x=0$, for which recovery is trivial. The largest positive integer for which the guarantee holds is $s=1$.\n\nThe problem asks for the largest integer $s_{\\max}$ such that *any* vector with sparsity $s \\le s_{\\max}$ is recoverable. This implies we are looking for the maximum integer value that $s$ can take. Based on the condition $s  \\frac{7}{6}$, the largest integer value for $s$ is $1$. Thus, we have $s_{\\max} = 1$. Any vector with sparsity $s=1$ is guaranteed to be recovered, but this guarantee does not extend to vectors with sparsity $s=2$.\n\nThe final answer is the pair $(\\mu(A), s_{\\max})$.\n$\\mu(A) = \\frac{3}{4}$\n$s_{\\max} = 1$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{3}{4}  1\n\\end{pmatrix}\n}\n$$", "id": "3440270"}, {"introduction": "Guarantees like the one based on mutual coherence are sufficient, but not necessary, for recovery. The Null Space Property (NSP) provides a sharper, necessary and sufficient condition. To truly understand why the NSP is so fundamental, this exercise challenges you to see what happens when it fails. You will construct a matrix that violates the NSP and demonstrate how this violation directly leads to a failure of $\\ell_1$ minimization, providing deep insight into the geometry of sparse recovery. [@problem_id:3440303]", "problem": "Consider a linear measurement model $y = A x$ with $A \\in \\mathbb{R}^{m \\times n}$ and an unknown vector $x \\in \\mathbb{R}^{n}$ that is $s$-sparse, meaning it has at most $s$ nonzero entries. The convex relaxation approach to sparse recovery replaces the nonconvex $\\ell_{0}$ minimization by $\\ell_{1}$ minimization, which seeks to recover $x$ via the convex program $\\min \\|x\\|_{1}$ subject to $A x = y$. A foundational guarantee for exact recovery by $\\ell_{1}$ minimization is the Null Space Property (NSP) of order $s$: a matrix $A$ satisfies the NSP of order $s$ if, for every nonzero $h \\in \\operatorname{Null}(A)$ and every index set $S \\subset \\{1,\\dots,n\\}$ with $|S| \\leq s$, one has $\\|h_{S}\\|_{1}  \\|h_{S^{c}}\\|_{1}$, where $h_{S}$ denotes the restriction of $h$ to indices in $S$ and $S^{c}$ is the complement of $S$.\n\nConstruct an explicit matrix $A \\in \\mathbb{R}^{3 \\times 4}$ that fails the Null Space Property of order $s = 2$. Exhibit a nonzero vector $h \\in \\operatorname{Null}(A)$ and an index set $S$ with $|S| = 2$ witnessing the failure. Using only the core definitions above and well-established implications between the Null Space Property and exact recovery by $\\ell_{1}$ minimization, provide an explicit $2$-sparse vector $x_{0}$ that is not uniquely recoverable by $\\ell_{1}$ minimization from measurements $y = A x_{0}$ by demonstrating a feasible perturbation along the null space that strictly reduces the objective $\\|x\\|_{1}$.\n\nFinally, compute the exact value of the ratio $r = \\|h_{S}\\|_{1} / \\|h_{S^{c}}\\|_{1}$ for your constructed witness and report this number as your final answer. No rounding is required.", "solution": "The problem statement is found to be valid as it is scientifically grounded in the established theory of compressed sensing, is well-posed, objective, and self-contained. It presents a standard exercise for understanding the Null Space Property (NSP) and its connection to the performance of $\\ell_{1}$ minimization for sparse recovery.\n\nThe core of the problem is to construct a matrix $A \\in \\mathbb{R}^{3 \\times 4}$ that fails the Null Space Property of order $s = 2$. The NSP of order $s$ states that for every non-zero vector $h$ in the null space of $A$, denoted $\\operatorname{Null}(A)$, and for every index set $S \\subset \\{1, 2, 3, 4\\}$ with cardinality $|S| \\leq s$, the inequality $\\|h_S\\|_1  \\|h_{S^c}\\|_1$ must hold. Here, $h_S$ is the vector $h$ restricted to the indices in $S$, and $S^c$ is its complement.\n\nTo construct a matrix $A$ that fails this property for $s=2$, we must find a non-zero vector $h \\in \\mathbb{R}^4$ and an index set $S$ with $|S|=2$ such that $\\|h_S\\|_1 \\geq \\|h_{S^c}\\|_1$, and then construct a matrix $A \\in \\mathbb{R}^{3 \\times 4}$ such that $A h = 0$.\n\n**Step 1: Construct a witness vector $h$ and index set $S$.**\nLet's choose an index set $S = \\{1, 2\\}$, which has cardinality $|S|=2$. The complementary set is $S^c = \\{3, 4\\}$. We need to find a vector $h = [h_1, h_2, h_3, h_4]^T$ such that $\\|h_S\\|_1 \\geq \\|h_{S^c}\\|_1$. To ensure a strong failure that facilitates the next steps, we will aim for a strict inequality.\n\nLet's choose the components of $h$ to be simple integers. A possible choice is $h = \\begin{pmatrix} 2 \\\\ 2 \\\\ -1 \\\\ -1 \\end{pmatrix}$.\nFor this vector $h$ and set $S$, we have:\n$h_S = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$ and $h_{S^c} = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}$.\n\nThe respective $\\ell_1$-norms are:\n$\\|h_S\\|_1 = |2| + |2| = 4$.\n$\\|h_{S^c}\\|_1 = |-1| + |-1| = 2$.\n\nSince $4 > 2$, we have $\\|h_S\\|_1 > \\|h_{S^c}\\|_1$. This violates the NSP condition $\\|h_S\\|_1  \\|h_{S^c}\\|_1$. Thus, $h = [2, 2, -1, -1]^T$ and $S = \\{1, 2\\}$ serve as a witness to the failure of the NSP of order $s=2$.\n\n**Step 2: Construct the matrix $A$.**\nWe need to construct a matrix $A \\in \\mathbb{R}^{3 \\times 4}$ such that $h \\in \\operatorname{Null}(A)$, i.e., $A h = 0$. Let the columns of $A$ be $a_1, a_2, a_3, a_4 \\in \\mathbb{R}^3$. The condition $A h = 0$ is equivalent to the linear combination of columns:\n$h_1 a_1 + h_2 a_2 + h_3 a_3 + h_4 a_4 = 0$.\nSubstituting the components of our chosen $h$:\n$2a_1 + 2a_2 - 1a_3 - 1a_4 = 0$, which can be rearranged to $a_4 = 2a_1 + 2a_2 - a_3$.\n\nTo ensure the matrix $A$ has a null space of dimension $1$ (spanned by $h$), we need $A$ to have rank $3$. We can achieve this by choosing $a_1, a_2, a_3$ to be linearly independent. The simplest choice is the standard basis vectors in $\\mathbb{R}^3$:\nLet $a_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$, $a_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$, and $a_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\n\nNow, we compute $a_4$:\n$a_4 = 2\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + 2\\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\\\ -1 \\end{pmatrix}$.\n\nThe resulting matrix $A$ is formed by these column vectors:\n$$ A = \\begin{pmatrix} 1  0  0  2 \\\\ 0  1  0  2 \\\\ 0  0  1  -1 \\end{pmatrix} $$\nBy construction, this matrix has rank $3$ and its null space contains our witness vector $h$. Since $\\operatorname{dim}(\\operatorname{Null}(A)) = n - \\operatorname{rank}(A) = 4 - 3 = 1$, the null space is precisely the span of $h$.\n\n**Step 3: Demonstrate non-unique recovery for a $2$-sparse vector.**\nThe failure of the NSP of order $s$ implies that there exists at least one $s$-sparse vector that is not the unique solution to the $\\ell_1$ minimization problem. We will construct such an example.\n\nLet's choose a $2$-sparse vector $x_0$ whose support (the set of indices of non-zero entries) is $S = \\{1, 2\\}$. A simple choice is $x_0 = \\begin{pmatrix} 4 \\\\ 4 \\\\ 0 \\\\ 0 \\end{pmatrix}$. This vector is $2$-sparse. Its $\\ell_1$-norm is $\\|x_0\\|_1 = |4| + |4| + |0| + |0| = 8$.\n\nThe corresponding measurement vector $y$ is:\n$$ y = A x_0 = \\begin{pmatrix} 1  0  0  2 \\\\ 0  1  0  2 \\\\ 0  0  1  -1 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 4 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 4 \\\\ 0 \\end{pmatrix} $$\n\nNow, consider a new vector $x_1$ constructed by perturbing $x_0$ with a vector from the null space of $A$. We can use our witness vector $h$. The problem statement suggests finding a perturbation that strictly reduces the $\\ell_1$-norm. Let's try a perturbation in the direction of $-h$.\nLet $x_1 = x_0 - c h$ for some scalar $c > 0$.\n$x_1 = \\begin{pmatrix} 4 \\\\ 4 \\\\ 0 \\\\ 0 \\end{pmatrix} - c \\begin{pmatrix} 2 \\\\ 2 \\\\ -1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 4 - 2c \\\\ 4 - 2c \\\\ c \\\\ c \\end{pmatrix}$.\n\nThis new vector $x_1$ is a feasible solution for any $c$, since $A x_1 = A(x_0 - ch) = A x_0 - c(Ah) = y - c(0) = y$.\nWe now check its $\\ell_1$-norm:\n$\\|x_1\\|_1 = |4 - 2c| + |4 - 2c| + |c| + |c|$.\nFor $c > 0$, this is $|4 - 2c| + |4 - 2c| + 2c$.\nTo ensure a simple calculation and strictly positive components, we choose a value of $c$ such that $4 - 2c > 0$, which means $c  2$. Let's choose $c=1$.\n\nFor $c=1$:\n$x_1 = \\begin{pmatrix} 4 - 2(1) \\\\ 4 - 2(1) \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\\\ 1 \\\\ 1 \\end{pmatrix}$.\nThis is a feasible solution as $A x_1 = y$. Its $\\ell_1$-norm is:\n$\\|x_1\\|_1 = |2| + |2| + |1| + |1| = 6$.\n\nComparing the norms: $\\|x_1\\|_1 = 6  8 = \\|x_0\\|_1$.\nWe have found a feasible solution $x_1$ that has a strictly smaller $\\ell_1$-norm than the original $2$-sparse vector $x_0$. This demonstrates that $x_0$ is not a solution to the $\\ell_1$ minimization problem $\\min \\|x\\|_1$ subject to $Ax=y$, and therefore is not uniquely recoverable. This is a direct consequence of the constructed matrix $A$ failing the NSP of order $2$.\n\n**Step 4: Compute the ratio $r$.**\nThe problem asks for the ratio $r = \\|h_S\\|_1 / \\|h_{S^c}\\|_1$ for the constructed witness vector $h$ and index set $S$.\nUsing the values computed in Step 1:\n$\\|h_S\\|_1 = 4$\n$\\|h_{S^c}\\|_1 = 2$\n\nThe ratio is:\n$r = \\frac{\\|h_S\\|_1}{\\|h_{S^c}\\|_1} = \\frac{4}{2} = 2$.\nThis value is an exact integer.", "answer": "$$\\boxed{2}$$", "id": "3440303"}]}