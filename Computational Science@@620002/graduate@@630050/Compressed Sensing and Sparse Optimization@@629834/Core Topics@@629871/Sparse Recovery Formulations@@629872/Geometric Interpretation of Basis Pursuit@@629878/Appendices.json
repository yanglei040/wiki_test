{"hands_on_practices": [{"introduction": "To build a strong intuition for Basis Pursuit, we can visualize it in a low-dimensional space. This practice explores the problem's geometry by framing it as the intersection of an affine subspace with a cross-polytope (the $\\ell_1$-ball). By constructing a concrete example in $\\mathbb{R}^3$ [@problem_id:3447927], you will see how the sparsity of the optimal solution corresponds directly to whether this intersection occurs at a vertex, an edge, or a higher-dimensional face of the polytope.", "problem": "Let $n = 3$ and $m = 2$. Consider the basis pursuit problem, which seeks a solution $x \\in \\mathbb{R}^n$ to the linear system $Ax = y$ that minimizes the $\\ell_1$ norm. Formally, the problem is to find\n$$\nx^\\star \\in \\arg\\min_{x \\in \\mathbb{R}^n} \\|x\\|_1 \\quad \\text{subject to} \\quad Ax = y,\n$$\nwhere $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$ is the $\\ell_1$ norm. Let $B_1^n = \\{x \\in \\mathbb{R}^n : \\|x\\|_1 \\le 1\\}$ denote the $\\ell_1$ unit ball (the cross-polytope in dimension $n$). In three dimensions ($n=3$), the set $B_1^3$ is a regular octahedron. Its edges are one-dimensional faces corresponding to points in the boundary with exactly two nonzero coordinates whose absolute values sum to $1$, while the third coordinate is zero.\n\nThe geometric interpretation of basis pursuit is that the affine set of feasible points $\\{x \\in \\mathbb{R}^n : Ax = y\\}$, which is an affine subspace of dimension $n - \\operatorname{rank}(A)$, intersects the boundary of $B_1^n$ at an optimal point. When $n=3$ and $m=2$, the feasible set is generically a line in $\\mathbb{R}^3$, which may intersect the boundary of $B_1^3$ at a vertex, along an edge interior point, or at a point strictly inside a two-dimensional face.\n\nThe dual problem to basis pursuit is\n$$\n\\max_{u \\in \\mathbb{R}^m} \\; y^\\top u \\quad \\text{subject to} \\quad \\|A^\\top u\\|_\\infty \\le 1,\n$$\nwhere $\\|v\\|_\\infty = \\max_i |v_i|$. At optimality, there exists a dual certificate $u^\\star$ with $A^\\top u^\\star \\in \\partial \\|x^\\star\\|_1$, where $\\partial \\|\\cdot\\|_1$ is the subdifferential of the $\\ell_1$ norm. Concretely, this means $(A^\\top u^\\star)_i = \\operatorname{sign}(x^\\star_i)$ for indices $i$ in the support $S = \\{i : x^\\star_i \\ne 0\\}$, and $|(A^\\top u^\\star)_j| \\le 1$ for indices $j \\notin S$. If, in addition, $|(A^\\top u^\\star)_j| < 1$ for all $j \\notin S$, then the optimal point $x^\\star$ is unique, and its contact with $B_1^3$ is on a lower-dimensional face determined by $S$ and the signs of $x^\\star$.\n\nYour tasks are:\n- Construct an explicit matrix $A \\in \\mathbb{R}^{2 \\times 3}$ and vector $y \\in \\mathbb{R}^2$ such that the basis pursuit solution $x^\\star$ lies along an edge of $B_1^3$ (that is, $x^\\star$ is on the boundary with exactly two nonzero coordinates and $\\|x^\\star\\|_1 = 1$), and compute the solution and its support to verify edge contact.\n- Use the dual problem to compute a dual certificate $u^\\star$ and verify that $(A^\\top u^\\star)_i = \\operatorname{sign}(x^\\star_i)$ for $i \\in S$ and $|(A^\\top u^\\star)_j| < 1$ for $j \\notin S$.\n- Contrast with cases where the solution contacts a vertex (support size $1$) and where the solution does not lie on the boundary of $B_1^3$ (that is, $\\|x^\\star\\|_1 < 1$), demonstrating how the geometric interpretation differs.\n\nFundamental base and definitions to use:\n- The $\\ell_1$ norm $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$ and the $\\ell_\\infty$ norm $\\|v\\|_\\infty = \\max_i |v_i|$.\n- The basis pursuit primal and dual formulations given above.\n- The subdifferential characterization of the $\\ell_1$ norm: If $x \\in \\mathbb{R}^n$ has support $S$, then $g \\in \\partial \\|x\\|_1$ if and only if $g_i = \\operatorname{sign}(x_i)$ for $i \\in S$ and $|g_j| \\le 1$ for $j \\notin S$.\n\nNumerical requirements:\n- Angles are not applicable in this problem.\n- No physical units are involved.\n\nTest suite and parameters:\n- Test case 1 (edge interior contact): $A = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & -1 \\end{bmatrix}$, $y = \\begin{bmatrix} \\tfrac{1}{3} \\\\ \\tfrac{2}{3} \\end{bmatrix}$.\n- Test case 2 (vertex contact): $A = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & -1 \\end{bmatrix}$, $y = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\n- Test case 3 (no boundary contact): $A = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & -1 \\end{bmatrix}$, $y = \\begin{bmatrix} 0.6 \\\\ 0.1 \\end{bmatrix}$.\n\nAlgorithmic requirements for your program:\n- Solve the basis pursuit primal problem via linear programming by introducing auxiliary variables $t \\in \\mathbb{R}^n$ and minimizing $\\sum_i t_i$ subject to $Ax = y$, $x_i - t_i \\le 0$, $-x_i - t_i \\le 0$, and $t_i \\ge 0$ for all $i$.\n- Solve the dual problem via linear programming: maximize $y^\\top u$ subject to $A^\\top u \\le \\mathbf{1}$ and $-A^\\top u \\le \\mathbf{1}$, where inequalities are elementwise and $\\mathbf{1}$ is the vector of ones.\n- For each test case, compute:\n    1. The optimal primal solution $x^\\star$ (rounded to six decimal places).\n    2. The support $S = \\{i : |x^\\star_i| > \\tau\\}$ using a numerical threshold $\\tau = 10^{-8}$, reported with $1$-based indices as a list of integers.\n    3. A boolean indicating whether $x^\\star$ lies on an edge interior of $B_1^3$, defined as $\\|\\;x^\\star\\;\\|_1 = 1$ within tolerance $10^{-6}$ and $|S| = 2$ with both nonzero entries strictly between $0$ and $1$ in magnitude (within tolerance $10^{-6}$).\n    4. A boolean indicating whether there exists a strict dual certificate for edge contact: for the computed dual optimizer $u^\\star$, check that $(A^\\top u^\\star)_i = \\operatorname{sign}(x^\\star_i)$ for $i \\in S$ within tolerance $10^{-6}$ and $|(A^\\top u^\\star)_j| < 1 - 10^{-6}$ for $j \\notin S$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case output must be a list of the form $[x^\\star, S, \\text{edge\\_interior}, \\text{strict\\_dual}]$, where $x^\\star$ is a list of floats rounded to six decimal places, $S$ is a list of integers (using $1$-based indexing), and both flags are booleans. For example: `[[[0.333333,0.666667,0.0],[1,2],true,true],...]`.", "solution": "The problem is well-posed and scientifically sound, representing a standard exercise in convex optimization and compressed sensing. It asks for the analysis of the basis pursuit problem and its dual, with a focus on geometric interpretation. We will first formalize the computational approach using linear programming and then analyze each test case.\n\nThe basis pursuit problem is given by:\n$$\nx^\\star \\in \\arg\\min_{x \\in \\mathbb{R}^n} \\|x\\|_1 \\quad \\text{subject to} \\quad Ax = y\n$$\nwhere $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$. This is a convex optimization problem, but the objective function $\\|x\\|_1$ is non-differentiable, making it unsuitable for standard gradient-based methods. However, it can be reformulated as a linear program (LP).\n\n**Primal Problem as a Linear Program**\n\nWe introduce an auxiliary vector of variables $t \\in \\mathbb{R}^n$. The goal is to find $x$ that minimizes $\\sum_{i=1}^n |x_i|$. Let $t_i \\ge |x_i|$ for each $i$. Then minimizing $\\sum_{i=1}^n t_i$ will force $t_i = |x_i|$ at the optimum. The constraint $t_i \\ge |x_i|$ is equivalent to the pair of linear inequalities $x_i \\le t_i$ and $-x_i \\le t_i$. The basis pursuit problem can thus be stated as the following LP over variables $x \\in \\mathbb{R}^n$ and $t \\in \\mathbb{R}^n$:\n$$\n\\min_{x, t} \\sum_{i=1}^n t_i \\\\\n\\text{subject to} \\quad Ax = y \\\\\n\\quad x - t \\le \\mathbf{0} \\\\\n\\quad -x - t \\le \\mathbf{0}\n$$\nThis LP has $2n$ variables and $m + 2n$ constraints.\n\n**Dual Problem and Optimality Conditions**\n\nThe dual of the basis pursuit problem is:\n$$\n\\max_{u \\in \\mathbb{R}^m} \\; y^\\top u \\quad \\text{subject to} \\quad \\|A^\\top u\\|_\\infty \\le 1\n$$\nwhere $\\|v\\|_\\infty = \\max_i |v_i|$. The constraint $\\|A^\\top u\\|_\\infty \\le 1$ can be expressed as a set of linear inequalities: $(A^\\top u)_j \\le 1$ and $-(A^\\top u)_j \\le 1$ for all $j=1, \\dots, n$. This is already in LP form. By negating the objective to fit the standard minimization form, we solve:\n$$\n\\min_{u} \\; -y^\\top u \\quad \\text{subject to} \\quad A^\\top u \\le \\mathbf{1}, \\quad -A^\\top u \\le \\mathbf{1}\n$$\n\nThe relationship between the optimal primal solution $x^\\star$ and an optimal dual solution $u^\\star$ is given by the Karush-Kuhn-Tucker (KKT) conditions, which in this context are expressed using the subdifferential $\\partial \\|\\cdot\\|_1$. The condition is $A^\\top u^\\star \\in \\partial \\|x^\\star\\|_1$.\nLet $S = \\{i : x^\\star_i \\ne 0\\}$ be the support of $x^\\star$. The subdifferential condition implies:\n$1$. $(A^\\top u^\\star)_i = \\operatorname{sign}(x^\\star_i)$ for all $i \\in S$.\n$2$. $|(A^\\top u^\\star)_j| \\le 1$ for all $j \\notin S$.\n\nIf a stronger condition holds, namely that $|(A^\\top u^\\star)_j| < 1$ for all $j \\notin S$, then the optimal primal solution $x^\\star$ is unique. This is often called a strict dual certificate.\n\n**Analysis of Test Cases**\n\nWe now analyze the three provided test cases. For all cases, $n=3$, $m=2$, and the matrix is $A = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & -1 \\end{bmatrix}$. Its transpose is $A^\\top = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & -1 \\end{bmatrix}$. The feasible set for $x$ is the line in $\\mathbb{R}^3$ defined by the system $x_1+x_3 = y_1$ and $x_2-x_3=y_2$.\n\n**Test Case 1: Edge Interior Contact**\n- Givens: $A$ as above, $y = \\begin{bmatrix} 1/3 \\\\ 2/3 \\end{bmatrix}$.\n- Analysis: The system of equations is $x_1 + x_3 = 1/3$ and $x_2 - x_3 = 2/3$. Parameterizing the solution space with $x_3=\\lambda$, we get $x(\\lambda) = (1/3 - \\lambda, 2/3 + \\lambda, \\lambda)$. We seek to minimize $\\|x(\\lambda)\\|_1 = |1/3 - \\lambda| + |2/3 + \\lambda| + |\\lambda|$. This is a convex, piecewise linear function with kinks at $\\lambda \\in \\{-2/3, 0, 1/3\\}$. The derivative with respect to $\\lambda$ changes from negative to positive at $\\lambda=0$. Thus, the minimum is at $\\lambda=0$.\n- Primal Solution: $x^\\star = (1/3, 2/3, 0)$.\n- Verification:\n    - $\\ell_1$ norm: $\\|x^\\star\\|_1 = |1/3| + |2/3| + 0 = 1$. The solution lies on the boundary of the unit $\\ell_1$ ball, $B_1^3$.\n    - Support: $S = \\{1, 2\\}$, so $|S|=2$.\n    - Geometry: With two non-zero components and $\\|x^\\star\\|_1=1$, the solution lies on an edge of the octahedron $B_1^3$. Since both $|x^\\star_1|=1/3$ and $|x^\\star_2|=2/3$ are strictly between $0$ and $1$, it is an edge interior point, not a vertex. This satisfies the `edge_interior` condition.\n- Dual Solution: We need to find $u^\\star$ and verify the uniqueness condition. For $i \\in S = \\{1,2\\}$, we must have $(A^\\top u^\\star)_i = \\operatorname{sign}(x^\\star_i)$.\n    - $(A^\\top u^\\star)_1 = u_1 = \\operatorname{sign}(1/3) = 1$.\n    - $(A^\\top u^\\star)_2 = u_2 = \\operatorname{sign}(2/3) = 1$.\n    - This gives a unique candidate for the dual optimizer: $u^\\star = (1, 1)$.\n- Verification: For $j=3 \\notin S$, we check the strict inequality condition:\n    - $(A^\\top u^\\star)_3 = u_1 - u_2 = 1 - 1 = 0$.\n    - Since $|(A^\\top u^\\star)_3| = 0 < 1$, the strict dual certificate condition holds. The solution $x^\\star$ is unique.\n\n**Test Case 2: Vertex Contact**\n- Givens: $A$ as above, $y = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\n- Analysis: The system is $x_1+x_3=1$ and $x_2-x_3=0$. Parameterizing with $x_3=\\lambda$, we get $x(\\lambda) = (1-\\lambda, \\lambda, \\lambda)$. We minimize $\\|x(\\lambda)\\|_1 = |1-\\lambda|+|\\lambda|+|\\lambda| = |1-\\lambda|+2|\\lambda|$. The minimum occurs at $\\lambda=0$.\n- Primal Solution: $x^\\star = (1, 0, 0)$.\n- Verification:\n    - $\\ell_1$ norm: $\\|x^\\star\\|_1 = |1| + 0 + 0 = 1$.\n    - Support: $S = \\{1\\}$, so $|S|=1$.\n    - Geometry: With one non-zero component and norm $1$, the solution is a vertex of $B_1^3$. It is not an edge interior point.\n- Dual Solution: For $i \\in S=\\{1\\}$, we must have $(A^\\top u^\\star)_1 = \\operatorname{sign}(x^\\star_1) = 1$, which means $u_1=1$. For $j \\in \\{2,3\\} \\notin S$, we must have $|(A^\\top u^\\star)_j| \\le 1$.\n    - $|(A^\\top u^\\star)_2| = |u_2| \\le 1$.\n    - $|(A^\\top u^\\star)_3| = |u_1 - u_2| = |1 - u_2| \\le 1$. This implies $0 \\le u_2 \\le 2$.\n    - Combining the constraints on $u_2$, we find $u_2 \\in [0, 1]$. Any $u^\\star = (1, u_2)$ with $u_2 \\in [0, 1]$ is a valid dual optimizer. An LP solver will return a vertex of this feasible segment, i.e., $u^\\star=(1, 0)$ or $u^\\star=(1, 1)$.\n- Verification: Let's assume the solver returns $u^\\star=(1, 0)$. We must check $|(A^\\top u^\\star)_j| < 1$ for $j \\in \\{2,3\\}$.\n    - $|(A^\\top u^\\star)_2| = |u_2| = |0| = 0 < 1$.\n    - $|(A^\\top u^\\star)_3| = |u_1 - u_2| = |1 - 0| = 1$. This is not strictly less than $1$.\n    - The strict dual certificate condition fails for this computed solution. The same would happen for $j=2$ if the solver returned $u^\\star=(1, 1)$.\n\n**Test Case 3: No Boundary Contact**\n- Givens: $A$ as above, $y = \\begin{bmatrix} 0.6 \\\\ 0.1 \\end{bmatrix}$.\n- Analysis: The system is $x_1+x_3=0.6$ and $x_2-x_3=0.1$. Parameterizing with $x_3=\\lambda$, we get $x(\\lambda) = (0.6-\\lambda, 0.1+\\lambda, \\lambda)$. We minimize $\\|x(\\lambda)\\|_1 = |0.6-\\lambda| + |0.1+\\lambda| + |\\lambda|$. The minimum again occurs at $\\lambda=0$.\n- Primal Solution: $x^\\star = (0.6, 0.1, 0)$.\n- Verification:\n    - $\\ell_1$ norm: $\\|x^\\star\\|_1 = |0.6| + |0.1| + 0 = 0.7$.\n    - Since $\\|x^\\star\\|_1 < 1$, the solution does not lie on the boundary of the unit ball $B_1^3$. It lies on the boundary of a smaller octahedron, $\\{x: \\|x\\|_1=0.7\\}$. The `edge_interior` condition, which requires $\\|x^\\star\\|_1=1$, is not met.\n    - Support: $S = \\{1, 2\\}$, so $|S|=2$.\n- Dual Solution: For $i \\in S=\\{1,2\\}$, we must have $(A^\\top u^\\star)_i = \\operatorname{sign}(x^\\star_i)$.\n    - $(A^\\top u^\\star)_1 = u_1 = \\operatorname{sign}(0.6) = 1$.\n    - $(A^\\top u^\\star)_2 = u_2 = \\operatorname{sign}(0.1) = 1$.\n    - So, $u^\\star = (1, 1)$.\n- Verification: For $j=3 \\notin S$, we check the strict inequality condition:\n    - $|(A^\\top u^\\star)_3| = |u_1 - u_2| = |1 - 1| = 0 < 1$.\n    - The strict dual certificate condition holds, confirming the uniqueness of the sparse solution $x^\\star$.\n\nThe Python code in the final answer will implement these LP solves and checks numerically.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Solves the basis pursuit problem for three test cases, analyzes the geometric\n    nature of the solution, and verifies the dual certificate conditions.\n    \"\"\"\n    \n    # Define test cases: A matrix and y vector.\n    A = np.array([[1, 0, 1], [0, 1, -1]])\n    \n    test_cases = [\n        (A, np.array([1/3, 2/3])),   # Case 1: Edge interior contact\n        (A, np.array([1.0, 0.0])),    # Case 2: Vertex contact\n        (A, np.array([0.6, 0.1])),    # Case 3: No boundary contact (of unit ball)\n    ]\n\n    results = []\n\n    for A_case, y_case in test_cases:\n        m, n = A_case.shape\n\n        # --- Solve Primal Problem (Basis Pursuit) ---\n        # min c'z s.t. A_ub z <= b_ub, A_eq z = b_eq\n        # z = [x, t] where x in R^n, t in R^n\n        \n        # c = [0...0, 1...1]\n        c_p = np.concatenate([np.zeros(n), np.ones(n)])\n        \n        # A_eq = [A, 0] such that A*x = y\n        A_eq_p = np.concatenate([A_case, np.zeros((m, n))], axis=1)\n        b_eq_p = y_case\n        \n        # A_ub = [[I, -I], [-I, -I]] for x-t<=0 and -x-t<=0\n        I_n = np.eye(n)\n        A_ub_p = np.vstack([np.hstack([I_n, -I_n]), np.hstack([-I_n, -I_n])])\n        b_ub_p = np.zeros(2 * n)\n        \n        primal_res = linprog(c_p, A_ub=A_ub_p, b_ub=b_ub_p, A_eq=A_eq_p, b_eq=b_eq_p, method='highs')\n        \n        x_star = primal_res.x[:n] if primal_res.success else np.full(n, np.nan)\n\n        # --- Solve Dual Problem ---\n        # max y'u s.t. ||A'u||_inf <= 1  => min -y'u s.t. A'u <= 1, -A'u <= 1\n        c_d = -y_case\n        \n        # A_ub = [A', -A']\n        A_ub_d = np.vstack([A_case.T, -A_case.T])\n        b_ub_d = np.ones(2 * n)\n        \n        dual_res = linprog(c_d, A_ub=A_ub_d, b_ub=b_ub_d, method='highs')\n        \n        u_star = dual_res.x if dual_res.success else np.full(m, np.nan)\n\n        # --- Perform Checks and Compile Results ---\n\n        # 1. Optimal primal solution x_star\n        x_star_rounded = [round(val, 6) for val in x_star]\n\n        # 2. Support S\n        tau = 1e-8\n        support_indices_0based = np.where(np.abs(x_star) > tau)[0]\n        S = (support_indices_0based + 1).tolist()\n\n        # 3. Edge interior contact flag\n        is_on_unit_boundary = np.isclose(np.linalg.norm(x_star, 1), 1.0, atol=1e-6)\n        has_support_2 = (len(S) == 2)\n        \n        edge_interior = False\n        if is_on_unit_boundary and has_support_2:\n            nonzero_vals = np.abs(x_star[support_indices_0based])\n            # Check if magnitude is strictly between 0 and 1\n            if np.all((nonzero_vals > 1e-6) & (nonzero_vals < 1.0 - 1e-6)):\n                edge_interior = True\n        \n        # 4. Strict dual certificate flag\n        g = A_case.T @ u_star\n        nonsupport_indices_0based = np.setdiff1d(np.arange(n), support_indices_0based)\n\n        cond1 = False\n        if len(support_indices_0based) > 0:\n            signs = np.sign(x_star[support_indices_0based])\n            cond1 = np.all(np.isclose(g[support_indices_0based], signs, atol=1e-6))\n        else: # If x_star is all zero, S is empty\n            cond1 = True\n\n        cond2 = False\n        if len(nonsupport_indices_0based) > 0:\n            cond2 = np.all(np.abs(g[nonsupport_indices_0based]) < 1.0 - 1e-6)\n        else: # If solution is dense, nonsupport is empty\n            cond2 = True\n            \n        strict_dual = cond1 and cond2\n\n        results.append(f\"[[{','.join(map(str, x_star_rounded))}],[{','.join(map(str, S))}],{str(edge_interior).lower()},{str(strict_dual).lower()}]\")\n        \n    # Final print statement\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3447927"}, {"introduction": "While geometric visualization is insightful, the success of Basis Pursuit depends critically on the properties of the sensing matrix $A$. This exercise delves into the fundamental reason for recovery failure by examining the Null Space Property (NSP). By constructing a matrix $A$ that violates the NSP [@problem_id:3447913], you will demonstrate how a non-zero vector in the null space of $A$ can connect a sparse solution to a less sparse one with the same (or smaller) $\\ell_1$-norm, leading to non-unique or incorrect recovery.", "problem": "Consider the linear inverse problem in compressed sensing and sparse optimization. Let the sensing matrix be the explicit choice\n$$\nA = \\begin{pmatrix} 1 & 1 & 1 \\\\ 0 & 1 & 2 \\end{pmatrix},\n$$\nand let the order parameter be $k=2$. \n\nTasks:\n- Using the definition of the null space property (NSP), show that $A$ violates the null space property of order $k=2$ by explicitly constructing a nonzero vector $h \\in \\ker(A)$ and an index set $S$ with $|S| \\leq 2$ for which $\\|h_{S}\\|_{1} \\geq \\|h_{S^{c}}\\|_{1}$ holds.\n- Select the $2$-sparse vector $x_{0} \\in \\mathbb{R}^{3}$ with support $S=\\{1,3\\}$ given by\n$$\nx_{0} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix},\n$$\nand define the measurement $y = Ax_{0}$. Consider the affine feasibility set $\\{x \\in \\mathbb{R}^{3} : Ax = y\\}$ and the one-dimensional family $x(t) = x_{0} + th$ for $t \\in \\mathbb{R}$, where $h$ is the vector found above.\n- Using only foundational definitions (the definition of the null space property, the definition of the basis pursuit program, and the definition of the $\\ell_{1}$ norm), compute the minimum value of the objective $\\|x\\|_{1}$ attained by basis pursuit, that is, the minimum of $\\|x\\|_{1}$ over all $x$ with $Ax = y$. Your answer must be a single real-valued number. \n\nYour derivation must rely on first principles and should include a geometric interpretation consistent with the cross-polytope view of the $\\ell_{1}$ ball intersected with the affine subspace $x_{0} + \\ker(A)$. The final answer is the minimum objective value, expressed as a single real number with no rounding required.", "solution": "The problem asks for the minimum $\\ell_1$-norm of a vector $x$ satisfying $Ax=y$, where the matrix $A$ is specifically chosen to violate the Null Space Property (NSP).\n\nFirst, we find the null space of $A$. A vector $h = (h_1, h_2, h_3)^\\top$ is in $\\ker(A)$ if $Ah=0$. This gives the system:\n$$\nh_1 + h_2 + h_3 = 0 \\\\\nh_2 + 2h_3 = 0\n$$\nFrom the second equation, $h_2 = -2h_3$. Substituting into the first gives $h_1 - 2h_3 + h_3 = 0$, which simplifies to $h_1 = h_3$. Thus, any vector in the null space is a multiple of $h = (1, -2, 1)^\\top$.\n\nNext, we verify that this $h$ violates the NSP of order $k=2$. The NSP of order $k$ requires $\\|v_S\\|_1 < \\|v_{S^c}\\|_1$ for all nonzero $v \\in \\ker(A)$ and all sets $S$ with $|S| \\leq k$. To show a violation, we need to find just one $S$ with $|S| \\leq 2$ where the inequality fails. Let's choose the set $S=\\{1,3\\}$, which has size $|S|=2$. The complement is $S^c=\\{2\\}$.\nFor our vector $h = (1, -2, 1)^\\top$:\n- The part of $h$ indexed by $S$ is $h_S = (1, 1)^\\top$. Its $\\ell_1$-norm is $\\|h_S\\|_1 = |1|+|1|=2$.\n- The part of $h$ indexed by $S^c$ is $h_{S^c} = (-2)$. Its $\\ell_1$-norm is $\\|h_{S^c}\\|_1 = |-2|=2$.\nThe condition $\\|h_S\\|_1 \\ge \\|h_{S^c}\\|_1$ holds, since $2 \\ge 2$. Thus, the NSP of order $2$ is violated.\n\nNow, we set up the basis pursuit problem. The given $2$-sparse vector is $x_0 = (1, 0, 1)^\\top$. The measurements are:\n$$\ny = Ax_0 = \\begin{pmatrix} 1 & 1 & 1 \\\\ 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n$$\nThe set of all solutions to $Ax=y$ is an affine line in $\\mathbb{R}^3$, which can be parameterized as $x(t) = x_0 + th$ for $t \\in \\mathbb{R}$:\n$$\nx(t) = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} + t \\begin{pmatrix} 1 \\\\ -2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1+t \\\\ -2t \\\\ 1+t \\end{pmatrix}\n$$\nBasis pursuit seeks to find the minimum of $\\|x\\|_1$ subject to $Ax=y$. This is equivalent to finding the minimum of the function $f(t) = \\|x(t)\\|_1$ over all $t \\in \\mathbb{R}$.\n$$\nf(t) = \\|x(t)\\|_1 = |1+t| + |-2t| + |1+t| = 2|t| + 2|t+1|\n$$\nThis is a convex, piecewise-linear function. We analyze it in intervals defined by the points where the absolute value arguments are zero ($t=0$ and $t=-1$).\n- For $t \\ge 0$: $f(t) = 2t + 2(t+1) = 4t + 2$. This is increasing.\n- For $-1 \\le t < 0$: $f(t) = 2(-t) + 2(t+1) = -2t + 2t + 2 = 2$. This is constant.\n- For $t < -1$: $f(t) = 2(-t) + 2(-(t+1)) = -2t - 2t - 2 = -4t - 2$. This is decreasing.\nThe function decreases to a minimum value of $2$ on the interval $t \\in [-1, 0]$ and then increases. Therefore, the minimum value of $\\|x(t)\\|_1$ is $2$.\n\nGeometrically, the set of feasible solutions is a line $x_0 + \\ker(A)$. Basis pursuit finds the point on this line closest to the origin in the $\\ell_1$-metric. This is equivalent to finding the smallest radius $r$ for which the $\\ell_1$-ball (an octahedron in $\\mathbb{R}^3$) $\\{x : \\|x\\|_1 \\le r\\}$ intersects the line. Our calculation shows this minimum radius is $r=2$. The intersection is not a single point but the entire line segment corresponding to $t \\in [-1, 0]$, which connects the points $x(-1)=(0,2,0)^\\top$ and $x(0)=(1,0,1)^\\top$. This segment lies on the face of the octahedron of radius 2. This non-uniqueness (an entire segment of solutions, including the original $2$-sparse $x_0$ and a different $1$-sparse vector $x(-1)$) is a direct geometric consequence of the NSP violation.", "answer": "$$\n\\boxed{2}\n$$", "id": "3447913"}, {"introduction": "Moving from the necessary conditions of the NSP, we now turn to a powerful sufficient condition for unique recovery derived from the dual problem. The Exact Recovery Condition (ERC) provides a practical way to certify that for a given support set, Basis Pursuit will succeed. This practice requires you to perform a direct calculation of the ERC for a specific matrix [@problem_id:3447892], providing a tangible link between algebraic properties of the sensing matrix and the geometric guarantee of a unique, sparse solution.", "problem": "Consider the linear sensing model with a matrix $A \\in \\mathbb{R}^{3 \\times 4}$ whose columns are\n$$\na_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\na_{2} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad\na_{3} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad\na_{4} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix},\n$$\nso that\n$$\nA = \\begin{pmatrix}\n1 & 1 & 0 & 1 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 1\n\\end{pmatrix}.\n$$\nLet the support be $S = \\{1,2\\}$ and the sign vector be $s = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\in \\{-1,1\\}^{|S|}$. Denote by $S^{c} = \\{3,4\\}$ the complement of $S$.\n\nBasis Pursuit (BP) is the convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{4}} \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = y,\n$$\nand the Exact Recovery Condition (ERC) is the condition that guarantees uniform recovery of all signals supported on $S$ with sign pattern $s$:\n$$\n\\left\\| A_{S^{c}}^{\\top} A_{S} (A_{S}^{\\top} A_{S})^{-1} s \\right\\|_{\\infty} < 1,\n$$\nwhere $A_{S}$ is the submatrix of $A$ consisting of the columns indexed by $S$, and $A_{S^{c}}$ is the submatrix consisting of the columns indexed by $S^{c}$.\n\nYour tasks are:\n- Compute $(A_{S}^{\\top} A_{S})^{-1}$.\n- Evaluate the vector $A_{S^{c}}^{\\top} A_{S} (A_{S}^{\\top} A_{S})^{-1} s$.\n- Decide whether the ERC holds for this $A$, $S$, and $s$, and explain the geometric consequence for BP in terms of the interaction between the descent cone of the $\\ell_{1}$ norm at the given sign pattern and the null space of $A$.\n\nProvide as your final answer the single real number $\\left\\| A_{S^{c}}^{\\top} A_{S} (A_{S}^{\\top} A_{S})^{-1} s \\right\\|_{\\infty}$. No rounding is necessary.", "solution": "The problem requires us to compute the value of the term inside the infinity norm in the Exact Recovery Condition (ERC) for a given matrix $A$, support set $S$, and sign vector $s$.\n\nFirst, we extract the submatrices $A_S$ and $A_{S^c}$ from $A$. The support is $S=\\{1,2\\}$ and its complement is $S^c=\\{3,4\\}$.\n$$\nA_{S} = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\\\ 0 & 0 \\end{pmatrix}, \\quad A_{S^{c}} = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\\\ 1 & 1 \\end{pmatrix}\n$$\nThe first task is to compute $(A_{S}^{\\top} A_{S})^{-1}$. We first find the Gram matrix $A_{S}^{\\top} A_{S}$:\n$$\nA_{S}^{\\top} A_{S} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\end{pmatrix}\n$$\nThe inverse of a $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$ is $\\frac{1}{ad-bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$. The determinant is $(1)(2) - (1)(1) = 1$. So the inverse is:\n$$\n(A_{S}^{\\top} A_{S})^{-1} = \\begin{pmatrix} 2 & -1 \\\\ -1 & 1 \\end{pmatrix}\n$$\nNext, we evaluate the vector inside the norm, which we'll call $v = A_{S^{c}}^{\\top} A_{S} (A_{S}^{\\top} A_{S})^{-1} s$. Let's compute this step-by-step. The sign vector is $s = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nFirst, calculate the product of the inverse Gram matrix and the sign vector:\n$$\n(A_{S}^{\\top} A_{S})^{-1} s = \\begin{pmatrix} 2 & -1 \\\\ -1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2(1) - 1(1) \\\\ -1(1) + 1(1) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\nNow, we need the matrix $A_{S^{c}}^{\\top} A_{S}$:\n$$\nA_{S^{c}}^{\\top} A_{S} = \\begin{pmatrix} 0 & 1 & 1 \\\\ 1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ 1 & 1 \\end{pmatrix}\n$$\nFinally, we compute the vector $v$:\n$$\nv = (A_{S^{c}}^{\\top} A_{S}) \\left( (A_{S}^{\\top} A_{S})^{-1} s \\right) = \\begin{pmatrix} 0 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0(1) + 1(0) \\\\ 1(1) + 1(0) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\nThe ERC requires $\\|v\\|_{\\infty} < 1$. We compute the infinity norm of $v$:\n$$\n\\|v\\|_{\\infty} = \\left\\| \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\right\\|_{\\infty} = \\max(|0|, |1|) = 1\n$$\nSince $1 \\not< 1$, the strict inequality of the ERC does not hold. This means that uniform recovery for all signals with support $S$ is not guaranteed. Geometrically, this result indicates that the null space of $A$ is not sufficiently \"incoherent\" with respect to the chosen support. The dual certificate, which defines a separating hyperplane, is not able to strictly separate the atoms in $S^c$ from those in $S$, leading to a potential failure of unique recovery.\n\nThe final answer is the computed value of the infinity norm.", "answer": "$$\\boxed{1}$$", "id": "3447892"}]}