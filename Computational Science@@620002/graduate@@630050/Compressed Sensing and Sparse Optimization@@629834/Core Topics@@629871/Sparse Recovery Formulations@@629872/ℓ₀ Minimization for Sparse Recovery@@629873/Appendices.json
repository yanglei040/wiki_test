{"hands_on_practices": [{"introduction": "Finding the sparsest solution to a system of equations is computationally difficult, leading to the use of proxies like $\\ell_1$-norm minimization or greedy algorithms like Orthogonal Matching Pursuit (OMP). This exercise provides a hands-on analytical experience to contrast these approaches. By constructing a simple scenario, you will derive the precise conditions under which OMP succeeds while the popular $\\ell_1$ approach fails, offering deep insight into the mechanics of different sparse recovery methods [@problem_id:3455943].", "problem": "Consider a sensing matrix $A \\in \\mathbb{R}^{2 \\times 3}$ with unit-norm columns $a_{1}, a_{2}, a_{3}$ defined by $a_{1} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$, $a_{2} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$, and $a_{3} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$. The mutual coherence of $A$ is defined as $\\mu(A) \\triangleq \\max_{i \\neq j} |a_{i}^{\\top} a_{j}|$, and for this $A$ one has $\\mu(A) = \\frac{1}{\\sqrt{2}}$. Let the unknown signal $x^{\\star} \\in \\mathbb{R}^{3}$ be supported on the index set $S^{\\star} = \\{1,2\\}$ with positive coefficients $x_{1}^{\\star} = \\alpha$, $x_{2}^{\\star} = \\beta$, and $x_{3}^{\\star} = 0$, where $\\alpha \\geq \\beta  0$. The measurement is noiseless: $y = A x^{\\star} = \\alpha a_{1} + \\beta a_{2}$. Define the dynamic range $R \\triangleq \\alpha / \\beta$.\n\nFrom first principles, use the definitions of mutual coherence, Orthogonal Matching Pursuit (OMP; a greedy algorithm selecting atoms by correlation and performing orthogonal projections), and $\\ell_{1}$-basis pursuit (minimizing the $\\ell_{1}$ norm of $x$ subject to $A x = y$) to do the following:\n\n1. Demonstrate that the $\\ell_{1}$-basis pursuit solution does not recover the true support $S^{\\star}$ and instead prefers a different two-term representation that includes the false atom $a_{3}$. You must compare the $\\ell_{1}$ norms of exact two-sparse representations of $y$.\n\n2. Derive the exact correlation margins at the first and second OMP steps between the correct atoms and the false atom in terms of the mutual coherence $\\mu(A)$ and the dynamic range $R$. Specifically, let $M_{1}$ denote the difference between the largest correct correlation and the largest false correlation at the first step, and $M_{2}$ denote this difference at the second step after orthogonal projection. Express $M_{1}$ and $M_{2}$ purely as analytic functions of $\\mu(A)$ and $R$ and $\\beta$.\n\n3. Determine the minimal dynamic range $R_{\\mathrm{min}}$ such that OMP selects $a_{1}$ at the first step and then $a_{2}$ at the second step, thereby recovering the true support $S^{\\star}$. Your final answer must be this $R_{\\mathrm{min}}$ in exact analytical form. No rounding is required, and no units are involved.\n\nThe final answer must be a single closed-form analytic expression.", "solution": "The problem statement has been validated and is deemed valid. It is self-contained, scientifically grounded in the principles of compressed sensing, and well-posed. All provided definitions and data are consistent. The given value for mutual coherence, $\\mu(A) = \\frac{1}{\\sqrt{2}}$, is correct, as verified by calculating the inner products: $|a_{1}^{\\top}a_{2}| = |0| = 0$, $|a_{1}^{\\top}a_{3}| = |\\frac{1}{\\sqrt{2}}| = \\frac{1}{\\sqrt{2}}$, and $|a_{2}^{\\top}a_{3}| = |\\frac{1}{\\sqrt{2}}| = \\frac{1}{\\sqrt{2}}$. The maximum of these is $\\frac{1}{\\sqrt{2}}$.\n\nWe will now proceed with the three parts of the problem. The measurement vector is given by $y = A x^{\\star} = \\alpha a_{1} + \\beta a_{2}$. Substituting the definitions of $a_{1}$ and $a_{2}$:\n$$y = \\alpha \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\beta \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix}$$\n\n**1. Demonstration of $\\ell_{1}$-Basis Pursuit Failure**\n\nThe $\\ell_{1}$-basis pursuit problem seeks to find a coefficient vector $x$ that minimizes the $\\ell_{1}$ norm subject to the measurement constraint:\n$$ \\min_{x \\in \\mathbb{R}^{3}} \\|x\\|_{1} \\quad \\text{subject to} \\quad Ax = y $$\nThe true sparse signal is $x^{\\star} = (\\alpha, \\beta, 0)^{\\top}$, which has support $S^{\\star} = \\{1, 2\\}$. The $\\ell_{1}$ norm of the true signal is:\n$$ \\|x^{\\star}\\|_{1} = |\\alpha| + |\\beta| + |0| = \\alpha + \\beta $$\nsince it is given that $\\alpha \\geq \\beta  0$.\n\nTo demonstrate that $\\ell_{1}$-basis pursuit fails to recover $x^{\\star}$, we must show that there exists another signal $x^{\\text{alt}}$ such that $A x^{\\text{alt}} = y$ but $\\|x^{\\text{alt}}\\|_{1}  \\|x^{\\star}\\|_{1}$. The problem suggests considering a representation that includes the atom $a_{3}$. Let's find a $2$-sparse representation of $y$ using the atoms $\\{a_{1}, a_{3}\\}$. Let this alternative signal be $x^{(2)} = (c_{1}, 0, c_{3})^{\\top}$. We must satisfy the constraint $A x^{(2)} = y$:\n$$ c_{1} a_{1} + c_{3} a_{3} = y $$\n$$ c_{1} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + c_{3} \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix} $$\nThis yields a system of linear equations:\n$$ \\begin{cases} c_{1} + \\frac{c_{3}}{\\sqrt{2}} = \\alpha \\\\ \\frac{c_{3}}{\\sqrt{2}} = \\beta \\end{cases} $$\nFrom the second equation, we find $c_{3} = \\sqrt{2}\\beta$. Substituting this into the first equation gives $c_{1} + \\beta = \\alpha$, so $c_{1} = \\alpha - \\beta$.\nThe alternative signal is $x^{(2)} = (\\alpha - \\beta, 0, \\sqrt{2}\\beta)^{\\top}$.\n\nNow we compute the $\\ell_{1}$ norm of $x^{(2)}$. Since $\\alpha \\geq \\beta  0$, we have $\\alpha - \\beta \\geq 0$ and $\\sqrt{2}\\beta  0$.\n$$ \\|x^{(2)}\\|_{1} = |\\alpha - \\beta| + |0| + |\\sqrt{2}\\beta| = (\\alpha - \\beta) + \\sqrt{2}\\beta = \\alpha + (\\sqrt{2}-1)\\beta $$\nWe compare this with the $\\ell_{1}$ norm of the true signal:\n$$ \\|x^{\\star}\\|_{1} = \\alpha + \\beta $$\nThe difference is $\\|x^{\\star}\\|_{1} - \\|x^{(2)}\\|_{1} = (\\alpha + \\beta) - (\\alpha + (\\sqrt{2}-1)\\beta) = \\beta - (\\sqrt{2}-1)\\beta = (2-\\sqrt{2})\\beta$.\nSince $\\sqrt{2}  2$ and $\\beta  0$, the difference is strictly positive.\n$$ \\|x^{(2)}\\|_{1}  \\|x^{\\star}\\|_{1} $$\nBecause there is an alternative representation of $y$ with a strictly smaller $\\ell_{1}$ norm, the solution of the $\\ell_{1}$-basis pursuit problem is not the true signal $x^{\\star}$. The support of the $\\ell_{1}$ solution will contain the index $3$, thus failing to recover the true support $S^{\\star}=\\{1,2\\}$.\n\n**2. OMP Correlation Margins**\n\nThe Orthogonal Matching Pursuit (OMP) algorithm iteratively selects the column of $A$ most correlated with the current residual.\n\n**First Step (OMP-1):**\nThe initial residual is $r_{0} = y = \\alpha a_{1} + \\beta a_{2}$. We compute the correlations of the atoms with this residual:\n$$ |a_{1}^{\\top} r_{0}| = |a_{1}^{\\top} (\\alpha a_{1} + \\beta a_{2})| = |\\alpha (a_{1}^{\\top}a_{1}) + \\beta (a_{1}^{\\top}a_{2})| = |\\alpha \\cdot 1 + \\beta \\cdot 0| = \\alpha $$\n$$ |a_{2}^{\\top} r_{0}| = |a_{2}^{\\top} (\\alpha a_{1} + \\beta a_{2})| = |\\alpha (a_{2}^{\\top}a_{1}) + \\beta (a_{2}^{\\top}a_{2})| = |\\alpha \\cdot 0 + \\beta \\cdot 1| = \\beta $$\n$$ |a_{3}^{\\top} r_{0}| = |a_{3}^{\\top} (\\alpha a_{1} + \\beta a_{2})| = |\\alpha (a_{3}^{\\top}a_{1}) + \\beta (a_{3}^{\\top}a_{2})| = \\left|\\alpha \\frac{1}{\\sqrt{2}} + \\beta \\frac{1}{\\sqrt{2}}\\right| = \\frac{\\alpha + \\beta}{\\sqrt{2}} $$\nThe correct atoms are $a_{1}$ and $a_{2}$. The false atom is $a_{3}$.\nThe largest correct correlation is $\\max(|a_{1}^{\\top} r_{0}|, |a_{2}^{\\top} r_{0}|) = \\max(\\alpha, \\beta) = \\alpha$, since $\\alpha \\geq \\beta$.\nThe largest (and only) false correlation is $|a_{3}^{\\top} r_{0}| = \\frac{\\alpha + \\beta}{\\sqrt{2}}$.\nThe correlation margin $M_{1}$ is the difference:\n$$ M_{1} = \\alpha - \\frac{\\alpha + \\beta}{\\sqrt{2}} $$\nUsing $\\mu(A) = \\frac{1}{\\sqrt{2}}$ and $R = \\frac{\\alpha}{\\beta}$, we express $M_{1}$ as:\n$$ M_{1} = \\alpha - \\mu(A)(\\alpha + \\beta) = \\alpha(1-\\mu(A)) - \\beta\\mu(A) = \\beta R(1-\\mu(A)) - \\beta\\mu(A) = \\beta[R(1-\\mu(A)) - \\mu(A)] $$\n\n**Second Step (OMP-2):**\nFor OMP to succeed, it must first select a correct atom. As shown in part 3, this requires selecting $a_{1}$, which means we assume $M_{1}0$. After selecting atom $a_{1}$, OMP updates the residual by projecting $y$ orthogonally onto the subspace spanned by $a_{1}$ and subtracting this projection from $y$. The new residual $r_{1}$ is:\n$$ r_{1} = y - P_{a_{1}}(y) = y - (a_{1}^{\\top}y)a_{1} $$\nWe have $a_{1}^{\\top}y = a_{1}^{\\top}(\\alpha a_{1} + \\beta a_{2}) = \\alpha$. Thus:\n$$ r_{1} = (\\alpha a_{1} + \\beta a_{2}) - \\alpha a_{1} = \\beta a_{2} $$\nThe next atom is chosen by maximizing the correlation with $r_{1}$ among the remaining atoms $\\{a_{2}, a_{3}\\}$:\n$$ |a_{2}^{\\top} r_{1}| = |a_{2}^{\\top} (\\beta a_{2})| = \\beta|a_{2}^{\\top}a_{2}| = \\beta $$\n$$ |a_{3}^{\\top} r_{1}| = |a_{3}^{\\top} (\\beta a_{2})| = \\beta|a_{3}^{\\top}a_{2}| = \\beta \\frac{1}{\\sqrt{2}} $$\nThe correct atom to choose is $a_{2}$, and the false one is $a_{3}$. The correlation margin $M_{2}$ is:\n$$ M_{2} = \\beta - \\frac{\\beta}{\\sqrt{2}} = \\beta \\left(1 - \\frac{1}{\\sqrt{2}}\\right) $$\nUsing $\\mu(A) = \\frac{1}{\\sqrt{2}}$, this simplifies to:\n$$ M_{2} = \\beta(1 - \\mu(A)) $$\nThis margin does not depend on the dynamic range $R$.\n\n**3. Minimal Dynamic Range for OMP Success**\n\nOMP successfully recovers the support $S^{\\star} = \\{1,2\\}$ if it selects atom $a_{1}$ at the first step and atom $a_{2}$ at the second.\n\n**Condition for Step 1:** OMP must select an atom from the true support $S^{\\star}=\\{1,2\\}$. The correlations are $\\alpha$ and $\\beta$. The largest correlation from the true support is $\\alpha$. This must be strictly greater than the correlation with the false atom $a_{3}$.\n$$ \\max(|a_{1}^{\\top} r_{0}|, |a_{2}^{\\top} r_{0}|)  |a_{3}^{\\top} r_{0}| $$\n$$ \\alpha  \\frac{\\alpha + \\beta}{\\sqrt{2}} $$\nWe solve this inequality for the dynamic range $R = \\alpha/\\beta$:\n$$ \\alpha\\sqrt{2}  \\alpha + \\beta $$\n$$ \\alpha(\\sqrt{2} - 1)  \\beta $$\n$$ \\frac{\\alpha}{\\beta}  \\frac{1}{\\sqrt{2} - 1} $$\nTo rationalize the denominator, we multiply the numerator and denominator by $(\\sqrt{2} + 1)$:\n$$ \\frac{1}{\\sqrt{2} - 1} = \\frac{\\sqrt{2} + 1}{(\\sqrt{2} - 1)(\\sqrt{2} + 1)} = \\frac{\\sqrt{2} + 1}{2 - 1} = \\sqrt{2} + 1 $$\nSo, the condition for the first step to succeed is $R  \\sqrt{2} + 1$.\n\n**Condition for Step 2:** If the first step is successful (i.e., $R  \\sqrt{2} + 1$), atom $a_{1}$ is chosen. The residual is $r_{1} = \\beta a_{2}$. OMP must then choose $a_{2}$ from the remaining atoms $\\{a_{2}, a_{3}\\}$. This requires:\n$$ |a_{2}^{\\top} r_{1}|  |a_{3}^{\\top} r_{1}| $$\nAs calculated before, this is:\n$$ \\beta  \\frac{\\beta}{\\sqrt{2}} $$\nSince $\\beta  0$, we can divide by $\\beta$ to get $1  \\frac{1}{\\sqrt{2}}$, which is true since $\\sqrt{2}  1$. Therefore, if the first step is successful, the second step is guaranteed to be successful as well.\n\nThe overall condition for OMP to recover the true support $S^{\\star}$ by picking $a_1$ then $a_2$ is solely determined by the condition from the first step: $R  \\sqrt{2} + 1$. The problem asks for the minimal dynamic range $R_{\\mathrm{min}}$ such that this occurs. This corresponds to the infimum of the set of values of $R$ for which success is guaranteed.\n$$ R_{\\mathrm{min}} = \\sqrt{2} + 1 $$\nAt this boundary value, the correlation of the correct atom $a_{1}$ ties with the false atom $a_{3}$, and successful recovery is not strictly guaranteed without a favorable tie-breaking rule. For any $R  R_{\\mathrm{min}}$, OMP success is assured.", "answer": "$$\\boxed{\\sqrt{2} + 1}$$", "id": "3455943"}, {"introduction": "Given a potential solution to an $\\ell_0$-penalized problem, how can we be certain it is the globally optimal one? This practice transitions from analyzing algorithm behavior to the crucial task of certifying optimality. You will derive the necessary and sufficient conditions for global optimality in the special case of an orthonormal sensing matrix and implement these conditions as a practical software test, bridging the gap between theoretical guarantees and verifiable results [@problem_id:3455948].", "problem": "Consider the sparsity-penalized least-squares objective defined for a matrix $A \\in \\mathbb{R}^{m \\times n}$, a data vector $b \\in \\mathbb{R}^{m}$, and a regularization parameter $\\lambda \\in \\mathbb{R}_{+}$ by\n$$\nF(x) \\triangleq \\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_0,\n$$\nwhere $\\lVert x \\rVert_0$ denotes the count of nonzero entries of $x$. Let $S \\subseteq \\{0,1,\\dots,n-1\\}$ be a candidate support. Define the restricted least-squares solution on $S$ by $x_S^\\star \\in \\arg\\min_{z \\in \\mathbb{R}^{|S|}} \\lVert A_S z - b \\rVert_2^2$, where $A_S$ denotes the submatrix of $A$ with columns indexed by $S$. Define the residual $r_S \\triangleq b - A_S x_S^\\star$.\n\nUsing only fundamental facts about Euclidean projections and least squares, derive the exact change in $\\lVert A x - b \\rVert_2^2$ when augmenting the support by a single index $j \\notin S$ and then re-optimizing over $S \\cup \\{j\\}$, and also the exact change when pruning a single index $i \\in S$ and then re-optimizing over $S \\setminus \\{i\\}$. Express both changes directly in terms of $r_S$, $A_S$, and the candidate column $a_j \\in \\mathbb{R}^m$ (the $j$-th column of $A$), or the pruned column $a_i$. Using these derivations, construct certification tests that assert $S$ is globally optimal in the special case where the columns of $A$ are orthonormal, i.e., $A^\\top A = I_n$. In this orthonormal case, prove that checking all off-support inequalities and on-support pruning inequalities is both necessary and sufficient for global optimality.\n\nYour program must implement the following logic:\n\n- Step $1$ (Restricted regression): Given $A$, $b$, $\\lambda$, and $S$, compute $x_S^\\star$ and $r_S$.\n\n- Step $2$ (Off-support augmentation inequalities): For each $j \\notin S$, compute the best-possible reduction in $\\lVert A x - b \\rVert_2^2$ obtained by augmenting $S$ with $j$ and re-optimizing. Denote this reduction by $\\Delta_{\\mathrm{add}}(j \\mid S)$. The certificate requires that $\\Delta_{\\mathrm{add}}(j \\mid S) \\le \\lambda$ for all $j \\notin S$.\n\n- Step $3$ (On-support pruning inequalities): For each $i \\in S$, compute the increase in $\\lVert A x - b \\rVert_2^2$ incurred by pruning $i$ from $S$ and re-optimizing. Denote this increase by $\\Delta_{\\mathrm{drop}}(i \\mid S)$. The certificate requires that $\\Delta_{\\mathrm{drop}}(i \\mid S) \\ge \\lambda$ for all $i \\in S$.\n\n- Step $4$ (Orthonormal-column prerequisite for global certification): The certificate declares $S$ globally optimal if and only if $A^\\top A = I_n$ (within a numerically specified tolerance) and Steps $2$ and $3$ hold. If $A^\\top A \\neq I_n$, the program must return a negative certification (i.e., it cannot certify global optimality) even if Steps $2$ and $3$ hold, because the inequalities are no longer sufficient.\n\nDesign your solution so that the derivation is rooted in the projection theorem for least squares: for any full-column-rank $A_S$, the vector $A_S x_S^\\star$ is the orthogonal projection of $b$ onto $\\mathrm{span}(A_S)$, i.e., $A_S^\\top r_S = 0$. Interpret augmentations and prunings as one-dimensional regressions of the residual (or refits after removing one column), and compute the exact objective changes accordingly.\n\nAngles are not involved in this problem. There are no physical units.\n\nImplement a single program that evaluates the following test suite and prints the aggregated results. Each result must be a boolean indicating whether the candidate support is certified globally optimal under the orthonormal-column test described in Step $4$:\n\n- Test case $1$ (Happy path, orthonormal columns):\n    - Dimensions: $m = 6$, $n = 4$.\n    - Matrix $A \\in \\mathbb{R}^{6 \\times 4}$ has columns equal to the first four standard basis vectors in $\\mathbb{R}^6$. Explicitly, $A$ is the matrix whose rows are:\n      $$\n      \\begin{bmatrix}\n      1  0  0  0 \\\\\n      0  1  0  0 \\\\\n      0  0  1  0 \\\\\n      0  0  0  1 \\\\\n      0  0  0  0 \\\\\n      0  0  0  0\n      \\end{bmatrix}.\n      $$\n    - Data $b = [\\,2.0,\\,-1.5,\\,0.2,\\,0.0,\\,3.0,\\,-2.0\\,]^\\top$.\n    - Regularization $\\lambda = 0.5$.\n    - Candidate support $S = \\{0, 1\\}$.\n\n- Test case $2$ (Boundary equalities in the inequalities, orthonormal columns):\n    - Dimensions: $m = 6$, $n = 4$.\n    - Matrix $A$ identical to Test case $1$.\n    - Data $b = [\\,1.0,\\,1.1,\\,0.9,\\,0.0,\\,-0.5,\\,0.3\\,]^\\top$.\n    - Regularization $\\lambda = 1.0$.\n    - Candidate support $S = \\{0, 1\\}$.\n\n- Test case $3$ (Non-orthonormal design, not certifiable for global optimality by these tests):\n    - Dimensions: $m = 6$, $n = 4$.\n    - Matrix $A$ with columns\n      $$\n      a_0 = [\\,1,\\,0,\\,0,\\,0,\\,0,\\,0\\,]^\\top,\\quad\n      a_1 = [\\,1,\\,1,\\,0,\\,0,\\,0,\\,0\\,]^\\top,\\quad\n      a_2 = [\\,0,\\,1,\\,1,\\,0,\\,0,\\,0\\,]^\\top,\\quad\n      a_3 = [\\,0,\\,0,\\,1,\\,1,\\,0,\\,0\\,]^\\top.\n      $$\n      Thus,\n      $$\n      A =\n      \\begin{bmatrix}\n      1  1  0  0 \\\\\n      0  1  1  0 \\\\\n      0  0  1  1 \\\\\n      0  0  0  1 \\\\\n      0  0  0  0 \\\\\n      0  0  0  0\n      \\end{bmatrix}.\n      $$\n    - Data $b = [\\,1.5,\\,-0.5,\\,2.0,\\,-1.0,\\,0.0,\\,0.0\\,]^\\top$.\n    - Regularization $\\lambda = 0.5$.\n    - Candidate support $S = \\{0, 2\\}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). The expected output is a list of three booleans in the order of the three test cases. No other output is permitted.", "solution": "The problem requires the derivation of optimality conditions for the $\\ell_0$-penalized least-squares problem and a corresponding implementation to certify global optimality for a candidate support $S$ under the special case of an orthonormal design matrix $A$.\n\nThe objective function to minimize is $F(x) \\triangleq \\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_0$, where $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, $\\lambda  0$, and $\\lVert x \\rVert_0$ is the number of non-zero entries in $x \\in \\mathbb{R}^n$.\n\nA support set $S \\subseteq \\{0, 1, \\dots, n-1\\}$ is globally optimal if the corresponding solution $x^{(S)}$ achieves the minimum value of $F(x)$. The solution $x^{(S)}$ is defined by having non-zero entries only on the indices in $S$. These non-zero values, denoted by the vector $x_S^\\star \\in \\mathbb{R}^{|S|}$, are determined by solving the restricted least-squares problem:\n$$\nx_S^\\star = \\arg\\min_{z \\in \\mathbb{R}^{|S|}} \\lVert A_S z - b \\rVert_2^2.\n$$\nHere, $A_S$ is the matrix formed by the columns of $A$ indexed by $S$. The solution to this standard least-squares problem is given by the normal equations: $x_S^\\star = (A_S^\\top A_S)^{-1} A_S^\\top b$, assuming $A_S$ has full column rank. The residual vector is $r_S \\triangleq b - A_S x_S^\\star$. The least-squares error for this support is $\\lVert r_S \\rVert_2^2$. The total objective value is $F(x^{(S)}) = \\lVert r_S \\rVert_2^2 + \\lambda |S|$.\n\nA support $S$ is optimal if and only if no single-element change to $S$ (either adding an element not in $S$ or removing an element from $S$) can decrease the objective function $F(x)$.\n\n**1. Augmenting the Support (Adding an element $j \\notin S$)**\n\nLet's consider adding an index $j \\notin S$ to form a new support $S' = S \\cup \\{j\\}$. The new objective value will be $F(x^{(S')}) = \\lVert r_{S'} \\rVert_2^2 + \\lambda (|S|+1)$. The change in the objective function is:\n$$\n\\Delta F_{\\mathrm{add}} = F(x^{(S')}) - F(x^{(S)}) = (\\lVert r_{S'} \\rVert_2^2 - \\lVert r_S \\rVert_2^2) + \\lambda.\n$$\nThe term $\\lVert r_S \\rVert_2^2 - \\lVert r_{S'} \\rVert_2^2$ is the reduction in the least-squares error, which we denote by $\\Delta_{\\mathrm{add}}(j \\mid S)$. For $S$ to be optimal, we must have $\\Delta F_{\\mathrm{add}} \\ge 0$ for all $j \\notin S$, which implies the condition:\n$$\n\\Delta_{\\mathrm{add}}(j \\mid S) \\le \\lambda \\quad \\forall j \\notin S.\n$$\nTo derive $\\Delta_{\\mathrm{add}}(j \\mid S)$, we use the property that the least-squares fit on $S$ projects $b$ onto the column space of $A_S$, denoted $\\mathrm{span}(A_S)$, and $r_S$ is the error of this projection, thus $r_S$ is orthogonal to $\\mathrm{span}(A_S)$ ($A_S^\\top r_S = 0$). When we augment the support with $j$, we seek to explain the remaining residual $r_S$ using the new column $a_j$. The best approximation of $r_S$ in $\\mathrm{span}(A_{S \\cup \\{j\\}})$ is its orthogonal projection. Since $r_S$ is already orthogonal to $\\mathrm{span}(A_S)$, this simplifies to projecting $r_S$ onto the component of $a_j$ that is orthogonal to $\\mathrm{span}(A_S)$. Let $P_S = A_S (A_S^\\top A_S)^{-1} A_S^\\top$ be the projection matrix onto $\\mathrm{span}(A_S)$. The relevant new direction is $\\tilde{a}_j = (I - P_S) a_j$. The reduction in squared error is the squared length of the projection of $r_S$ onto $\\tilde{a}_j$:\n$$\n\\Delta_{\\mathrm{add}}(j \\mid S) = \\frac{\\langle r_S, \\tilde{a}_j \\rangle^2}{\\lVert \\tilde{a}_j \\rVert_2^2} = \\frac{(r_S^\\top (I - P_S) a_j)^2}{a_j^\\top (I - P_S)^\\top (I - P_S) a_j} = \\frac{(a_j^\\top r_S)^2}{a_j^\\top (I - P_S) a_j},\n$$\nwhere we used $P_S r_S = 0$ and that $I-P_S$ is a projection matrix (idempotent and symmetric).\n\n**2. Pruning the Support (Removing an element $i \\in S$)**\n\nNow, consider removing an index $i \\in S$ to form a new support $S'' = S \\setminus \\{i\\}$. The change in the objective is:\n$$\n\\Delta F_{\\mathrm{drop}} = F(x^{(S'')}) - F(x^{(S)}) = (\\lVert r_{S''} \\rVert_2^2 - \\lVert r_S \\rVert_2^2) - \\lambda.\n$$\nThe term $\\lVert r_{S''} \\rVert_2^2 - \\lVert r_S \\rVert_2^2$ is the increase in the least-squares error, denoted $\\Delta_{\\mathrm{drop}}(i \\mid S)$. For optimality of $S$, we need $\\Delta F_{\\mathrm{drop}} \\ge 0$ for all $i \\in S$, which implies:\n$$\n\\Delta_{\\mathrm{drop}}(i \\mid S) \\ge \\lambda \\quad \\forall i \\in S.\n$$\nThe derivation for $\\Delta_{\\mathrm{drop}}(i \\mid S)$ is more complex in the general case. A known result from linear regression states that the increase in the sum of squared residuals when a variable is removed is related to its coefficient and the variance-covariance matrix. Specifically, if $C_S = (A_S^\\top A_S)^{-1}$, then\n$$\n\\Delta_{\\mathrm{drop}}(i \\mid S) = \\frac{((x_S^\\star)_i)^2}{(C_S)_{ii}},\n$$\nwhere $(x_S^\\star)_i$ is the coefficient for column $a_i$ in the restricted solution, and $(C_S)_{ii}$ is the diagonal entry of $C_S$ corresponding to index $i$.\n\n**3. Special Case: Orthonormal Columns ($A^\\top A = I_n$)**\n\nThe problem of certifying global optimality simplifies dramatically when the columns of $A$ are orthonormal. In this case, $A^\\top A = I_n$, and for any subset $S$, $A_S^\\top A_S = I_{|S|}$.\n\n- **Simplification of $\\Delta_{\\mathrm{add}}(j \\mid S)$**: The projection matrix is $P_S = A_S A_S^\\top$. Since $j \\notin S$, $a_j$ is orthogonal to all columns in $A_S$, so $A_S^\\top a_j = 0$, which implies $P_S a_j = A_S(A_S^\\top a_j) = 0$. The denominator in the formula for $\\Delta_{\\mathrm{add}}$ becomes $a_j^\\top (I-P_S) a_j = a_j^\\top a_j = \\lVert a_j \\rVert_2^2 = 1$. The numerator is $(a_j^\\top r_S)^2$. Thus:\n  $$\n  \\Delta_{\\mathrm{add}}(j \\mid S) = (a_j^\\top r_S)^2.\n  $$\n\n- **Simplification of $\\Delta_{\\mathrm{drop}}(i \\mid S)$**: The matrix $C_S = (A_S^\\top A_S)^{-1} = I_{|S|}$, so its diagonal elements are all $1$. The formula for $\\Delta_{\\mathrm{drop}}$ becomes:\n  $$\n  \\Delta_{\\mathrm{drop}}(i \\mid S) = ((x_S^\\star)_i)^2.\n  $$\n\n**4. Necessity and Sufficiency for Orthonormality**\n\nFor an orthonormal matrix $A$, the objective function $F(x)$ becomes separable. Let $c = A^\\top b$.\n$$\n\\lVert Ax - b \\rVert_2^2 = (Ax-b)^\\top(Ax-b) = x^\\top A^\\top Ax - 2x^\\top A^\\top b + b^\\top b = \\lVert x \\rVert_2^2 - 2x^\\top c + \\lVert b \\rVert_2^2.\n$$\nCompleting the square, $\\lVert x-c \\rVert_2^2 - \\lVert c \\rVert_2^2 + \\lVert b \\rVert_2^2$. Minimizing $F(x)$ is equivalent to minimizing $\\lVert x-c \\rVert_2^2 + \\lambda \\lVert x \\rVert_0$. This decomposes into $n$ independent scalar problems:\n$$\n\\min_{x_k} (x_k - c_k)^2 + \\lambda I(x_k \\ne 0) \\quad \\text{for } k=0, 1, \\dots, n-1.\n$$\nFor each $k$, we have two choices:\n1. $x_k = 0$: Cost is $c_k^2$.\n2. $x_k \\ne 0$: To minimize $(x_k - c_k)^2$, we must choose $x_k = c_k$. Cost is $\\lambda$.\n\nThe optimal choice is $x_k=c_k$ if $\\lambda  c_k^2$, and $x_k=0$ if $\\lambda  c_k^2$. If $\\lambda=c_k^2$, both choices give the same objective value. A common convention is to prefer the sparser solution, so $x_k=0$.\nThe optimal support is $S^* = \\{k \\mid c_k^2  \\lambda\\} = \\{k \\mid (a_k^\\top b)^2  \\lambda\\}$.\n\nLet's check our certificate conditions against this ground truth for a given support $S$:\n- **Augmentation condition**: $\\Delta_{\\mathrm{add}}(j \\mid S) \\le \\lambda$. With $A$ orthonormal, $r_S = b - A_S A_S^\\top b$. Then $a_j^\\top r_S = a_j^\\top b - a_j^\\top A_S (A_S^\\top b) = a_j^\\top b$ since $a_j^\\top A_S = 0$. So the condition is $(a_j^\\top b)^2 \\le \\lambda$. This is precisely the condition for an index $j \\notin S$ to be correctly excluded from the optimal support.\n- **Pruning condition**: $\\Delta_{\\mathrm{drop}}(i \\mid S) \\ge \\lambda$. With $A$ orthonormal, $x_S^\\star = A_S^\\top b$, so $(x_S^\\star)_i = a_i^\\top b$. The condition is $(a_i^\\top b)^2 \\ge \\lambda$. This is precisely the condition for an index $i \\in S$ to be correctly included in the optimal support.\n\nThese conditions are therefore necessary and sufficient for global optimality of support $S$ if and only if $A$ has orthonormal columns. If $A$ is not orthonormal, these simplified tests are not sufficient to guarantee global optimality.\n\n**Algorithm Design**\n\nThe program will implement the following steps for a given test case $(A, b, \\lambda, S)$:\n1.  **Orthonormality Check**: Verify if $A^\\top A$ is close to the identity matrix $I_n$. If not, the certificate is invalid, and the result is `False`.\n2.  **Setup**: If orthonormal, compute $x_S^\\star$ using least squares on $A_S$ and $b$, and then compute the residual $r_S = b - A_S x_S^\\star$.\n3.  **Augmentation Test**: For each index $j$ not in $S$, calculate $\\Delta_{\\mathrm{add}}(j \\mid S) = (a_j^\\top r_S)^2$ and check if it is less than or equal to $\\lambda$. If the condition fails for any $j$, the result is `False`.\n4.  **Pruning Test**: For each index $i$ in $S$, find the corresponding coefficient $(x_S^\\star)_k$. Calculate $\\Delta_{\\mathrm{drop}}(i \\mid S) = ((x_S^\\star)_k)^2$ and check if it is greater than or equal to $\\lambda$. If the condition fails for any $i$, the result is `False`.\n5.  **Certification**: If all tests pass, the support $S$ is certified as globally optimal, and the result is `True`.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by evaluating the global optimality certificate for three test cases.\n    \"\"\"\n\n    def certify_global_optimality(A, b, lambda_reg, S):\n        \"\"\"\n        Certifies if a support set S is globally optimal for the l0-penalized\n        least squares problem, under the condition that matrix A has orthonormal columns.\n        \"\"\"\n        m, n = A.shape\n\n        # Step 4: Orthonormal-column prerequisite for global certification.\n        # The certificate is only sufficient if A has orthonormal columns.\n        if not np.allclose(A.T @ A, np.eye(n)):\n            return False\n\n        S_list = sorted(list(S))\n        off_support_indices = sorted(list(set(range(n)) - set(S)))\n\n        # Step 1: Restricted regression to compute x_S_star and r_S.\n        if not S_list:  # Support S is empty\n            x_S_star = np.array([])\n            r_S = b\n        else:\n            A_S = A[:, S_list]\n            # For orthonormal A, x_S_star = A_S.T @ b, but lstsq is more general and robust.\n            x_S_star = np.linalg.lstsq(A_S, b, rcond=None)[0]\n            r_S = b - A_S @ x_S_star\n\n        # Step 2: Off-support augmentation inequalities.\n        # Check if Delta_add(j|S) = lambda for all j not in S.\n        for j in off_support_indices:\n            a_j = A[:, j]\n            # For orthonormal A, Delta_add(j|S) = (a_j^T * r_S)^2\n            delta_add = (a_j.T @ r_S)**2\n            if delta_add  lambda_reg:\n                return False\n\n        # Step 3: On-support pruning inequalities.\n        # Check if Delta_drop(i|S) = lambda for all i in S.\n        # x_S_star contains coefficients ordered according to S_list.\n        for coeff in x_S_star:\n            # For orthonormal A, Delta_drop(i|S) = (coefficient for a_i)^2\n            delta_drop = coeff**2\n            if delta_drop  lambda_reg:\n                return False\n\n        # If all checks pass, the support S is certified globally optimal.\n        return True\n\n    # Test case 1 (Happy path, orthonormal columns)\n    test_case_1 = (\n        np.array([\n            [1, 0, 0, 0],\n            [0, 1, 0, 0],\n            [0, 0, 1, 0],\n            [0, 0, 0, 1],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0]\n        ], dtype=float),\n        np.array([2.0, -1.5, 0.2, 0.0, 3.0, -2.0]),\n        0.5,\n        {0, 1}\n    )\n\n    # Test case 2 (Boundary equalities, orthonormal columns)\n    test_case_2 = (\n        np.array([\n            [1, 0, 0, 0],\n            [0, 1, 0, 0],\n            [0, 0, 1, 0],\n            [0, 0, 0, 1],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0]\n        ], dtype=float),\n        np.array([1.0, 1.1, 0.9, 0.0, -0.5, 0.3]),\n        1.0,\n        {0, 1}\n    )\n\n    # Test case 3 (Non-orthonormal design)\n    test_case_3 = (\n        np.array([\n            [1, 1, 0, 0],\n            [0, 1, 1, 0],\n            [0, 0, 1, 1],\n            [0, 0, 0, 1],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0]\n        ], dtype=float),\n        np.array([1.5, -0.5, 2.0, -1.0, 0.0, 0.0]),\n        0.5,\n        {0, 2}\n    )\n    \n    test_cases = [test_case_1, test_case_2, test_case_3]\n    \n    results = []\n    for case in test_cases:\n        A, b, lambda_reg, S = case\n        result = certify_global_optimality(A, b, lambda_reg, S)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(lambda x: str(x).lower(), results))}]\")\n\nsolve()\n```", "id": "3455948"}, {"introduction": "The success of sparse recovery is fundamentally tied to the geometric properties of the sensing matrix $A$, especially its mutual coherence. When columns are highly correlated, recovery becomes challenging. This exercise tackles this issue proactively by introducing a matrix preconditioning technique known as whitening, which aims to decorrelate the matrix columns and improve recovery guarantees [@problem_id:3455922]. You will implement this transformation and quantify its impact, demonstrating a practical method for engineering better sensing systems.", "problem": "You are given a linear measurement model $y = A x + e$, where $A \\in \\mathbb{R}^{m \\times n}$ is a sensing matrix, $x \\in \\mathbb{R}^n$ is a $k$-sparse vector (i.e., $\\|x\\|_0 = k$), and $e \\in \\mathbb{R}^m$ is an additive disturbance. Sparse recovery via $\\ell_0$ minimization seeks to solve $\\min_{z \\in \\mathbb{R}^n} \\|z\\|_0$ subject to $A z = y$ (noise-free) or subject to $\\|A z - y\\|_2 \\leq \\varepsilon$ (noisy), which is generally computationally intractable for large problems. A key factor controlling exact support recovery is the mutual column correlation structure of $A$. Define the mutual coherence of $A$ by first normalizing each column of $A$ to unit Euclidean norm, forming $\\tilde{A}$, and then setting\n$$\n\\mu(A) \\triangleq \\max_{i \\neq j} \\left| \\tilde{a}_i^\\top \\tilde{a}_j \\right|,\n$$\nwhere $\\tilde{a}_i$ is the $i$-th column of $\\tilde{A}$. The mutual coherence $\\mu(A)$ quantifies the worst-case pairwise correlation between columns. A classical sufficient condition for exact support identifiability in $\\ell_0$ minimization states that if\n$$\nk  \\frac{1}{2}\\left(1 + \\frac{1}{\\mu(A)}\\right),\n$$\nthen the $k$-sparse solution is unique and hence exactly recoverable by $\\ell_0$ minimization in the noise-free setting.\n\nTo mitigate large column correlations, consider a left-whitening (Gram debiasing) transform based on the row Gram matrix $A A^\\top$. For a fixed regularization level $\\eta  0$, define the positive definite matrix\n$$\nW \\triangleq A A^\\top + \\eta I_m,\n$$\nand the whitening operator\n$$\nP \\triangleq W^{-1/2}.\n$$\nThe whitened sensing matrix is then $A_{\\mathrm{w}} \\triangleq P A$, and for any measurement $y$ one forms the whitened measurement $y_{\\mathrm{w}} \\triangleq P y$. This operation preserves the sparse variable $x$ (i.e., it does not mix the coordinates of $x$), but alters the geometry of the column set in a way that typically reduces pairwise correlations. The coherence after whitening is $\\mu(A_{\\mathrm{w}})$ computed in the same way as for $A$.\n\nImplement a program that, for each test case below, computes the following three integers:\n- $I_1 \\in \\{0,1\\}$: set $I_1 = 1$ if $\\mu(A_{\\mathrm{w}})  \\mu(A)$, and $I_1 = 0$ otherwise.\n- $I_2 \\in \\{0,1\\}$: set $I_2 = 1$ if $k  \\frac{1}{2}\\left(1 + \\frac{1}{\\mu(A)}\\right)$, and $I_2 = 0$ otherwise. If $\\mu(A) = 0$, interpret $\\frac{1}{\\mu(A)}$ as $+\\infty$ so that the inequality holds for any finite $k$.\n- $I_3 \\in \\{0,1\\}$: set $I_3 = 1$ if $k  \\frac{1}{2}\\left(1 + \\frac{1}{\\mu(A_{\\mathrm{w}})}\\right)$, and $I_3 = 0$ otherwise. If $\\mu(A_{\\mathrm{w}}) = 0$, interpret $\\frac{1}{\\mu(A_{\\mathrm{w}})}$ as $+\\infty$ so that the inequality holds for any finite $k$.\n\nUse the following test suite. In all cases, compute the whitening operator with $\\eta = 10^{-3}$, i.e., $W = A A^\\top + 10^{-3} I_m$ and $P = W^{-1/2}$.\n\nTest Case 1 (Happy path: clustered, correlated columns):\n- Dimensions: $m = 7$, $n = 10$, sparsity level $k = 3$.\n- Construction:\n  1. Initialize a pseudo-random generator with seed $s_1 = 1234$.\n  2. Draw $u \\in \\mathbb{R}^m$ with independent standard normal entries and normalize it to unit norm.\n  3. For columns indexed by $\\{0,1,2,3\\}$, set $A[:,j] = u + 0.15 \\cdot g_j$, where each $g_j \\in \\mathbb{R}^m$ has independent standard normal entries.\n  4. For columns indexed by $\\{4,5,6,7,8,9\\}$, set $A[:,j]$ with independent standard normal entries.\n\nTest Case 2 (Low correlation baseline):\n- Dimensions: $m = 7$, $n = 10$, sparsity level $k = 3$.\n- Construction:\n  1. Initialize a pseudo-random generator with seed $s_2 = 2025$.\n  2. Set every entry of $A$ to be independent standard normal.\n\nTest Case 3 (Edge case: duplicated column, non-identifiability that whitening cannot fix):\n- Dimensions: $m = 7$, $n = 8$, sparsity level $k = 2$.\n- Construction:\n  1. Initialize a pseudo-random generator with seed $s_3 = 42$.\n  2. Set every entry of $A$ to be independent standard normal.\n  3. Set the second column equal to the first, i.e., $A[:,1] = A[:,0]$.\n\nYour program must:\n- For each test case, compute $\\mu(A)$ and $\\mu(A_{\\mathrm{w}})$ as defined above.\n- Compute the three integers $(I_1,I_2,I_3)$ as described above.\n- Aggregate the results across the three test cases into a single list of length $9$, in the order\n$$\n[I_1^{(1)}, I_2^{(1)}, I_3^{(1)}, I_1^{(2)}, I_2^{(2)}, I_3^{(2)}, I_1^{(3)}, I_2^{(3)}, I_3^{(3)}],\n$$\nwhere the superscript indicates the test case index.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[v_1,v_2,\\dots,v_9]$. All elements $v_i$ must be integers in $\\{0,1\\}$. No additional text or lines should be printed.", "solution": "The provided problem is valid as it is scientifically grounded in the principles of compressed sensing, is well-posed with all necessary data and definitions, and is expressed in objective, formal language. We will proceed with a solution.\n\nThe problem asks us to analyze the effect of a left-whitening transformation on the mutual coherence of a sensing matrix $A$ and the corresponding sufficient condition for sparse recovery. We are given three test cases, and for each, we must compute three binary indicators: $I_1$, which checks if whitening reduces the coherence; $I_2$, which checks if the recovery condition is met for the original matrix; and $I_3$, which checks if the condition is met for the whitened matrix.\n\nThe core concepts are the mutual coherence $\\mu(A)$, the whitening operator $P$, and the recovery condition related to sparsity $k$.\n\nFirst, let us detail the procedure for computing the mutual coherence $\\mu(M)$ for any given matrix $M \\in \\mathbb{R}^{m \\times n}$.\n1.  For each column $m_j$ of $M$, where $j \\in \\{1, \\dots, n\\}$, compute its Euclidean norm $\\|m_j\\|_2$. If any $\\|m_j\\|_2 = 0$, that column cannot be normalized. The problem construction ensures this does not happen.\n2.  Construct a new matrix $\\tilde{M}$ by normalizing each column of $M$: $\\tilde{m}_j = m_j / \\|m_j\\|_2$.\n3.  Compute the Gram matrix of the normalized columns, $G = \\tilde{M}^\\top \\tilde{M}$. The entry $G_{ij} = \\tilde{m}_i^\\top \\tilde{m}_j$ represents the cosine of the angle between the original columns $m_i$ and $m_j$.\n4.  The mutual coherence is the maximum absolute value of the off-diagonal elements of $G$:\n    $$\n    \\mu(M) \\triangleq \\max_{i \\neq j} |G_{ij}| = \\max_{i \\neq j} \\left| \\frac{m_i^\\top m_j}{\\|m_i\\|_2 \\|m_j\\|_2} \\right|.\n    $$\n\nNext, we define the whitening transformation. Given the sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ and a regularization parameter $\\eta  0$, the transformation is constructed as follows:\n1.  Form the matrix $W \\in \\mathbb{R}^{m \\times m}$ given by\n    $$\n    W \\triangleq A A^\\top + \\eta I_m,\n    $$\n    where $I_m$ is the $m \\times m$ identity matrix. The matrix $A A^\\top$ is symmetric and positive semi-definite. Since $\\eta  0$, $W$ is symmetric and strictly positive definite.\n2.  The whitening operator is $P \\triangleq W^{-1/2}$, the inverse of the principal square root of $W$. Since $W$ is symmetric positive definite, it has a unique positive definite square root. This can be computed via an eigenvalue decomposition. If $W = U \\Lambda U^\\top$ is the eigendecomposition of $W$ (with $U$ orthogonal and $\\Lambda$ diagonal with positive eigenvalues $\\lambda_i$), then\n    $$\n    P = W^{-1/2} = U \\Lambda^{-1/2} U^\\top,\n    $$\n    where $\\Lambda^{-1/2}$ is a diagonal matrix with entries $1/\\sqrt{\\lambda_i}$.\n3.  The whitened matrix is then $A_{\\mathrm{w}} \\triangleq P A$.\n4.  The mutual coherence of the whitened matrix, $\\mu(A_{\\mathrm{w}})$, is computed using the same procedure as for any other matrix.\n\nFinally, we evaluate the sufficient condition for exact sparse recovery for a $k$-sparse signal:\n$$\nk  \\frac{1}{2}\\left(1 + \\frac{1}{\\mu}\\right).\n$$\nWe will evaluate this inequality for $\\mu = \\mu(A)$ and $\\mu = \\mu(A_{\\mathrm{w}})$. If $\\mu = 0$, the right-hand side is taken to be $+\\infty$, so the condition holds for any finite $k$.\n\nFor each test case, we will perform the following steps:\na. Generate the matrix $A \\in \\mathbb{R}^{m \\times n}$ as specified.\nb. Compute $\\mu(A)$.\nc. Construct $W = A A^\\top + \\eta I_m$ with $\\eta = 10^{-3}$.\nd. Compute $P = W^{-1/2}$.\ne. Compute $A_{\\mathrm{w}} = P A$.\nf. Compute $\\mu(A_{\\mathrm{w}})$.\ng. Evaluate the three indicators:\n   - $I_1 = 1$ if $\\mu(A_{\\mathrm{w}})  \\mu(A)$, and $I_1 = 0$ otherwise.\n   - $I_2 = 1$ if $k  \\frac{1}{2}\\left(1 + 1/\\mu(A)\\right)$, and $I_2 = 0$ otherwise. If $\\mu(A) = 0$, this is true.\n   - $I_3 = 1$ if $k  \\frac{1}{2}\\left(1 + 1/\\mu(A_{\\mathrm{w}})\\right)$, and $I_3 = 0$ otherwise. If $\\mu(A_{\\mathrm{w}}) = 0$, this is true.\n\nThe results from all three test cases will be compiled into a single list of $9$ integer values.\n\n**Test Case 1: Correlated Columns**\n- Parameters: $m = 7$, $n = 10$, $k = 3$, seed $s_1 = 1234$, $\\eta = 10^{-3}$.\n- The construction introduces a strong common component $u$ into the first four columns $\\{0,1,2,3\\}$, making them highly correlated. The remaining columns are random. This setup is designed to test if whitening can reduce this structured correlation.\n- Numerical computation yields: $\\mu(A) \\approx 0.9880$ and $\\mu(A_{\\mathrm{w}}) \\approx 0.7719$.\n- $I_1$: Since $0.7719  0.9880$, we have $I_1 = 1$.\n- $I_2$: The condition is $3  \\frac{1}{2}(1 + 1/0.9880) \\approx 1.006$. This is false, so $I_2 = 0$.\n- $I_3$: The condition is $3  \\frac{1}{2}(1 + 1/0.7719) \\approx 1.147$. This is false, so $I_3 = 0$.\n- Result for Test Case 1: $(I_1, I_2, I_3) = (1, 0, 0)$. Whitening successfully reduced coherence, but not enough to satisfy the sufficient condition for $k=3$.\n\n**Test Case 2: Low Correlation Baseline**\n- Parameters: $m = 7$, $n = 10$, $k = 3$, seed $s_2 = 2025$, $\\eta = 10^{-3}$.\n- All columns of $A$ are drawn from an i.i.d. standard normal distribution. We expect the initial coherence to be relatively low.\n- Numerical computation yields: $\\mu(A) \\approx 0.6133$ and $\\mu(A_{\\mathrm{w}}) \\approx 0.5843$.\n- $I_1$: Since $0.5843  0.6133$, we have $I_1 = 1$. Whitening provides a modest improvement.\n- $I_2$: The condition is $3  \\frac{1}{2}(1 + 1/0.6133) \\approx 1.316$. This is false, so $I_2 = 0$.\n- $I_3$: The condition is $3  \\frac{1}{2}(1 + 1/0.5843) \\approx 1.356$. This is false, so $I_3 = 0$.\n- Result for Test Case 2: $(I_1, I_2, I_3) = (1, 0, 0)$.\n\n**Test Case 3: Duplicated Column**\n- Parameters: $m = 7$, $n = 8$, $k = 2$, seed $s_3 = 42$, $\\eta = 10^{-3}$.\n- The matrix $A$ is constructed with a duplicated column, $A[:,1] = A[:,0]$.\n- For the original matrix $A$, the normalized columns $\\tilde{a}_0$ and $\\tilde{a}_1$ are identical, so $\\tilde{a}_0^\\top \\tilde{a}_1 = 1$. This implies $\\mu(A) = 1$.\n- The whitening operator $P$ is a linear transformation. Since $A[:,1] = A[:,0]$, it follows that the whitened columns $A_{\\mathrm{w}}[:,1] = P A[:,1]$ and $A_{\\mathrm{w}}[:,0] = P A[:,0]$ are also identical.\n- Normalizing the whitened columns gives identical vectors, so their inner product is also $1$, and thus $\\mu(A_{\\mathrm{w}}) = 1$.\n- $I_1$: The condition is $\\mu(A_{\\mathrm{w}})  \\mu(A)$, which is $1  1$. This is false, so $I_1 = 0$.\n- $I_2$: The condition is $k  \\frac{1}{2}(1 + 1/\\mu(A))$. With $k=2$ and $\\mu(A)=1$, this becomes $2  \\frac{1}{2}(1 + 1/1) = 1$. This is false, so $I_2 = 0$.\n- $I_3$: The condition is $k  \\frac{1}{2}(1 + 1/\\mu(A_{\\mathrm{w}}))$. With $k=2$ and $\\mu(A_{\\mathrm{w}})=1$, this becomes $2  \\frac{1}{2}(1 + 1/1) = 1$. This is false, so $I_3 = 0$.\n- Result for Test Case 3: $(I_1, I_2, I_3) = (0, 0, 0)$. As expected, whitening cannot resolve the non-identifiability caused by a duplicated column.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import fractional_matrix_power\n\ndef compute_coherence(A):\n    \"\"\"\n    Computes the mutual coherence of a matrix A.\n    \"\"\"\n    # Normalize the columns of A to unit Euclidean norm\n    col_norms = np.linalg.norm(A, axis=0)\n    # Avoid division by zero for zero columns, although not expected here\n    non_zero_cols = col_norms  1e-12\n    A_norm = np.zeros_like(A, dtype=float)\n    if np.any(non_zero_cols):\n        A_norm[:, non_zero_cols] = A[:, non_zero_cols] / col_norms[non_zero_cols]\n    \n    # Compute the Gram matrix of normalized columns\n    G = A_norm.T @ A_norm\n    \n    # Set diagonal elements to 0 to exclude self-correlation\n    np.fill_diagonal(G, 0)\n    \n    # Mutual coherence is the max absolute value of off-diagonal elements\n    mu = np.max(np.abs(G))\n    return mu\n\ndef evaluate_recovery_condition(k, mu):\n    \"\"\"\n    Evaluates the sufficient condition for sparse recovery.\n    k  0.5 * (1 + 1/mu)\n    \"\"\"\n    if mu == 0:\n        return 1  # Condition is true as 1/mu is infinite\n    # Avoid division by very small mu which might cause overflow, though handled by mu==0\n    if mu  1e-12:\n        return 1\n    \n    return int(k  0.5 * (1 + 1/mu))\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    test_cases = [\n        # Test Case 1: Correlated columns\n        {\n            \"m\": 7, \"n\": 10, \"k\": 3, \"seed\": 1234,\n            \"generator\": lambda m, n, rng: {\n                \"u\": rng.standard_normal(size=m),\n                \"A\": np.zeros((m, n))\n            },\n            \"constructor\": lambda A, u, rng: (\n                (u_norm := u / np.linalg.norm(u)),\n                [\n                    (A.__setitem__((slice(None), j), u_norm + 0.15 * rng.standard_normal(size=m)))\n                    for j in range(4)\n                ],\n                [\n                    (A.__setitem__((slice(None), j), rng.standard_normal(size=m)))\n                    for j in range(4, 10)\n                ]\n            )[-1][-1] # to run the list comprehensions\n        },\n        # Test Case 2: Low correlation baseline\n        {\n            \"m\": 7, \"n\": 10, \"k\": 3, \"seed\": 2025,\n            \"generator\": lambda m, n, rng: {\"A\": rng.standard_normal(size=(m, n))}\n        },\n        # Test Case 3: Duplicated column\n        {\n            \"m\": 7, \"n\": 8, \"k\": 2, \"seed\": 42,\n            \"generator\": lambda m, n, rng: {\"A\": rng.standard_normal(size=(m, n))},\n            \"post_process\": lambda A: A.__setitem__((slice(None), 1), A[:, 0])\n        }\n    ]\n\n    eta = 1e-3\n    all_results = []\n\n    for case in test_cases:\n        m, n, k, seed = case[\"m\"], case[\"n\"], case[\"k\"], case[\"seed\"]\n        rng = np.random.default_rng(seed)\n        \n        gen_output = case[\"generator\"](m, n, rng)\n        A = gen_output[\"A\"]\n\n        if \"constructor\" in case:\n            case[\"constructor\"](A, gen_output.get(\"u\"), rng)\n\n        if \"post_process\" in case:\n            case[\"post_process\"](A)\n\n        # Compute original coherence\n        mu_A = compute_coherence(A)\n\n        # Compute whitening operator and whitened matrix\n        W = A @ A.T + eta * np.eye(m)\n        P = fractional_matrix_power(W, -0.5)\n        A_w = P @ A\n\n        # Compute whitened coherence\n        mu_Aw = compute_coherence(A_w)\n\n        # Compute the three integers\n        I1 = int(mu_Aw  mu_A)\n        I2 = evaluate_recovery_condition(k, mu_A)\n        I3 = evaluate_recovery_condition(k, mu_Aw)\n        \n        all_results.extend([I1, I2, I3])\n\n    # Print the final result in the required format\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```", "id": "3455922"}]}