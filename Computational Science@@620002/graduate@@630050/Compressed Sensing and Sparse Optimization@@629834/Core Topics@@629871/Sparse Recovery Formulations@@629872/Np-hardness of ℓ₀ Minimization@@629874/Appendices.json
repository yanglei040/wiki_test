{"hands_on_practices": [{"introduction": "Before delving into the formal theory of NP-hardness, it is essential to build intuition about why the $\\ell_0$ minimization problem is so difficult. This first practice problem provides a hands-on experience with the combinatorial explosion that lies at the heart of the problem's intractability. By working through a small, carefully constructed example, you will directly confront the need to search over a rapidly growing number of possible supports to find the sparsest solution [@problem_id:3463365].", "problem": "Consider the cardinality-constrained least-squares subset selection problem: given a matrix $X \\in \\mathbb{R}^{m \\times n}$, a vector $y \\in \\mathbb{R}^{m}$, and a sparsity budget $k \\in \\{0,1,\\dots,n\\}$, find\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\|y - Xx\\|_{2}^{2} \\quad \\text{subject to} \\quad \\|x\\|_{0} \\le k,\n$$\nwhere $\\|x\\|_{0}$ denotes the number of nonzero entries of $x$. This is the canonical formulation of $\\ell_0$-constrained least squares (also called exact subset selection), which is known to be computationally intractable in general and is a prototypical example of a problem that is hard for Nondeterministic Polynomial-time (NP).\n\nWork through the following concrete instance to expose, from first principles, the combinatorial nature of the search over supports and to compute the exact optimal objective value.\n\n- Let $m = 6$, $n = 6$, $k = 2$.\n- Let $X = I_{6}$, the $6 \\times 6$ identity matrix.\n- Let $y = \\begin{pmatrix} 7 \\\\ -2 \\\\ 3 \\\\ -6 \\\\ 1 \\\\ 4 \\end{pmatrix}$.\n\nTasks:\n- Argue from definitions why solving the problem requires, in principle, examining supports $S \\subseteq \\{1,2,\\dots,6\\}$ with $|S| \\le k$, and explicitly identify how many such supports there are in this instance, thereby illustrating the combinatorial search space that underlies intractability.\n- Using only basic properties of Euclidean projections and linear least squares on a fixed support, determine the exact minimal value of the objective $\\|y - Xx\\|_{2}^{2}$ for this instance.\n\nYour final answer should be the exact real number equal to the minimal value of $\\|y - Xx\\|_{2}^{2}$ for the specified $(X,y,k)$. No rounding is required. Do not use any heuristic or relaxation; base your reasoning strictly on the definition of the problem and fundamental properties of projections in Euclidean space.", "solution": "The problem asks for the solution to an instance of the cardinality-constrained least-squares subset selection problem, stated as:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\|y - Xx\\|_{2}^{2} \\quad \\text{subject to} \\quad \\|x\\|_{0} \\le k\n$$\nThe specific instance is defined by the parameters $m = 6$, $n = 6$, $k = 2$, with the matrix $X = I_{6}$ (the $6 \\times 6$ identity matrix) and the vector $y = \\begin{pmatrix} 7 \\\\ -2 \\\\ 3 \\\\ -6 \\\\ 1 \\\\ 4 \\end{pmatrix}$. We must first address the combinatorial nature of the problem and then compute the exact minimal objective value.\n\nFirst, we analyze the structure of the problem. The constraint $\\|x\\|_{0} \\le k$ dictates that the solution vector $x \\in \\mathbb{R}^{n}$ can have at most $k$ non-zero components. The set of indices of these non-zero components, $S = \\{j \\in \\{1, \\dots, n\\} \\mid x_j \\neq 0\\}$, is called the support of $x$. The constraint is equivalent to requiring that the support $S$ of $x$ has cardinality $|S| \\le k$.\n\nFor any fixed choice of a support $S$, the problem becomes a standard linear least-squares problem over the variables $\\{x_j\\}_{j \\in S}$, while $x_j=0$ for all $j \\notin S$. Let $x_S$ be the vector of these variables and let $X_S$ be the submatrix of $X$ formed by the columns with indices in $S$. The problem for a fixed $S$ is $\\min_{x_S} \\|y - X_S x_S\\|_{2}^{2}$. To solve the original problem, one must in principle find the optimal solution for every possible support $S$ with $|S| \\le k$ and then select the support that yields the globally minimum objective value. This search over all possible supports is combinatorial in nature and is the source of the problem's NP-hardness.\n\nIn this instance, with $n=6$ and $k=2$, we must consider all supports $S \\subseteq \\{1, 2, 3, 4, 5, 6\\}$ such that $|S| \\le 2$. The total number of such supports is the sum of the number of supports of size $0$, $1$, and $2$. This count is given by the sum of binomial coefficients:\n$$\n\\sum_{j=0}^{k} \\binom{n}{j} = \\binom{6}{0} + \\binom{6}{1} + \\binom{6}{2}\n$$\nCalculating each term:\n- The number of supports of size $0$ is $\\binom{6}{0} = 1$ (the empty set).\n- The number of supports of size $1$ is $\\binom{6}{1} = 6$.\n- The number of supports of size $2$ is $\\binom{6}{2} = \\frac{6 \\times 5}{2 \\times 1} = 15$.\nThe total number of supports to examine is $1 + 6 + 15 = 22$. This demonstrates the combinatorial search space, which grows rapidly with $n$ and $k$.\n\nNext, we compute the exact minimal value of the objective $\\|y - Xx\\|_{2}^{2}$. A crucial feature of this instance is that $X = I_6$. The columns of $I_6$ are the standard basis vectors $e_1, \\dots, e_6$, which are orthonormal.\n\nFor a given support $S$, the columns of the submatrix $X_S$ are also orthonormal. The solution to the least-squares problem $\\min_{x_S} \\|y - X_S x_S\\|_{2}^{2}$ is given by $x_S = (X_S^T X_S)^{-1} X_S^T y$. Since the columns of $X_S$ are orthonormal, $X_S^T X_S = I_{|S|}$, the identity matrix of size $|S|$. The solution simplifies to $x_S = X_S^T y$. This means that for an index $j \\in S$, the optimal value of $x_j$ is the $j$-th component of $y$, i.e., $x_j = (e_j)^T y = y_j$. For indices $j \\notin S$, we have $x_j=0$.\n\nSo, for a fixed support $S$, the optimal solution vector is $x^*$, where $x_j^* = y_j$ if $j \\in S$ and $x_j^* = 0$ if $j \\notin S$.\nThe objective function for this problem is $\\|y - Xx\\|_{2}^{2}$. Since $X = I_6$, this becomes $\\|y - I_6 x\\|_{2}^{2} = \\|y - x\\|_{2}^{2}$.\nSubstituting the optimal $x^*$ for support $S$, the objective value is:\n$$\n\\|y - x^*\\|_{2}^{2} = \\sum_{j=1}^{6} (y_j - x_j^*)^2 = \\sum_{j \\in S} (y_j - y_j)^2 + \\sum_{j \\notin S} (y_j - 0)^2 = \\sum_{j \\notin S} y_j^2\n$$\nTo find the overall minimum objective value, we need to choose a support $S$ with $|S| \\le k$ that minimizes $\\sum_{j \\notin S} y_j^2$. This is equivalent to maximizing the sum of the squared components that are *removed* from the total sum, i.e., maximizing $\\sum_{j \\in S} y_j^2$, since $\\sum_{j \\notin S} y_j^2 = \\sum_{j=1}^{6} y_j^2 - \\sum_{j \\in S} y_j^2$ and $\\sum_{j=1}^{6} y_j^2$ is a constant.\n\nThe task reduces to selecting up to $k=2$ indices from $\\{1, \\dots, 6\\}$ corresponding to the components of $y$ with the largest squared values.\nThe given vector is $y = \\begin{pmatrix} 7 \\\\ -2 \\\\ 3 \\\\ -6 \\\\ 1 \\\\ 4 \\end{pmatrix}$. Let's compute the squares of its components:\n$y_1^2 = 7^2 = 49$\n$y_2^2 = (-2)^2 = 4$\n$y_3^2 = 3^2 = 9$\n$y_4^2 = (-6)^2 = 36$\n$y_5^2 = 1^2 = 1$\n$y_6^2 = 4^2 = 16$\n\nWe want to choose the support $S$ with $|S| \\le 2$ that maximizes $\\sum_{j \\in S} y_j^2$. Since all $y_j^2 \\ge 0$, the maximum will be achieved for a support of size exactly $k=2$. We identify the two largest values among $\\{49, 4, 9, 36, 1, 16\\}$. These are $49$ and $36$, which correspond to indices $j=1$ and $j=4$.\n\nTherefore, the optimal support is $S = \\{1, 4\\}$.\nThe minimal objective value is the sum of the squares of the components *not* in $S$. The indices not in $S$ are $\\{2, 3, 5, 6\\}$.\nThe minimum value is:\n$$\n\\min \\|y - Xx\\|_{2}^{2} = \\sum_{j \\in \\{2,3,5,6\\}} y_j^2 = y_2^2 + y_3^2 + y_5^2 + y_6^2\n$$\nSubstituting the values:\n$$\n(-2)^2 + 3^2 + 1^2 + 4^2 = 4 + 9 + 1 + 16 = 30\n$$\nThe optimal solution vector is $x = \\begin{pmatrix} 7 \\\\ 0 \\\\ 0 \\\\ -6 \\\\ 0 \\\\ 0 \\end{pmatrix}$, which satisfies $\\|x\\|_0 = 2 \\le k$. The corresponding minimum squared error is $30$.", "answer": "$$\\boxed{30}$$", "id": "3463365"}, {"introduction": "Given that direct $\\ell_0$ minimization is computationally infeasible for large problems, a cornerstone of compressed sensing and sparse optimization is to replace the intractable $\\ell_0$ \"norm\" with the convex $\\ell_1$ norm. This leads to a tractable problem that can be solved efficiently. This exercise explores a crucial caveat: the solution to the $\\ell_1$ minimization problem is not always the same as the solution to the original $\\ell_0$ problem. By analyzing a case where they differ, you will gain a deeper appreciation for the theoretical conditions, such as the Restricted Isometry Property (RIP), that guarantee their equivalence [@problem_id:3463377].", "problem": "Consider the linear system given by the matrix $A \\in \\mathbb{R}^{2 \\times 3}$ and the measurement vector $y \\in \\mathbb{R}^{2}$,\n$$\nA \\;=\\; \\begin{pmatrix}\n1 & 0 & \\tfrac{2}{5} \\\\\n0 & 1 & \\tfrac{2}{5}\n\\end{pmatrix},\n\\qquad\ny \\;=\\; \\begin{pmatrix}\n\\tfrac{2}{5} \\\\\n\\tfrac{2}{5}\n\\end{pmatrix}.\n$$\nIn the context of compressed sensing and sparse optimization, recall the core definitions:\n- The $\\ell_0$ 'norm' (cardinality) of a vector $x \\in \\mathbb{R}^{n}$, denoted $\\|x\\|_{0}$, is the number of nonzero entries in $x$.\n- The $\\ell_1$ norm of a vector $x \\in \\mathbb{R}^{n}$, denoted $\\|x\\|_{1}$, is $\\sum_{i=1}^{n} |x_{i}|$.\n- The Restricted Isometry Property (RIP) of order $k$ holds for a matrix $A$ with restricted isometry constant $\\delta_{k} \\in [0,1)$ if for all $k$-sparse vectors $x \\in \\mathbb{R}^{n}$,\n$$\n(1 - \\delta_{k}) \\, \\|x\\|_{2}^{2} \\;\\le\\; \\|A x\\|_{2}^{2} \\;\\le\\; (1 + \\delta_{k}) \\, \\|x\\|_{2}^{2}.\n$$\n\nUsing these definitions as the fundamental base:\n1. Show that the matrix $A$ fails the Restricted Isometry Property of order $3$ by explicitly constructing a nonzero $3$-sparse vector in the null space of $A$ and explaining the implication for the restricted isometry constant.\n2. Solve the $\\ell_0$-minimization problem $\\min \\{\\|x\\|_{0} : A x = y\\}$ and identify a sparsest solution.\n3. Solve the $\\ell_1$-minimization problem $\\min \\{\\|x\\|_{1} : A x = y\\}$, and demonstrate that the minimizer differs from the $\\ell_0$ minimizer.\n4. Compute the minimum value of the $\\ell_1$ objective $\\min \\{\\|x\\|_{1} : A x = y\\}$ exactly. Express your final answer as a single exact number (for example, a reduced fraction). No rounding is required, and no physical units apply.", "solution": "The problem requires a four-part analysis of a sparse optimization problem defined by a matrix $A \\in \\mathbb{R}^{2 \\times 3}$ and a vector $y \\in \\mathbb{R}^{2}$.\n\nFirst, we validate the problem statement.\nThe givens are:\n- $A = \\begin{pmatrix} 1 & 0 & \\tfrac{2}{5} \\\\ 0 & 1 & \\tfrac{2}{5} \\end{pmatrix}$\n- $y = \\begin{pmatrix} \\tfrac{2}{5} \\\\ \\tfrac{2}{5} \\end{pmatrix}$\n- $\\|x\\|_{0}$ is the number of nonzero entries in $x$.\n- $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$.\n- The Restricted Isometry Property (RIP) of order $k$: $(1 - \\delta_{k}) \\, \\|x\\|_{2}^{2} \\le \\|A x\\|_{2}^{2} \\le (1 + \\delta_{k}) \\, \\|x\\|_{2}^{2}$ for all $k$-sparse vectors $x$, with $\\delta_k \\in [0,1)$.\n\nThe problem is scientifically grounded in the field of compressed sensing, is well-posed with all necessary information provided, and is stated objectively. The definitions are standard, and the problem is a valid and illustrative example of concepts in sparse recovery. Therefore, a solution can be derived.\n\nPart 1: Show that the matrix $A$ fails the Restricted Isometry Property of order $3$.\nA vector $x \\in \\mathbb{R}^{3}$ is $3$-sparse if it has at most $3$ nonzero entries. Since any nonzero vector in $\\mathbb{R}^{3}$ satisfies this, we need to find a nonzero vector $x$ for which the RIP inequality is violated. A powerful way to show failure is to find a nonzero $k$-sparse vector in the null space of $A$.\n\nLet $x = (x_1, x_2, x_3)^T$ be a vector in the null space of $A$, meaning $Ax=0$. This gives the system of equations:\n$$\n\\begin{cases}\nx_1 + \\frac{2}{5} x_3 = 0 \\\\\nx_2 + \\frac{2}{5} x_3 = 0\n\\end{cases}\n$$\nFrom this system, we find $x_1 = -\\frac{2}{5} x_3$ and $x_2 = -\\frac{2}{5} x_3$. The null space, $\\ker(A)$, consists of all vectors of the form $x = t(-\\frac{2}{5}, -\\frac{2}{5}, 1)^T$ for any scalar $t \\in \\mathbb{R}$.\n\nLet's choose a nonzero vector from this space, for instance by setting $t=5$. This gives the vector $z = (-2, -2, 5)^T$.\nThis vector $z$ is nonzero, so it is a $3$-sparse vector.\nFor this vector, $Az = 0$, which implies $\\|Az\\|_2^2 = 0$.\nThe squared $\\ell_2$ norm of $z$ is $\\|z\\|_2^2 = (-2)^2 + (-2)^2 + 5^2 = 4 + 4 + 25 = 33$.\nThe RIP inequality for $k=3$ is $(1 - \\delta_3) \\|x\\|_2^2 \\le \\|Ax\\|_2^2$. Substituting our vector $z$:\n$$\n(1 - \\delta_3) \\cdot 33 \\le 0\n$$\nSince $\\|z\\|_2^2 = 33 > 0$, this inequality forces $(1 - \\delta_3) \\le 0$, which implies $\\delta_3 \\ge 1$.\nThe definition of the Restricted Isometry Property requires the restricted isometry constant $\\delta_k$ to be in the interval $[0, 1)$. Since we have shown that for $A$ to satisfy the property for $k=3$ would require $\\delta_3 \\ge 1$, the matrix $A$ fails to satisfy the RIP of order $3$.\n\nPart 2: Solve the $\\ell_{0}$-minimization problem $\\min \\{\\|x\\|_{0} : A x = y\\}$.\nWe are looking for the solution to $Ax=y$ with the minimum number of nonzero entries. A solution $x = (x_1, x_2, x_3)^T$ must satisfy:\n$$\n\\begin{cases}\nx_1 + \\frac{2}{5} x_3 = \\frac{2}{5} \\\\\nx_2 + \\frac{2}{5} x_3 = \\frac{2}{5}\n\\end{cases}\n$$\nWe seek a solution $x$ with the smallest possible $\\|x\\|_0$.\n- Is there a $0$-sparse solution? A $0$-sparse solution is $x = (0, 0, 0)^T$. $A \\cdot 0 = 0 \\neq y$, so no $0$-sparse solution exists.\n- Is there a $1$-sparse solution? We test each possibility.\n  - If $x=(x_1, 0, 0)^T$, the equations become $x_1 = 2/5$ and $0 = 2/5$, a contradiction.\n  - If $x=(0, x_2, 0)^T$, the equations become $0 = 2/5$ and $x_2 = 2/5$, a contradiction.\n  - If $x=(0, 0, x_3)^T$, the equations become $\\frac{2}{5} x_3 = \\frac{2}{5}$ and $\\frac{2}{5} x_3 = \\frac{2}{5}$. Both imply $x_3=1$.\nThus, $x_{s_0} = (0, 0, 1)^T$ is a solution to $Ax=y$. The sparsity of this solution is $\\|x_{s_0}\\|_0 = 1$.\nSince we have found a $1$-sparse solution and no $0$-sparse solution exists, the minimum value of $\\|x\\|_0$ is $1$, and a sparsest solution is $x_{s_0} = (0, 0, 1)^T$.\n\nPart 3: Solve the $\\ell_{1}$-minimization problem $\\min \\{\\|x\\|_{1} : A x = y\\}$ and show the minimizer differs.\nThe set of all solutions to $Ax=y$ can be characterized as $x = x_p + x_h$, where $x_p$ is any particular solution and $x_h$ is any vector in the null space of $A$.\nFrom Part 2, we have a particular solution $x_p = (0, 0, 1)^T$.\nFrom Part 1, the null space consists of vectors $x_h = t(-\\frac{2}{5}, -\\frac{2}{5}, 1)^T$ for $t \\in \\mathbb{R}$.\nSo, any solution is of the form:\n$$\nx(t) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} + t \\begin{pmatrix} -2/5 \\\\ -2/5 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -2/5 t \\\\ -2/5 t \\\\ 1+t \\end{pmatrix}\n$$\nWe want to minimize the $\\ell_1$ norm of $x(t)$:\n$$\n\\|x(t)\\|_1 = |-\\tfrac{2}{5}t| + |-\\tfrac{2}{5}t| + |1+t| = 2 \\cdot \\tfrac{2}{5}|t| + |1+t| = \\tfrac{4}{5}|t| + |1+t|\n$$\nLet $f(t) = \\frac{4}{5}|t| + |1+t|$. This function is convex and piecewise linear, with points of non-differentiability at $t=-1$ and $t=0$. We analyze it in intervals:\n- For $t \\ge 0$: $f(t) = \\frac{4}{5}t + (1+t) = \\frac{9}{5}t + 1$. This is an increasing function. Its minimum on $[0, \\infty)$ is at $t=0$, where $f(0)=1$.\n- For $-1 \\le t < 0$: $f(t) = \\frac{4}{5}(-t) + (1+t) = -\\frac{4}{5}t + 1+t = \\frac{1}{5}t + 1$. This is an increasing function. Its minimum on $[-1, 0)$ is at $t=-1$, where $f(-1) = \\frac{1}{5}(-1) + 1 = \\frac{4}{5}$.\n- For $t < -1$: $f(t) = \\frac{4}{5}(-t) - (1+t) = -\\frac{4}{5}t - 1 - t = -\\frac{9}{5}t - 1$. This is a decreasing function.\nThe global minimum of $f(t)$ occurs at $t=-1$.\nThe $\\ell_1$-minimizing solution, $x_{s_1}$, is obtained by setting $t=-1$:\n$$\nx_{s_1} = x(-1) = \\begin{pmatrix} -2/5 (-1) \\\\ -2/5 (-1) \\\\ 1+(-1) \\end{pmatrix} = \\begin{pmatrix} 2/5 \\\\ 2/5 \\\\ 0 \\end{pmatrix}\n$$\nComparing the minimizers:\nThe sparsest solution (from $\\ell_0$-minimization) is $x_{s_0} = (0, 0, 1)^T$.\nThe $\\ell_1$-minimizing solution is $x_{s_1} = (2/5, 2/5, 0)^T$.\nClearly, $x_{s_0} \\neq x_{s_1}$. This demonstrates that for this system, $\\ell_1$-minimization does not recover the sparsest solution. The sparsity of the $\\ell_1$ solution is $\\|x_{s_1}\\|_0 = 2$, which is greater than the minimal sparsity of $1$.\n\nPart 4: Compute the minimum value of the $\\ell_{1}$ objective.\nAs calculated in Part 3, the minimum value of the objective function $f(t) = \\|x(t)\\|_1$ is achieved at $t=-1$. The minimum value is:\n$$\nf(-1) = \\tfrac{4}{5}|-1| + |1+(-1)| = \\tfrac{4}{5}(1) + |0| = \\tfrac{4}{5}\n$$\nThus, the minimum value of $\\|x\\|_1$ for any $x$ satisfying $Ax=y$ is exactly $\\frac{4}{5}$.", "answer": "$$\\boxed{\\frac{4}{5}}$$", "id": "3463377"}, {"introduction": "The final step in understanding the difficulty of $\\ell_0$ minimization is to engage with the formal proof of its NP-hardness. Such proofs are typically constructed via a polynomial-time reduction, a method for showing that a problem is at least as hard as another problem already known to be NP-hard. This exercise challenges you to analyze the fundamental principles of a reduction from the classic NP-complete problem, Exact Cover by 3-Sets, to the sparse feasibility problem. By evaluating different encoding strategies, you will learn how the discrete, combinatorial nature of a problem can be translated into the language of sparse linear algebra [@problem_id:3463372].", "problem": "Consider the decision version of the sparse feasibility problem: given a matrix of rational coefficients $A \\in \\mathbb{Q}^{m \\times n}$, a target vector $b \\in \\mathbb{Q}^{m}$, and an integer budget $k \\in \\mathbb{N}$, decide whether there exists a vector $x \\in \\mathbb{R}^{n}$ satisfying $A x = b$ and $\\|x\\|_{0} \\leq k$, where $\\|x\\|_{0}$ denotes the number of nonzero entries of $x$. It is known that deciding the existence of an exact set cover of cardinality $k$ in an instance $(U, \\mathcal{S}, k)$ of Exact Cover by $3$-Sets (each $S \\in \\mathcal{S}$ has $|S| = 3$ and each $u \\in U$ must be covered by exactly one selected set) is complete for Nondeterministic Polynomial time (NP). The goal is to construct, in polynomial time with respect to $|U| + |\\mathcal{S}|$, a matrix $A$ and a vector $b$ with rational coefficients that encode the exact cover instance so that supports of solutions $x$ to $A x = b$ of cardinality at most $k$ correspond bijectively to feasible exact covers $C \\subseteq \\mathcal{S}$ of cardinality $k$.\n\nYour task is to identify which construction principle correctly achieves such a bijection based on first principles. In particular, the encoding must satisfy the following two properties:\n\n- Soundness: If $C \\subseteq \\mathcal{S}$ is a feasible exact cover of cardinality $k$, then there exists $x \\in \\mathbb{R}^{n}$ with $\\|x\\|_{0} = k$ and $A x = b$ whose support indexes exactly the sets in $C$.\n- Completeness: If there exists $x \\in \\mathbb{R}^{n}$ with $A x = b$ and $\\|x\\|_{0} \\leq k$, then the support of $x$ indexes a family $C \\subseteq \\mathcal{S}$ that is a feasible exact cover of cardinality at most $k$; furthermore, at optimal sparsity, each nonzero entry of $x$ equals $1$.\n\nEvery column of $A$ is allowed to be designed as a gadget that represents one discrete choice, and rows of $A$ can be grouped into blocks that enforce different aspects of the discrete constraints. Coefficients must be rational, and all constructions must be implementable in time polynomial in $|U| + |\\mathcal{S}|$ with polynomial bit-length numbers. Begin from fundamental definitions: support of a vector, polynomial-time reduction, linear equations, and incidence constraints. Avoid relying on any unstated specialized theorem beyond linear algebra over $\\mathbb{Q}$ and basic properties of sparsity.\n\nWhich option below describes a correct and complete encoding principle achieving the required bijection?\n\nA. Build a block-structured matrix $A$ with two blocks $A^{\\mathrm{inc}} \\in \\mathbb{Q}^{|U| \\times |\\mathcal{S}|}$ and $A^{\\mathrm{lock}} \\in \\mathbb{Q}^{2 \\times |\\mathcal{S}|}$ and set $A = \\begin{bmatrix} A^{\\mathrm{inc}} \\\\ A^{\\mathrm{lock}} \\end{bmatrix}$. Let $A^{\\mathrm{inc}}$ be the incidence matrix: for each element $u \\in U$ and set $S \\in \\mathcal{S}$, set $(A^{\\mathrm{inc}})_{u,S} = 1$ if $u \\in S$, and $(A^{\\mathrm{inc}})_{u,S} = 0$ otherwise. Let $b^{\\mathrm{inc}} = \\mathbf{1} \\in \\mathbb{Q}^{|U|}$ so that each element must be covered exactly once. In $A^{\\mathrm{lock}}$, use two rational rows to enforce unit usage and a fixed budget: set the first row to all ones, i.e., $(A^{\\mathrm{lock}})_{1,S} = 1$ for all $S$, and the second row to a rational geometric code $(A^{\\mathrm{lock}})_{2,S} = \\beta^{\\mathrm{index}(S)}$ with $\\beta \\in \\mathbb{Q}$ chosen as $\\beta = 2$ and $\\mathrm{index}(S) \\in \\{1,2,\\dots,|\\mathcal{S}|\\}$. Set $b^{\\mathrm{lock}} = \\begin{bmatrix} k \\\\ T \\end{bmatrix}$, where $T$ is the sum of the second-row entries over any feasible exact cover. This enforces that any solution with $\\|x\\|_{0} \\leq k$ has nonzeros equal to $1$ and uniquely identifies the chosen sets via the geometric code, ensuring a bijection between supports and feasible exact covers.\n\nB. Use random real coefficients in the rows of $A$ drawn independently from a continuous distribution on $\\mathbb{R}$ so that, with probability $1$, distinct subsets of columns have distinct sums. Set $b$ equal to the sum over the unknown chosen subset and argue that the resulting random system almost surely has a unique representation; thus, supports of solutions correspond bijectively to feasible solutions. Since the randomness almost surely avoids collisions, this yields the desired polynomial-time reduction.\n\nC. Replace the discrete equalities by inequalities $A x \\leq b$ with $A \\in \\mathbb{Q}^{m \\times n}$ and $b \\in \\mathbb{Q}^{m}$, and invoke CarathÃ©odory's theorem to argue that any minimal support solution lies at a vertex of the corresponding polyhedron. Therefore, by choosing the polyhedron appropriately, the vertices correspond bijectively to feasible discrete solutions, and the supports of minimal $\\ell_0$ solutions identify the discrete solutions.\n\nD. Introduce, for each decision variable (each set $S \\in \\mathcal{S}$), two auxiliary columns $c_{S}^{\\mathrm{on}}$ and $c_{S}^{\\mathrm{off}}$, and add one rational equation per decision enforcing $x_{S}^{\\mathrm{on}} + x_{S}^{\\mathrm{off}} = 1$. Use the incidence constraints with right-hand side equal to $\\mathbf{1}$ to require that each element is covered exactly once, and minimize $\\|x\\|_{0}$ subject to these equalities. Argue that any solution with minimal $\\|x\\|_{0}$ will set exactly one of $x_{S}^{\\mathrm{on}}$ or $x_{S}^{\\mathrm{off}}$ to $1$ (and the other to $0$), and that the incidence constraints then force the selected sets to form a feasible exact cover; hence supports of minimizers correspond bijectively to feasible exact covers of the intended cardinality.\n\nSelect all correct options. Provide your reasoning starting from definitions of support, polynomial-time reduction, and linear encoding of constraints, and then justify soundness and completeness of the claimed bijection in the chosen option(s).", "solution": "The user wants me to validate the problem statement and then identify the correct principle for an NP-hardness reduction.\n\n### Problem Validation\n\nFirst, I will validate the problem statement according to the specified criteria.\n\n**Step 1: Extract Givens**\n-   **Target Problem:** Sparse feasibility. Given $A \\in \\mathbb{Q}^{m \\times n}$, $b \\in \\mathbb{Q}^{m}$, and $k_{budget} \\in \\mathbb{N}$, decide if $\\exists x \\in \\mathbb{R}^{n}$ with $A x = b$ and $\\|x\\|_{0} \\leq k_{budget}$.\n-   **Source Problem:** Exact Cover by $3$-Sets (X3C). Given a universe $U$, a collection of $3$-element subsets $\\mathcal{S} \\subseteq \\mathcal{P}(U)$, and an integer $k_{cover} \\in \\mathbb{N}$, decide if there exists a subcollection $C \\subseteq \\mathcal{S}$ such that $|C|=k_{cover}$ and $C$ is an exact cover of $U$ (i.e., every element of $U$ is in exactly one set in $C$). This problem is NP-complete. Note that for an exact cover to exist, we must have $|U| = 3k_{cover}$.\n-   **Task:** Identify a correct principle to construct, in polynomial time, an instance $(A, b)$ from an X3C instance $(U, \\mathcal{S}, k_{cover})$ that yields a bijection between feasible exact covers of cardinality $k_{cover}$ and supports of solutions to $A x = b$ with sparsity at most some $k_{budget}$.\n-   **Required Properties:**\n    1.  **Soundness:** A feasible exact cover of size $k_{cover}$ implies the existence of a solution $x$ to $Ax=b$ with $\\|x\\|_0 = k_{budget}$ (for some appropriately chosen $k_{budget}$) whose support encodes the cover.\n    2.  **Completeness:** A solution $x$ to $Ax=b$ with $\\|x\\|_0 \\leq k_{budget}$ implies its support encodes an exact cover of cardinality at most $k_{cover}$, and at optimal sparsity, all nonzero entries of $x$ are $1$.\n-   **Constraints:** Coefficients in $A$ and $b$ must be rational. Construction must be polynomial in $|U|+|\\mathcal{S}|$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientific Groundedness:** The problem is rooted in computational complexity theory, specifically the concept of NP-hardness proofs via polynomial-time reduction. The source and target problems are standard, well-defined computational problems. The concepts of linear algebra and sparsity are rigorously defined. The problem is scientifically sound.\n-   **Well-Posedness & Objectivity:** The goal is clearly stated: identify a correct reduction principle among the given options. The criteria for correctness (soundness, completeness, poly-time construction, rational coefficients) are objective and precise. While the problem uses the symbol $k$ for both the cover cardinality and the sparsity budget, this is a common notational shortcut in the field, with the meaning being distinguishable from context. The problem is sufficiently well-posed and objective to admit a rigorous analysis.\n-   **Consistency and Completeness:** The setup is self-contained, providing the definitions of the source and target problems and the required properties of the reduction. There are no internal contradictions.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will proceed to analyze the options.\n\n### Solution Derivation\n\nThe task is to find a correct polynomial-time reduction from Exact Cover by $3$-Sets (X3C) to the sparse feasibility problem. A reduction maps an instance of X3C, $(U, \\mathcal{S}, k_{cover})$, to an instance of sparse feasibility, $(A, b, k_{budget})$. The reduction is correct if the X3C instance has a solution if and only if the sparse feasibility instance does. A crucial aspect of a reduction is that the target instance $(A, b, k_{budget})$ must be constructed using only the information from the source instance $(U, \\mathcal{S}, k_{cover})$, without knowledge of its solution.\n\nLet $n = |\\mathcal{S}|$ be the number of available sets. We will analyze each option.\n\n**Analysis of Option A**\n\nThis option proposes a construction of a matrix $A \\in \\mathbb{Q}^{(|U|+2) \\times n}$ where each column corresponds to a set $S \\in \\mathcal{S}$. The system of equations $Ax=b$ is intended to enforce the X3C constraints.\nThe first block of equations, $A^{\\mathrm{inc}}x = \\mathbf{1}$, correctly models the constraint that each element $u \\in U$ must be covered exactly once, assuming $x_S \\in \\{0, 1\\}$.\nThe first row of the $A^{\\mathrm{lock}}$ block, $\\mathbf{1}^T x = k_{cover}$, correctly enforces the cover cardinality, again assuming $x_S \\in \\{0, 1\\}$.\nThe critical flaw lies in the second part of the locking mechanism. It defines the right-hand side $b$ using a value $T$ described as \"the sum of the second-row entries over any feasible exact cover.\" Let the indices of the sets in $\\mathcal{S}$ be $\\{1, 2, \\dots, n\\}$. The second locking row has entries $2^j$. If $C_1$ and $C_2$ are two distinct feasible exact covers, the corresponding sums $T_1 = \\sum_{S_j \\in C_1} 2^j$ and $T_2 = \\sum_{S_j \\in C_2} 2^j$ will be different due to the uniqueness of binary representations. An X3C instance can have multiple solutions. For example, for $U=\\{1,2,3,4,5,6\\}$ and $\\mathcal{S}=\\{\\{1,2,3\\}, \\{4,5,6\\}, \\{1,4,5\\}, \\{2,3,6\\}\\}$, both $\\{\\{1,2,3\\}, \\{4,5,6\\}\\}$ and $\\{\\{1,4,5\\}, \\{2,3,6\\}\\}$ are exact covers. These two covers would yield different values for $T$. Because the value of $T$ (and thus the vector $b$) depends on knowing a specific solution to the X3C problem, the construction is not a valid reduction. A reduction must be able to construct its output instance without solving the input instance.\n\nVerdict on A: **Incorrect**.\n\n**Analysis of Option B**\n\nThis option has several fundamental flaws:\n$1$. **Rationality Violation:** The problem explicitly requires $A \\in \\mathbb{Q}^{m \\times n}$. This option proposes using coefficients from a continuous distribution on $\\mathbb{R}$, which will be irrational with probability $1$.\n$2$. **Computability Violation:** A polynomial-time reduction must be implementable on a standard model of computation like a Turing machine. Drawing random real numbers is not a primitive operation in this model.\n$3. $ **Deterministic Proof Required:** NP-hardness is typically proven via deterministic polynomial-time reductions. While probabilistic reductions exist, they define different complexity classes and relationships. The problem asks for a construction, implying a deterministic procedure.\n$4$. **Circular Dependency:** Similar to option A, it suggests setting \"$b$ equal to the sum over the unknown chosen subset,\" which means the construction of $b$ depends on the solution, which is not permitted.\n\nVerdict on B: **Incorrect**.\n\n**Analysis of Option C**\n\nThis option proposes changing the problem formulation from a system of equalities $Ax=b$ to a system of inequalities $Ax \\leq b$. The target problem specified in the prompt is the sparse feasibility problem for linear *equalities*. This option attempts to reduce X3C to a different problem, making it an invalid answer to the question asked. Furthermore, the argument provided is non-constructive and vague (\"by choosing the polyhedron appropriately\"). It does not specify the actual matrix $A$ and vector $b$ for the reduction.\n\nVerdict on C: **Incorrect**.\n\n**Analysis of Option D**\n\nThis option describes a valid and standard technique. Let's analyze it in detail for an X3C instance $(U, \\mathcal{S})$. Since each set in $\\mathcal{S}$ has size $3$, any exact cover must have cardinality $k_{cover} = |U|/3$. If the input $k_{cover}$ is different, the instance is trivially unsatisfiable. We can assume $k_{cover} = |U|/3$.\n\n**Construction:**\nFor each set $S_i \\in \\mathcal{S}$ (for $i \\in \\{1, \\dots, n\\}$ where $n=|\\mathcal{S}|$), we introduce two variables, $y_i$ (representing $x_S^{\\mathrm{on}}$) and $z_i$ (representing $x_S^{\\mathrm{off}}$). The total solution vector is $x = [y_1, \\dots, y_n, z_1, \\dots, z_n]^T \\in \\mathbb{R}^{2n}$.\nThe linear system $Ax=b$ is defined by two sets of equations:\n$1$. For each $i \\in \\{1, \\dots, n\\}$: $y_i + z_i = 1$. This block consists of $n$ equations.\n$2$. For each element $u \\in U$: $\\sum_{i: u \\in S_i} y_i = 1$. This block consists of $|U|$ equations.\n\nThe matrix $A$ and vector $b$ are constructed from these equations. All coefficients are $0$ or $1$, so they are rational. The construction time is polynomial in $|U|$ and $n=|\\mathcal{S}|$.\nThe reduction then proposes to check for the existence of a solution $x$ with sparsity $\\|x\\|_0 \\leq k_{budget}$ where we set the budget $k_{budget}=n=|\\mathcal{S}|$.\n\n**Soundness:**\nAssume there exists an exact cover $C \\subseteq \\mathcal{S}$. We must construct a solution $x$ to $Ax=b$ with $\\|x\\|_0 \\leq n$. Let's define:\n-   $y_i = 1$ if $S_i \\in C$, and $y_i=0$ if $S_i \\notin C$.\n-   $z_i = 1 - y_i$. So, $z_i=0$ if $S_i \\in C$, and $z_i=1$ if $S_i \\notin C$.\nThis vector $x = (y_1, \\dots, y_n, z_1, \\dots, z_n)$ satisfies the system $Ax=b$:\n-   $y_i + z_i = 1$ is satisfied by definition.\n-   Since $C$ is an exact cover, for each $u \\in U$, there is exactly one $S_i \\in C$ that contains $u$. Thus, $\\sum_{i:u \\in S_i} y_i=1$.\nNow we check the sparsity: For each $i \\in \\{1, \\dots, n\\}$, exactly one of $\\{y_i, z_i\\}$ is $1$ and the other is $0$. Therefore, there is exactly one nonzero entry for each pair of variables $(y_i, z_i)$. The total number of nonzero entries is $\\|x\\|_0 = n$. This satisfies the condition $\\|x\\|_0 \\leq k_{budget}=n$.\nThus, the construction is sound.\n\n**Completeness:**\nAssume there exists a solution $x=(y, z)$ such that $Ax=b$ and $\\|x\\|_0 \\leq n=|\\mathcal{S}|$. We must show that this implies the existence of an exact cover.\nFor each $i \\in \\{1, \\dots, n\\}$, the equation $y_i + z_i = 1$ implies that at least one of $y_i$ or $z_i$ must be nonzero. Summing over all $i$, the total number of nonzero entries must be at least $n$.\n$$ \\|x\\|_0 = \\sum_{i=1}^n (\\|y_i\\|_0 + \\|z_i\\|_0) \\geq n $$\nWe are given $\\|x\\|_0 \\leq n$. Combining these, we must have $\\|x\\|_0 = n$.\nThis equality holds if and only if for each $i$, exactly one of $y_i$ or $z_i$ is nonzero. Let's say for a given $i$, $y_i \\neq 0$ and $z_i = 0$. Then the constraint $y_i + z_i = 1$ implies $y_i=1$. Similarly, if $y_i=0$ and $z_i \\neq 0$, then $z_i=1$.\nTherefore, any solution with sparsity at most $n$ must have a sparsity of exactly $n$, and its components must satisfy $(y_i, z_i) \\in \\{(1,0), (0,1)\\}$ for all $i \\in \\{1, \\dots, n\\}$.\nThis forces all $y_i$ to be either $0$ or $1$.\nNow consider the second block of equations: $\\sum_{i: u \\in S_i} y_i = 1$. Since $y_i \\in \\{0, 1\\}$, this equation means that for each element $u \\in U$, it is contained in exactly one set $S_i$ for which $y_i=1$.\nLet $C = \\{ S_i \\in \\mathcal{S} \\mid y_i=1 \\}$. The set $C$ is therefore an exact cover of $U$.\nThe cardinality of this cover is $\\sum y_i$. Summing the incidence equations over all $u \\in U$ yields $\\sum_{u \\in U} \\sum_{i: u \\in S_i} y_i = \\sum_{u \\in U} 1 = |U|$. By swapping the order of summation, $\\sum_{i=1}^n y_i |S_i| = |U|$. Since $|S_i|=3$ for X3C, we have $3 \\sum_{i=1}^n y_i = |U|$, which implies that the cardinality of the cover $C$ is $|C|=\\sum y_i = |U|/3$. This matches the required cardinality $k_{cover}$.\nThe construction correctly establishes the required equivalence. The nonzero entries of the optimal solution are all $1$. The support of the $y$ part of the solution vector bijectively corresponds to the sets in the exact cover.\n\nVerdict on D: **Correct**.", "answer": "$$\\boxed{D}$$", "id": "3463372"}]}