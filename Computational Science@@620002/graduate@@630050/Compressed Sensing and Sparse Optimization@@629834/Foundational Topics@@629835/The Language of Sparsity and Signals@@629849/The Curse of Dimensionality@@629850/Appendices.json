{"hands_on_practices": [{"introduction": "This exercise establishes a foundational limit in compressed sensing by exploring the Null Space Property (NSP). By using first principles of linear algebra, you will construct a scenario where signal recovery is guaranteed to fail if the number of measurements $m$ is too small relative to the sparsity level $k$ [@problem_id:3486783]. This demonstrates that the curse of dimensionality imposes a hard, non-negotiable constraint, providing a crucial \"impossibility\" result that motivates the need for more sophisticated analysis.", "problem": "Let $d$, $m$, and $k$ be positive integers with $d \\geq 2k$. Recall that a matrix $A \\in \\mathbb{R}^{m \\times d}$ satisfies the Null Space Property (NSP) of order $k$ if for every nonzero vector $h \\in \\ker(A)$ and every index set $S \\subset \\{1,2,\\dots,d\\}$ with $|S| \\leq k$, one has $\\|h_S\\|_1  \\|h_{S^c}\\|_1$. The NSP is a necessary and sufficient condition for exact recovery of all $k$-sparse signals via $\\ell_1$-minimization.\n\nUsing only fundamental linear algebra and norm inequalities, do the following:\n\n1. Construct, for the regime $m  2k$, an explicit $A \\in \\mathbb{R}^{m \\times d}$ and a nonzero $h \\in \\ker(A)$ together with a set $S \\subset \\{1,\\dots,d\\}$ of size $|S|=k$ such that $\\|h_S\\|_1 \\ge \\|h_{S^c}\\|_1$, thereby demonstrating failure of the NSP of order $k$. Your construction should rely only on the fact that any collection of more than $m$ vectors in $\\mathbb{R}^{m}$ is linearly dependent, and on the observation that for a vector supported on at most $2k$ indices, the sum of the largest $k$ absolute entries is at least half of the $\\ell_1$ norm. Provide geometric intuition in terms of the dimension of the null space and its intersection with coordinate subspaces, illustrating a manifestation of the curse of dimensionality when $m$ is too small relative to $k$ and $d$.\n\n2. From your construction and reasoning, determine the smallest integer $m_{\\min}(k)$ such that when $m  m_{\\min}(k)$, NSP of order $k$ must fail for every $A \\in \\mathbb{R}^{m \\times d}$ with $d \\geq 2k$, while when $m \\geq m_{\\min}(k)$, this unavoidable obstruction is no longer enforced by mere dimension and linear dependence counting. Provide your final answer as a closed-form expression in $k$. Do not include any units. If you choose to present a numerical value for a particular $k$, do not round; however, a general closed-form expression in $k$ is required as the final answer.", "solution": "`...`", "answer": "$$\n\\boxed{2k}\n$$", "id": "3486783"}, {"introduction": "Building upon the idea of fundamental limits, this practice delves into the powerful geometric tools that precisely characterize recovery performance for random measurements. You will derive the asymptotic behavior of the statistical dimension of the $\\ell_1$ norm's descent cone, a key quantity that governs the phase transition for successful recovery [@problem_id:3486664]. This analytical exercise reveals the celebrated $m \\approx C k \\ln(n/k)$ scaling, offering a profound insight into how the ambient dimension $n$ and sparsity $k$ interact in high-dimensional space.", "problem": "Consider a fixed vector $x_{0} \\in \\mathbb{R}^{n}$ that is $k$-sparse with support $S \\subset \\{1,\\dots,n\\}$ and $|S|=k$, and consider linear measurements $y = A x_{0}$ with a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ whose entries are independent and identically distributed (i.i.d.) standard normal random variables. Let $\\|\\cdot\\|_{1}$ denote the $\\ell_{1}$ norm on $\\mathbb{R}^{n}$. Define the descent cone of $\\|\\cdot\\|_{1}$ at $x_{0}$ by\n$$\n\\mathcal{D} := \\left\\{ d \\in \\mathbb{R}^{n} : \\exists\\, t  0 \\text{ such that } \\|x_{0} + t d\\|_{1} \\le \\|x_{0}\\|_{1} \\right\\}.\n$$\nLet the statistical dimension of a closed convex cone $\\mathcal{C} \\subset \\mathbb{R}^{n}$ be defined by\n$$\n\\delta(\\mathcal{C}) := \\mathbb{E}\\left[\\|\\Pi_{\\mathcal{C}}(g)\\|_{2}^{2}\\right],\n$$\nwhere $\\Pi_{\\mathcal{C}}$ is the Euclidean projection onto $\\mathcal{C}$ and $g \\sim \\mathcal{N}(0, I_{n})$ is a standard normal vector in $\\mathbb{R}^{n}$. It is a well-tested fact from conic integral geometry that for convex recovery via $\\ell_{1}$ minimization, there is a sharp success/failure threshold that occurs near $m \\approx \\delta(\\mathcal{D})$.\n\nStarting from first principles and core definitions, perform the following steps:\n\n1. Express the normal cone $\\mathcal{N}$ of $\\|\\cdot\\|_{1}$ at $x_{0}$ in terms of the subdifferential $\\partial \\|\\cdot\\|_{1}(x_{0})$, and use the relationship between a cone and its polar cone to rewrite $\\delta(\\mathcal{D})$ in terms of the expected squared distance of a standard normal vector to a scaled version of $\\partial \\|\\cdot\\|_{1}(x_{0})$.\n\n2. Using only basic properties of the standard normal distribution, derive a one-parameter variational representation for $\\delta(\\mathcal{D})$ of the form\n$$\n\\delta(\\mathcal{D}) = \\inf_{\\tau \\ge 0} \\Psi_{n,k}(\\tau),\n$$\nwhere $\\Psi_{n,k}(\\tau)$ is an explicit expression involving the standard normal probability density function and tail distribution.\n\n3. Analyze the asymptotic regime $n \\to \\infty$, $k \\to \\infty$, with $k/n \\to 0$. Using Laplace-type asymptotics for Gaussian tails, determine the leading-order behavior of $\\delta(\\mathcal{D})$ as an explicit function of $n$ and $k$.\n\nYour final answer must be the leading-order asymptotic term for $\\delta(\\mathcal{D})$ in the specified regime, given as a single closed-form analytic expression in terms of $n$ and $k$. Do not include lower-order terms. No rounding is required. Express all logarithms using the natural logarithm $\\ln(\\cdot)$.", "solution": "The problem asks for the leading-order asymptotic behavior of the statistical dimension $\\delta(\\mathcal{D})$ of the descent cone $\\mathcal{D}$ associated with the $\\ell_1$-norm at a $k$-sparse vector $x_0 \\in \\mathbb{R}^n$.\n\n### Part 1: Relating Statistical Dimension to the Normal Cone\n\nLet $f(x) = \\|x\\|_1$. The descent cone of $f$ at $x_0$ is given by the set of directions $d$ for which the directional derivative is non-positive:\n$$\n\\mathcal{D} = \\{d \\in \\mathbb{R}^n : f'(x_0; d) \\le 0\\}\n$$\nThe directional derivative of a convex function is the support function of its subdifferential: $f'(x_0; d) = \\max_{z \\in \\partial f(x_0)} \\langle z, d \\rangle$. Thus,\n$$\n\\mathcal{D} = \\{d \\in \\mathbb{R}^n : \\langle z, d \\rangle \\le 0 \\text{ for all } z \\in \\partial f(x_0)\\}\n$$\nLet $K = \\partial \\|x_0\\|_1$ be the subdifferential of the $\\ell_1$-norm at $x_0$. The normal cone to the sublevel set $\\{x : \\|x\\|_1 \\le \\|x_0\\|_1\\}$ at $x_0$ is the conic hull of the subdifferential, $\\mathcal{N} = \\text{cone}(K)$.\nThe set $\\mathcal{D}$ is, by its definition above, the polar cone of the set $K$. Since $K$ itself is not necessarily a cone, we consider its conic hull $\\mathcal{N}$. The polar cone of $\\mathcal{N}$ is:\n$$\n\\mathcal{N}^* = \\{ d \\in \\mathbb{R}^n : \\langle v, d \\rangle \\le 0 \\text{ for all } v \\in \\mathcal{N} \\}\n$$\nSince any $v \\in \\mathcal{N}$ can be written as $v = \\lambda z$ for some $\\lambda \\ge 0$ and $z \\in K$, the condition $\\langle v, d \\rangle \\le 0$ for all $v \\in \\mathcal{N}$ is equivalent to $\\langle z, d \\rangle \\le 0$ for all $z \\in K$. Therefore, $\\mathcal{D} = \\mathcal{N}^*$.\n\nA key identity relates the statistical dimension of a closed convex cone $\\mathcal{C}$ to its polar cone $\\mathcal{C}^*$:\n$$\n\\delta(\\mathcal{C}) = \\mathbb{E}\\left[ \\text{dist}(g, \\mathcal{C}^*)^2 \\right]\n$$\nwhere $g \\sim \\mathcal{N}(0, I_n)$ and $\\text{dist}(g, A) = \\inf_{a \\in A} \\|g - a\\|_2$ is the Euclidean distance from a point $g$ to a set $A$. This identity follows from Moreau's decomposition theorem, $g = \\Pi_{\\mathcal{C}}(g) + \\Pi_{\\mathcal{C}^*}(g)$, which implies $\\|\\Pi_{\\mathcal{C}}(g)\\|_2 = \\|g - \\Pi_{\\mathcal{C}^*}(g)\\|_2 = \\text{dist}(g, \\mathcal{C}^*)$.\n\nApplying this identity to our descent cone $\\mathcal{D}$, we use the fact that $\\mathcal{D}^* = (\\mathcal{N}^*)^* = \\mathcal{N}$ because $\\mathcal{N}$ is a closed convex cone.\n$$\n\\delta(\\mathcal{D}) = \\mathbb{E}\\left[ \\text{dist}(g, \\mathcal{D}^*)^2 \\right] = \\mathbb{E}\\left[ \\text{dist}(g, \\mathcal{N})^2 \\right]\n$$\nThe normal cone is $\\mathcal{N} = \\text{cone}(K) = \\{\\lambda z : \\lambda \\ge 0, z \\in K\\}$. The squared distance of $g$ to $\\mathcal{N}$ is\n$$\n\\text{dist}(g, \\mathcal{N})^2 = \\min_{v \\in \\mathcal{N}} \\|g - v\\|_2^2 = \\min_{\\lambda \\ge 0, z \\in K} \\|g - \\lambda z\\|_2^2 = \\min_{\\lambda \\ge 0} \\min_{z \\in K} \\|g - \\lambda z\\|_2^2 = \\min_{\\lambda \\ge 0} \\text{dist}(g, \\lambda K)^2\n$$\nHere, $\\lambda K$ is a \"scaled version\" of $\\partial \\|x_0\\|_1 = K$. So we have expressed $\\delta(\\mathcal{D})$ in terms of the expectation of a quantity involving the distance to a scaled version of the subdifferential.\n\n### Part 2: Variational Representation\n\nA powerful result from the theory of Gaussian processes (Gordon's inequality, or related principles) allows for the interchange of expectation and minimization in this context.\n$$\n\\delta(\\mathcal{D}) = \\mathbb{E}\\left[ \\min_{\\tau \\ge 0} \\text{dist}(g, \\tau K)^2 \\right] = \\min_{\\tau \\ge 0} \\mathbb{E}\\left[ \\text{dist}(g, \\tau K)^2 \\right]\n$$\nWe define $\\Psi_{n,k}(\\tau) := \\mathbb{E}\\left[\\text{dist}(g, \\tau K)^2\\right]$ and find an explicit expression for it.\nThe subdifferential $K = \\partial \\|x_0\\|_1$ is given by $K = \\{ z \\in \\mathbb{R}^n : z_S = s, \\|z_{S^c}\\|_\\infty \\le 1 \\}$, where $S$ is the support of $x_0$ with $|S|=k$, $S^c$ is its complement, and $s = \\text{sign}((x_0)_S)$.\nThe squared distance is\n$$\n\\text{dist}(g, \\tau K)^2 = \\min_{z \\in K} \\|g - \\tau z\\|_2^2 = \\|g_S - \\tau s\\|_2^2 + \\min_{\\|z_{S^c}\\|_\\infty \\le 1} \\|g_{S^c} - \\tau z_{S^c}\\|_2^2\n$$\nThe minimization over $z_{S^c}$ separates coordinate-wise. For each $i \\in S^c$, we minimize $(g_i - \\tau z_i)^2$ subject to $|z_i| \\le 1$. This is equivalent to minimizing $(g_i - w_i)^2$ subject to $w_i \\in [-\\tau, \\tau]$. The minimum is achieved by projecting $g_i$ onto the interval $[-\\tau, \\tau]$, and the squared distance is $\\text{dist}(g_i, [-\\tau, \\tau])^2 = (\\max(0, |g_i|-\\tau))^2 = (|g_i|-\\tau)_+^2$.\nSo, $\\text{dist}(g, \\tau K)^2 = \\|g_S - \\tau s\\|_2^2 + \\sum_{i \\in S^c} (|g_i|-\\tau)_+^2$.\n\nWe now take the expectation over $g \\sim \\mathcal{N}(0, I_n)$.\nThe first term: $\\mathbb{E}[\\|g_S - \\tau s\\|_2^2] = \\sum_{i \\in S} \\mathbb{E}[(g_i - \\tau s_i)^2]$. Since $s_i^2 = 1$ and $g_i \\sim \\mathcal{N}(0,1)$ are independent with $\\mathbb{E}[g_i]=0, \\mathbb{E}[g_i^2]=1$, we have $\\mathbb{E}[(g_i - \\tau s_i)^2] = \\mathbb{E}[g_i^2] - 2\\tau s_i \\mathbb{E}[g_i] + \\tau^2 s_i^2 = 1 - 0 + \\tau^2 = 1+\\tau^2$. Summing over $k$ indices in $S$ gives $k(1+\\tau^2)$.\n\nThe second term: $\\sum_{i \\in S^c} \\mathbb{E}[(|g_i|-\\tau)_+^2] = (n-k)\\mathbb{E}[(|Z|-\\tau)_+^2]$ for $Z \\sim \\mathcal{N}(0,1)$.\nLet $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-z^2/2)$ be the standard normal PDF and $Q(\\tau) = \\int_\\tau^\\infty \\phi(z)dz$ be the tail probability.\n$$\n\\mathbb{E}[(|Z|-\\tau)_+^2] = \\int_{-\\infty}^\\infty (\\max(0, |z|-\\tau))^2 \\phi(z)dz = 2 \\int_\\tau^\\infty (z-\\tau)^2 \\phi(z)dz\n$$\nExpanding the square and integrating (using identities $\\int_\\tau^\\infty z\\phi(z)dz = \\phi(\\tau)$ and $\\int_\\tau^\\infty z^2\\phi(z)dz = \\tau\\phi(\\tau)+Q(\\tau)$) yields:\n$$\n\\mathbb{E}[(|Z|-\\tau)_+^2] = 2 \\left[ (\\tau\\phi(\\tau)+Q(\\tau)) - 2\\tau(\\phi(\\tau)) + \\tau^2 Q(\\tau) \\right] = 2(1+\\tau^2)Q(\\tau) - 2\\tau\\phi(\\tau)\n$$\nCombining the terms, we get the variational form:\n$$\n\\Psi_{n,k}(\\tau) = k(1+\\tau^2) + (n-k) \\left[ 2(1+\\tau^2)Q(\\tau) - 2\\tau\\phi(\\tau) \\right]\n$$\nwith $\\delta(\\mathcal{D}) = \\inf_{\\tau \\ge 0} \\Psi_{n,k}(\\tau)$.\n\n### Part 3: Asymptotic Analysis\n\nTo find the infimum, we differentiate $\\Psi_{n,k}(\\tau)$ with respect to $\\tau$ and set it to zero. Let $\\tau^*$ be the minimizer.\n$$\n\\frac{d\\Psi_{n,k}}{d\\tau} = 2k\\tau + (n-k) \\left[ 4\\tau Q(\\tau) - 4\\phi(\\tau) \\right] = 0\n$$\nThis gives the first-order condition (FOC) for $\\tau^*  0$:\n$$\nk\\tau^* = 2(n-k) \\left[ \\phi(\\tau^*) - \\tau^*Q(\\tau^*) \\right]\n$$\nWe substitute this condition back into the expression for $\\Psi_{n,k}(\\tau^*)$ to simplify it.\n$$\n\\delta(\\mathcal{D}) = \\Psi_{n,k}(\\tau^*) = k(1+\\tau^{*2}) + (n-k)\\left[ 2(1+\\tau^{*2})Q(\\tau^*) - 2\\tau^*\\phi(\\tau^*) \\right]\n$$\nFrom the FOC, we can express $(n-k)$ in terms of $k$ and $\\tau^*$: $(n-k) = \\frac{k\\tau^*}{2(\\phi(\\tau^*)-\\tau^*Q(\\tau^*))}$. Substituting this:\n\\begin{align*}\n\\Psi_{n,k}(\\tau^*) = k(1+\\tau^{*2}) + \\frac{k\\tau^*}{2(\\phi-\\tau^*Q)} \\left[ 2(1+\\tau^{*2})Q - 2\\tau^*\\phi \\right] \\\\\n= k(1+\\tau^{*2}) - \\frac{k\\tau^*}{(\\phi-\\tau^*Q)} \\left[ \\tau^*\\phi - (1+\\tau^{*2})Q \\right] \\\\\n= k \\left[ \\frac{(1+\\tau^{*2})(\\phi-\\tau^*Q) - \\tau^{*2}\\phi + \\tau^*(1+\\tau^{*2})Q}{\\phi-\\tau^*Q} \\right] \\\\\n= k \\left[ \\frac{\\phi + \\tau^{*2}\\phi - \\tau^*Q - \\tau^{*3}Q - \\tau^{*2}\\phi + \\tau^*Q + \\tau^{*3}Q}{\\phi-\\tau^*Q} \\right] \\\\\n= k \\frac{\\phi(\\tau^*)}{\\phi(\\tau^*) - \\tau^*Q(\\tau^*)}\n\\end{align*}\nIn the regime $n\\to\\infty, k\\to\\infty, k/n\\to 0$, the ratio $(n-k)/k$ is large, which implies from the FOC that $\\tau^*$ must be large. For large $\\tau$, we use the asymptotic expansion for the Mills ratio: $Q(\\tau) \\sim \\phi(\\tau)(\\frac{1}{\\tau} - \\frac{1}{\\tau^3} + \\dots)$.\nThis gives $\\phi(\\tau) - \\tau Q(\\tau) \\sim \\phi(\\tau) - \\tau \\phi(\\tau)(\\frac{1}{\\tau} - \\frac{1}{\\tau^3}) = \\frac{\\phi(\\tau)}{\\tau^2}$.\nSubstituting this leading-order behavior into the simplified expression for $\\delta(\\mathcal{D})$:\n$$\n\\delta(\\mathcal{D}) \\approx k \\frac{\\phi(\\tau^*)}{\\phi(\\tau^*)/\\tau^{*2}} = k\\tau^{*2}\n$$\nNow we find the leading-order behavior of $\\tau^{*2}$. We use the asymptotic form in the FOC:\n$$\nk\\tau^* \\approx 2(n-k) \\frac{\\phi(\\tau^*)}{\\tau^{*2}} = 2(n-k) \\frac{1}{\\tau^{*2}\\sqrt{2\\pi}} \\exp\\left(-\\frac{\\tau^{*2}}{2}\\right)\n$$\nTaking the natural logarithm of both sides:\n$$\n\\ln(k) + 3\\ln(\\tau^*) \\approx \\ln(2(n-k)) - \\frac{\\tau^{*2}}{2} - \\frac{1}{2}\\ln(2\\pi)\n$$\nRearranging for $\\tau^{*2}$:\n$$\n\\frac{\\tau^{*2}}{2} + 3\\ln(\\tau^*) \\approx \\ln\\left(\\frac{n-k}{k}\\right) + \\text{const}\n$$\nIn the limit $n/k \\to \\infty$, $\\ln(n/k)$ is large. For large $\\tau^*$, the $\\tau^{*2}/2$ term dominates the left-hand side. The leading-order balance is:\n$$\n\\frac{\\tau^{*2}}{2} \\approx \\ln\\left(\\frac{n}{k}\\right) \\implies \\tau^{*2} \\approx 2\\ln\\left(\\frac{n}{k}\\right)\n$$\nCombining this with $\\delta(\\mathcal{D}) \\approx k\\tau^{*2}$, we arrive at the leading-order asymptotic behavior:\n$$\n\\delta(\\mathcal{D}) \\approx k \\left( 2\\ln\\left(\\frac{n}{k}\\right) \\right) = 2k\\ln\\left(\\frac{n}{k}\\right)\n$$\nThis expression represents the leading term in the asymptotic expansion for the statistical dimension of the descent cone.", "answer": "$$\n\\boxed{2k\\ln\\left(\\frac{n}{k}\\right)}\n$$", "id": "3486664"}, {"introduction": "The previous practices characterized the challenges of high dimensionality; this one provides a powerful, constructive method for overcoming them. By implementing an estimator based on the union-of-subspaces framework, you will quantitatively compare the measurement requirements for unstructured versus tree-structured sparse signals [@problem_id:3486799]. This computational exercise provides a tangible demonstration of how exploiting prior signal structure dramatically reduces the model's effective dimension, thereby mitigating the curse of dimensionality and making recovery practical.", "problem": "You are asked to formalize how model-based, tree-structured sparsity mitigates the curse of dimensionality in compressed sensing by reducing the number of measurements required for uniform recovery compared to unstructured sparsity. Work in an ambient dimension of $n$ with $k$-sparse signals, and consider random sub-Gaussian measurement matrices. The fundamental basis you must use is the union-of-subspaces viewpoint and standard concentration-of-measure for sub-Gaussian embeddings of low-dimensional sets: to embed a single $k$-dimensional subspace with distortion at most $\\delta \\in (0,1)$, one requires on the order of $m = \\mathcal{O}(\\delta^{-2} k)$ measurements; to embed a union of $L$ such subspaces uniformly, a standard net-and-union-bound argument inflates the requirement by a term on the order of $\\log L$. This principle is widely accepted and is to be taken as the starting point.\n\nFrom this base, you must derive, justify, and implement explicit measurement complexity estimators for two models:\n\n- Unstructured $k$-sparsity: the model is the union of all coordinate subspaces spanned by any $k$ columns of the identity in $\\mathbb{R}^n$. Its model cardinality is $L_{\\mathrm{un}} = \\binom{n}{k}$.\n\n- Tree-structured $k$-sparsity on a rooted $d$-ary tree: a support is any connected rooted subtree of size $k$ embedded in a large complete rooted $d$-ary tree with $n$ nodes. An upper bound on the number of such supports is obtained by counting shapes and embeddings. A well-tested combinatorial fact is that the number of rooted ordered $d$-ary tree shapes on $k$ nodes equals\n$$\nT_d(k) = \\frac{1}{((d-1)k+1)} \\binom{d k}{k}.\n$$\nA conservative and standard placement bound is that the number of embedded supports does not exceed $n \\cdot T_d(k)$, and of course cannot exceed $\\binom{n}{k}$. Hence, the structured model cardinality can be bounded by\n$$\nL_{\\mathrm{str}} \\leq \\min\\!\\left\\{ n \\cdot T_d(k), \\binom{n}{k} \\right\\}.\n$$\n\nUsing the union-of-subspaces embedding principle together with the above cardinalities, derive an estimator of the form\n$$\nm \\approx \\left\\lceil c_0 \\, \\delta^{-2} \\, \\big( k + \\log L \\big) \\right\\rceil,\n$$\nwhere $c_0 > 0$ is a fixed absolute constant, $\\log$ is the natural logarithm, and $L$ is the model cardinality. This formula is to be applied with $L = L_{\\mathrm{un}}$ for the unstructured model and with $L = L_{\\mathrm{str}}$ for the tree-structured model. You must implement this estimator exactly as written, using the natural logarithm, with $\\log \\binom{a}{b}$ and $\\log T_d(k)$ computed numerically in a stable manner via the logarithm of the gamma function when needed. You may assume that $n$ is sufficiently large relative to $k$ so that the embedding count bound is meaningful.\n\nYour program must compute, for each parameter set in the test suite below, the pair of integers $[ m_{\\mathrm{un}}, m_{\\mathrm{str}} ]$, where $m_{\\mathrm{un}}$ is the unstructured estimate and $m_{\\mathrm{str}}$ is the structured estimate. All logarithms must be natural logarithms, and the final answers are integers obtained by applying the ceiling function. No physical units are involved. Angles are not used. Percentages are not used.\n\nTest Suite (each tuple is $(n,k,d,\\delta,c_0)$):\n- Case $1$ (happy path, large ambient dimension): $(n,k,d,\\delta,c_0) = (16384, 64, 2, 0.25, 2.0)$.\n- Case $2$ (higher branching factor): $(n,k,d,\\delta,c_0) = (4096, 32, 4, 0.25, 2.0)$.\n- Case $3$ (boundary case $k=1$): $(n,k,d,\\delta,c_0) = (2048, 1, 2, 0.20, 2.0)$.\n- Case $4$ (moderate size and branching): $(n,k,d,\\delta,c_0) = (8192, 16, 3, 0.30, 2.0)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the four cases as a comma-separated list of lists, each inner list being $[ m_{\\mathrm{un}}, m_{\\mathrm{str}} ]$. For example, the output should have the form\n\"[ [m_un_case1,m_str_case1],[m_un_case2,m_str_case2],[m_un_case3,m_str_case3],[m_un_case4,m_str_case4] ]\"\nwith no spaces removed or additional text added. Use exact integer values as computed by your implementation.", "solution": "```python\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Computes and prints the estimated number of measurements for unstructured\n    and tree-structured sparsity models based on the problem specification.\n    \"\"\"\n\n    test_cases = [\n        # (n, k, d, delta, c0)\n        (16384, 64, 2, 0.25, 2.0),\n        (4096, 32, 4, 0.25, 2.0),\n        (2048, 1, 2, 0.20, 2.0),\n        (8192, 16, 3, 0.30, 2.0),\n    ]\n\n    def log_binom(n, k):\n        \"\"\"\n        Computes log(n choose k) using the log-gamma function for numerical stability.\n        Handles edge cases k=0 or k=n. If k > n or k  0, returns -inf.\n        \"\"\"\n        if k  0 or k > n:\n            return -np.inf\n        if k == 0 or k == n:\n            return 0\n        # For symmetry and to potentially use smaller numbers if k > n/2\n        if k > n / 2:\n            k = n - k\n        return gammaln(n + 1) - gammaln(k + 1) - gammaln(n - k + 1)\n\n    def log_T_d(k, d):\n        \"\"\"\n        Computes log of the number of rooted ordered d-ary tree shapes on k nodes.\n        T_d(k) = (1 / ((d-1)k + 1)) * (dk choose k)\n        \"\"\"\n        if k == 0:\n            return 0  # There is one tree of size 0 (the empty tree)\n        if k == 1:\n            return 0  # One tree of size 1 (a single node), log(1) = 0\n            \n        log_binom_term = log_binom(d * k, k)\n        denominator_term = np.log((d - 1) * k + 1)\n        return log_binom_term - denominator_term\n\n    results = []\n    for case in test_cases:\n        n, k, d, delta, c0 = case\n\n        # Pre-compute the common factor\n        pre_factor = c0 / (delta**2)\n\n        # --- Unstructured Model Calculation ---\n        # L_un = binom(n, k)\n        log_L_un = log_binom(n, k)\n        \n        # m_un = ceil(c0 * delta^-2 * (k + log(L_un)))\n        m_un = int(np.ceil(pre_factor * (k + log_L_un)))\n\n        # --- Tree-Structured Model Calculation ---\n        # L_str = min(n * T_d(k), binom(n, k))\n        # log(L_str) = min(log(n) + log(T_d(k)), log(binom(n,k)))\n        if k == 0:\n             # A k=0 sparse signal is the zero vector, requiring 0 measurements.\n             # In the context of embedding a 0-dim subspace, k=0 also works.\n             log_L_str = -np.inf # Effectively makes m_str zero.\n        else:\n             log_Td = log_T_d(k, d)\n             log_L_str_candidate = np.log(n) + log_Td\n             log_L_str = min(log_L_str_candidate, log_L_un)\n\n        # m_str = ceil(c0 * delta^-2 * (k + log(L_str)))\n        if k == 0:\n            m_str = 0\n        else:\n            m_str = int(np.ceil(pre_factor * (k + log_L_str)))\n            \n        results.append([m_un, m_str])\n\n    # Format the output exactly as specified\n    output_str = \"[\" + \",\".join(map(str, results)) + \"]\"\n    # This function would print, but we return the string to be placed in the answer tag.\n    return output_str\n\n# The value for the answer tag is the output of this execution.\n# solve()\n```", "answer": "[[17195, 4942],[8185, 4133],[431, 431],[3096, 1228]]", "id": "3486799"}]}