{"hands_on_practices": [{"introduction": "The ideal measure of sparsity is the $\\ell_0$ pseudo-norm, but its discrete, combinatorial nature makes optimization intractable. This exercise provides a rigorous, hands-on derivation of the connection between the continuous $\\ell_p$ quasi-norm and the $\\ell_0$ pseudo-norm in the limit as $p \\to 0^+$. Understanding this asymptotic relationship is key to appreciating why $\\ell_p$-minimization with small $p$ serves as a powerful surrogate for finding the sparsest possible solutions [@problem_id:3469680].", "problem": "Let $x \\in \\mathbb{R}^{n}$ be fixed and let $p \\in (0,1]$. Define the $\\ell_{p}$ quasi-norm by $\\|x\\|_{p} = \\left(\\sum_{i=1}^{n} |x_{i}|^{p}\\right)^{1/p}$ and the $\\ell_{0}$ pseudo-norm by $\\|x\\|_{0} = \\#\\{i : x_{i} \\neq 0\\}$. Consider the following two tasks grounded in the basic definitions of $\\ell_{p}$ quasi-norms, the expansion $\\exp(z) = 1 + z + o(z)$ as $z \\to 0$, and the Maximum A Posteriori (MAP) formulation for Gaussian likelihoods:\n\n1) Starting from first principles, derive the exact first-order asymptotic expansion of $\\|x\\|_{p}^{p}$ as $p \\to 0^{+}$, in the form $\\|x\\|_{p}^{p} = \\|x\\|_{0} + p \\, S(x) + o(p)$, and determine the closed-form expression of the coefficient $S(x)$ in terms of $x$.\n\n2) Consider the MAP estimation problem with Gaussian likelihood and $\\ell_{p}$ penalty\n$$\nF_{p}(u) = \\frac{1}{2 \\sigma^{2}} \\|y - A u\\|_{2}^{2} + \\lambda(p) \\, \\|u\\|_{p}^{p},\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, and $\\sigma  0$ are fixed and independent of $p$, and $\\lambda(p)  0$ is a $p$-dependent regularization parameter. Let $\\gamma  0$ be a fixed target sparsity penalty for the $\\ell_{0}$-regularized objective $\\frac{1}{2 \\sigma^{2}} \\|y - A u\\|_{2}^{2} + \\gamma \\, \\|u\\|_{0}$. Determine, in terms of limits with respect to $p \\to 0^{+}$, the necessary and sufficient asymptotic conditions on $\\lambda(p)$ so that $F_{p}(u)$ converges pointwise in $u$ to $\\frac{1}{2 \\sigma^{2}} \\|y - A u\\|_{2}^{2} + \\gamma \\, \\|u\\|_{0}$ up to $u$-independent additive terms. Express these conditions as the two limits $L_{1} = \\lim_{p \\to 0^{+}} \\lambda(p)$ and $L_{2} = \\lim_{p \\to 0^{+}} p \\, \\lambda(p)$.\n\nProvide your final answer as a single row vector containing, in order, the expression for $S(x)$, then $L_{1}$, then $L_{2}$. The final answer must be a closed-form analytic expression. No rounding is required and no units are involved.", "solution": "This problem consists of two related parts. The first part requires deriving the first-order asymptotic expansion of the $\\ell_p$ quasi-norm squared, $\\|x\\|_{p}^{p}$, as $p \\to 0^{+}$. The second part applies this result to a maximum a posteriori (MAP) estimation problem to find conditions for the convergence of an $\\ell_p$-regularized objective function to an $\\ell_0$-regularized one.\n\nPart 1: Asymptotic Expansion of $\\|x\\|_{p}^{p}$\n\nLet $x \\in \\mathbb{R}^{n}$ be a fixed vector. The term to be expanded is $\\|x\\|_{p}^{p}$, which is defined as:\n$$\n\\|x\\|_{p}^{p} = \\sum_{i=1}^{n} |x_{i}|^{p}\n$$\nWe are interested in the behavior of this expression as $p \\to 0^{+}$.\n\nLet us partition the set of indices $\\{1, 2, \\dots, n\\}$ into two disjoint sets:\n- $I_{0} = \\{i \\mid x_{i} = 0\\}$, the set of indices corresponding to zero components of $x$.\n- $I_{+} = \\{i \\mid x_{i} \\neq 0\\}$, the set of indices corresponding to non-zero components of $x$.\n\nThe sum can be split over these two sets:\n$$\n\\|x\\|_{p}^{p} = \\sum_{i \\in I_{0}} |x_{i}|^{p} + \\sum_{i \\in I_{+}} |x_{i}|^{p}\n$$\nFor any index $i \\in I_{0}$, we have $|x_{i}| = 0$. For any $p  0$, $|x_{i}|^{p} = 0^{p} = 0$. Thus, the first sum is zero:\n$$\n\\sum_{i \\in I_{0}} |x_{i}|^{p} = 0\n$$\nThe problem reduces to analyzing the second sum over the non-zero components. The number of non-zero components is, by definition, the $\\ell_0$ pseudo-norm of $x$, denoted by $\\|x\\|_{0} = |I_{+}|$.\n\nFor each index $i \\in I_{+}$, we have $|x_{i}|  0$. We can use the identity $a^b = \\exp(b \\ln a)$ for $a0$. Let $a = |x_i|$ and $b = p$.\n$$\n|x_{i}|^{p} = \\exp(p \\ln|x_{i}|)\n$$\nThe problem provides the first-order Taylor expansion for the exponential function around $z=0$: $\\exp(z) = 1 + z + o(z)$. As $p \\to 0^{+}$, the argument of the exponential, $z = p \\ln|x_{i}|$, also approaches $0$ for any fixed $x_i \\neq 0$. Applying this expansion, we get:\n$$\n|x_{i}|^{p} = 1 + p \\ln|x_{i}| + o(p)\n$$\nNow, we can substitute this expansion back into the sum over $I_{+}$:\n$$\n\\sum_{i \\in I_{+}} |x_{i}|^{p} = \\sum_{i \\in I_{+}} \\left(1 + p \\ln|x_{i}| + o(p)\\right)\n$$\nBy the properties of summation, we can separate the terms:\n$$\n\\sum_{i \\in I_{+}} 1 + \\sum_{i \\in I_{+}} p \\ln|x_{i}| + \\sum_{i \\in I_{+}} o(p)\n$$\nLet's evaluate each term:\n1. The first term is the sum of $1$ over the set $I_{+}$. The number of elements in $I_{+}$ is $|I_{+}| = \\|x\\|_{0}$. So, $\\sum_{i \\in I_{+}} 1 = \\|x\\|_{0}$.\n2. The second term is $\\sum_{i \\in I_{+}} p \\ln|x_{i}| = p \\left(\\sum_{i \\in I_{+}} \\ln|x_{i}|\\right)$.\n3. The third term is a sum of $\\|x\\|_{0}$ terms, each of which is $o(p)$. Since $\\|x\\|_{0}$ is a finite constant (at most $n$), the sum is also $o(p)$: $\\sum_{i \\in I_{+}} o(p) = \\|x\\|_{0} \\cdot o(p) = o(p)$.\n\nCombining these results, we obtain the asymptotic expansion for $\\|x\\|_{p}^{p}$:\n$$\n\\|x\\|_{p}^{p} = \\|x\\|_{0} + p \\left(\\sum_{i \\in I_{+}} \\ln|x_{i}|\\right) + o(p)\n$$\nThis expression is in the required form $\\|x\\|_{p}^{p} = \\|x\\|_{0} + p \\, S(x) + o(p)$. By direct comparison, we identify the coefficient $S(x)$:\n$$\nS(x) = \\sum_{i \\in I_{+}} \\ln|x_{i}| = \\sum_{i: x_{i} \\neq 0} \\ln|x_{i}|\n$$\n\nPart 2: Asymptotic Conditions on $\\lambda(p)$\n\nWe are given the $p$-dependent objective function:\n$$\nF_{p}(u) = \\frac{1}{2 \\sigma^{2}} \\|y - A u\\|_{2}^{2} + \\lambda(p) \\, \\|u\\|_{p}^{p}\n$$\nand the target $\\ell_0$-regularized objective function:\n$$\nF_{0}(u) = \\frac{1}{2 \\sigma^{2}} \\|y - A u\\|_{2}^{2} + \\gamma \\, \\|u\\|_{0}\n$$\nWe seek the necessary and sufficient conditions on $\\lambda(p)$ such that $F_{p}(u)$ converges pointwise in $u$ to $F_{0}(u)$ up to a $u$-independent additive term as $p \\to 0^{+}$. This means that for any fixed $u \\in \\mathbb{R}^n$, the limit\n$$\n\\lim_{p \\to 0^{+}} \\left( F_{p}(u) - F_{0}(u) \\right) = C\n$$\nmust exist and be a constant $C$ independent of $u$.\n\nLet's analyze the difference $F_{p}(u) - F_{0}(u)$:\n$$\nF_{p}(u) - F_{0}(u) = \\lambda(p) \\, \\|u\\|_{p}^{p} - \\gamma \\, \\|u\\|_{0}\n$$\nUsing the expansion for $\\|u\\|_{p}^{p}$ derived in Part 1 with $x=u$:\n$$\n\\|u\\|_{p}^{p} = \\|u\\|_{0} + p \\, S(u) + o(p) \\quad \\text{where} \\quad S(u) = \\sum_{i: u_{i} \\neq 0} \\ln|u_{i}|\n$$\nSubstituting this into the difference gives:\n$$\nF_{p}(u) - F_{0}(u) = \\lambda(p) \\left( \\|u\\|_{0} + p \\, S(u) + o(p) \\right) - \\gamma \\, \\|u\\|_{0}\n$$\n$$\n= (\\lambda(p) - \\gamma) \\|u\\|_{0} + p \\, \\lambda(p) \\, S(u) + \\lambda(p) \\, o(p)\n$$\nFor the limit of this expression as $p \\to 0^{+}$ to be a constant $C$ independent of $u$, we examine its behavior for different choices of $u$.\n\nFirst, consider $u = 0$, the zero vector. In this case, $\\|u\\|_0 = 0$ and $S(u)$ is an empty sum, which is $0$. The expression becomes $0$, so its limit is $0$. Therefore, the constant $C$ must be $0$. The condition simplifies to:\n$$\n\\lim_{p \\to 0^{+}} \\left( (\\lambda(p) - \\gamma) \\|u\\|_{0} + p \\, \\lambda(p) \\, S(u) + \\lambda(p) \\, o(p) \\right) = 0 \\quad \\text{for all } u \\in \\mathbb{R}^n.\n$$\nThe last term, $\\lambda(p)o(p)$, can be written as $\\lambda(p) p \\frac{o(p)}{p}$. As we will show, $\\lim_{p\\to 0^+} \\lambda(p)$ must be finite, so $\\lambda(p)$ is bounded near $p=0$. As $\\lim_{p\\to 0^+} \\frac{o(p)}{p}=0$, the term $\\lambda(p) \\, o(p)$ approaches $0$ and can be ignored in the subsequent analysis of the necessary conditions.\n\nTo find the necessary conditions, we test with specific non-zero vectors $u$:\n1. Let $u$ be a vector with a single non-zero component, $u_k = 1$, and all other components zero. For this $u$, $\\|u\\|_{0} = 1$ and $S(u) = \\ln|1| = 0$. The limit condition becomes:\n$$\n\\lim_{p \\to 0^{+}} \\left( (\\lambda(p) - \\gamma) \\cdot 1 + p \\, \\lambda(p) \\cdot 0 \\right) = 0 \\implies \\lim_{p \\to 0^{+}} (\\lambda(p) - \\gamma) = 0\n$$\nThis implies $\\lim_{p \\to 0^{+}} \\lambda(p) = \\gamma$. This is the condition for $L_1$. So, $L_1 = \\gamma$.\n\n2. Let $u$ be a vector with a single non-zero component, $u_k = c$, where $|c| \\neq 1$ (e.g., $c=e$). For this $u$, $\\|u\\|_{0} = 1$ and $S(u) = \\ln|c| \\neq 0$. The limit condition becomes:\n$$\n\\lim_{p \\to 0^{+}} \\left( (\\lambda(p) - \\gamma) \\cdot 1 + p \\, \\lambda(p) \\cdot \\ln|c| \\right) = 0\n$$\nSince we have already established from the first case that $\\lim_{p \\to 0^{+}} (\\lambda(p) - \\gamma) = 0$, the condition simplifies to:\n$$\n\\lim_{p \\to 0^{+}} \\left( p \\, \\lambda(p) \\cdot \\ln|c| \\right) = 0 \\implies (\\ln|c|) \\lim_{p \\to 0^{+}} (p \\, \\lambda(p)) = 0\n$$\nSince we chose $c$ such that $\\ln|c| \\neq 0$, it is necessary that $\\lim_{p \\to 0^{+}} p \\, \\lambda(p) = 0$. This is the condition for $L_2$. So, $L_2 = 0$.\n\nThus, the necessary conditions are $L_{1} = \\lim_{p \\to 0^{+}} \\lambda(p) = \\gamma$ and $L_{2} = \\lim_{p \\to 0^{+}} p \\, \\lambda(p) = 0$.\n\nNow, we show these conditions are also sufficient. Assume $L_1 = \\gamma$ and $L_2 = 0$. Then for any fixed $u$:\n$$\n\\lim_{p \\to 0^{+}} (F_{p}(u) - F_{0}(u)) = \\lim_{p \\to 0^{+}} \\left( (\\lambda(p) - \\gamma) \\|u\\|_{0} + p \\, \\lambda(p) \\, S(u) + \\lambda(p) \\, o(p) \\right)\n$$\nEvaluating the limit of each term:\n- $\\lim_{p \\to 0^{+}} (\\lambda(p) - \\gamma) \\|u\\|_{0} = (\\lim_{p \\to 0^{+}} \\lambda(p) - \\gamma) \\|u\\|_{0} = (\\gamma - \\gamma) \\|u\\|_{0} = 0$.\n- $\\lim_{p \\to 0^{+}} p \\, \\lambda(p) \\, S(u) = (\\lim_{p \\to 0^{+}} p \\, \\lambda(p)) S(u) = 0 \\cdot S(u) = 0$.\n- $\\lim_{p \\to 0^{+}} \\lambda(p) \\, o(p) = \\lim_{p \\to 0^{+}} \\lambda(p) \\cdot \\lim_{p \\to 0^{+}} o(p) = \\gamma \\cdot 0 = 0$.\n\nThe sum of these limits is $0$. Thus, $\\lim_{p \\to 0^{+}} (F_{p}(u) - F_{0}(u)) = 0$. This limit is a constant ($0$) independent of $u$, so the conditions are sufficient.\n\nIn summary, the results are:\n1. $S(x) = \\sum_{i: x_{i} \\neq 0} \\ln|x_{i}|$\n2. $L_{1} = \\gamma$\n3. $L_{2} = 0$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\sum_{i: x_i \\neq 0} \\ln|x_i|  \\gamma  0 \\end{pmatrix}}\n$$", "id": "3469680"}, {"introduction": "The sparsity-promoting power of $\\ell_p$ quasi-norms for $p \\in (0,1)$ is best understood through their unique geometry. This exercise challenges you to connect the analytical properties of the $\\ell_p$ penalty to the geometric features of its level setsâ€”the non-convex 'star-shaped' unit balls. By exploring concepts like exposed points and support functions, you will build a strong intuition for why these penalties are so effective at identifying sparse solutions and why those solutions are robust [@problem_id:3469684].", "problem": "Consider the quasi-norm $\\|x\\|_p = \\left( \\sum_{i=1}^n |x_i|^p \\right)^{1/p}$ with $p\\in(0,1)$ on $\\mathbb{R}^n$, and its unit ball $\\mathcal{B}_p := \\{ x \\in \\mathbb{R}^n : \\|x\\|_p \\le 1 \\}$. The boundary $\\partial \\mathcal{B}_p = \\{ x \\in \\mathbb{R}^n : \\|x\\|_p = 1 \\}$ is nonconvex and exhibits sharp geometry near coordinate axes. For a vector $v \\in \\mathbb{R}^n$, the support function of a set $\\mathcal{S} \\subset \\mathbb{R}^n$ is defined as $h_{\\mathcal{S}}(v) := \\sup_{x\\in\\mathcal{S}} \\langle v, x \\rangle$. A point $x^\\star\\in\\mathcal{S}$ is called an exposed point (not requiring convexity of $\\mathcal{S}$) if there exists $v\\in\\mathbb{R}^n$ such that $\\langle v, x^\\star\\rangle  \\langle v, x\\rangle$ for all $x\\in\\mathcal{S}$ with $x\\neq x^\\star$.\n\nIn the data-fitting context of compressed sensing, consider the regularized least-squares objective $F(x) := \\tfrac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_p^p$ with a matrix $A \\in \\mathbb{R}^{m\\times n}$, data $b \\in \\mathbb{R}^m$, parameter $\\lambda0$, and $p\\in(0,1)$. The penalty $\\|x\\|_p^p = \\sum_{i=1}^n |x_i|^p$ is separable across coordinates and has a one-dimensional profile $t\\mapsto |t|^p$ that is concave on $[0,\\infty)$ and features an infinite derivative at $t=0$.\n\nUsing only first principles and core definitions, reason about the geometry of $\\mathcal{B}_p$ near sparse vectors and its implications for robust sparse support identification in minimizers of $F$. Select all statements that are true.\n\n- A. For any $p\\in(0,1)$ and any $v\\in\\mathbb{R}^n$, the support function of the $\\ell_p$ unit ball satisfies $h_{\\mathcal{B}_p}(v) = \\|v\\|_\\infty$. If $\\|v\\|_\\infty$ is attained at a unique coordinate $j$, then the unique global maximizer of $\\langle v, x\\rangle$ over $\\mathcal{B}_p$ is the $1$-sparse point $x^\\star = \\mathrm{sign}(v_j)\\, e_j$. Consequently, the only exposed points of $\\mathcal{B}_p$ are the axis points $\\{\\pm e_i\\}_{i=1}^n$.\n\n- B. For any $p\\in(0,1)$, the $\\ell_p$ unit ball is strictly convex, so for generic $v$ the maximizer of $\\langle v, x\\rangle$ over $\\mathcal{B}_p$ has full support.\n\n- C. In the problem $\\min_x \\tfrac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_p^p$ with $p\\in(0,1)$, coordinates at zero in a local minimizer $x^\\star$ are locally stable under small perturbations of $b$, because the one-dimensional penalized subproblem $\\min_{t\\in\\mathbb{R}} \\tfrac{1}{2}(t - y)^2 + \\lambda |t|^p$ exhibits a hard-thresholding effect due to the infinite derivative of $|t|^p$ at $t=0$. This stability is geometrically linked to the sharp axes of $\\mathcal{B}_p$.\n\n- D. The infinite derivative of $|t|^p$ at $t=0$ implies that any local minimizer of $\\tfrac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_p^p$ must be $1$-sparse (i.e., have exactly one nonzero coordinate).\n\n- E. The boundary $\\partial \\mathcal{B}_p$ has positive reach and is prox-regular at sparse points, which guarantees exact support identification in finite time by proximal-gradient methods for any noise level in $b$.\n\nChoose all correct options.", "solution": "The user wants a critical validation of the problem statement, followed by a detailed solution deriving the correct options.\n\n### Step 1: Extract Givens\n- **Quasi-norm:** $\\|x\\|_p = \\left( \\sum_{i=1}^n |x_i|^p \\right)^{1/p}$ with $p\\in(0,1)$ on $\\mathbb{R}^n$.\n- **Unit ball:** $\\mathcal{B}_p := \\{ x \\in \\mathbb{R}^n : \\|x\\|_p \\le 1 \\}$. This is equivalent to $\\sum_{i=1}^n |x_i|^p \\le 1$.\n- **Boundary:** $\\partial \\mathcal{B}_p = \\{ x \\in \\mathbb{R}^n : \\|x\\|_p = 1 \\}$.\n- **Property of boundary:** Nonconvex and exhibits sharp geometry near coordinate axes.\n- **Support function:** $h_{\\mathcal{S}}(v) := \\sup_{x\\in\\mathcal{S}} \\langle v, x \\rangle$ for a set $\\mathcal{S} \\subset \\mathbb{R}^n$ and a vector $v \\in \\mathbb{R}^n$.\n- **Exposed point:** $x^\\star\\in\\mathcal{S}$ is an exposed point if there exists $v\\in\\mathbb{R}^n$ such that $\\langle v, x^\\star\\rangle  \\langle v, x\\rangle$ for all $x\\in\\mathcal{S}$ with $x\\neq x^\\star$.\n- **Optimization problem:** $F(x) := \\tfrac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_p^p$ with $A \\in \\mathbb{R}^{m\\times n}$, $b \\in \\mathbb{R}^m$, $\\lambda0$, and $p\\in(0,1)$.\n- **Penalty term:** $\\|x\\|_p^p = \\sum_{i=1}^n |x_i|^p$.\n- **Property of penalty term:** $t\\mapsto |t|^p$ is concave on $[0,\\infty)$ and has an infinite derivative at $t=0$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically sound, well-posed, and objective. It is rooted in the established mathematical field of sparse optimization and compressed sensing. All definitions and statements of properties (e.g., non-convexity of $\\mathcal{B}_p$, concavity and infinite derivative of $|t|^p$) are standard and correct. The problem is self-contained and asks for an evaluation of statements based on first principles, which is a valid and formalizable task. No flaws are detected.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution will proceed by analyzing each option.\n\n---\n\n### Analysis of the Geometry of $\\mathcal{B}_p$ and the Support Function\n\nFor $p \\in (0,1)$, the function $\\phi(t) = t^p$ is strictly concave for $t0$ since $\\phi''(t) = p(p-1)t^{p-2}  0$. This concavity leads to the non-convexity of the unit ball $\\mathcal{B}_p$. For example, $e_1, e_2 \\in \\mathcal{B}_p$ (standard basis vectors), but their midpoint $\\frac{1}{2}e_1 + \\frac{1}{2}e_2$ has $\\|\\frac{1}{2}e_1 + \\frac{1}{2}e_2\\|_p^p = (\\frac{1}{2})^p + (\\frac{1}{2})^p = 2^{1-p}  1$, so it lies outside $\\mathcal{B}_p$.\n\nLet us analyze the support function $h_{\\mathcal{B}_p}(v) = \\sup_{x \\in \\mathcal{B}_p} \\langle v, x \\rangle$. The supremum is achieved on the boundary $\\partial \\mathcal{B}_p$, where $\\sum_{i=1}^n |x_i|^p = 1$. Due to the signs, we have $\\langle v, x \\rangle = \\sum_i v_i x_i \\le \\sum_i |v_i| |x_i|$. The maximum will be achieved when $\\mathrm{sign}(x_i) = \\mathrm{sign}(v_i)$ for all $i$. So we seek to maximize $\\sum_i |v_i| |x_i|$ subject to $\\sum_i |x_i|^p = 1$ and $x_i \\ge 0$. Let $u_i = |x_i|$ and $c_i = |v_i|$. We want to maximize the linear function $f(u) = \\sum_i c_i u_i$ on the non-convex set $U = \\{u \\in \\mathbb{R}^n : u_i \\ge 0, \\sum_i u_i^p = 1\\}$.\n\nLet $c_j = \\max_i c_i = \\|v\\|_\\infty$. For any point $u \\in U$, we have $u_i \\in [0,1]$. Since $p \\in (0,1)$, we have $t^p \\ge t$ for $t \\in [0,1]$. Thus $\\sum u_i \\le \\sum u_i^p = 1$ is false.\n\nA correct argument is as follows: Let $u \\in U$. Then $f(u) = \\sum_i c_i u_i \\le \\sum_i c_j u_i = c_j \\sum_i u_i = \\|v\\|_\\infty \\sum_i u_i$. We need to maximize $\\sum_i u_i$ subject to $\\sum_i u_i^p = 1$. Let $g(u) = \\sum_i u_i$. Consider a point with support of size $k \\ge 1$. Using Lagrange multipliers, a critical point must have $u_i$ equal for all $i$ in the support. Let this value be $c$. Then $k c^p = 1 \\implies c = k^{-1/p}$. The value of $g$ is $k \\cdot c = k^{1-1/p}$. Since $p \\in (0,1)$, the exponent $1-1/p$ is negative. Thus, $k^{1-1/p}$ is a decreasing function of $k$. The maximum is achieved for the smallest possible support size, $k=1$. In this case, $u$ is a standard basis vector $e_j$, and $g(e_j)=1$.\nTherefore, for any $u \\in U$, $\\sum_i u_i \\le 1$.\nThis gives $f(u) = \\sum c_i u_i \\le \\|v\\|_\\infty \\sum u_i \\le \\|v\\|_\\infty$. The value $\\|v\\|_\\infty$ is achieved by setting $u=e_j$ where $j$ is an index for which $|v_j|=\\|v\\|_\\infty$. This corresponds to $x=\\mathrm{sign}(v_j) e_j$, which is in $\\mathcal{B}_p$.\nThus, $h_{\\mathcal{B}_p}(v) = \\|v\\|_\\infty$.\n\n### Option-by-Option Analysis\n\n**A. For any $p\\in(0,1)$ and any $v\\in\\mathbb{R}^n$, the support function of the $\\ell_p$ unit ball satisfies $h_{\\mathcal{B}_p}(v) = \\|v\\|_\\infty$. If $\\|v\\|_\\infty$ is attained at a unique coordinate $j$, then the unique global maximizer of $\\langle v, x\\rangle$ over $\\mathcal{B}_p$ is the $1$-sparse point $x^\\star = \\mathrm{sign}(v_j)\\, e_j$. Consequently, the only exposed points of $\\mathcal{B}_p$ are the axis points $\\{\\pm e_i\\}_{i=1}^n$.**\n\nThe first part, $h_{\\mathcal{B}_p}(v) = \\|v\\|_\\infty$, is correct as derived above.\nThe maximizers of $\\langle v, x \\rangle$ are found among the points $x=\\mathrm{sign}(v_j) e_j$ for indices $j$ where $|v_j| = \\|v\\|_\\infty$. If this maximum is attained at a unique coordinate $j$, then there is a single such $j$, and the maximizer $x^\\star = \\mathrm{sign}(v_j) e_j$ is unique. By setting $v = e_j$ or $v = -e_j$, we see that $e_j$ and $-e_j$ are exposed points for any $j=1, \\dots, n$.\nAn exposed point must be a unique maximizer of a linear functional. Our analysis showed that any maximizer of a linear functional over $\\mathcal{B}_p$ must be $1$-sparse. A point with support size greater than $1$ can never be a maximizer. Therefore, no point other than $\\{\\pm e_i\\}_{i=1}^n$ can be an exposed point. The statement is correct.\n\n**Verdict: Correct**\n\n**B. For any $p\\in(0,1)$, the $\\ell_p$ unit ball is strictly convex, so for generic $v$ the maximizer of $\\langle v, x\\rangle$ over $\\mathcal{B}_p$ has full support.**\n\nThe $\\ell_p$ unit ball for $p \\in (0,1)$ is famously non-convex, as shown by the counterexample $x=e_1, y=e_2$. The midpoint $\\frac{1}{2}(x+y)$ is not in $\\mathcal{B}_p$. Therefore, it is certainly not strictly convex. The first part of the statement is false.\nFurthermore, the analysis for option A shows that for any $v$, the maximizer(s) of $\\langle v, x\\rangle$ are always $1$-sparse, which is the opposite of having full support (for $n1$). The second part of the statement is also false.\n\n**Verdict: Incorrect**\n\n**C. In the problem $\\min_x \\tfrac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_p^p$ with $p\\in(0,1)$, coordinates at zero in a local minimizer $x^\\star$ are locally stable under small perturbations of $b$, because the one-dimensional penalized subproblem $\\min_{t\\in\\mathbb{R}} \\tfrac{1}{2}(t - y)^2 + \\lambda |t|^p$ exhibits a hard-thresholding effect due to the infinite derivative of $|t|^p$ at $t=0$. This stability is geometrically linked to the sharp axes of $\\mathcal{B}_p$.**\n\nLet's analyze the 1D problem $g(t) = \\frac{1}{2}(t - y)^2 + \\lambda |t|^p$. The first term is convex and smooth. The penalty $\\lambda|t|^p$ is concave on $[0, \\infty)$ and $(-\\infty, 0]$. Its derivative is $\\lambda p \\cdot \\mathrm{sign}(t) |t|^{p-1}$ for $t \\ne 0$. As $t \\to 0$, the gradient of the penalty term blows up to $\\pm\\infty$. This means that for $t=0$ to be a local minimizer of $g(t)$, the derivative of the smooth part $\\frac{1}{2}(t-y)^2$ at $t=0$, which is $-y$, can be any finite value. The \"infinite\" gradient of the penalty will always overwhelm it.\nHowever, for $t=0$ to be the *global* minimizer, its value, $g(0)=\\frac{1}{2}y^2$, must be less than the value at any other local minimizer $t^\\star \\ne 0$. This comparison leads to a threshold phenomenon: there exists a $y_{thresh}  0$ such that if $|y|  y_{thresh}$, the global minimum is at $t=0$, and if $|y|  y_{thresh}$, the global minimum is non-zero. This is a \"hard-thresholding\" operator, unlike the \"soft-thresholding\" from the $\\ell_1$ penalty.\nIn the full problem, a zero-coordinate $x_i^\\star=0$ in a local minimizer $x^\\star$ is stable if it remains zero for small perturbations of the problem, e.g., in $b$. A small change in $b$ causes a small change in the gradient of the least-squares term. The hard-thresholding property implies that as long as this change is not large enough to cross the threshold, the coordinate will remain zero. This provides stability to the zero-set (the support). The infinite derivative is the analytical reason, and the \"sharp axes\" or \"cusps\" of the level sets of $\\|x\\|_p^p$ (which are scaled versions of $\\mathcal{B}_p$) is the correct corresponding geometric intuition. The statement is conceptually sound and reflects established knowledge in the field.\n\n**Verdict: Correct**\n\n**D. The infinite derivative of $|t|^p$ at $t=0$ implies that any local minimizer of $\\tfrac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_p^p$ must be $1$-sparse (i.e., have exactly one nonzero coordinate).**\n\nThis statement is an oversimplification. While the penalty term strongly favors sparse solutions, it does not mandate that all local minimizers must be $1$-sparse. Consider a simple case where $A=I$ (the identity matrix). The objective becomes separable: $F(x) = \\sum_{i=1}^n \\left( \\frac{1}{2}(x_i-b_i)^2 + \\lambda|x_i|^p \\right)$. The global minimizer is found by minimizing each term individually. From the analysis in C, if $|b_i|$ is larger than the threshold $y_{thresh}$, the corresponding optimal $x_i^\\star$ will be non-zero. If we choose a vector $b$ with several components that are large in magnitude (e.g., $b_i \\gg y_{thresh}$ for $i=1, \\dots, k$), the minimizer $x^\\star$ will have at least $k$ non-zero components. Thus, a local (and in this case, global) minimizer is not necessarily $1$-sparse.\n\n**Verdict: Incorrect**\n\n**E. The boundary $\\partial \\mathcal{B}_p$ has positive reach and is prox-regular at sparse points, which guarantees exact support identification in finite time by proximal-gradient methods for any noise level in $b$.**\n\nThis statement contains several advanced terms, all incorrectly applied.\n1.  **Positive Reach:** A set has positive reach at a point if a ball of some positive radius can be rolled along the boundary near that point. The boundary $\\partial\\mathcal{B}_p$ has inward-pointing cusps at the sparse points $\\{\\pm e_i\\}$. Such cusps imply the reach is zero at these points.\n2.  **Prox-regularity:** Prox-regularity is a property related to the single-valuedness and continuity of the projection operator. Sets with zero reach, like $\\partial\\mathcal{B}_p$ at its cusps, are not prox-regular.\n3.  **Proximal-gradient methods:** Standard proximal-gradient methods require the non-smooth part of the objective to be convex. Here, the penalty $\\|x\\|_p^p$ is non-convex. While non-convex versions of the algorithm exist, their convergence theory is weaker and more complex, and they are not guaranteed to find the global minimum or the correct support.\n4.  **Guarantees... for any noise level:** No signal recovery algorithm can guarantee exact support identification for *any* noise level. If the noise is sufficiently large, it will overwhelm the signal, making recovery impossible. All such guarantees in the literature rely on assumptions about the signal-to-noise ratio being sufficiently high.\nEach part of this statement is flawed.\n\n**Verdict: Incorrect**", "answer": "$$\\boxed{AC}$$", "id": "3469684"}, {"introduction": "While $\\ell_p$ penalties are effective for model selection, their use in estimation comes with a crucial trade-off: bias. This practice delves into the statistical properties of $\\ell_p$-penalized estimators, asking you to quantify the shrinkage effect they impose on non-zero coefficients. By analyzing this bias and comparing it to a debiased estimator, you will gain insight into a fundamental challenge in penalized regression and the practical steps taken to mitigate it [@problem_id:3469670].", "problem": "Consider the scalar denoising subproblem that arises from separability under an orthonormal design in compressed sensing and sparse optimization: for given observation $y \\in \\mathbb{R}$, regularization exponent $p \\in (0,1]$, and penalty weight $\\lambda  0$, define the $\\ell_{p}$-penalized estimator $x^{\\star}(y)$ as any minimizer of\n$$\n\\min_{x \\in \\mathbb{R}} \\;\\; \\frac{1}{2}\\,(y - x)^{2} + \\lambda\\,|x|^{p}.\n$$\nAssume $y  0$ and that the minimizer is strictly positive, so that first-order stationarity applies. Let the true coefficient be $a  0$ and consider the large-coefficient regime $a \\to \\infty$.\n\n(a) In the noiseless case $y = a$, use the stationarity condition (without invoking any pre-derived proximal formulas) to derive the leading-order asymptotic shrinkage factor $s(a)$, defined by $x^{\\star}(a) = s(a)\\,a$, to its first nontrivial order in the small parameter $\\lambda\\,a^{p-2}$. Your answer must be a closed-form analytic expression in $a$, $p$, and $\\lambda$.\n\n(b) In the noisy case $y = a + \\varepsilon$ with $\\varepsilon$ a zero-mean Gaussian random variable of variance $\\sigma^{2}  0$, suppose that a separate support-selection step has correctly retained the index of $a$, and that subsequent least squares (LS) debiasing on the selected support sets $x^{\\mathrm{db}} := y$. Using a first-order expansion in $\\varepsilon$ about $a$ and working to the first non-vanishing orders in $\\lambda$ and $\\sigma$, compute the leading-order difference in mean squared error\n$$\n\\Delta \\;:=\\; \\mathbb{E}\\!\\left[(x^{\\star}(y) - a)^{2}\\right] \\;-\\; \\mathbb{E}\\!\\left[(x^{\\mathrm{db}} - a)^{2}\\right],\n$$\nexpressed in closed form as a function of $a$, $p$, $\\lambda$, and $\\sigma$, keeping terms up to order $\\lambda\\,\\sigma^{2}$ and $\\lambda^{2}$.\n\nProvide your final answer as a single $1 \\times 2$ row matrix containing, in order, the expressions for $s(a)$ from part (a) and $\\Delta$ from part (b). No rounding is required and no units are involved. Use radians if any angles appear (no angles are expected here).", "solution": "The user wants me to solve a two-part problem concerning an $\\ell_p$-penalized estimator.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Objective Function**: $\\min_{x \\in \\mathbb{R}} \\;\\; \\frac{1}{2}\\,(y - x)^{2} + \\lambda\\,|x|^{p}$\n- **Estimator**: $x^{\\star}(y)$ is a minimizer of the objective function.\n- **Parameters**: Observation $y \\in \\mathbb{R}$, regularization exponent $p \\in (0,1]$, penalty weight $\\lambda  0$.\n- **Assumptions**: $y  0$, the minimizer $x^{\\star}(y)$ is strictly positive ($x^{\\star}(y)0$), and first-order stationarity applies.\n- **True Coefficient**: $a  0$.\n- **Regime**: Large-coefficient, $a \\to \\infty$.\n\n**Part (a) Givens**:\n- **Case**: Noiseless, $y = a$.\n- **Definition**: Shrinkage factor $s(a)$ is defined by $x^{\\star}(a) = s(a)\\,a$.\n- **Task**: Derive the leading-order asymptotic expression for $s(a)$ to its first nontrivial order in the small parameter $\\lambda\\,a^{p-2}$.\n\n**Part (b) Givens**:\n- **Case**: Noisy, $y = a + \\varepsilon$, where $\\varepsilon$ is a zero-mean Gaussian random variable with variance $\\sigma^2  0$ (i.e., $\\mathbb{E}[\\varepsilon] = 0$, $\\mathbb{E}[\\varepsilon^2]=\\sigma^2$).\n- **Debiased Estimator**: $x^{\\mathrm{db}} := y$.\n- **Task**: Compute the leading-order difference in mean squared error (MSE), $\\Delta \\;:=\\; \\mathbb{E}[(x^{\\star}(y) - a)^{2}] - \\mathbb{E}[(x^{\\mathrm{db}} - a)^{2}]$.\n- **Methodology**: Use a first-order expansion in $\\varepsilon$ about $a$. Keep terms up to order $\\lambda\\,\\sigma^{2}$ and $\\lambda^{2}$.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded**: The problem is a standard exercise in the statistical analysis of penalized estimators, a core topic in high-dimensional statistics, compressed sensing, and machine learning. The setup is scientifically and mathematically sound.\n2.  **Well-Posed**: The problem is well-posed. The objective function, while non-convex for $p \\in (0,1)$, admits a unique positive minimizer for sufficiently large positive $y$. The asymptotic analysis is a standard mathematical technique. All tasks are clearly defined.\n3.  **Objective**: The problem is stated in precise, objective mathematical language, free from ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. I will proceed with the detailed solution.\n\n### Part (a): Noiseless Case and Shrinkage Factor\n\nThe objective function to minimize is\n$$f(x) = \\frac{1}{2}(y - x)^2 + \\lambda |x|^p.$$\nIn the noiseless case, $y=a$. We are given that $a  0$ and the minimizer $x^{\\star}(a)$ is strictly positive. Therefore, $|x^{\\star}(a)| = x^{\\star}(a)$, and we can write the objective function as\n$$f(x) = \\frac{1}{2}(a - x)^2 + \\lambda x^p.$$\nThe problem states that first-order stationarity applies. We find the minimizer $x^{\\star} = x^{\\star}(a)$ by setting the first derivative of $f(x)$ with respect to $x$ to zero:\n$$\\frac{df}{dx} = -(a - x) + \\lambda p x^{p-1} = 0.$$\nThis gives the stationarity condition:\n$$x^{\\star} - a + \\lambda p (x^{\\star})^{p-1} = 0.$$\nWe are asked to find the shrinkage factor $s(a)$, defined by $x^{\\star} = s(a)a$. Substituting this into the stationarity condition yields:\n$$s(a)a - a + \\lambda p (s(a)a)^{p-1} = 0.$$\nFactoring out $a$ and rearranging, we get:\n$$a(s(a) - 1) = -\\lambda p s(a)^{p-1} a^{p-1}.$$\n$$s(a) - 1 = -\\lambda p \\,s(a)^{p-1} a^{p-2}.$$\nThis is an implicit equation for $s(a)$. We are interested in the large-coefficient regime, $a \\to \\infty$. Since $p \\in (0,1]$, the exponent $p-2$ is negative. Thus, the parameter $\\epsilon = \\lambda a^{p-2}$ is small as $a \\to \\infty$. The equation shows that $s(a)-1$ is of order $\\epsilon$, which means $s(a)$ is close to $1$. We can therefore approximate $s(a)^{p-1} \\approx 1^{p-1} = 1$ on the right-hand side to obtain the first-order approximation:\n$$s(a) - 1 \\approx -\\lambda p (1) a^{p-2}.$$\nThis gives the asymptotic shrinkage factor to its first nontrivial order:\n$$s(a) = 1 - \\lambda p a^{p-2}.$$\n\nAs an alternative check, from the stationarity condition, $x^{\\star} = a - \\lambda p (x^{\\star})^{p-1}$. In the limit $a \\to \\infty$, we expect $x^{\\star} \\to a$. So, we can approximate $(x^{\\star})^{p-1} \\approx a^{p-1}$.\n$$x^{\\star} \\approx a - \\lambda p a^{p-1}.$$\nThen the shrinkage factor is $s(a) = \\frac{x^{\\star}}{a} \\approx \\frac{a - \\lambda p a^{p-1}}{a} = 1 - \\lambda p a^{p-2}$, which confirms the result.\n\n### Part (b): MSE Difference in the Noisy Case\n\nWe need to compute $\\Delta = \\mathbb{E}[(x^{\\star}(y) - a)^{2}] - \\mathbb{E}[(x^{\\mathrm{db}} - a)^{2}]$, where $y = a + \\varepsilon$.\n\nFirst, we compute the MSE of the debiased estimator $x^{\\mathrm{db}} = y$:\n$$\\mathbb{E}[(x^{\\mathrm{db}} - a)^{2}] = \\mathbb{E}[(y - a)^{2}] = \\mathbb{E}[((a + \\varepsilon) - a)^{2}] = \\mathbb{E}[\\varepsilon^{2}] = \\sigma^{2}.$$\nNext, we analyze the MSE of the penalized estimator $x^{\\star}(y)$. We are instructed to use a first-order expansion of $x^{\\star}(y)$ in $\\varepsilon$ about $y=a$. Let $x^{\\star}(y)$ be the function mapping an observation $y$ to its estimator.\n$$x^{\\star}(y) = x^{\\star}(a + \\varepsilon) \\approx x^{\\star}(a) + \\varepsilon \\frac{dx^{\\star}}{dy}\\bigg|_{y=a}.$$\nThe error of the estimator is:\n$$x^{\\star}(y) - a \\approx (x^{\\star}(a) - a) + \\varepsilon \\frac{dx^{\\star}}{dy}(a).$$\nThe MSE is the expectation of the square of this error:\n$$\\mathbb{E}[(x^{\\star}(y) - a)^{2}] \\approx \\mathbb{E}\\left[\\left( (x^{\\star}(a) - a) + \\varepsilon \\frac{dx^{\\star}}{dy}(a) \\right)^2\\right].$$\nExpanding the square gives:\n$$(x^{\\star}(a) - a)^2 + \\varepsilon^2 \\left(\\frac{dx^{\\star}}{dy}(a)\\right)^2 + 2(x^{\\star}(a) - a) \\varepsilon \\frac{dx^{\\star}}{dy}(a).$$\nTaking the expectation, and using $\\mathbb{E}[\\varepsilon] = 0$ and $\\mathbb{E}[\\varepsilon^2] = \\sigma^2$, we obtain:\n$$\\mathbb{E}[(x^{\\star}(y) - a)^{2}] \\approx (x^{\\star}(a) - a)^2 + \\sigma^2 \\left(\\frac{dx^{\\star}}{dy}(a)\\right)^2.$$\nWe need the terms $(x^{\\star}(a) - a)$ and $\\frac{dx^{\\star}}{dy}(a)$.\nFrom the stationarity condition in part (a), $x^{\\star}(a) - a = -\\lambda p (x^{\\star}(a))^{p-1}$. To the leading order, $x^{\\star}(a) \\approx a$, so the bias term is:\n$$x^{\\star}(a) - a \\approx -\\lambda p a^{p-1}.$$\nThe squared bias is approximately $(x^{\\star}(a) - a)^2 \\approx \\lambda^2 p^2 a^{2p-2}$. This term is of order $\\lambda^2$.\n\nTo find the derivative $\\frac{dx^{\\star}}{dy}$, we use implicit differentiation on the general stationarity condition, $x^{\\star}(y) - y + \\lambda p (x^{\\star}(y))^{p-1} = 0$:\n$$\\frac{d}{dy}\\left[x^{\\star}(y) - y + \\lambda p (x^{\\star}(y))^{p-1}\\right] = 0.$$\n$$\\frac{dx^{\\star}}{dy} - 1 + \\lambda p (p-1) (x^{\\star}(y))^{p-2} \\frac{dx^{\\star}}{dy} = 0.$$\n$$\\frac{dx^{\\star}}{dy} \\left[1 + \\lambda p (p-1) (x^{\\star}(y))^{p-2}\\right] = 1.$$\n$$\\frac{dx^{\\star}}{dy} = \\frac{1}{1 + \\lambda p (p-1) (x^{\\star}(y))^{p-2}}.$$\nWe evaluate this derivative at $y=a$. Using $x^{\\star}(a) \\approx a$, we have:\n$$\\frac{dx^{\\star}}{dy}(a) \\approx \\frac{1}{1 + \\lambda p (p-1) a^{p-2}}.$$\nSince $\\lambda a^{p-2}$ is a small parameter, we can use the Taylor expansion $(1+z)^{-1} \\approx 1-z$ for small $z$:\n$$\\frac{dx^{\\star}}{dy}(a) \\approx 1 - \\lambda p (p-1) a^{p-2}.$$\nNow we need the square of the derivative:\n$$\\left(\\frac{dx^{\\star}}{dy}(a)\\right)^2 \\approx \\left(1 - \\lambda p (p-1) a^{p-2}\\right)^2 \\approx 1 - 2\\lambda p (p-1) a^{p-2},$$\nwhere we have ignored terms of order $(\\lambda a^{p-2})^2$, i.e., $\\lambda^2 a^{2p-4}$, as they will lead to terms of order $\\lambda^2\\sigma^2$, which is higher than requested.\n\nNow, we assemble the MSE of $x^{\\star}(y)$:\n$$\\mathbb{E}[(x^{\\star}(y) - a)^{2}] \\approx (x^{\\star}(a) - a)^2 + \\sigma^2 \\left(\\frac{dx^{\\star}}{dy}(a)\\right)^2.$$\n$$\\mathbb{E}[(x^{\\star}(y) - a)^{2}] \\approx \\lambda^2 p^2 a^{2p-2} + \\sigma^2 \\left(1 - 2\\lambda p (p-1) a^{p-2}\\right).$$\n$$\\mathbb{E}[(x^{\\star}(y) - a)^{2}] \\approx \\lambda^2 p^2 a^{2p-2} + \\sigma^2 - 2\\lambda p(p-1)\\sigma^2 a^{p-2}.$$\nThe terms are of order $\\lambda^2$ and $\\lambda\\sigma^2$, which are the orders requested.\n\nFinally, we compute the difference $\\Delta$:\n$$\\Delta = \\mathbb{E}[(x^{\\star}(y) - a)^{2}] - \\mathbb{E}[(x^{\\mathrm{db}} - a)^{2}].$$\n$$\\Delta \\approx \\left(\\lambda^2 p^2 a^{2p-2} + \\sigma^2 - 2\\lambda p(p-1)\\sigma^2 a^{p-2}\\right) - \\sigma^2.$$\n$$\\Delta \\approx \\lambda^2 p^2 a^{2p-2} - 2\\lambda p(p-1)\\sigma^2 a^{p-2}.$$\nThis is the leading-order difference in MSE, expressed in terms of $a$, $p$, $\\lambda$, and $\\sigma$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 - \\lambda p a^{p-2}  \\lambda^{2} p^{2} a^{2p-2} - 2\\lambda p(p-1)\\sigma^{2} a^{p-2}\n\\end{pmatrix}\n}\n$$", "id": "3469670"}]}