{"hands_on_practices": [{"introduction": "Proximal operators are fundamental building blocks in modern convex optimization, enabling efficient algorithms for non-differentiable problems. This exercise demonstrates their power through the lens of duality, using the Moreau decomposition to connect the proximal operator of the $\\ell_{\\infty}$ norm to a projection onto the dual $\\ell_{1}$ ball. By working through this calculation [@problem_id:3448183], you will gain a concrete understanding of how abstract dual relationships manifest in practical computational steps.", "problem": "Let $n \\in \\mathbb{N}$ and consider the $\\ell_{p}$ norms on $\\mathbb{R}^{n}$, with the dual-norm relation that the dual of $\\|\\cdot\\|_{\\infty}$ is $\\|\\cdot\\|_{1}$. Denote the closed $\\ell_{\\infty}$ ball of radius $\\alpha$ by $B_{\\infty}^{n}(\\alpha) \\coloneqq \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{\\infty} \\leq \\alpha\\}$ and the closed $\\ell_{1}$ ball of radius $\\lambda$ by $B_{1}^{n}(\\lambda) \\coloneqq \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{1} \\leq \\lambda\\}$. The proximal operator of a proper, closed, convex function $f$ is $\\operatorname{prox}_{f}(v) \\coloneqq \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ f(x) + \\frac{1}{2}\\|x - v\\|_{2}^{2} \\right\\}$, and the convex conjugate is $f^{*}(y) \\coloneqq \\sup_{x \\in \\mathbb{R}^{n}} \\{\\langle y, x \\rangle - f(x)\\}$.\n\nStarting from these definitions and the general Moreau decomposition for convex functions and their convex conjugates, do the following:\n\n1. Using only the definition of Euclidean projection and separability, derive that the Euclidean projection onto the $\\ell_{\\infty}$ ball is coordinate-wise clamping: for any $v \\in \\mathbb{R}^{n}$ and any $\\alpha > 0$, the projection $\\Pi_{B_{\\infty}^{n}(\\alpha)}(v)$ satisfies, componentwise, $[\\Pi_{B_{\\infty}^{n}(\\alpha)}(v)]_{i} = \\min\\{\\max\\{v_{i}, -\\alpha\\}, \\alpha\\}$.\n\n2. Using the dual norm relation and the convex conjugate of the $\\ell_{\\infty}$ norm, specialize the Moreau decomposition to obtain an expression for $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{\\infty}}(v)$ in terms of a Euclidean projection onto an $\\ell_{1}$ ball.\n\n3. Apply your expression to compute $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{\\infty}}(v)$ for \n$$\nv = \\begin{pmatrix} 4 \\\\ -3 \\\\ 2 \\\\ -1 \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix} \\in \\mathbb{R}^{6}, \\quad \\lambda = 4.\n$$\n\nExpress your final answer as a single row vector with exact rational entries. No rounding is required. Provide only the proximal vector as your final answer.", "solution": "The problem is valid as it is scientifically grounded in the principles of convex analysis and optimization, is well-posed with all necessary definitions, and is stated objectively. We proceed with the solution in three parts as requested.\n\nPart 1: Derivation of the Euclidean projection onto the $\\ell_{\\infty}$ ball.\n\nThe Euclidean projection of a vector $v \\in \\mathbb{R}^{n}$ onto a closed convex set $C \\subset \\mathbb{R}^{n}$ is defined as the unique point in $C$ that is closest to $v$ in the Euclidean norm. It is the solution to the optimization problem:\n$$\n\\Pi_{C}(v) \\coloneqq \\arg\\min_{x \\in C} \\frac{1}{2}\\|x - v\\|_{2}^{2}\n$$\nIn this problem, the set $C$ is the closed $\\ell_{\\infty}$ ball of radius $\\alpha > 0$, denoted $B_{\\infty}^{n}(\\alpha)$. This set is defined as:\n$$\nB_{\\infty}^{n}(\\alpha) = \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{\\infty} \\leq \\alpha\\} = \\{x \\in \\mathbb{R}^{n} : \\max_{i=1,\\dots,n} |x_{i}| \\leq \\alpha\\}\n$$\nThe constraint $\\|x\\|_{\\infty} \\leq \\alpha$ is equivalent to a set of component-wise constraints: $|x_{i}| \\leq \\alpha$ for all $i \\in \\{1, \\dots, n\\}$. Each of these can be written as $-\\alpha \\leq x_{i} \\leq \\alpha$.\n\nThe objective function is the squared Euclidean norm, which is separable with respect to the components of $x$:\n$$\n\\frac{1}{2}\\|x - v\\|_{2}^{2} = \\frac{1}{2}\\sum_{i=1}^{n} (x_{i} - v_{i})^{2}\n$$\nSince both the objective function and the constraints are separable, the $n$-dimensional optimization problem can be decomposed into $n$ independent one-dimensional optimization problems. For each component $i$, we solve:\n$$\n\\min_{x_i} \\frac{1}{2}(x_{i} - v_{i})^{2} \\quad \\text{subject to} \\quad -\\alpha \\leq x_{i} \\leq \\alpha\n$$\nThis is a simple problem of finding the point in the interval $[-\\alpha, \\alpha]$ that is closest to $v_{i}$. The solution, which we denote as $[\\Pi_{B_{\\infty}^{n}(\\alpha)}(v)]_{i}$, is given by:\n1. If $v_{i}$ is already within the interval, i.e., $-\\alpha \\leq v_{i} \\leq \\alpha$, the closest point is $v_{i}$ itself. So, $[\\Pi_{B_{\\infty}^{n}(\\alpha)}(v)]_{i} = v_{i}$.\n2. If $v_{i}$ is greater than the upper bound, i.e., $v_{i} > \\alpha$, the closest point is the upper bound. So, $[\\Pi_{B_{\\infty}^{n}(\\alpha)}(v)]_{i} = \\alpha$.\n3. If $v_{i}$ is less than the lower bound, i.e., $v_{i} < -\\alpha$, the closest point is the lower bound. So, $[\\Pi_{B_{\\infty}^{n}(\\alpha)}(v)]_{i} = -\\alpha$.\n\nThese three cases can be combined into a single expression using min and max functions:\n$$\n[\\Pi_{B_{\\infty}^{n}(\\alpha)}(v)]_{i} = \\min\\{\\alpha, \\max\\{-\\alpha, v_{i}\\}\\}\n$$\nThis operation is known as clamping or clipping, which completes the derivation.\n\nPart 2: Expression for $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{\\infty}}(v)$ using Moreau decomposition.\n\nMoreau's decomposition states that for any proper, closed, convex function $f: \\mathbb{R}^{n} \\to \\mathbb{R} \\cup \\{\\infty\\}$ and its convex conjugate $f^{*}$, any vector $v \\in \\mathbb{R}^{n}$ can be uniquely decomposed as:\n$$\nv = \\operatorname{prox}_{f}(v) + \\operatorname{prox}_{f^{*}}(v)\n$$\nWe are interested in the proximal operator of the function $f(x) = \\lambda \\|x\\|_{\\infty}$, where $\\lambda > 0$. First, we compute the convex conjugate $f^{*}(y)$:\n$$\nf^{*}(y) = \\sup_{x \\in \\mathbb{R}^{n}} \\{\\langle y, x \\rangle - f(x)\\} = \\sup_{x \\in \\mathbb{R}^{n}} \\{\\langle y, x \\rangle - \\lambda \\|x\\|_{\\infty}\\}\n$$\nWe analyze two cases based on the $\\ell_{1}$ norm of $y$:\n\nCase 1: $\\|y\\|_{1} > \\lambda$. Let us choose a vector $x_c = c \\cdot \\text{sign}(y)$ for a scalar $c > 0$, where $\\text{sign}(y)$ is the component-wise sign vector. Then $\\langle y, x_c \\rangle = c \\sum_{i} y_i \\text{sign}(y_i) = c \\sum_{i} |y_i| = c \\|y\\|_{1}$. The $\\ell_{\\infty}$ norm is $\\|x_c\\|_{\\infty} = \\max_i |c \\cdot \\text{sign}(y_i)| = c$. The expression becomes $c\\|y\\|_{1} - \\lambda c = c(\\|y\\|_{1} - \\lambda)$. Since $\\|y\\|_{1} > \\lambda$, this quantity is positive and approaches $\\infty$ as $c \\to \\infty$. Thus, $f^{*}(y) = \\infty$.\n\nCase 2: $\\|y\\|_{1} \\leq \\lambda$. By Hölder's inequality, $|\\langle y, x \\rangle| \\leq \\|y\\|_{1} \\|x\\|_{\\infty}$. This gives $\\langle y, x \\rangle \\leq \\|y\\|_{1} \\|x\\|_{\\infty}$. The expression for the supremum is therefore bounded:\n$$\n\\langle y, x \\rangle - \\lambda \\|x\\|_{\\infty} \\leq \\|y\\|_{1} \\|x\\|_{\\infty} - \\lambda \\|x\\|_{\\infty} = (\\|y\\|_{1} - \\lambda)\\|x\\|_{\\infty}\n$$\nSince $\\|y\\|_{1} \\leq \\lambda$, the term $(\\|y\\|_{1} - \\lambda) \\leq 0$. As $\\|x\\|_{\\infty} \\geq 0$, the entire expression is non-positive. The maximum value of $0$ is achieved when $x = 0$. Thus, $f^{*}(y) = 0$.\n\nCombining these results, the convex conjugate $f^{*}(y)$ is the indicator function of the closed $\\ell_{1}$ ball of radius $\\lambda$:\n$$\nf^{*}(y) = I_{B_{1}^{n}(\\lambda)}(y) \\coloneqq \\begin{cases} 0 & \\text{if } \\|y\\|_{1} \\leq \\lambda \\\\ \\infty & \\text{if } \\|y\\|_{1} > \\lambda \\end{cases}\n$$\nNow, we find the proximal operator of $f^{*}$:\n$$\n\\operatorname{prox}_{f^{*}}(v) = \\operatorname{prox}_{I_{B_{1}^{n}(\\lambda)}}(v) = \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ I_{B_{1}^{n}(\\lambda)}(x) + \\frac{1}{2}\\|x - v\\|_{2}^{2} \\right\\}\n$$\nThe indicator function restricts the domain of minimization to the set $B_{1}^{n}(\\lambda)$. The problem is equivalent to:\n$$\n\\arg\\min_{x \\in B_{1}^{n}(\\lambda)} \\frac{1}{2}\\|x - v\\|_{2}^{2}\n$$\nThis is the definition of the Euclidean projection of $v$ onto the $\\ell_{1}$ ball $B_{1}^{n}(\\lambda)$, which is denoted by $\\Pi_{B_{1}^{n}(\\lambda)}(v)$.\n\nSubstituting this result into Moreau's decomposition:\n$$\nv = \\operatorname{prox}_{\\lambda \\|\\cdot\\|_{\\infty}}(v) + \\Pi_{B_{1}^{n}(\\lambda)}(v)\n$$\nRearranging for the desired term, we obtain the expression:\n$$\n\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{\\infty}}(v) = v - \\Pi_{B_{1}^{n}(\\lambda)}(v)\n$$\n\nPart 3: Application to a specific vector.\n\nWe must compute $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{\\infty}}(v)$ for $v = \\begin{pmatrix} 4 & -3 & 2 & -1 & \\frac{1}{2} & 0 \\end{pmatrix}^{T}$ and $\\lambda=4$. Let $n=6$.\nUsing the formula from Part 2, the result is $x = v - \\Pi_{B_{1}^{6}(4)}(v)$.\n\nFirst, we compute the projection $u = \\Pi_{B_{1}^{6}(4)}(v)$. We check the $\\ell_{1}$ norm of $v$:\n$$\n\\|v\\|_{1} = |4| + |-3| + |2| + |-1| + |\\frac{1}{2}| + |0| = 4 + 3 + 2 + 1 + 0.5 + 0 = 10.5\n$$\nSince $\\|v\\|_{1} > 4$, the projection $u$ is not $v$ itself, but a point on the boundary of the $\\ell_{1}$ ball, meaning $\\|u\\|_{1} = 4$.\nThe projection onto an $\\ell_{1}$ ball is given by the soft-thresholding operator, $u_i = \\text{sign}(v_i) \\max\\{0, |v_i| - \\theta\\}$, for a threshold $\\theta \\ge 0$ chosen such that $\\|u\\|_{1} = \\lambda$. We need to find $\\theta > 0$ that solves the equation:\n$$\n\\sum_{i=1}^{6} \\max\\{0, |v_i| - \\theta\\} = 4\n$$\nThe absolute values of the components of $v$ are $\\{4, 3, 2, 1, 0.5, 0\\}$. Let $g(\\theta) = \\sum_{i} \\max\\{0, |v_i| - \\theta\\}$.\n$g(\\theta)$ is a continuous, non-increasing, piecewise linear function of $\\theta$. We test values of $\\theta$ in intervals defined by the sorted magnitudes of $v_i$.\nFor $\\theta \\in [1, 2]$, the equation becomes:\n$$\n(4 - \\theta) + (3 - \\theta) + (2 - \\theta) = 9 - 3\\theta\n$$\nWe set this equal to $4$:\n$$\n9 - 3\\theta = 4 \\implies 3\\theta = 5 \\implies \\theta = \\frac{5}{3}\n$$\nSince $1 < \\frac{5}{3} \\approx 1.67 \\leq 2$, this is the correct value of $\\theta$.\n\nNow we compute the components of the projection $u$:\n$u_{1} = \\text{sign}(4) \\max\\{0, 4 - \\frac{5}{3}\\} = 1 \\cdot (\\frac{12}{3} - \\frac{5}{3}) = \\frac{7}{3}$.\n$u_{2} = \\text{sign}(-3) \\max\\{0, 3 - \\frac{5}{3}\\} = -1 \\cdot (\\frac{9}{3} - \\frac{5}{3}) = -\\frac{4}{3}$.\n$u_{3} = \\text{sign}(2) \\max\\{0, 2 - \\frac{5}{3}\\} = 1 \\cdot (\\frac{6}{3} - \\frac{5}{3}) = \\frac{1}{3}$.\n$u_{4} = \\text{sign}(-1) \\max\\{0, 1 - \\frac{5}{3}\\} = -1 \\cdot 0 = 0$.\n$u_{5} = \\text{sign}(\\frac{1}{2}) \\max\\{0, \\frac{1}{2} - \\frac{5}{3}\\} = 1 \\cdot 0 = 0$.\n$u_{6} = \\text{sign}(0) \\max\\{0, 0 - \\frac{5}{3}\\} = 0$.\nSo, $u = \\Pi_{B_{1}^{6}(4)}(v) = \\begin{pmatrix} \\frac{7}{3} & -\\frac{4}{3} & \\frac{1}{3} & 0 & 0 & 0 \\end{pmatrix}^{T}$.\n\nFinally, we compute the proximal vector $x = v - u$:\n$x_{1} = 4 - \\frac{7}{3} = \\frac{12 - 7}{3} = \\frac{5}{3}$.\n$x_{2} = -3 - (-\\frac{4}{3}) = -\\frac{9}{3} + \\frac{4}{3} = -\\frac{5}{3}$.\n$x_{3} = 2 - \\frac{1}{3} = \\frac{6 - 1}{3} = \\frac{5}{3}$.\n$x_{4} = -1 - 0 = -1$.\n$x_{5} = \\frac{1}{2} - 0 = \\frac{1}{2}$.\n$x_{6} = 0 - 0 = 0$.\n\nThe resulting proximal vector is $x = \\begin{pmatrix} \\frac{5}{3} & -\\frac{5}{3} & \\frac{5}{3} & -1 & \\frac{1}{2} & 0 \\end{pmatrix}^{T}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{5}{3} & -\\frac{5}{3} & \\frac{5}{3} & -1 & \\frac{1}{2} & 0\n\\end{pmatrix}\n}\n$$", "id": "3448183"}, {"introduction": "Having seen the utility of projecting onto an $\\ell_1$ ball, this practice challenges you to build that essential tool from the ground up. You will derive the projection algorithm starting from the Karush-Kuhn-Tucker (KKT) optimality conditions, a cornerstone of constrained optimization. Implementing this algorithm [@problem_id:3448189] translates theory into a tangible, efficient function, a crucial skill for anyone developing sparse optimization solvers.", "problem": "Consider the unit $\\ell_1$ ball in $\\mathbb{R}^n$ defined by $B_1^n = \\{ x \\in \\mathbb{R}^n : \\|x\\|_1 \\le 1 \\}$. The Euclidean projection of a vector $v \\in \\mathbb{R}^n$ onto $B_1^n$ is the unique solution $x^\\star$ to the convex optimization problem $\\min_{x \\in \\mathbb{R}^n} \\tfrac{1}{2}\\|x - v\\|_2^2$ subject to $\\|x\\|_1 \\le 1$. Your task is to derive, from first principles appropriate to compressed sensing and sparse optimization, the sorted-thresholding algorithm for computing $x^\\star$, prove the algorithm’s correctness, analyze its computational complexity, and implement it.\n\nDerivation requirements:\n- Begin from core definitions and principles: convex projection; the Karush–Kuhn–Tucker (KKT) optimality conditions for constrained convex optimization; the subdifferential of the $\\ell_1$ norm; and the basic relationship between a norm and its dual norm (the dual of the $\\ell_1$ norm is the $\\ell_\\infty$ norm).\n- Do not assume any target formulas for thresholds or sorted values. Derive the full algorithm logically from the stationarity, feasibility, complementary slackness, and dual feasibility conditions.\n- Conclude with an implementable algorithm that uses sorted magnitudes of $v$ to determine a global threshold and applies soft-thresholding coordinatewise.\n\nCorrectness requirements:\n- Prove existence and uniqueness of the projection $x^\\star$ using properties of strictly convex objectives and convex closed feasible sets.\n- Establish the precise soft-thresholding form of the solution and the consistency of the threshold computed by the sorted procedure.\n- Connect the construction to the geometry of $\\ell_p$ balls via dual norms, showing how the dual $\\ell_\\infty$ norm bounds the subgradient at the solution.\n\nComplexity requirements:\n- Provide a tight bound for the algorithm’s asymptotic complexity in terms of $n$, highlighting contributions from sorting and linear passes.\n- Discuss possible improvements and the trade-offs involved (e.g., partial selection versus full sort).\n\nImplementation requirements:\n- Implement a function that, given $v \\in \\mathbb{R}^n$, returns the projection $x^\\star$ onto $B_1^n$, along with the computed threshold. The implementation must be purely numerical with no external input and must respect numerical tolerances in comparisons.\n\nTest suite:\n- Use the following fixed test vectors, each as a list of real numbers, where $n$ varies:\n    1. $v_1 = [0.3, -0.2, 0.5, 0.0]$ (boundary case with $\\|v_1\\|_1 = 1$).\n    2. $v_2 = [0.0, 0.0, 0.0]$ (inside the ball).\n    3. $v_3 = [3.0, -1.0, 0.0, 2.0]$ (outside the ball with mixed signs).\n    4. $v_4 = [0.5, 0.5, 0.5, 0.5]$ (outside with equal magnitudes).\n    5. $v_5 = [-0.7, -0.7, -0.7]$ (outside with equal negative magnitudes).\n    6. $v_6 = [10^{-12}, -10^{-12}, 10^{-12}, -10^{-12}]$ (tiny magnitudes well inside).\n    7. $v_7 = [2.0]$ (one-dimensional, outside).\n    8. $v_8 = [0.4, -0.6]$ (boundary case with $\\|v_8\\|_1 = 1$).\n- For each test vector $v$, compute $x^\\star$ and a numerical threshold $\\theta$. Let $\\|\\cdot\\|_p$ denote the $\\ell_p$ norm and use a tolerance $\\varepsilon = 10^{-10}$ for all inequality checks.\n- For each test case, you must compute and output the following three quantities, in order:\n    1. $\\|x^\\star\\|_1$ as a float.\n    2. A boolean indicating whether $v$ is inside the unit $\\ell_1$ ball, i.e., whether $\\|v\\|_1 \\le 1$ holds within tolerance.\n    3. A boolean indicating whether the numerically checked KKT conditions hold for the computed $x^\\star$ and $\\theta$:\n        - If $\\|v\\|_1 \\le 1$, verify that $x^\\star = v$ within tolerance.\n        - If $\\|v\\|_1 > 1$, verify that $\\|x^\\star\\|_1$ equals $1$ within tolerance; for all indices $i$ with $x^\\star_i \\ne 0$, check $x^\\star_i = \\operatorname{sign}(v_i)\\max(|v_i| - \\theta, 0)$ within tolerance; and for all indices $i$ with $x^\\star_i = 0$, check $|v_i| \\le \\theta + \\varepsilon$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must concatenate the three outputs for each test case in order, yielding $24$ entries total:\n  $[\\|x^\\star(v_1)\\|_1,\\ \\text{inside}(v_1),\\ \\text{KKT\\_ok}(v_1),\\ \\ldots,\\ \\|x^\\star(v_8)\\|_1,\\ \\text{inside}(v_8),\\ \\text{KKT\\_ok}(v_8)]$.", "solution": "The user-provided problem is evaluated for validity.\n\n### Step 1: Extract Givens\n- **Problem Domain**: Projection onto the unit $\\ell_1$ ball in $\\mathbb{R}^n$.\n- **Feasible Set**: $B_1^n = \\{ x \\in \\mathbb{R}^n : \\|x\\|_1 \\le 1 \\}$.\n- **Optimization Problem**: $\\min_{x \\in \\mathbb{R}^n} \\tfrac{1}{2}\\|x - v\\|_2^2$ subject to $\\|x\\|_1 \\le 1$. The solution is denoted $x^\\star$.\n- **Task**: Derive, prove correctness, analyze complexity, and implement the sorted-thresholding algorithm for computing $x^\\star$.\n- **Derivation Requirements**: Use first principles including convex projection, KKT conditions, subdifferentials, and dual norms. Do not assume formulas.\n- **Correctness Requirements**: Prove existence/uniqueness, establish the soft-thresholding form, and connect to dual norms.\n- **Complexity Requirements**: Provide a tight asymptotic bound and discuss improvements.\n- **Implementation Requirements**: Implement a function taking $v$ and returning $x^\\star$ and the threshold $\\theta$.\n- **Test Suite**: A list of $8$ specific test vectors $v_1, \\dots, v_8$ is provided.\n- **Output Metrics**: For each test vector $v$, compute and report:\n    1. $\\|x^\\star\\|_1$ (float).\n    2. A boolean for whether $\\|v\\|_1 \\le 1$ (within tolerance $\\varepsilon = 10^{-10}$).\n    3. A boolean for whether the computed $x^\\star$ and $\\theta$ satisfy the KKT conditions numerically (within tolerance $\\varepsilon = 10^{-10}$).\n- **Final Output Format**: A single-line comma-separated list of $24$ results enclosed in square brackets.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is a standard exercise in convex optimization and has significant applications in signal processing, statistics, and machine learning (e.g., as a substep in an ADMM solver for Lasso). All concepts involved—convex sets, $\\ell_p$ norms, KKT conditions, subdifferentials, dual norms—are fundamental and well-established in mathematics. The problem is scientifically and mathematically sound.\n2.  **Well-Posed**: The objective function $f(x) = \\frac{1}{2}\\|x - v\\|_2^2$ is strictly convex. The feasible set $B_1^n$ is a non-empty, closed, convex set. The projection of a point onto a non-empty, closed, convex set in a Hilbert space exists and is unique. Therefore, the problem is well-posed, admitting a unique solution $x^\\star$.\n3.  **Objective**: The problem is stated in precise mathematical language, free from ambiguity, subjectivity, or opinion.\n4.  **Completeness**: The problem statement is self-contained. It provides the objective function, the constraint set, and clear requirements for the derivation, proof, analysis, and implementation, including specific test cases and output formats.\n5.  **Triviality**: The derivation of the algorithm from KKT conditions is a non-trivial exercise that requires careful handling of the non-differentiable $\\ell_1$ norm and its subdifferential. It is a canonical but substantive problem.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a well-defined, standard problem in convex optimization with a clear set of requirements. The solution process may proceed.\n\n### Solution\nThe problem is to find the Euclidean projection of a vector $v \\in \\mathbb{R}^n$ onto the closed unit $\\ell_1$ ball, $B_1^n$. This is equivalent to solving the constrained convex optimization problem:\n$$\nx^\\star = \\arg\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2} \\|x - v\\|_2^2 \\quad \\text{subject to} \\quad \\|x\\|_1 \\le 1\n$$\nLet the objective function be $f(x) = \\frac{1}{2}\\|x - v\\|_2^2$ and the inequality constraint be $g(x) = \\|x\\|_1 - 1 \\le 0$.\n\n**1. Existence and Uniqueness of the Solution**\nThe objective function $f(x)$ is a quadratic function whose Hessian is the identity matrix, $\\nabla^2 f(x) = I$. Since the identity matrix is positive definite, $f(x)$ is strictly convex. The feasible set $B_1^n = \\{x \\in \\mathbb{R}^n : g(x) \\le 0\\}$ is a convex set (as all norm balls are convex), closed, and non-empty (e.g., $0 \\in B_1^n$). The minimization of a strictly convex function over a non-empty, closed, convex set has a unique solution. Thus, a unique projection $x^\\star$ is guaranteed to exist for any $v \\in \\mathbb{R}^n$.\n\n**2. Derivation from Karush–Kuhn–Tucker (KKT) Conditions**\nSince the problem is convex and satisfies Slater's condition (e.g., $x=0$ is strictly feasible as $\\|0\\|_1 < 1$), the KKT conditions are necessary and sufficient for optimality. We form the Lagrangian:\n$$\nL(x, \\lambda) = f(x) + \\lambda g(x) = \\frac{1}{2} \\|x - v\\|_2^2 + \\lambda (\\|x\\|_1 - 1)\n$$\nwhere $\\lambda \\in \\mathbb{R}$ is the Lagrange multiplier associated with the inequality constraint. The KKT conditions for an optimal solution $x^\\star$ are:\n1.  **Stationarity**: $0 \\in \\nabla_x L(x^\\star, \\lambda) = \\nabla f(x^\\star) + \\lambda \\partial_x g(x^\\star)$. Since $g(x)$ involves the non-differentiable $\\ell_1$ norm, we use the subdifferential.\n2.  **Primal Feasibility**: $\\|x^\\star\\|_1 - 1 \\le 0$.\n3.  **Dual Feasibility**: $\\lambda \\ge 0$.\n4.  **Complementary Slackness**: $\\lambda (\\|x^\\star\\|_1 - 1) = 0$.\n\nThe gradient of $f(x)$ is $\\nabla f(x) = x - v$. The subdifferential of $g(x)$ is $\\partial g(x) = \\partial \\|x\\|_1$. The stationarity condition thus becomes:\n$$\n0 \\in x^\\star - v + \\lambda \\partial \\|x^\\star\\|_1 \\quad \\iff \\quad v - x^\\star \\in \\lambda \\partial \\|x^\\star\\|_1\n$$\nWe analyze two cases based on the position of $v$.\n\n**Case 1: $v$ is inside or on the boundary of the $\\ell_1$ ball ($ \\|v\\|_1 \\le 1 $)**\nLet's test the hypothesis that $x^\\star = v$.\n- **Primal Feasibility**: $\\|x^\\star\\|_1 = \\|v\\|_1 \\le 1$. This condition is satisfied.\n- To satisfy **Complementary Slackness**, if $\\|v\\|_1 < 1$, we must have $\\lambda = 0$. If $\\|v\\|_1 = 1$, $\\lambda$ can be non-negative. Let's try $\\lambda=0$.\n- **Dual Feasibility**: $\\lambda = 0 \\ge 0$. Satisfied.\n- **Stationarity**: With $x^\\star = v$ and $\\lambda = 0$, the condition becomes $v - v \\in 0 \\cdot \\partial \\|v\\|_1$, which simplifies to $0 \\in \\{0\\}$. This is true.\nSince all KKT conditions are satisfied with $x^\\star = v$ and $\\lambda = 0$, this is the unique optimal solution.\n\n**Case 2: $v$ is outside the $\\ell_1$ ball ($ \\|v\\|_1 > 1 $)**\nIn this case, $x^\\star \\ne v$. For the solution to be as close as possible to $v$, it must lie on the boundary of the feasible set.\n- **Primal Feasibility**: We must have $\\|x^\\star\\|_1 = 1$.\n- **Complementary Slackness**: $\\lambda (\\|x^\\star\\|_1 - 1) = \\lambda (1-1) = 0$. This condition is satisfied for any $\\lambda$.\n- **Dual Feasibility**: We need to show $\\lambda > 0$. If we assume $\\lambda=0$, stationarity would give $x^\\star - v = 0$, so $x^\\star = v$. But this contradicts primal feasibility, as $\\|v\\|_1 > 1$. Therefore, we must have $\\lambda > 0$.\n- **Stationarity**: $v - x^\\star \\in \\lambda \\partial \\|x^\\star\\|_1$. We analyze this component-wise. The subdifferential of the $\\ell_1$ norm is given by $(\\partial \\|x\\|_1)_i = \\{\\operatorname{sign}(x_i)\\}$ if $x_i \\ne 0$, and $(\\partial \\|x\\|_1)_i = [-1, 1]$ if $x_i = 0$.\n\nThe stationarity condition $v_i - x_i^\\star \\in \\lambda (\\partial \\|x^\\star\\|_1)_i$ for each component $i=1, \\dots, n$ implies:\n- If $x_i^\\star \\ne 0$: $v_i - x_i^\\star = \\lambda \\operatorname{sign}(x_i^\\star)$. Intuitively, to minimize distance, the sign of a non-zero component of the projection $x_i^\\star$ must match the sign of $v_i$. Thus, $\\operatorname{sign}(x_i^\\star) = \\operatorname{sign}(v_i)$.\n  This gives $x_i^\\star = v_i - \\lambda \\operatorname{sign}(v_i) = \\operatorname{sign}(v_i)(|v_i| - \\lambda)$. Since $|x_i^\\star| > 0$, we must have $|v_i| > \\lambda$.\n- If $x_i^\\star = 0$: $v_i - 0 \\in \\lambda [-1, 1]$, which means $|v_i| \\le \\lambda$.\n\nCombining these, we can express the solution for each component using a a single formula involving a positive threshold, which we denote $\\theta = \\lambda$:\n$$\nx_i^\\star = \\operatorname{sign}(v_i) \\max(0, |v_i| - \\theta)\n$$\nThis is the well-known soft-thresholding operator, $S_\\theta(v_i)$.\nTo find the value of the threshold $\\theta > 0$, we use the only remaining condition, $\\|x^\\star\\|_1 = 1$:\n$$\n\\sum_{i=1}^n |x_i^\\star| = \\sum_{i=1}^n \\max(0, |v_i| - \\theta) = 1\n$$\nLet's define a function $h(\\theta) = \\sum_{i=1}^n \\max(0, |v_i| - \\theta)$. We need to find the root $\\theta > 0$ of $h(\\theta) = 1$. The function $h(\\theta)$ is continuous, non-increasing, and piecewise linear. At $\\theta=0$, $h(0) = \\sum |v_i| = \\|v\\|_1 > 1$. As $\\theta \\to \\max_i |v_i|$ and beyond, $h(\\theta) \\to 0$. By the Intermediate Value Theorem, a unique solution $\\theta > 0$ exists.\n\nTo find this $\\theta$ systematically, let us sort the absolute values of $v$ in descending order: $|v|_{(1)} \\ge |v|_{(2)} \\ge \\dots \\ge |v|_{(n)}$. The breakpoints of the piecewise linear function $h(\\theta)$ occur at these values. Suppose the solution $\\theta$ lies in the interval $(|v|_{(\\rho+1)}, |v|_{(\\rho)}]$ for some index $\\rho \\in \\{1, \\dots, n\\}$ (where we define $|v|_{(n+1)} = 0$). For such a $\\theta$, exactly $\\rho$ components of $x^\\star$ are non-zero. The equation $h(\\theta)=1$ becomes:\n$$\n\\sum_{i=1}^{\\rho} (|v|_{(i)} - \\theta) + \\sum_{i=\\rho+1}^{n} 0 = 1\n$$\n$$\n\\left(\\sum_{i=1}^{\\rho} |v|_{(i)}\\right) - \\rho\\theta = 1 \\quad \\implies \\quad \\theta = \\frac{1}{\\rho}\\left(\\sum_{i=1}^{\\rho} |v|_{(i)} - 1\\right)\n$$\nThis value of $\\theta$ is valid only if it falls in the assumed interval, i.e., $|v|_{(\\rho+1)} < \\theta \\le |v|_{(\\rho)}$, which is equivalent to $\\theta > |v|_{(\\rho+1)}$ and $\\theta- |v|_{(\\rho)} \\le 0$. The second part can be rewritten as\n$ \\frac{1}{\\rho}\\left(\\sum_{i=1}^{\\rho} |v|_{(i)} - 1\\right) \\le |v|_{(\\rho)} \\iff \\sum_{i=1}^{\\rho} |v|_{(i)} - 1 \\le \\rho |v|_{(\\rho)} \\iff \\sum_{i=1}^{\\rho} (|v|_{(i)} - |v|_{(\\rho)}) \\ge 1$. This is always true for $\\rho > 1$. The key condition is the first one. We need to find $\\rho$ such that the computed $\\theta$ is larger than $|v|_{(\\rho+1)}$. The function $(\\sum_{i=1}^k |v|_{(i)} - 1)/k$ is not monotonic in $k$, so we must find the correct $\\rho$ by checking the condition. The valid $\\rho$ is the largest integer $k \\in \\{1, \\dots, n\\}$ that satisfies $|v|_{(k)} > \\frac{1}{k}(\\sum_{i=1}^k |v|_{(i)} - 1)$.\n\n**3. Connection to Dual Norms and Geometric Interpretation**\nThe stationarity condition $v - x^\\star = z$ where $z \\in \\lambda \\partial \\|x^\\star\\|_1$ is key. A vector $s$ is a subgradient of the $\\ell_1$ norm at $x$, i.e., $s \\in \\partial\\|x\\|_1$, if and only if $\\|s\\|_\\infty \\le 1$ and $s^T x = \\|x\\|_1$. The $\\ell_\\infty$ norm is the dual norm of the $\\ell_1$ norm.\nSo, the vector $z/\\lambda$ must be a subgradient of $\\|x^\\star\\|_1$. This means $\\|(v - x^\\star)/\\lambda\\|_\\infty \\le 1$, which implies $\\|v - x^\\star\\|_\\infty \\le \\lambda = \\theta$.\nThis gives a geometric view: the vector from the solution $x^\\star$ to the original point $v$ has its largest component (in absolute value) bounded by the Lagrange multiplier $\\theta$. For components $i$ where $x_i^\\star \\ne 0$, we have $|v_i - x_i^\\star| = \\theta$. For components $i$ where $x_i^\\star = 0$, we have $|v_i| \\le \\theta$. This is precisely what the soft-thresholding operator implements.\n\n**4. Algorithmic Summary and Complexity Analysis**\nThe complete algorithm is as follows:\n1.  Given $v \\in \\mathbb{R}^n$.\n2.  Compute $\\|v\\|_1$. If $\\|v\\|_1 \\le 1$, the solution is $x^\\star = v$ and the threshold is $\\theta = 0$.\n3.  If $\\|v\\|_1 > 1$:\n    a. Create a vector $u$ of the absolute values of $v$: $u_i = |v_i|$.\n    b. Sort $u$ in descending order to get $u_{(1)} \\ge u_{(2)} \\ge \\dots \\ge u_{(n)}$.\n    c. Find the largest integer $\\rho \\in \\{1, \\dots, n\\}$ such that $u_{(\\rho)} - \\frac{1}{\\rho}(\\sum_{i=1}^{\\rho} u_{(i)} - 1) > 0$.\n    d. Compute the threshold $\\theta = \\frac{1}{\\rho}(\\sum_{i=1}^{\\rho} u_{(i)} - 1)$.\n    e. The solution is given by $x_i^\\star = \\operatorname{sign}(v_i) \\max(0, |v_i| - \\theta)$ for all $i$.\n\n**Complexity Analysis**:\n- The dominant operation is sorting the absolute values of $v$. This has a time complexity of $O(n \\log n)$.\n- Computing the cumulative sums takes $O(n)$ time.\n- Finding the index $\\rho$ takes one pass, which is $O(n)$.\n- The final soft-thresholding step is also $O(n)$.\n- Therefore, the total computational complexity is bounded by the sort: **$O(n \\log n)$**.\n\n**Possible Improvements**: A full sort is not strictly necessary. The problem is to find a value $\\theta$ such that the set of indices $\\{i : |v_i| > \\theta\\}$ has cardinality $\\rho$ and $\\theta = \\frac{1}{\\rho}(\\sum_{|v_i|>\\theta} |v_i| - 1)$. This structure allows for an $O(n)$ solution using a median-of-medians selection algorithm to find the correct pivot value. However, such algorithms are more complex to implement and have a larger constant factor, so for moderately sized $n$, an $O(n \\log n)$ sort-based approach is often preferred for its simplicity and practical performance.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the projection algorithm on a suite of test cases.\n    \"\"\"\n    \n    # Tolerance for floating-point comparisons\n    EPS = 1e-10\n\n    def project_l1_ball(v: np.ndarray) -> tuple[np.ndarray, float]:\n        \"\"\"\n        Projects a vector v onto the L1 unit ball.\n\n        Args:\n            v: A numpy array representing the vector to project.\n\n        Returns:\n            A tuple containing:\n            - The projected vector x_star.\n            - The computed threshold theta.\n        \"\"\"\n        # Trivial case: if v is already in the ball, the projection is v itself.\n        if np.sum(np.abs(v)) <= 1.0 + EPS:\n            return v.copy(), 0.0\n\n        # Non-trivial case: v is outside the ball.\n        # Sort the absolute values of v in descending order.\n        u = np.sort(np.abs(v))[::-1]\n        \n        # Compute cumulative sums of the sorted absolute values.\n        cssv = np.cumsum(u)\n        \n        # Find the correct rho (number of non-zero elements in the projection).\n        # We need to find the largest k such that u_k > (S_k - 1) / k.\n        # This is equivalent to k * u_k - S_k > -1.\n        # The following vectorized operation finds all k satisfying this and takes the last one.\n        rho_candidates = np.arange(1, len(v) + 1)\n        \n        # The condition from the derivation is u_k - (cssv_k - 1)/k > 0\n        # We find all indices k that satisfy the condition and rho is the largest one.\n        # In NumPy, indices are 0-based, so rho will be the index + 1.\n        indices = np.where(u > (cssv - 1) / rho_candidates)[0]\n        \n        # If indices is empty, it's a floating point edge case. rho is n.\n        if len(indices) == 0:\n            rho = len(v)\n        else:\n            rho = indices[-1] + 1\n        \n        # Compute the threshold theta.\n        theta = (cssv[rho - 1] - 1) / rho\n        \n        # Apply the soft-thresholding operator.\n        x_star = np.sign(v) * np.maximum(0, np.abs(v) - theta)\n        \n        return x_star, theta\n\n    def check_kkt(v: np.ndarray, x_star: np.ndarray, theta: float, is_inside: bool) -> bool:\n        \"\"\"\n        Numerically verifies the KKT conditions for the projection.\n\n        Args:\n            v: The original vector.\n            x_star: The computed projection.\n            theta: The computed threshold (Lagrange multiplier).\n            is_inside: Boolean indicating if ||v||_1 <= 1.\n\n        Returns:\n            True if KKT conditions hold within tolerance, False otherwise.\n        \"\"\"\n        if is_inside:\n            # If v was inside, x_star must be equal to v.\n            return np.allclose(x_star, v, atol=EPS)\n        else:\n            # If v was outside, check boundary, soft-thresholding, and subgradient conditions.\n            \n            # 1. Primal Feasibility on the boundary: ||x_star||_1 == 1\n            l1_norm_check = np.isclose(np.sum(np.abs(x_star)), 1.0, atol=EPS)\n            if not l1_norm_check:\n                return False\n\n            # 2. Stationarity check\n            soft_thresh_ok = True\n            subgrad_ok = True\n            \n            for i in range(len(v)):\n                # Check condition for components that are non-zero in the solution\n                if not np.isclose(x_star[i], 0.0, atol=EPS):\n                    expected_x_i = np.sign(v[i]) * max(abs(v[i]) - theta, 0)\n                    if not np.isclose(x_star[i], expected_x_i, atol=EPS):\n                        soft_thresh_ok = False\n                        break\n                # Check condition for components that are zero in the solution\n                else:\n                    if not (abs(v[i]) <= theta + EPS):\n                        subgrad_ok = False\n                        break\n            \n            return l1_norm_check and soft_thresh_ok and subgrad_ok\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        [0.3, -0.2, 0.5, 0.0],\n        [0.0, 0.0, 0.0],\n        [3.0, -1.0, 0.0, 2.0],\n        [0.5, 0.5, 0.5, 0.5],\n        [-0.7, -0.7, -0.7],\n        [1e-12, -1e-12, 1e-12, -1e-12],\n        [2.0],\n        [0.4, -0.6],\n    ]\n\n    results = []\n    for v_list in test_cases:\n        v_np = np.array(v_list, dtype=float)\n        \n        # Compute projection\n        x_star, theta = project_l1_ball(v_np)\n        \n        # 1. ||x*||_1\n        l1_norm_x_star = np.sum(np.abs(x_star))\n        \n        # 2. Is v inside the ball?\n        is_inside = (np.sum(np.abs(v_np)) <= 1.0 + EPS)\n\n        # 3. Do KKT conditions hold?\n        kkt_ok = check_kkt(v_np, x_star, theta, is_inside)\n        \n        results.extend([l1_norm_x_star, str(kkt_ok).lower()])\n        # Inserting is_inside boolean between the two\n        results.insert(-1, str(is_inside).lower())\n\n\n    # Final print statement in the exact required format.\n    # Convert all items to string for join\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3448189"}, {"introduction": "This advanced exercise moves from building algorithmic components to analyzing why they work so effectively. You will explore the geometry of the $\\ell_1$ norm at a deeper level by characterizing its descent cone, the set of directions that do not increase the norm at a given sparse vector. By computing the cone's statistical dimension [@problem_id:3448241], you will uncover the profound connection between local geometry and the global performance of sparse recovery, explaining the phase transition phenomena observed in compressed sensing.", "problem": "Consider a vector $x^\\star \\in \\mathbb{R}^{n}$ with support $S \\subset \\{1,2,\\dots,n\\}$ of cardinality $|S|=k$ and sign pattern $\\operatorname{sign}(x^\\star_{S}) \\in \\{-1,+1\\}^{k}$, where $x^\\star_{S^c}=\\mathbf{0}$. Let $f(x)=\\|x\\|_{1}$ be the $\\ell_{1}$ norm. The descent cone $\\mathcal{D}(x^\\star)$ of a convex function $f$ at a point $x^\\star$ is defined as the set of directions $d \\in \\mathbb{R}^{n}$ such that there exists a $t>0$ with $f(x^\\star + t d) \\leq f(x^\\star)$. The statistical dimension $\\delta(\\mathcal{D})$ of a closed convex cone $\\mathcal{D} \\subset \\mathbb{R}^{n}$ is the average squared length of the projection of a standard Gaussian vector onto $\\mathcal{D}$, equivalently expressible via conic integral geometry through its polar cone.\n\nStarting from the fundamental definitions of the subdifferential of $f$ at $x^\\star$, the descent cone, and the statistical dimension of a cone, perform the following:\n\n- Derive an explicit characterization of the descent cone $\\mathcal{D}(x^\\star)$ of $f(x)=\\|x\\|_{1}$ at the $k$-sparse point $x^\\star$ in terms of the support $S$ and the sign pattern $\\operatorname{sign}(x^\\star_{S})$.\n- Using conic integral geometry, compute the statistical dimension $\\delta(\\mathcal{D})$ of this descent cone as a closed-form analytic expression involving $n$, $k$, and the Gaussian probability density function $\\varphi$ and cumulative distribution function $\\Phi$ of the standard normal distribution. Your derivation should start from the definition of the subdifferential and the polar relationship between the descent cone and the normal cone, and should explicitly minimize over any necessary auxiliary parameters.\n- Finally, use the expression for $\\delta(\\mathcal{D})$ to predict the Gaussian measurement threshold $m \\approx \\delta(\\mathcal{D})$ for the phase transition in basis pursuit recovery with independent and identically distributed standard Gaussian measurements.\n\nProvide your final answer as the exact analytic expression for $\\delta(\\mathcal{D})$ in terms of $n$, $k$, and the optimizer $\\tau^\\star$. No numerical approximation or rounding is required, and no physical units are involved. Use the standard normal probability density function $\\varphi(t)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\!\\big(-\\frac{t^{2}}{2}\\big)$ and cumulative distribution function $\\Phi(t)=\\int_{-\\infty}^{t}\\varphi(u)\\,\\mathrm{d}u$ in your expression.", "solution": "The problem asks for the characterization of the descent cone of the $\\ell_1$-norm at a sparse vector, the computation of its statistical dimension, and the connection to phase transitions in compressed sensing.\n\n### Step 1: Characterization of the Descent Cone $\\mathcal{D}(x^\\star)$\n\nThe problem concerns the function $f(x) = \\|x\\|_1$ and a $k$-sparse vector $x^\\star \\in \\mathbb{R}^n$ with support $S$ (where $|S|=k$) and sign pattern $z = \\operatorname{sign}(x^\\star_S) \\in \\{-1, +1\\}^k$. By definition, $x^\\star_i \\neq 0$ for $i \\in S$ and $x^\\star_j = 0$ for $j \\in S^c$.\n\nThe descent cone of a convex function $f$ at a point $x^\\star$ is given by $\\mathcal{D}(x^\\star) = \\{d \\in \\mathbb{R}^n \\mid f'(x^\\star; d) \\le 0\\}$, where $f'(x^\\star; d)$ is the directional derivative of $f$ at $x^\\star$ in the direction $d$. For a convex function, this directional derivative is given by the support function of the subdifferential $\\partial f(x^\\star)$:\n$$ f'(x^\\star; d) = \\sup_{v \\in \\partial f(x^\\star)} \\langle v, d \\rangle $$\nThe subdifferential of the $\\ell_1$-norm, $f(x) = \\|x\\|_1 = \\sum_i |x_i|$, at a point $x$ is the set:\n$$ \\partial f(x) = \\{ v \\in \\mathbb{R}^n \\mid v_i = \\operatorname{sign}(x_i) \\text{ if } x_i \\neq 0, \\text{ and } v_i \\in [-1, 1] \\text{ if } x_i = 0 \\} $$\nFor the given point $x^\\star$, the components on the support $S$ are non-zero, and the components on the complement $S^c$ are zero. Therefore, the subdifferential is:\n$$ \\partial f(x^\\star) = \\{ v \\in \\mathbb{R}^n \\mid v_S = \\operatorname{sign}(x^\\star_S) = z, \\text{ and } \\|v_{S^c}\\|_{\\infty} \\le 1 \\} $$\nwhere $v_S$ denotes the subvector of $v$ with indices in $S$, and $\\| \\cdot \\|_\\infty$ is the maximum absolute value norm.\n\nWe can now compute the directional derivative:\n$$ f'(x^\\star; d) = \\sup_{v_S = z, \\|v_{S^c}\\|_{\\infty} \\le 1} \\langle v, d \\rangle = \\sup_{v_S = z, \\|v_{S^c}\\|_{\\infty} \\le 1} (\\langle v_S, d_S \\rangle + \\langle v_{S^c}, d_{S^c} \\rangle) $$\nThe supremum separates into two parts:\n$$ f'(x^\\star; d) = \\langle z, d_S \\rangle + \\sup_{\\|v_{S^c}\\|_{\\infty} \\le 1} \\langle v_{S^c}, d_{S^c} \\rangle $$\nThe second term is the definition of the dual norm of the $\\ell_\\infty$-norm, which is the $\\ell_1$-norm. Thus:\n$$ \\sup_{\\|v_{S^c}\\|_{\\infty} \\le 1} \\langle v_{S^c}, d_{S^c} \\rangle = \\|d_{S^c}\\|_1 $$\nSo, the directional derivative is $f'(x^\\star; d) = \\langle z, d_S \\rangle + \\|d_{S^c}\\|_1$.\nThe descent cone $\\mathcal{D}(x^\\star)$ is the set of directions $d$ for which this quantity is non-positive.\n$$ \\mathcal{D}(x^\\star) = \\{ d \\in \\mathbb{R}^n \\mid \\langle z, d_S \\rangle + \\|d_{S^c}\\|_1 \\le 0 \\} $$\nThis provides the explicit characterization of the descent cone.\n\n### Step 2: Computation of the Statistical Dimension $\\delta(\\mathcal{D})$\n\nThe statistical dimension of the cone $\\mathcal{D}(x^\\star)$ is a central quantity in compressed sensing theory. A powerful result from this field states that for a cone of the form $\\mathcal{D} = \\{x \\in \\mathbb{R}^n : F(x) \\le 0\\}$, where $F$ is a convex, positively homogeneous function (a gauge), its statistical dimension can be computed via an optimization problem involving the proximal operator of $F$.\n$$ \\delta(\\mathcal{D}) = \\min_{\\tau \\ge 0} \\mathbb{E}_{g \\sim \\mathcal{N}(0, I_n)} \\left[ \\|g - \\operatorname{prox}_{\\tau F}(g)\\|^2 \\right] $$\nwhere the proximal operator is defined as:\n$$ \\operatorname{prox}_{\\tau F}(g) = \\arg\\min_{u \\in \\mathbb{R}^n} \\left\\{ \\frac{1}{2}\\|u-g\\|^2 + \\tau F(u) \\right\\} $$\nIn our case, $F(d) = \\langle z, d_S \\rangle + \\|d_{S^c}\\|_1$, which is convex and positively homogeneous. We proceed by computing the proximal operator. The minimization problem separates over the index sets $S$ and $S^c$:\n$$ \\min_{d_S, d_{S^c}} \\left\\{ \\frac{1}{2}\\|d_S-g_S\\|^2 + \\frac{1}{2}\\|d_{S^c}-g_{S^c}\\|^2 + \\tau(\\langle z, d_S \\rangle + \\|d_{S^c}\\|_1) \\right\\} $$\nThis is equivalent to solving two independent problems.\n\n1.  **For the components on the support $S$:**\n    $$ \\min_{d_S \\in \\mathbb{R}^k} \\left\\{ \\frac{1}{2}\\|d_S - g_S\\|^2 + \\tau \\langle z, d_S \\rangle \\right\\} $$\n    This is an unconstrained quadratic minimization. Setting the gradient with respect to $d_S$ to zero yields:\n    $$ (d_S - g_S) + \\tau z = 0 \\implies d_S^* = g_S - \\tau z $$\n\n2.  **For the components on the complement $S^c$:**\n    $$ \\min_{d_{S^c} \\in \\mathbb{R}^{n-k}} \\left\\{ \\frac{1}{2}\\|d_{S^c} - g_{S^c}\\|^2 + \\tau \\|d_{S^c}\\|_1 \\right\\} $$\n    This is the definition of the proximal operator for the $\\ell_1$-norm, which is the soft-thresholding operator $S_\\tau(\\cdot)$. The solution is applied component-wise:\n    $$ (d_{S^c}^*)_j = S_\\tau(g_j) = \\operatorname{sign}(g_j) \\max(|g_j| - \\tau, 0) $$\n\nLet $u^* = \\operatorname{prox}_{\\tau F}(g)$. We have $u^*_S = g_S - \\tau z$ and $u^*_{S^c} = S_\\tau(g_{S^c})$. We now compute the squared norm of the residual $g - u^*$:\n$$ \\|g - u^*\\|^2 = \\|g_S - u_S^*\\|^2 + \\|g_{S^c} - u_{S^c}^*\\|^2 $$\nThe first term is:\n$$ \\|g_S - (g_S - \\tau z)\\|^2 = \\|\\tau z\\|^2 = \\tau^2 \\sum_{i \\in S} z_i^2 = \\tau^2 k $$\nThe second term is the squared norm of the difference between a vector and its soft-thresholded version. This quantity is well-known:\n$$ \\|g_{S^c} - S_\\tau(g_{S^c})\\|^2 = \\sum_{j \\in S^c} \\|g_j - S_\\tau(g_j)\\|^2 = \\sum_{j \\in S^c} \\left( g_j^2 \\cdot \\mathbb{I}(|g_j| \\le \\tau) + \\tau^2 \\cdot \\mathbb{I}(|g_j| > \\tau) \\right) $$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function.\n\nThe statistical dimension is the minimum over $\\tau \\ge 0$ of the expectation of this quantity. Let $L(\\tau) = \\mathbb{E}[\\|g - u^*\\|^2]$.\n$$ L(\\tau) = \\mathbb{E}[k\\tau^2] + \\mathbb{E}\\left[\\sum_{j \\in S^c} \\left( g_j^2 \\cdot \\mathbb{I}(|g_j| \\le \\tau) + \\tau^2 \\cdot \\mathbb{I}(|g_j| > \\tau) \\right)\\right] $$\nSince the components of $g$ are i.i.d. standard normal variables, $\\gamma \\sim \\mathcal{N}(0, 1)$, the expectation for the sum is $(n-k)$ times the expectation for a single component:\n$$ L(\\tau) = k\\tau^2 + (n-k) \\mathbb{E}_{\\gamma \\sim \\mathcal{N}(0,1)} \\left[ \\gamma^2 \\cdot \\mathbb{I}(|\\gamma| \\le \\tau) + \\tau^2 \\cdot \\mathbb{I}(|\\gamma| > \\tau) \\right] $$\nLet's evaluate the expectation. Let $\\varphi(t) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-t^2/2)$ and $\\Phi(t) = \\int_{-\\infty}^t \\varphi(u) du$.\n$$ \\mathbb{E}\\left[ \\tau^2 \\cdot \\mathbb{I}(|\\gamma| > \\tau) \\right] = \\tau^2 \\mathbb{P}(|\\gamma| > \\tau) = \\tau^2 (2(1 - \\Phi(\\tau))) $$\n$$ \\mathbb{E}\\left[ \\gamma^2 \\cdot \\mathbb{I}(|\\gamma| \\le \\tau) \\right] = \\int_{-\\tau}^{\\tau} t^2 \\varphi(t) dt $$\nUsing integration by parts, $\\int t^2 \\varphi(t) dt = -t\\varphi(t) + \\Phi(t)$, we get:\n$$ \\int_{-\\tau}^{\\tau} t^2 \\varphi(t) dt = [-t\\varphi(t) + \\Phi(t)]_{-\\tau}^{\\tau} = (-\\tau\\varphi(\\tau) + \\Phi(\\tau)) - (\\tau\\varphi(-\\tau) + \\Phi(-\\tau)) $$\nSince $\\varphi(t)$ is even, $\\varphi(-\\tau) = \\varphi(\\tau)$, and $\\Phi(-\\tau) = 1-\\Phi(\\tau)$:\n$$ \\int_{-\\tau}^{\\tau} t^2 \\varphi(t) dt = -2\\tau\\varphi(\\tau) + \\Phi(\\tau) - (1-\\Phi(\\tau)) = 2\\Phi(\\tau) - 1 - 2\\tau\\varphi(\\tau) $$\nCombining the terms, the expectation is:\n$$ (2\\Phi(\\tau) - 1 - 2\\tau\\varphi(\\tau)) + 2\\tau^2(1-\\Phi(\\tau)) $$\nThus, the statistical dimension is the minimum of the following function over $\\tau \\ge 0$:\n$$ \\delta(\\mathcal{D}) = \\min_{\\tau \\ge 0} \\left\\{ k\\tau^2 + (n-k)\\left[ 2\\Phi(\\tau) - 1 - 2\\tau\\varphi(\\tau) + 2\\tau^2(1-\\Phi(\\tau)) \\right] \\right\\} $$\nLet $\\tau^\\star$ be the value of $\\tau \\ge 0$ that achieves this minimum. The problem asks for the expression for $\\delta(\\mathcal{D})$ in terms of this optimizer $\\tau^\\star$.\n\n### Step 3: Gaussian Measurement Threshold\n\nThe statistical dimension of the descent cone $\\delta(\\mathcal{D})$ has a profound connection to the performance of convex optimization methods for signal recovery. For the basis pursuit problem (recovering a sparse signal $x^\\star$ from linear measurements $y=Ax^\\star$ by solving $\\min \\|x\\|_1$ s.t. $Ax=y$), when the measurement matrix $A$ has i.i.d. standard Gaussian entries, a phase transition occurs. The signal $x^\\star$ is recovered with high probability if the number of measurements $m$ exceeds a certain threshold, and fails with high probability if $m$ is below it. This threshold is precisely predicted by the statistical dimension of the descent cone.\n$$ m \\approx \\delta(\\mathcal{D}(x^\\star)) $$\nThus, the expression derived for $\\delta(\\mathcal{D})$ provides the theoretical prediction for the number of Gaussian measurements required for successful recovery.\n\nThe final answer is the value of the objective function at the optimizer $\\tau^\\star$.\n$$ \\delta(\\mathcal{D}) = k(\\tau^\\star)^2 + (n-k)\\left[ 2\\Phi(\\tau^\\star) - 1 - 2\\tau^\\star\\varphi(\\tau^\\star) + 2(\\tau^\\star)^2(1-\\Phi(\\tau^\\star)) \\right] $$", "answer": "$$ \\boxed{k(\\tau^\\star)^{2} + (n-k)\\left(2\\Phi(\\tau^\\star) - 1 - 2\\tau^\\star\\varphi(\\tau^\\star) + 2(\\tau^\\star)^{2}(1-\\Phi(\\tau^\\star))\\right)} $$", "id": "3448241"}]}