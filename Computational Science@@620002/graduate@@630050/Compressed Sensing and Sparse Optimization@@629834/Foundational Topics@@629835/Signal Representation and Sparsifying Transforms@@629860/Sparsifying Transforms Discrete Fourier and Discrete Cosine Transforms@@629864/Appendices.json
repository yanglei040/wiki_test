{"hands_on_practices": [{"introduction": "Choosing the right sparsifying transform is the first critical step in many signal processing applications. This practice explores the fundamental trade-off between the Discrete Fourier Transform (DFT) and the Discrete Cosine Transform (DCT) by examining their implicit assumptions about a signal's boundaries. By analyzing how each transform handles non-periodic signals, you will develop a core intuition for why the DCT is often superior for natural signals and images.[@problem_id:3478629]", "problem": "You are given a real, finite-length signal $x \\in \\mathbb{R}^N$ obtained by uniform sampling of a function $f:[0,1]\\to\\mathbb{R}$ at $N$ equispaced points. Suppose $f$ is piecewise continuously differentiable on $(0,1)$, with $f' \\in \\mathrm{BV}(0,1)$ (bounded variation), and that all jump discontinuities of $f$ are confined to the domain boundaries in the following sense: $f$ is continuous on $(0,1)$ and any mismatch occurs only between the one-sided limits $f(0^+)$ and $f(1^-)$ that are adjacent under periodic wrap-around. There are no interior jumps on $(0,1)$. You wish to choose a sparsifying transform for compressed sensing recovery that minimizes the number of significant transform coefficients for a target approximation error. Consider the Discrete Fourier Transform (DFT) and the Discrete Cosine Transform (DCT) of type II, viewed as orthonormal bases on $\\mathbb{R}^N$ derived, respectively, from a periodic extension and an even extension of $f$.\n\nUsing only the core definitions of the DFT and DCT as orthonormal expansions associated with, respectively, periodic and even boundary conditions, and the standard relationship between smoothness, boundary conditions, and decay rates of Fourier-type coefficients derived by integration by parts and jump contributions, decide which of the following criteria are correct for choosing between the DFT and the DCT in the setting described. Select all that apply.\n\nA. When $f$ is nonperiodic and its only edge discontinuities are localized at $x=0$ and $x=1$, the DFT’s implicit periodic extension introduces a wrap-around jump, yielding Fourier coefficients that decay like $O(k^{-1})$, whereas the DCT’s even extension removes that artificial jump and, under $f' \\in \\mathrm{BV}(0,1)$, produces cosine coefficients that decay like $O(k^{-2})$. Therefore, the DCT is typically sparser than the DFT in this case.\n\nB. If $f$ is exactly periodic on $[0,1]$ with matched traces across the boundary (for example, $f^{(r)}(0^+) = f^{(r)}(1^-)$ for some $r \\ge 0$), then the DFT aligns with the true boundary conditions and does not incur a wrap-around discontinuity. In this situation, the DFT is at least as sparse as the DCT for general periodic content, and can be preferable.\n\nC. For uniform time-domain subsampling in compressed sensing, the mutual coherence between the sampling basis and the transform basis is identical for the DFT and the DCT, so the transform choice cannot affect recoverability; only the sampling pattern matters.\n\nD. For a fixed coefficient budget $m$, truncated DCT expansions exhibit reduced boundary Gibbs oscillations compared to truncated DFT expansions when discontinuities are confined to the endpoints, so fewer large coefficients are required by the DCT to achieve a target mean-squared error.\n\nE. In $2$-dimensional images with sharp edges that meet and cross the image frame boundary, a separable $2$-dimensional DCT tends to produce sparser coefficient sets than a separable $2$-dimensional DFT under the same sampling, because the even-symmetric extension mitigates wrap-around discontinuities along the frame boundary.\n\nProvide your selection and justify it by reasoning strictly from the transform definitions, the nature of the boundary extensions they impose, and coefficient decay implications that follow from smoothness and jump contributions. Do not assume any unproven shortcut formulas beyond the standard integration by parts argument and the fact that jump discontinuities induce $O(k^{-1})$ decay in Fourier-type series.", "solution": "The problem asks to evaluate several criteria for choosing between the Discrete Fourier Transform (DFT) and the Discrete Cosine Transform (DCT) based on their ability to create sparse representations. The choice depends on how their implicit boundary conditions handle the given signal properties.\n\nThe signal $x$ is sampled from a function $f$ that is continuous on $(0,1)$ but potentially non-periodic, meaning $f(0^+) \\neq f(1^-)$. Its derivative $f'$ has bounded variation.\n\n- **DFT Analysis**: The DFT assumes a periodic extension of the signal. For a non-periodic $f$, this creates a jump discontinuity at the boundaries. A jump discontinuity in the function itself limits the coefficient decay rate. By integration by parts, the Fourier coefficients $c_k$ will have a magnitude that decays slowly, as $O(k^{-1})$. This slow decay means many coefficients are significant, resulting in a non-sparse representation.\n\n- **DCT Analysis**: The Type-II DCT assumes an even-symmetric extension. This extension creates a new function that is continuous across the boundaries, even if $f(0^+) \\neq f(1^-)$. However, the derivative of this extended function will have a jump discontinuity at the boundaries (unless $f'(0^+) = 0$ and $f'(1^-)=0$). Since the original $f'$ has bounded variation, the extended function is continuous with a first derivative of bounded variation. This level of smoothness leads to a much faster coefficient decay rate, specifically $O(k^{-2})$. A faster decay rate implies that the signal's energy is more concentrated in the low-frequency coefficients, making the representation sparser.\n\nNow let's evaluate the options:\n\n**A. Correct.** This statement accurately summarizes the analysis above. The DFT's periodic extension of a non-periodic signal introduces a jump, leading to $O(k^{-1})$ decay. The DCT's even extension avoids this jump, resulting in a continuous extended function whose smoothness ($f' \\in \\mathrm{BV}(0,1)$) allows for a faster $O(k^{-2})$ decay. This makes the DCT representation sparser.\n\n**B. Correct.** If the signal is inherently periodic (i.e., $f^{(r)}(0^+) = f^{(r)}(1^-)$ for relevant $r$), the DFT's assumption matches the signal's reality. The DCT's even extension would be artificial and could introduce its own discontinuities in derivatives, leading to a less sparse representation than the DFT, which would be the natural and optimal choice.\n\n**C. Incorrect.** First, the mutual coherence between the time-domain sampling basis and the transform basis is not identical for DFT and DCT. Second, and more importantly, the effectiveness of compressed sensing recovery depends strongly on the sparsity level $k$ of the signal in the chosen basis. The entire purpose of selecting a good transform is to minimize $k$. A sparser signal (smaller $k$) is easier to recover. Thus, the choice of transform is critical.\n\n**D. Correct.** The Gibbs phenomenon (boundary oscillations) is a direct result of truncating a series with slow coefficient decay, like the DFT of a discontinuous periodic function. Because the DCT coefficients decay much faster ($O(k^{-2})$ vs. $O(k^{-1})$), the approximation error for a fixed number of coefficients is much smaller, and the Gibbs oscillations at the boundaries are significantly reduced.\n\n**E. Correct.** This correctly extends the 1D principle to 2D. For natural images, which are generally smooth but have edges that run to the frame boundary, the separable 2D DFT creates artificial \"wrap-around\" discontinuities. The separable 2D DCT's reflective boundary conditions handle these edges much more gracefully, leading to better energy compaction and a sparser representation, which is why it is used in JPEG.", "answer": "$$\\boxed{ABDE}$$", "id": "3478629"}, {"introduction": "Beyond visual inspection of coefficients, we can quantify a transform's effectiveness using information theory. This exercise asks you to compare the DFT and DCT through the lens of Shannon entropy, a measure of uncertainty and information content. You will reason about why a better transform yields coefficients that are not only sparser but also more decorrelated, approaching the performance of the optimal but signal-dependent Karhunen–Loève Transform (KLT).[@problem_id:3478654]", "problem": "Consider a vectorized image block modeled as a zero-mean, wide-sense stationary Gaussian random field that is smooth in the sense of strong positive correlation. Specifically, let $\\mathbf{x} \\in \\mathbb{R}^{n}$ be the vectorization of an $N \\times N$ block with $n = N^{2}$, and let its covariance be $\\mathbf{K} \\in \\mathbb{R}^{n \\times n}$ that is symmetric positive definite and Toeplitz-block-Toeplitz, induced by an isotropic first-order autoregressive field with correlation parameter $\\rho \\in (0,1)$ close to $1$. Define two orthonormal transforms applied separably in two dimensions:\n\n- The two-dimensional Discrete Fourier Transform (DFT) under periodic boundary conditions, represented by a unitary matrix $\\mathbf{F} \\in \\mathbb{C}^{n \\times n}$, yielding coefficients $\\mathbf{u} = \\mathbf{F} \\mathbf{x}$.\n- The two-dimensional Discrete Cosine Transform Type-II (DCT-II) under even-symmetric boundary conditions, represented by a real orthonormal matrix $\\mathbf{C} \\in \\mathbb{R}^{n \\times n}$, yielding coefficients $\\mathbf{v} = \\mathbf{C} \\mathbf{x}$.\n\nSuppose the coefficients are coded using an independent, per-coefficient model: each coefficient is treated as an independent scalar Gaussian, and the total coding cost proxy is the sum of the Shannon differential entropies of the marginal coefficient distributions before quantization, that is, $\\sum_{i=1}^{n} h(u_{i})$ for the DFT and $\\sum_{i=1}^{n} h(v_{i})$ for the DCT, where $h(\\cdot)$ denotes the Shannon differential entropy of a continuous random variable. Assume identical high-rate uniform quantization step across coefficients is used later, but focus solely on the continuous distributions prior to quantization.\n\nFor smooth blocks with $\\rho \\approx 1$, which transform yields a lower total marginal entropy under this independent model, and why?\n\nChoose the best answer:\n\nA. The DCT yields lower total marginal entropy because its even-symmetric boundary conditions reduce edge discontinuities, concentrating energy into low spatial frequencies and better approximating the Karhunen–Loève Transform (KLT), which more nearly diagonalizes the covariance for free-boundary blocks, thus lowering the sum of marginal entropies.\n\nB. The DFT yields lower total marginal entropy because it exactly diagonalizes Toeplitz-block-Toeplitz covariances for any block, producing independent coefficients with minimal entropy.\n\nC. Both transforms yield identical total marginal entropy because all orthonormal transforms preserve differential entropy exactly, so the sum of marginal entropies is invariant.\n\nD. The DFT yields lower total marginal entropy because its complex-valued coefficients encode phase, thereby reducing uncertainty compared to the DCT’s real coefficients.", "solution": "The problem statement has been validated as scientifically grounded, well-posed, and objective. It poses a standard, non-trivial question in the field of transform coding and sparse representations. We may proceed with the solution.\n\nThe core of the problem is to compare the efficacy of two transforms, the Discrete Fourier Transform (DFT) and the Discrete Cosine Transform Type-II (DCT-II), in terms of decorrelation for a specific signal model. The chosen metric is the sum of the marginal differential entropies of the transform coefficients.\n\nLet $\\mathbf{x} \\in \\mathbb{R}^{n}$ be the zero-mean Gaussian random vector representing the vectorized image block, with covariance matrix $\\mathbf{K} = E[\\mathbf{x}\\mathbf{x}^T]$. Let $\\mathbf{A}$ be a generic orthonormal transform matrix. The transformed coefficients are given by the vector $\\mathbf{y} = \\mathbf{A}\\mathbf{x}$. Since $\\mathbf{x}$ is a Gaussian vector, $\\mathbf{y}$ is also a Gaussian vector with zero mean and covariance matrix $\\mathbf{K_y} = \\mathbf{A}\\mathbf{K}\\mathbf{A}^H$.\n\nThe total coding cost proxy is given by the sum of the marginal entropies, $\\sum_{i=1}^{n} h(y_i)$. For a zero-mean Gaussian random variable $y_i$ with variance $\\sigma_i^2 = (\\mathbf{K_y})_{ii}$, the differential entropy is $h(y_i) = \\frac{1}{2} \\log_2(2\\pi e \\sigma_i^2)$. Thus, we want to minimize $\\sum_{i=1}^{n} \\frac{1}{2} \\log_2(2\\pi e \\sigma_i^2)$.\n\nLet us relate this quantity to the joint entropy of the vector $\\mathbf{y}$, which is $h(\\mathbf{y})$. The relationship is given by:\n$$ \\sum_{i=1}^{n} h(y_i) = h(\\mathbf{y}) + I(\\mathbf{y}) $$\nwhere $I(\\mathbf{y})$ is the mutual information among the components of $\\mathbf{y}$. $I(\\mathbf{y}) \\ge 0$, with equality if and only if the components $y_i$ are mutually independent.\n\nThe joint entropy of a Gaussian vector $\\mathbf{y}$ is $h(\\mathbf{y}) = \\frac{1}{2} \\log_2((2\\pi e)^n \\det(\\mathbf{K_y}))$. Since $\\mathbf{A}$ is a unitary/orthonormal transform, $|\\det(\\mathbf{A})|=1$, and thus $\\det(\\mathbf{K_y}) = \\det(\\mathbf{A}\\mathbf{K}\\mathbf{A}^H) = |\\det(\\mathbf{A})|^2\\det(\\mathbf{K}) = \\det(\\mathbf{K})$. This means the joint entropy is invariant under the transform: $h(\\mathbf{y}) = h(\\mathbf{x})$.\n\nSince $h(\\mathbf{y})$ is constant for any orthonormal transform applied to $\\mathbf{x}$, minimizing the sum of marginal entropies $\\sum_{i=1}^{n} h(y_i)$ is equivalent to minimizing the mutual information $I(\\mathbf{y})$. Mutual information measures the statistical dependence among the coefficients. Therefore, the goal is to find the transform that produces the most decorrelated (and for Gaussian sources, the most independent) coefficients.\n\nThe optimal transform for decorrelating any random vector is the Karhunen–Loève Transform (KLT). The basis vectors of the KLT are the eigenvectors of the covariance matrix $\\mathbf{K}$. The KLT diagonalizes the covariance matrix, producing perfectly uncorrelated coefficients, for which $I(\\mathbf{y}_{KLT}) = 0$.\n\nThe problem states that the signal $\\mathbf{x}$ has a covariance matrix $\\mathbf{K}$ that is Toeplitz-block-Toeplitz (TBT), arising from an isotropic first-order autoregressive (AR($1$)) field with correlation parameter $\\rho \\approx 1$. For such a signal model, the KLT is not a standard, signal-independent transform like the DFT or DCT. The question then becomes: which of the two, DFT or DCT, better approximates the KLT for this specific signal model?\n\n1.  **Discrete Fourier Transform (DFT)**: The $2$D DFT, represented by the matrix $\\mathbf{F}$, exactly diagonalizes $2$D circulant-block-circulant (CBC) matrices. A TBT matrix is not a CBC matrix. The DFT implicitly assumes that the signal is periodic. When applying the DFT to a finite block, it is equivalent to periodically extending the block in both dimensions. For a smooth image block where $\\rho \\approx 1$, the values at opposite edges (e.g., right vs. left) are generally not equal. The periodic extension thus introduces a sharp discontinuity at the block boundaries. This artifact introduces significant high-frequency energy in the DFT domain, which means the DFT is not very efficient at decorrelating the signal or compacting its energy.\n\n2.  **Discrete Cosine Transform (DCT-II)**: The $2$D DCT-II, represented by the matrix $\\mathbf{C}$, is equivalent to performing a DFT on an even-symmetrically extended version of the signal block. For a smooth block, reflecting it at the boundaries typically creates a much smaller discontinuity than the periodic wraparound of the DFT. The signal and its reflection join with similar values and slopes, preserving smoothness across the extended boundary. It is a well-established result that for a $1$D AR($1$) process with $\\rho \\to 1$, the eigenvectors of the corresponding Toeplitz covariance matrix asymptotically approach the basis vectors of the DCT-II. This property extends to the $2$D separable case. Therefore, for smooth signals ($\\rho \\approx 1$), the DCT-II provides a remarkably good approximation to the true KLT.\n\nBecause the DCT-II better approximates the KLT for the given signal model, it performs a much better decorrelation of the data compared to the DFT. This means the mutual information among the DCT coefficients, $I(\\mathbf{v})$, will be significantly smaller than the mutual information among the DFT coefficients, $I(\\mathbf{u})$. Consequently, since $h(\\mathbf{u}) = h(\\mathbf{v}) = h(\\mathbf{x})$, we have:\n$$ \\sum_{i=1}^{n} h(v_i) = h(\\mathbf{v}) + I(\\mathbf{v})  h(\\mathbf{u}) + I(\\mathbf{u}) = \\sum_{i=1}^{n} h(u_i) $$\nThus, the DCT yields a lower total marginal entropy.\n\nNow we evaluate the given options.\n\n**A. The DCT yields lower total marginal entropy because its even-symmetric boundary conditions reduce edge discontinuities, concentrating energy into low spatial frequencies and better approximating the Karhunen–Loève Transform (KLT), which more nearly diagonalizes the covariance for free-boundary blocks, thus lowering the sum of marginal entropies.**\nThis statement is a comprehensive and accurate summary of the reasoning derived above. The DCT's implicit even-symmetric boundary condition is better suited for smooth blocks than the DFT's periodic one. This superior boundary handling leads to better energy compaction (concentration in low frequencies) because the DCT basis functions are a better approximation to the optimal KLT basis functions for this signal type. As explained, a better approximation to the KLT leads to more decorrelated coefficients, which in turn minimizes the sum of marginal entropies.\n**Verdict: Correct.**\n\n**B. The DFT yields lower total marginal entropy because it exactly diagonalizes Toeplitz-block-Toeplitz covariances for any block, producing independent coefficients with minimal entropy.**\nThis statement is factually incorrect. The DFT basis vectors (complex exponentials) are the eigenvectors of circulant matrices, not general Toeplitz matrices. A $2$D DFT diagonalizes circulant-block-circulant matrices, which arise from doubly periodic processes. The covariance matrix $\\mathbf{K}$ in the problem is Toeplitz-block-Toeplitz, which corresponds to a process on a finite grid with free boundaries and is not diagonalized by the DFT. Therefore, the DFT coefficients $\\mathbf{u}$ are not independent.\n**Verdict: Incorrect.**\n\n**C. Both transforms yield identical total marginal entropy because all orthonormal transforms preserve differential entropy exactly, so the sum of marginal entropies is invariant.**\nThis statement confuses joint entropy with the sum of marginal entropies. While it is true that orthonormal transforms preserve the *joint* differential entropy ($h(\\mathbf{u}) = h(\\mathbf{v}) = h(\\mathbf{x})$), they do not preserve the *sum* of the *marginal* entropies. The sum of marginal entropies depends on the residual correlation between coefficients, as captured by the mutual information term $I(\\mathbf{y})$. Different transforms achieve different degrees of decorrelation, leading to different values for this sum.\n**Verdict: Incorrect.**\n\n**D. The DFT yields lower total marginal entropy because its complex-valued coefficients encode phase, thereby reducing uncertainty compared to the DCT’s real coefficients.**\nThis reasoning is not sound. The measure of uncertainty is entropy, which depends on the probability distribution of the coefficients (specifically, their variances), not on whether they are real or complex. The argument that encoding \"phase\" reduces uncertainty is unfounded in information theory. The core issue is decorrelation efficiency, which is determined by how well the transform's basis functions match the signal's statistical properties (via the KLT), not by the algebraic field ($\\mathbb{R}$ or $\\mathbb{C}$) of the coefficients.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3478654"}, {"introduction": "A good sparsifying transform is crucial not only for representation but also for robust recovery from incomplete measurements. This final practice guides you through a complete performance analysis of a compressed sensing system, incorporating the practical effects of quantization noise. By deriving the mean-squared error for an oracle estimator, you will see how the theoretical properties of the DFT and DCT, specifically their incoherence with the sampling basis, determine the ultimate recovery quality.[@problem_id:3478586]", "problem": "Consider a signal model in which an ambient dimension-$n$ real-valued signal $x \\in \\mathbb{R}^{n}$ is synthesized from a coefficient vector $\\theta \\in \\mathbb{R}^{n}$ via an orthonormal dictionary $\\Phi \\in \\mathbb{R}^{n \\times n}$ as $x = \\Phi \\theta$, where $\\Phi$ is either the Discrete Fourier Transform (DFT) dictionary or the Discrete Cosine Transform (DCT) dictionary, both taken in orthonormal form. The coefficient vector $\\theta$ is $k$-sparse with support $S \\subset \\{1,2,\\dots,n\\}$ of size $|S| = k$. Measurements are obtained by uniformly subsampling the time domain with a sampling operator $S_{\\mathrm{m}} \\in \\mathbb{R}^{m \\times n}$ that selects $m$ rows of the identity matrix uniformly at random without replacement, so the unquantized measurement vector is $y = S_{\\mathrm{m}} x = S_{\\mathrm{m}} \\Phi \\theta \\in \\mathbb{R}^{m}$. Each measurement is passed through a uniform mid-rise quantizer with dynamic range $[-1,1]$ and $2^{q}$ levels, where $q \\in \\mathbb{N}$ is the bit-depth. Assume subtractive dither is used so that the quantization error is modeled as an independent random vector $\\varepsilon \\in \\mathbb{R}^{m}$ with components that are independent and identically distributed as uniform on $\\left[-\\Delta/2, \\Delta/2\\right]$, where $\\Delta = 2 / 2^{q}$, and thus have variance $\\Delta^{2}/12$. Assume oracle recovery that knows the true support $S$ and estimates the active coefficients by least squares restricted to that support, i.e., defining $A_{S} := S_{\\mathrm{m}} \\Phi_{S} \\in \\mathbb{R}^{m \\times k}$, the oracle estimator is $\\widehat{\\theta}_{S} = \\arg\\min_{u \\in \\mathbb{R}^{k}} \\left\\| y + \\varepsilon - A_{S} u \\right\\|_{2}$. Work in a regime where $n \\ge m \\ge k$ and where the randomness is over both the sampling operator $S_{\\mathrm{m}}$ and the quantization error $\\varepsilon$, with an arbitrary but fixed support $S$.\n\nStarting from core definitions of orthonormal transforms and least squares estimation under additive noise, and using the high-resolution dithered quantization model, derive the leading-order scaling law for the expected oracle coefficient-recovery mean-squared error $\\mathbb{E}\\left[ \\left\\| \\widehat{\\theta}_{S} - \\theta_{S} \\right\\|_{2}^{2} \\right]$ as a function of the bit-depth $q$, sparsity level $k$, ambient dimension $n$, and number of samples $m$. Explicitly compare the scaling under the orthonormal Discrete Fourier Transform (DFT) dictionary versus the orthonormal Discrete Cosine Transform (DCT) dictionary, and assume the quantizer dynamic range is matched to $[-1,1]$ for both cases.\n\nProvide the final answer as a single closed-form analytic expression for the leading-order expected mean-squared error in terms of $q$, $k$, $n$, and $m$. No numerical rounding is required, and the answer should be dimensionless.", "solution": "The problem asks for the leading-order scaling law for the expected oracle coefficient-recovery mean-squared error (MSE) for a sparse signal observed through random sampling and uniform quantization.\n\nLet us begin by formalizing the problem setup. The signal model is $x = \\Phi \\theta$, where $x \\in \\mathbb{R}^{n}$ is the signal, $\\theta \\in \\mathbb{R}^{n}$ is a $k$-sparse coefficient vector with support $S$, and $\\Phi \\in \\mathbb{R}^{n \\times n}$ is an orthonormal dictionary. The measurements are obtained by subsampling $x$ at $m$ indices, followed by quantization. The use of subtractive dither allows us to model this process as acquiring a noisy measurement vector $y_{obs} \\in \\mathbb{R}^{m}$ given by:\n$$y_{obs} = S_{\\mathrm{m}} x + \\varepsilon = S_{\\mathrm{m}} \\Phi \\theta + \\varepsilon$$\nwhere $S_{\\mathrm{m}} \\in \\mathbb{R}^{m \\times n}$ is the random sampling operator and $\\varepsilon \\in \\mathbb{R}^{m}$ is the quantization error vector.\n\nThe coefficient vector $\\theta$ is $k$-sparse, meaning it has only $k$ non-zero entries. Let $S \\subset \\{1, 2, \\dots, n\\}$ be the support set of $\\theta$ with $|S|=k$, and let $\\theta_S \\in \\mathbb{R}^k$ be the vector of non-zero coefficients. Similarly, let $\\Phi_S \\in \\mathbb{R}^{n \\times k}$ be the sub-matrix of $\\Phi$ containing the columns corresponding to the support $S$. The measurement model can be rewritten in terms of the non-zero coefficients:\n$$y_{obs} = S_{\\mathrm{m}} \\Phi_S \\theta_S + \\varepsilon$$\nLet us define the sensing matrix restricted to the support $S$ as $A_S := S_{\\mathrm{m}} \\Phi_S \\in \\mathbb{R}^{m \\times k}$. The model then becomes a standard linear model:\n$$y_{obs} = A_S \\theta_S + \\varepsilon$$\nThe problem specifies that the quantization error components $\\varepsilon_i$ are independent and identically distributed (i.i.d.) uniform random variables on $[-\\Delta/2, \\Delta/2]$, where $\\Delta = 2/2^q$. This implies that the noise has zero mean, $\\mathbb{E}[\\varepsilon] = 0$, and a diagonal covariance matrix, $\\mathrm{Cov}(\\varepsilon) = \\mathbb{E}[\\varepsilon \\varepsilon^T] = \\sigma_{\\varepsilon}^2 I_m$. The variance $\\sigma_{\\varepsilon}^2$ of a uniform distribution over an interval of width $\\Delta$ is $\\Delta^2/12$.\n\nThe oracle estimator knows the true support $S$ and computes the estimate of $\\theta_S$ by solving the least-squares problem:\n$$\\widehat{\\theta}_S = \\arg\\min_{u \\in \\mathbb{R}^{k}} \\| y_{obs} - A_S u \\|_2^2$$\nThe solution to this standard linear least-squares problem is given by:\n$$\\widehat{\\theta}_S = (A_S^T A_S)^{-1} A_S^T y_{obs}$$\nThis solution is unique provided that $A_S^T A_S$ is invertible, which is true with high probability in the specified regime $m \\ge k$.\n\nWe can now find the estimation error vector by substituting the expression for $y_{obs}$:\n$$\\widehat{\\theta}_S - \\theta_S = (A_S^T A_S)^{-1} A_S^T (A_S \\theta_S + \\varepsilon) - \\theta_S$$\n$$\\widehat{\\theta}_S - \\theta_S = (A_S^T A_S)^{-1} (A_S^T A_S) \\theta_S + (A_S^T A_S)^{-1} A_S^T \\varepsilon - \\theta_S$$\n$$\\widehat{\\theta}_S - \\theta_S = \\theta_S + (A_S^T A_S)^{-1} A_S^T \\varepsilon - \\theta_S = (A_S^T A_S)^{-1} A_S^T \\varepsilon$$\nThe quantity of interest is the expected mean-squared error, $\\mathbb{E}[\\|\\widehat{\\theta}_S - \\theta_S\\|_2^2]$. The expectation is taken over the randomness in both the sampling operator $S_{\\mathrm{m}}$ and the quantization error $\\varepsilon$. We use the law of total expectation, first conditioning on a fixed realization of $S_{\\mathrm{m}}$ (and thus a fixed $A_S$):\n$$\\mathbb{E}[\\|\\widehat{\\theta}_S - \\theta_S\\|_2^2] = \\mathbb{E}_{S_{\\mathrm{m}}} \\left[ \\mathbb{E}_{\\varepsilon} \\left[ \\|(A_S^T A_S)^{-1} A_S^T \\varepsilon\\|_2^2 \\mid S_{\\mathrm{m}} \\right] \\right]$$\nLet's analyze the inner expectation. Let $B = (A_S^T A_S)^{-1} A_S^T$. The expression is the expected squared norm of the vector $B\\varepsilon$.\n$$\\mathbb{E}_{\\varepsilon} [\\|B\\varepsilon\\|_2^2 \\mid S_{\\mathrm{m}}] = \\mathbb{E}_{\\varepsilon} [(B\\varepsilon)^T (B\\varepsilon) \\mid S_{\\mathrm{m}}] = \\mathbb{E}_{\\varepsilon} [\\varepsilon^T B^T B \\varepsilon \\mid S_{\\mathrm{m}}]$$\nThis is the expectation of a quadratic form. For a random vector $\\varepsilon$ with mean $0$ and covariance $\\sigma_{\\varepsilon}^2 I_m$, the expectation is given by $\\mathrm{Tr}(B^T B \\cdot \\sigma_{\\varepsilon}^2 I_m) = \\sigma_{\\varepsilon}^2 \\mathrm{Tr}(B^T B)$.\nLet's compute the trace term:\n$$\\mathrm{Tr}(B^T B) = \\mathrm{Tr}(A_S (A_S^T A_S)^{-2} A_S^T)$$\nUsing the cyclic property of the trace, $\\mathrm{Tr}(XY) = \\mathrm{Tr}(YX)$:\n$$\\mathrm{Tr}(B^T B) = \\mathrm{Tr}( (A_S^T A_S)^{-2} A_S^T A_S ) = \\mathrm{Tr}((A_S^T A_S)^{-1})$$\nThus, the MSE for a fixed $S_{\\mathrm{m}}$ is $\\sigma_{\\varepsilon}^2 \\mathrm{Tr}((A_S^T A_S)^{-1})$. The total expected MSE is:\n$$\\mathbb{E}[\\|\\widehat{\\theta}_S - \\theta_S\\|_2^2] = \\sigma_{\\varepsilon}^2 \\cdot \\mathbb{E}_{S_{\\mathrm{m}}} \\left[ \\mathrm{Tr}((A_S^T A_S)^{-1}) \\right]$$\nTo find the leading-order scaling, we analyze the term $\\mathbb{E}_{S_{\\mathrm{m}}}[\\mathrm{Tr}((A_S^T A_S)^{-1})]$. For a matrix $G$ that concentrates around its mean $\\mathbb{E}[G]$, a first-order approximation (related to the delta method) is $\\mathbb{E}[G^{-1}] \\approx (\\mathbb{E}[G])^{-1}$. We apply this to the matrix $G = A_S^T A_S$.\nFirst, we compute the expectation of $G = A_S^T A_S = \\Phi_S^T S_{\\mathrm{m}}^T S_{\\mathrm{m}} \\Phi_S$:\n$$\\mathbb{E}_{S_{\\mathrm{m}}}[A_S^T A_S] = \\Phi_S^T \\mathbb{E}_{S_{\\mathrm{m}}}[S_{\\mathrm{m}}^T S_{\\mathrm{m}}] \\Phi_S$$\nThe matrix $S_{\\mathrm{m}}^T S_{\\mathrm{m}}$ is a diagonal $n \\times n$ matrix with $m$ entries equal to $1$ at the sampled locations and $0$ otherwise. Since sampling is uniform without replacement, the probability of any index being sampled is $m/n$. Therefore, the expected value of each diagonal entry is $m/n$, and $\\mathbb{E}_{S_{\\mathrm{m}}}[S_{\\mathrm{m}}^T S_{\\mathrm{m}}] = \\frac{m}{n}I_n$.\nSubstituting this back, we get:\n$$\\mathbb{E}_{S_{\\mathrm{m}}}[A_S^T A_S] = \\Phi_S^T \\left(\\frac{m}{n}I_n\\right) \\Phi_S = \\frac{m}{n} \\Phi_S^T \\Phi_S$$\nSince $\\Phi$ is an orthonormal matrix, its columns are orthonormal. Thus, $\\Phi_S$, being a sub-collection of these columns, has orthonormal columns, which means $\\Phi_S^T \\Phi_S = I_k$. So,\n$$\\mathbb{E}_{S_{\\mathrm{m}}}[A_S^T A_S] = \\frac{m}{n} I_k$$\nNow we apply the approximation:\n$$\\mathbb{E}_{S_{\\mathrm{m}}}[(A_S^T A_S)^{-1}] \\approx (\\mathbb{E}_{S_{\\mathrm{m}}}[A_S^T A_S])^{-1} = \\left(\\frac{m}{n}I_k\\right)^{-1} = \\frac{n}{m}I_k$$\nTaking the trace of this resulting matrix:\n$$\\mathbb{E}_{S_{\\mathrm{m}}}[\\mathrm{Tr}((A_S^T A_S)^{-1})] \\approx \\mathrm{Tr}\\left(\\frac{n}{m}I_k\\right) = \\frac{n}{m}\\mathrm{Tr}(I_k) = \\frac{nk}{m}$$\nThis approximation is valid for finding the leading-order behavior, particularly when the measurement matrix $A_S$ is well-conditioned. This condition is met for dictionaries that are incoherent with the sampling basis. Both the Discrete Fourier Transform (DFT) and Discrete Cosine Transform (DCT) dictionaries are highly incoherent with the standard basis (which corresponds to time-domain sampling). Therefore, the leading-order scaling law is identical for both DFT and DCT.\n\nNow we combine the pieces. The expected MSE is:\n$$\\mathbb{E}[\\|\\widehat{\\theta}_S - \\theta_S\\|_2^2] \\approx \\sigma_{\\varepsilon}^2 \\frac{nk}{m}$$\nThe final step is to substitute the expression for the noise variance $\\sigma_{\\varepsilon}^2$. The quantization step size is $\\Delta = 2/2^q = 2 \\cdot 2^{-q} = 2^{1-q}$. The variance of the uniform noise is:\n$$\\sigma_{\\varepsilon}^2 = \\frac{\\Delta^2}{12} = \\frac{(2^{1-q})^2}{12} = \\frac{2^{2-2q}}{12} = \\frac{4 \\cdot (2^{-q})^2}{12} = \\frac{4 \\cdot 4^{-q}}{12} = \\frac{1}{3} 4^{-q}$$\nSubstituting this into the MSE expression gives the final scaling law:\n$$\\mathbb{E}[\\|\\widehat{\\theta}_S - \\theta_S\\|_2^2] \\approx \\left(\\frac{1}{3} 4^{-q}\\right) \\frac{nk}{m} = \\frac{nk}{3m \\cdot 4^q}$$\nThis expression represents the leading-order scaling of the expected oracle MSE as a function of the problem parameters $q, k, n,$ and $m$. It shows that the error increases with sparsity $k$ and ambient dimension $n$, decreases with the number of measurements $m$, and decreases exponentially with the quantization bit-depth $q$.", "answer": "$$\n\\boxed{\\frac{nk}{3m \\cdot 4^{q}}}\n$$", "id": "3478586"}]}