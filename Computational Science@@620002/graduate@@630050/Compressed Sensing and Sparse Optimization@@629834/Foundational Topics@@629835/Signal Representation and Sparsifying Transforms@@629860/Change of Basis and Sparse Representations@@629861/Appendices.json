{"hands_on_practices": [{"introduction": "The ability to uniquely represent a signal from a small number of measurements is a cornerstone of sparse optimization. This uniqueness is not arbitrary; it is directly governed by the geometric properties of the dictionary matrix used for representation. This exercise introduces the concept of the $\\mathrm{spark}$ of a matrix, a fundamental quantity that connects the linear dependence of dictionary columns to the uniqueness of sparse solutions [@problem_id:3434598]. By calculating the $\\mathrm{spark}$ from first principles, you will gain a hands-on understanding of the precise conditions under which a sparse signal can be unambiguously identified.", "problem": "Let $\\Phi \\in \\mathbb{R}^{2 \\times 4}$ be the dictionary matrix\n$$\n\\Phi=\\begin{bmatrix}\n1  0  1  1\\\\\n0  1  1  -1\n\\end{bmatrix},\n$$\nwhose columns $\\phi_{1},\\phi_{2},\\phi_{3},\\phi_{4} \\in \\mathbb{R}^{2}$ are used to form sparse representations $y=\\Phi x$ with coefficient vectors $x \\in \\mathbb{R}^{4}$. The number of nonzero entries of a vector $x$ is its $\\ell_{0}$-“norm” (cardinality), and a vector $x$ is called $k$-sparse if it has at most $k$ nonzero entries. The spark of a matrix, denoted $\\mathrm{spark}(\\Phi)$, is defined as the smallest number of columns of $\\Phi$ that are linearly dependent.\n\nUsing only foundational linear-algebraic definitions and facts, do the following:\n- Compute $\\mathrm{spark}(\\Phi)$ by reasoning from the definition and the ambient dimension constraint.\n- Based on the definition of spark and the notion of $k$-sparse representations, determine whether every $1$-sparse representation $y=\\Phi x$ is unique, and whether non-unique $2$-sparse representations can exist. Justify your conclusions purely from first principles and, if non-uniqueness is possible for $k=2$, exhibit explicit distinct $2$-sparse representations of the same $y$.\n\nProvide the value of $\\mathrm{spark}(\\Phi)$ as your final answer. No rounding is required.", "solution": "The problem is validated as scientifically sound, well-posed, and objective. It is a standard exercise in the fundamentals of sparse representation theory.\n\nThe problem asks for three things: the computation of the spark of a given matrix $\\Phi$, an analysis of the uniqueness of $1$-sparse representations, and an analysis of the uniqueness of $2$-sparse representations, all justified from first principles.\n\nThe given dictionary matrix is $\\Phi \\in \\mathbb{R}^{2 \\times 4}$, with columns $\\phi_1, \\phi_2, \\phi_3, \\phi_4 \\in \\mathbb{R}^{2}$:\n$$\n\\Phi = \\begin{bmatrix} \\phi_1  \\phi_2  \\phi_3  \\phi_4 \\end{bmatrix} = \\begin{bmatrix}\n1  0  1  1\\\\\n0  1  1  -1\n\\end{bmatrix}\n$$\n\n**Part 1: Computation of $\\mathrm{spark}(\\Phi)$**\n\nThe spark of a matrix $\\Phi$, denoted $\\mathrm{spark}(\\Phi)$, is defined as the smallest number of columns of $\\Phi$ that are linearly dependent. We will examine subsets of columns of increasing size.\n\n1.  **Subsets of size $1$**: A single column vector $\\phi_i$ is linearly dependent if and only if it is the zero vector. The columns of $\\Phi$ are:\n    $$\n    \\phi_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad\n    \\phi_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad\n    \\phi_3 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad\n    \\phi_4 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n    $$\n    None of these vectors is the zero vector. Therefore, no subset of size $1$ is linearly dependent. This implies $\\mathrm{spark}(\\Phi)  1$.\n\n2.  **Subsets of size $2$**: A set of two vectors $\\{\\phi_i, \\phi_j\\}$ in $\\mathbb{R}^2$ with $i \\neq j$ is linearly dependent if and only if one is a scalar multiple of the other, or equivalently, if the determinant of the matrix formed by these two columns is zero. We check all $\\binom{4}{2}=6$ pairs:\n    -   $\\det(\\begin{bmatrix} \\phi_1  \\phi_2 \\end{bmatrix}) = \\det(\\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}) = 1 \\neq 0$.\n    -   $\\det(\\begin{bmatrix} \\phi_1  \\phi_3 \\end{bmatrix}) = \\det(\\begin{bmatrix} 1  1 \\\\ 0  1 \\end{bmatrix}) = 1 \\neq 0$.\n    -   $\\det(\\begin{bmatrix} \\phi_1  \\phi_4 \\end{bmatrix}) = \\det(\\begin{bmatrix} 1  1 \\\\ 0  -1 \\end{bmatrix}) = -1 \\neq 0$.\n    -   $\\det(\\begin{bmatrix} \\phi_2  \\phi_3 \\end{bmatrix}) = \\det(\\begin{bmatrix} 0  1 \\\\ 1  1 \\end{bmatrix}) = -1 \\neq 0$.\n    -   $\\det(\\begin{bmatrix} \\phi_2  \\phi_4 \\end{bmatrix}) = \\det(\\begin{bmatrix} 0  1 \\\\ 1  -1 \\end{bmatrix}) = -1 \\neq 0$.\n    -   $\\det(\\begin{bmatrix} \\phi_3  \\phi_4 \\end{bmatrix}) = \\det(\\begin{bmatrix} 1  1 \\\\ 1  -1 \\end{bmatrix}) = -1 - 1 = -2 \\neq 0$.\n    Since no determinant is zero, every pair of columns is linearly independent. Thus, $\\mathrm{spark}(\\Phi)  2$.\n\n3.  **Subsets of size $3$**: The columns of $\\Phi$ are vectors in the ambient space $\\mathbb{R}^2$. A fundamental theorem of linear algebra states that any set of $p$ vectors in an $m$-dimensional vector space is linearly dependent if $p  m$. Here, the dimension of the space is $m=2$. For any subset of $p=3$ columns, we have $3  2$. Therefore, any set of $3$ columns of $\\Phi$ must be linearly dependent.\n\nSince we have established that $\\mathrm{spark}(\\Phi)  2$ and that any set of $3$ columns is linearly dependent, the smallest number of linearly dependent columns must be exactly $3$.\nTherefore, $\\mathrm{spark}(\\Phi) = 3$.\n\n**Part 2: Uniqueness of Sparse Representations**\n\nA representation $y = \\Phi x$ is non-unique if there exist two distinct coefficient vectors $x_1, x_2$ such that $\\Phi x_1 = \\Phi x_2 = y$. This is equivalent to $\\Phi(x_1 - x_2) = 0$, where $z = x_1 - x_2$ is a non-zero vector in the null space of $\\Phi$. The vector $z$ represents a linear dependency among the columns of $\\Phi$ corresponding to its non-zero entries. By the definition of spark, the minimum number of non-zero entries in any such vector $z$ is precisely $\\mathrm{spark}(\\Phi)$. Therefore, for any $z \\in \\mathrm{null}(\\Phi), z \\neq 0$, we must have $\\|z\\|_0 \\ge \\mathrm{spark}(\\Phi) = 3$.\n\n**Uniqueness for $1$-sparse representations ($k=1$):**\nA vector $x$ is $1$-sparse if it has at most one non-zero entry, i.e., $\\|x\\|_0 \\le 1$.\nAssume, for the sake of contradiction, that there exist two distinct $1$-sparse vectors, $x_1$ and $x_2$, such that $\\Phi x_1 = \\Phi x_2 = y$.\nSince $x_1$ and $x_2$ are distinct, their difference $z = x_1 - x_2$ is a non-zero vector in the null space of $\\Phi$.\nThe number of non-zero entries in $z$ is bounded by the sum of the non-zero entries in $x_1$ and $x_2$:\n$$\n\\|z\\|_0 = \\|x_1 - x_2\\|_0 \\le \\|x_1\\|_0 + \\|x_2\\|_0\n$$\nSince $x_1$ and $x_2$ are $1$-sparse, $\\|x_1\\|_0 \\le 1$ and $\\|x_2\\|_0 \\le 1$. Thus,\n$$\n\\|z\\|_0 \\le 1 + 1 = 2\n$$\nHowever, as established earlier, any non-zero vector $z$ in the null space of $\\Phi$ must satisfy $\\|z\\|_0 \\ge \\mathrm{spark}(\\Phi) = 3$. This gives the contradiction $3 \\le \\|z\\|_0 \\le 2$.\nTherefore, our initial assumption must be false. No two distinct $1$-sparse vectors can produce the same representation $y$. Every $1$-sparse representation is unique.\n\n**Non-uniqueness for $2$-sparse representations ($k=2$):**\nA vector $x$ is $2$-sparse if $\\|x\\|_0 \\le 2$. We investigate whether non-unique $2$-sparse representations can exist.\nLet us again consider two distinct $2$-sparse vectors $x_1, x_2$ giving the same $y$. Their difference $z = x_1 - x_2$ must be a non-zero vector in $\\mathrm{null}(\\Phi)$.\nThe bound on the sparsity of $z$ is now:\n$$\n\\|z\\|_0 \\le \\|x_1\\|_0 + \\|x_2\\|_0 \\le 2 + 2 = 4\n$$\nCombining with the spark condition, we require a null space vector $z \\neq 0$ such that $3 \\le \\|z\\|_0 \\le 4$. This is not a contradiction, so non-unique $2$-sparse representations may exist. To confirm this, we must exhibit an explicit example.\n\nWe seek a non-zero vector $z = (z_1, z_2, z_3, z_4)^T$ such that $\\Phi z = 0$. This gives the system:\n$$\nz_1 + z_3 + z_4 = 0 \\\\\nz_2 + z_3 - z_4 = 0\n$$\nA simple non-zero solution can be found by observing a linear dependency among the columns. By inspection, $\\phi_3 = \\phi_1 + \\phi_2$. This can be rewritten as $1 \\cdot \\phi_1 + 1 \\cdot \\phi_2 - 1 \\cdot \\phi_3 + 0 \\cdot \\phi_4 = 0$. This corresponds to a vector $z = (1, 1, -1, 0)^T$ in the null space of $\\Phi$. Note that $\\|z\\|_0=3$, which is consistent with $\\mathrm{spark}(\\Phi)=3$.\n\nNow we must decompose $z$ as $z = x_1 - x_2$ where $x_1$ and $x_2$ are distinct $2$-sparse vectors.\nLet us define $x_1$ and $x_2$ based on the positive and negative entries of $z$.\nLet $x_1 = (1, 1, 0, 0)^T$. This vector is $2$-sparse since $\\|x_1\\|_0 = 2$.\nLet $x_2 = (0, 0, 1, 0)^T$. This vector is $1$-sparse since $\\|x_2\\|_0 = 1$. Since $1 \\le 2$, $x_2$ is also a $2$-sparse vector.\nThe vectors $x_1$ and $x_2$ are distinct.\nWe check that they generate the same signal $y$:\n$$\ny_1 = \\Phi x_1 = 1 \\cdot \\phi_1 + 1 \\cdot \\phi_2 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n$$\ny_2 = \\Phi x_2 = 1 \\cdot \\phi_3 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nSince $y_1 = y_2$, we have found two distinct $2$-sparse vectors, $x_1$ and $x_2$, that produce the same representation $y = (1, 1)^T$.\nThis demonstrates that non-unique $2$-sparse representations can and do exist for this dictionary $\\Phi$.", "answer": "$$\n\\boxed{3}\n$$", "id": "3434598"}, {"introduction": "While the $\\mathrm{spark}$ provides a sharp condition for uniqueness, practical signal recovery must also be stable in the presence of noise and resilient to small perturbations. The Restricted Isometry Property (RIP) offers a more powerful framework for guaranteeing such robust recovery by ensuring that a sensing matrix approximately preserves the lengths of sparse vectors. This practice provides a direct, hands-on calculation of the Restricted Isometry Constant (RIC), $\\delta_s$, for a given matrix, moving the abstract definition of RIP into the realm of concrete computation [@problem_id:3434608]. This will solidify your understanding of how matrix quality is quantitatively measured in compressed sensing.", "problem": "Consider the linear measurements model in compressed sensing, where a measurement matrix $A \\in \\mathbb{R}^{3 \\times 4}$ and a representation basis $\\Phi \\in \\mathbb{R}^{4 \\times 4}$ act on coefficient vectors $x \\in \\mathbb{R}^{4}$. Let the matrix $A$ and the basis $\\Phi$ be given by\n$$\nA=\\frac{1}{2}\\begin{bmatrix}\n1  0  1  -1 \\\\\n0  1  1  1 \\\\\n1  1  0  1\n\\end{bmatrix}, \\qquad \\Phi=I_4,\n$$\nwhere $I_4$ denotes the $4 \\times 4$ identity matrix. The Restricted Isometry Property (RIP) of order $s$ quantifies how well $A\\Phi$ preserves the Euclidean norm of $s$-sparse vectors, via the Restricted Isometry Constant $\\delta_s$ defined by the smallest nonnegative number such that\n$$\n(1-\\delta_s)\\|x\\|_2^2 \\le \\|A\\Phi x\\|_2^2 \\le (1+\\delta_s)\\|x\\|_2^2\n$$\nfor all $s$-sparse vectors $x$.\n\nFocus on the case $s=2$. Enumerate all $\\binom{4}{2}$ two-column submatrices of $A\\Phi$ induced by supports $T \\subset \\{1,2,3,4\\}$ with $|T|=2$, compute the smallest and largest singular values of each such $3 \\times 2$ submatrix, and, using the fundamental definition of the Restricted Isometry Constant, determine an exact, closed-form expression for $\\delta_2$ of $A\\Phi$. Express the final answer as an exact analytic expression with no rounding.", "solution": "The problem statement is parsed and validated. The problem is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. It is a standard exercise in compressed sensing theory. Therefore, we proceed with the solution.\n\nThe problem asks for the calculation of the Restricted Isometry Constant (RIC) $\\delta_2$ for a given matrix $A\\Phi$.\nLet the matrix of interest be $M = A\\Phi$. The input matrices are given by\n$$\nA=\\frac{1}{2}\\begin{bmatrix}\n1  0  1  -1 \\\\\n0  1  1  1 \\\\\n1  1  0  1\n\\end{bmatrix}, \\qquad \\Phi=I_4\n$$\nwhere $I_4$ is the $4 \\times 4$ identity matrix. Thus, the matrix $M$ is simply $A$:\n$$\nM = A = \\frac{1}{2}\\begin{bmatrix}\n1  0  1  -1 \\\\\n0  1  1  1 \\\\\n1  1  0  1\n\\end{bmatrix}\n$$\nThe Restricted Isometry Constant $\\delta_s$ is the smallest non-negative number satisfying the inequality\n$$\n(1-\\delta_s)\\|x\\|_2^2 \\le \\|M x\\|_2^2 \\le (1+\\delta_s)\\|x\\|_2^2\n$$\nfor all $s$-sparse vectors $x \\in \\mathbb{R}^4$. An $s$-sparse vector $x$ has at most $s$ non-zero entries. Let $T = \\text{supp}(x)$ be the set of indices of the non-zero entries of $x$, with $|T| \\le s$. Let $x_T$ be the subvector of $x$ containing only these non-zero entries, and let $M_T$ be the submatrix of $M$ formed by the columns indexed by $T$. Then $Mx = M_T x_T$. The condition becomes\n$$\n(1-\\delta_s)\\|x_T\\|_2^2 \\le \\|M_T x_T\\|_2^2 \\le (1+\\delta_s)\\|x_T\\|_2^2\n$$\nfor all $x_T \\in \\mathbb{R}^{|T|}$. This is equivalent to stating that all eigenvalues of the Gram matrices $M_T^T M_T$ must lie in the interval $[1-\\delta_s, 1+\\delta_s]$ for all index sets $T$ with $|T| = s$.\nLet $\\lambda_{\\min}(M_T^T M_T)$ and $\\lambda_{\\max}(M_T^T M_T)$ be the minimum and maximum eigenvalues of $M_T^T M_T$, respectively. These eigenvalues are equal to the squared singular values of $M_T$. To satisfy the condition for all $T$ with $|T|=s$, $\\delta_s$ must be chosen such that:\n$$\n\\max_{T:|T|=s} \\lambda_{\\max}(M_T^T M_T) \\le 1 + \\delta_s \\quad \\text{and} \\quad \\min_{T:|T|=s} \\lambda_{\\min}(M_T^T M_T) \\ge 1 - \\delta_s\n$$\nSince we seek the smallest such $\\delta_s \\ge 0$, we have:\n$$\n\\delta_s = \\max \\left( \\max_{T:|T|=s} \\lambda_{\\max}(M_T^T M_T) - 1, 1 - \\min_{T:|T|=s} \\lambda_{\\min}(M_T^T M_T) \\right)\n$$\nWe are interested in the case $s=2$. There are $\\binom{4}{2} = 6$ possible supports $T$ of size $2$. We must form the corresponding $3 \\times 2$ submatrices $M_T$ and compute the eigenvalues of the $2 \\times 2$ Gram matrices $M_T^T M_T$.\n\nLet the columns of $M$ be $m_1, m_2, m_3, m_4$.\n$$\nm_1 = \\frac{1}{2}\\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}, \\quad m_2 = \\frac{1}{2}\\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad m_3 = \\frac{1}{2}\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad m_4 = \\frac{1}{2}\\begin{pmatrix} -1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nThe entries of the Gram matrices $M_T^T M_T$ are the inner products $m_i^T m_j$. Let us pre-compute all of them. The full Gram matrix $M^T M$ is:\n$$\nM^T M = \\begin{pmatrix} m_1^T m_1  m_1^T m_2  m_1^T m_3  m_1^T m_4 \\\\ m_2^T m_1  m_2^T m_2  m_2^T m_3  m_2^T m_4 \\\\ m_3^T m_1  m_3^T m_2  m_3^T m_3  m_3^T m_4 \\\\ m_4^T m_1  m_4^T m_2  m_4^T m_3  m_4^T m_4 \\end{pmatrix}\n$$\nThe individual inner products are:\n$m_1^T m_1 = \\frac{1}{4}(1^2+0^2+1^2) = \\frac{2}{4} = \\frac{1}{2}$\n$m_2^T m_2 = \\frac{1}{4}(0^2+1^2+1^2) = \\frac{2}{4} = \\frac{1}{2}$\n$m_3^T m_3 = \\frac{1}{4}(1^2+1^2+0^2) = \\frac{2}{4} = \\frac{1}{2}$\n$m_4^T m_4 = \\frac{1}{4}((-1)^2+1^2+1^2) = \\frac{3}{4}$\n$m_1^T m_2 = \\frac{1}{4}(1\\cdot 0 + 0\\cdot 1 + 1\\cdot 1) = \\frac{1}{4}$\n$m_1^T m_3 = \\frac{1}{4}(1\\cdot 1 + 0\\cdot 1 + 1\\cdot 0) = \\frac{1}{4}$\n$m_1^T m_4 = \\frac{1}{4}(1\\cdot(-1) + 0\\cdot 1 + 1\\cdot 1) = 0$\n$m_2^T m_3 = \\frac{1}{4}(0\\cdot 1 + 1\\cdot 1 + 1\\cdot 0) = \\frac{1}{4}$\n$m_2^T m_4 = \\frac{1}{4}(0\\cdot(-1) + 1\\cdot 1 + 1\\cdot 1) = \\frac{2}{4} = \\frac{1}{2}$\n$m_3^T m_4 = \\frac{1}{4}(1\\cdot(-1) + 1\\cdot 1 + 0\\cdot 1) = 0$\n\nNow, we analyze the $6$ submatrices $M_T^T M_T$.\n\nCase 1: $T = \\{1, 2\\}$\n$M_{\\{1,2\\}}^T M_{\\{1,2\\}} = \\begin{pmatrix} m_1^T m_1  m_1^T m_2 \\\\ m_2^T m_1  m_2^T m_2 \\end{pmatrix} = \\begin{pmatrix} 1/2  1/4 \\\\ 1/4  1/2 \\end{pmatrix}$.\nThe eigenvalues $\\lambda$ satisfy $\\det(\\begin{pmatrix} 1/2-\\lambda  1/4 \\\\ 1/4  1/2-\\lambda \\end{pmatrix}) = 0$, so $(1/2-\\lambda)^2 - (1/4)^2 = 0$. This gives $\\lambda - 1/2 = \\pm 1/4$.\n$\\lambda_{\\max} = 1/2 + 1/4 = 3/4$.\n$\\lambda_{\\min} = 1/2 - 1/4 = 1/4$.\n\nCase 2: $T = \\{1, 3\\}$\n$M_{\\{1,3\\}}^T M_{\\{1,3\\}} = \\begin{pmatrix} m_1^T m_1  m_1^T m_3 \\\\ m_3^T m_1  m_3^T m_3 \\end{pmatrix} = \\begin{pmatrix} 1/2  1/4 \\\\ 1/4  1/2 \\end{pmatrix}$.\nThis is identical to the previous case.\n$\\lambda_{\\max} = 3/4$, $\\lambda_{\\min} = 1/4$.\n\nCase 3: $T = \\{1, 4\\}$\n$M_{\\{1,4\\}}^T M_{\\{1,4\\}} = \\begin{pmatrix} m_1^T m_1  m_1^T m_4 \\\\ m_4^T m_1  m_4^T m_4 \\end{pmatrix} = \\begin{pmatrix} 1/2  0 \\\\ 0  3/4 \\end{pmatrix}$.\nThis matrix is diagonal, so its eigenvalues are its diagonal entries.\n$\\lambda_{\\max} = 3/4$, $\\lambda_{\\min} = 1/2$.\n\nCase 4: $T = \\{2, 3\\}$\n$M_{\\{2,3\\}}^T M_{\\{2,3\\}} = \\begin{pmatrix} m_2^T m_2  m_2^T m_3 \\\\ m_3^T m_2  m_3^T m_3 \\end{pmatrix} = \\begin{pmatrix} 1/2  1/4 \\\\ 1/4  1/2 \\end{pmatrix}$.\nThis is identical to the first case.\n$\\lambda_{\\max} = 3/4$, $\\lambda_{\\min} = 1/4$.\n\nCase 5: $T = \\{2, 4\\}$\n$M_{\\{2,4\\}}^T M_{\\{2,4\\}} = \\begin{pmatrix} m_2^T m_2  m_2^T m_4 \\\\ m_4^T m_2  m_4^T m_4 \\end{pmatrix} = \\begin{pmatrix} 1/2  1/2 \\\\ 1/2  3/4 \\end{pmatrix}$.\nThe characteristic equation is $\\lambda^2 - \\text{tr}(M_T^T M_T)\\lambda + \\det(M_T^T M_T) = 0$.\n$\\text{tr} = 1/2 + 3/4 = 5/4$.\n$\\det = (1/2)(3/4) - (1/2)^2 = 3/8 - 1/4 = 1/8$.\n$\\lambda^2 - \\frac{5}{4}\\lambda + \\frac{1}{8} = 0$.\n$\\lambda = \\frac{5/4 \\pm \\sqrt{(5/4)^2 - 4(1/8)}}{2} = \\frac{5/4 \\pm \\sqrt{25/16 - 1/2}}{2} = \\frac{5/4 \\pm \\sqrt{17/16}}{2} = \\frac{5/4 \\pm \\sqrt{17}/4}{2} = \\frac{5 \\pm \\sqrt{17}}{8}$.\n$\\lambda_{\\max} = \\frac{5 + \\sqrt{17}}{8}$.\n$\\lambda_{\\min} = \\frac{5 - \\sqrt{17}}{8}$.\n\nCase 6: $T = \\{3, 4\\}$\n$M_{\\{3,4\\}}^T M_{\\{3,4\\}} = \\begin{pmatrix} m_3^T m_3  m_3^T m_4 \\\\ m_4^T m_3  m_4^T m_4 \\end{pmatrix} = \\begin{pmatrix} 1/2  0 \\\\ 0  3/4 \\end{pmatrix}$.\nThis is identical to case $3$.\n$\\lambda_{\\max} = 3/4$, $\\lambda_{\\min} = 1/2$.\n\nWe summarize the eigenvalues for all $s=2$ submatrices:\n- $T = \\{1,2\\}: \\lambda_{\\min}=1/4$, $\\lambda_{\\max}=3/4$.\n- $T = \\{1,3\\}: \\lambda_{\\min}=1/4$, $\\lambda_{\\max}=3/4$.\n- $T = \\{1,4\\}: \\lambda_{\\min}=1/2$, $\\lambda_{\\max}=3/4$.\n- $T = \\{2,3\\}: \\lambda_{\\min}=1/4$, $\\lambda_{\\max}=3/4$.\n- $T = \\{2,4\\}: \\lambda_{\\min}=\\frac{5-\\sqrt{17}}{8}$, $\\lambda_{\\max}=\\frac{5+\\sqrt{17}}{8}$.\n- $T = \\{3,4\\}: \\lambda_{\\min}=1/2$, $\\lambda_{\\max}=3/4$.\n\nNow we find the overall minimum and maximum eigenvalues.\n$\\min_{T:|T|=2} \\lambda_{\\min}(M_T^T M_T) = \\min\\{1/4, 1/2, \\frac{5-\\sqrt{17}}{8}\\}$.\nSince $4  \\sqrt{17}  5$, specifically $4.12  \\sqrt{17}  4.13$, we have $5-\\sqrt{17}  1$. So $\\frac{5-\\sqrt{17}}{8}  1/8$. Also $1/4=2/8$. Thus $\\frac{5-\\sqrt{17}}{8}$ is the smallest value.\n$\\min_{T:|T|=2} \\lambda_{\\min}(M_T^T M_T) = \\frac{5-\\sqrt{17}}{8}$.\n\n$\\max_{T:|T|=2} \\lambda_{\\max}(M_T^T M_T) = \\max\\{3/4, \\frac{5+\\sqrt{17}}{8}\\}$.\n$3/4 = 6/8$. Since $\\sqrt{17}  1$, we have $5+\\sqrt{17}  6$. So $\\frac{5+\\sqrt{17}}{8}  6/8$.\n$\\max_{T:|T|=2} \\lambda_{\\max}(M_T^T M_T) = \\frac{5+\\sqrt{17}}{8}$.\n\nFinally, we compute $\\delta_2$:\n$$\n\\delta_2 = \\max \\left( \\frac{5+\\sqrt{17}}{8} - 1, 1 - \\frac{5-\\sqrt{17}}{8} \\right)\n$$\n$$\n\\delta_2 = \\max \\left( \\frac{5+\\sqrt{17}-8}{8}, \\frac{8-(5-\\sqrt{17})}{8} \\right)\n$$\n$$\n\\delta_2 = \\max \\left( \\frac{\\sqrt{17}-3}{8}, \\frac{3+\\sqrt{17}}{8} \\right)\n$$\nSince $\\sqrt{17}0$, it is evident that $3+\\sqrt{17}  \\sqrt{17}-3$.\nTherefore, the restricted isometry constant is:\n$$\n\\delta_2 = \\frac{3+\\sqrt{17}}{8}\n$$", "answer": "$$\\boxed{\\frac{3+\\sqrt{17}}{8}}$$", "id": "3434608"}, {"introduction": "With an understanding of the theoretical properties that enable sparse recovery, we can now turn to the algorithms that achieve it in practice. Compressive Sampling Matching Pursuit (CoSaMP) is a powerful iterative algorithm that elegantly balances the identification of significant signal components with the enforcement of sparsity. This exercise guides you through one full iteration of CoSaMP, from computing a proxy to identify potential support, to refining the estimate with a least-squares solve, and finally pruning the result [@problem_id:3434637]. This step-by-step process demystifies the inner workings of a key recovery algorithm and demonstrates how theoretical principles are operationalized.", "problem": "Consider a linear measurement model in compressed sensing, where a signal $x \\in \\mathbb{R}^{5}$ is $k$-sparse in the canonical basis (i.e., the change-of-basis matrix is the identity), and the measurements are given by $y = A x$ with $A \\in \\mathbb{R}^{3 \\times 5}$. Let $A = \\begin{bmatrix} 1  0  1  0  0 \\\\ 0  1  1  1  0 \\\\ 1  0  0  1  1 \\end{bmatrix}$, $\\Phi = I_{5}$, and $x = \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\\\ 0 \\\\ 0 \\end{bmatrix}$, so that $y = A x$. Start from the foundational definitions: linear measurements $y = A x$, sparsity of $x$ in the identity basis, support as the set of indices of nonzero entries, and least-squares fitting as minimizing the Euclidean norm of the residual. Perform one full iteration of Compressive Sampling Matching Pursuit (CoSaMP), with target sparsity $k = 2$, starting from the zero initial estimate $x^{(0)} = 0$ and residual $r^{(0)} = y$. Follow these fundamental steps:\n\n- Compute the proxy $u = A^{\\top} r^{(0)}$.\n- Identify the index set $\\Omega$ corresponding to the $2k$ largest entries of $|u|$ in magnitude.\n- Merge with the current support to form $\\mathcal{T} = \\Omega \\cup \\operatorname{supp}(x^{(0)})$.\n- Solve the least-squares problem $b = \\arg\\min_{z \\in \\mathbb{R}^{5},\\, \\operatorname{supp}(z) \\subseteq \\mathcal{T}} \\| y - A z \\|_{2}$; if there are multiple minimizers, choose the one with minimal Euclidean norm.\n- Prune $b$ to its $k$ largest entries in magnitude to obtain $x^{(1)}$.\n- Form the updated residual $r^{(1)} = y - A x^{(1)}$.\n\nWhat is the Euclidean norm $\\| r^{(1)} \\|_{2}$? Provide your final answer as a closed-form analytic expression. Do not approximate or round.", "solution": "The problem requires performing one full iteration of the Compressive Sampling Matching Pursuit (CoSaMP) algorithm to find the Euclidean norm of the updated residual, $\\| r^{(1)} \\|_{2}$. We are given the linear measurement model $y = Ax$, where the signal $x \\in \\mathbb{R}^{5}$ is sparse in the canonical basis.\n\nThe provided data are:\n- Measurement matrix: $A = \\begin{bmatrix} 1  0  1  0  0 \\\\ 0  1  1  1  0 \\\\ 1  0  0  1  1 \\end{bmatrix} \\in \\mathbb{R}^{3 \\times 5}$\n- True signal: $x = \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\\\ 0 \\\\ 0 \\end{bmatrix} \\in \\mathbb{R}^{5}$\n- Target sparsity for the algorithm: $k = 2$\n- Initial signal estimate: $x^{(0)} = 0 \\in \\mathbb{R}^{5}$\n- The change-of-basis matrix is the identity matrix $\\Phi = I_{5}$.\n\nThe iteration follows a prescribed sequence of steps. First, we compute the measurement vector $y$.\n$$y = Ax = \\begin{bmatrix} 1  0  1  0  0 \\\\ 0  1  1  1  0 \\\\ 1  0  0  1  1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1(1) + 1(2) \\\\ 1(2) \\\\ 1(1) \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 2 \\\\ 1 \\end{bmatrix}$$\nThe initial residual is $r^{(0)} = y = \\begin{bmatrix} 3 \\\\ 2 \\\\ 1 \\end{bmatrix}$.\n\nNow, we proceed with the steps of the CoSaMP iteration.\n\n1.  **Compute the proxy**: The proxy $u$ is given by $u = A^{\\top} r^{(0)}$.\n    $$A^{\\top} = \\begin{bmatrix} 1  0  1 \\\\ 0  1  0 \\\\ 1  1  0 \\\\ 0  1  1 \\\\ 0  0  1 \\end{bmatrix}$$\n    $$u = A^{\\top} r^{(0)} = \\begin{bmatrix} 1  0  1 \\\\ 0  1  0 \\\\ 1  1  0 \\\\ 0  1  1 \\\\ 0  0  1 \\end{bmatrix} \\begin{bmatrix} 3 \\\\ 2 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1(3) + 1(1) \\\\ 1(2) \\\\ 1(3) + 1(2) \\\\ 1(2) + 1(1) \\\\ 1(1) \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 2 \\\\ 5 \\\\ 3 \\\\ 1 \\end{bmatrix}$$\n\n2.  **Identify the support**: We identify the index set $\\Omega$ corresponding to the $2k$ largest entries of $|u|$ in magnitude. Here, $k=2$, so $2k=4$. The vector $u$ has all positive entries, so $|u|=u$. The entries of $u$ in descending order of magnitude are $5, 4, 3, 2$, which correspond to indices $3, 1, 4, 2$.\n    $$\\Omega = \\{1, 2, 3, 4\\}$$\n\n3.  **Merge the support**: The new candidate support $\\mathcal{T}$ is the union of $\\Omega$ and the support of the previous estimate $x^{(0)}$.\n    $$\\operatorname{supp}(x^{(0)}) = \\operatorname{supp}(0) = \\emptyset$$\n    $$\\mathcal{T} = \\Omega \\cup \\operatorname{supp}(x^{(0)}) = \\{1, 2, 3, 4\\}$$\n\n4.  **Solve the least-squares problem**: We must find the vector $b$ with support contained in $\\mathcal{T}$ that minimizes $\\|y - Az\\|_2$. This is equivalent to solving for $b_{\\mathcal{T}}$ from the system $A_{\\mathcal{T}} b_{\\mathcal{T}} \\approx y$ in a least-squares sense, where $A_{\\mathcal{T}}$ is the submatrix of $A$ containing the columns indexed by $\\mathcal{T}$.\n    $$A_{\\mathcal{T}} = \\begin{bmatrix} 1  0  1  0 \\\\ 0  1  1  1 \\\\ 1  0  0  1 \\end{bmatrix}$$\n    The problem specifies that if there are multiple minimizers, we choose the one with minimal Euclidean norm. This corresponds to the solution $b_{\\mathcal{T}} = A_{\\mathcal{T}}^{\\dagger} y$, where $A_{\\mathcal{T}}^{\\dagger}$ is the Moore-Penrose pseudoinverse of $A_{\\mathcal{T}}$. Since $A_{\\mathcal{T}}$ is a $3 \\times 4$ matrix with rank $3$ (it has full row rank), its pseudoinverse is given by $A_{\\mathcal{T}}^{\\dagger} = A_{\\mathcal{T}}^{\\top}(A_{\\mathcal{T}}A_{\\mathcal{T}}^{\\top})^{-1}$.\n    First, we compute $A_{\\mathcal{T}}A_{\\mathcal{T}}^{\\top}$:\n    $$A_{\\mathcal{T}}A_{\\mathcal{T}}^{\\top} = \\begin{bmatrix} 1  0  1  0 \\\\ 0  1  1  1 \\\\ 1  0  0  1 \\end{bmatrix} \\begin{bmatrix} 1  0  1 \\\\ 0  1  0 \\\\ 1  1  0 \\\\ 0  1  1 \\end{bmatrix} = \\begin{bmatrix} 2  1  1 \\\\ 1  3  1 \\\\ 1  1  2 \\end{bmatrix}$$\n    Next, we find the inverse of this $3 \\times 3$ matrix. The determinant is $\\det(A_{\\mathcal{T}}A_{\\mathcal{T}}^{\\top}) = 2(6-1) - 1(2-1) + 1(1-3) = 10 - 1 - 2 = 7$.\n    The inverse is:\n    $$(A_{\\mathcal{T}}A_{\\mathcal{T}}^{\\top})^{-1} = \\frac{1}{7} \\begin{bmatrix} 5  -1  -2 \\\\ -1  3  -1 \\\\ -2  -1  5 \\end{bmatrix}$$\n    Now we can calculate $b_{\\mathcal{T}}$:\n    $$b_{\\mathcal{T}} = A_{\\mathcal{T}}^{\\top}(A_{\\mathcal{T}}A_{\\mathcal{T}}^{\\top})^{-1} y = \\begin{bmatrix} 1  0  1 \\\\ 0  1  0 \\\\ 1  1  0 \\\\ 0  1  1 \\end{bmatrix} \\left( \\frac{1}{7} \\begin{bmatrix} 5  -1  -2 \\\\ -1  3  -1 \\\\ -2  -1  5 \\end{bmatrix} \\begin{bmatrix} 3 \\\\ 2 \\\\ 1 \\end{bmatrix} \\right)$$\n    $$b_{\\mathcal{T}} = \\begin{bmatrix} 1  0  1 \\\\ 0  1  0 \\\\ 1  1  0 \\\\ 0  1  1 \\end{bmatrix} \\left( \\frac{1}{7} \\begin{bmatrix} 15-2-2 \\\\ -3+6-1 \\\\ -6-2+5 \\end{bmatrix} \\right) = \\begin{bmatrix} 1  0  1 \\\\ 0  1  0 \\\\ 1  1  0 \\\\ 0  1  1 \\end{bmatrix} \\left( \\frac{1}{7} \\begin{bmatrix} 11 \\\\ 2 \\\\ -3 \\end{bmatrix} \\right) = \\frac{1}{7} \\begin{bmatrix} 11-3 \\\\ 2 \\\\ 11+2 \\\\ 2-3 \\end{bmatrix} = \\frac{1}{7} \\begin{bmatrix} 8 \\\\ 2 \\\\ 13 \\\\ -1 \\end{bmatrix}$$\n    The full vector $b \\in \\mathbb{R}^5$ is constructed by placing these values at the indices in $\\mathcal{T}$ and zeros elsewhere:\n    $$b = \\begin{bmatrix} 8/7 \\\\ 2/7 \\\\ 13/7 \\\\ -1/7 \\\\ 0 \\end{bmatrix}$$\n\n5.  **Prune the estimate**: We obtain the new signal estimate $x^{(1)}$ by keeping the $k=2$ largest-magnitude components of $b$. The magnitudes of the non-zero components of $b$ are $|8/7|$, $|2/7|$, $|13/7|$, and $|-1/7|$, which are $8/7$, $2/7$, $13/7$, and $1/7$. The two largest are $13/7$ (at index $3$) and $8/7$ (at index $1$).\n    Thus, we set all other components to zero:\n    $$x^{(1)} = \\begin{bmatrix} 8/7 \\\\ 0 \\\\ 13/7 \\\\ 0 \\\\ 0 \\end{bmatrix}$$\n\n6.  **Update the residual**: The new residual $r^{(1)}$ is $r^{(1)} = y - Ax^{(1)}$.\n    $$Ax^{(1)} = \\begin{bmatrix} 1  0  1  0  0 \\\\ 0  1  1  1  0 \\\\ 1  0  0  1  1 \\end{bmatrix} \\begin{bmatrix} 8/7 \\\\ 0 \\\\ 13/7 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 8/7 + 13/7 \\\\ 13/7 \\\\ 8/7 \\end{bmatrix} = \\begin{bmatrix} 21/7 \\\\ 13/7 \\\\ 8/7 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 13/7 \\\\ 8/7 \\end{bmatrix}$$\n    $$r^{(1)} = y - Ax^{(1)} = \\begin{bmatrix} 3 \\\\ 2 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 3 \\\\ 13/7 \\\\ 8/7 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 14/7 - 13/7 \\\\ 7/7 - 8/7 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1/7 \\\\ -1/7 \\end{bmatrix}$$\n\nFinally, we compute the Euclidean norm of the updated residual $r^{(1)}$.\n$$\\|r^{(1)}\\|_{2} = \\sqrt{0^2 + (1/7)^2 + (-1/7)^2} = \\sqrt{\\frac{1}{49} + \\frac{1}{49}} = \\sqrt{\\frac{2}{49}} = \\frac{\\sqrt{2}}{7}$$\nThe Euclidean norm of the residual after one iteration is $\\frac{\\sqrt{2}}{7}$.", "answer": "$$\\boxed{\\frac{\\sqrt{2}}{7}}$$", "id": "3434637"}]}