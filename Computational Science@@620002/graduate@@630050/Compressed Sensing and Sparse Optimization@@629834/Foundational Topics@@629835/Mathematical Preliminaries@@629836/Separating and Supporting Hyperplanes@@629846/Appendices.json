{"hands_on_practices": [{"introduction": "We begin with a foundational exercise to build geometric intuition. This practice involves finding a separating hyperplane between two simple, disjoint convex sets: a unit disk and a single point. By constructing the maximum-margin separating hyperplane from first principles, you will engage with core concepts such as projection onto a convex set and see how the geometry of the problem directly dictates the solution [@problem_id:3475696].", "problem": "Consider the role of separating and supporting hyperplanes in sparse optimization and compressed sensing, where convex geometry governs feasibility and recovery via dual certificates. Let the closed convex set be the Euclidean unit ball $C=\\{x\\in \\mathbb{R}^{2}:\\|x\\|_{2}\\leq 1\\}$ and the singleton set be $D=\\{y_{0}\\}$ with $y_{0}=(2,0)^{\\top}$. Starting from first principles (definitions of convexity, support function, projection onto a closed convex set, and the Cauchy–Schwarz inequality), derive the maximum-margin separating hyperplane between $C$ and $D$ and determine whether the separation is strong in the sense that there exist $a\\in \\mathbb{R}^{2}$ and scalars $\\alpha<\\beta$ such that $a^{\\top}x\\leq \\alpha$ for all $x\\in C$ and $a^{\\top}y\\geq \\beta$ for all $y\\in D$. Represent the hyperplane in normal form $a^{\\top}x=b$ with the constraint $\\|a\\|_{2}=1$, choose $b$ to be the support value of $C$ in the direction $a$, and compute the geometric margin $\\gamma$ defined by $\\gamma=\\min_{y\\in D}a^{\\top}y-\\sup_{x\\in C}a^{\\top}x$. Provide exact values. Return your answer as a single row matrix $(a_{1}\\ a_{2}\\ b\\ \\gamma)$, where $(a_{1},a_{2})$ are the components of $a$, $b$ is the offset, and $\\gamma$ is the margin. No rounding is required.", "solution": "The problem is first validated to ensure it is well-posed, scientifically sound, and complete.\n\n### Step 1: Extract Givens\n- Closed convex set: $C = \\{x \\in \\mathbb{R}^{2} : \\|x\\|_{2} \\leq 1\\}$. This is the closed unit ball in $\\mathbb{R}^{2}$.\n- Singleton set: $D = \\{y_{0}\\}$ where $y_{0} = (2, 0)^{\\top}$.\n- Hyperplane representation: $a^{\\top}x = b$ with the normalization constraint $\\|a\\|_{2} = 1$.\n- Definition of offset $b$: $b = \\sup_{x \\in C} a^{\\top}x$.\n- Definition of strong separation: Existence of $a \\in \\mathbb{R}^{2}$ and scalars $\\alpha < \\beta$ such that $a^{\\top}x \\leq \\alpha$ for all $x \\in C$ and $a^{\\top}y \\geq \\beta$ for all $y \\in D$.\n- Definition of geometric margin $\\gamma$: $\\gamma = \\min_{y \\in D} a^{\\top}y - \\sup_{x \\in C} a^{\\top}x$.\n- Required output: A row matrix $(a_{1}, a_{2}, b, \\gamma)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined and mathematically sound. It involves fundamental concepts from convex analysis: convex sets, separating hyperplanes, and support functions. The sets $C$ and $D$ are non-empty, convex, and disjoint. $C$ is compact (closed and bounded) and $D$ is compact (as it is a singleton). The separating hyperplane theorem and its strong-separation corollary for disjoint compact convex sets guarantee that a strongly separating hyperplane exists. The problem is self-contained, objective, and asks for a computable result based on standard definitions. Therefore, the problem is deemed valid.\n\n### Step 3: Derivation of the Solution\nThe maximum-margin separating hyperplane between two disjoint convex sets is determined by the vector connecting their two closest points. Let $x^{*} \\in C$ and $y^{*} \\in D$ be these points. The normal vector to the hyperplane, $a$, will be parallel to the vector $y^{*} - x^{*}$.\n\nSince $D = \\{y_{0}\\}$ is a singleton set, the closest point in $D$ is trivially $y^{*} = y_{0} = (2, 0)^{\\top}$.\n\nThe problem now is to find the point $x^{*} \\in C$ that is closest to $y_{0}$. This point is the projection of $y_{0}$ onto the closed convex set $C$. We denote this as $x^{*} = P_{C}(y_{0})$.\nThe set $C$ is the closed unit ball in $\\mathbb{R}^{2}$. The projection of a point $z$ onto $C$ is given by\n$$P_{C}(z) = \\begin{cases} z & \\text{if } \\|z\\|_{2} \\leq 1 \\\\ \\frac{z}{\\|z\\|_{2}} & \\text{if } \\|z\\|_{2} > 1 \\end{cases}$$\nFor our point $y_{0} = (2, 0)^{\\top}$, the Euclidean norm is $\\|y_{0}\\|_{2} = \\sqrt{2^{2} + 0^{2}} = 2$.\nSince $\\|y_{0}\\|_{2} = 2 > 1$, the projection is:\n$$x^{*} = P_{C}(y_{0}) = \\frac{y_{0}}{\\|y_{0}\\|_{2}} = \\frac{(2, 0)^{\\top}}{2} = (1, 0)^{\\top}$$\n\nThe vector connecting the two closest points is $y_{0} - x^{*} = (2, 0)^{\\top} - (1, 0)^{\\top} = (1, 0)^{\\top}$.\nThe normal vector $a$ of the separating hyperplane must be proportional to this vector. The problem states that $a$ must be a unit vector, i.e., $\\|a\\|_{2} = 1$. We normalize the vector $y_{0} - x^{*}$:\n$$a = \\frac{y_{0} - x^{*}}{\\|y_{0} - x^{*}\\|_{2}} = \\frac{(1, 0)^{\\top}}{\\sqrt{1^{2} + 0^{2}}} = \\frac{(1, 0)^{\\top}}{1} = (1, 0)^{\\top}$$\nThus, the components of $a$ are $a_{1} = 1$ and $a_{2} = 0$.\n\nNext, we determine the offset $b$. The problem defines $b$ as the support value of $C$ in the direction $a$:\n$$b = \\sup_{x \\in C} a^{\\top}x$$\nThis is the definition of the support function of $C$, $\\sigma_{C}(a)$. For $C = \\{x : \\|x\\|_{2} \\leq 1\\}$, the support function relates to the norm of the direction vector. By the Cauchy-Schwarz inequality, $a^{\\top}x \\leq \\|a\\|_{2}\\|x\\|_{2}$. Since $\\|x\\|_{2} \\leq 1$ for all $x \\in C$ and $\\|a\\|_{2} = 1$, we have $a^{\\top}x \\leq 1$. This supremum is achieved when $x$ is a unit vector in the same direction as $a$, i.e., $x = a/\\|a\\|_{2} = a$.\nIn our case, with $a = (1, 0)^{\\top}$:\n$$b = \\sup_{x=(x_{1}, x_{2})^{\\top} \\in C} (1, 0) \\cdot (x_{1}, x_{2})^{\\top} = \\sup_{x_{1}^{2}+x_{2}^{2} \\leq 1} x_{1}$$\nThe maximum value of $x_{1}$ for a point within or on the unit circle is $1$, achieved at $x = (1, 0)^{\\top}$, which is our $x^{*}$.\nTherefore, $b = 1$.\nThe separating hyperplane is $a^{\\top}x = b$, which is $1 \\cdot x_{1} + 0 \\cdot x_{2} = 1$, or simply $x_{1} = 1$.\n\nWe now verify if the separation is strong. We need to find scalars $\\alpha < \\beta$ such that $a^{\\top}x \\leq \\alpha$ for all $x \\in C$ and $a^{\\top}y \\geq \\beta$ for all $y \\in D$.\nFor any $x \\in C$, we have $a^{\\top}x = x_{1} \\leq 1$. So, we have $\\sup_{x \\in C} a^{\\top}x = 1$. We can choose $\\alpha = 1$.\nFor any $y \\in D$, $y$ is just $y_{0} = (2, 0)^{\\top}$. We have $a^{\\top}y_{0} = (1, 0) \\cdot (2, 0)^{\\top} = 2$. So, we have $\\inf_{y \\in D} a^{\\top}y = 2$. We can choose $\\beta = 2$.\nSince $\\alpha = 1 < \\beta = 2$, strong separation is confirmed.\n\nFinally, we compute the geometric margin $\\gamma$, defined as:\n$$\\gamma = \\min_{y \\in D} a^{\\top}y - \\sup_{x \\in C} a^{\\top}x$$\nFrom our previous calculations:\nThe term $\\min_{y \\in D} a^{\\top}y$ is simply $a^{\\top}y_{0}$, which is $2$.\nThe term $\\sup_{x \\in C} a^{\\top}x$ is $b$, which is $1$.\nSo, the margin is:\n$$\\gamma = 2 - 1 = 1$$\nThis margin is equal to the distance between the two closest points, $\\|y_{0} - x^{*}\\|_{2} = \\|(1, 0)^{\\top}\\|_{2} = 1$, as expected for a maximum-margin separator.\n\nThe final result is the row matrix $(a_{1}, a_{2}, b, \\gamma)$. Substituting our computed values: $(1, 0, 1, 1)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 0 & 1 & 1\n\\end{pmatrix}\n}\n$$", "id": "3475696"}, {"introduction": "Building on the geometric concept of separation, we now apply it to an algebraic problem: certifying the infeasibility of a system of linear inequalities. This exercise leverages Farkas’ lemma, a fundamental theorem of the alternative, to construct an explicit certificate of impossibility. You will see how a linear combination of inequalities can reveal a fundamental contradiction, which is geometrically equivalent to finding a hyperplane that separates a convex cone from a point [@problem_id:3475727].", "problem": "In compressed sensing and sparse optimization, infeasibility of linear inequality systems often admits a finite certificate interpretable as a separating hyperplane for a convex cone of slacks. Consider the system of linear inequalities defined by the matrix $A \\in \\mathbb{R}^{2 \\times 2}$ and vector $b \\in \\mathbb{R}^{2}$ given by\n$$\nA=\\begin{pmatrix}1 & -1\\\\ -1 & 1\\end{pmatrix},\\qquad b=\\begin{pmatrix}-1\\\\ -1\\end{pmatrix}.\n$$\nAssume familiarity with Farkas’ lemma as a foundational separation result. Use this lemma as the core starting point to produce an explicit separating hyperplane certifying the infeasibility of the system $Ax \\le b$. Concretely, construct a nonzero vector $y \\in \\mathbb{R}^{2}$ with $y \\ge 0$ such that $y^{\\top}A=0$ and normalize it by the condition $y^{\\top}b=-1$. Report this $y$ explicitly. No rounding is required, and your final answer should be given exactly as a vector. Do not include any units.", "solution": "The problem requires the construction of a certificate of infeasibility for a given system of linear inequalities, $Ax \\le b$. The theoretical foundation for such a certificate is Farkas' lemma.\n\nOne form of Farkas' lemma states that for a matrix $A \\in \\mathbb{R}^{m \\times n}$ and a vector $b \\in \\mathbb{R}^{m}$, exactly one of the following two statements is true:\n1.  There exists a vector $x \\in \\mathbb{R}^{n}$ such that $Ax \\le b$. (The system is feasible).\n2.  There exists a vector $y \\in \\mathbb{R}^{m}$ such that $y \\ge 0$, $y^{\\top}A = 0$, and $y^{\\top}b < 0$.\n\nThe existence of such a vector $y$ serves as a certificate that the system $Ax \\le b$ is infeasible. The problem asks for an explicit construction of this vector $y$ for the given $A$ and $b$, with an additional normalization condition.\n\nFirst, let us write out the system of inequalities explicitly. We are given:\n$$A=\\begin{pmatrix}1 & -1\\\\ -1 & 1\\end{pmatrix}, \\qquad b=\\begin{pmatrix}-1\\\\ -1\\end{pmatrix}$$\nLet $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathbb{R}^{2}$. The system $Ax \\le b$ is:\n$$ \\begin{pmatrix}1 & -1\\\\ -1 & 1\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\le \\begin{pmatrix}-1\\\\ -1\\end{pmatrix} $$\nThis corresponds to the two inequalities:\n1. $x_1 - x_2 \\le -1$\n2. $-x_1 + x_2 \\le -1$\n\nThe second inequality can be multiplied by $-1$, which reverses the inequality sign:\n$x_1 - x_2 \\ge 1$.\nSo, the system requires that $x_1 - x_2 \\le -1$ and $x_1 - x_2 \\ge 1$ simultaneously. This is a contradiction, as a number cannot be both less than or equal to $-1$ and greater than or equal to $1$. Therefore, the system is indeed infeasible, as suggested by the problem statement.\n\nNow, we proceed to find the certificate vector $y = \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} \\in \\mathbb{R}^{2}$. According to the problem, this vector must satisfy three conditions:\n(i) $y \\ge 0$, which means $y_1 \\ge 0$ and $y_2 \\ge 0$.\n(ii) $y^{\\top}A = 0$.\n(iii) $y^{\\top}b = -1$.\n\nLet's solve for $y$ using these conditions.\n\nCondition (ii): $y^{\\top}A = 0$.\nThis is a matrix-vector product, where $y^{\\top}$ is a row vector:\n$$ \\begin{pmatrix} y_1 & y_2 \\end{pmatrix} \\begin{pmatrix}1 & -1\\\\ -1 & 1\\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\end{pmatrix} $$\nPerforming the multiplication gives:\n$$ \\begin{pmatrix} y_1 - y_2 & -y_1 + y_2 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\end{pmatrix} $$\nThis yields a single independent equation:\n$$ y_1 - y_2 = 0 \\implies y_1 = y_2 $$\nThis means the vector $y$ must be of the form $y = \\begin{pmatrix} k \\\\ k \\end{pmatrix}$ for some scalar $k \\in \\mathbb{R}$.\n\nCondition (i): $y \\ge 0$.\nFor $y = \\begin{pmatrix} k \\\\ k \\end{pmatrix}$ to satisfy $y \\ge 0$, we must have $k \\ge 0$. The problem asks for a nonzero vector $y$, so we require $k > 0$.\n\nCondition (iii): $y^{\\top}b = -1$.\nWe substitute the form of $y$ and the given vector $b$ into this equation:\n$$ y^{\\top}b = \\begin{pmatrix} k & k \\end{pmatrix} \\begin{pmatrix}-1\\\\ -1\\end{pmatrix} = -1 $$\n$$ k(-1) + k(-1) = -1 $$\n$$ -2k = -1 $$\nSolving for $k$, we find:\n$$ k = \\frac{1}{2} $$\nThis value $k = \\frac{1}{2}$ satisfies the condition $k > 0$.\n\nTherefore, the vector $y$ is:\n$$ y = \\begin{pmatrix} k \\\\ k \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} $$\n\nThis vector $y$ is the required certificate of infeasibility. We can verify that it satisfies all the conditions.\n- $y = \\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix}$ is nonzero and its components are non-negative.\n- $y^{\\top}A = \\begin{pmatrix} 1/2 & 1/2 \\end{pmatrix} \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1/2-1/2 & -1/2+1/2 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\end{pmatrix}$.\n- $y^{\\top}b = \\begin{pmatrix} 1/2 & 1/2 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix} = -1/2 - 1/2 = -1$.\n\nThe vector $y$ can be interpreted as the weights of a linear combination of the original inequalities that results in the contradiction $0 \\le -1$. Multiplying the first inequality by $y_1 = 1/2$ and the second by $y_2 = 1/2$ and adding them yields $(y^{\\top}A)x \\le y^{\\top}b$, which simplifies to $0 \\le -1$. This confirms how $y$ certifies impossibility.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2} \\\\\n\\frac{1}{2}\n\\end{pmatrix}\n}\n$$", "id": "3475727"}, {"introduction": "Our final practice explores the critical role of supporting hyperplanes in certifying optimality for convex optimization problems, a cornerstone of sparse recovery. Using the subgradient of the $\\ell_1$-norm, you will construct a dual certificate to verify whether a candidate solution for a Basis Pursuit problem is truly optimal. This exercise [@problem_id:3475665] provides a hands-on understanding of duality and how the geometric properties of the $\\ell_1$-ball's boundary are used to establish rigorous proofs of correctness in sparse signal processing.", "problem": "Consider the Basis Pursuit problem in compressed sensing: minimize the $\\ell_1$-norm subject to linear measurements,\n$$\\min_{x \\in \\mathbb{R}^{n}} \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = y,$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ and $y \\in \\mathbb{R}^{m}$ are given. A dual certificate is a vector $u \\in \\mathbb{R}^{m}$ such that the Karush–Kuhn–Tucker conditions hold, which can be characterized via the subgradient of the $\\ell_1$-norm: $A^{\\top} u \\in \\partial \\|x^{\\star}\\|_{1}$ and $y^{\\top} u = \\|x^{\\star}\\|_{1}$, where $x^{\\star}$ is a candidate primal solution. The subgradient $\\partial \\|x^{\\star}\\|_{1}$ at $x^{\\star}$ consists of vectors $s \\in \\mathbb{R}^{n}$ with $s_{j} = \\operatorname{sign}(x^{\\star}_{j})$ for indices $j$ in the support of $x^{\\star}$ and $s_{j} \\in [-1,1]$ for indices $j$ outside the support. Geometrically, this amounts to the existence of a supporting hyperplane for the $\\ell_1$-ball at $x^{\\star}$ that is consistent with the measurement operator.\n\nStarting from the foundational definitions of convex duality in optimization (Fenchel conjugacy and the subgradient characterization of the $\\ell_1$-norm) and the separating/supporting hyperplane interpretation of optimality in convex programs, analyze the following instance:\n- $A = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix}$,\n- $y = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$,\n- $x^{\\star} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\n\nUsing only first principles from convex analysis (the definition of the subgradient of the $\\ell_1$-norm, Lagrangian duality for equality-constrained convex programs, and the geometric interpretation of supporting hyperplanes), determine whether there exists a dual certificate $u \\in \\mathbb{R}^{2}$ that certifies optimality of $x^{\\star}$. If such a $u$ exists, construct it explicitly. If none exists, compute the unique value of $(A^{\\top} u)_{3}$ that is forced by the subgradient equalities on the support of $x^{\\star}$. Provide your final answer as a single real number or a single closed-form analytic expression. No rounding is required.", "solution": "The problem asks us to determine the existence of a dual certificate $u$ for a candidate solution $x^{\\star}$ to a Basis Pursuit problem, using first principles of convex analysis.\n\nThe Basis Pursuit problem is given by:\n$$ \\min_{x \\in \\mathbb{R}^{n}} \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = y $$\nHere, $n=3$, $m=2$, and the given data are:\n$$ A = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix}, \\quad y = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad x^{\\star} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} $$\n\nFirst, we verify if the candidate solution $x^{\\star}$ is feasible, i.e., if it satisfies the constraint $A x^{\\star} = y$.\n$$ A x^{\\star} = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (1)(1) + (0)(1) + (1)(0) \\\\ (0)(1) + (1)(1) + (1)(0) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\nSince this matches the given vector $y$, $x^{\\star}$ is a feasible point.\n\nFor $x^{\\star}$ to be an optimal solution, it must satisfy the Karush-Kuhn-Tucker (KKT) conditions. For an equality-constrained convex problem, the KKT conditions derived from the Lagrangian $L(x, u) = \\|x\\|_{1} + u^{\\top}(y - Ax)$ are primal feasibility and stationarity. Since $\\|x\\|_1$ is non-differentiable, the stationarity condition is expressed using subgradients:\n$$ 0 \\in \\partial_{x} L(x^{\\star}, u) = \\partial \\|x^{\\star}\\|_{1} - A^{\\top}u $$\nThis is equivalent to the existence of a dual vector (certificate) $u \\in \\mathbb{R}^{m}$ such that $A^{\\top}u \\in \\partial \\|x^{\\star}\\|_{1}$. A vector $s = A^{\\top}u$ that satisfies this condition is called a dual polynomial.\n\nLet's characterize the subgradient of the $\\ell_1$-norm at $x^{\\star}$. The set $\\partial \\|x^{\\star}\\|_{1}$ consists of all vectors $s \\in \\mathbb{R}^{n}$ such that:\n$$ s_j = \\begin{cases} \\operatorname{sign}(x^{\\star}_j) & \\text{if } x^{\\star}_j \\neq 0 \\\\ \\alpha_j \\in [-1, 1] & \\text{if } x^{\\star}_j = 0 \\end{cases} $$\nFor our specific $x^{\\star} = (1, 1, 0)^{\\top}$, the support is $\\mathcal{T} = \\{j \\mid x^{\\star}_j \\neq 0\\} = \\{1, 2\\}$. The non-support index is $3$.\nThus, a vector $s = (s_1, s_2, s_3)^{\\top}$ is in $\\partial \\|x^{\\star}\\|_{1}$ if and only if:\n1. $s_1 = \\operatorname{sign}(x^{\\star}_1) = \\operatorname{sign}(1) = 1$.\n2. $s_2 = \\operatorname{sign}(x^{\\star}_2) = \\operatorname{sign}(1) = 1$.\n3. $s_3 \\in [-1, 1]$.\n\nNow, we compute the vector $A^{\\top}u$. Let $u = (u_1, u_2)^{\\top} \\in \\mathbb{R}^2$.\n$$ A^{\\top} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} $$\n$$ A^{\\top}u = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} \\begin{pmatrix} u_1 \\\\ u_2 \\end{pmatrix} = \\begin{pmatrix} u_1 \\\\ u_2 \\\\ u_1 + u_2 \\end{pmatrix} $$\nThe optimality condition $A^{\\top}u \\in \\partial \\|x^{\\star}\\|_{1}$ translates to finding $u_1, u_2$ that satisfy the following system:\n1. $(A^{\\top}u)_1 = u_1 = 1$.\n2. $(A^{\\top}u)_2 = u_2 = 1$.\n3. $(A^{\\top}u)_3 = u_1 + u_2 \\in [-1, 1]$.\n\nThe first two conditions, which are imposed by the subgradient equalities on the support of $x^{\\star}$, uniquely determine the candidate dual certificate: $u_1 = 1$ and $u_2 = 1$. So, if a dual certificate exists, it must be $u = (1, 1)^{\\top}$.\n\nNow we must check if this unique candidate for $u$ satisfies the third condition, which corresponds to the non-support index $j=3$.\nWe substitute the values of $u_1$ and $u_2$ into the third component of $A^{\\top}u$:\n$$ (A^{\\top}u)_3 = u_1 + u_2 = 1 + 1 = 2 $$\nThe condition requires this value to be in the interval $[-1, 1]$. However, $2 \\notin [-1, 1]$. This condition is violated.\n\nTherefore, there is no dual vector $u \\in \\mathbb{R}^2$ that satisfies the KKT conditions for optimality. We conclude that a dual certificate does not exist, and thus, $x^{\\star} = (1, 1, 0)^{\\top}$ is not the optimal solution to the given Basis Pursuit problem.\n\nGeometrically, this means that no supporting hyperplane to the $\\ell_1$-ball at $x^\\star$ contains the affine feasible set $\\{x \\mid Ax=y\\}$. More formally, the set of subgradients $\\partial \\|x^\\star\\|_1$ and the range of the operator $A^\\top$ are disjoint sets.\n\nThe problem then asks for the unique value of $(A^{\\top} u)_{3}$ that is forced by the subgradient equalities on the support of $x^{\\star}$, in the case that no dual certificate exists. As derived above, the subgradient equalities on the support $\\mathcal{T}=\\{1, 2\\}$ are $(A^{\\top} u)_{1} = 1$ and $(A^{\\top} u)_{2} = 1$. These equalities force $u_1 = 1$ and $u_2 = 1$. These values, in turn, determine the value of the third component:\n$$ (A^{\\top}u)_3 = u_1 + u_2 = 1 + 1 = 2 $$\nThis value is unique because the dual vector components $u_1$ and $u_2$ were uniquely determined by the equations corresponding to the support of $x^\\star$.", "answer": "$$\\boxed{2}$$", "id": "3475665"}]}