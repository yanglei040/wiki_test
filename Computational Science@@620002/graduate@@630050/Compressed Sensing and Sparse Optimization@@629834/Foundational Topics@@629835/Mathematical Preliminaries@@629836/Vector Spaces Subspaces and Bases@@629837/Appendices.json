{"hands_on_practices": [{"introduction": "The choice of the $\\ell_1$ norm as a convex surrogate for sparsity is the cornerstone of compressed sensing. This first exercise invites you to rigorously explore the geometric relationship between the $\\ell_1$ and $\\ell_2$ norms. By deriving the sharp constants that bind these norms for both dense and sparse vectors, you will develop a fundamental intuition for why minimizing the $\\ell_1$ norm naturally promotes solutions that lie on low-dimensional coordinate subspaces [@problem_id:3493063].", "problem": "Let $V=\\mathbb{R}^{n}$ be equipped with the canonical basis $\\{e_{1},\\dots,e_{n}\\}$ and the standard inner product. For $x=(x_{1},\\dots,x_{n})\\in\\mathbb{R}^{n}$, define the norms $\\|x\\|_{1}=\\sum_{i=1}^{n}|x_{i}|$ and $\\|x\\|_{2}=\\left(\\sum_{i=1}^{n}x_{i}^{2}\\right)^{1/2}$. Consider the identity maps between the normed spaces $(\\mathbb{R}^{n},\\|\\cdot\\|_{1})$ and $(\\mathbb{R}^{n},\\|\\cdot\\|_{2})$. Starting only from the definitions of $\\|\\cdot\\|_{1}$ and $\\|\\cdot\\|_{2}$, the Cauchy–Schwarz inequality, and properties of the canonical basis, derive the sharp constants $a_{n}$ and $b_{n}$ such that for all $x\\in\\mathbb{R}^{n}\\setminus\\{0\\}$,\n$$\na_{n}\\leq \\frac{\\|x\\|_{1}}{\\|x\\|_{2}}\\leq b_{n}.\n$$\nThen, for a fixed sparsity level $k$ with $1\\leq k\\leq n$, define the set of $k$-sparse vectors\n$$\n\\Sigma_{k}=\\{x\\in\\mathbb{R}^{n}:\\#\\{i:x_{i}\\neq 0\\}\\leq k\\},\n$$\nand derive the sharp constant $b_{n,k}$ such that for all $x\\in\\Sigma_{k}\\setminus\\{0\\}$,\n$$\n\\frac{\\|x\\|_{1}}{\\|x\\|_{2}}\\leq b_{n,k}.\n$$\nExplain the geometric meaning of these inequalities in terms of the shapes and relative containments of the unit balls of $\\|\\cdot\\|_{1}$ and $\\|\\cdot\\|_{2}$, and articulate how this geometry promotes sparsity when using $\\ell_1$-regularization in Compressed Sensing (CS). Your derivation must begin from the stated foundational definitions and inequalities and should connect the role of the canonical basis and coordinate subspaces to the behavior of $\\|\\cdot\\|_{1}$ on sparse vectors.\n\nReport your final answer as the row vector $(\\alpha_{n},\\beta_{n},\\beta_{n,k})$, where $\\alpha_{n}=\\inf_{x\\neq 0}\\|x\\|_{1}/\\|x\\|_{2}$, $\\beta_{n}=\\sup_{x\\neq 0}\\|x\\|_{1}/\\|x\\|_{2}$, and $\\beta_{n,k}=\\sup_{x\\in\\Sigma_{k}\\setminus\\{0\\}}\\|x\\|_{1}/\\|x\\|_{2}$. No numerical approximation is required.", "solution": "The problem statement is a well-posed mathematical problem in linear algebra and analysis. It is scientifically grounded, objective, and self-contained, providing all necessary definitions and constraints. The problem is valid.\n\nWe are tasked with finding the sharp constants $a_n$, $b_n$, and $b_{n,k}$ for inequalities relating the $\\ell_1$ and $\\ell_2$ norms in $\\mathbb{R}^n$, and explaining the geometric and practical implications of these relationships in the context of compressed sensing. The ratio $\\frac{\\|x\\|_{1}}{\\|x\\|_{2}}$ is homogeneous of degree zero, meaning its value is constant for any non-zero scalar multiple of $x$. Therefore, finding the infimum and supremum over all $x \\in \\mathbb{R}^n \\setminus \\{0\\}$ is equivalent to finding the minimum and maximum over the set of vectors with unit $\\ell_2$-norm, i.e., $\\|x\\|_2 = 1$.\n\nLet $x = (x_1, \\dots, x_n) \\in \\mathbb{R}^n$. The norms are defined as $\\|x\\|_{1}=\\sum_{i=1}^{n}|x_{i}|$ and $\\|x\\|_{2}=\\left(\\sum_{i=1}^{n}x_{i}^{2}\\right)^{1/2}$.\n\n**Derivation of the lower bound $a_n$**\n\nWe seek to find the infimum of $\\frac{\\|x\\|_{1}}{\\|x\\|_{2}}$. Let's consider the square of the $\\ell_1$-norm:\n$$\n\\|x\\|_1^2 = \\left(\\sum_{i=1}^{n}|x_i|\\right)^2 = \\sum_{i=1}^{n}|x_i|^2 + \\sum_{i \\neq j} |x_i||x_j|\n$$\nThe first term is exactly the square of the $\\ell_2$-norm: $\\sum_{i=1}^{n}|x_i|^2 = \\sum_{i=1}^{n}x_i^2 = \\|x\\|_2^2$. The second term, representing the sum of all cross-products, is non-negative, since $|x_i| \\ge 0$ for all $i$. Therefore, we have the inequality:\n$$\n\\|x\\|_1^2 \\ge \\|x\\|_2^2\n$$\nSince norms are non-negative, we can take the square root of both sides to obtain $\\|x\\|_1 \\ge \\|x\\|_2$. For any $x \\neq 0$, this implies:\n$$\n\\frac{\\|x\\|_1}{\\|x\\|_2} \\ge 1\n$$\nThis establishes $a_n \\ge 1$. To show that this bound is sharp, we must find a vector $x$ for which the equality holds. Equality holds if and only if the sum of cross-products is zero, i.e., $\\sum_{i \\neq j} |x_i||x_j| = 0$. This condition is met if and only if at most one component $x_i$ is non-zero. Such vectors are scalar multiples of the canonical basis vectors $\\{e_1, \\dots, e_n\\}$.\nLet's choose $x = e_j$ for some $j \\in \\{1, \\dots, n\\}$.\nThen $\\|x\\|_1 = |1| = 1$ and $\\|x\\|_2 = \\sqrt{1^2} = 1$. For this vector, the ratio is $\\frac{\\|x\\|_1}{\\|x\\|_2} = 1$.\nThus, the infimum is $1$, and the constant is sharp. Let $\\alpha_n = a_n$.\n$$\n\\alpha_n = \\inf_{x \\neq 0} \\frac{\\|x\\|_1}{\\|x\\|_2} = 1\n$$\n\n**Derivation of the upper bound $b_n$**\n\nWe seek the supremum of $\\frac{\\|x\\|_{1}}{\\|x\\|_{2}}$. We use the Cauchy–Schwarz inequality, which states that for any two vectors $u, v \\in \\mathbb{R}^n$, $|\\langle u, v \\rangle| \\le \\|u\\|_2 \\|v\\|_2$.\nLet us define two vectors: $u = (|x_1|, |x_2|, \\dots, |x_n|)$ and $v = (1, 1, \\dots, 1)$.\nTheir inner product is:\n$$\n\\langle u, v \\rangle = \\sum_{i=1}^n |x_i| \\cdot 1 = \\|x\\|_1\n$$\nThe $\\ell_2$-norms of these vectors are:\n$$\n\\|u\\|_2 = \\left(\\sum_{i=1}^n |x_i|^2\\right)^{1/2} = \\left(\\sum_{i=1}^n x_i^2\\right)^{1/2} = \\|x\\|_2\n$$\n$$\n\\|v\\|_2 = \\left(\\sum_{i=1}^n 1^2\\right)^{1/2} = \\sqrt{n}\n$$\nApplying the Cauchy–Schwarz inequality:\n$$\n\\|x\\|_1 = \\langle u, v \\rangle \\le \\|u\\|_2 \\|v\\|_2 = \\|x\\|_2 \\sqrt{n}\n$$\nFor any $x \\neq 0$, this gives the upper bound:\n$$\n\\frac{\\|x\\|_1}{\\|x\\|_2} \\le \\sqrt{n}\n$$\nThis establishes $b_n \\le \\sqrt{n}$. To show this bound is sharp, we must find a vector for which equality holds. Equality in the Cauchy–Schwarz inequality holds if and only if $u$ is a scalar multiple of $v$, i.e., $u = \\lambda v$ for some $\\lambda \\in \\mathbb{R}$. This means $(|x_1|, \\dots, |x_n|) = \\lambda (1, \\dots, 1)$, which implies $|x_1| = |x_2| = \\dots = |x_n| = \\lambda$.\nLet's choose $x = (1, 1, \\dots, 1)$.\nThen $\\|x\\|_1 = \\sum_{i=1}^n |1| = n$ and $\\|x\\|_2 = \\sqrt{\\sum_{i=1}^n 1^2} = \\sqrt{n}$. The ratio for this vector is $\\frac{\\|x\\|_1}{\\|x\\|_2} = \\frac{n}{\\sqrt{n}} = \\sqrt{n}$.\nThus, the supremum is $\\sqrt{n}$, and the constant is sharp. Let $\\beta_n = b_n$.\n$$\n\\beta_n = \\sup_{x \\neq 0} \\frac{\\|x\\|_1}{\\|x\\|_2} = \\sqrt{n}\n$$\n\n**Derivation of the upper bound $b_{n,k}$ for sparse vectors**\n\nNow we restrict our attention to the set of $k$-sparse vectors, $\\Sigma_k = \\{x \\in \\mathbb{R}^n : \\#\\{i : x_i \\neq 0\\} \\le k\\}$. We want to find the supremum of $\\frac{\\|x\\|_1}{\\|x\\|_2}$ for $x \\in \\Sigma_k \\setminus \\{0\\}$.\nLet $x \\in \\Sigma_k$, and let $S$ be the support of $x$, i.e., the set of indices where $x_i \\neq 0$. Let $m = |S| = \\#\\{i : x_i \\neq 0\\}$. By definition of $\\Sigma_k$, we have $m \\le k$.\nThe $\\ell_1$ and $\\ell_2$ norms of $x$ only depend on its non-zero components. Let $x_S \\in \\mathbb{R}^m$ be the vector consisting of the non-zero entries of $x$.\n$$\n\\|x\\|_1 = \\sum_{i \\in S} |x_i| = \\|x_S\\|_1\n$$\n$$\n\\|x\\|_2 = \\left(\\sum_{i \\in S} x_i^2\\right)^{1/2} = \\|x_S\\|_2\n$$\nThe ratio is $\\frac{\\|x\\|_1}{\\|x\\|_2} = \\frac{\\|x_S\\|_1}{\\|x_S\\|_2}$. Since $x_S$ is a vector in $\\mathbb{R}^m$, we can apply the upper bound derived previously to this vector.\n$$\n\\frac{\\|x_S\\|_1}{\\|x_S\\|_2} \\le \\sqrt{m}\n$$\nSince $m = |S| \\le k$, we have $\\sqrt{m} \\le \\sqrt{k}$. Therefore, for any $x \\in \\Sigma_k \\setminus \\{0\\}$:\n$$\n\\frac{\\|x\\|_1}{\\|x\\|_2} \\le \\sqrt{k}\n$$\nThis establishes $b_{n,k} \\le \\sqrt{k}$. To show this bound is sharp, we must find a vector in $\\Sigma_k$ that achieves it. The maximum value of $\\sqrt{m}$ for $m \\le k$ is $\\sqrt{k}$. Equality is achieved for a vector $x$ that has exactly $k$ non-zero components, all of which have the same absolute value.\nLet's construct such a vector. For example, let $x$ be the vector with its first $k$ components equal to $1$ and the remaining $n-k$ components equal to $0$.\n$x = (1, \\dots, 1, 0, \\dots, 0)$. This vector has support size $k$, so $x \\in \\Sigma_k$.\n$\\|x\\|_1 = \\sum_{i=1}^k |1| = k$.\n$\\|x\\|_2 = \\sqrt{\\sum_{i=1}^k 1^2} = \\sqrt{k}$.\nThe ratio is $\\frac{\\|x\\|_1}{\\|x\\|_2} = \\frac{k}{\\sqrt{k}} = \\sqrt{k}$.\nThus, the supremum over $\\Sigma_k$ is $\\sqrt{k}$, and the constant is sharp. Let $\\beta_{n,k} = b_{n,k}$.\n$$\n\\beta_{n,k} = \\sup_{x \\in \\Sigma_k \\setminus \\{0\\}} \\frac{\\|x\\|_1}{\\|x\\|_2} = \\sqrt{k}\n$$\n\n**Geometric Meaning and Connection to Compressed Sensing (CS)**\n\nThe inequalities $1 \\le \\frac{\\|x\\|_1}{\\|x\\|_2} \\le \\sqrt{n}$ can be rewritten as $\\|x\\|_2 \\le \\|x\\|_1 \\le \\sqrt{n}\\|x\\|_2$. These describe the geometric relationship between the unit balls of the respective norms. Let $B_p = \\{x \\in \\mathbb{R}^n : \\|x\\|_p \\le 1\\}$ for $p=1,2$.\nThe inequality $\\|x\\|_2 \\le \\|x\\|_1$ is equivalent to the set inclusion $B_1 \\subset B_2$. The unit $\\ell_1$-ball (a cross-polytope, e.g., a square rotated by $45^\\circ$ in $\\mathbb{R}^2$ or an octahedron in $\\mathbb{R}^3$) is contained within the unit $\\ell_2$-ball (the standard Euclidean ball). They touch at the points where equality holds, which are the vectors $\\pm e_j$. These points are precisely the vertices of $B_1$ and lie on the surface of $B_2$.\nThe inequality $\\|x\\|_1 \\le \\sqrt{n}\\|x\\|_2$ is equivalent to $B_2 \\subset \\sqrt{n}B_1$. The unit $\\ell_2$-ball is contained within the $\\ell_1$-ball scaled by a factor of $\\sqrt{n}$. They touch at points where $|x_1| = \\dots = |x_n|$, which are the \"corners\" of the Euclidean ball in the $\\ell_1$ sense.\n\nIn Compressed Sensing, we aim to recover a sparse signal $x_0 \\in \\mathbb{R}^n$ from a small number of linear measurements $y=Ax_0$, where $A$ is an $m \\times n$ matrix with $m \\ll n$. Since the system is underdetermined, there is an entire affine subspace of solutions $S_y = \\{x \\in \\mathbb{R}^n : Ax=y\\}$. The challenge is to select $x_0$ from this subspace.\nA natural approach is to find the sparsest vector in $S_y$, which is the solution to $\\min \\|x\\|_0$ subject to $Ax=y$. This is computationally intractable (NP-hard).\nThe key insight of CS is to relax this by solving for the vector with the minimum $\\ell_1$-norm instead:\n$$\n\\min \\|x\\|_1 \\quad \\text{subject to} \\quad Ax=y\n$$\nThis is a convex optimization problem and can be solved efficiently. The geometric reason for its success lies in the shape of the $\\ell_1$-ball. The problem is equivalent to finding the smallest $\\ell_1$-ball that intersects the affine subspace $S_y$. Because the $\\ell_1$-ball has \"spikes\" or vertices pointing along the coordinate axes (the canonical basis vectors), an expanding $\\ell_1$-ball is most likely to first touch the affine subspace $S_y$ at one of these vertices or a low-dimensional edge/face. A point on a vertex is $1$-sparse; a point on an edge is $2$-sparse, and so on. This geometry inherently promotes sparse solutions.\nIn contrast, minimizing the $\\ell_2$-norm (the classical least-norm solution) corresponds to finding the point in $S_y$ closest to the origin. This is equivalent to finding the first point of contact between $S_y$ and an expanding $\\ell_2$-ball. Due to the perfect roundness of the $\\ell_2$-ball, the point of contact is generally not on a coordinate axis and the resulting solution is dense.\n\nThe inequality $\\frac{\\|x\\|_1}{\\|x\\|_2} \\le \\sqrt{k}$ for $k$-sparse vectors is fundamental to the theory of CS. It shows that on the set of sparse vectors, the $\\ell_1$ and $\\ell_2$ norms are much more closely related (by a factor of $\\sqrt{k}$) than on the whole space (factor of $\\sqrt{n}$). This property, often discussed in the context of the Restricted Isometry Property (RIP), ensures that $\\ell_1$-minimization can successfully recover sparse signals under certain conditions on the measurement matrix $A$.\n\nThe final answer is composed of the derived sharp constants $(\\alpha_n, \\beta_n, \\beta_{n,k})$.\n$\\alpha_n = 1$\n$\\beta_n = \\sqrt{n}$\n$\\beta_{n,k} = \\sqrt{k}$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  \\sqrt{n}  \\sqrt{k}\n\\end{pmatrix}\n}\n$$", "id": "3493063"}, {"introduction": "While geometric intuition is powerful, compressed sensing theory relies on quantitative guarantees, chief among them the Restricted Isometry Property (RIP). This practice shifts our focus from analytical derivation to computational verification, challenging you to implement a procedure to calculate the Restricted Isometry Constant ($\\delta_s$) for a given measurement matrix. By numerically evaluating how well a matrix preserves the geometry of sparse subspaces, you will gain a practical understanding of what constitutes a 'good' measurement design [@problem_id:3493111].", "problem": "You are asked to construct, analyze, and numerically certify a linear measurement operator within linear algebraic foundations tailored to sparse signal models. Let $A \\in \\mathbb{R}^{m \\times n}$ be a real matrix. The set of all $s$-sparse vectors is the union of all coordinate subspaces spanned by subsets of the standard basis of $\\mathbb{R}^{n}$ of size at most $s$. Formally, define the family of $s$-sparse subspaces as \n$$\n\\mathcal{U}_s \\triangleq \\bigcup_{\\substack{S \\subseteq [n] \\\\ |S| \\le s}} \\left\\{ x \\in \\mathbb{R}^{n} : \\operatorname{supp}(x) \\subseteq S \\right\\}.\n$$\nA matrix $A$ is a subspace embedding for all $s$-sparse subspaces if there exists a smallest constant $\\delta_s \\in [0,1)$ such that, for every $x \\in \\mathcal{U}_s$, the restricted isometry inequality holds:\n$$\n(1 - \\delta_s) \\, \\|x\\|_2^2 \\le \\|A x\\|_2^2 \\le (1 + \\delta_s) \\, \\|x\\|_2^2.\n$$\nThe quantity $\\delta_s$ is the restricted isometry constant (RIC) of order $s$. The rows of $A$ must be linearly independent so that they form a basis (not necessarily orthonormal) for the row space, that is, $\\operatorname{rank}(A) = m$.\n\nYour task is to:\n- Design a procedure to construct $A$ so that its rows form a basis and $A$ acts as a subspace embedding for all $s$-sparse subspaces. Use a construction that is justified from first principles. One canonical choice is to take $A$ with independent and identically distributed Gaussian entries with mean $0$ and variance $1/m$, which implies $\\mathbb{E}[A^\\top A] = I_n$, and with probability $1$ the rows are linearly independent for $m \\le n$. As a boundary case, you may also consider the identity matrix $A = I_n$ when $m = n$, which yields an exact embedding with $\\delta_s = 0$ for all $s \\le n$.\n- Exactly quantify the optimal restricted isometry constant $\\delta_s$ for a given $A$ by computing the smallest $\\delta \\ge 0$ that satisfies the above inequality for all $x \\in \\mathcal{U}_s$. Equivalently, for every support set $S \\subseteq [n]$ with $|S| \\le s$, let $A_S \\in \\mathbb{R}^{m \\times |S|}$ denote the restriction of $A$ to the columns indexed by $S$, and let $G_S \\triangleq A_S^\\top A_S \\in \\mathbb{R}^{|S| \\times |S|}$. Then \n$$\n\\delta_s \\;=\\; \\max_{\\substack{S \\subseteq [n] \\\\ |S| \\le s}} \\left\\| G_S - I_{|S|} \\right\\|_2 \\;=\\; \\max_{\\substack{S \\subseteq [n] \\\\ |S| \\le s}} \\max_{1 \\le i \\le |S|} \\left| \\lambda_i(G_S) - 1 \\right|,\n$$\nwhere $\\lambda_i(\\cdot)$ are the eigenvalues and $\\|\\cdot\\|_2$ denotes the spectral norm.\n\nProgram requirements:\n- Implement a program that constructs $A$ according to the following test suite, verifies $\\operatorname{rank}(A) = m$, and computes the exact $\\delta_s$ by exhaustive enumeration of all supports $S \\subseteq [n]$ with $|S| = s$ and evaluation of the spectral norm $\\|A_S^\\top A_S - I\\|_2$. If $\\operatorname{rank}(A) \\ne m$ for a random construction, resample until $\\operatorname{rank}(A) = m$.\n- The construction protocols for $A$ are:\n  - Identity protocol: set $A = I_n$ when $m = n$, which yields $\\delta_s = 0$ for all $s \\le n$.\n  - Gaussian protocol: draw $A_{ij} \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, 1/m)$, with a fixed pseudorandom seed for reproducibility.\n\nUse the following test suite:\n- Test $1$: $n = 8$, $m = 8$, $s = 3$, protocol $=$ identity.\n- Test $2$: $n = 12$, $m = 9$, $s = 2$, protocol $=$ Gaussian with seed $= 7$.\n- Test $3$: $n = 12$, $m = 6$, $s = 3$, protocol $=$ Gaussian with seed $= 13$.\n- Test $4$: $n = 10$, $m = 6$, $s = 1$, protocol $=$ Gaussian with seed $= 42$.\n\nOutput format:\n- Your program should produce a single line of output containing the $4$ results (one per test, in the listed order) as a comma-separated list enclosed in square brackets, with each number rounded to $6$ decimal places, for example, $[a,b,c,d]$.\n\nAll quantities are purely mathematical and unitless. Angles are not involved.\n\nYour program must be self-contained and accept no input. It must compute and report the exact values of $\\delta_s$ for the given tests by exhaustive enumeration over all supports of size $s$ and spectral norm evaluation.", "solution": "The problem requires the construction of a linear measurement operator, represented by a matrix $A \\in \\mathbb{R}^{m \\times n}$, and the subsequent computation of its order-$s$ Restricted Isometry Constant (RIC), denoted $\\delta_s$. The operator must satisfy specific structural properties, namely that its rows are linearly independent, and it acts as a good subspace embedding for sparse vectors.\n\nFirst, we formalize the definition of the RIC. A matrix $A$ satisfies the Restricted Isometry Property (RIP) of order $s$ if there exists a constant $\\delta_s \\in [0, 1)$ such that for all $s$-sparse vectors $x$ (i.e., vectors with at most $s$ non-zero entries), the following inequality holds:\n$$\n(1 - \\delta_s) \\|x\\|_2^2 \\le \\|A x\\|_2^2 \\le (1 + \\delta_s) \\|x\\|_2^2\n$$\nThe smallest such $\\delta_s$ is the RIC. An equivalent and computationally tractable definition for $\\delta_s$ is given in terms of the spectral properties of submatrices of $A$. Let $S \\subseteq \\{1, 2, \\dots, n\\}$ be a set of column indices, and let $A_S$ be the submatrix of $A$ formed by the columns indexed by $S$. Let $G_S = A_S^\\top A_S$ be the Gram matrix associated with these columns. The RIC $\\delta_s$ is then the maximum possible deviation from isometry over all subspaces of dimension up to $s$:\n$$\n\\delta_s = \\max_{\\substack{S \\subseteq \\{1, \\dots, n\\} \\\\ |S| \\le s}} \\|A_S^\\top A_S - I_{|S|}\\|_2\n$$\nwhere $I_{|S|}$ is the identity matrix of size $|S| \\times |S|$, and $\\|\\cdot\\|_2$ denotes the spectral norm. The spectral norm of a symmetric matrix is the maximum absolute value of its eigenvalues. Therefore, we can write:\n$$\n\\delta_s = \\max_{\\substack{S \\subseteq \\{1, \\dots, n\\} \\\\ |S| \\le s}} \\max\\left( \\lambda_{\\max}(G_S) - 1, 1 - \\lambda_{\\min}(G_S) \\right)\n$$\nwhere $\\lambda_{\\max}(G_S)$ and $\\lambda_{\\min}(G_S)$ are the maximum and minimum eigenvalues of $G_S$, respectively.\n\nThe problem statement directs us to compute $\\delta_s$ by enumerating over all supports $S$ of size exactly $s$. This is a valid simplification. The property $\\delta_k \\le \\delta_{k+1}$ holds for RICs. This monotonicity arises from the eigenvalue interlacing theorem. If $S' \\subset S$ with $|S'|=|S|-1$, then $G_{S'}$ is a principal submatrix of $G_S$. By Cauchy's interlacing theorem, the eigenvalues of $G_{S'}$ interlace those of $G_S$. This implies $\\lambda_{\\min}(G_S) \\le \\lambda_{\\min}(G_{S'})$ and $\\lambda_{\\max}(G_{S'}) \\le \\lambda_{\\max}(G_S)$. Consequently, the deviation from $1$, as captured by $\\max(\\lambda_{\\max}-1, 1-\\lambda_{\\min})$, tends to be maximized for larger support sets. Thus, the maximum over all $|S| \\le s$ will be achieved for a support of size $s$. We can therefore compute $\\delta_s$ as:\n$$\n\\delta_s = \\max_{\\substack{S \\subseteq \\{1, \\dots, n\\} \\\\ |S| = s}} \\|A_S^\\top A_S - I_s\\|_2\n$$\n\nThe overall computational procedure is as follows:\n1.  For each test case, construct the matrix $A \\in \\mathbb{R}^{m \\times n}$ according to the specified protocol ('identity' or 'Gaussian').\n    -  For the 'identity' protocol, $A$ is the identity matrix $I_n$.\n    -  For the 'Gaussian' protocol, the entries $A_{ij}$ are drawn independently from a normal distribution $\\mathcal{N}(0, 1/m)$. A fixed random seed ensures reproducibility.\n2.  Verify that the constructed matrix $A$ has full row rank, i.e., $\\operatorname{rank}(A) = m$. For a matrix with entries drawn from a continuous distribution (like Gaussian), this condition holds with probability $1$ as long as $m \\le n$.\n3.  Initialize a variable $\\delta_{\\text{max}} = 0$.\n4.  Generate all unique combinations of column indices $S$ of size $s$ from the set $\\{0, 1, \\dots, n-1\\}$.\n5.  For each index set $S$:\n    a.  Extract the submatrix $A_S \\in \\mathbb{R}^{m \\times s}$.\n    b.  Compute the Gram matrix $G_S = A_S^\\top A_S$.\n    c.  Since $G_S$ is a symmetric matrix, calculate its eigenvalues. Let them be $\\lambda_1, \\dots, \\lambda_s$.\n    d.  Determine the spectral norm $\\|G_S - I_s\\|_2 = \\max_{i} |\\lambda_i - 1|$. This is equivalent to $\\max(\\lambda_{\\max} - 1, 1 - \\lambda_{\\min})$.\n    e.  Update $\\delta_{\\text{max}} = \\max(\\delta_{\\text{max}}, \\|G_S - I_s\\|_2)$.\n6.  The final value of $\\delta_{\\text{max}}$ is the desired RIC, $\\delta_s$.\n\nThis procedure will be applied to the four specified test cases.\n\n- **Test 1:** $n = 8, m = 8, s = 3$, protocol $=$ identity.\n  The matrix is $A = I_8$. For any submatrix $A_S$ formed by $s=3$ columns of the identity matrix, $A_S^\\top A_S = I_3$. The eigenvalues of $I_3$ are all $1$. Thus, $\\|A_S^\\top A_S - I_3\\|_2 = \\max|1 - 1| = 0$. This is true for all $S$, so $\\delta_3=0$.\n\n- **Test 2:** $n = 12, m = 9, s = 2$, protocol $=$ Gaussian with seed $= 7$.\n  We construct an $A \\in \\mathbb{R}^{9 \\times 12}$ with entries from $\\mathcal{N}(0, 1/9)$ using seed $7$. We then iterate through all $\\binom{12}{2} = 66$ subsets of columns of size $2$. For each $A_S \\in \\mathbb{R}^{9 \\times 2}$, we compute the two eigenvalues of the $2 \\times 2$ matrix $A_S^\\top A_S$ and find the maximal deviation from $1$. The RIC $\\delta_2$ is the maximum of these deviations over all $66$ subsets.\n\n- **Test 3:** $n = 12, m = 6, s = 3$, protocol $=$ Gaussian with seed $= 13$.\n  We construct an $A \\in \\mathbb{R}^{6 \\times 12}$ with entries from $\\mathcal{N}(0, 1/6)$ using seed $13$. We iterate through all $\\binom{12}{3} = 220$ subsets of columns of size $3$. For each $A_S \\in \\mathbb{R}^{6 \\times 3}$, we find the eigenvalues of the $3 \\times 3$ matrix $A_S^\\top A_S$ and compute the corresponding deviation. The maximum of these deviations is $\\delta_3$.\n\n- **Test 4:** $n = 10, m = 6, s = 1$, protocol $=$ Gaussian with seed $= 42$.\n  We construct an $A \\in \\mathbb{R}^{6 \\times 10}$ with entries from $\\mathcal{N}(0, 1/6)$ using seed $42$. For $s=1$, each subset $S$ contains a single column, say $a_j$. Then $A_S = a_j$, and $G_S = a_j^\\top a_j = \\|a_j\\|_2^2$, which is a $1 \\times 1$ matrix. Its single eigenvalue is $\\|a_j\\|_2^2$. The RIC is $\\delta_1 = \\max_{j} |\\|a_j\\|_2^2 - 1|$. We compute this by finding the squared norm of each of the $10$ columns and determining the maximum deviation from $1$.", "answer": "```python\nimport numpy as np\nfrom itertools import combinations\n\ndef calculate_ric(n, m, s, protocol, seed=None):\n    \"\"\"\n    Constructs a matrix A and computes its Restricted Isometry Constant (RIC) delta_s.\n\n    Args:\n        n (int): Number of columns of A.\n        m (int): Number of rows of A.\n        s (int): Sparsity level.\n        protocol (str): Construction protocol ('identity' or 'Gaussian').\n        seed (int, optional): Seed for the random number generator.\n\n    Returns:\n        float: The computed RIC delta_s.\n    \"\"\"\n    \n    # Step 1: Construct matrix A\n    if protocol == 'identity':\n        if m != n:\n            raise ValueError(\"Identity protocol requires m == n.\")\n        A = np.eye(n)\n    elif protocol == 'Gaussian':\n        if seed is None:\n            raise ValueError(\"Gaussian protocol requires a seed.\")\n        rng = np.random.default_rng(seed)\n        # Resample until rank condition is met. For a continuous distribution,\n        # rank  m is a zero-probability event, so a loop is not practically needed.\n        # A single generation and check is sufficient.\n        A = rng.normal(loc=0.0, scale=1.0 / np.sqrt(m), size=(m, n))\n    else:\n        raise ValueError(f\"Unknown protocol: {protocol}\")\n\n    # Step 2: Verify rank(A) = m\n    # This is a mandatory condition specified in the problem statement.\n    rank_A = np.linalg.matrix_rank(A)\n    if rank_A != m:\n        # This case is highly improbable for the Gaussian protocol with m = n.\n        # If it happens, it might indicate an issue with the seed or extreme parameters.\n        raise RuntimeError(f\"Matrix rank is {rank_A}, but expected {m}. Resampling would be needed.\")\n\n    # Step 3  4: Initialize max delta and iterate through all subsets\n    max_delta = 0.0\n    column_indices = range(n)\n    \n    # As justified in the solution text, we only need to check subsets of size exactly s.\n    for s_indices in combinations(column_indices, s):\n        # Step 5a: Extract submatrix A_S\n        A_S = A[:, s_indices]\n        \n        # Step 5b: Compute Gram matrix G_S\n        G_S = A_S.T @ A_S\n        \n        # Step 5c: Compute eigenvalues of G_S\n        # G_S is symmetric, so we use eigvalsh which is efficient and returns sorted eigenvalues.\n        eigenvalues = np.linalg.eigvalsh(G_S)\n        \n        # Step 5d: Compute spectral norm of G_S - I\n        # This is max(|lambda_max - 1|, |lambda_min - 1|)\n        delta_S = max(np.abs(eigenvalues[-1] - 1.0), np.abs(eigenvalues[0] - 1.0))\n        \n        # Step 5e: Update max delta\n        if delta_S  max_delta:\n            max_delta = delta_S\n            \n    # Step 6: Return the final RIC\n    return max_delta\n\ndef solve():\n    \"\"\"\n    Executes the test suite and prints the results.\n    \"\"\"\n    test_cases = [\n        {'n': 8, 'm': 8, 's': 3, 'protocol': 'identity', 'seed': None},\n        {'n': 12, 'm': 9, 's': 2, 'protocol': 'Gaussian', 'seed': 7},\n        {'n': 12, 'm': 6, 's': 3, 'protocol': 'Gaussian', 'seed': 13},\n        {'n': 10, 'm': 6, 's': 1, 'protocol': 'Gaussian', 'seed': 42},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = calculate_ric(\n            n=case['n'],\n            m=case['m'],\n            s=case['s'],\n            protocol=case['protocol'],\n            seed=case['seed']\n        )\n        results.append(result)\n\n    # Format the final output as specified\n    formatted_results = [f'{r:.6f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3493111"}, {"introduction": "A small Restricted Isometry Constant suggests robust recovery, but is it the whole story? This final exercise presents a carefully constructed counterexample to challenge that simple conclusion and introduce the critical concept of coherence. You will demonstrate that even when a measurement matrix has a perfect isometry constant for 1-sparse signals, basis pursuit can fail if the operator is poorly aligned with the underlying sparsity basis, revealing a deeper geometric condition for successful sparse recovery [@problem_id:3493094].", "problem": "Let $n=2$. Consider the canonical basis $\\{e_{1}, e_{2}\\}$ of the vector space $\\mathbb{R}^{2}$. Define the structured sparse model as the $1$-sparse vectors in the subspace $V=\\operatorname{span}\\{e_{1},e_{2}\\}$. Let the measurement operator be the single-row linear map $A:\\mathbb{R}^{2}\\to\\mathbb{R}$ given by the row vector $A=\\begin{bmatrix}1  1\\end{bmatrix}$, which corresponds to taking a measurement in the measurement basis vector $u=\\frac{1}{\\sqrt{2}}(e_{1}+e_{2})$ aligned with the two-dimensional subspace $V$. A signal $x^{\\star}\\in\\mathbb{R}^{2}$ is acquired by $y=Ax^{\\star}$, and reconstruction is attempted using basis pursuit, i.e., the convex program that minimizes the $\\ell_1$-norm of $x$ subject to the linear constraint $Ax=y$.\n\nStarting only from core definitions of the Restricted Isometry Property (RIP) and basis pursuit, do the following:\n\n1. Using the definition of the $s$-restricted isometry constant $\\delta_{s}$ of a matrix (Restricted Isometry Property (RIP)), derive the exact value of $\\delta_{1}$ for the given $A$.\n2. Exhibit a specific $1$-sparse $x^{\\star}\\in V$ for which basis pursuit fails to uniquely identify $x^{\\star}$ from $y=Ax^{\\star}$, by finding a distinct feasible $x\\neq x^{\\star}$ such that $Ax=Ax^{\\star}$ and $\\|x\\|_{1}\\leq \\|x^{\\star}\\|_{1}$. Your construction should explain how the chosen measurement basis aligns with the structured sparse subspace $V$ to produce this failure.\n\nAnswer specification: Report only the exact value of $\\delta_{1}$ for the given $A$. Express your final answer as an exact number with no units and no rounding.", "solution": "The posed problem is subjected to validation and is deemed valid as it is scientifically grounded, self-contained, and well-posed. All definitions and data provided are standard within the field of compressed sensing and lead to a unique, verifiable solution.\n\nThe problem asks for two main tasks: first, to derive the $1$-restricted isometry constant $\\delta_1$ for the given measurement matrix $A$, and second, to demonstrate a failure case of basis pursuit for a $1$-sparse signal.\n\nPart 1: Derivation of the Restricted Isometry Constant $\\delta_1$.\n\nThe $s$-restricted isometry constant $\\delta_s$ of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is defined as the smallest non-negative number such that for all $s$-sparse vectors $x \\in \\mathbb{R}^n$, the following inequality holds:\n$$ (1-\\delta_s)\\|x\\|_2^2 \\leq \\|Ax\\|_2^2 \\leq (1+\\delta_s)\\|x\\|_2^2 $$\nIn this problem, the dimension is $n=2$, the sparsity level is $s=1$, and the measurement matrix is $A = \\begin{bmatrix}1  1\\end{bmatrix}$. The vector space is $\\mathbb{R}^2$.\n\nA $1$-sparse vector in $\\mathbb{R}^2$ is a vector with at most one non-zero component. Any such vector $x$ can be written in the form $x = c e_1$ or $x = c e_2$ for some scalar $c \\in \\mathbb{R}$, where $\\{e_1, e_2\\}$ is the canonical basis of $\\mathbb{R}^2$, with $e_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ and $e_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$. We consider the case where $x$ is non-zero, so $c \\neq 0$.\n\nLet's analyze the expression $\\|Ax\\|_2^2$ for these two forms of $1$-sparse vectors.\n\nCase 1: $x = c e_1 = \\begin{bmatrix} c \\\\ 0 \\end{bmatrix}$.\nThe squared $\\ell_2$-norm of $x$ is $\\|x\\|_2^2 = c^2 + 0^2 = c^2$.\nThe measurement is $Ax = \\begin{bmatrix}1  1\\end{bmatrix} \\begin{bmatrix} c \\\\ 0 \\end{bmatrix} = c$.\nSince the measurement is a scalar, its squared $\\ell_2$-norm is $\\|Ax\\|_2^2 = c^2$.\nIn this case, we have $\\|Ax\\|_2^2 = \\|x\\|_2^2$.\n\nCase 2: $x = c e_2 = \\begin{bmatrix} 0 \\\\ c \\end{bmatrix}$.\nThe squared $\\ell_2$-norm of $x$ is $\\|x\\|_2^2 = 0^2 + c^2 = c^2$.\nThe measurement is $Ax = \\begin{bmatrix}1  1\\end{bmatrix} \\begin{bmatrix} 0 \\\\ c \\end{bmatrix} = c$.\nIts squared $\\ell_2$-norm is $\\|Ax\\|_2^2 = c^2$.\nAgain, we have $\\|Ax\\|_2^2 = \\|x\\|_2^2$.\n\nFor any $1$-sparse vector $x \\in \\mathbb{R}^2$, we have shown that $\\|Ax\\|_2^2 = \\|x\\|_2^2$.\nSubstituting this identity into the RIP inequality for $s=1$:\n$$ (1-\\delta_1)\\|x\\|_2^2 \\leq \\|x\\|_2^2 \\leq (1+\\delta_1)\\|x\\|_2^2 $$\nFor any non-zero vector $x$, we can divide all parts of the inequality by the positive quantity $\\|x\\|_2^2$:\n$$ 1-\\delta_1 \\leq 1 \\quad \\text{and} \\quad 1 \\leq 1+\\delta_1 $$\nThe first inequality, $1-\\delta_1 \\leq 1$, simplifies to $-\\delta_1 \\leq 0$, or $\\delta_1 \\geq 0$.\nThe second inequality, $1 \\leq 1+\\delta_1$, simplifies to $0 \\leq \\delta_1$.\nBoth inequalities imply that $\\delta_1$ must be a non-negative number. By definition, the restricted isometry constant $\\delta_s$ is the *smallest* non-negative number satisfying the property. The smallest non-negative number that satisfies $\\delta_1 \\geq 0$ is $\\delta_1 = 0$.\n\nPart 2: Failure of Basis Pursuit.\n\nWe must find a $1$-sparse signal $x^{\\star} \\in \\mathbb{R}^2$ for which basis pursuit fails to uniquely recover it. This requires finding another vector $x \\neq x^{\\star}$ such that $Ax = Ax^{\\star}$ and $\\|x\\|_1 \\leq \\|x^{\\star}\\|_1$.\nThe basis pursuit problem is $\\min_{z} \\|z\\|_1$ subject to $Az=y$. Failure of unique recovery occurs if the minimizer is not unique.\n\nLet us select a $1$-sparse signal $x^{\\star}$. A simple choice is $x^{\\star} = e_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\nThis signal is $1$-sparse, and its $\\ell_1$-norm is $\\|x^{\\star}\\|_1 = |1| + |0| = 1$.\nThe measurement obtained from this signal is $y = Ax^{\\star} = \\begin{bmatrix}1  1\\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = 1$.\n\nNow, we need to find another vector $x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$, with $x \\neq x^{\\star}$, that is a potential solution to the basis pursuit problem. This vector must satisfy:\n1. $Ax = y \\implies x_1 + x_2 = 1$.\n2. $\\|x\\|_1 \\leq \\|x^{\\star}\\|_1 \\implies |x_1| + |x_2| \\leq 1$.\n\nConsider the vector $x = e_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$.\nThis vector is distinct from $x^{\\star}$. Let's check if it satisfies the conditions.\n1. $Ax = \\begin{bmatrix}1  1\\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = 1$. This matches the measurement $y$, so $x$ is a feasible solution.\n2. $\\|x\\|_1 = |0| + |1| = 1$. This satisfies $\\|x\\|_1 \\leq \\|x^{\\star}\\|_1$, as $\\|x^{\\star}\\|_1 = 1$.\n\nWe have found two distinct vectors, $x^{\\star}=e_1$ and $x=e_2$, which are both $1$-sparse, produce the same measurement $y=1$, and have the same minimal $\\ell_1$-norm of $1$. The basis pursuit optimization problem has at least two solutions. Therefore, it fails to uniquely identify $x^{\\star}$.\n\nThe reason for this failure lies in the relationship between the measurement operator and the basis in which the signals are sparse (the canonical basis $\\{e_1, e_2\\}$). The measurement is given by $y = x_1 + x_2$. This operation is symmetric with respect to the two coordinates. Swapping $x_1$ and $x_2$ does not change the measurement.\nThe nullspace of $A$ is $\\mathcal{N}(A) = \\{ z \\in \\mathbb{R}^2 \\mid z_1 + z_2 = 0 \\}$. This space is spanned by the vector $h = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} = e_1 - e_2$.\nRecovery of $x^{\\star}$ is guaranteed to be unique if for every non-zero $h \\in \\mathcal{N}(A)$, $\\|x^{\\star}+h\\|_1  \\|x^{\\star}\\|_1$.\nIn our case, let $x^{\\star} = e_1$ and choose $h' = e_2 - e_1 = -h \\in \\mathcal{N}(A)$.\nThe alternative solution is $x = x^{\\star} + h' = e_1 + (e_2 - e_1) = e_2$.\nWe evaluate the condition: $\\|e_1 + (e_2-e_1)\\|_1 = \\|e_2\\|_1 = 1$.\nThis is not strictly greater than $\\|e_1\\|_1 = 1$. The condition for unique recovery is violated.\nThis failure is a direct consequence of the high coherence between the measurement operator and the sparsity-inducing basis. The measurement vector associated with $A$, which is proportional to $u = \\frac{1}{\\sqrt{2}}(e_1+e_2)$, has equal projections onto the basis vectors $e_1$ and $e_2$. This alignment makes the contributions from $e_1$ and $e_2$ indistinguishable from the single measurement, leading to the ambiguity in reconstruction.\n\nThe value requested for the final answer is $\\delta_1$. As derived in the first part, $\\delta_1=0$.", "answer": "$$\\boxed{0}$$", "id": "3493094"}]}