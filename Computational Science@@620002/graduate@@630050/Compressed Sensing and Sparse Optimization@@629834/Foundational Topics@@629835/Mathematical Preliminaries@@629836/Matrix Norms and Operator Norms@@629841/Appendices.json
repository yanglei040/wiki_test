{"hands_on_practices": [{"introduction": "Mastering matrix norms begins with direct computation from their core definitions. This practice invites you to calculate the three most common induced norms—the $\\ell_1$, $\\ell_\\infty$, and spectral ($\\ell_2$) norms—for a simple $2 \\times 2$ matrix. By working through the specific mechanics for each norm, you will solidify your understanding of how they capture different aspects of a matrix's \"size\" or \"strength\" [@problem_id:3460936].", "problem": "In stability and error amplification analyses for finite difference or finite element time-stepping of linear partial differential equations, operator norms induced by vector norms quantify how a discrete update amplifies data across a grid. Consider the $2 \\times 2$ block matrix $A = \\begin{pmatrix} 1  2 \\\\ -3  4 \\end{pmatrix}$ representing a local two-degree-of-freedom coupling that arises, for example, in a block-iterative preconditioner for a convection–diffusion discretization. Using only the definition of the induced operator norm $\\|A\\|_{p} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{p}}{\\|x\\|_{p}}$ and, for the Euclidean case, the variational characterization of symmetric positive semidefinite matrices via the Rayleigh quotient, compute exactly the three induced operator norms $\\|A\\|_{1}$, $\\|A\\|_{\\infty}$, and $\\|A\\|_{2}$ for the given $A$. Then determine, by direct reasoning from these definitions, the strict ordering of their magnitudes. Provide as your final answer the triple $(\\|A\\|_{1}, \\|A\\|_{\\infty}, \\|A\\|_{2})$ in that order. Do not round; give exact values. No units are required.", "solution": "The problem requires the computation of three induced operator norms, $\\|A\\|_{1}$, $\\|A\\|_{\\infty}$, and $\\|A\\|_{2}$, for the matrix $A = \\begin{pmatrix} 1  2 \\\\ -3  4 \\end{pmatrix}$, and the determination of the strict ordering of their magnitudes. The computations must be based on the fundamental definition of an induced norm, $\\|A\\|_{p} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{p}}{\\|x\\|_{p}}$.\n\nFirst, we compute the $\\|A\\|_{1}$ norm.\nThe $1$-norm of a vector $v = (v_1, v_2, \\dots, v_m)^T$ is $\\|v\\|_1 = \\sum_{i=1}^m |v_i|$.\nLet $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ be a vector in $\\mathbb{R}^2$, so $\\|x\\|_1 = |x_1| + |x_2|$.\nThe vector $Ax$ is $Ax = \\begin{pmatrix} 1  2 \\\\ -3  4 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} x_1 + 2x_2 \\\\ -3x_1 + 4x_2 \\end{pmatrix}$.\nThe $1$-norm of $Ax$ is $\\|Ax\\|_1 = |x_1 + 2x_2| + |-3x_1 + 4x_2|$.\nUsing the triangle inequality, we can establish an upper bound:\n$\\|Ax\\|_1 \\le |x_1| + |2x_2| + |-3x_1| + |4x_2| = |x_1| + 2|x_2| + 3|x_1| + 4|x_2| = 4|x_1| + 6|x_2|$.\nThis can be bounded further:\n$4|x_1| + 6|x_2| \\le 6|x_1| + 6|x_2| = 6(|x_1| + |x_2|) = 6\\|x\\|_1$.\nThus, $\\frac{\\|Ax\\|_1}{\\|x\\|_1} \\le 6$ for any $x \\neq 0$. This implies $\\|A\\|_1 \\le 6$.\nThe general formula for the induced $1$-norm is the maximum absolute column sum. For matrix $A$, the absolute column sums are:\nColumn 1: $|1| + |-3| = 1 + 3 = 4$.\nColumn 2: $|2| + |4| = 2 + 4 = 6$.\nThe maximum is $6$. To show that this bound is achieved, we select a vector $x$ that corresponds to the column with the maximum sum. The maximum sum occurs for the second column ($j=2$). We choose $x=e_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nFor this $x$, $\\|x\\|_1 = |0| + |1| = 1$.\n$Ax = \\begin{pmatrix} 1  2 \\\\ -3  4 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\end{pmatrix}$.\n$\\|Ax\\|_1 = |2| + |4| = 6$.\nThe ratio is $\\frac{\\|Ax\\|_1}{\\|x\\|_1} = \\frac{6}{1} = 6$.\nSince we found a vector for which the ratio is $6$, and we showed that the ratio can never exceed $6$, the supremum is exactly $6$.\nTherefore, $\\|A\\|_1 = 6$.\n\nSecond, we compute the $\\|A\\|_{\\infty}$ norm.\nThe $\\infty$-norm of a vector $v = (v_1, v_2, \\dots, v_m)^T$ is $\\|v\\|_{\\infty} = \\max_{i} |v_i|$.\nFor $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$, $\\|x\\|_{\\infty} = \\max(|x_1|, |x_2|)$.\nThe $\\infty$-norm of $Ax$ is $\\|Ax\\|_{\\infty} = \\max(|x_1 + 2x_2|, |-3x_1 + 4x_2|)$.\nUsing the triangle inequality on each component:\n$|x_1 + 2x_2| \\le |x_1| + 2|x_2| \\le \\|x\\|_{\\infty} + 2\\|x\\|_{\\infty} = 3\\|x\\|_{\\infty}$.\n$|-3x_1 + 4x_2| \\le 3|x_1| + 4|x_2| \\le 3\\|x\\|_{\\infty} + 4\\|x\\|_{\\infty} = 7\\|x\\|_{\\infty}$.\nSo, $\\|Ax\\|_{\\infty} \\le \\max(3\\|x\\|_{\\infty}, 7\\|x\\|_{\\infty}) = 7\\|x\\|_{\\infty}$.\nThis implies $\\frac{\\|Ax\\|_{\\infty}}{\\|x\\|_{\\infty}} \\le 7$, so $\\|A\\|_{\\infty} \\le 7$.\nThe general formula for the induced $\\infty$-norm is the maximum absolute row sum. For matrix $A$, the absolute row sums are:\nRow 1: $|1| + |2| = 1 + 2 = 3$.\nRow 2: $|-3| + |4| = 3 + 4 = 7$.\nThe maximum is $7$. To show this bound is achieved, we select a vector $x$ where the components are chosen to maximize the entry of $Ax$ corresponding to the row with the maximum sum. The maximum sum occurs for the second row ($i=2$). We choose $x$ such that its components have signs matching the signs of the entries in that row, i.e., $x_1 = \\text{sgn}(-3) = -1$ and $x_2 = \\text{sgn}(4) = 1$. Let $x = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$.\nFor this $x$, $\\|x\\|_{\\infty} = \\max(|-1|, |1|) = 1$.\n$Ax = \\begin{pmatrix} 1  2 \\\\ -3  4 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1+2 \\\\ 3+4 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 7 \\end{pmatrix}$.\n$\\|Ax\\|_{\\infty} = \\max(|1|, |7|) = 7$.\nThe ratio is $\\frac{\\|Ax\\|_{\\infty}}{\\|x\\|_{\\infty}} = \\frac{7}{1} = 7$.\nSince we found a vector for which the ratio is $7$, and the ratio never exceeds $7$, the supremum is exactly $7$.\nTherefore, $\\|A\\|_{\\infty} = 7$.\n\nThird, we compute the $\\|A\\|_{2}$ norm.\nThe induced $2$-norm, or spectral norm, is defined by $\\|A\\|_2 = \\sup_{x \\neq 0} \\frac{\\|Ax\\|_2}{\\|x\\|_2}$. The square of this norm is given by:\n$\\|A\\|_2^2 = \\sup_{x \\neq 0} \\frac{\\|Ax\\|_2^2}{\\|x\\|_2^2} = \\sup_{x \\neq 0} \\frac{(Ax)^T(Ax)}{x^T x} = \\sup_{x \\neq 0} \\frac{x^T A^T A x}{x^T x}$.\nThe expression $\\frac{x^T B x}{x^T x}$ is the Rayleigh quotient for a matrix $B$. The problem directs us to use the variational characterization for symmetric positive semidefinite matrices. The matrix $B = A^T A$ is symmetric and at least positive semidefinite. The variational characterization (specifically, the Rayleigh-Ritz theorem) states that the supremum of the Rayleigh quotient for a symmetric matrix is its largest eigenvalue, $\\lambda_{\\max}$.\nSo, $\\|A\\|_2^2 = \\lambda_{\\max}(A^T A)$, and $\\|A\\|_2 = \\sqrt{\\lambda_{\\max}(A^T A)}$.\nWe compute the matrix $A^T A$:\n$A^T = \\begin{pmatrix} 1  -3 \\\\ 2  4 \\end{pmatrix}$.\n$A^T A = \\begin{pmatrix} 1  -3 \\\\ 2  4 \\end{pmatrix} \\begin{pmatrix} 1  2 \\\\ -3  4 \\end{pmatrix} = \\begin{pmatrix} (1)(1)+(-3)(-3)  (1)(2)+(-3)(4) \\\\ (2)(1)+(4)(-3)  (2)(2)+(4)(4) \\end{pmatrix} = \\begin{pmatrix} 1+9  2-12 \\\\ 2-12  4+16 \\end{pmatrix} = \\begin{pmatrix} 10  -10 \\\\ -10  20 \\end{pmatrix}$.\nNext, we find the eigenvalues of $A^T A$ by solving the characteristic equation $\\det(A^T A - \\lambda I) = 0$.\n$\\det\\begin{pmatrix} 10-\\lambda  -10 \\\\ -10  20-\\lambda \\end{pmatrix} = (10-\\lambda)(20-\\lambda) - (-10)(-10) = 0$.\n$200 - 10\\lambda - 20\\lambda + \\lambda^2 - 100 = 0$.\n$\\lambda^2 - 30\\lambda + 100 = 0$.\nUsing the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$:\n$\\lambda = \\frac{30 \\pm \\sqrt{(-30)^2 - 4(1)(100)}}{2} = \\frac{30 \\pm \\sqrt{900 - 400}}{2} = \\frac{30 \\pm \\sqrt{500}}{2}$.\nSimplifying the square root: $\\sqrt{500} = \\sqrt{100 \\times 5} = 10\\sqrt{5}$.\n$\\lambda = \\frac{30 \\pm 10\\sqrt{5}}{2} = 15 \\pm 5\\sqrt{5}$.\nThe two eigenvalues are $\\lambda_1 = 15 + 5\\sqrt{5}$ and $\\lambda_2 = 15 - 5\\sqrt{5}$. The largest eigenvalue is $\\lambda_{\\max}(A^T A) = 15 + 5\\sqrt{5}$.\nThe $2$-norm is the square root of this value:\n$\\|A\\|_2 = \\sqrt{15 + 5\\sqrt{5}}$.\n\nFinally, we determine the strict ordering of the magnitudes of these norms. The computed values are:\n$\\|A\\|_1 = 6$\n$\\|A\\|_{\\infty} = 7$\n$\\|A\\|_2 = \\sqrt{15 + 5\\sqrt{5}}$\nClearly, $\\|A\\|_1 = 6  7 = \\|A\\|_{\\infty}$.\nNow we compare $\\|A\\|_2$ with $\\|A\\|_1$. To do this, we compare their squares:\n$\\|A\\|_2^2 = 15 + 5\\sqrt{5}$ and $\\|A\\|_1^2 = 6^2 = 36$.\nWe test if $15 + 5\\sqrt{5}  36$. This is equivalent to $5\\sqrt{5}  21$.\nSquaring both positive sides gives $(5\\sqrt{5})^2  21^2$, which is $25 \\times 5  441$, or $125  441$. This inequality is true.\nTherefore, $\\|A\\|_2^2  \\|A\\|_1^2$, and since norms are non-negative, $\\|A\\|_2  \\|A\\|_1$.\nCombining the results, the strict ordering is $\\|A\\|_2  \\|A\\|_1  \\|A\\|_{\\infty}$.\n\nThe problem requests the final answer as the triple $(\\|A\\|_{1}, \\|A\\|_{\\infty}, \\|A\\|_{2})$.\nThe values are $\\|A\\|_{1} = 6$, $\\|A\\|_{\\infty} = 7$, and $\\|A\\|_{2} = \\sqrt{15 + 5\\sqrt{5}}$.", "answer": "$$\n\\boxed{(6, 7, \\sqrt{15 + 5\\sqrt{5}})}\n$$", "id": "3460936"}, {"introduction": "Beyond pure definition, operator norms are critical tools in designing and analyzing numerical algorithms. This exercise connects the spectral norm, $\\|A\\|_2$, to the convergence of iterative methods by using it to set the optimal step size for a gradient-based algorithm. You will contrast the exact step size with a practical estimate derived from Gershgorin's circle theorem, offering insight into the trade-offs between analytical precision and computational feasibility in algorithm tuning [@problem_id:3459620].", "problem": "Consider a linear sensing model in compressed sensing where measurements are generated via a matrix $A \\in \\mathbb{R}^{n \\times m}$. For iterative gradient-based sparse recovery methods such as the Iterative Shrinkage-Thresholding Algorithm (ISTA), a standard step size is chosen as $\\alpha = 1/L$, where $L$ is the Lipschitz constant of the gradient of the quadratic data fidelity term $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$. The fundamental base facts needed are: the spectral norm $\\|A\\|_{2}$ equals the largest singular value of $A$; the squared spectral norm $\\|A\\|_{2}^{2}$ equals the largest eigenvalue of the symmetric positive semidefinite matrix $A^{\\top}A$; and the Gershgorin circle theorem bounds all eigenvalues of a square matrix $M$ within the union of discs centered at $M_{ii}$ with radius $\\sum_{j \\neq i} |M_{ij}|$. \n\nLet the sensing matrix $A \\in \\mathbb{R}^{2 \\times 3}$ have unit-norm columns arranged as an equiangular set:\n$$\nA = \\begin{pmatrix}\n1  -\\frac{1}{2}  -\\frac{1}{2} \\\\\n0  \\frac{\\sqrt{3}}{2}  -\\frac{\\sqrt{3}}{2}\n\\end{pmatrix}.\n$$\nYou will:\n- Use the Gershgorin circle theorem on the Gram matrix $G = A^{\\top}A$ to bound $\\|A\\|_{2}^{2}$ and thereby obtain a conservative step size $\\alpha_{\\mathrm{G}} = 1/\\overline{L}$ where $\\overline{L}$ is the Gershgorin-based upper bound on $\\|A\\|_{2}^{2}$.\n- Compute the exact value $\\|A\\|_{2}^{2}$ from first principles using either the Singular Value Decomposition (SVD) or by relating nonzero eigenvalues of $A^{\\top}A$ and $A A^{\\top}$, and thereby obtain the exact step size $\\alpha_{\\ast} = 1/\\|A\\|_{2}^{2}$.\n- Quantify the tightness of the Gershgorin step size by evaluating the ratio $R = \\alpha_{\\mathrm{G}} / \\alpha_{\\ast}$.\n\nReport the single ratio $R$ as your final answer. No rounding is required. Express the final answer as a single exact number or closed-form fraction without units.", "solution": "The problem requires the calculation and comparison of two step sizes for a gradient-based optimization algorithm. The step sizes are derived from the Lipschitz constant of the gradient of the function $f(x) = \\frac{1}{2}\\|Ax - b\\|_{2}^{2}$.\n\nThe gradient of $f(x)$ with respect to $x$ is $\\nabla f(x) = A^{\\top}(Ax - b)$. The Hessian of $f(x)$ is $\\nabla^2 f(x) = A^{\\top}A$. The Lipschitz constant, $L$, of the gradient $\\nabla f(x)$ is the induced $2$-norm of its Hessian matrix. For a real matrix, the induced $2$-norm is its spectral norm.\n$$L = \\|\\nabla^2 f(x)\\|_{2} = \\|A^{\\top}A\\|_{2}$$\nSince the matrix $A^{\\top}A$ is symmetric and positive semidefinite, its spectral norm is equal to its largest eigenvalue, $\\lambda_{\\max}(A^{\\top}A)$. Furthermore, the eigenvalues of $A^{\\top}A$ are the squares of the singular values of $A$, so $\\lambda_{\\max}(A^{\\top}A) = \\sigma_{\\max}(A)^2 = \\|A\\|_{2}^{2}$.\nTherefore, the exact Lipschitz constant is $L = \\|A\\|_{2}^{2}$. The optimal step size is $\\alpha_{\\ast} = 1/L = 1/\\|A\\|_{2}^{2}$.\n\nWe are given the sensing matrix $A \\in \\mathbb{R}^{2 \\times 3}$:\n$$\nA = \\begin{pmatrix}\n1  -\\frac{1}{2}  -\\frac{1}{2} \\\\\n0  \\frac{\\sqrt{3}}{2}  -\\frac{\\sqrt{3}}{2}\n\\end{pmatrix}\n$$\n\nFirst, we will find a conservative step size $\\alpha_{\\mathrm{G}}$ using the Gershgorin circle theorem to obtain an upper bound $\\overline{L}$ on $L = \\|A\\|_{2}^{2}$. This requires computing the Gram matrix $G = A^{\\top}A$.\n$$\nA^{\\top} = \\begin{pmatrix}\n1  0 \\\\\n-\\frac{1}{2}  \\frac{\\sqrt{3}}{2} \\\\\n-\\frac{1}{2}  -\\frac{\\sqrt{3}}{2}\n\\end{pmatrix}\n$$\nThe Gram matrix is:\n$$\nG = A^{\\top}A = \\begin{pmatrix}\n1  0 \\\\\n-\\frac{1}{2}  \\frac{\\sqrt{3}}{2} \\\\\n-\\frac{1}{2}  -\\frac{\\sqrt{3}}{2}\n\\end{pmatrix}\n\\begin{pmatrix}\n1  -\\frac{1}{2}  -\\frac{1}{2} \\\\\n0  \\frac{\\sqrt{3}}{2}  -\\frac{\\sqrt{3}}{2}\n\\end{pmatrix}\n$$\nThe entries are $G_{ij} = a_i^{\\top}a_j$, where $a_i$ is the $i$-th column of $A$.\n$G_{11} = 1^2 + 0^2 = 1$\n$G_{22} = (-\\frac{1}{2})^2 + (\\frac{\\sqrt{3}}{2})^2 = \\frac{1}{4} + \\frac{3}{4} = 1$\n$G_{33} = (-\\frac{1}{2})^2 + (-\\frac{\\sqrt{3}}{2})^2 = \\frac{1}{4} + \\frac{3}{4} = 1$\n$G_{12} = 1(-\\frac{1}{2}) + 0(\\frac{\\sqrt{3}}{2}) = -\\frac{1}{2}$\n$G_{13} = 1(-\\frac{1}{2}) + 0(-\\frac{\\sqrt{3}}{2}) = -\\frac{1}{2}$\n$G_{23} = (-\\frac{1}{2})(-\\frac{1}{2}) + (\\frac{\\sqrt{3}}{2})(-\\frac{\\sqrt{3}}{2}) = \\frac{1}{4} - \\frac{3}{4} = -\\frac{1}{2}$\nThus, the Gram matrix is:\n$$\nG = \\begin{pmatrix}\n1  -\\frac{1}{2}  -\\frac{1}{2} \\\\\n-\\frac{1}{2}  1  -\\frac{1}{2} \\\\\n-\\frac{1}{2}  -\\frac{1}{2}  1\n\\end{pmatrix}\n$$\nThe Gershgorin circle theorem states that every eigenvalue of a square matrix $M$ is located in at least one of the Gershgorin discs $D(M_{ii}, R_i)$, where $R_i = \\sum_{j \\neq i} |M_{ij}|$. For the matrix $G$, the centers are the diagonal entries $G_{ii} = 1$ for $i \\in \\{1, 2, 3\\}$. The radii are:\n$R_1 = |G_{12}| + |G_{13}| = |-\\frac{1}{2}| + |-\\frac{1}{2}| = 1$\n$R_2 = |G_{21}| + |G_{23}| = |-\\frac{1}{2}| + |-\\frac{1}{2}| = 1$\n$R_3 = |G_{31}| + |G_{32}| = |-\\frac{1}{2}| + |-\\frac{1}{2}| = 1$\nAll three discs are identical: $D(1, 1)$, which corresponds to the interval $[1-1, 1+1] = [0, 2]$ on the real axis, since $G$ is symmetric and its eigenvalues are real. The union of the discs is $[0, 2]$. Thus, any eigenvalue $\\lambda$ of $G$ must satisfy $0 \\le \\lambda \\le 2$. This provides an upper bound on the largest eigenvalue: $\\lambda_{\\max}(G) \\le 2$.\nWe set $\\overline{L} = 2$. The conservative step size is $\\alpha_{\\mathrm{G}} = 1/\\overline{L} = 1/2$.\n\nNext, we compute the exact value of $L = \\|A\\|_{2}^{2} = \\lambda_{\\max}(A^{\\top}A)$. It is a standard result in linear algebra that the non-zero eigenvalues of $A^{\\top}A$ are the same as the non-zero eigenvalues of $AA^{\\top}$. Since $A$ is a $2 \\times 3$ matrix, $AA^{\\top}$ is a smaller $2 \\times 2$ matrix, whose eigenvalues are easier to compute.\n$$\nAA^{\\top} = \\begin{pmatrix}\n1  -\\frac{1}{2}  -\\frac{1}{2} \\\\\n0  \\frac{\\sqrt{3}}{2}  -\\frac{\\sqrt{3}}{2}\n\\end{pmatrix}\n\\begin{pmatrix}\n1  0 \\\\\n-\\frac{1}{2}  \\frac{\\sqrt{3}}{2} \\\\\n-\\frac{1}{2}  -\\frac{\\sqrt{3}}{2}\n\\end{pmatrix}\n$$\n$$\nAA^{\\top} = \\begin{pmatrix}\n1 + \\frac{1}{4} + \\frac{1}{4}  0 - \\frac{\\sqrt{3}}{4} + \\frac{\\sqrt{3}}{4} \\\\\n0 - \\frac{\\sqrt{3}}{4} + \\frac{\\sqrt{3}}{4}  0 + \\frac{3}{4} + \\frac{3}{4}\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{3}{2}  0 \\\\\n0  \\frac{3}{2}\n\\end{pmatrix}\n$$\nThe matrix $AA^{\\top}$ is a diagonal matrix, so its eigenvalues are its diagonal entries. The eigenvalues are $\\lambda_1 = \\frac{3}{2}$ and $\\lambda_2 = \\frac{3}{2}$. These are the two non-zero eigenvalues of the $3 \\times 3$ matrix $A^{\\top}A$. The third eigenvalue of $A^{\\top}A$ must be $0$.\nTherefore, the full set of eigenvalues of $A^{\\top}A$ is $\\{\\frac{3}{2}, \\frac{3}{2}, 0\\}$.\nThe largest eigenvalue is $\\lambda_{\\max}(A^{\\top}A) = \\frac{3}{2}$.\nThe exact Lipschitz constant is $L = \\|A\\|_{2}^{2} = \\frac{3}{2}$.\nThe exact optimal step size is $\\alpha_{\\ast} = 1/L = 1/(\\frac{3}{2}) = \\frac{2}{3}$.\n\nFinally, we compute the ratio $R = \\alpha_{\\mathrm{G}} / \\alpha_{\\ast}$ to quantify the tightness of the Gershgorin-based step size.\n$$\nR = \\frac{\\alpha_{\\mathrm{G}}}{\\alpha_{\\ast}} = \\frac{1/2}{2/3} = \\frac{1}{2} \\cdot \\frac{3}{2} = \\frac{3}{4}\n$$\nThe ratio of the conservative step size to the exact step size is $3/4$.", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "3459620"}, {"introduction": "Operator norms are indispensable for providing theoretical guarantees in fields like compressed sensing, where we aim to recover sparse signals from limited data. This practice delves into the performance analysis of a support detection algorithm by relating its success to key operator norms of the sensing matrix. By working through this hypothetical scenario, you will see how $\\|A\\|_{1 \\to 2}$ and $\\|A\\|_{2 \\to 2}$ arise naturally in bounding signal and noise terms, ultimately determining the minimum signal strength required for reliable recovery [@problem_id:3459610].", "problem": "Consider a binary sensing matrix $A \\in \\{0,1\\}^{m \\times n}$ used for sparse support detection in compressed sensing. Assume $A$ has the following regularity properties: every column $a_{j}$ of $A$ has exactly $r$ ones (so $\\|a_{j}\\|_{2} = \\sqrt{r}$ for all $j$), and any two distinct columns $a_{j}$ and $a_{k}$ have an inner product $a_{j}^{\\top} a_{k} = \\lambda$ with a constant overlap parameter $0 \\leq \\lambda  r$. Let $x_{\\star} \\in \\mathbb{R}^{n}$ be a $k$-sparse signal with equal positive amplitudes $\\alpha$ on its support $S \\subset \\{1,\\dots,n\\}$, i.e., $x_{\\star,j} = \\alpha$ if $j \\in S$ and $x_{\\star,j} = 0$ otherwise. Let the observed data be $y = A x_{\\star} + e$, where $e \\in \\mathbb{R}^{m}$ is an additive noise vector satisfying $\\|e\\|_{2} \\leq \\eta$.\n\nYou consider support detection by thresholding the correlation scores $z = A^{\\top} y$, declaring an index $j$ to be in the support $S$ if $z_{j} \\geq \\tau$ for some threshold $\\tau$. Using only the core definitions of operator norms,\n$$\n\\|A\\|_{p \\to q} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{q}}{\\|x\\|_{p}},\n$$\nthe spectral norm characterization via singular values, the Cauchy–Schwarz inequality, and basic eigenvalue facts for rank-one perturbations of scaled identity matrices, perform the following steps:\n\n1. Derive closed-form expressions for $\\|A\\|_{1 \\to 2}$ and $\\|A\\|_{2 \\to 2}$ in terms of $r$, $\\lambda$, and $n$, and hence evaluate the quantity $D = \\|A\\|_{2 \\to 2} - \\|A\\|_{1 \\to 2}$.\n\n2. Using the noise bound and the structure of $A$, determine a sufficient condition on $\\alpha$ and $\\tau$ that guarantees perfect support detection by thresholding, i.e., every $j \\in S$ satisfies $z_{j} \\geq \\tau$ and every $j \\notin S$ satisfies $z_{j}  \\tau$. Choose the threshold $\\tau$ optimally within the gap implied by your bounds.\n\nYour final answer should be a single closed-form analytical expression for the minimum amplitude $\\alpha_{\\min}$ (as a function of $r$, $\\lambda$, $k$, and $\\eta$) that guarantees perfect support detection by thresholding under the stated assumptions. No rounding is required and no physical units are involved. Express the final answer exactly.", "solution": "The solution proceeds in two parts as requested by the problem.\n\nPart 1: Derivation of Operator Norms\n\nFirst, we determine the value of $\\|A\\|_{1 \\to 2}$. The definition of this operator norm is\n$$\n\\|A\\|_{1 \\to 2} = \\sup_{x \\neq 0} \\frac{\\|Ax\\|_{2}}{\\|x\\|_{1}}\n$$\nLet $x \\in \\mathbb{R}^{n}$. The vector $Ax$ can be written as a linear combination of the columns of $A$, denoted by $a_j \\in \\mathbb{R}^{m}$:\n$$\nAx = \\sum_{j=1}^{n} x_j a_j\n$$\nUsing the triangle inequality for the $\\ell_2$-norm, we have:\n$$\n\\|Ax\\|_{2} = \\left\\|\\sum_{j=1}^{n} x_j a_j\\right\\|_{2} \\leq \\sum_{j=1}^{n} \\|x_j a_j\\|_{2} = \\sum_{j=1}^{n} |x_j| \\|a_j\\|_{2}\n$$\nThe problem states that every column $a_j$ has exactly $r$ ones, which implies that the $\\ell_2$-norm of each column is constant: $\\|a_j\\|_{2} = \\sqrt{r}$ for all $j \\in \\{1, \\dots, n\\}$.\nSubstituting this into the inequality, we get:\n$$\n\\|Ax\\|_{2} \\leq \\sum_{j=1}^{n} |x_j| \\sqrt{r} = \\sqrt{r} \\sum_{j=1}^{n} |x_j| = \\sqrt{r} \\|x\\|_{1}\n$$\nThis implies that $\\frac{\\|Ax\\|_{2}}{\\|x\\|_{1}} \\leq \\sqrt{r}$ for any $x \\neq 0$. To show that the supremum is indeed $\\sqrt{r}$, we must demonstrate that this bound is attainable. Consider a test vector $x = e_j$ for any $j \\in \\{1, \\dots, n\\}$, where $e_j$ is the $j$-th standard basis vector. For this vector, $\\|x\\|_{1} = 1$. The product $Ax$ is:\n$$\nAx = Ae_j = a_j\n$$\nThe $\\ell_2$-norm is $\\|Ax\\|_{2} = \\|a_j\\|_{2} = \\sqrt{r}$.\nFor this choice of $x$, the ratio is $\\frac{\\|Ax\\|_{2}}{\\|x\\|_{1}} = \\frac{\\sqrt{r}}{1} = \\sqrt{r}$.\nThus, the supremum is achieved, and we conclude:\n$$\n\\|A\\|_{1 \\to 2} = \\sqrt{r}\n$$\nNext, we determine the value of $\\|A\\|_{2 \\to 2}$, also known as the spectral norm. The spectral norm is given by the largest singular value of $A$, $\\sigma_{\\max}(A)$. The singular values are the square roots of the eigenvalues of the matrix $A^{\\top}A$. Therefore, $\\|A\\|_{2 \\to 2} = \\sqrt{\\lambda_{\\max}(A^{\\top}A)}$.\nLet us construct the matrix $G = A^{\\top}A$. The entry $G_{jk}$ is given by the inner product $a_j^{\\top}a_k$. From the problem statement:\n-   For $j = k$, $G_{jj} = a_j^{\\top}a_j = \\|a_j\\|_{2}^{2} = r$.\n-   For $j \\neq k$, $G_{jk} = a_j^{\\top}a_k = \\lambda$.\nThe resulting $n \\times n$ matrix is:\n$$\nG = A^{\\top}A = \\begin{pmatrix} r  \\lambda  \\dots  \\lambda \\\\ \\lambda  r  \\dots  \\lambda \\\\ \\vdots  \\vdots  \\ddots  \\vdots \\\\ \\lambda  \\lambda  \\dots  r \\end{pmatrix}\n$$\nThis matrix can be expressed as a sum of a scaled identity matrix $I$ and a scaled all-ones matrix $J$:\n$$\nG = (r-\\lambda)I + \\lambda J\n$$\nThe matrix $J$ is a rank-one matrix. Its eigenvalues are known: it has one eigenvalue equal to $n$ (corresponding to eigenvector $\\mathbf{1}$, the vector of all ones) and $n-1$ eigenvalues equal to $0$ (the corresponding eigenspace is the set of vectors orthogonal to $\\mathbf{1}$).\nLet $v$ be an eigenvector of $J$ with eigenvalue $\\mu$. Then:\n$$\nGv = ((r-\\lambda)I + \\lambda J)v = (r-\\lambda)v + \\lambda(Jv) = (r-\\lambda)v + \\lambda(\\mu v) = (r-\\lambda+\\lambda\\mu)v\n$$\nSo, the eigenvalues of $G$ can be found by substituting the eigenvalues of $J$.\n1.  For the eigenvalue $\\mu = n$ of $J$, the corresponding eigenvalue of $G$ is $r-\\lambda+\\lambda n = r+\\lambda(n-1)$.\n2.  For the eigenvalue $\\mu = 0$ of $J$ (with multiplicity $n-1$), the corresponding eigenvalue of $G$ is $r-\\lambda+\\lambda(0) = r-\\lambda$.\nWe need to find the largest eigenvalue. We compare $r+\\lambda(n-1)$ and $r-\\lambda$. The problem states $0 \\leq \\lambda  r$ and $n$ is the number of columns, so $n \\ge 1$. Since $x_\\star$ is $k$-sparse, $n \\ge k$. As distinct columns are assumed ($a_j^\\top a_k$ for $j \\neq k$), we must have $n \\ge 2$. For $n \\ge 2$ and $\\lambda \\ge 0$, we have $\\lambda(n-1) \\ge 0$ and $-\\lambda \\le 0$. Thus, $r+\\lambda(n-1) \\ge r-\\lambda$. The inequality is strict if $\\lambda  0$.\nThe largest eigenvalue is $\\lambda_{\\max}(A^{\\top}A) = r+\\lambda(n-1)$.\nThe spectral norm is therefore:\n$$\n\\|A\\|_{2 \\to 2} = \\sqrt{r+\\lambda(n-1)}\n$$\nFinally, we evaluate the quantity $D$:\n$$\nD = \\|A\\|_{2 \\to 2} - \\|A\\|_{1 \\to 2} = \\sqrt{r+\\lambda(n-1)} - \\sqrt{r}\n$$\n\nPart 2: Sufficient Condition for Support Detection\n\nThe support detection mechanism is based on thresholding the correlation scores $z = A^{\\top}y$. Substituting the measurement model $y = Ax_{\\star} + e$, we get:\n$$\nz = A^{\\top}(Ax_{\\star} + e) = A^{\\top}Ax_{\\star} + A^{\\top}e\n$$\nLet's analyze the $j$-th component $z_j$ for an arbitrary index $j$.\n$$\nz_j = (A^{\\top}Ax_{\\star})_j + (A^{\\top}e)_j = \\sum_{l=1}^{n} (A^{\\top}A)_{jl} x_{\\star,l} + a_j^{\\top}e\n$$\nThe signal $x_{\\star}$ is $k$-sparse with support $S$ and has entries $x_{\\star,l} = \\alpha$ for $l \\in S$ and $x_{\\star,l}=0$ for $l \\notin S$.\nWe distinguish two cases.\n\nCase 1: $j \\in S$ (in-support index)\nIn this case, the sum over $l$ includes the term where $l=j$:\n$$\n(A^{\\top}Ax_{\\star})_j = \\alpha \\sum_{l \\in S} (A^{\\top}A)_{jl} = \\alpha \\left( (A^{\\top}A)_{jj} + \\sum_{l \\in S, l \\neq j} (A^{\\top}A)_{jl} \\right)\n$$\nUsing $(A^{\\top}A)_{jj} = r$ and $(A^{\\top}A)_{jl} = \\lambda$ for $l \\neq j$, and noting there are $k-1$ elements in $S \\setminus \\{j\\}$:\n$$\n(A^{\\top}Ax_{\\star})_j = \\alpha(r + (k-1)\\lambda)\n$$\nSo, for $j \\in S$, the correlation score is $z_j = \\alpha(r + (k-1)\\lambda) + a_j^{\\top}e$.\n\nCase 2: $j \\notin S$ (out-of-support index)\nIn this case, for all $l \\in S$, we have $l \\neq j$.\n$$\n(A^{\\top}Ax_{\\star})_j = \\alpha \\sum_{l \\in S} (A^{\\top}A)_{jl} = \\alpha \\sum_{l \\in S} \\lambda\n$$\nSince $|S|=k$, the sum is over $k$ terms:\n$$\n(A^{\\top}Ax_{\\star})_j = \\alpha (k\\lambda)\n$$\nSo, for $j \\notin S$, the correlation score is $z_j = \\alpha k\\lambda + a_j^{\\top}e$.\n\nFor perfect support detection, we need to find a threshold $\\tau$ such that $z_j \\geq \\tau$ for all $j \\in S$ and $z_j  \\tau$ for all $j \\notin S$. A sufficient condition for this to be possible is that the minimum possible value for an in-support score is strictly greater than the maximum possible value for an out-of-support score:\n$$\n\\min_{j \\in S} z_j  \\max_{j \\notin S} z_j\n$$\nThe uncertainty is due to the noise term $a_j^{\\top}e$. We can bound its magnitude using the Cauchy-Schwarz inequality and the given noise bound $\\|e\\|_2 \\leq \\eta$:\n$$\n|a_j^{\\top}e| \\leq \\|a_j\\|_2 \\|e\\|_2\n$$\nSince $\\|a_j\\|_2 = \\sqrt{r}$ for all $j$, we have $|a_j^{\\top}e| \\leq \\sqrt{r}\\eta$. This implies $-\\sqrt{r}\\eta \\leq a_j^{\\top}e \\leq \\sqrt{r}\\eta$.\n\nUsing this bound, we can establish lower and upper bounds on the scores:\nFor $j \\in S$: $z_j \\ge \\alpha(r+(k-1)\\lambda) - \\sqrt{r}\\eta$.\nFor $j \\notin S$: $z_j \\le \\alpha k\\lambda + \\sqrt{r}\\eta$.\n\nThe sufficient condition for separability becomes:\n$$\n\\alpha(r+(k-1)\\lambda) - \\sqrt{r}\\eta  \\alpha k\\lambda + \\sqrt{r}\\eta\n$$\nWe now solve this inequality for the signal amplitude $\\alpha$:\n$$\n\\alpha(r+k\\lambda-\\lambda) - \\alpha k\\lambda  2\\sqrt{r}\\eta\n$$\n$$\n\\alpha(r-\\lambda)  2\\sqrt{r}\\eta\n$$\nGiven that $r  \\lambda$, the term $r-\\lambda$ is positive, so we can divide by it without changing the inequality direction:\n$$\n\\alpha  \\frac{2\\sqrt{r}\\eta}{r-\\lambda}\n$$\nThis inequality provides the sufficient condition on $\\alpha$ to guarantee perfect support detection. The minimum amplitude $\\alpha_{\\min}$ that satisfies this condition is the right-hand side of the inequality. Note that the dependency on the sparsity level $k$ cancels out during the derivation, which is a specific feature of this matrix structure.\n\nThe optimal threshold $\\tau$ would lie in the gap between the two bounds, for instance, at the midpoint: $\\tau_{\\text{opt}} = \\frac{1}{2}\\left(\\left(\\alpha k\\lambda + \\sqrt{r}\\eta\\right) + \\left(\\alpha(r+(k-1)\\lambda) - \\sqrt{r}\\eta\\right)\\right) = \\frac{\\alpha}{2}(r+(2k-1)\\lambda)$.\n\nThe minimum amplitude $\\alpha_{\\min}$ is the final requested quantity.\n$$\n\\alpha_{\\min} = \\frac{2\\sqrt{r}\\eta}{r-\\lambda}\n$$", "answer": "$$\n\\boxed{\\frac{2\\sqrt{r}\\eta}{r-\\lambda}}\n$$", "id": "3459610"}]}