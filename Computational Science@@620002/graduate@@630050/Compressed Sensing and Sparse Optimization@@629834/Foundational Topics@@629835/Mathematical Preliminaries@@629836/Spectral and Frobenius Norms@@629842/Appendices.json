{"hands_on_practices": [{"introduction": "To build a strong intuition for matrix norms, it is helpful to think of the squared Frobenius norm, $\\|A\\|_F^2$, as the total \"energy\" of a matrix. This practice explores how this energy can be distributed among its singular values. By constructing matrices where this energy is either spread thinly across many modes or concentrated entirely in one, you will directly investigate how the spectral norm, $\\|A\\|_2$, which measures the peak amplification, relates to the total energy budget [@problem_id:3158808].", "problem": "In an iterative linear solver inside a computational science pipeline, a matrixâ€™s overall energy budget is constrained while its stability is governed by how much that energy is concentrated. Let $n \\ge 2$ be an integer and let $E > 0$ be a prescribed energy level. Using only the definitions of the Frobenius norm and the spectral norm in terms of singular values, and the basic properties of singular value decomposition, perform the following:\n\n1. Construct an explicit full-rank $n \\times n$ matrix $A$ with Frobenius norm $E$ by distributing the energy across many singular values rather than concentrating it in one. Then, compute the ratio $\\rho_{\\text{full}}(n) = \\|A\\|_{F} / \\|A\\|_{2}$.\n\n2. Construct an explicit rank-$1$ matrix $B$ with Frobenius norm $E$ and compute the ratio $\\rho_{\\text{rank1}} = \\|B\\|_{F} / \\|B\\|_{2}$.\n\nExpress the final answer as a single row matrix containing $\\rho_{\\text{full}}(n)$ and $\\rho_{\\text{rank1}}$. No numerical approximation is required; provide exact symbolic expressions.", "solution": "We use the definitions of the Frobenius norm and spectral norm in terms of a matrix's singular values. For an $n \\times n$ matrix $M$ with singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_n \\ge 0$:\n- The spectral norm is the largest singular value: $\\|M\\|_2 = \\sigma_1$.\n- The Frobenius norm is the square root of the sum of squared singular values: $\\|M\\|_F = \\sqrt{\\sum_{i=1}^{n} \\sigma_i^2}$.\n\nThe problem states that the \"energy,\" or squared Frobenius norm, is fixed, so for any matrix we construct, we must have $\\|M\\|_F^2 = E^2$, which means $\\sum_{i=1}^{n} \\sigma_i^2 = E^2$.\n\n**Part 1: Full-rank matrix $A$**\n\nTo construct a full-rank matrix $A$ where energy is distributed, we make all singular values equal and positive: $\\sigma_1 = \\sigma_2 = \\dots = \\sigma_n = \\sigma > 0$.\nThe Frobenius norm constraint gives:\n$$ \\|A\\|_F^2 = \\sum_{i=1}^{n} \\sigma^2 = n \\sigma^2 = E^2 \\implies \\sigma = \\frac{E}{\\sqrt{n}} $$\nAn explicit matrix with these singular values is $A = \\frac{E}{\\sqrt{n}}I$, where $I$ is the identity matrix. For this matrix:\n- The Frobenius norm is $\\|A\\|_F = E$ by construction.\n- The spectral norm is the largest singular value: $\\|A\\|_2 = \\sigma_1 = \\frac{E}{\\sqrt{n}}$.\n\nThe ratio is:\n$$ \\rho_{\\text{full}}(n) = \\frac{\\|A\\|_F}{\\|A\\|_2} = \\frac{E}{E/\\sqrt{n}} = \\sqrt{n} $$\n\n**Part 2: Rank-1 matrix $B$**\n\nTo construct a rank-1 matrix $B$, we concentrate all energy in one singular value. This means $\\sigma_1 > 0$ and $\\sigma_2 = \\dots = \\sigma_n = 0$.\nThe Frobenius norm constraint gives:\n$$ \\|B\\|_F^2 = \\sigma_1^2 + 0 + \\dots + 0 = E^2 \\implies \\sigma_1 = E $$\nAn explicit matrix with these singular values is a diagonal matrix with $E$ as the first entry and zeros elsewhere. For this matrix:\n- The Frobenius norm is $\\|B\\|_F = E$ by construction.\n- The spectral norm is the largest singular value: $\\|B\\|_2 = \\sigma_1 = E$.\n\nThe ratio is:\n$$ \\rho_{\\text{rank1}} = \\frac{\\|B\\|_F}{\\|B\\|_2} = \\frac{E}{E} = 1 $$\n\nThus, the required row matrix contains $\\rho_{\\text{full}}(n) = \\sqrt{n}$ and $\\rho_{\\text{rank1}} = 1$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sqrt{n} & 1\n\\end{pmatrix}\n}\n$$", "id": "3158808"}, {"introduction": "Building on the concept of energy distribution, this exercise asks you to formalize the relationship between the peak gain (spectral norm) and total energy (Frobenius norm). You will derive the universal upper bound for the ratio $\\|A\\|_2 / \\|A\\|_F$ and identify the specific class of matrices for which this bound is achieved. This practice provides a rigorous proof for the idea that concentrating all of a matrix's energy into a single mode maximizes its directional gain relative to its overall size [@problem_id:2179391].", "problem": "In the analysis of linear systems, the \"gain\" of a transformation represented by a matrix can be quantified in various ways. Consider a linear transformation from $\\mathbb{R}^n$ to $\\mathbb{R}^m$ defined by a non-zero real matrix $A$ of size $m \\times n$.\n\nTwo common norms used to measure the \"size\" of the matrix $A$ are the spectral norm and the Frobenius norm.\n- The **spectral norm**, denoted $\\|A\\|_2$, is defined as the maximum amplification the matrix applies to any unit vector:\n$$ \\|A\\|_2 = \\max_{\\|x\\|_2=1} \\|Ax\\|_2 $$\n- The **Frobenius norm**, denoted $\\|A\\|_F$, is defined as the square root of the sum of the squares of all matrix elements $a_{ij}$:\n$$ \\|A\\|_F = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}^2} $$\n\nWe can define a \"gain efficiency ratio\" $E(A)$ as the ratio of the spectral norm to the Frobenius norm:\n$$ E(A) = \\frac{\\|A\\|_2}{\\|A\\|_F} $$\nThis ratio compares the maximum possible directional gain to a measure of the total \"energy\" of the matrix.\n\nYour task is to analyze this efficiency ratio. Determine the theoretical maximum value of $E(A)$ over all possible non-zero $m \\times n$ real matrices. In addition, find the rank that a matrix must have in order to achieve this maximum efficiency.\n\nPresent your answer as a row matrix $\\begin{pmatrix} v & r \\end{pmatrix}$, where $v$ is the maximum value of the ratio and $r$ is the rank of the matrices that achieve this maximum.", "solution": "Let $A \\in \\mathbb{R}^{m \\times n}$ be non-zero. Consider the singular value decomposition $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\sigma_{2}, \\dots, \\sigma_{p})$ with $p = \\min\\{m,n\\}$ and singular values ordered so that $\\sigma_{1} \\geq \\sigma_{2} \\geq \\dots \\geq \\sigma_{p} \\geq 0$.\n\nBy standard properties of norms and singular values,\n$$\n\\|A\\|_{2} = \\sigma_{1}, \\quad \\|A\\|_{F} = \\sqrt{\\sum_{i=1}^{p} \\sigma_{i}^{2}}.\n$$\nTherefore the efficiency ratio is\n$$\nE(A) = \\frac{\\|A\\|_{2}}{\\|A\\|_{F}} = \\frac{\\sigma_{1}}{\\sqrt{\\sum_{i=1}^{p} \\sigma_{i}^{2}}}.\n$$\nSince $\\sigma_{1}^{2} \\leq \\sum_{i=1}^{p} \\sigma_{i}^{2}$, it follows that\n$$\nE(A) \\leq 1.\n$$\nBecause $A$ is non-zero, we have $\\sigma_{1} > 0$, so the ratio is well-defined. Equality $E(A) = 1$ holds if and only if\n$$\n\\sigma_{1}^{2} = \\sum_{i=1}^{p} \\sigma_{i}^{2},\n$$\nwhich is equivalent to $\\sigma_{2} = \\sigma_{3} = \\dots = \\sigma_{p} = 0$. This condition means that $A$ has exactly one non-zero singular value, i.e., $\\operatorname{rank}(A) = 1$.\n\nTo see that the bound is tight and achievable, take any non-zero vectors $u \\in \\mathbb{R}^{m}$ and $v \\in \\mathbb{R}^{n}$, and set $A = u v^{\\top}$. Then $A$ has a single non-zero singular value $\\sigma_{1} = \\|u\\|_{2}\\|v\\|_{2}$ and all others zero. Consequently,\n$$\n\\|A\\|_{2} = \\sigma_{1} = \\|u\\|_{2}\\|v\\|_{2}, \\quad \\|A\\|_{F} = \\sqrt{\\sigma_{1}^{2}} = \\|u\\|_{2}\\|v\\|_{2},\n$$\nso $E(A) = 1$, and $\\operatorname{rank}(A) = 1$.\n\nThus, the theoretical maximum of $E(A)$ over all non-zero $m \\times n$ real matrices is $1$, and it is achieved precisely by rank-$1$ matrices.", "answer": "$$\\boxed{\\begin{pmatrix} 1 & 1 \\end{pmatrix}}$$", "id": "2179391"}, {"introduction": "We now apply our understanding of spectral and Frobenius norms to the analysis of measurement matrices in compressed sensing, a key topic in sparse optimization. In this context, columns are often normalized, which fixes the matrix's Frobenius norm, yet its performance is dictated by more subtle properties. This advanced practice guides you through deriving the connection between inter-column coherence, the spectral norm, and the Restricted Isometry Property (RIP) constant, revealing the trade-offs that govern the design of effective sensing matrices [@problem_id:3479750].", "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ be a measurement matrix used in compressed sensing whose columns $a_{1},\\dots,a_{n} \\in \\mathbb{R}^{m}$ are normalized so that $\\|a_{j}\\|_{2}=1$ for all $j=1,\\dots,n$. Consequently, the Frobenius norm satisfies $\\|A\\|_{F}^{2}=\\sum_{j=1}^{n}\\|a_{j}\\|_{2}^{2}=n$, so $\\|A\\|_{F}=\\sqrt{n}$ is fixed by column normalization. Define the spectral norm $\\|A\\|_{2}=\\sup_{\\|x\\|_{2}=1}\\|A x\\|_{2}$, the coherence $\\mu(A)=\\max_{i \\neq j}|\\langle a_{i},a_{j}\\rangle|$, and the Restricted Isometry Property (RIP) constant $\\delta_{s}$ as the smallest number such that for all $s$-sparse $x \\in \\mathbb{R}^{n}$, $(1-\\delta_{s})\\|x\\|_{2}^{2} \\le \\|A x\\|_{2}^{2} \\le (1+\\delta_{s})\\|x\\|_{2}^{2}$.\n\nStarting from these definitions and fundamental spectral facts about Gram matrices and eigenvalue localization for symmetric matrices, carry out the following:\n\n1. Construct a parameterized family of $n$ column vectors with unit $2$-norm and pairwise inner products equal to a parameter $\\mu \\in [0,1)$ to illustrate how, despite fixed $\\|A\\|_{F}=\\sqrt{n}$, creating nearly dependent columns (i.e., taking $\\mu$ close to $1$) can inflate $\\|A\\|_{2}$. Use the Gram matrix viewpoint to determine the resulting largest singular value of $A$ in terms of $n$ and $\\mu$.\n\n2. For a general matrix $A$ with unit-norm columns and coherence bounded by $\\mu(A) \\le \\mu$, derive a worst-case upper bound $U(n,\\mu)$ on $\\|A\\|_{2}$ using only the structure of the Gram matrix $A^{\\top}A$ and standard eigenvalue localization techniques.\n\n3. Using only the definitions and well-tested facts, derive an explicit upper bound $B(s,\\mu)$ on the RIP constant $\\delta_{s}$ in terms of the coherence $\\mu$ and the sparsity level $s$.\n\nYour final answer must consist of the two closed-form expressions $U(n,\\mu)$ and $B(s,\\mu)$, expressed symbolically. Do not include any inequalities or equations in the final answer. If you present more than one expression, write them together as a single row matrix using the LaTeX $\\mathrm{pmatrix}$ environment. No rounding is required.", "solution": "This problem requires us to derive an upper bound on the spectral norm and the Restricted Isometry Property (RIP) constant for a matrix with unit-norm columns and bounded coherence. We will use the Gram matrix and the Gershgorin Circle Theorem.\n\nLet $G = A^{\\top}A$ be the Gram matrix. Its entries are $G_{ij} = \\langle a_i, a_j \\rangle$. Since the columns of $A$ have unit norm, the diagonal entries are $G_{ii} = \\|a_i\\|_2^2 = 1$. The off-diagonal entries are bounded by the coherence: $|G_{ij}| \\le \\mu$ for $i \\ne j$. The spectral norm of $A$ is given by $\\|A\\|_2 = \\sqrt{\\lambda_{\\max}(G)}$, where $\\lambda_{\\max}(G)$ is the largest eigenvalue of $G$.\n\n**1. Derivation of the upper bound $U(n,\\mu)$ on $\\|A\\|_{2}$**\n\nWe apply the Gershgorin Circle Theorem to the Gram matrix $G$. Since $G$ is a real symmetric matrix, its eigenvalues are real. The theorem states that every eigenvalue $\\lambda$ of $G$ lies in at least one of the intervals $[G_{ii} - R_i, G_{ii} + R_i]$, where $R_i = \\sum_{j \\neq i} |G_{ij}|$.\n\nFor our matrix $G$:\n- The center of each interval is $G_{ii} = 1$.\n- The radius $R_i$ for each row $i$ is bounded as: $R_i = \\sum_{j \\neq i} |G_{ij}| \\le \\sum_{j \\neq i} \\mu = (n-1)\\mu$.\n\nThus, every eigenvalue $\\lambda$ of $G$ must satisfy $|\\lambda - 1| \\le (n-1)\\mu$, which implies:\n$$ 1 - (n-1)\\mu \\le \\lambda \\le 1 + (n-1)\\mu $$\nThis bound must hold for the largest eigenvalue, so $\\lambda_{\\max}(G) \\le 1 + (n-1)\\mu$. Taking the square root gives the upper bound on the spectral norm:\n$$ \\|A\\|_2 = \\sqrt{\\lambda_{\\max}(G)} \\le \\sqrt{1 + (n-1)\\mu} $$\nThis bound is tight, as it is achieved by a matrix whose Gram matrix has all off-diagonal entries equal to $\\mu$. Therefore, the requested upper bound is $U(n,\\mu) = \\sqrt{1 + (n-1)\\mu}$.\n\n**2. Derivation of the upper bound $B(s,\\mu)$ on the RIP constant $\\delta_s$**\n\nThe RIP constant $\\delta_s$ is the smallest number such that for any submatrix $A_S$ formed by $k \\le s$ columns of $A$, the eigenvalues of the sub-Gram matrix $G_S = A_S^{\\top}A_S$ lie in the interval $[1-\\delta_s, 1+\\delta_s]$.\n\nLet's apply the Gershgorin Circle Theorem to $G_S$. This is a $k \\times k$ matrix where $k \\le s$. Its diagonal entries are 1, and its off-diagonal entries are bounded in magnitude by $\\mu$. For any eigenvalue $\\lambda$ of $G_S$, there exists an index $i \\in S$ such that:\n$$ |\\lambda - 1| \\le \\sum_{j \\in S, j \\neq i} |(G_S)_{ij}| \\le \\sum_{j \\in S, j \\neq i} \\mu = (k-1)\\mu $$\nSince $k \\le s$, we have $(k-1)\\mu \\le (s-1)\\mu$. Therefore, for any $k \\le s$, all eigenvalues of $G_S$ must lie within the interval $[1 - (s-1)\\mu, 1 + (s-1)\\mu]$.\n\nThis means the inequalities defining the RIP are satisfied with $\\delta_s = (s-1)\\mu$. As we need an upper bound on the RIP constant for any matrix in this class, we can set:\n$$ B(s,\\mu) = (s-1)\\mu $$\nThis is a standard result connecting coherence and the RIP.\n\nThe two expressions for the final answer are $U(n,\\mu) = \\sqrt{1 + (n-1)\\mu}$ and $B(s,\\mu) = (s-1)\\mu$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\sqrt{1 + (n-1)\\mu} & (s-1)\\mu \\end{pmatrix}}\n$$", "id": "3479750"}]}