{"hands_on_practices": [{"introduction": "A cornerstone of convex geometry is the concept that a compact convex set is the convex hull of its extreme points. This first practice invites you to demonstrate this principle for the $\\ell_1$-ball, a central object in sparse optimization. By constructively representing any point within the ball as a combination of its vertices—the signed canonical basis vectors—you will gain a deeper intuition for its structure and touch upon the powerful implications of Carathéodory's theorem. [@problem_id:3440599]", "problem": "In the analysis of sparse recovery via convex relaxation, one frequently encounters the atomic norm defined by the set of atoms given by the signed canonical basis in dimension $n$, namely $\\mathcal{A} = \\{\\pm e_{i} : i \\in \\{1,\\dots,n\\}\\}$, where $e_{i}$ denotes the $i$-th canonical basis vector in $\\mathbb{R}^{n}$. The corresponding atomic norm ball is the $\\ell_1$ unit ball $B_{1} = \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{1} \\leq 1\\}$, which is a centrally symmetric convex polytope.\n\nStarting only from the definitions of convex combination, convex hull, and the $\\ell_1$ norm, carry out the following:\n\n1. Provide a constructive representation that, for any $x \\in B_{1}$, expresses $x$ as a convex combination of signed canonical basis vectors $v_{j} \\in \\mathcal{A}$ with nonnegative weights $\\lambda_{j}$ that sum to $1$, i.e., $x = \\sum_{j=1}^{m} \\lambda_{j} v_{j}$ with $\\lambda_{j} \\geq 0$ and $\\sum_{j=1}^{m} \\lambda_{j} = 1$. Your construction must give explicit weights in terms of $x$ and must be valid for all $x$ with $\\|x\\|_{1} \\leq 1$.\n\n2. Quantify, in terms of the ambient dimension $n$, the smallest integer $M(n)$ such that every point $x \\in B_{1}$ can be represented as above using no more than $M(n)$ signed canonical basis vectors. Provide your final answer for $M(n)$ as a closed-form analytic expression in $n$.\n\nThe final answer must be a single analytic expression. No rounding is required and no physical units are involved.", "solution": "The problem statement has been validated and is deemed valid. It is a well-posed mathematical problem in the field of convex geometry, specifically concerning the properties of the $\\ell_1$ unit ball in $\\mathbb{R}^n$. All terms are standard and the premises are factually correct within the mathematical context.\n\nThe problem is divided into two parts. First, we must provide a constructive method to express any point $x$ in the $\\ell_1$ unit ball, $B_1 = \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{1} \\leq 1\\}$, as a convex combination of the atoms $\\mathcal{A} = \\{\\pm e_{i} : i \\in \\{1,\\dots,n\\}\\}$. Second, we must determine the smallest integer $M(n)$ which is the maximum number of atoms required for any such representation.\n\n**Part 1: Constructive Representation**\n\nLet $x = (x_1, x_2, \\dots, x_n)$ be an arbitrary point in the $\\ell_1$ unit ball $B_1$. By definition, its $\\ell_1$ norm is $\\|x\\|_1 = \\sum_{i=1}^{n} |x_i| \\leq 1$. We seek to find coefficients $\\lambda_j \\ge 0$ and atoms $v_j \\in \\mathcal{A}$ such that $x = \\sum_j \\lambda_j v_j$ and $\\sum_j \\lambda_j = 1$.\n\nFor each component $x_i$ of the vector $x$, we define its positive and negative parts:\n$x_i^+ = \\max(x_i, 0)$\n$x_i^- = \\max(-x_i, 0)$\nThese definitions ensure that $x_i = x_i^+ - x_i^-$ and $|x_i| = x_i^+ + x_i^-$. Also, for any given $i$, at least one of $x_i^+$ or $x_i^-$ must be zero.\n\nWe can express the vector $x$ as a linear combination of the canonical basis vectors $e_i$:\n$$x = \\sum_{i=1}^{n} x_i e_i = \\sum_{i=1}^{n} (x_i^+ - x_i^-) e_i$$\nThis can be rewritten using the atoms from $\\mathcal{A}$ by distributing the sum:\n$$x = \\sum_{i=1}^{n} x_i^+ e_i + \\sum_{i=1}^{n} x_i^- (-e_i)$$\nThe vectors in this sum, $e_i$ and $-e_i$, are all members of the atomic set $\\mathcal{A}$. The coefficients, $x_i^+$ and $x_i^-$, are all non-negative by definition.\n\nLet's compute the sum of these coefficients:\n$$S_c = \\sum_{i=1}^{n} (x_i^+ + x_i^-) = \\sum_{i=1}^{n} |x_i| = \\|x\\|_1$$\nBy the problem definition, we have $S_c = \\|x\\|_1 \\le 1$.\n\nCase 1: $\\|x\\|_1 = 1$.\nIf $\\|x\\|_1 = 1$, the sum of the non-negative coefficients $x_i^+$ and $x_i^-$ is exactly $1$. In this case, the representation\n$$x = \\sum_{i=1}^{n} x_i^+ e_i + \\sum_{i=1}^{n} x_i^- (-e_i)$$\nis a valid convex combination of atoms from $\\mathcal{A}$. The weights are explicitly $\\lambda_{e_i} = x_i^+$ and $\\lambda_{-e_i} = x_i^-$.\n\nCase 2: $\\|x\\|_1  1$.\nIf $\\|x\\|_1 = S_c  1$, the sum of coefficients is less than $1$, so it is not a convex combination. To form a valid convex combination, the sum of weights must be $1$. We can achieve this by adding a term that sums to $x$ while ensuring the total weights sum to $1$. This is possible because the origin, $0 \\in \\mathbb{R}^n$, is in the convex hull of $\\mathcal{A}$. We can introduce the origin with a weight of $(1 - S_c)$:\n$$x = \\left(\\sum_{i=1}^{n} x_i^+ e_i + \\sum_{i=1}^{n} x_i^- (-e_i)\\right) + (1 - S_c) \\cdot 0$$\nThe coefficients are now $\\{x_i^+\\}_{i=1}^n$, $\\{x_i^-\\}_{i=1}^n$, and $(1-S_c)$. They are all non-negative and their sum is $S_c + (1-S_c) = 1$. This expresses $x$ as a convex combination of atoms from $\\mathcal{A}$ and the origin. To complete the construction, we must express the origin as a convex combination of atoms from $\\mathcal{A}$. A simple way is to use any atom and its negative, for instance $e_1$ and $-e_1$:\n$$0 = \\frac{1}{2} e_1 + \\frac{1}{2} (-e_1)$$\nSubstituting this into our expression for $x$:\n$$x = \\left(\\sum_{i=1}^{n} x_i^+ e_i + \\sum_{i=1}^{n} x_i^- (-e_i)\\right) + (1 - S_c) \\left(\\frac{1}{2} e_1 + \\frac{1}{2} (-e_1)\\right)$$\nWe can group the terms by the atoms:\n$$x = \\left(x_1^+ + \\frac{1 - S_c}{2}\\right) e_1 + \\left(x_1^- + \\frac{1 - S_c}{2}\\right) (-e_1) + \\sum_{i=2}^{n} x_i^+ e_i + \\sum_{i=2}^{n} x_i^- (-e_i)$$\nThis is the required constructive representation. The weights are explicitly given in terms of the components of $x$ and its $\\ell_1$ norm $S_c = \\|x\\|_1$.\nThe weights are:\n- For atom $e_1$: $\\lambda_{e_1} = x_1^+ + \\frac{1-\\|x\\|_1}{2}$\n- For atom $-e_1$: $\\lambda_{-e_1} = x_1^- + \\frac{1-\\|x\\|_1}{2}$\n- For atoms $e_i$, $i \\ge 2$: $\\lambda_{e_i} = x_i^+$\n- For atoms $-e_i$, $i \\ge 2$: $\\lambda_{-e_i} = x_i^-$\nAll other weights are zero. Since $\\|x\\|_1 \\le 1$ and $x_i^\\pm \\ge 0$, all these weights are non-negative. Their sum is $(x_1^+ + x_1^-) + (1-\\|x\\|_1) + \\sum_{i=2}^n(x_i^+ + x_i^-) = |x_1| + 1 - \\|x\\|_1 + \\sum_{i=2}^n |x_i| = \\sum_{i=1}^n |x_i| + 1 - \\|x\\|_1 = \\|x\\|_1 + 1 - \\|x\\|_1 = 1$. The construction is valid for all $x \\in B_1$.\n\n**Part 2: Quantification of M(n)**\n\nWe need to find the smallest integer $M(n)$ such that any point $x \\in B_1 = \\text{conv}(\\mathcal{A})$ can be written as a convex combination of at most $M(n)$ points from $\\mathcal{A}$. This number is known as the Carathéodory number for the set $\\mathcal{A}$.\n\nCarathéodory's theorem states that if a point lies in the convex hull of a set $P$ in an affine space of dimension $d$, it can be expressed as a convex combination of at most $d+1$ points from $P$. Here, our points $\\mathcal{A}$ are in $\\mathbb{R}^n$, which has dimension $n$. Therefore, any point $x \\in B_1$ can be represented as a convex combination of at most $n+1$ atoms from $\\mathcal{A}$. This provides an upper bound: $M(n) \\le n+1$.\n\nTo determine if this bound is tight, we must check if there exists a point in $B_1$ that requires exactly $n+1$ atoms for its representation. Let's construct such a point.\nConsider the point $x = (c, c, \\dots, c) \\in \\mathbb{R}^n$ for some constant $c  0$. For this point to be in $B_1$, we must have $\\|x\\|_1 = \\sum_{i=1}^n |c| = nc \\le 1$. Let's choose $c$ such that $0  nc  1$, for example, $c = \\frac{1}{2n}$. The point is $x = (\\frac{1}{2n}, \\frac{1}{2n}, \\dots, \\frac{1}{2n})$. Its $\\ell_1$ norm is $\\|x\\|_1 = n(\\frac{1}{2n}) = \\frac{1}{2}$, so it is in $B_1$.\n\nSuppose $x$ can be expressed as a convex combination of $m$ atoms, $\\{v_1, \\dots, v_m\\} \\subset \\mathcal{A}$:\n$$x = \\sum_{j=1}^{m} \\lambda_j v_j, \\quad \\text{with } \\lambda_j \\ge 0 \\text{ and } \\sum_{j=1}^{m} \\lambda_j = 1.$$\nLooking at the $k$-th component of this vector equation for any $k \\in \\{1, \\dots, n\\}$:\n$$x_k = \\frac{1}{2n} = \\sum_{j=1}^{m} \\lambda_j (v_j)_k$$\nwhere $(v_j)_k$ is the $k$-th component of the atom $v_j$. Since $\\lambda_j \\ge 0$ and the sum is positive, for each $k$, there must be at least one atom $v_j$ in the sum for which $(v_j)_k  0$. The only atom in $\\mathcal{A}$ with a positive $k$-th component is $e_k$. Thus, for each $k \\in \\{1, \\dots, n\\}$, the atom $e_k$ must be included in the set $\\{v_1, \\dots, v_m\\}$.\nThis implies that the set of atoms used must contain the set $\\{e_1, e_2, \\dots, e_n\\}$. Therefore, the number of atoms $m$ must be at least $n$.\n\nNow, let's test if $m=n$ is possible. If $m=n$, the set of atoms must be exactly $\\{e_1, e_2, \\dots, e_n\\}$. The convex combination representation would be:\n$$x = \\sum_{i=1}^{n} \\lambda_i e_i$$\nThis implies that for each $i$, $x_i = \\lambda_i$. So, $\\lambda_i = \\frac{1}{2n}$ for all $i=1, \\dots, n$.\nThe sum of these coefficients is:\n$$\\sum_{i=1}^{n} \\lambda_i = \\sum_{i=1}^{n} \\frac{1}{2n} = n \\cdot \\frac{1}{2n} = \\frac{1}{2}$$\nFor a convex combination, the sum of coefficients must be $1$. Since $\\frac{1}{2} \\ne 1$, the point $x=(\\frac{1}{2n}, \\dots, \\frac{1}{2n})$ cannot be represented as a convex combination of $n$ atoms.\n\nTherefore, this point requires more than $n$ atoms. Since Carathéodory's theorem guarantees a representation with at most $n+1$ atoms, the minimum number of atoms required for this particular point must be exactly $n+1$.\nSince we have found a point in $B_1$ that requires $n+1$ atoms, the smallest integer $M(n)$ that works for all points in $B_1$ cannot be smaller than $n+1$.\nThis establishes the lower bound $M(n) \\ge n+1$.\n\nCombining the upper bound from Carathéodory's theorem ($M(n) \\le n+1$) and the lower bound derived from our specific example ($M(n) \\ge n+1$), we conclude that $M(n)$ must be exactly $n+1$.", "answer": "$$\n\\boxed{n+1}\n$$", "id": "3440599"}, {"introduction": "Optimization over a convex set often involves finding points on its boundary that are 'most aligned' with a certain direction. This practice explores this idea by asking you to identify the exposed face of the $\\ell_1$-ball that maximizes a given linear functional. This exercise is key to understanding how optimization algorithms select specific features or sparse solutions, directly linking the geometry of convex polytopes to the outcomes of optimization procedures. [@problem_id:3440587]", "problem": "Consider the $\\ell_1$ unit ball $B_{1} \\subset \\mathbb{R}^{n}$ defined by $B_{1} = \\{ y \\in \\mathbb{R}^{n} : \\|y\\|_{1} \\leq 1 \\}$ and a linear functional $f(y) = \\langle y, c \\rangle$ for a fixed vector $c \\in \\mathbb{R}^{n}$. In compressed sensing and sparse optimization, the $\\ell_1$ norm plays a central role in promoting sparsity, as in Basis Pursuit (BP), and the geometry of $B_{1}$ determines which sparse patterns are favored by linear objectives. An exposed face of a convex set $C$ with respect to a functional $f$ is defined as $F = \\{ x \\in C : f(x) = \\sup_{z \\in C} f(z) \\}$.\n\nStarting from core definitions of norms, dual norms, and exposed faces in convex analysis, derive the supremum value $\\sup_{y \\in B_{1}} \\langle y, c \\rangle$ and characterize the exposed face $F_{c} = \\{ y \\in B_{1} : \\langle y, c \\rangle = \\sup_{z \\in B_{1}} \\langle z, c \\rangle \\}$ in terms of the index set where $|c_{i}|$ attains its maximum and the corresponding sign pattern. Then, specialize to the concrete case with $n = 7$ and\n$$\nc = (2,\\,-2,\\,2,\\,-1,\\,0,\\,\\tfrac{1}{2},\\,-2).\n$$\nDetermine the dimension of the exposed face $F_{c}$ of $B_{1}$ induced by $f(y) = \\langle y, c \\rangle$ for this specific $c$. Provide your final answer as a single integer.", "solution": "The problem is well-posed and consistent with standard definitions in convex analysis and optimization. We may proceed with the solution.\n\nThe problem asks for the dimension of an exposed face of the $\\ell_1$ unit ball $B_1 = \\{ y \\in \\mathbb{R}^{n} : \\|y\\|_{1} \\leq 1 \\}$, where $\\|y\\|_1 = \\sum_{i=1}^n |y_i|$. The face is induced by the linear functional $f(y) = \\langle y, c \\rangle$ for a given vector $c \\in \\mathbb{R}^n$. The exposed face $F_c$ is defined as the set of points in $B_1$ where the functional attains its maximum value.\n$$F_{c} = \\{ y \\in B_{1} : \\langle y, c \\rangle = \\sup_{z \\in B_{1}} \\langle z, c \\rangle \\}$$\n\nFirst, we must determine the supremum value $\\sup_{z \\in B_{1}} \\langle z, c \\rangle$. This value is given by the dual norm of the $\\ell_1$ norm, which is the $\\ell_\\infty$ norm, evaluated at $c$. The $\\ell_\\infty$ norm is defined as $\\|c\\|_\\infty = \\max_{1 \\le i \\le n} |c_i|$. We can prove this directly.\n\nFor any $y \\in B_1$, the inner product $\\langle y, c \\rangle$ can be written as $\\sum_{i=1}^n y_i c_i$. We can bound this term:\n$$ \\langle y, c \\rangle = \\sum_{i=1}^n y_i c_i \\le \\sum_{i=1}^n |y_i c_i| = \\sum_{i=1}^n |y_i| |c_i| $$\nSince $|c_i| \\le \\|c\\|_\\infty$ for all $i$, we have:\n$$ \\sum_{i=1}^n |y_i| |c_i| \\le \\sum_{i=1}^n |y_i| \\|c\\|_\\infty = \\|c\\|_\\infty \\sum_{i=1}^n |y_i| = \\|c\\|_\\infty \\|y\\|_1 $$\nAs $y \\in B_1$, we know $\\|y\\|_1 \\le 1$. Therefore, we have the upper bound:\n$$ \\langle y, c \\rangle \\le \\|c\\|_\\infty \\|y\\|_1 \\le \\|c\\|_\\infty $$\nThis shows that $\\sup_{y \\in B_{1}} \\langle y, c \\rangle \\le \\|c\\|_\\infty$. To show that this supremum is indeed $\\|c\\|_\\infty$, we must demonstrate that this bound is attained for some $y \\in B_1$. Let $k$ be an index for which $|c_k| = \\|c\\|_\\infty$. Consider the vector $y^* \\in \\mathbb{R}^n$ with components defined as:\n$$ y^*_i = \\begin{cases} \\operatorname{sign}(c_k)  \\text{if } i=k \\\\ 0  \\text{if } i \\neq k \\end{cases} $$\nwhere $\\operatorname{sign}(x)$ is $1$ if $x0$, $-1$ if $x0$, and can be taken as $0$ if $x=0$. If $\\|c\\|_\\infty = 0$, then $c=0$ and the problem is trivial. We assume $c \\neq 0$, so $\\|c\\|_\\infty  0$ and $c_k \\neq 0$.\nThe $\\ell_1$ norm of this vector is $\\|y^*\\|_1 = |\\operatorname{sign}(c_k)| = 1$, so $y^* \\in B_1$.\nThe value of the functional at $y^*$ is:\n$$ \\langle y^*, c \\rangle = \\sum_{i=1}^n y^*_i c_i = y^*_k c_k = \\operatorname{sign}(c_k) c_k = |c_k| = \\|c\\|_\\infty $$\nThus, the supremum is attained, and $\\sup_{z \\in B_{1}} \\langle z, c \\rangle = \\|c\\|_\\infty$.\n\nNow, we characterize the exposed face $F_c = \\{ y \\in B_1 : \\langle y, c \\rangle = \\|c\\|_\\infty \\}$. The equality $\\langle y, c \\rangle = \\|c\\|_\\infty$ holds if and only if all inequalities in the derivation above are equalities.\nLet's analyze the conditions for equality:\n1.  $\\|c\\|_\\infty \\|y\\|_1 = \\|c\\|_\\infty$: Since we assume $c \\neq 0$, this requires $\\|y\\|_1 = 1$.\n2.  $\\sum_{i=1}^n |y_i| |c_i| = \\|c\\|_\\infty \\|y\\|_1$: With $\\|y\\|_1=1$, this becomes $\\sum_{i=1}^n |y_i| |c_i| = \\|c\\|_\\infty$. This can be rewritten as $\\sum_{i=1}^n |y_i| |c_i| = \\|c\\|_\\infty \\sum_{i=1}^n |y_i|$, or $\\sum_{i=1}^n |y_i| (\\|c\\|_\\infty - |c_i|) = 0$. Since each term $|y_i| (\\|c\\|_\\infty - |c_i|)$ is non-negative, the sum is zero if and only if each term is zero. This implies that if $|c_i|  \\|c\\|_\\infty$, then $|y_i|$ must be $0$. Let $I_{\\max} = \\{i : |c_i| = \\|c\\|_\\infty \\}$. This condition states that $y_i = 0$ for all $i \\notin I_{\\max}$.\n3.  $\\sum_{i=1}^n y_i c_i = \\sum_{i=1}^n |y_i||c_i|$: This equality holds if and only if $y_i c_i \\ge 0$ for all $i$. For $i \\in I_{\\max}$, this means $y_i$ must have the same sign as $c_i$ (or be zero). That is, $\\operatorname{sign}(y_i) = \\operatorname{sign}(c_i)$ if $y_i \\ne 0$.\n\nCombining these conditions, a vector $y$ lies in the face $F_c$ if and only if:\na) $y_i = 0$ for all $i \\notin I_{\\max}$.\nb) For all $i \\in I_{\\max}$, $y_i$ has the same sign as $c_i$ (i.e., $y_i c_i \\ge 0$).\nc) $\\sum_{i \\in I_{\\max}} |y_i| = 1$.\n\nLet $k = |I_{\\max}|$. The vectors in $F_c$ are of the form $y = \\sum_{i \\in I_{\\max}} y_i e_i$, where $e_i$ is the $i$-th standard basis vector. Let $\\alpha_i = |y_i|$ for $i \\in I_{\\max}$. The conditions become $\\alpha_i \\ge 0$ and $\\sum_{i \\in I_{\\max}} \\alpha_i = 1$. The vector $y$ can be written as $y = \\sum_{i \\in I_{\\max}} \\alpha_i (\\operatorname{sign}(c_i) e_i)$.\nThis shows that $F_c$ is the convex hull of the $k$ vertices $\\{ \\operatorname{sign}(c_i)e_i \\}_{i \\in I_{\\max}}$. These $k$ vectors are vertices of the $\\ell_1$ ball. They are linearly independent since they involve different standard basis vectors. Therefore, they are also affinely independent. The dimension of the convex hull of $k$ affinely independent points is $k-1$.\nThus, the dimension of the exposed face $F_c$ is $\\dim(F_c) = |I_{\\max}| - 1$.\n\nWe are given the specific case $n=7$ and $c = (2, -2, 2, -1, 0, \\frac{1}{2}, -2)$.\nFirst, we find the $\\ell_\\infty$ norm of $c$. We compute the absolute values of its components:\n$|c_1|=2$, $|c_2|=2$, $|c_3|=2$, $|c_4|=1$, $|c_5|=0$, $|c_6|=\\frac{1}{2}$, $|c_7|=2$.\nThe maximum of these values is $\\|c\\|_\\infty = 2$.\n\nNext, we identify the index set $I_{\\max}$ where the absolute value of the component equals $\\|c\\|_\\infty$:\n$$I_{\\max} = \\{ i \\in \\{1, \\dots, 7\\} : |c_i| = 2 \\}$$\nBy inspection, the indices are $1$, $2$, $3$, and $7$.\nSo, $I_{\\max} = \\{1, 2, 3, 7\\}$.\n\nThe number of elements in this set is $|I_{\\max}| = 4$.\nThe dimension of the exposed face $F_c$ is given by $|I_{\\max}| - 1$.\n$$\\dim(F_c) = 4 - 1 = 3$$\nThe face $F_c$ is the convex hull of the four vertices $\\{e_1, -e_2, e_3, -e_7\\}$. A point $y$ on this face has $y_4=y_5=y_6=0$, with components $y_1, y_2, y_3, y_7$ satisfying $y_1 \\ge 0$, $y_2 \\le 0$, $y_3 \\ge 0$, $y_7 \\le 0$ and $y_1 - y_2 + y_3 - y_7 = 1$. This set is a $3$-dimensional simplex.", "answer": "$$\\boxed{3}$$", "id": "3440587"}, {"introduction": "To understand the conditions for optimality at the boundary of a convex set, one must characterize the directions that 'point away' from the set. This concept is formalized by the normal cone, a central tool in convex analysis and optimization. In this practice, you will compute the normal cone to the $\\ell_1$-ball by leveraging its connection to the subdifferential of the $\\ell_1$-norm, providing a concrete example of how to derive the first-order optimality conditions for constrained problems. [@problem_id:3440610]", "problem": "Consider the unit $\\ell_1$ ball in $\\mathbb{R}^{n}$, defined as $B_{1} := \\{y \\in \\mathbb{R}^{n} : \\|y\\|_{1} \\leq 1\\}$. In compressed sensing and sparse optimization, the unit $\\ell_1$ ball arises as a convex feasible set used to promote sparsity. Let $x \\in \\mathbb{R}^{n}$ be a boundary point of $B_{1}$, so that $\\|x\\|_{1} = 1$. Denote by $S := \\{i \\in \\{1,\\dots,n\\} : x_{i} \\neq 0\\}$ the support of $x$, and let $s \\in \\mathbb{R}^{n}$ be the sign vector of $x$ defined by $s_{i} := \\operatorname{sign}(x_{i})$ for all $i \\in \\{1,\\dots,n\\}$, with the convention that $s_{i}$ is only specified for $i \\in S$. Using only fundamental convex analysis definitions, compute the normal cone $N_{B_{1}}(x)$ to $B_{1}$ at $x$ in terms of $S$ and $s$ via the subdifferential of the $\\ell_1$ norm.\n\nYour derivation must start from foundational definitions, including: the definition of a convex set, the definition of the normal cone $N_{C}(x) := \\{u \\in \\mathbb{R}^{n} : \\langle u, y - x \\rangle \\leq 0 \\text{ for all } y \\in C\\}$, and the definition of the subdifferential $\\partial f(x) := \\{g \\in \\mathbb{R}^{n} : f(y) \\geq f(x) + \\langle g, y - x \\rangle \\text{ for all } y \\in \\mathbb{R}^{n}\\}$ for a convex function $f$. You may also use the fact that the unit $\\ell_1$ ball is the sublevel set of the convex function $f(y) = \\|y\\|_{1}$.\n\nExpress your final answer as a single closed-form set expression that depends only on $S$ and $s$ and characterizes $N_{B_{1}}(x)$. Do not include any derivation or justification in the final answer. No rounding is required, and there are no physical units involved.", "solution": "The problem asks for the computation of the normal cone $N_{B_{1}}(x)$ to the unit $\\ell_1$ ball $B_{1} := \\{y \\in \\mathbb{R}^{n} : \\|y\\|_{1} \\leq 1\\}$ at a boundary point $x$, where $\\|x\\|_{1} = 1$. The derivation must be based on fundamental definitions from convex analysis, particularly the relationship between the normal cone and the subdifferential of the norm function.\n\nLet the function $f: \\mathbb{R}^{n} \\to \\mathbb{R}$ be the $\\ell_1$ norm, defined as $f(y) = \\|y\\|_{1} = \\sum_{i=1}^{n} |y_{i}|$. The set $B_{1}$ is the $1$-sublevel set of this function, i.e., $B_{1} = \\{y \\in \\mathbb{R}^{n} : f(y) \\leq 1\\}$. The function $f(y) = \\|y\\|_{1}$ is convex, being a sum of convex functions $|y_i|$, and therefore $B_{1}$ is a convex set.\n\nThe normal cone to a convex set $C$ at a point $x \\in C$ is defined as:\n$$N_{C}(x) := \\{u \\in \\mathbb{R}^{n} : \\langle u, y - x \\rangle \\leq 0 \\text{ for all } y \\in C\\}$$\nFor a convex set defined as a sublevel set $C = \\{y : f(y) \\leq c\\}$ where $f$ is a convex function, and for a point $x$ on the boundary where $f(x) = c$, there is a fundamental relationship between the normal cone $N_{C}(x)$ and the subdifferential of $f$ at $x$, denoted $\\partial f(x)$. The subdifferential is defined as:\n$$\\partial f(x) := \\{g \\in \\mathbb{R}^{n} : f(y) \\geq f(x) + \\langle g, y - x \\rangle \\text{ for all } y \\in \\mathbb{R}^{n}\\}$$\nThe relationship states that the normal cone is the conic hull of the subdifferential:\n$$N_{C}(x) = \\text{cone}(\\partial f(x)) = \\{\\lambda g : \\lambda \\geq 0, g \\in \\partial f(x)\\}$$\nThis result holds under certain regularity conditions, known as constraint qualifications. A standard one is Slater's condition, which requires the existence of a point $y_{0}$ such that $f(y_{0})  c$. In our case, $c=1$, and we can choose $y_{0} = 0 \\in \\mathbb{R}^{n}$, for which $f(0) = \\|0\\|_{1} = 0  1$. Thus, Slater's condition is satisfied, and we can validly use this formula. Our task therefore reduces to computing the subdifferential $\\partial f(x)$ for $f(x) = \\|x\\|_{1}$.\n\nThe $\\ell_1$ norm is a separable function, $f(x) = \\sum_{i=1}^{n} |x_{i}|$. The subdifferential of a sum of convex functions is the Minkowski sum of their individual subdifferentials. In this separable case, it is the Cartesian product of the subdifferentials of the component functions:\n$$\\partial f(x) = \\partial(|x_{1}|) \\times \\partial(|x_{2}|) \\times \\dots \\times \\partial(|x_{n}|)$$\nWe need to find the subdifferential of the one-dimensional absolute value function $h(t) = |t|$.\nFor $t \\neq 0$, $h(t)$ is differentiable, and its derivative is $h'(t) = \\operatorname{sign}(t)$. In this case, the subdifferential contains only the derivative: $\\partial h(t) = \\{\\operatorname{sign}(t)\\}$.\nFor $t = 0$, $h(t)$ is not differentiable. A subgradient $g$ must satisfy $|y| \\geq |0| + g(y - 0)$ for all $y \\in \\mathbb{R}$, which simplifies to $|y| \\geq gy$.\nIf we take $y=1$, we get $1 \\geq g$.\nIf we take $y=-1$, we get $1 \\geq -g$, which implies $g \\geq -1$.\nThus, any subgradient $g$ must satisfy $-1 \\leq g \\leq 1$. Conversely, any $g \\in [-1, 1]$ satisfies $|y| \\geq gy$ because $|gy| = |g||y| \\leq |y|$. So, the subdifferential at $t=0$ is the entire interval $\\partial h(0) = [-1, 1]$.\n\nCombining these results, the subdifferential of the absolute value function is:\n$$\\partial |t| = \\begin{cases} \\{\\operatorname{sign}(t)\\}  \\text{if } t \\neq 0 \\\\ [-1, 1]  \\text{if } t = 0 \\end{cases}$$\nWe can now assemble the subdifferential $\\partial \\|x\\|_{1}$. Let $S := \\{i \\in \\{1,\\dots,n\\} : x_{i} \\neq 0\\}$ be the support of $x$ and $s_{i} := \\operatorname{sign}(x_{i})$ for $i \\in S$. A vector $g \\in \\mathbb{R}^{n}$ is in $\\partial \\|x\\|_{1}$ if and only if its components $g_{i}$ satisfy:\n-   $g_{i} \\in \\partial|x_{i}| = \\{s_{i}\\}$ for $i \\in S$ (since $x_{i} \\neq 0$).\n-   $g_{i} \\in \\partial|x_{i}| = [-1, 1]$ for $i \\notin S$ (since $x_{i} = 0$).\n\nTherefore, the subdifferential of the $\\ell_1$ norm at $x$ is the set:\n$$\\partial \\|x\\|_{1} = \\{g \\in \\mathbb{R}^{n} : g_{i} = s_{i} \\text{ for } i \\in S, \\text{ and } g_{i} \\in [-1, 1] \\text{ for } i \\notin S\\}$$\nFinally, we compute the normal cone $N_{B_{1}}(x)$ by taking the conic hull of this set:\n$$N_{B_{1}}(x) = \\{\\lambda g : \\lambda \\geq 0, g \\in \\partial \\|x\\|_{1}\\}$$\nLet $u \\in N_{B_{1}}(x)$. Then $u = \\lambda g$ for some $\\lambda \\geq 0$ and $g \\in \\partial \\|x\\|_{1}$. Let's analyze the components of $u$:\n-   For $i \\in S$: $u_{i} = \\lambda g_{i} = \\lambda s_{i}$.\n-   For $i \\notin S$: $u_{i} = \\lambda g_{i}$, where $g_{i} \\in [-1, 1]$. This implies $|u_{i}| = |\\lambda g_{i}| = \\lambda |g_{i}| \\leq \\lambda$.\n\nThese two conditions must hold for some $\\lambda \\geq 0$. We can express this more explicitly. Since $x$ is on the boundary of $B_{1}$, $\\|x\\|_{1}=1$, which implies $x$ is not the zero vector, so the support set $S$ is non-empty. For any $j \\in S$, we have $s_{j} \\in \\{-1, 1\\}$, so $s_{j}^{2}=1$. From $u_{j} = \\lambda s_{j}$, we can express $\\lambda$ as $\\lambda = s_{j} u_{j}$. Since $\\lambda \\geq 0$, this means $s_{j} u_{j} \\geq 0$.\nFurthermore, for any other index $i \\in S$, we must have $u_{i} = \\lambda s_{i} = (s_{j} u_{j}) s_{i}$, which implies $s_{i} u_{i} = s_{j} u_{j}$. This shows that the quantity $s_{i} u_{i}$ must be a constant, non-negative value for all $i \\in S$. Let's call this constant $\\lambda_0$.\nSo, the conditions on $u$ are:\n1.  There exists a constant $\\lambda_0 \\geq 0$ such that $u_{i} = \\lambda_0 s_{i}$ for all $i \\in S$.\n2.  For all $i \\notin S$, $|u_i| \\leq \\lambda_0$.\n\nWe can combine these into a single set expression for the normal cone. A vector $u \\in \\mathbb{R}^{n}$ belongs to $N_{B_{1}}(x)$ if and only if there exists a single non-negative scalar $\\lambda_0$ that relates all its components as described above.\n\nThis yields the final characterization of the normal cone $N_{B_{1}}(x)$ in terms of the support $S$ and the sign vector $s$ of the point $x$.", "answer": "$$\n\\boxed{\n\\{u \\in \\mathbb{R}^{n} \\mid \\exists \\lambda \\geq 0 \\text{ such that } u_{i} = \\lambda s_{i} \\text{ for all } i \\in S \\text{ and } |u_{i}| \\leq \\lambda \\text{ for all } i \\notin S\\}\n}\n$$", "id": "3440610"}]}