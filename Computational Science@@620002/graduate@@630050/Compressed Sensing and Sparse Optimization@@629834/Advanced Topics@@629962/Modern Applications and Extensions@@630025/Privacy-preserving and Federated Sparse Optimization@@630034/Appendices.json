{"hands_on_practices": [{"introduction": "In federated learning with local differential privacy, the central server never sees true client data, only noisy versions. This first practice is a foundational exercise to understand the statistical consequences of this model [@problem_id:3468479]. By deriving the properties of the aggregated noisy gradients, we can quantify how averaging over many clients helps to mitigate the variance introduced by the privacy mechanism.", "problem": "Consider a federated sparse optimization setting in compressed sensing and sparse optimization with $m$ clients. Each client $i \\in \\{1,\\dots,m\\}$ holds a $d$-dimensional gradient vector $g_{i} \\in \\mathbb{R}^{d}$ that is $k$-sparse, meaning it has at most $k$ nonzero coordinates, with $1 \\leq k \\leq d$. To ensure local privacy, each client reports a locally perturbed gradient $\\tilde{g}_{i} = g_{i} + \\eta_{i}$ to the server, where $\\eta_{i} \\in \\mathbb{R}^{d}$ is an additive noise vector drawn independently across clients and coordinates, with distribution $\\mathcal{N}(0, s^{2} I_{d})$ (zero-mean Gaussian with covariance $s^{2} I_{d}$). Assume the noise scale $s > 0$ is known to the server and is chosen according to Differential Privacy (DP) specifications, but the server has no access to the true $g_{i}$.\n\nThe server seeks to estimate the true aggregate average gradient $\\bar{g} = \\frac{1}{m} \\sum_{i=1}^{m} g_{i}$ from the received perturbed reports $\\{\\tilde{g}_{i}\\}_{i=1}^{m}$. Starting from fundamental definitions of expectation and variance, and the well-tested properties that (i) expectation is linear and (ii) the variance of a sum of independent random variables equals the sum of their variances, derive an unbiased estimator of $\\bar{g}$ expressed solely in terms of $\\{\\tilde{g}_{i}\\}_{i=1}^{m}$ and compute the per-coordinate variance of this estimator as a closed-form function of the local noise scale $s$ and the client count $m$.\n\nExpress the final answer as closed-form analytic expressions. No rounding is required. No physical units are involved. If you provide multiple final expressions, present them together as a single row matrix.", "solution": "The problem statement is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n-   Number of clients: $m$\n-   Client index: $i \\in \\{1, \\dots, m\\}$\n-   True gradient for client $i$: $g_i \\in \\mathbb{R}^d$\n-   Sparsity of $g_i$: $k$-sparse, at most $k$ nonzero coordinates, with $1 \\leq k \\leq d$\n-   Perturbed gradient reported by client $i$: $\\tilde{g}_i = g_i + \\eta_i$\n-   Noise vector for client $i$: $\\eta_i \\in \\mathbb{R}^d$\n-   Noise distribution: $\\eta_i$ are drawn independently across clients and coordinates, with $\\eta_i \\sim \\mathcal{N}(0, s^2 I_d)$, where $I_d$ is the $d \\times d$ identity matrix. This implies $\\mathbb{E}[\\eta_i] = 0$ and the covariance matrix is $\\text{Cov}(\\eta_i) = s^2 I_d$. The components $(\\eta_i)_j$ for $j \\in \\{1, \\dots, d\\}$ are i.i.d. $\\mathcal{N}(0, s^2)$.\n-   Noise scale: $s > 0$\n-   Server's goal: Estimate the true aggregate average gradient $\\bar{g} = \\frac{1}{m} \\sum_{i=1}^{m} g_i$.\n-   Task 1: Derive an unbiased estimator of $\\bar{g}$ using only $\\{\\tilde{g}_i\\}_{i=1}^{m}$.\n-   Task 2: Compute the per-coordinate variance of this estimator as a function of $s$ and $m$.\n-   Constraints on derivation: Must use fundamental definitions of expectation and variance, linearity of expectation, and the variance of a sum of independent random variables.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It describes a canonical scenario in federated learning with local differential privacy implemented via the Gaussian mechanism. The variables, distributions, and objectives are specified with mathematical precision. The problem does not violate any scientific principles, is not based on false premises, and contains no ambiguities or contradictions. The mention of $k$-sparsity sets a specific context within sparse optimization, but the questions asked pertain to fundamental statistical properties of the noisy average, which can be derived independently of the sparsity property. The problem is self-contained and a unique solution can be derived from the provided information and principles.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\n### Derivation of the Solution\n\nThe server receives the set of perturbed gradients $\\{\\tilde{g}_i\\}_{i=1}^{m}$. A natural estimator for the true average gradient $\\bar{g} = \\frac{1}{m} \\sum_{i=1}^{m} g_i$ is the average of these received perturbed gradients. Let us define this estimator as $\\hat{g}$.\n\n**Part 1: Derivation of an Unbiased Estimator**\n\nLet the proposed estimator be $\\hat{g} = \\frac{1}{m} \\sum_{i=1}^{m} \\tilde{g}_i$. To verify that this estimator is unbiased, we must show that its expected value, $\\mathbb{E}[\\hat{g}]$, is equal to the true quantity being estimated, $\\bar{g}$. The expectation is taken over the distribution of the noise vectors $\\{\\eta_i\\}$. The true gradients $\\{g_i\\}$ are treated as fixed, deterministic quantities.\n\nWe begin by applying the linearity of expectation to the expression for $\\hat{g}$:\n$$\n\\mathbb{E}[\\hat{g}] = \\mathbb{E}\\left[ \\frac{1}{m} \\sum_{i=1}^{m} \\tilde{g}_i \\right] = \\frac{1}{m} \\sum_{i=1}^{m} \\mathbb{E}[\\tilde{g}_i]\n$$\nNext, we compute the expectation of a single perturbed gradient $\\tilde{g}_i = g_i + \\eta_i$. Again, using the linearity of expectation:\n$$\n\\mathbb{E}[\\tilde{g}_i] = \\mathbb{E}[g_i + \\eta_i] = \\mathbb{E}[g_i] + \\mathbb{E}[\\eta_i]\n$$\nThe term $g_i$ is a deterministic vector, so its expectation is the vector itself: $\\mathbb{E}[g_i] = g_i$. The noise vector $\\eta_i$ is drawn from a zero-mean Gaussian distribution, $\\mathcal{N}(0, s^2 I_d)$, so its expectation is the zero vector: $\\mathbb{E}[\\eta_i] = 0$.\nSubstituting these into the expression for $\\mathbb{E}[\\tilde{g}_i]$ gives:\n$$\n\\mathbb{E}[\\tilde{g}_i] = g_i + 0 = g_i\n$$\nThis shows that each individual perturbed gradient $\\tilde{g}_i$ is an unbiased estimator of the corresponding true gradient $g_i$.\n\nNow, we substitute this result back into the expression for $\\mathbb{E}[\\hat{g}]$:\n$$\n\\mathbb{E}[\\hat{g}] = \\frac{1}{m} \\sum_{i=1}^{m} g_i\n$$\nBy definition provided in the problem statement, the true aggregate average gradient is $\\bar{g} = \\frac{1}{m} \\sum_{i=1}^{m} g_i$. Therefore, we have shown:\n$$\n\\mathbb{E}[\\hat{g}] = \\bar{g}\n$$\nThis confirms that the estimator $\\hat{g} = \\frac{1}{m} \\sum_{i=1}^{m} \\tilde{g}_i$ is an unbiased estimator of $\\bar{g}$. This is the first part of our required answer.\n\n**Part 2: Computation of the Per-Coordinate Variance**\n\nWe are asked to find the per-coordinate variance of the estimator $\\hat{g}$. This corresponds to calculating $\\text{Var}(\\hat{g}_j)$ for any coordinate $j \\in \\{1, \\dots, d\\}$, where $\\hat{g}_j$ is the $j$-th component of the vector $\\hat{g}$.\n\nThe $j$-th component of the estimator is:\n$$\n\\hat{g}_j = \\left( \\frac{1}{m} \\sum_{i=1}^{m} \\tilde{g}_i \\right)_j = \\frac{1}{m} \\sum_{i=1}^{m} (\\tilde{g}_i)_j\n$$\nThe variance of $\\hat{g}_j$ is given by:\n$$\n\\text{Var}(\\hat{g}_j) = \\text{Var}\\left( \\frac{1}{m} \\sum_{i=1}^{m} (\\tilde{g}_i)_j \\right)\n$$\nUsing the variance property $\\text{Var}(c X) = c^2 \\text{Var}(X)$ for a constant $c$:\n$$\n\\text{Var}(\\hat{g}_j) = \\left(\\frac{1}{m}\\right)^2 \\text{Var}\\left( \\sum_{i=1}^{m} (\\tilde{g}_i)_j \\right) = \\frac{1}{m^2} \\text{Var}\\left( \\sum_{i=1}^{m} (\\tilde{g}_i)_j \\right)\n$$\nThe random variables in the sum are $(\\tilde{g}_1)_j, (\\tilde{g}_2)_j, \\dots, (\\tilde{g}_m)_j$. The noise vectors $\\eta_i$ are independent across clients $i$. Since $\\tilde{g}_i = g_i + \\eta_i$, the randomness in $\\tilde{g}_i$ comes solely from $\\eta_i$. Thus, the random variables $\\tilde{g}_i$ are independent for different clients $i$. This implies their components, $(\\tilde{g}_i)_j$, are also independent across $i$.\n\nAccording to the property that the variance of a sum of independent random variables is the sum of their variances:\n$$\n\\text{Var}\\left( \\sum_{i=1}^{m} (\\tilde{g}_i)_j \\right) = \\sum_{i=1}^{m} \\text{Var}\\left( (\\tilde{g}_i)_j \\right)\n$$\nNow we must compute the variance of a single component, $\\text{Var}((\\tilde{g}_i)_j)$. We have $(\\tilde{g}_i)_j = (g_i)_j + (\\eta_i)_j$. Since $(g_i)_j$ is a constant, it does not contribute to the variance (i.e., $\\text{Var}(X+c) = \\text{Var}(X)$).\n$$\n\\text{Var}\\left( (\\tilde{g}_i)_j \\right) = \\text{Var}\\left( (g_i)_j + (\\eta_i)_j \\right) = \\text{Var}\\left( (\\eta_i)_j \\right)\n$$\nThe noise vector $\\eta_i$ is drawn from $\\mathcal{N}(0, s^2 I_d)$, meaning each of its components $(\\eta_i)_j$ is an independent random variable drawn from a one-dimensional Gaussian distribution $\\mathcal{N}(0, s^2)$. The variance of this distribution is by definition $s^2$.\n$$\n\\text{Var}\\left( (\\eta_i)_j \\right) = s^2\n$$\nThis holds for any client $i$ and any coordinate $j$. Substituting this back into the sum of variances:\n$$\n\\sum_{i=1}^{m} \\text{Var}\\left( (\\tilde{g}_i)_j \\right) = \\sum_{i=1}^{m} s^2 = m s^2\n$$\nFinally, substituting this result into our expression for $\\text{Var}(\\hat{g}_j)$:\n$$\n\\text{Var}(\\hat{g}_j) = \\frac{1}{m^2} (m s^2) = \\frac{s^2}{m}\n$$\nThis is the per-coordinate variance of the estimator $\\hat{g}$. It is a function of the noise scale $s$ and the number of clients $m$, as required.\n\nThe final answer consists of the expression for the unbiased estimator and its per-coordinate variance.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{m} \\sum_{i=1}^{m} \\tilde{g}_{i} & \\frac{s^2}{m} \\end{pmatrix}}\n$$", "id": "3468479"}, {"introduction": "Beyond affecting the final model, privacy-preserving noise can impact the runtime dynamics of an optimization algorithm. This exercise explores a crucial practical challenge: monitoring convergence without violating privacy [@problem_id:3468480]. We will analyze how noisy reports of a convergence metric can cause an algorithm to terminate prematurely and derive the probability of this undesirable event.", "problem": "Consider a federated convex sparse optimization problem (for example, an $\\ell_{1}$-regularized least-squares objective) solved collaboratively by $m$ clients. At a given iteration, client $i \\in \\{1,\\dots,m\\}$ holds a primal-dual pair that induces a local duality gap $g_{i} \\geq 0$, where the duality gap is the difference between primal and dual objective values evaluated at the current primal-dual pair. A central coordinator monitors convergence via a stopping rule based on client-reported duality gaps.\n\nTo protect client data, each client applies Differential Privacy (DP) using the Laplace mechanism. Specifically, each client $i$ releases a privatized gap $\\tilde{g}_{i} = g_{i} + Z_{i}$, where $Z_{i}$ are independent and identically distributed Laplace random variables with mean $0$ and scale $b = \\Delta_{g} / \\epsilon$. Here, $\\Delta_{g} > 0$ is a known global upper bound on the $\\ell_{1}$-sensitivity of the duality gap with respect to a change in a single data record at a client, and $\\epsilon > 0$ is the privacy budget. The Laplace mechanism with scale $b = \\Delta_{g} / \\epsilon$ guarantees $\\epsilon$-DP for releasing each $g_{i}$.\n\nThe coordinator uses the stopping rule that terminates the algorithm at this iteration if and only if $\\min_{1 \\leq i \\leq m} \\tilde{g}_{i} \\leq \\tau$, where $\\tau > 0$ is a fixed tolerance. Assume that the true state is strictly nonconverged in the sense that $g_{i} > \\tau$ for all $i \\in \\{1,\\dots,m\\}$.\n\nStarting from the fundamental definitions of Differential Privacy and the distribution of the Laplace mechanism, derive a closed-form expression for the probability that the coordinator terminates prematurely at this iteration due solely to the DP noise. Your final expression must be a single analytic expression in terms of $\\{g_{i}\\}_{i=1}^{m}$, $\\tau$, $\\epsilon$, and $\\Delta_{g}$.\n\nState your final answer as a single closed-form analytic expression. No numerical approximation is required, and no units are involved.", "solution": "The problem statement is first validated for scientific soundness, self-consistency, and clarity.\n\n### Step 1: Extract Givens\n-   Number of clients: $m \\in \\mathbb{Z}^{+}$.\n-   Local duality gap for client $i$: $g_{i} \\geq 0$ for $i \\in \\{1, \\dots, m\\}$.\n-   Privatization mechanism: Additive noise from the Laplace distribution.\n-   Privatized duality gap for client $i$: $\\tilde{g}_{i} = g_{i} + Z_{i}$.\n-   Noise distribution: $Z_{i}$ are independent and identically distributed (i.i.d.) random variables, with $Z_{i} \\sim \\text{Laplace}(0, b)$.\n-   Laplace scale parameter: $b = \\Delta_{g} / \\epsilon$.\n-   Sensitivity: $\\Delta_{g} > 0$.\n-   Privacy budget: $\\epsilon > 0$.\n-   Termination condition (stopping rule): $\\min_{1 \\leq i \\leq m} \\tilde{g}_{i} \\leq \\tau$.\n-   Termination tolerance: $\\tau > 0$.\n-   Assumption on true state: $g_{i} > \\tau$ for all $i \\in \\{1, \\dots, m\\}$.\n-   Objective: Derive a closed-form expression for the probability of premature termination, $P(\\min_{1 \\leq i \\leq m} \\tilde{g}_{i} \\leq \\tau)$, in terms of $\\{g_{i}\\}_{i=1}^{m}$, $\\tau$, $\\epsilon$, and $\\Delta_{g}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined within the context of privacy-preserving machine learning and convex optimization.\n-   **Scientifically Grounded**: The problem uses standard concepts: duality gaps in optimization, federated learning structure, and the Laplace mechanism for achieving $\\epsilon$-differential privacy. These are all established and mainstream topics.\n-   **Well-Posed**: All variables, distributions, and conditions are explicitly defined, leading to a unique, solvable problem in probability theory. The goal is clearly stated.\n-   **Objective**: The language is formal and unambiguous. The problem is free from subjective claims.\n-   **Completeness**: The problem provides all necessary information. The condition $g_{i} > \\tau$ is critical and explicitly stated, ensuring the termination is indeed \"premature\" (i.e., would not have occurred without the privacy noise).\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Solution Derivation\nThe objective is to compute the probability of premature termination, which is the event that the algorithm stops despite the true duality gaps being above the tolerance $\\tau$. Let this event be denoted by $A$. The condition for termination is $\\min_{1 \\leq i \\leq m} \\tilde{g}_{i} \\leq \\tau$. Therefore, we want to calculate $P(A) = P(\\min_{1 \\leq i \\leq m} \\tilde{g}_{i} \\leq \\tau)$.\n\nIt is often simpler to calculate the probability of the complement event, $A^c$, and then use the relation $P(A) = 1 - P(A^c)$. The complement event $A^c$ is that the algorithm does not terminate, which means $\\min_{1 \\leq i \\leq m} \\tilde{g}_{i} > \\tau$. This is equivalent to the condition that $\\tilde{g}_{i} > \\tau$ for all clients $i=1, \\dots, m$.\n$$\nP(A^c) = P(\\tilde{g}_{1} > \\tau \\text{ and } \\tilde{g}_{2} > \\tau \\text{ and } \\dots \\text{ and } \\tilde{g}_{m} > \\tau)\n$$\nThe privatized gaps are defined as $\\tilde{g}_{i} = g_{i} + Z_{i}$. Since the true gaps $g_{i}$ are deterministic values at the given iteration and the noise terms $Z_{i}$ are independent random variables, the privatized gaps $\\tilde{g}_{i}$ are also independent random variables.\nDue to this independence, the joint probability is the product of the individual probabilities:\n$$\nP(A^c) = \\prod_{i=1}^{m} P(\\tilde{g}_{i} > \\tau)\n$$\nNow, we must find the probability $P(\\tilde{g}_{i} > \\tau)$ for a single client $i$.\n$$\nP(\\tilde{g}_{i} > \\tau) = P(g_{i} + Z_{i} > \\tau) = P(Z_{i} > \\tau - g_{i})\n$$\nThe random variable $Z_{i}$ follows a Laplace distribution with mean $0$ and scale $b$, denoted $Z_{i} \\sim \\text{Laplace}(0, b)$. The probability density function (PDF) of $Z_{i}$ is given by:\n$$\nf_{Z_i}(z) = \\frac{1}{2b} \\exp\\left(-\\frac{|z|}{b}\\right)\n$$\nTo find $P(Z_{i} > \\tau - g_{i})$, we can use the cumulative distribution function (CDF), $F_{Z_i}(x) = P(Z_{i} \\leq x)$. The CDF of the Laplace distribution is:\n$$\nF_{Z_i}(x) = \\int_{-\\infty}^{x} f_{Z_i}(z) dz = \\begin{cases} \\frac{1}{2} \\exp\\left(\\frac{x}{b}\\right) & \\text{if } x < 0 \\\\ 1 - \\frac{1}{2} \\exp\\left(-\\frac{x}{b}\\right) & \\text{if } x \\geq 0 \\end{cases}\n$$\nThe probability we need is $P(Z_{i} > \\tau - g_{i}) = 1 - P(Z_{i} \\leq \\tau - g_{i}) = 1 - F_{Z_i}(\\tau - g_{i})$.\n\nAccording to the problem statement, we assume that $g_{i} > \\tau$ for all $i$. This implies that the argument of the CDF, $\\tau - g_{i}$, is negative. Therefore, we must use the first case of the CDF formula:\n$$\nP(Z_{i} \\leq \\tau - g_{i}) = F_{Z_i}(\\tau - g_{i}) = \\frac{1}{2} \\exp\\left(\\frac{\\tau - g_{i}}{b}\\right) \\quad (\\text{since } \\tau - g_{i} < 0)\n$$\nSubstituting this back, we get the probability for a single client:\n$$\nP(\\tilde{g}_{i} > \\tau) = 1 - \\frac{1}{2} \\exp\\left(\\frac{\\tau - g_{i}}{b}\\right)\n$$\nNow we can compute the probability of the complement event $A^c$ by taking the product over all $m$ clients:\n$$\nP(A^c) = \\prod_{i=1}^{m} \\left(1 - \\frac{1}{2} \\exp\\left(\\frac{\\tau - g_{i}}{b}\\right)\\right)\n$$\nFinally, the probability of premature termination, $P(A)$, is $1 - P(A^c)$:\n$$\nP(A) = 1 - \\prod_{i=1}^{m} \\left(1 - \\frac{1}{2} \\exp\\left(\\frac{\\tau - g_{i}}{b}\\right)\\right)\n$$\nThe last step is to substitute the given expression for the scale parameter $b = \\Delta_{g} / \\epsilon$.\n$$\n\\frac{\\tau - g_{i}}{b} = \\frac{\\tau - g_{i}}{\\Delta_{g} / \\epsilon} = \\frac{\\epsilon (\\tau - g_{i})}{\\Delta_{g}}\n$$\nThis gives the final closed-form expression for the probability of premature termination in terms of the specified parameters:\n$$\nP(A) = 1 - \\prod_{i=1}^{m} \\left(1 - \\frac{1}{2} \\exp\\left(\\frac{\\epsilon(\\tau - g_{i})}{\\Delta_{g}}\\right)\\right)\n$$\nThis expression captures the probability that at least one client reports a privatized gap $\\tilde{g}_i$ less than or equal to the tolerance $\\tau$, triggering termination, given that all true gaps $g_i$ are strictly greater than $\\tau$.", "answer": "$$\\boxed{1 - \\prod_{i=1}^{m} \\left(1 - \\frac{1}{2} \\exp\\left(\\frac{\\epsilon(\\tau - g_{i})}{\\Delta_{g}}\\right)\\right)}$$", "id": "3468480"}, {"introduction": "The ultimate goal of many optimization routines is to produce a high-quality statistical model, and privacy preservation must be integrated into this goal. This practice connects the dots between privacy noise, measurement uncertainty, and model specification in the context of sparse regression [@problem_id:3468478]. You will derive the optimal regularization parameter for the Lasso that explicitly balances the need for sparsity against the combined noise from both the measurement process and the differential privacy mechanism, ensuring control over the false discovery rate.", "problem": "Consider a federated sparse linear regression setting with $K$ clients, each holding $n/K$ samples, so that the total sample count is $n$. The global design matrix has $p$ columns that are orthonormal at the federation level, meaning $X^{\\top}X = n I_{p}$. The response vector is generated by a sparse linear model $y = X\\beta + \\varepsilon$, where the noise satisfies $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ and the support of $\\beta$ has size $m_{1}$; the remaining $m_{0} = p - m_{1}$ coefficients are exactly zero. Each client $k \\in \\{1, \\dots, K\\}$ computes its local correlation statistic $s^{(k)} = {X^{(k)}}^{\\top}y^{(k)}/(n/K)$ and applies the Gaussian mechanism for Differential Privacy (DP), adding independent noise $\\eta^{(k)} \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{DP}}^{2} I_{p})$ to $s^{(k)}$. The server averages the privatized local statistics to obtain the federated aggregate\n$$\n\\bar{s} = \\frac{1}{K}\\sum_{k=1}^{K}\\left(s^{(k)} + \\eta^{(k)}\\right)\n$$\nDefine the Least Absolute Shrinkage and Selection Operator (Lasso) estimator as the optimizer of the objective\n$$\n\\min_{w \\in \\mathbb{R}^{p}} \\; \\frac{1}{2n}\\|y - X w\\|_{2}^{2} + \\lambda \\|w\\|_{1}.\n$$\nUnder the orthonormal design assumption, the Lasso selects feature $j$ into the model if and only if $|\\bar{s}_{j}| > \\lambda$. For a null coordinate $j$ with $\\beta_{j} = 0$, $\\bar{s}_{j}$ is a zero-mean Gaussian random variable whose variance is the sum of the federated measurement noise and the aggregated DP noise,\n$$\n\\sigma_{\\mathrm{eff}}^{2} = \\frac{\\sigma^{2}}{n} + \\frac{\\sigma_{\\mathrm{DP}}^{2}}{K}\n$$\nDefine the False Discovery Rate (FDR) as $\\mathrm{FDR} = \\mathbb{E}[V/R]$, where $V$ is the number of selected null coordinates and $R$ is the total number of selected coordinates. In the strong-signal regime for the true support (that is, signals are sufficiently separated so that all $m_{1}$ true coordinates are selected, while false negatives are negligible), the expected total number of selections is $m_{1} + \\mathbb{E}[V]$. A target FDR level $\\bar{q}$ is specified with $\\bar{q} \\in \\left(0, \\frac{m_{0}}{m_{0}+m_{1}}\\right]$. Using the standard normal cumulative distribution function $\\Phi$ and its inverse $\\Phi^{-1}$, compute the minimal threshold $\\lambda$ as a closed-form expression in terms of $\\sigma$, $\\sigma_{\\mathrm{DP}}$, $n$, $K$, $m_{0}$, $m_{1}$, and $\\bar{q}$ that ensures the FDR is at most $\\bar{q}$. Express your final answer as a single analytic expression in terms of $\\Phi$ and $\\Phi^{-1}$ with no numerical approximation.", "solution": "The user wants to find the minimal threshold, $\\lambda$, for feature selection in a federated sparse linear regression problem that ensures the False Discovery Rate (FDR) is at most a target level $\\bar{q}$.\n\nThe problem is deemed valid as it is scientifically grounded in established principles of statistics, federated learning, and differential privacy. It is well-posed, self-contained, and objective, allowing for a unique, meaningful solution through rigorous mathematical derivation.\n\nLet's begin the derivation.\nThe selection rule for a feature $j$ is given by $|\\bar{s}_{j}| > \\lambda$, where $\\bar{s}_{j}$ is the $j$-th component of the aggregated correlation statistic.\nFor a null coordinate $j$ (where the true coefficient $\\beta_{j}$ is $0$), the statistic $\\bar{s}_{j}$ is a zero-mean Gaussian random variable with variance $\\sigma_{\\mathrm{eff}}^{2}$.\n$$\n\\bar{s}_{j} \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{eff}}^{2}) \\quad \\text{for} \\quad \\beta_{j}=0\n$$\nwhere the effective variance $\\sigma_{\\mathrm{eff}}^{2}$ is given by\n$$\n\\sigma_{\\mathrm{eff}}^{2} = \\frac{\\sigma^{2}}{n} + \\frac{\\sigma_{\\mathrm{DP}}^{2}}{K}\n$$\nA false discovery occurs when a null coordinate is selected. The probability of a single false discovery for a null coordinate $j$ is $P(|\\bar{s}_{j}| > \\lambda)$. To compute this, we can normalize $\\bar{s}_{j}$ to a standard normal variable $Z_{j} = \\bar{s}_{j}/\\sigma_{\\mathrm{eff}}$, where $Z_{j} \\sim \\mathcal{N}(0, 1)$.\n$$\nP(|\\bar{s}_{j}| > \\lambda) = P\\left(\\left|\\frac{\\bar{s}_{j}}{\\sigma_{\\mathrm{eff}}}\\right| > \\frac{\\lambda}{\\sigma_{\\mathrm{eff}}}\\right) = P\\left(|Z_{j}| > \\frac{\\lambda}{\\sigma_{\\mathrm{eff}}}\\right)\n$$\nUsing the cumulative distribution function (CDF) of the standard normal distribution, $\\Phi(z) = P(Z \\le z)$, this probability can be written as:\n$$\nP\\left(|Z_{j}| > \\frac{\\lambda}{\\sigma_{\\mathrm{eff}}}\\right) = P\\left(Z_{j} > \\frac{\\lambda}{\\sigma_{\\mathrm{eff}}}\\right) + P\\left(Z_{j} < -\\frac{\\lambda}{\\sigma_{\\mathrm{eff}}}\\right) = 2 \\cdot P\\left(Z_{j} > \\frac{\\lambda}{\\sigma_{\\mathrm{eff}}}\\right) = 2 \\left(1 - \\Phi\\left(\\frac{\\lambda}{\\sigma_{\\mathrm{eff}}}\\right)\\right)\n$$\nLet $V$ be the number of false discoveries (selected null coordinates). There are $m_{0}$ null coordinates in total. Since the statistics $\\bar{s}_{j}$ for null coordinates are independent and identically distributed, the expected number of false discoveries, $\\mathbb{E}[V]$, is the sum of the probabilities of each null coordinate being selected:\n$$\n\\mathbb{E}[V] = \\sum_{j:\\beta_j=0} P(|\\bar{s}_{j}| > \\lambda) = m_{0} \\cdot P(|\\bar{s}_{j}| > \\lambda) = 2m_{0}\\left(1 - \\Phi\\left(\\frac{\\lambda}{\\sigma_{\\mathrm{eff}}}\\right)\\right)\n$$\nThe problem defines the False Discovery Rate as $\\mathrm{FDR} = \\mathbb{E}[V/R]$, where $R$ is the total number of selected coordinates. In the strong-signal regime, all $m_{1}$ true coordinates are selected, so $R = m_{1} + V$. The problem statement implies the use of the common approximation $\\mathrm{FDR} \\approx \\frac{\\mathbb{E}[V]}{\\mathbb{E}[R]}$. Since $\\mathbb{E}[R] = \\mathbb{E}[m_{1} + V] = m_{1} + \\mathbb{E}[V]$, the condition to control the FDR at level $\\bar{q}$ is:\n$$\n\\frac{\\mathbb{E}[V]}{m_{1} + \\mathbb{E}[V]} \\le \\bar{q}\n$$\nWe want to find the minimal $\\lambda$ that satisfies this condition. A smaller $\\lambda$ leads to a larger $\\mathbb{E}[V]$. The left-hand side of the inequality is an increasing function of $\\mathbb{E}[V]$. Therefore, to find the minimal $\\lambda$, we should solve for the case where the inequality is tight, which corresponds to the maximum allowable value for $\\mathbb{E}[V]$.\n$$\n\\frac{\\mathbb{E}[V]}{m_{1} + \\mathbb{E}[V]} = \\bar{q}\n$$\nSolving for $\\mathbb{E}[V]$:\n$$\n\\mathbb{E}[V] = \\bar{q}(m_{1} + \\mathbb{E}[V])\n$$\n$$\n\\mathbb{E}[V] - \\bar{q}\\mathbb{E}[V] = \\bar{q}m_{1}\n$$\n$$\n\\mathbb{E}[V](1 - \\bar{q}) = \\bar{q}m_{1}\n$$\n$$\n\\mathbb{E}[V] = \\frac{\\bar{q}m_{1}}{1 - \\bar{q}}\n$$\nNow, we substitute the expression for $\\mathbb{E}[V]$ in terms of $\\lambda$:\n$$\n2m_{0}\\left(1 - \\Phi\\left(\\frac{\\lambda}{\\sigma_{\\mathrm{eff}}}\\right)\\right) = \\frac{\\bar{q}m_{1}}{1 - \\bar{q}}\n$$\nWe proceed to solve for $\\lambda$. First, isolate the term with the CDF:\n$$\n1 - \\Phi\\left(\\frac{\\lambda}{\\sigma_{\\mathrm{eff}}}\\right) = \\frac{\\bar{q}m_{1}}{2m_{0}(1 - \\bar{q})}\n$$\n$$\n\\Phi\\left(\\frac{\\lambda}{\\sigma_{\\mathrm{eff}}}\\right) = 1 - \\frac{\\bar{q}m_{1}}{2m_{0}(1 - \\bar{q})}\n$$\nApplying the inverse normal CDF, $\\Phi^{-1}$:\n$$\n\\frac{\\lambda}{\\sigma_{\\mathrm{eff}}} = \\Phi^{-1}\\left(1 - \\frac{\\bar{q}m_{1}}{2m_{0}(1 - \\bar{q})}\\right)\n$$\nFinally, we solve for $\\lambda$ and substitute the expression for $\\sigma_{\\mathrm{eff}}$:\n$$\n\\lambda = \\sigma_{\\mathrm{eff}} \\cdot \\Phi^{-1}\\left(1 - \\frac{\\bar{q}m_{1}}{2m_{0}(1 - \\bar{q})}\\right)\n$$\n$$\n\\lambda = \\sqrt{\\frac{\\sigma^{2}}{n} + \\frac{\\sigma_{\\mathrm{DP}}^{2}}{K}} \\cdot \\Phi^{-1}\\left(1 - \\frac{\\bar{q}m_{1}}{2m_{0}(1 - \\bar{q})}\\right)\n$$\nThis is the minimal value of $\\lambda$ that ensures the FDR is at most $\\bar{q}$ under the given assumptions.", "answer": "$$\n\\boxed{\\sqrt{\\frac{\\sigma^{2}}{n} + \\frac{\\sigma_{\\mathrm{DP}}^{2}}{K}} \\Phi^{-1}\\left(1 - \\frac{\\bar{q} m_{1}}{2 m_{0} (1 - \\bar{q})}\\right)}\n$$", "id": "3468478"}]}