{"hands_on_practices": [{"introduction": "The first step in recovering a signal from quantized data is to precisely define what constitutes a valid solution. Since quantization maps a continuous range of values to a single symbol, a reconstructed signal is considered 'consistent' if its linear measurements fall into the correct quantization bins. This exercise [@problem_id:3472918] provides hands-on practice in translating this physical constraint into a mathematical object—a convex penalty function—which is a fundamental building block for modern, optimization-based recovery frameworks.", "problem": "Consider a noiseless compressed sensing model with an unknown $k$-sparse vector $x \\in \\mathbb{R}^{n}$ and a known sensing matrix $A \\in \\mathbb{R}^{m \\times n}$. Let $a_{i}^{\\top}$ denote the $i$-th row of $A$, so that the $i$-th scalar measurement before quantization is $z_{i} = a_{i}^{\\top} x$. Measurements are quantized by a nonuniform scalar quantizer $Q$ specified by a strictly increasing sequence of thresholds $\\{t_{k}\\}_{k=1}^{K+1}$, with $t_{1} < t_{2} < \\cdots < t_{K+1}$, and with the operating range ensured by $a_{i}^{\\top} x \\in [t_{1}, t_{K+1})$ for all $i \\in \\{1,\\dots,m\\}$. The quantizer maps a scalar $z$ to a bin index $Q(z) \\in \\{1,\\dots,K\\}$ according to the rule $Q(z) = k$ if and only if $t_{k} \\leq z < t_{k+1}$. The observed labels are $y_{i} = Q(a_{i}^{\\top} x)$ for $i = 1,\\dots,m$, collected in $y \\in \\{1,\\dots,K\\}^{m}$.\n\nStarting from the definition of the nonuniform quantizer and the interval-membership characterization of bin indices, formalize the measurement consistency constraints on $A x$ implied by the observations $y$, and derive a single closed-form analytic expression for a convex hinge penalty that is equal to zero if and only if all interval constraints are satisfied (i.e., for each $i$, the condition $t_{y_{i}} \\leq a_{i}^{\\top} x < t_{y_{i}+1}$ holds), and otherwise penalizes violations linearly and one-sidedly on each boundary. Express your final answer as an explicit function of $A$, $x$, $y$, and the thresholds $\\{t_{k}\\}_{k=1}^{K+1}$.\n\nYour final answer must be a single closed-form analytic expression. No numerical approximation or rounding is required.", "solution": "The problem is first validated to be scientifically grounded, well-posed, objective, and formally complete. It represents a standard task in the formulation of optimization problems for signal recovery from quantized measurements in compressed sensing. We may therefore proceed with the derivation.\n\nThe problem requires the formulation of a convex penalty function that enforces measurement consistency. Let us begin by formalizing the consistency constraints themselves.\n\nAccording to the problem statement, the scalar quantizer $Q$ maps an input $z$ to an integer bin index $k \\in \\{1, \\dots, K\\}$ based on a set of strictly increasing thresholds $\\{t_{j}\\}_{j=1}^{K+1}$. The quantization rule is given by:\n$$\nQ(z) = k \\quad \\text{if and only if} \\quad t_{k} \\leq z < t_{k+1}\n$$\nThe unquantized measurements are $z_{i} = a_{i}^{\\top} x$ for $i \\in \\{1, \\dots, m\\}$, where $a_{i}^{\\top}$ is the $i$-th row of the sensing matrix $A$ and $x \\in \\mathbb{R}^{n}$ is the unknown signal vector. The observed data are the bin indices $y_{i} = Q(z_{i}) = Q(a_{i}^{\\top} x)$.\n\nFor each measurement $i$, the observation $y_{i}$ implies that the true value $a_{i}^{\\top} x$ must lie within a specific interval defined by the thresholds. Applying the quantization rule, the observation $y_i$ is equivalent to the following set of constraints on $x$:\n$$\nt_{y_{i}} \\leq a_{i}^{\\top} x < t_{y_{i}+1}\n$$\nThis must hold for every measurement, so we have a system of $m$ such interval constraints, for $i = 1, \\dots, m$. Each interval constraint can be decomposed into two separate inequality constraints:\n1. $a_{i}^{\\top} x \\geq t_{y_{i}}$\n2. $a_{i}^{\\top} x < t_{y_{i}+1}$\n\nThe problem asks for a convex hinge penalty that is zero if and only if these constraints are satisfied. A hinge penalty, by definition, penalizes the violation of an inequality in a one-sided, linear fashion. For a general inequality constraint of the form $f(x) \\leq 0$, the corresponding hinge penalty is $\\max\\{0, f(x)\\}$. For a constraint of the form $g(x) \\geq 0$, which is equivalent to $-g(x) \\leq 0$, the penalty is $\\max\\{0, -g(x)\\}$.\n\nLet us apply this principle to our two inequalities for the $i$-th measurement.\n\nFor the first inequality, $a_{i}^{\\top} x \\geq t_{y_{i}}$, we can rewrite it as $t_{y_{i}} - a_{i}^{\\top} x \\leq 0$. A violation occurs when $t_{y_{i}} - a_{i}^{\\top} x > 0$. The hinge penalty for violating this lower bound is:\n$$\nP_{\\text{lower}, i}(x) = \\max\\{0, t_{y_{i}} - a_{i}^{\\top} x\\}\n$$\nThis function is zero if $a_{i}^{\\top} x \\geq t_{y_{i}}$ and grows linearly with the magnitude of the violation, $t_{y_{i}} - a_{i}^{\\top} x$, when $a_{i}^{\\top} x < t_{y_{i}}$.\n\nFor the second inequality, $a_{i}^{\\top} x < t_{y_{i}+1}$, we first relax the strict inequality to $a_{i}^{\\top} x \\leq t_{y_{i}+1}$ to facilitate the formulation of a convex penalty function. This is standard practice in optimization, as penalties on open sets are problematic. The relaxed constraint is $a_{i}^{\\top} x - t_{y_{i}+1} \\leq 0$. A violation occurs when $a_{i}^{\\top} x - t_{y_{i}+1} > 0$. The hinge penalty for violating this upper bound is:\n$$\nP_{\\text{upper}, i}(x) = \\max\\{0, a_{i}^{\\top} x - t_{y_{i}+1}\\}\n$$\nThis function is zero if $a_{i}^{\\top} x \\leq t_{y_{i}+1}$ and grows linearly with the magnitude of the violation, $a_{i}^{\\top} x - t_{y_{i}+1}$, when $a_{i}^{\\top} x > t_{y_{i}+1}$.\n\nThe total penalty for the $i$-th measurement, which must be zero if and only if both constraints are met, is the sum of the individual penalties:\n$$\nP_{i}(x) = P_{\\text{lower}, i}(x) + P_{\\text{upper}, i}(x) = \\max\\{0, t_{y_{i}} - a_{i}^{\\top} x\\} + \\max\\{0, a_{i}^{\\top} x - t_{y_{i}+1}\\}\n$$\nThis combined penalty $P_{i}(x)$ is zero if and only if both terms are zero, which occurs when $t_{y_{i}} - a_{i}^{\\top} x \\leq 0$ and $a_{i}^{\\top} x - t_{y_{i}+1} \\leq 0$. This is precisely the condition $t_{y_{i}} \\leq a_{i}^{\\top} x \\leq t_{y_{i}+1}$.\n\nTo obtain the total penalty for all measurements, we sum the individual penalties over $i = 1, \\dots, m$. Let this total penalty be denoted by $\\mathcal{L}(x)$.\n$$\n\\mathcal{L}(x) = \\sum_{i=1}^{m} P_{i}(x) = \\sum_{i=1}^{m} \\left( \\max\\{0, t_{y_{i}} - a_{i}^{\\top} x\\} + \\max\\{0, a_{i}^{\\top} x - t_{y_{i}+1}\\} \\right)\n$$\nThis final expression fulfills all the requirements of the problem:\n- It is a function of $A$, $x$, $y$, and the thresholds $\\{t_{k}\\}$. The term $a_{i}^{\\top} x$ involves $A$ and $x$. The indices on the thresholds, $y_{i}$ and $y_{i}+1$, come from the observation vector $y$.\n- It is a convex function of $x$. The terms $t_{y_{i}} - a_{i}^{\\top} x$ and $a_{i}^{\\top} x - t_{y_{i}+1}$ are affine in $x$. The function $\\max\\{0, u\\}$ is convex and non-decreasing. The composition of a convex non-decreasing function with an affine function is convex. The sum of convex functions is convex.\n- It is equal to zero if and only if all interval constraints (in their relaxed form $t_{y_{i}} \\leq a_{i}^{\\top} x \\leq t_{y_{i}+1}$) are satisfied for all $i \\in \\{1, \\dots, m\\}$. Since each term in the sum is non-negative, the sum is zero if and only if all individual terms are zero.\n- It penalizes violations linearly and one-sidedly at each boundary, as is the nature of the hinge penalty.\n\nThis expression is the desired closed-form analytic representation of the measurement consistency penalty.", "answer": "$$\n\\boxed{\\sum_{i=1}^{m} \\left( \\max\\{0, t_{y_i} - a_{i}^{\\top} x\\} + \\max\\{0, a_{i}^{\\top} x - t_{y_{i}+1}\\} \\right)}\n$$", "id": "3472918"}, {"introduction": "With a mathematical measure of consistency established, the next challenge is to design an algorithm that finds a sparse signal minimizing this error. This practice [@problem_id:3472923] delves into the extreme but important case of 1-bit compressed sensing, where only the sign of each measurement is retained. You will derive the complete update rule for the Binary Iterative Hard Thresholding (BIHT) algorithm, a powerful method that combines gradient descent on a surrogate loss function with a hard thresholding step to enforce sparsity.", "problem": "Let $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ be a sensing matrix with rows $\\mathbf{a}_{i}^{\\top}$, and let $\\mathbf{x}^{\\star} \\in \\mathbb{R}^{n}$ be an unknown $s$-sparse vector. In one-bit Compressed Sensing (CS), only the signs of linear measurements are observed, so the measurement model is $\\mathbf{y} = \\operatorname{sign}(\\mathbf{A}\\mathbf{x}^{\\star}) \\in \\{-1, +1\\}^{m}$. A common approach to recover a sparse $\\mathbf{x}$ consistent with $\\mathbf{y}$ is to penalize sign-inconsistent measurements using the squared hinge loss\n$$\nL(\\mathbf{x}) = \\sum_{i=1}^{m} \\left(\\max\\!\\left(0, - y_{i}\\,\\mathbf{a}_{i}^{\\top}\\mathbf{x}\\right)\\right)^{2},\n$$\nwhere $\\max(0, \\cdot)$ is applied element-wise. Binary Iterative Hard Thresholding (BIHT) seeks a sparse minimizer of $L(\\mathbf{x})$ by repeating a steepest-descent step on $L(\\mathbf{x})$, followed by a hard thresholding operator that retains the $s$ entries of largest magnitude and zeros out the rest.\n\nStarting from the definitions of one-bit quantized measurements, the squared hinge loss, steepest-descent iterations for nonsmooth objectives via subgradients, and the hard-thresholding operator $H_{s}(\\cdot)$ that preserves the $s$ largest entries in magnitude, derive the explicit analytical expression of the BIHT update that maps $\\mathbf{x}^{t}$ to the next iterate. Express your final update as a single closed-form expression in terms of $\\mathbf{A}$, $\\mathbf{y}$, $\\mathbf{x}^{t}$, a positive step size parameter $\\mu$, and $s$, using the element-wise positive-part operator $[\\,\\cdot\\,]_{+} := \\max(0, \\cdot)$. Your final answer must be a single closed-form analytic expression without an equality sign. No numerical evaluation is required.", "solution": "The user wants me to derive the update rule for the Binary Iterative Hard Thresholding (BIHT) algorithm.\n\nHere's my plan:\n1.  **Validate the problem statement.**\n    *   **Step 1: Extract Givens.** I will list all the provided definitions and equations.\n        *   Sensing matrix: $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ with rows $\\mathbf{a}_{i}^{\\top}$.\n        *   Unknown vector: $\\mathbf{x}^{\\star} \\in \\mathbb{R}^{n}$, which is $s$-sparse.\n        *   Measurement model: $\\mathbf{y} = \\operatorname{sign}(\\mathbf{A}\\mathbf{x}^{\\star}) \\in \\{-1, +1\\}^{m}$.\n        *   Loss function: $L(\\mathbf{x}) = \\sum_{i=1}^{m} \\left(\\max\\!\\left(0, - y_{i}\\,\\mathbf{a}_{i}^{\\top}\\mathbf{x}\\right)\\right)^{2}$.\n        *   Algorithm: Binary Iterative Hard Thresholding (BIHT).\n        *   BIHT steps:\n            1.  Steepest-descent step on $L(\\mathbf{x})$.\n            2.  Hard thresholding operator $H_{s}(\\cdot)$ that retains the $s$ largest magnitude entries.\n        *   Goal: Derive the update rule $\\mathbf{x}^{t+1} = f(\\mathbf{x}^{t}, \\mathbf{A}, \\mathbf{y}, \\mu, s)$.\n        *   Notation: Use $[\\cdot]_{+} := \\max(0, \\cdot)$.\n        *   Step size: $\\mu > 0$.\n\n    *   **Step 2: Validate Using Extracted Givens.**\n        *   **Scientifically Grounded?** Yes, this problem is firmly set within the field of compressed sensing and sparse optimization. One-bit compressed sensing, the squared hinge loss, and iterative hard thresholding algorithms are standard topics. The BIHT algorithm is a known method.\n        *   **Well-Posed?** Yes, the goal is to derive a specific update rule based on given definitions. This is a well-defined mathematical derivation. A unique expression should exist based on the standard subgradient descent approach.\n        *   **Objective?** Yes, the problem is stated using precise mathematical language. There are no subjective or opinion-based statements.\n        *   **Flaws?**\n            1.  **Scientific Unsoundness?** No. The math is correct. The loss function is a standard surrogate for the 0-1 loss in classification and is used in this context. The use of subgradients for non-smooth optimization is standard.\n            2.  **Non-Formalizable/Irrelevant?** No. It's directly relevant to the topic of recovery algorithms for quantized measurements.\n            3.  **Incomplete/Contradictory?** The problem states \"steepest-descent iterations for nonsmooth objectives via subgradients\". This is key. For a non-smooth convex function, the steepest descent direction is given by $-\\nabla L(\\mathbf{x})$ if differentiable, or more generally, the direction of the negative subgradient of minimal norm. For a non-convex function like this (due to the subsequent thresholding), \"steepest descent\" is usually interpreted as gradient descent or subgradient descent. The problem asks for the update based on a steepest-descent step, which implies using a (sub)gradient. The loss function $L(\\mathbf{x})$ is non-smooth but its subgradient can be calculated. The problem is sufficiently specified to proceed.\n            4.  **Unrealistic/Infeasible?** No. All definitions are standard in the context of signal processing and machine learning.\n            5.  **Ill-Posed?** No. The derivation is straightforward.\n            6.  **Pseudo-Profound/Trivial?** No. It requires a proper derivation involving subgradient calculus.\n            7.  **Outside Scientific Verifiability?** No. The derivation is mathematically verifiable.\n\n    *   **Step 3: Verdict and Action.** The problem is **valid**. I will proceed to the solution.\n\n2.  **Solve the problem.**\n    *   The goal is to find the expression for the BIHT update. The update has two steps:\n        1.  A subgradient descent step.\n        2.  A hard thresholding step.\n\n    *   Let the iterate at step $t$ be $\\mathbf{x}^{t}$.\n    *   The first step is a subgradient descent update on the loss function $L(\\mathbf{x})$. Let's call the intermediate result $\\mathbf{z}^{t+1}$.\n        $$ \\mathbf{z}^{t+1} = \\mathbf{x}^{t} - \\mu \\mathbf{g}^{t} $$\n        where $\\mathbf{g}^{t}$ is a subgradient of $L(\\mathbf{x})$ evaluated at $\\mathbf{x} = \\mathbf{x}^{t}$. We write $\\mathbf{g}^{t} \\in \\partial L(\\mathbf{x}^{t})$.\n\n    *   The second step is the hard thresholding operation:\n        $$ \\mathbf{x}^{t+1} = H_{s}(\\mathbf{z}^{t+1}) $$\n\n    *   So, the full update is:\n        $$ \\mathbf{x}^{t+1} = H_{s}\\left(\\mathbf{x}^{t} - \\mu \\mathbf{g}^{t}\\right) $$\n        I need to compute the subgradient $\\mathbf{g}^{t} \\in \\partial L(\\mathbf{x}^{t})$.\n\n    *   The loss function is $L(\\mathbf{x}) = \\sum_{i=1}^{m} L_i(\\mathbf{x})$, where $L_i(\\mathbf{x}) = (\\max(0, -y_i \\mathbf{a}_i^\\top \\mathbf{x}))^2$. Let's use the notation $[\\cdot]_+ = \\max(0, \\cdot)$. So, $L_i(\\mathbf{x}) = [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+^2$.\n    *   The overall subgradient is the sum of the subgradients of the individual terms: $\\partial L(\\mathbf{x}) = \\sum_{i=1}^{m} \\partial L_i(\\mathbf{x})$.\n    *   Let's find the subgradient of $L_i(\\mathbf{x})$. Let $u_i(\\mathbf{x}) = -y_i \\mathbf{a}_i^\\top \\mathbf{x}$. Then $L_i(\\mathbf{x}) = ([u_i(\\mathbf{x})]_+)^2$.\n    *   The function $f(u) = ([u]_+)^2$ is continuously differentiable. Its derivative is:\n        $$ f'(u) = \\begin{cases} 2u & \\text{if } u > 0 \\\\ 0 & \\text{if } u \\leq 0 \\end{cases} $$\n        This can be written as $f'(u) = 2[u]_+$.\n    *   The function $u_i(\\mathbf{x})$ is a linear function of $\\mathbf{x}$, so its gradient is $\\nabla u_i(\\mathbf{x}) = -y_i \\mathbf{a}_i$.\n    *   Since $L_i(\\mathbf{x})$ is a composition of a differentiable function $f(u)$ and a differentiable function $u_i(\\mathbf{x})$, the chain rule applies, and $L_i(\\mathbf{x})$ is differentiable everywhere. Its gradient is:\n        $$ \\nabla L_i(\\mathbf{x}) = f'(u_i(\\mathbf{x})) \\cdot \\nabla u_i(\\mathbf{x}) $$\n        $$ \\nabla L_i(\\mathbf{x}) = 2 [u_i(\\mathbf{x})]_+ \\cdot (-y_i \\mathbf{a}_i) $$\n        $$ \\nabla L_i(\\mathbf{x}) = 2 [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ \\cdot (-y_i \\mathbf{a}_i) $$\n\n    *   The problem statement mentions \"steepest-descent iterations for nonsmooth objectives via subgradients\". Let's re-check if $L(\\mathbf{x})$ is truly non-smooth.\n        $L(\\mathbf{x}) = \\sum_{i=1}^m ([-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+)^2$.\n        The function $g(z) = ([z]_+)^2$ is continuously differentiable. Its derivative is $g'(z) = 2[z]_+$.\n        Since $z = -y_i \\mathbf{a}_i^\\top \\mathbf{x}$ is a linear function of $\\mathbf{x}$, the composite function $L_i(\\mathbf{x})$ is also continuously differentiable. Thus, the total loss $L(\\mathbf{x})$ is continuously differentiable.\n        The use of the term \"subgradient\" in the problem description is slightly imprecise but standard in this context, as the squared hinge loss is a smooth relaxation of the non-smooth hinge loss $\\max(0, 1 - y_i f(\\mathbf{x}_i))$. For the squared hinge loss, the gradient exists everywhere. The problem likely uses \"subgradient\" as a general term for gradient-based methods on convex-like objectives. I will proceed with the gradient calculation.\n\n    *   The gradient of the total loss $L(\\mathbf{x})$ is the sum of the gradients of the terms $L_i(\\mathbf{x})$:\n        $$ \\nabla L(\\mathbf{x}) = \\sum_{i=1}^{m} \\nabla L_i(\\mathbf{x}) $$\n        $$ \\nabla L(\\mathbf{x}) = \\sum_{i=1}^{m} 2 [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ (-y_i \\mathbf{a}_i) $$\n        Let's try to simplify this expression.\n        Let $\\mathbf{r}(\\mathbf{x}) = - \\operatorname{diag}(\\mathbf{y}) \\mathbf{A} \\mathbf{x}$. $\\operatorname{diag}(\\mathbf{y})$ is a diagonal matrix with $y_i$ on the diagonal. The $i$-th component of $\\mathbf{r}(\\mathbf{x})$ is $r_i(\\mathbf{x}) = -y_i \\sum_j A_{ij} x_j = -y_i \\mathbf{a}_i^\\top \\mathbf{x}$.\n        So, $\\nabla L(\\mathbf{x}) = \\sum_{i=1}^{m} 2 [r_i(\\mathbf{x})]_+ (-y_i \\mathbf{a}_i)$.\n        Let's rewrite the sum in matrix form.\n        The sum is $\\sum_{i=1}^m (\\text{scalar})_i \\cdot (\\text{vector})_i$.\n        The vector part is $-y_i \\mathbf{a}_i$. Let's consider the matrix $\\mathbf{A}$. The columns of $\\mathbf{A}^\\top$ are $\\mathbf{a}_i$. So we are summing columns of $\\mathbf{A}^\\top$.\n        $\\nabla L(\\mathbf{x}) = \\sum_{i=1}^{m} \\mathbf{a}_i (-2 y_i [ -y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+)$.\n        This looks like $\\mathbf{A}^\\top$ times a vector.\n        Let $\\mathbf{v}(\\mathbf{x}) \\in \\mathbb{R}^m$ be a vector with components $v_i(\\mathbf{x}) = -2 y_i [ -y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+$.\n        Then $\\nabla L(\\mathbf{x}) = \\mathbf{A}^\\top \\mathbf{v}(\\mathbf{x})$.\n        So, $\\nabla L(\\mathbf{x}) = \\mathbf{A}^\\top \\left( -2 \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y}) \\mathbf{A} \\mathbf{x}]_+ \\right)$.\n        The notation $[\\cdot]_+$ is element-wise. $\\operatorname{diag}(\\mathbf{y})$ is a bit cumbersome. Let's use element-wise product `o`.\n        Let $\\mathbf{z} = \\mathbf{A}\\mathbf{x}$. Then we have $-y_i z_i$ where $z_i = \\mathbf{a}_i^\\top \\mathbf{x}$.\n        The gradient is $\\nabla L(\\mathbf{x}) = \\sum_{i=1}^{m} 2 [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ (-y_i \\mathbf{a}_i)$.\n        Let's check the scalar part: $-y_i [-y_i (\\mathbf{A}\\mathbf{x})_i]_+$.\n        Since $y_i \\in \\{-1, +1\\}$, we have $y_i^2 = 1$.\n        $-y_i [-y_i (\\mathbf{A}\\mathbf{x})_i]_+ = -y_i \\max(0, -y_i (\\mathbf{A}\\mathbf{x})_i)$.\n        If $-y_i (\\mathbf{A}\\mathbf{x})_i > 0$, this is $-y_i (-y_i (\\mathbf{A}\\mathbf{x})_i) = y_i^2 (\\mathbf{A}\\mathbf{x})_i = (\\mathbf{A}\\mathbf{x})_i$.\n        If $-y_i (\\mathbf{A}\\mathbf{x})_i \\leq 0$, this is $-y_i (0) = 0$.\n        This is not quite right.\n        Let's check my chain rule step.\n        $\\nabla L_i(\\mathbf{x}) = 2 [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ (-y_i \\mathbf{a}_i)$. This is correct.\n\n        Let's consider the term $c_i = [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+$. This is the magnitude of the \"correction\" needed for measurement $i$.\n        The gradient is $\\nabla L(\\mathbf{x}) = \\sum_{i=1}^{m} 2 c_i (-y_i \\mathbf{a}_i)$.\n        This is $\\sum_{i=1}^{m} \\mathbf{a}_i (-2 y_i c_i) = \\mathbf{A}^\\top \\mathbf{d}$, where $\\mathbf{d}$ is a vector with components $d_i = -2 y_i c_i = -2 y_i [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+$.\n\n        So, the subgradient descent step (which is just a gradient descent step here) at $\\mathbf{x} = \\mathbf{x}^t$ is:\n        $\\mathbf{z}^{t+1} = \\mathbf{x}^t - \\mu \\nabla L(\\mathbf{x}^t)$\n        $\\mathbf{z}^{t+1} = \\mathbf{x}^t - \\mu \\sum_{i=1}^{m} 2 [-y_i \\mathbf{a}_i^\\top \\mathbf{x}^t]_+ (-y_i \\mathbf{a}_i)$\n        $\\mathbf{z}^{t+1} = \\mathbf{x}^t + 2\\mu \\sum_{i=1}^{m} y_i [-y_i \\mathbf{a}_i^\\top \\mathbf{x}^t]_+ \\mathbf{a}_i$\n        This can be written in matrix form. The sum is $\\sum_i \\mathbf{a}_i (\\text{scalar})_i$.\n        This is $\\mathbf{A}^\\top$ times a vector. Let the vector be $\\mathbf{w} \\in \\mathbb{R}^m$.\n        The $i$-th component of $\\mathbf{w}$ is $w_i = y_i [-y_i \\mathbf{a}_i^\\top \\mathbf{x}^t]_+$.\n        So $\\sum_{i=1}^{m} y_i [-y_i \\mathbf{a}_i^\\top \\mathbf{x}^t]_+ \\mathbf{a}_i = \\mathbf{A}^\\top \\mathbf{w}$.\n        Let's define the vector $\\mathbf{u} = \\mathbf{A}\\mathbf{x}^t$.\n        The vector $\\mathbf{w}$ has components $w_i = y_i [-y_i u_i]_+$.\n        So, the update for $\\mathbf{z}^{t+1}$ is:\n        $\\mathbf{z}^{t+1} = \\mathbf{x}^t + 2\\mu \\mathbf{A}^\\top \\mathbf{w}$\n        where $\\mathbf{w}$ is a vector with components $w_i = y_i [-y_i (\\mathbf{A}\\mathbf{x}^t)_i]_+$.\n\n    *   Let's check the argument of the positive part operator: $-y_i (\\mathbf{A}\\mathbf{x}^t)_i$. This is the $i$-th element of the vector $-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^t$.\n    *   Let $\\mathbf{p}^t = -\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^t$.\n    *   Then the vector of positive parts is $[\\mathbf{p}^t]_+$.\n    *   The vector $\\mathbf{w}$ has components $w_i = y_i [p^t_i]_+$. So $\\mathbf{w} = \\operatorname{diag}(\\mathbf{y}) [\\mathbf{p}^t]_+$.\n    *   Substituting $\\mathbf{p}^t$:  $\\mathbf{w} = \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^t]_+$.\n    *   So, $\\mathbf{z}^{t+1} = \\mathbf{x}^t + 2\\mu \\mathbf{A}^\\top \\left( \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^t]_+ \\right)$.\n    *   This expression looks correct. It uses the given notation $[\\cdot]_+$. It involves $\\mathbf{A}, \\mathbf{y}, \\mathbf{x}^t, \\mu$.\n\n    *   The problem is to express the final update, which involves the hard thresholding step.\n    *   $\\mathbf{x}^{t+1} = H_s(\\mathbf{z}^{t+1})$.\n    *   So, the full BIHT update rule is:\n        $$ \\mathbf{x}^{t+1} = H_s\\left( \\mathbf{x}^t + 2\\mu \\mathbf{A}^\\top \\left( \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^t]_+ \\right) \\right) $$\n\n    *   Let's examine the term inside the summation again.\n        $y_i [-y_i \\mathbf{a}_i^\\top \\mathbf{x}^t]_+ = y_i \\max(0, -y_i \\mathbf{a}_i^\\top \\mathbf{x}^t)$.\n        Let $q_i = \\mathbf{a}_i^\\top \\mathbf{x}^t$.\n        The term is $y_i \\max(0, -y_i q_i)$.\n        If $y_i=+1$, the term is $\\max(0, -q_i)$.\n        If $y_i=-1$, the term is $-\\max(0, q_i)$.\n        \n        So, if $y_i=+1$: the term is non-zero only if $-q_i > 0$, i.e., $q_i < 0$. If $q_i<0$, the term is $-q_i$. This means we have a sign mismatch ($y_i=+1$ but $q_i<0$) and we are adding a corrective term.\n        If $y_i=-1$: the term is non-zero only if $q_i > 0$. If $q_i > 0$, the term is $-q_i$. Again, sign mismatch ($y_i=-1$ but $q_i>0$), and we are adding a correction.\n\n        So the term $y_i [-y_i \\mathbf{a}_i^\\top \\mathbf{x}^t]_+$ represents the \"residual\" scaled by the sign.\n        Let's try to simplify the diag(y) notation.\n        Let $\\mathbf{v} = -\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^t$. Its components are $v_i = -y_i (\\mathbf{A}\\mathbf{x}^t)_i$.\n        The vector inside the brackets is $[\\mathbf{v}]_+$.\n        The resulting vector is $\\operatorname{diag}(\\mathbf{y}) [\\mathbf{v}]_+$. Its $i$-th component is $y_i [v_i]_+ = y_i [-y_i (\\mathbf{A}\\mathbf{x}^t)_i]_+$.\n        Let's call the vector multiplying $\\mathbf{A}^\\top$ as $\\mathbf{r}$ (for residual or correction).\n        $r_i = y_i [-y_i (\\mathbf{A}\\mathbf{x}^t)_i]_+$.\n        Check what happens when signs match: $y_i \\operatorname{sign}((\\mathbf{A}\\mathbf{x}^t)_i) = 1$. This means $y_i$ and $(\\mathbf{A}\\mathbf{x}^t)_i$ have the same sign.\n        So $-y_i (\\mathbf{A}\\mathbf{x}^t)_i \\le 0$. Then $[-y_i (\\mathbf{A}\\mathbf{x}^t)_i]_+ = 0$, so $r_i=0$. This is correct, no gradient contribution if the sign is correct.\n        Check what happens when signs mismatch: $y_i \\operatorname{sign}((\\mathbf{A}\\mathbf{x}^t)_i) = -1$. This means $y_i$ and $(\\mathbf{A}\\mathbf{x}^t)_i$ have opposite signs.\n        So $-y_i (\\mathbf{A}\\mathbf{x}^t)_i > 0$. Then $[-y_i (\\mathbf{A}\\mathbf{x}^t)_i]_+ = -y_i (\\mathbf{A}\\mathbf{x}^t)_i$.\n        In this case, $r_i = y_i (-y_i (\\mathbf{A}\\mathbf{x}^t)_i) = -y_i^2 (\\mathbf{A}\\mathbf{x}^t)_i = -(\\mathbf{A}\\mathbf{x}^t)_i$.\n        \n        So we can define a vector $\\mathbf{r}$ where\n        $$ r_i = \\begin{cases} -(\\mathbf{A}\\mathbf{x}^t)_i & \\text{if } y_i (\\mathbf{A}\\mathbf{x}^t)_i < 0 \\\\ 0 & \\text{if } y_i (\\mathbf{A}\\mathbf{x}^t)_i \\ge 0 \\end{cases} $$\n        Note: The case $y_i (\\mathbf{A}\\mathbf{x}^t)_i = 0$ is included in the second case, which gives $r_i = 0$. This is consistent with the expression $y_i [-y_i (\\mathbf{A}\\mathbf{x}^t)_i]_+$.\n        \n        The term inside the summation is $2\\mu \\mathbf{A}^\\top \\mathbf{r}$, where $\\mathbf{r}$ is this residual vector.\n        Let's find a more compact form for $\\mathbf{r}$.\n        The original expression for the gradient was $\\nabla L(\\mathbf{x}) = \\sum_{i=1}^{m} 2 [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ (-y_i \\mathbf{a}_i) = \\mathbf{A}^\\top (-2 \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y}) \\mathbf{A} \\mathbf{x}]_+)$.\n        The gradient update is $\\mathbf{x}^t - \\mu \\nabla L(\\mathbf{x}^t) = \\mathbf{x}^t - \\mu \\mathbf{A}^\\top (-2 \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y}) \\mathbf{A} \\mathbf{x}^t]_+)$.\n        $= \\mathbf{x}^t + 2\\mu \\mathbf{A}^\\top (\\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y}) \\mathbf{A} \\mathbf{x}^t]_+)$.\n        This expression seems correct and uses the specified notation. The use of $\\operatorname{diag}(\\mathbf{y})$ is standard matrix algebra.\n\n    *   An alternative way to write $\\operatorname{diag}(\\mathbf{y}) \\mathbf{v}$ is $\\mathbf{y} \\odot \\mathbf{v}$ (Hadamard product), but the problem doesn't specify this notation. I'll stick to matrix-vector products. $\\operatorname{diag}(\\mathbf{y})$ is a matrix.\n    *   The problem asks for \"a single closed-form analytic expression\".\n        $H_s(\\cdot)$ is a well-defined operator.\n        The argument of $H_s(\\cdot)$ is: $\\mathbf{x}^t + 2\\mu \\mathbf{A}^\\top (\\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y}) \\mathbf{A} \\mathbf{x}^t]_+)$.\n    *   This expression involves:\n        - $\\mathbf{x}^t$: current iterate\n        - $\\mu$: step size\n        - $\\mathbf{A}$: sensing matrix\n        - $\\mathbf{y}$: measurement vector\n        - $[\\cdot]_+$: positive part operator\n        - $\\operatorname{diag}(\\mathbf{y})$: diagonal matrix from vector $\\mathbf{y}$\n        - $H_s(\\cdot)$: hard thresholding operator\n    *   All these components are defined or standard. The expression is closed-form.\n    *   The final expression for the update should be $\\mathbf{x}^{t+1} = ...$. The problem asks for the expression `that maps x^t to the next iterate`, and `without an equality sign`. So I should just provide the RHS.\n\n    *   Let's double-check the gradient calculation.\n        $L(\\mathbf{x}) = \\sum_{i=1}^{m} ([-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+)^2$.\n        Let $f(z) = z^2$. Let $g(w) = [w]_+$. Let $h(\\mathbf{x}) = -y_i \\mathbf{a}_i^\\top \\mathbf{x}$.\n        $L_i(\\mathbf{x}) = f(g(h(\\mathbf{x})))$.\n        $h$ is non-smooth? No, $h$ is linear. $g$ is non-smooth. $f$ is smooth.\n        Ah, I used $f(u) = ([u]_+)^2$. This is what I called $L_i$ as a function of $u_i$. My analysis of $f'(u) = 2[u]_+$ is correct. The function $u \\mapsto [u]_+$ is not differentiable at $u=0$, but $u \\mapsto ([u]_+)^2$ IS differentiable at $u=0$.\n        Let $g(u) = (\\max(0, u))^2$.\n        For $u > 0$, $g(u) = u^2$, $g'(u) = 2u$.\n        For $u < 0$, $g(u) = 0$, $g'(u) = 0$.\n        Let's check differentiability at $u=0$.\n        $\\lim_{h \\to 0^+} \\frac{g(0+h)-g(0)}{h} = \\lim_{h \\to 0^+} \\frac{h^2-0}{h} = \\lim_{h \\to 0^+} h = 0$.\n        $\\lim_{h \\to 0^-} \\frac{g(0+h)-g(0)}{h} = \\lim_{h \\to 0^-} \\frac{0-0}{h} = 0$.\n        The left and right derivatives match and are equal to $0$. So $g'(0)=0$.\n        So the derivative is $g'(u) = 2 \\max(0, u) = 2[u]_+$.\n        The function is continuously differentiable. So the term \"subgradient\" in the problem is indeed a slight misnomer, it should be \"gradient\". However, the field often uses these terms interchangeably when dealing with IHT-style algorithms, as the hard thresholding step is non-convex and the overall algorithm's convergence analysis often needs tools from non-smooth optimization. From a purely calculation perspective, using the gradient is the correct interpretation of \"steepest-descent\".\n        So my gradient calculation is correct.\n        $\\nabla_{\\mathbf{x}} ([-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+)^2 = 2 [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ \\cdot \\nabla_{\\mathbf{x}} (-y_i \\mathbf{a}_i^\\top \\mathbf{x})$\n        $= 2 [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ \\cdot (-y_i \\mathbf{a}_i)$.\n        $\\nabla L(\\mathbf{x}) = \\sum_{i=1}^m 2 [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ (-y_i \\mathbf{a}_i) = -2 \\sum_{i=1}^m y_i [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ \\mathbf{a}_i$.\n        The gradient step is $\\mathbf{x}^t - \\mu \\nabla L(\\mathbf{x}^t) = \\mathbf{x}^t + 2\\mu \\sum_{i=1}^m y_i [-y_i \\mathbf{a}_i^\\top \\mathbf{x}^t]_+ \\mathbf{a}_i$.\n        The summation is $\\sum_{i=1}^m \\mathbf{a}_i \\cdot \\big( y_i [-y_i (\\mathbf{A}\\mathbf{x}^t)_i]_+ \\big)$.\n        This is $\\mathbf{A}^\\top \\mathbf{w}$, where $w_i = y_i [-y_i (\\mathbf{A}\\mathbf{x}^t)_i]_+$.\n        As reasoned earlier, $\\mathbf{w} = \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y}) \\mathbf{A} \\mathbf{x}^t]_+$.\n        So the intermediate vector is $\\mathbf{z}^{t+1} = \\mathbf{x}^t + 2\\mu \\mathbf{A}^\\top (\\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y}) \\mathbf{A} \\mathbf{x}^t]_+)$.\n        The final update is $\\mathbf{x}^{t+1} = H_s(\\mathbf{z}^{t+1})$.\n        The question asks for the expression that maps $\\mathbf{x}^t$ to the next iterate. This means $\\mathbf{x}^{t+1}$. But it also says \"without an equality sign\". So I need to provide the RHS of the update rule for $\\mathbf{x}^{t+1}$.\n        Final expression to provide: $H_{s}\\left( \\mathbf{x}^{t} + 2\\mu \\mathbf{A}^{\\top} \\left( \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^{t}]_{+} \\right) \\right)$.\n\n    *   Let's check formatting requirements.\n        - Mandatory LaTeX for ALL math entities.\n        - No markdown headers.\n        - Final answer in `\\boxed{...}`.\n        - Final answer should be a single analytical expression.\n        - No units (not applicable here).\n        - Use `\\frac`, `^`, `\\exp`, `\\ln`, `\\arcsin` etc. (not really applicable here, except for powers like `\\mathbf{x}^t`).\n        - `\\mathbf{A}^{\\top}` for transpose is standard and good.\n        - The use of `\\operatorname{diag}` is also standard for creating a diagonal matrix from a vector.\n        - The use of `[\\cdot]_+` is specified in the problem.\n        - The hard thresholding operator is $H_s(\\cdot)$.\n\n    *   The structure of the solution text:\n        - Start with the BIHT general form: gradient step + hard thresholding.\n        - State the loss function $L(\\mathbf{x})$.\n        - Compute the gradient $\\nabla L(\\mathbf{x})$. This is the main calculation.\n        - Assemble the gradient step update.\n        - Combine with the hard thresholding operator to get the final expression.\n        - Ensure every symbol is in LaTeX.\n\n    *   Detailed plan for the solution write-up:\n        1.  State that the Binary Iterative Hard Thresholding (BIHT) update from iterate $\\mathbf{x}^t$ to $\\mathbf{x}^{t+1}$ consists of two steps: a steepest-descent step on the loss function $L(\\mathbf{x})$ followed by a hard thresholding operation $H_s(\\cdot)$.\n        2.  The steepest-descent step with step size $\\mu > 0$ updates an intermediate vector $\\mathbf{z}^{t+1}$ as follows: $\\mathbf{z}^{t+1} = \\mathbf{x}^t - \\mu \\mathbf{g}^t$, where $\\mathbf{g}^t$ is a subgradient (or gradient, where it exists) of $L(\\mathbf{x})$ at $\\mathbf{x}=\\mathbf{x}^t$.\n        3.  The second step is $\\mathbf{x}^{t+1} = H_s(\\mathbf{z}^{t+1})$.\n        4.  The task reduces to finding the analytical form of the (sub)gradient of the loss function $L(\\mathbf{x})$.\n        5.  Write down the loss function: $L(\\mathbf{x}) = \\sum_{i=1}^{m} ([\\!-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+)^2$.\n        6.  Explain that this function is, in fact, differentiable everywhere. The term \"subgradient\" is used loosely, and we can compute the standard gradient.\n        7.  Let $f_i(c) = ([c]_+)^2$. Its derivative is $f_i'(c) = 2[c]_+$.\n        8.  Using the chain rule, compute the gradient of the $i$-th term, $L_i(\\mathbf{x}) = ([\\!-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+)^2$:\n            $\\nabla L_i(\\mathbf{x}) = 2[-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ \\cdot \\nabla_\\mathbf{x}(-y_i \\mathbf{a}_i^\\top \\mathbf{x}) = 2[-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ (-y_i \\mathbf{a}_i)$.\n        9.  Sum over all $i$ to get the total gradient:\n            $\\nabla L(\\mathbf{x}) = \\sum_{i=1}^m \\nabla L_i(\\mathbf{x}) = \\sum_{i=1}^m 2[-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ (-y_i \\mathbf{a}_i)$.\n        10. Rewrite the gradient in matrix notation. The sum is of the form $\\sum_i \\mathbf{a}_i (\\text{scalar}_i)$, which can be expressed as $\\mathbf{A}^\\top$ times a vector.\n            $\\nabla L(\\mathbf{x}) = -2 \\sum_{i=1}^m \\mathbf{a}_i \\, y_i [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ = -2 \\mathbf{A}^\\top \\mathbf{w}$, where the $i$-th component of $\\mathbf{w}$ is $w_i = y_i [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+$.\n        11. Express $\\mathbf{w}$ using matrix/vector operations. The term $\\mathbf{a}_i^\\top \\mathbf{x}$ is the $i$-th element of $\\mathbf{A}\\mathbf{x}$. The multiplication by $-y_i$ can be written using $\\operatorname{diag}(\\mathbf{y})$.\n            So, the vector of arguments inside $[\\cdot]_+$ is $-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}$.\n            Then $\\mathbf{w} = \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}]_+$.\n        12. Substitute this back into the gradient expression:\n            $\\nabla L(\\mathbf{x}) = -2 \\mathbf{A}^\\top \\left( \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}]_+ \\right)$.\n        13. Construct the steepest-descent step at $\\mathbf{x}=\\mathbf{x}^t$:\n            $\\mathbf{z}^{t+1} = \\mathbf{x}^t - \\mu \\nabla L(\\mathbf{x}^t) = \\mathbf{x}^t + 2\\mu \\mathbf{A}^\\top \\left( \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^t]_+ \\right)$.\n        14. Apply the hard thresholding operator $H_s(\\cdot)$ to obtain the final BIHT update expression:\n            $\\mathbf{x}^{t+1} = H_s(\\mathbf{z}^{t+1})$.\n        15. Write down the final expression for the RHS as requested.\n            $H_{s}\\left( \\mathbf{x}^{t} + 2\\mu \\mathbf{A}^{\\top} \\left( \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^{t}]_{+} \\right) \\right)$.\n        This seems like a complete and logical derivation. I will carefully write it down ensuring all mathematical entities are enclosed in LaTeX delimiters.", "answer": "$$\n\\boxed{\nH_{s}\\left( \\mathbf{x}^{t} + 2\\mu \\mathbf{A}^{\\top} \\left( \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^{t}]_{+} \\right) \\right)\n}\n$$", "id": "3472923"}, {"introduction": "The success of any recovery algorithm is fundamentally limited by the quality of the initial measurements. This final practice [@problem_id:3472956] shifts our focus from the reconstruction algorithm to the design of the data acquisition system itself. You will explore a critical trade-off in quantizer design—balancing the fine-grained quantization error against the coarse information loss from saturation—and derive the optimal step size that minimizes a combined error metric for signals with Gaussian statistics.", "problem": "Consider a linear compressive sensing acquisition model with measurements $z_{i} = a_{i}^{\\top} x$ for $i = 1, \\dots, m$, where $x \\in \\mathbb{R}^{n}$ is a random signal and $a_{i} \\in \\mathbb{R}^{n}$ denotes the $i$-th row of the sensing matrix $A \\in \\mathbb{R}^{m \\times n}$. Assume $x$ is Gaussian with zero mean and covariance matrix $\\Sigma_{x}$, and the sensing matrix is designed such that all measurement variances are equal, that is $a_{i}^{\\top} \\Sigma_{x} a_{i} = \\sigma^{2}$ for a known scalar $\\sigma > 0$ and every $i$. Each measurement $z_{i}$ is quantized by a symmetric uniform mid-rise quantizer with $b$ bits and step size $\\Delta > 0$. The quantizer saturates whenever $|z_{i}| \\geq R(\\Delta)$, where the dynamic range is $R(\\Delta) = 2^{b-1} \\Delta$, and maps all values beyond the dynamic range to the nearest representable level. Let the standard normal tail function (also known as the $Q$-function) be defined by $Q(u) = \\int_{u}^{\\infty} \\frac{1}{\\sqrt{2 \\pi}} \\exp\\!\\left( -\\frac{t^{2}}{2} \\right) \\, dt$.\n\n1. Using only the measurement distribution implied by the Gaussian model and the definition of the quantizer, derive the probability of saturation for any single measurement as a function of $\\Delta$, $b$, and $\\sigma$.\n\n2. In sparse recovery from quantized measurements, two dominant contributors to the expected reconstruction error are the quantization error within the dynamic range and the information loss due to saturation. A common surrogate objective to balance these effects is\n$$\nJ(\\Delta) = \\alpha \\Delta^{2} + \\beta p_{\\mathrm{sat}}(\\Delta),\n$$\nwhere $\\alpha > 0$ and $\\beta > 0$ are known trade-off weights reflecting the impact of quantization error and saturation on the recovery, and $p_{\\mathrm{sat}}(\\Delta)$ is the saturation probability you derived in part 1. Derive, from first principles, the unique step size $\\Delta^{\\star}$ that minimizes $J(\\Delta)$, expressed in closed form in terms of $\\alpha$, $\\beta$, $\\sigma$, and $b$. Your final answer must be a single closed-form analytic expression.\n\nExpress the final answer as a symbolic expression. No rounding is required. Do not include any physical units. The boxed final answer must be the expression for $\\Delta^{\\star}$ only.", "solution": "This problem is divided into two parts. First, we must derive the saturation probability for a single quantized measurement. Second, we must use this probability to find the optimal quantizer step size $\\Delta$ that minimizes a given objective function.\n\n**Part 1: Derivation of the Saturation Probability**\n\nThe problem statement provides the linear measurement model $z_{i} = a_{i}^{\\top} x$, where the signal $x \\in \\mathbb{R}^{n}$ is a zero-mean Gaussian random vector with covariance matrix $\\Sigma_{x}$, i.e., $x \\sim \\mathcal{N}(0, \\Sigma_{x})$.\n\nA linear transformation of a Gaussian random vector is also Gaussian. Therefore, each measurement $z_{i}$ is a Gaussian random variable. Its mean is:\n$$\n\\mathbb{E}[z_{i}] = \\mathbb{E}[a_{i}^{\\top} x] = a_{i}^{\\top} \\mathbb{E}[x] = a_{i}^{\\top} 0 = 0\n$$\nThe variance of $z_{i}$ is given by:\n$$\n\\text{Var}(z_{i}) = \\mathbb{E}[(z_{i} - \\mathbb{E}[z_{i}])^{2}] = \\mathbb{E}[z_{i}^{2}] = \\mathbb{E}[(a_{i}^{\\top} x)(x^{\\top} a_{i})] = a_{i}^{\\top} \\mathbb{E}[x x^{\\top}] a_{i} = a_{i}^{\\top} \\Sigma_{x} a_{i}\n$$\nThe problem states that the sensing matrix $A$ is designed such that $a_{i}^{\\top} \\Sigma_{x} a_{i} = \\sigma^{2}$ for all $i=1, \\dots, m$. Thus, each measurement $z_{i}$ has an identical distribution: a Gaussian distribution with mean $0$ and variance $\\sigma^{2}$, denoted as $z_i \\sim \\mathcal{N}(0, \\sigma^{2})$.\n\nSaturation of the quantizer occurs when the magnitude of a measurement $|z_i|$ exceeds or equals the dynamic range limit $R(\\Delta)$. The dynamic range is given as $R(\\Delta) = 2^{b-1} \\Delta$. The saturation event is therefore $|z_{i}| \\geq 2^{b-1} \\Delta$.\n\nThe probability of saturation, $p_{\\mathrm{sat}}(\\Delta)$, is the probability of this event:\n$$\np_{\\mathrm{sat}}(\\Delta) = P(|z_{i}| \\geq 2^{b-1} \\Delta)\n$$\nSince the distribution of $z_i$ is symmetric about its mean of $0$, we can write this as:\n$$\np_{\\mathrm{sat}}(\\Delta) = P(z_{i} \\geq 2^{b-1} \\Delta) + P(z_{i} \\leq -2^{b-1} \\Delta) = 2 P(z_{i} \\geq 2^{b-1} \\Delta)\n$$\nTo express this probability using the standard normal tail function, $Q(u)$, we standardize the random variable $z_{i}$. Let $U = z_{i}/\\sigma$. Then $U$ is a standard normal random variable, $U \\sim \\mathcal{N}(0, 1)$.\nThe probability can now be written in terms of $U$:\n$$\nP(z_{i} \\geq 2^{b-1} \\Delta) = P(\\sigma U \\geq 2^{b-1} \\Delta) = P\\left(U \\geq \\frac{2^{b-1} \\Delta}{\\sigma}\\right)\n$$\nBy the definition of the $Q$-function, $Q(u) = P(U \\geq u)$, we have:\n$$\nP(z_{i} \\geq 2^{b-1} \\Delta) = Q\\left(\\frac{2^{b-1} \\Delta}{\\sigma}\\right)\n$$\nTherefore, the probability of saturation for any single measurement is:\n$$\np_{\\mathrm{sat}}(\\Delta) = 2 Q\\left(\\frac{2^{b-1} \\Delta}{\\sigma}\\right)\n$$\n\n**Part 2: Derivation of the Optimal Step Size $\\Delta^{\\star}$**\n\nWe are asked to find the step size $\\Delta^{\\star}$ that minimizes the objective function $J(\\Delta)$:\n$$\nJ(\\Delta) = \\alpha \\Delta^{2} + \\beta p_{\\mathrm{sat}}(\\Delta)\n$$\nSubstituting the expression for $p_{\\mathrm{sat}}(\\Delta)$ from Part 1 gives:\n$$\nJ(\\Delta) = \\alpha \\Delta^{2} + 2 \\beta Q\\left(\\frac{2^{b-1} \\Delta}{\\sigma}\\right)\n$$\nTo find the minimum, we compute the derivative of $J(\\Delta)$ with respect to $\\Delta$ and set it to zero. We must find $\\frac{dJ}{d\\Delta}$.\nThe derivative of the first term is $2 \\alpha \\Delta$. For the second term, we use the chain rule and the derivative of the $Q$-function. The $Q$-function is defined as $Q(u) = \\int_{u}^{\\infty} \\frac{1}{\\sqrt{2 \\pi}} \\exp(-\\frac{t^{2}}{2}) dt$. By the Fundamental Theorem of Calculus (Leibniz rule), its derivative is $\\frac{dQ(u)}{du} = -\\frac{1}{\\sqrt{2 \\pi}} \\exp(-\\frac{u^{2}}{2})$.\n\nLet $u(\\Delta) = \\frac{2^{b-1} \\Delta}{\\sigma}$. Then $\\frac{du}{d\\Delta} = \\frac{2^{b-1}}{\\sigma}$. The derivative of the second term is:\n$$\n\\frac{d}{d\\Delta}\\left[2 \\beta Q\\left(\\frac{2^{b-1} \\Delta}{\\sigma}\\right)\\right] = 2 \\beta \\cdot \\frac{dQ(u)}{du} \\cdot \\frac{du}{d\\Delta} = 2 \\beta \\left( -\\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{u^{2}}{2}\\right) \\right) \\left( \\frac{2^{b-1}}{\\sigma} \\right)\n$$\n$$\n= -\\frac{2 \\beta \\cdot 2^{b-1}}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2} \\left(\\frac{2^{b-1} \\Delta}{\\sigma}\\right)^{2}\\right) = -\\frac{\\beta \\cdot 2^{b}}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{2\\sigma^{2}}\\right)\n$$\nNow, we set the total derivative $\\frac{dJ}{d\\Delta}$ to zero:\n$$\n\\frac{dJ}{d\\Delta} = 2 \\alpha \\Delta - \\frac{\\beta \\cdot 2^{b}}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{2\\sigma^{2}}\\right) = 0\n$$\nRearranging the terms, we obtain:\n$$\n2 \\alpha \\Delta = \\frac{\\beta \\cdot 2^{b}}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{2\\sigma^{2}}\\right)\n$$\nThis is a transcendental equation. To solve for $\\Delta$, we manipulate it into a form compatible with the Lambert W function, which is defined by $z = W(z)\\exp(W(z))$.\nLet us square both sides of the equation:\n$$\n4 \\alpha^{2} \\Delta^{2} = \\frac{\\beta^{2} \\cdot 2^{2b}}{\\sigma^{2} \\cdot 2\\pi} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{\\sigma^{2}}\\right)\n$$\n$$\n4 \\alpha^{2} \\Delta^{2} = \\frac{\\beta^{2} \\cdot 2^{2b-1}}{\\pi\\sigma^{2}} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{\\sigma^{2}}\\right)\n$$\nLet's define a new variable $Y = \\frac{2^{2b-2} \\Delta^{2}}{\\sigma^{2}}$. This implies $\\Delta^{2} = \\frac{\\sigma^{2}}{2^{2b-2}}Y$. Substituting this into the squared equation:\n$$\n4 \\alpha^{2} \\left(\\frac{\\sigma^{2}}{2^{2b-2}}Y\\right) = \\frac{\\beta^{2} \\cdot 2^{2b-1}}{\\pi\\sigma^{2}} \\exp(-Y)\n$$\n$$\n\\frac{4 \\pi \\alpha^{2} \\sigma^{4}}{\\beta^{2} \\cdot 2^{2b-1} \\cdot 2^{2b-2}} Y = \\exp(-Y)\n$$\n$$\n\\frac{4 \\pi \\alpha^{2} \\sigma^{4}}{\\beta^{2} \\cdot 2^{4b-3}} Y = \\exp(-Y)\n$$\n$$\n\\frac{\\pi \\alpha^{2} \\sigma^{4}}{\\beta^{2} \\cdot 2^{4b-5}} Y = \\exp(-Y)\n$$\nMultiplying by $\\exp(Y)$ and rearranging gives the canonical form for the Lambert W function:\n$$\nY \\exp(Y) = \\frac{\\beta^{2} \\cdot 2^{4b-5}}{\\pi \\alpha^{2} \\sigma^{4}}\n$$\nApplying the Lambert W function, we solve for $Y$:\n$$\nY = W\\left(\\frac{\\beta^{2} \\cdot 2^{4b-5}}{\\pi \\alpha^{2} \\sigma^{4}}\\right)\n$$\nWe substitute back $Y = \\frac{2^{2b-2} (\\Delta^{\\star})^{2}}{\\sigma^{2}}$ to find the optimal step size $\\Delta^{\\star}$:\n$$\n\\frac{2^{2b-2} (\\Delta^{\\star})^{2}}{\\sigma^{2}} = W\\left(\\frac{\\beta^{2} \\cdot 2^{4b-5}}{\\pi \\alpha^{2} \\sigma^{4}}\\right)\n$$\n$$\n(\\Delta^{\\star})^{2} = \\frac{\\sigma^{2}}{2^{2b-2}} W\\left(\\frac{\\beta^{2} \\cdot 2^{4b-5}}{\\pi \\alpha^{2} \\sigma^{4}}\\right)\n$$\nSince the step size $\\Delta$ must be positive, we take the positive square root:\n$$\n\\Delta^{\\star} = \\sqrt{\\frac{\\sigma^{2}}{2^{2b-2}} W\\left(\\frac{\\beta^{2} \\cdot 2^{4b-5}}{\\pi \\alpha^{2} \\sigma^{4}}\\right)} = \\frac{\\sigma}{2^{b-1}} \\sqrt{W\\left(\\frac{\\beta^{2} \\cdot 2^{4b-5}}{\\pi \\alpha^{2} \\sigma^{4}}\\right)}\n$$\nTo confirm this is a unique minimum, we check the second derivative of $J(\\Delta)$:\n$$\n\\frac{d^2 J}{d\\Delta^2} = \\frac{d}{d\\Delta}\\left[2 \\alpha \\Delta - \\frac{\\beta \\cdot 2^{b}}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{2\\sigma^{2}}\\right)\\right]\n$$\n$$\n\\frac{d^2 J}{d\\Delta^2} = 2 \\alpha - \\frac{\\beta \\cdot 2^{b}}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{2\\sigma^{2}}\\right) \\left(-\\frac{2 \\cdot 2^{2b-2} \\Delta}{2\\sigma^{2}}\\right)\n$$\n$$\n\\frac{d^2 J}{d\\Delta^2} = 2 \\alpha + \\frac{\\beta \\cdot 2^{b} \\cdot 2^{2b-2} \\Delta}{\\sigma^3\\sqrt{2\\pi}} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{2\\sigma^{2}}\\right)\n$$\nFor $\\Delta > 0$, and given that $\\alpha > 0$, $\\beta > 0$, $\\sigma > 0$, all terms in the expression for the second derivative are positive. Thus, $\\frac{d^2 J}{d\\Delta^2} > 0$ for all $\\Delta > 0$, which means $J(\\Delta)$ is a strictly convex function on $\\Delta > 0$. Therefore, the critical point we found corresponds to a unique global minimum. The argument of the Lambert W function is positive, so its output is a unique positive real number, ensuring $\\Delta^{\\star}$ is a unique positive real number.", "answer": "$$\\boxed{\\frac{\\sigma}{2^{b-1}} \\sqrt{W\\left(\\frac{\\beta^{2} 2^{4b-5}}{\\pi \\alpha^{2} \\sigma^{4}}\\right)}}$$", "id": "3472956"}]}