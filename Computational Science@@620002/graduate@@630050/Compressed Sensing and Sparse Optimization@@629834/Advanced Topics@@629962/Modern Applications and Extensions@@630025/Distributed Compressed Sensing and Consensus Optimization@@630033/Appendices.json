{"hands_on_practices": [{"introduction": "The Alternating Direction Method of Multipliers (ADMM) is a powerful framework for solving large-scale and distributed optimization problems. This first exercise provides practice with the fundamental mechanics of ADMM by having you derive the update steps for the consensus LASSO problem. Mastering this derivation is key to understanding how global optimization problems can be decomposed into a series of local computations and a coordinating consensus step, a core pattern in distributed machine learning and signal processing [@problem_id:3444486].", "problem": "Consider a distributed sensing network with $\\{A_{\\ell} \\in \\mathbb{R}^{m_{\\ell} \\times n},\\, y_{\\ell} \\in \\mathbb{R}^{m_{\\ell}}\\}_{\\ell=1}^{L}$, where $L \\in \\mathbb{N}$ agents each measure a common $n$-dimensional signal through the linear model $y_{\\ell} \\approx A_{\\ell} z^{\\star}$ with additive noise. The goal is to estimate a sparse consensus vector $z \\in \\mathbb{R}^{n}$ by solving the consensus Least Absolute Shrinkage and Selection Operator (LASSO) problem\n$$\n\\min_{\\{x_{\\ell}\\}_{\\ell=1}^{L},\\, z \\in \\mathbb{R}^{n}} \\sum_{\\ell=1}^{L} \\frac{1}{2} \\|A_{\\ell} x_{\\ell} - y_{\\ell}\\|_{2}^{2} + \\lambda \\|z\\|_{1} \\quad \\text{subject to} \\quad x_{\\ell} = z \\quad \\text{for all } \\ell,\n$$\nwhere $\\lambda > 0$ is the $\\ell_{1}$ regularization parameter. Use the Alternating Direction Method of Multipliers (ADMM) with scaled dual variables $\\{u_{\\ell}\\}_{\\ell=1}^{L}$ and penalty parameter $\\rho > 0$ to enforce consensus.\n\nStarting from the constrained formulation and the corresponding augmented Lagrangian with scaled dual variables, derive the explicit closed-form ADMM updates for the local primal variables $\\{x_{\\ell}\\}$ and the global consensus variable $z$ at iteration $k+1$ using only the problem data $\\{A_{\\ell}, y_{\\ell}\\}$, the current iterates $\\{z^{k}, u_{\\ell}^{k}\\}$, and the parameters $\\lambda$ and $\\rho$. Assume that for each $\\ell$, the matrix $A_{\\ell}^{\\top} A_{\\ell} + \\rho I$ is invertible, where $I$ denotes the $n \\times n$ identity matrix.\n\nExpress your final answer as a single closed-form analytic expression containing the three update mappings for the local primal variables, the global consensus variable, and the scaled dual variables. Your answer must be symbolic (no numerical evaluation), and it must be written in terms of $A_{\\ell}$, $y_{\\ell}$, $z^{k}$, $u_{\\ell}^{k}$, $\\lambda$, $\\rho$, and $L$. Do not provide inequalities or equations in the final answer; provide only the analytic expressions for the updates.", "solution": "The problem asks for the derivation of the Alternating Direction Method of Multipliers (ADMM) updates for a distributed consensus LASSO formulation.\n\nThe optimization problem is given by:\n$$ \\min_{\\{x_{\\ell}\\}_{\\ell=1}^{L},\\, z \\in \\mathbb{R}^{n}} \\sum_{\\ell=1}^{L} \\frac{1}{2} \\|A_{\\ell} x_{\\ell} - y_{\\ell}\\|_{2}^{2} + \\lambda \\|z\\|_{1} \\quad \\text{subject to} \\quad x_{\\ell} = z \\quad \\text{for all } \\ell \\in \\{1, \\ldots, L\\} $$\nThis is a consensus problem where each agent $\\ell$ has a local variable $x_{\\ell}$ and a local data-fitting term $f_{\\ell}(x_{\\ell}) = \\frac{1}{2} \\|A_{\\ell} x_{\\ell} - y_{\\ell}\\|_{2}^{2}$. All agents must agree on a single consensus variable $z$, which is regularized by the $\\ell_1$-norm to promote sparsity.\n\nThe ADMM algorithm addresses this by forming the augmented Lagrangian. For this problem, using the scaled dual variables $\\{u_{\\ell}\\}$, the augmented Lagrangian $\\mathcal{L}_{\\rho}$ is:\n$$ \\mathcal{L}_{\\rho}(\\{x_{\\ell}\\}, z, \\{u_{\\ell}\\}) = \\sum_{\\ell=1}^{L} \\left( \\frac{1}{2} \\|A_{\\ell} x_{\\ell} - y_{\\ell}\\|_{2}^{2} \\right) + \\lambda \\|z\\|_{1} + \\sum_{\\ell=1}^{L} \\frac{\\rho}{2} \\|x_{\\ell} - z + u_{\\ell}\\|_{2}^{2} - \\sum_{\\ell=1}^{L} \\frac{\\rho}{2} \\|u_{\\ell}\\|_{2}^{2} $$\nHere, $\\rho > 0$ is the penalty parameter. The ADMM algorithm proceeds by iterating three update steps at each iteration $k+1$:\n1.  Minimizing $\\mathcal{L}_{\\rho}$ with respect to the local variables $\\{x_{\\ell}\\}$.\n2.  Minimizing $\\mathcal{L}_{\\rho}$ with respect to the global consensus variable $z$.\n3.  Updating the scaled dual variables $\\{u_{\\ell}\\}$.\n\nLet's derive each update step explicitly.\n\n**1. $x_{\\ell}$-update (Local variable update)**\nAt iteration $k+1$, we update each $x_{\\ell}$ by minimizing the augmented Lagrangian with respect to $x_{\\ell}$, keeping other variables fixed at their most recent values ($z^k$, $u_{\\ell}^k$). Due to the structure of the Lagrangian, the minimization with respect to $\\{x_{\\ell}\\}$ decouples into $L$ independent problems, one for each agent $\\ell$.\n$$ x_{\\ell}^{k+1} = \\arg\\min_{x_{\\ell}} \\left( \\frac{1}{2} \\|A_{\\ell} x_{\\ell} - y_{\\ell}\\|_{2}^{2} + \\frac{\\rho}{2} \\|x_{\\ell} - z^{k} + u_{\\ell}^{k}\\|_{2}^{2} \\right) $$\nThis is a minimization of an unconstrained, strictly convex quadratic function of $x_{\\ell}$. The minimum is found by setting the gradient with respect to $x_{\\ell}$ to zero. Let the objective be $J(x_{\\ell})$.\n$$ \\nabla_{x_{\\ell}} J(x_{\\ell}) = A_{\\ell}^{\\top}(A_{\\ell} x_{\\ell} - y_{\\ell}) + \\rho(x_{\\ell} - z^{k} + u_{\\ell}^{k}) = 0 $$\nRearranging the terms to solve for $x_{\\ell}$:\n$$ A_{\\ell}^{\\top}A_{\\ell} x_{\\ell} + \\rho I x_{\\ell} = A_{\\ell}^{\\top}y_{\\ell} + \\rho(z^{k} - u_{\\ell}^{k}) $$\n$$ (A_{\\ell}^{\\top}A_{\\ell} + \\rho I) x_{\\ell} = A_{\\ell}^{\\top}y_{\\ell} + \\rho(z^{k} - u_{\\ell}^{k}) $$\nAs per the problem statement, the matrix $(A_{\\ell}^{\\top}A_{\\ell} + \\rho I)$ is invertible. Thus, we obtain the closed-form update for $x_{\\ell}^{k+1}$:\n$$ x_{\\ell}^{k+1} = (A_{\\ell}^{\\top}A_{\\ell} + \\rho I)^{-1} \\left( A_{\\ell}^{\\top}y_{\\ell} + \\rho(z^{k} - u_{\\ell}^{k}) \\right) $$\nThis update is performed in parallel for each agent $\\ell = 1, \\dots, L$.\n\n**2. $z$-update (Global consensus update)**\nNext, we update the consensus variable $z$ by minimizing $\\mathcal{L}_{\\rho}$ with respect to $z$, using the newly computed values $\\{x_{\\ell}^{k+1}\\}$:\n$$ z^{k+1} = \\arg\\min_{z} \\left( \\lambda \\|z\\|_{1} + \\sum_{\\ell=1}^{L} \\frac{\\rho}{2} \\|x_{\\ell}^{k+1} - z + u_{\\ell}^{k}\\|_{2}^{2} \\right) $$\nWe can rewrite the quadratic term as:\n$$ \\sum_{\\ell=1}^{L} \\frac{\\rho}{2} \\|x_{\\ell}^{k+1} - z + u_{\\ell}^{k}\\|_{2}^{2} = \\frac{\\rho}{2} \\sum_{\\ell=1}^{L} \\|z - (x_{\\ell}^{k+1} + u_{\\ell}^{k})\\|_{2}^{2} $$\nBy completing the square with respect to $z$, this sum can be expressed in terms of the average $\\bar{v}^{k+1} = \\frac{1}{L}\\sum_{\\ell=1}^{L}(x_{\\ell}^{k+1} + u_{\\ell}^{k})$:\n$$ \\frac{\\rho}{2} \\sum_{\\ell=1}^{L} \\|z - (x_{\\ell}^{k+1} + u_{\\ell}^{k})\\|_{2}^{2} = \\frac{L\\rho}{2} \\|z - \\bar{v}^{k+1}\\|_{2}^{2} + \\text{constant terms w.r.t. } z $$\nThus, the $z$-minimization problem becomes:\n$$ z^{k+1} = \\arg\\min_{z} \\left( \\lambda \\|z\\|_{1} + \\frac{L\\rho}{2} \\|z - \\bar{v}^{k+1}\\|_{2}^{2} \\right) $$\nThis is the standard form for the proximal operator of the $\\ell_1$-norm. Specifically, it is equivalent to:\n$$ z^{k+1} = \\arg\\min_{z} \\left( \\frac{\\lambda}{L\\rho} \\|z\\|_{1} + \\frac{1}{2} \\left\\|z - \\frac{1}{L}\\sum_{\\ell=1}^{L}(x_{\\ell}^{k+1} + u_{\\ell}^{k})\\right\\|_{2}^{2} \\right) $$\nThe solution is given by the soft-thresholding operator, denoted by $S_{\\gamma}(\\cdot)$, where for a vector $v$ and threshold $\\gamma > 0$, $[S_{\\gamma}(v)]_i = \\text{sign}(v_i) \\max(|v_i| - \\gamma, 0)$.\nThe update for $z^{k+1}$ is:\n$$ z^{k+1} = S_{\\frac{\\lambda}{L\\rho}} \\left( \\frac{1}{L}\\sum_{j=1}^{L}(x_{j}^{k+1} + u_{j}^{k}) \\right) $$\nwhere the index $j$ is used for summation to avoid ambiguity.\n\n**3. $u_{\\ell}$-update (Dual variable update)**\nFinally, the scaled dual variables are updated using the primal residuals $r_{\\ell}^{k+1} = x_{\\ell}^{k+1} - z^{k+1}$:\n$$ u_{\\ell}^{k+1} = u_{\\ell}^{k} + x_{\\ell}^{k+1} - z^{k+1} $$\nThis update is performed in parallel for each agent $\\ell = 1, \\dots, L$.\n\nIn summary, the three sequential update mappings for iteration $k+1$ are:\n1.  $x_{\\ell}^{k+1} = (A_{\\ell}^{\\top}A_{\\ell} + \\rho I)^{-1}(A_{\\ell}^{\\top}y_{\\ell} + \\rho(z^{k} - u_{\\ell}^{k}))$\n2.  $z^{k+1} = S_{\\frac{\\lambda}{L\\rho}}\\left(\\frac{1}{L}\\sum_{j=1}^{L}(x_{j}^{k+1} + u_{j}^{k})\\right)$\n3.  $u_{\\ell}^{k+1} = u_{\\ell}^{k} + x_{\\ell}^{k+1} - z^{k+1}$\n\nThe problem requires these three update mappings as a single expression.", "answer": "$$ \\boxed{ \\begin{pmatrix} (A_{\\ell}^{\\top}A_{\\ell} + \\rho I)^{-1}(A_{\\ell}^{\\top}y_{\\ell} + \\rho(z^{k} - u_{\\ell}^{k})) & S_{\\frac{\\lambda}{L\\rho}}\\left(\\frac{1}{L}\\sum_{j=1}^{L}(x_{j}^{k+1} + u_{j}^{k})\\right) & u_{\\ell}^{k} + x_{\\ell}^{k+1} - z^{k+1} \\end{pmatrix} } $$", "id": "3444486"}, {"introduction": "While solving an optimization problem with a given set of parameters is a core skill, selecting those parameters in the first place is an equally critical challenge, especially in a distributed setting. This practice explores a fully decentralized, data-driven approach for tuning the regularization parameter in a sparse recovery problem by combining Stein's Unbiased Risk Estimate (SURE) with an average consensus protocol. This exercise bridges statistical estimation theory and distributed optimization, demonstrating how a network of agents can collaboratively perform model selection without a central coordinator [@problem_id:3444471].", "problem": "You are tasked with designing and implementing an adaptive, distributed procedure for selecting the soft-threshold parameter and estimating the sparsity level of an unknown signal from noisy and distributed observations, using Stein’s Unbiased Risk Estimate (SURE) and average consensus.\n\nGiven a network of agents indexed by $i \\in \\{1,\\dots,N\\}$, each agent observes the same unknown vector $x_{0} \\in \\mathbb{R}^{n}$ through the Gaussian denoising model\n$$\nz_{i} = x_{0} + \\varepsilon_{i}, \\quad \\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma_{i}^{2} I_{n}),\n$$\nwhere $\\sigma_{i} > 0$ is known to agent $i$. Each agent forms a local estimator using the element-wise soft-thresholding operator $\\eta(\\cdot; \\lambda)$ at a common scalar threshold $\\lambda \\ge 0$:\n$$\n\\hat{x}_{i}(\\lambda) = \\eta(z_{i}; \\lambda), \\quad \\text{with} \\quad \\eta(t; \\lambda) = \\mathrm{sign}(t)\\,\\max(|t| - \\lambda, 0).\n$$\nThe objective is to select $\\lambda$ that minimizes the average mean-squared error over the network:\n$$\n\\min_{\\lambda \\ge 0} \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\left[ \\| \\hat{x}_{i}(\\lambda) - x_{0} \\|_{2}^{2} \\right],\n$$\nusing only local computations and neighbor exchanges, and then to estimate the sparsity level $s = \\|x_{0}\\|_{0}$ via distributed consensus on local support-size estimates.\n\nFoundational base you must use:\n- Gaussian denoising model and properties of the normal distribution.\n- Stein’s Unbiased Risk Estimate (SURE) for independent identically distributed Gaussian noise: for an estimator $\\delta(Z)$ of $x_{0}$ based on $Z \\sim x_{0} + \\mathcal{N}(0,\\sigma^{2} I_{n})$, an unbiased estimator of $\\mathbb{E}\\left[\\|\\delta(Z) - x_{0}\\|_{2}^{2}\\right]$ is constructed from observable quantities by applying Stein’s lemma and the divergence of $\\delta(\\cdot)$ wherever it exists.\n- Average consensus over a connected undirected graph using a doubly-stochastic weight matrix: repeated linear iterations of local state vectors converge to the network-wide arithmetic average.\n\nYour implementation must:\n1. For each agent $i$, compute the local SURE curve on a fixed grid $\\Lambda = \\{\\lambda_{0}, \\lambda_{1}, \\dots, \\lambda_{G-1}\\}$ using only local data $(z_{i}, \\sigma_{i})$ and the definition of SURE via Stein’s lemma and the divergence of the soft-thresholding estimator. Do not use any formula that has not been derived from the above base.\n2. Perform average consensus over the network on the SURE curves (as vectors of length $G$) using a symmetric, doubly-stochastic Metropolis–Hastings weight matrix constructed from the given graph. Iterate the linear consensus update for a specified number of steps and use the resulting approximate network-average SURE curve to select\n$$\n\\lambda^{\\star} \\in \\arg\\min_{\\lambda \\in \\Lambda} \\frac{1}{N} \\sum_{i=1}^{N} \\mathrm{SURE}_{i}(\\lambda).\n$$\n3. At the selected $\\lambda^{\\star}$, each agent computes its local soft-thresholded estimate $\\hat{x}_{i}(\\lambda^{\\star})$ and the local support-size estimate $s_{i}(\\lambda^{\\star}) = \\|\\hat{x}_{i}(\\lambda^{\\star})\\|_{0}$ with a numerical threshold for zero testing specified in this problem.\n4. Perform average consensus over the scalars $\\{s_{i}(\\lambda^{\\star})\\}_{i=1}^{N}$ to obtain a network consensus estimate of the sparsity level, denoted $\\hat{s}$, by rounding the consensus average to the nearest integer.\n5. Return, for each test case, a triplet containing the selected threshold $\\lambda^{\\star}$ (as a floating-point number), the consensus sparsity estimate $\\hat{s}$ (as an integer), and a boolean indicating whether the estimate is acceptable according to the acceptance criterion defined for each test case below.\n\nNumerical and algorithmic specifications:\n- Use a common grid $\\Lambda$ with $G = 401$ uniformly spaced points from $0$ to $4$ inclusive, i.e., $\\lambda_{g} = \\frac{4g}{400}$ for $g \\in \\{0,\\dots,400\\}$.\n- Use a zero-testing tolerance of $\\tau = 10^{-6}$ to define $\\|\\cdot\\|_{0}$ numerically, i.e., count an entry as nonzero if its absolute value exceeds $\\tau$.\n- Construct the Metropolis–Hastings weights for an undirected graph with adjacency matrix $A$ as follows: for $i \\neq j$, set $W_{ij} = \\frac{1}{1 + \\max\\{d_{i}, d_{j}\\}}$ if $(i,j)$ is an edge and $0$ otherwise, where $d_{i}$ is the degree of node $i$; for the diagonal, set $W_{ii} = 1 - \\sum_{j \\neq i} W_{ij}$. Use $T = 50$ consensus iterations for both SURE curves and sparsity counts.\n- All computations are dimensionless; no physical units apply.\n\nTest suite:\nImplement three test cases as specified below. In all cases, when generating the random support and noise, use the specified random seed for reproducibility.\n\n- Test case $1$ (happy path):\n  - $n = 128$, $N = 6$, true sparsity $s = 10$, nonzero amplitudes equal to a constant $a = 3$, with random signs.\n  - Noise standard deviations $\\sigma_{i} = 0.25$ for all $i$.\n  - Graph: ring topology on $N = 6$ nodes, i.e., edges between $i$ and $i \\pm 1$ modulo $N$.\n  - Random seed $12345$ for support selection, sign selection, and noise.\n  - Acceptance criterion: report success if $|\\hat{s} - s| \\le 2$.\n\n- Test case $2$ (boundary: zero signal):\n  - $n = 128$, $N = 4$, true sparsity $s = 0$, $x_{0} = 0$.\n  - Noise standard deviations $\\sigma_{i} = 0.4$ for all $i$.\n  - Graph: fully connected on $N = 4$ nodes.\n  - Random seed $54321$ for noise.\n  - Acceptance criterion: report success if $\\hat{s} = s$.\n\n- Test case $3$ (heterogeneous noise and topology):\n  - $n = 64$, $N = 5$, true sparsity $s = 5$, nonzero amplitudes equal to a constant $a = 2.5$, with random signs.\n  - Noise standard deviations $\\sigma = [0.3, 0.3, 0.8, 0.3, 0.3]$.\n  - Graph: line topology on $N = 5$ nodes, i.e., edges between $i$ and $i+1$.\n  - Random seed $24680$ for support selection, sign selection, and noise.\n  - Acceptance criterion: report success if $|\\hat{s} - s| \\le 2$.\n\nOutput format:\nYour program should produce a single line of output containing the results of all test cases as a list of triplets, one per test case, where each triplet is of the form $[\\lambda^{\\star}, \\hat{s}, \\mathrm{success}]$. The overall output must be a JSON-like single-line string of the form\n$$\n\\big[ [\\lambda^{\\star}_{1}, \\hat{s}_{1}, \\mathrm{success}_{1}], [\\lambda^{\\star}_{2}, \\hat{s}_{2}, \\mathrm{success}_{2}], [\\lambda^{\\star}_{3}, \\hat{s}_{3}, \\mathrm{success}_{3}] \\big],\n$$\nwith no additional text.\n\nYour implementation must be fully self-contained and must not require any user input. All numerical values must be computed by your code as specified above using the given seeds. The final printed line must adhere exactly to the specified format.", "solution": "The problem requires the design and implementation of a distributed algorithm for selecting a soft-thresholding parameter $\\lambda$ and estimating the sparsity of an unknown signal $x_0 \\in \\mathbb{R}^n$. The system consists of a network of $N$ agents, where each agent $i$ possesses a noisy observation $z_i = x_0 + \\varepsilon_i$, with $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2 I_n)$.\n\nThe solution proceeds by first validating the problem statement and then, upon confirmation of its validity, providing a detailed derivation of the algorithm followed by its implementation.\n\n### Problem Validation\n\n**Step 1: Extraction of Givens**\n- **Signal Model**: $z_{i} = x_{0} + \\varepsilon_{i}$, for $i \\in \\{1, \\dots, N\\}$, where $x_{0} \\in \\mathbb{R}^{n}$ is an unknown signal, $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma_{i}^{2} I_{n})$ is Gaussian noise, and $\\sigma_{i} > 0$ is a known standard deviation.\n- **Estimator**: Element-wise soft-thresholding $\\hat{x}_{i}(\\lambda) = \\eta(z_{i}; \\lambda)$, with $\\eta(t; \\lambda) = \\mathrm{sign}(t)\\,\\max(|t| - \\lambda, 0)$.\n- **Objective**: Minimize the network-average Mean-Squared Error (MSE): $\\min_{\\lambda \\ge 0} \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\left[ \\| \\hat{x}_{i}(\\lambda) - x_{0} \\|_{2}^{2} \\right]$.\n- **Sparsity**: $s = \\|x_{0}\\|_{0}$.\n- **Methodology**: Use Stein’s Unbiased Risk Estimate (SURE) and average consensus.\n- **Numerical Specifications**:\n    - Lambda grid $\\Lambda$: $G=401$ points in $[0, 4]$, $\\lambda_g = 4g/400$.\n    - Zero-norm tolerance: $\\tau = 10^{-6}$.\n    - Consensus iterations: $T=50$.\n    - Weight matrix: Metropolis-Hastings for an undirected graph with adjacency matrix $A$ and degrees $d_i$: $W_{ij} = (1 + \\max\\{d_i, d_j\\})^{-1}$ if $(i,j)$ is an edge, and $W_{ii} = 1 - \\sum_{j \\neq i} W_{ij}$.\n- **Test Cases**: Three specific test cases are provided with all parameters ($n, N, s, a, \\sigma_i$), graph topologies (ring, fully connected, line), random seeds, and success criteria.\n\n**Step 2: Validation Check**\nThe problem is scientifically and mathematically well-grounded, relying on standard principles of statistical signal processing (Gaussian denoising, SURE) and distributed optimization (average consensus). The formulation is self-contained, with all necessary parameters, models, and numerical specifications clearly defined. The test cases are concrete and reproducible. The question is formal, objective, and does not violate any scientific principles or contain logical contradictions. It represents a standard, non-trivial problem in the field of distributed learning and optimization.\n\n**Step 3: Verdict**\nThe problem is **valid**. A full solution will be developed.\n\n### Algorithmic Solution\n\nThe core of the problem is to minimize the network-average risk, which is a function of the threshold parameter $\\lambda$:\n$$\nJ(\\lambda) = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\left[ \\| \\hat{x}_{i}(\\lambda) - x_{0} \\|_{2}^{2} \\right]\n$$\nThis objective is not directly computable because it involves an expectation and the unknown signal $x_0$. We circumvent this by using Stein's Unbiased Risk Estimate (SURE), which provides a data-driven, unbiased estimate of the MSE for each agent.\n\n**1. Stein's Unbiased Risk Estimate (SURE)**\nFor an agent $i$ with observation $z_i \\sim \\mathcal{N}(x_0, \\sigma_i^2 I_n)$, the risk $R_i(\\lambda) = \\mathbb{E}\\left[ \\| \\eta(z_i; \\lambda) - x_{0} \\|_{2}^{2} \\right]$ can be estimated without knowledge of $x_0$. According to Stein's lemma, for a weakly differentiable function $\\delta:\\mathbb{R}^n \\to \\mathbb{R}^n$, an unbiased estimate of the risk $\\mathbb{E}[\\|\\delta(z_i) - x_0\\|_2^2]$ is given by:\n$$\n\\mathrm{SURE}_i(\\lambda) = \\| \\delta(z_i) - z_i \\|_{2}^{2} - n\\sigma_{i}^{2} + 2\\sigma_{i}^{2} \\nabla_{z_i} \\cdot \\delta(z_i)\n$$\nIn our case, the estimator is $\\delta(z_i) = \\hat{x}_i(\\lambda) = \\eta(z_i; \\lambda)$, which is applied element-wise. The divergence term $\\nabla_{z_i} \\cdot \\eta(z_i; \\lambda)$ is the sum of the partial derivatives of its components:\n$$\n\\nabla_{z_i} \\cdot \\eta(z_i; \\lambda) = \\sum_{j=1}^{n} \\frac{\\partial}{\\partial z_{ij}} \\eta(z_{ij}; \\lambda)\n$$\nThe derivative of the scalar soft-thresholding function $\\eta(t; \\lambda)$ is $\\frac{d}{dt}\\eta(t; \\lambda) = 1$ for $|t| > \\lambda$ and $0$ for $|t| < \\lambda$. At the points $|t| = \\lambda$ where the derivative is undefined, the set has Lebesgue measure zero for a continuous random variable $t$, so we can ignore it for integration purposes. The derivative is thus the indicator function $\\mathbb{I}(|t| > \\lambda)$.\nTherefore, the divergence is the number of components of $z_i$ whose magnitude exceeds $\\lambda$:\n$$\n\\nabla_{z_i} \\cdot \\eta(z_i; \\lambda) = \\sum_{j=1}^{n} \\mathbb{I}(|z_{ij}| > \\lambda) = \\| \\eta(z_i; \\lambda) \\|_{0}\n$$\nwhere $\\|\\cdot\\|_0$ counts the number of non-zero elements. Substituting this into the SURE formula gives the local risk estimate for agent $i$:\n$$\n\\mathrm{SURE}_i(\\lambda) = \\| \\eta(z_i; \\lambda) - z_i \\|_{2}^{2} - n\\sigma_i^2 + 2\\sigma_i^2 \\| \\eta(z_i; \\lambda) \\|_{0}\n$$\nThe first term, the squared error between the estimate and the observation, can be expressed more simply. For each component $j$, if $|z_{ij}| \\le \\lambda$, $\\eta(z_{ij};\\lambda) = 0$, so the squared difference is $z_{ij}^2$. If $|z_{ij}| > \\lambda$, $\\eta(z_{ij};\\lambda) = z_{ij} - \\lambda\\,\\mathrm{sign}(z_{ij})$, so the squared difference is $\\lambda^2$. This can be written compactly as $\\min(|z_{ij}|, \\lambda)^2$. The full term is thus $\\sum_{j=1}^n \\min(|z_{ij}|, \\lambda)^2$.\n\n**2. Distributed Optimization via Average Consensus**\nThe global objective can now be approximated by minimizing the average of the local SURE functions:\n$$\n\\min_{\\lambda \\ge 0} \\frac{1}{N} \\sum_{i=1}^{N} \\mathrm{SURE}_{i}(\\lambda)\n$$\nSince each agent $i$ can only compute its own $\\mathrm{SURE}_i(\\lambda)$, they must collaborate to find the minimum of the average. This is achieved using an average consensus algorithm. The agents operate over a connected, undirected graph. They iteratively update their local state by taking a weighted average of their own state and their neighbors' states. The process is governed by a doubly-stochastic weight matrix $W$:\n$$\nv_i^{(k+1)} = \\sum_{j=1}^N W_{ij} v_j^{(k)}\n$$\nwhere $v_i^{(k)}$ is the state of agent $i$ at iteration $k$. For a connected non-bipartite graph, as $k \\to \\infty$, $v_i^{(k)} \\to \\frac{1}{N} \\sum_{j=1}^N v_j^{(0)}$ for all $i$. The problem specifies the Metropolis-Hastings rule for constructing $W$, which guarantees it is symmetric and doubly-stochastic, ensuring convergence to the arithmetic average.\n\n**3. The Complete Distributed Procedure**\nThe algorithm proceeds in two main phases, both involving consensus.\n\n**Phase 1: Optimal Threshold Selection**\n1.  **Local SURE Curve Computation**: Each agent $i$ computes its SURE values for every $\\lambda$ in the predefined grid $\\Lambda = \\{\\lambda_0, \\dots, \\lambda_{G-1}\\}$. This results in a local SURE vector $S_i \\in \\mathbb{R}^G$, where $(S_i)_g = \\mathrm{SURE}_i(\\lambda_g)$.\n2.  **Consensus on SURE Curves**: The agents run $T$ iterations of average consensus to compute the average SURE curve. Let $S^{(k)}$ be the $N \\times G$ matrix where the $i$-th row is the SURE vector of agent $i$ at iteration $k$. The update rule is $S^{(k+1)} = W S^{(k)}$, with $S^{(0)}$ being the matrix of initially computed local SURE curves.\n3.  **Optimal Threshold ($\\lambda^\\star$) Identification**: After $T$ iterations, each row of $S^{(T)}$ is an approximation of the average SURE curve $\\bar{S} = \\frac{1}{N}\\sum_i S_i$. Each agent can then independently find the optimal threshold by finding the minimum of its local result: $\\lambda^\\star = \\Lambda[\\arg\\min_{g} (S_i^{(T)})_g]$.\n\n**Phase 2: Sparsity Estimation**\n1.  **Local Sparsity Calculation**: Using the agreed-upon $\\lambda^\\star$, each agent $i$ computes its local estimate of the signal support size: $s_i(\\lambda^\\star) = \\| \\eta(z_i; \\lambda^\\star) \\|_{0}$, where the zero-norm is evaluated numerically with tolerance $\\tau$.\n2.  **Consensus on Sparsity Estimates**: The agents perform a second round of consensus, this time on the scalar values $\\{s_i(\\lambda^\\star)\\}_{i=1}^N$. Let the initial state of agent $i$ be $c_i^{(0)} = s_i(\\lambda^\\star)$. After $T$ iterations of $c_i^{(k+1)} = \\sum_{j} W_{ij} c_j^{(k)}$, each agent obtains an estimate $c_i^{(T)}$ that approximates the network average $\\bar{s} = \\frac{1}{N}\\sum_j s_j(\\lambda^\\star)$.\n3.  **Final Sparsity Estimate ($\\hat{s}$)**: The final consensus estimate of the sparsity is obtained by rounding the average to the nearest integer: $\\hat{s} = \\text{round}(c_i^{(T)})$.\n\nThis complete procedure allows the network of agents to collectively determine the optimal regularization parameter and estimate the signal sparsity in a fully decentralized manner, leveraging local SURE calculations and network-wide consensus.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases for the distributed SURE-based\n    parameter selection and sparsity estimation problem.\n    \"\"\"\n\n    def create_graph(N, topology):\n        \"\"\"Creates an adjacency matrix for a given topology.\"\"\"\n        adj = np.zeros((N, N), dtype=int)\n        if topology == 'ring':\n            for i in range(N):\n                adj[i, (i + 1) % N] = 1\n                adj[i, (i - 1 + N) % N] = 1\n        elif topology == 'line':\n            if N > 1:\n                for i in range(N - 1):\n                    adj[i, i + 1] = 1\n                    adj[i + 1, i] = 1\n        elif topology == 'fully_connected':\n            adj = np.ones((N, N), dtype=int) - np.eye(N, dtype=int)\n        return adj\n\n    def metropolis_hastings_weights(adj):\n        \"\"\"Constructs a Metropolis-Hastings doubly-stochastic weight matrix.\"\"\"\n        N = adj.shape[0]\n        W = np.zeros((N, N))\n        degrees = adj.sum(axis=1)\n        for i in range(N):\n            for j in range(i + 1, N):\n                if adj[i, j] > 0:\n                    val = 1.0 / (1.0 + max(degrees[i], degrees[j]))\n                    W[i, j] = val\n                    W[j, i] = val\n        \n        row_sums = W.sum(axis=1)\n        for i in range(N):\n            W[i, i] = 1.0 - row_sums[i]\n            \n        return W\n\n    def soft_threshold(z, lam):\n        \"\"\"Element-wise soft-thresholding operator.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - lam, 0)\n\n    def l0_norm(x, tau):\n        \"\"\"Numerical L0 norm with a tolerance.\"\"\"\n        return np.sum(np.abs(x) > tau)\n\n    def run_test_case(n, N, s, a, sigmas, topology, seed, acceptance_criterion):\n        \"\"\"Executes a single test case.\"\"\"\n        # --- 1. Setup ---\n        rng = np.random.default_rng(seed)\n        G = 401\n        T = 50\n        tau = 1e-6\n        lambda_grid = np.linspace(0, 4, G)\n\n        # --- 2. Generate Graph  Weights ---\n        adj_matrix = create_graph(N, topology)\n        W = metropolis_hastings_weights(adj_matrix)\n\n        # --- 3. Generate Signal  Data ---\n        x0 = np.zeros(n)\n        if s > 0:\n            support = rng.choice(n, s, replace=False)\n            signs = rng.choice([-1, 1], s)\n            x0[support] = a * signs\n        \n        observations = np.zeros((N, n))\n        for i in range(N):\n            noise = rng.normal(0, sigmas[i], n)\n            observations[i, :] = x0 + noise\n\n        # --- 4. Local SURE Computations ---\n        local_sure_curves = np.zeros((N, G))\n        for i in range(N):\n            z_i = observations[i, :]\n            sigma_i = sigmas[i]\n            for g, lam in enumerate(lambda_grid):\n                x_hat_i = soft_threshold(z_i, lam)\n                \n                # Using the stable form for the first term: sum(min(|z_j|, lam)^2)\n                term1 = np.sum(np.minimum(np.abs(z_i), lam)**2)\n                \n                # Divergence term: 2 * sigma^2 * ||x_hat||_0\n                l0_x_hat_i = l0_norm(x_hat_i, tau)\n                term2 = -n * sigma_i**2\n                term3 = 2 * sigma_i**2 * l0_x_hat_i\n                \n                local_sure_curves[i, g] = term1 + term2 + term3\n\n        # --- 5. Consensus on SURE curves ---\n        consensus_sures = local_sure_curves.copy()\n        for _ in range(T):\n            consensus_sures = W @ consensus_sures\n        \n        # Any agent's curve is now the approximation of the average\n        avg_sure_curve = consensus_sures[0, :]\n\n        # --- 6. Select lambda_star ---\n        best_lambda_idx = np.argmin(avg_sure_curve)\n        lambda_star = lambda_grid[best_lambda_idx]\n\n        # --- 7. Local Sparsity Estimation ---\n        local_sparsities = np.zeros(N)\n        for i in range(N):\n            z_i = observations[i, :]\n            x_hat_i_star = soft_threshold(z_i, lambda_star)\n            local_sparsities[i] = l0_norm(x_hat_i_star, tau)\n            \n        # --- 8. Consensus on Sparsity ---\n        consensus_sparsities = local_sparsities.reshape(-1, 1)\n        for _ in range(T):\n            consensus_sparsities = W @ consensus_sparsities\n            \n        # Any agent's value is now the approximation of the average\n        avg_s = consensus_sparsities[0, 0]\n        s_hat = int(np.round(avg_s))\n\n        # --- 9. Check Acceptance Criterion ---\n        success = acceptance_criterion(s_hat, s)\n\n        # --- 10. Return Result ---\n        return [lambda_star, s_hat, success]\n\n    # Define test cases\n    test_cases = [\n        # Test Case 1: Happy Path\n        {\n            \"n\": 128, \"N\": 6, \"s\": 10, \"a\": 3.0,\n            \"sigmas\": np.full(6, 0.25),\n            \"topology\": \"ring\",\n            \"seed\": 12345,\n            \"acceptance_criterion\": lambda s_hat, s: abs(s_hat - s) = 2\n        },\n        # Test Case 2: Zero Signal\n        {\n            \"n\": 128, \"N\": 4, \"s\": 0, \"a\": 0.0,\n            \"sigmas\": np.full(4, 0.4),\n            \"topology\": \"fully_connected\",\n            \"seed\": 54321,\n            \"acceptance_criterion\": lambda s_hat, s: s_hat == s\n        },\n        # Test Case 3: Heterogeneous\n        {\n            \"n\": 64, \"N\": 5, \"s\": 5, \"a\": 2.5,\n            \"sigmas\": np.array([0.3, 0.3, 0.8, 0.3, 0.3]),\n            \"topology\": \"line\",\n            \"seed\": 24680,\n            \"acceptance_criterion\": lambda s_hat, s: abs(s_hat - s) = 2\n        }\n    ]\n    \n    results = []\n    for params in test_cases:\n        result = run_test_case(**params)\n        # Format lambda to a reasonable precision for output consistency\n        result[0] = round(result[0], 5)\n        results.append(result)\n\n    # Format the final output string exactly as requested\n    print(f\"[{','.join(map(str, results))}]\".replace(\"'\", \"\"))\n\nsolve()\n```", "id": "3444471"}, {"introduction": "Standard consensus algorithms often assume an ideal network where all nodes operate correctly, but real-world systems must be resilient to failures and even malicious behavior. This advanced practice introduces the challenge of Byzantine adversaries, which can send arbitrary data to disrupt the computation. You will design a robust recovery algorithm using a trimmed-mean aggregator and derive a theoretical bound on its performance degradation, providing essential experience in the principles of robust distributed computing [@problem_id:3444450].", "problem": "You are given a distributed sparse recovery setting with $N$ worker nodes. A global unknown $s$-sparse vector $x^{\\star} \\in \\mathbb{R}^{p}$ is measured locally at node $i \\in \\{1,\\dots,N\\}$ by $y_i = A_i x^{\\star} + w_i$, where $A_i \\in \\mathbb{R}^{m \\times p}$ has independent and identically distributed Gaussian entries with variance $1/m$, and $w_i \\in \\mathbb{R}^{m}$ is zero-mean Gaussian noise with covariance $\\sigma^2 I_m$. A subset of $q$ nodes are adversarial (Byzantine) and may send arbitrarily corrupted updates.\n\nThe goal is to design a robust, single-shot distributed sparse support recovery method based on coordinate-wise trimmed-mean aggregation and to bound the increase in the false discovery rate (FDR) for the recovered support in the presence of $q$ adversaries.\n\nDefinitions and setup:\n\n- Let the per-node proxy be $z_i = A_i^{\\top} y_i \\in \\mathbb{R}^{p}$. A robust aggregator computes a coordinate-wise trimmed-mean with trimming parameter $b=q$: for each coordinate $j \\in \\{1,\\dots,p\\}$, collect $\\{z_{i,j}\\}_{i=1}^N$, sort, drop the $q$ smallest and the $q$ largest values, and average the remaining $N - 2q$ values to obtain the aggregated coordinate $\\widehat{z}_j$. The robust support estimate is $\\widehat{S}_{\\mathrm{rob}} = \\mathrm{Top}_s\\left(|\\widehat{z}|\\right)$, the indices of the $s$ largest magnitudes of $\\widehat{z}$.\n\n- As a baseline with no attackers, let $z^{\\mathrm{ben}} = \\frac{1}{N-q}\\sum_{i \\in \\mathcal{B}} z_i$, where $\\mathcal{B}$ is the set of benign nodes of cardinality $N-q$. The baseline support estimate is $\\widehat{S}_{\\mathrm{base}} = \\mathrm{Top}_s\\left(|z^{\\mathrm{ben}}|\\right)$.\n\n- For a support estimate $\\widehat{S}$, define the false discovery rate as $\\mathrm{FDR}(\\widehat{S}) = \\frac{|\\widehat{S} \\setminus S^{\\star}|}{|\\widehat{S}|}$, where $S^{\\star} = \\mathrm{supp}(x^{\\star})$. The increase in false discovery rate due to adversaries is $\\Delta_{\\mathrm{FDR}} = \\mathrm{FDR}(\\widehat{S}_{\\mathrm{rob}}) - \\mathrm{FDR}(\\widehat{S}_{\\mathrm{base}})$.\n\nTask:\n\n- Propose a robust distributed sparse support recovery method using the above coordinate-wise trimmed-mean aggregation with trimming parameter $b=q$.\n\n- Derive, from first principles and widely accepted facts in robust statistics and sparse recovery, a computable upper bound on $\\Delta_{\\mathrm{FDR}}$ as a function of $N$, $q$, and the empirical distribution of benign local proxies $\\{z_i\\}_{i \\in \\mathcal{B}}$. Your derivation must begin from fundamental definitions (e.g., order statistics, set cardinalities, and properties of the trimmed mean) and produce a bound that can be evaluated solely from observable benign proxy statistics and $q$.\n\n- Implement the proposed robust method and the derived bound. Then, using the specific test suite below, compute for each case whether the empirical increase in false discovery rate $\\Delta_{\\mathrm{FDR}}$ is less than or equal to your derived upper bound. The final output must be a list of Boolean values, one per test case, indicating whether the bound holds.\n\nAdversary model for simulation:\n\n- Up to $q$ nodes are adversarial. In the simulation, adversaries are assumed to send corrupted proxies $\\widetilde{z}_i$ that are independent and can have large magnitude. You must choose a concrete corruption consistent with the parameters below to ensure a scientifically realistic worst-case stress test, but your bound must not depend on unknown attacker values beyond $q$.\n\nTest suite (each test case fixes all parameters):\n\n- Case $1$: $N = 9$, $q = 0$, $p = 120$, $s = 5$, $m = 50$, $\\sigma = 0.05$, corruption magnitude parameter $B_{\\mathrm{adv}} = 0$.\n\n- Case $2$: $N = 9$, $q = 2$, $p = 120$, $s = 5$, $m = 50$, $\\sigma = 0.05$, $B_{\\mathrm{adv}} = 20.0$.\n\n- Case $3$: $N = 7$, $q = 3$, $p = 120$, $s = 5$, $m = 50$, $\\sigma = 0.05$, $B_{\\mathrm{adv}} = 100.0$.\n\nImplementation details:\n\n- Construct $x^{\\star}$ with exactly $s$ nonzeros of equal magnitude and random signs at uniformly random support locations.\n\n- Generate each $A_i$ with independent and identically distributed entries $\\mathcal{N}(0, 1/m)$ and $w_i$ with $\\mathcal{N}(0, \\sigma^2 I_m)$.\n\n- For adversarial nodes, produce $\\widetilde{z}_i$ with independent entries whose magnitudes are on the order of $B_{\\mathrm{adv}}$ so that they meaningfully stress the aggregator.\n\n- Use the coordinate-wise trimmed-mean with trimming parameter $b=q$ for the robust aggregate, and the simple mean over benign nodes for the baseline.\n\n- Define the increase in false discovery rate $\\Delta_{\\mathrm{FDR}}$ as above. Compute a data-driven, theoretically justified upper bound from your derivation using only $N$, $q$, and the benign proxies’ coordinate-wise ranges.\n\nRequired final output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True]\"), where each Boolean indicates whether the computed empirical $\\Delta_{\\mathrm{FDR}}$ is less than or equal to your derived bound for the corresponding test case.\n\nAngle units are not applicable. No physical units are involved. All numerical results must be in standard decimal form. The program must be self-contained, require no user input, and must use fixed random seeds internally to ensure reproducibility.", "solution": "The user-provided problem is a valid and well-posed question in the domain of distributed sparse signal processing and robust statistics. It requests the derivation of an upper bound for the increase in false discovery rate when using a coordinate-wise trimmed-mean aggregator in the presence of Byzantine adversaries, followed by a numerical verification of this bound.\n\nThe problem statement has been validated and found to be:\n- **Scientifically Grounded**: The setup uses standard models from compressed sensing ($y_i = A_i x^{\\star} + w_i$) and robust statistics (trimmed-mean estimator). All definitions are consistent with established literature.\n- **Well-Posed**: The task is to derive and verify an upper bound, which is a mathematically precise objective. The parameters for the test cases are specified and ensure the trimmed-mean is well-defined (i.e., $N-2q > 0$).\n- **Objective**: The problem is stated in precise mathematical language, free from subjectivity or ambiguity.\n\nThus, we may proceed with a complete solution.\n\n### Principle-Based Derivation of the False Discovery Rate Bound\n\nOur objective is to derive a computable upper bound on $\\Delta_{\\mathrm{FDR}} = \\mathrm{FDR}(\\widehat{S}_{\\mathrm{rob}}) - \\mathrm{FDR}(\\widehat{S}_{\\mathrm{base}})$.\n\n**1. Preliminaries and Definitions**\nLet $S^{\\star}$ be the true support of the $s$-sparse vector $x^{\\star}$, with $|S^{\\star}|=s$. The support estimates are $\\widehat{S}_{\\mathrm{rob}}$ and $\\widehat{S}_{\\mathrm{base}}$, each of size $s$.\nThe False Discovery Rate (FDR) is $\\mathrm{FDR}(\\widehat{S}) = \\frac{|\\widehat{S} \\setminus S^{\\star}|}{s}$.\nThe increase in FDR is $\\Delta_{\\mathrm{FDR}} = \\frac{1}{s} (|\\widehat{S}_{\\mathrm{rob}} \\setminus S^{\\star}| - |\\widehat{S}_{\\mathrm{base}} \\setminus S^{\\star}|)$.\n\nThe baseline aggregate is $z^{\\mathrm{ben}} = \\frac{1}{N-q}\\sum_{i \\in \\mathcal{B}} z_i$, where $\\mathcal{B}$ is the set of $N-q$ benign nodes. The baseline support is $\\widehat{S}_{\\mathrm{base}} = \\mathrm{Top}_s(|z^{\\mathrm{ben}}|)$.\nThe robust aggregate $\\widehat{z}$ is the coordinate-wise trimmed-mean of all $N$ proxies (from $N-q$ benign nodes and $q$ adversarial nodes), with trimming parameter $b=q$. For each coordinate $j \\in \\{1, \\dots, p\\}$, let the sorted proxy values be $z_{(1),j} \\le z_{(2),j} \\le \\dots \\le z_{(N),j}$. Then $\\widehat{z}_j = \\frac{1}{N-2q}\\sum_{k=q+1}^{N-q} z_{(k),j}$. The robust support is $\\widehat{S}_{\\mathrm{rob}} = \\mathrm{Top}_s(|\\widehat{z}|)$.\n\n**2. Bounding the Perturbation from Adversaries**\nWe first establish a bound on the difference between the robust aggregate $\\widehat{z}_j$ and the baseline aggregate $z^{\\mathrm{ben}}_j$ for any coordinate $j$.\n\nLet $Z_j^{(\\mathcal{B})} = \\{z_{i,j}\\}_{i \\in \\mathcal{B}}$ be the set of $N-q$ benign proxy values for coordinate $j$.\nLet $m_j^{(\\mathcal{B})} = \\min(Z_j^{(\\mathcal{B})})$ and $M_j^{(\\mathcal{B})} = \\max(Z_j^{(\\mathcal{B})})$.\nThe baseline aggregate $z^{\\mathrm{ben}}_j$ is the mean of these values, so it is naturally bounded by their range: $m_j^{(\\mathcal{B})} \\le z^{\\mathrm{ben}}_j \\le M_j^{(\\mathcal{B})}$.\n\nThe robust aggregate $\\widehat{z}_j$ is the trimmed-mean of the full set of $N$ proxies, which includes $q$ arbitrary adversarial values. A fundamental property of the $q$-trimmed-mean in the presence of $q$ adversaries is that its value is contained within the range of the benign data.\nTo see this, consider the $N-2q$ values being averaged, $\\{z_{(q+1),j}, \\dots, z_{(N-q),j}\\}$. The smallest of these, $z_{(q+1),j}$, is the $(q+1)$-th order statistic of the combined set of $N$ values. This set of $q+1$ smallest values can contain at most $q$ adversarial values, so it must contain at least one benign value. Thus, $z_{(q+1),j} \\ge m_j^{(\\mathcal{B})}$. By a symmetric argument, $z_{(N-q),j} \\le M_j^{(\\mathcal{B})}$. Since $\\widehat{z}_j$ is an average of values between $z_{(q+1),j}$ and $z_{(N-q),j}$, it follows that $m_j^{(\\mathcal{B})} \\le \\widehat{z}_j \\le M_j^{(\\mathcal{B})}$.\n\nBoth $\\widehat{z}_j$ and $z^{\\mathrm{ben}}_j$ lie in the interval $[m_j^{(\\mathcal{B})}, M_j^{(\\mathcal{B})}]$. Therefore, the maximum possible difference between them is the length of this interval:\n$$|\\widehat{z}_j - z^{\\mathrm{ben}}_j| \\le M_j^{(\\mathcal{B})} - m_j^{(\\mathcal{B})} =: R_j^{(\\mathcal{B})}$$\nwhere $R_j^{(\\mathcal{B})}$ is the range of the benign proxy values for coordinate $j$. This provides a data-driven bound on the coordinate-wise perturbation caused by the adversaries. This further implies a bound on the change in magnitude:\n$$||\\widehat{z}_j| - |z^{\\mathrm{ben}}_j|| \\le |\\widehat{z}_j - z^{\\mathrm{ben}}_j| \\le R_j^{(\\mathcal{B})}$$\n\n**3. Analyzing Support Set Changes**\nLet $I_{in} = \\widehat{S}_{\\mathrm{rob}} \\setminus \\widehat{S}_{\\mathrm{base}}$ be the set of indices that enter the top-$s$ list, and $I_{out} = \\widehat{S}_{\\mathrm{base}} \\setminus \\widehat{S}_{\\mathrm{rob}}$ be the indices that leave. Since both support sets have size $s$, we must have $|I_{in}| = |I_{out}| =: K$.\n\nThe change in the number of false discoveries is $| \\widehat{S}_{\\mathrm{rob}} \\setminus S^{\\star} | - | \\widehat{S}_{\\mathrm{base}} \\setminus S^{\\star} |$. This can be written in terms of $I_{in}$ and $I_{out}$:\n$$ \\Delta_{\\mathrm{FDR}} = \\frac{1}{s} \\left( |I_{in} \\setminus S^{\\star}| - |I_{out} \\cap (\\widehat{S}_{\\mathrm{base}} \\setminus S^{\\star})| \\right) $$\nTo obtain an upper bound, we consider the worst-case scenario where the increase in false discoveries is maximized. This occurs when all entering indices are false discoveries ($I_{in} \\cap S^{\\star} = \\emptyset$) and no existing false discoveries are removed ($I_{out} \\cap (\\widehat{S}_{\\mathrm{base}} \\setminus S^{\\star}) = \\emptyset$). In this case, the expression becomes:\n$$ \\Delta_{\\mathrm{FDR}} \\le \\frac{|I_{in}|}{s} = \\frac{K}{s} $$\nOur task reduces to finding an upper bound on $K$, the number of elements that change their membership in the top-$s$ set.\n\n**4. Bounding the Number of Swaps, K**\nAn index $j \\in I_{in}$ and an index $k \\in I_{out}$ implies a rank inversion. In the baseline, $k$ was preferred to $j$, so $|z^{\\mathrm{ben}}_k| \\ge |z^{\\mathrm{ben}}_j|$. After the attack, $j$ is preferred to $k$, so $|\\widehat{z}_j| > |\\widehat{z}_k|$.\n\nWe can use the perturbation bound to find a necessary condition for such a swap:\n$$ |\\widehat{z}_j| \\le |z^{\\mathrm{ben}}_j| + R_j^{(\\mathcal{B})} $$\n$$ |\\widehat{z}_k| \\ge |z^{\\mathrm{ben}}_k| - R_k^{(\\mathcal{B})} $$\nFor the swap $|\\widehat{z}_j| > |\\widehat{z}_k|$ to occur, it must be that:\n$$ |z^{\\mathrm{ben}}_j| + R_j^{(\\mathcal{B})} > |z^{\\mathrm{ben}}_k| - R_k^{(\\mathcal{B})} $$\n$$ \\implies |z^{\\mathrm{ben}}_k| - |z^{\\mathrm{ben}}_j|  R_j^{(\\mathcal{B})} + R_k^{(\\mathcal{B})} $$\nThis inequality provides a necessary condition for a pair of indices $(j, k)$, with $j \\notin \\widehat{S}_{\\mathrm{base}}$ and $k \\in \\widehat{S}_{\\mathrm{base}}$, to swap their relative ranking such that $j$ could potentially enter the top-$s$ set while $k$ leaves.\n\n**5. A Computable Upper Bound**\nWe can now define sets of \"vulnerable\" indices based on this condition. These are indices whose rank could plausibly change due to the bounded perturbation.\nLet $V_{out}$ be the set of indices not in $\\widehat{S}_{\\mathrm{base}}$ that are vulnerable to being promoted into it:\n$$ V_{out} = \\{ j \\notin \\widehat{S}_{\\mathrm{base}} \\mid \\exists k \\in \\widehat{S}_{\\mathrm{base}} \\text{ s.t. } |z^{\\mathrm{ben}}_k| - |z^{\\mathrm{ben}}_j|  R_j^{(\\mathcal{B})} + R_k^{(\\mathcal{B})} \\} $$\nLet $V_{in}$ be the set of indices in $\\widehat{S}_{\\mathrm{base}}$ that are vulnerable to being demoted out of it:\n$$ V_{in} = \\{ k \\in \\widehat{S}_{\\mathrm{base}} \\mid \\exists j \\notin \\widehat{S}_{\\mathrm{base}} \\text{ s.t. } |z^{\\mathrm{ben}}_k| - |z^{\\mathrm{ben}}_j|  R_j^{(\\mathcal{B})} + R_k^{(\\mathcal{B})} \\} $$\nAny index that is promoted, $j \\in I_{in}$, must be vulnerable, so $I_{in} \\subseteq V_{out}$.\nAny index that is demoted, $k \\in I_{out}$, must be vulnerable, so $I_{out} \\subseteq V_{in}$.\nThis implies that $K = |I_{in}| \\le |V_{out}|$ and $K = |I_{out}| \\le |V_{in}|$.\nTherefore, we can bound $K$ by $K \\le \\min(|V_{out}|, |V_{in}|)$.\n\nSubstituting this into our inequality for $\\Delta_{\\mathrm{FDR}}$, we arrive at the final, computable upper bound:\n$$ \\Delta_{\\mathrm{FDR}} \\le \\frac{\\min(|V_{out}|, |V_{in}|)}{s} $$\nThis bound depends only on $N, q$ (implicitly via the set $\\mathcal{B}$ of size $N-q$), and the empirical statistics of the benign proxies ($z^{\\mathrm{ben}}$ and $\\{R_j^{(\\mathcal{B})}\\}_{j=1}^p$), as required.\n\nThe proposed algorithm is to first compute the robust support estimate $\\widehat{S}_{\\mathrm{rob}}$ via coordinate-wise trimmed-mean aggregation. The theoretical bound is then computed using the derived formula based on the statistics of the benign proxies. Finally, a comparison determines if the empirically observed $\\Delta_{\\mathrm{FDR}}$ respects the theoretical upper bound.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the distributed sparse support recovery problem for a given set of test cases.\n\n    For each case, it performs the following steps:\n    1.  Generates synthetic data: a sparse signal `x_star`, and local measurements\n        at N nodes, where q nodes are adversarial.\n    2.  Computes local proxies `z_i` at each node. Adversarial nodes produce\n        corrupted proxies.\n    3.  Calculates the baseline support estimate `S_base` by averaging benign proxies.\n    4.  Calculates the robust support estimate `S_rob` using a coordinate-wise\n        trimmed-mean aggregator over all proxies.\n    5.  Computes the empirical increase in false discovery rate `delta_fdr_empirical`.\n    6.  Derives a theoretical upper bound `delta_fdr_bound` on this increase, based on\n        the statistics of the benign proxies.\n    7.  Checks if the empirical value is less than or equal to the theoretical bound.\n    8.  Outputs a list of booleans indicating the result for each test case.\n    \"\"\"\n    np.random.seed(42)  # For reproducibility\n\n    test_cases = [\n        # (N, q, p, s, m, sigma, B_adv)\n        (9, 0, 120, 5, 50, 0.05, 0.0),\n        (9, 2, 120, 5, 50, 0.05, 20.0),\n        (7, 3, 120, 5, 50, 0.05, 100.0),\n    ]\n\n    results = []\n    for N, q, p, s, m, sigma, B_adv in test_cases:\n        # Step 1: Generate Data\n        # Generate true sparse vector x_star\n        x_star = np.zeros(p)\n        support_indices = np.random.choice(p, s, replace=False)\n        S_star = set(support_indices)\n        magnitudes = np.ones(s)\n        signs = np.random.choice([-1, 1], s)\n        x_star[support_indices] = magnitudes * signs\n\n        # Generate proxies for all nodes\n        benign_proxies = []\n        all_proxies = []\n        benign_node_indices = list(range(N - q))\n        \n        for i in range(N):\n            if i in benign_node_indices:  # Benign node\n                A_i = np.random.normal(0, 1 / np.sqrt(m), (m, p))\n                w_i = np.random.normal(0, sigma, m)\n                y_i = A_i @ x_star + w_i\n                z_i = A_i.T @ y_i\n                benign_proxies.append(z_i)\n                all_proxies.append(z_i)\n            else:  # Adversarial node\n                # Adversaries inject large-magnitude noise\n                z_tilde_i = np.random.uniform(-B_adv, B_adv, p)\n                all_proxies.append(z_tilde_i)\n\n        Z_benign = np.array(benign_proxies) if len(benign_proxies) > 0 else np.array([]).reshape(0,p)\n        Z_all = np.array(all_proxies)\n\n        # Step 2: Compute Aggregates and Supports\n        # Baseline aggregation (mean over benign nodes)\n        if N > q:\n            z_ben = np.mean(Z_benign, axis=0)\n        else: # All nodes are adversarial\n            z_ben = np.zeros(p)\n        \n        magnitudes_base = np.abs(z_ben)\n        base_indices_sorted = np.argsort(magnitudes_base)\n        S_base_indices = base_indices_sorted[-s:]\n        S_base = set(S_base_indices)\n\n        # Robust aggregation (trimmed-mean over all nodes)\n        z_rob = np.zeros(p)\n        if N > 2 * q:\n            for j in range(p):\n                # Sort, trim, and average\n                sorted_vals = np.sort(Z_all[:, j])\n                trimmed_vals = sorted_vals[q:-q] if q > 0 else sorted_vals\n                z_rob[j] = np.mean(trimmed_vals)\n        \n        magnitudes_rob = np.abs(z_rob)\n        S_rob_indices = np.argsort(magnitudes_rob)[-s:]\n        S_rob = set(S_rob_indices)\n        \n        # Step 3: Compute Empirical FDR Increase\n        fdr_base = len(S_base - S_star) / s\n        fdr_rob = len(S_rob - S_star) / s\n        delta_fdr_empirical = fdr_rob - fdr_base\n\n        # Step 4: Compute Theoretical Bound\n        if N > q:\n            # Benign proxy ranges R_j\n            R_benign = np.ptp(Z_benign, axis=0)\n            \n            # S_base_indices and not_S_base_indices\n            not_S_base_indices = base_indices_sorted[:-s]\n\n            # Construct vulnerable sets\n            V_in = set()\n            V_out = set()\n            \n            # This is O(s * (p-s)), which is acceptable for given parameters\n            for k in S_base_indices:\n                for j in not_S_base_indices:\n                    mag_k = magnitudes_base[k]\n                    mag_j = magnitudes_base[j]\n                    R_k = R_benign[k]\n                    R_j = R_benign[j]\n\n                    if mag_k - mag_j  R_j + R_k:\n                        V_in.add(k)\n                        V_out.add(j)\n\n            K_bound = min(len(V_in), len(V_out))\n            delta_fdr_bound = K_bound / s\n        else:\n            # If no benign nodes, baseline is zero, robust may not be.\n            # Bound derivation assumes some benign nodes for z_ben and R_j.\n            # In this scenario, the bound is not well-defined.\n            # However, problem constraints ensure N > q and N > 2q.\n            # For completeness, if this case occurred, we'd predict unconstrained change.\n            delta_fdr_bound = 1.0 \n\n        # Step 5: Compare and store result\n        # Add a small epsilon for floating-point comparisons\n        results.append(delta_fdr_empirical = delta_fdr_bound + 1e-9)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3444450"}]}