{"hands_on_practices": [{"introduction": "Understanding the Tikhonov-regularized solution begins with analyzing its fundamental algebraic structure. This exercise provides a hands-on opportunity to derive the solution in a transformed coordinate system defined by the problem's operators [@problem_id:3490519]. By working in an eigenbasis, you will see precisely how regularization acts as a spectral filter, selectively damping components of the solution to ensure stability.", "problem": "Consider a linear inverse problem in compressed sensing, where an unknown signal vector $x \\in \\mathbb{R}^{p}$ is estimated from measurements $y \\in \\mathbb{R}^{m}$ via a measurement matrix $X \\in \\mathbb{R}^{m \\times p}$. The estimate $x_{\\lambda}$ is obtained by minimizing a Tikhonov-regularized least squares objective\n$$\nJ(x) = \\frac{1}{2} \\|X x - y\\|_{2}^{2} + \\frac{\\lambda}{2} \\|L x\\|_{2}^{2},\n$$\nwhere $L \\in \\mathbb{R}^{p \\times p}$ is a fixed linear regularization operator and $\\lambda > 0$ is the regularization parameter. Assume $X^{\\top} X$ and $L^{\\top} L$ are symmetric positive semidefinite, commute, and are simultaneously diagonalizable by an orthogonal matrix $Q \\in \\mathbb{R}^{p \\times p}$. Let the diagonalizations be\n$$\nQ^{\\top}(X^{\\top} X)Q = \\operatorname{diag}(a_{1}, a_{2}, \\dots, a_{p}), \\quad Q^{\\top}(L^{\\top} L)Q = \\operatorname{diag}(b_{1}, b_{2}, \\dots, b_{p}),\n$$\nand define $w := Q^{\\top} X^{\\top} y \\in \\mathbb{R}^{p}$.\n\nWork from first principles to derive the normal equations for the minimizer $x_{\\lambda}$ and use the simultaneous diagonalization to obtain a closed-form representation for $x_{\\lambda}$ in the eigenbasis of $X^{\\top} X$ and $L^{\\top} L$. Then, for the concrete case $p = 4$, with eigenvalues and transformed data\n$$\n(a_{1}, a_{2}, a_{3}, a_{4}) = (9, 4, 1, 0), \\quad (b_{1}, b_{2}, b_{3}, b_{4}) = (1, 4, 9, 16), \\quad (w_{1}, w_{2}, w_{3}, w_{4}) = (3, 2, 1, 0),\n$$\nand regularization level $\\lambda = 2$, compute the exact value of the squared Euclidean norm $\\|x_{\\lambda}\\|_{2}^{2}$ as a single closed-form expression. No rounding is required.", "solution": "The objective is to find the minimizer $x_{\\lambda}$ of the Tikhonov-regularized least squares cost function:\n$$\nJ(x) = \\frac{1}{2} \\|X x - y\\|_{2}^{2} + \\frac{\\lambda}{2} \\|L x\\|_{2}^{2}\n$$\nTo find the minimizer, we compute the gradient of $J(x)$ with respect to $x$ and set it to zero. Expanding the objective function gives:\n$$\nJ(x) = \\frac{1}{2} (x^{\\top}X^{\\top}X x - 2 x^{\\top}X^{\\top}y + y^{\\top}y) + \\frac{\\lambda}{2} x^{\\top}L^{\\top}L x\n$$\nThe gradient with respect to $x$ is:\n$$\n\\nabla_x J(x) = X^{\\top}X x - X^{\\top}y + \\lambda L^{\\top}L x\n$$\nSetting the gradient to zero, $\\nabla_x J(x) = 0$, gives the normal equations for the minimizer $x_{\\lambda}$:\n$$\n(X^{\\top}X + \\lambda L^{\\top}L) x_{\\lambda} = X^{\\top}y\n$$\nNext, we use the simultaneous diagonalization. We are given an orthogonal matrix $Q$ ($Q^{\\top}Q = I$) such that $X^{\\top}X = QAQ^{\\top}$ and $L^{\\top}L = QBQ^{\\top}$, where $A=\\operatorname{diag}(a_i)$ and $B=\\operatorname{diag}(b_i)$. Substituting these into the normal equations:\n$$\n(QAQ^{\\top} + \\lambda QBQ^{\\top}) x_{\\lambda} = X^{\\top}y \\implies Q(A + \\lambda B)Q^{\\top} x_{\\lambda} = X^{\\top}y\n$$\nMultiplying from the left by $Q^{\\top}$ and using the definitions $\\tilde{x}_{\\lambda} = Q^{\\top}x_{\\lambda}$ and $w = Q^{\\top}X^{\\top}y$:\n$$\n(A + \\lambda B)\\tilde{x}_{\\lambda} = w\n$$\nSince $A + \\lambda B$ is a diagonal matrix, this system decouples into $p$ independent scalar equations:\n$$\n(a_i + \\lambda b_i) (\\tilde{x}_{\\lambda})_i = w_i \\implies (\\tilde{x}_{\\lambda})_i = \\frac{w_i}{a_i + \\lambda b_i}\n$$\nThe problem asks for $\\|x_{\\lambda}\\|_{2}^{2}$. Since $Q$ is orthogonal, it preserves the Euclidean norm, so $\\|x_{\\lambda}\\|_{2}^{2} = \\|\\tilde{x}_{\\lambda}\\|_{2}^{2}$. We can compute this by summing the squares of the components of $\\tilde{x}_{\\lambda}$:\n$$\n\\|x_{\\lambda}\\|_{2}^{2} = \\sum_{i=1}^{p} \\left( \\frac{w_i}{a_i + \\lambda b_i} \\right)^2\n$$\nNow, we substitute the specific values: $p=4$, $\\lambda=2$, $(a_1, a_2, a_3, a_4) = (9, 4, 1, 0)$, $(b_1, b_2, b_3, b_4) = (1, 4, 9, 16)$, and $(w_1, w_2, w_3, w_4) = (3, 2, 1, 0)$.\n\nFor $i=1$: $(\\tilde{x}_{\\lambda})_1 = \\frac{3}{9 + 2(1)} = \\frac{3}{11}$\nFor $i=2$: $(\\tilde{x}_{\\lambda})_2 = \\frac{2}{4 + 2(4)} = \\frac{2}{12} = \\frac{1}{6}$\nFor $i=3$: $(\\tilde{x}_{\\lambda})_3 = \\frac{1}{1 + 2(9)} = \\frac{1}{19}$\nFor $i=4$: $(\\tilde{x}_{\\lambda})_4 = \\frac{0}{0 + 2(16)} = 0$\n\nFinally, we compute the squared norm:\n$$\n\\|x_{\\lambda}\\|_{2}^{2} = \\left(\\frac{3}{11}\\right)^2 + \\left(\\frac{1}{6}\\right)^2 + \\left(\\frac{1}{19}\\right)^2 + 0^2 = \\frac{9}{121} + \\frac{1}{36} + \\frac{1}{361}\n$$\nThe common denominator is $121 \\times 36 \\times 361 = 1572516$.\n$$\n\\|x_{\\lambda}\\|_{2}^{2} = \\frac{9 \\cdot (36 \\cdot 361) + 1 \\cdot (121 \\cdot 361) + 1 \\cdot (121 \\cdot 36)}{1572516} = \\frac{9 \\cdot 12996 + 43681 + 4356}{1572516}\n$$\n$$\n\\|x_{\\lambda}\\|_{2}^{2} = \\frac{116964 + 43681 + 4356}{1572516} = \\frac{165001}{1572516}\n$$", "answer": "$$\\boxed{\\frac{165001}{1572516}}$$", "id": "3490519"}, {"introduction": "Once we can compute a ridge regression solution for a given regularization parameter $\\lambda$, the critical next step is choosing an optimal value for this hyperparameter. This comprehensive practice guides you through deriving and implementing powerful, data-driven model selection techniques from first principles [@problem_id:3490562]. You will explore the theoretical underpinnings of the hat matrix, effective degrees of freedom, and cross-validation, translating these concepts into a working program.", "problem": "You are given a linear inverse problem of estimating a coefficient vector $ \\beta \\in \\mathbb{R}^p $ from measurements $ y \\in \\mathbb{R}^n $ and a design matrix $ X \\in \\mathbb{R}^{n \\times p} $ via Tikhonov regularization. Consider the penalized least squares estimator defined as the minimizer of the functional\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\ \\| y - X \\beta \\|_2^2 + \\lambda \\, \\| L \\beta \\|_2^2,\n$$\nwhere $ \\lambda \\ge 0 $ is a regularization parameter and $ L \\in \\mathbb{R}^{m \\times p} $ is a prescribed linear operator. Start from the fundamental base of linear algebra and optimization, including the normal equations for unconstrained least squares, properties of orthogonal projections and linear smoothers, and the Sherman–Morrison–Woodbury identity. Do not assume pre-derived formulas for the hat matrix, effective degrees of freedom, or cross-validation shortcuts.\n\nTasks:\n1. Derive the estimator $ \\hat{\\beta}_\\lambda $ by setting the gradient to zero and solving the resulting normal equations. Then, express the fitted response $ \\hat{y}_\\lambda = X \\hat{\\beta}_\\lambda $ in the form $ \\hat{y}_\\lambda = S_\\lambda \\, y $, identifying the linear smoother (hat matrix) $ S_\\lambda \\in \\mathbb{R}^{n \\times n} $ explicitly in terms of $ X $, $ L $, and $ \\lambda $.\n2. Using the definition of effective degrees of freedom for a linear smoother as $ \\mathrm{df}_\\lambda = \\mathrm{trace}\\!\\left( \\frac{\\partial \\hat{y}_\\lambda}{\\partial y} \\right) $, derive a computable expression for $ \\mathrm{df}_\\lambda $ from $ S_\\lambda $. Justify all steps from first principles.\n3. For Leave-One-Out Cross-Validation (LOOCV), define for each index $ i \\in \\{1,\\dots,n\\} $ the leave-one-out fitted value $ \\hat{y}^{(-i)}_{\\lambda, i} $ obtained by refitting the model on the dataset with the $ i $-th observation removed. Derive a formula for the leave-one-out residuals $ e^{\\mathrm{LOO}}_i = y_i - \\hat{y}^{(-i)}_{\\lambda, i} $ expressed in terms of the full-sample residuals and diagonal entries of $ S_\\lambda $. Use this to obtain a closed-form expression for the LOOCV mean squared error $ \\mathrm{LOOCV}_\\lambda = \\frac{1}{n} \\sum_{i=1}^n (e^{\\mathrm{LOO}}_i)^2 $ without explicitly refitting $ n $ times.\n4. Define the Generalized Cross-Validation (GCV) mean squared error $ \\mathrm{GCV}_\\lambda $ by replacing the pointwise leverages in LOOCV by their average derived from $ \\mathrm{df}_\\lambda $. Derive a computable expression for $ \\mathrm{GCV}_\\lambda $.\n\nThen implement a program that, for the following test suite, computes for each case the triple $ [ \\mathrm{df}_\\lambda, \\mathrm{LOOCV}_\\lambda, \\mathrm{GCV}_\\lambda ] $ and prints a single line containing the list of these triples. All floating-point outputs must be rounded to exactly $ 8 $ decimal places, using standard rounding to nearest with ties to even. The output format must be a single line containing a comma-separated list of the results enclosed in square brackets, where each result itself is a list, for example $ [[a_1,b_1,c_1],[a_2,b_2,c_2],\\dots] $.\n\nTest suite:\n- Case A (happy path, ridge regression): $ n = 5 $, $ p = 3 $, with\n$$\nX^{(A)} = \\begin{bmatrix}\n1 & -1 & 2 \\\\\n0 & 2 & -1 \\\\\n3 & 0 & 1 \\\\\n-2 & 1 & 0 \\\\\n1 & 1 & 1\n\\end{bmatrix}, \\quad\ny^{(A)} = \\begin{bmatrix}\n1 \\\\ -1 \\\\ 3 \\\\ 0 \\\\ 2\n\\end{bmatrix}, \\quad\nL^{(A)} = I_3, \\quad\n\\lambda^{(A)} = 0.5.\n$$\n- Case B (boundary, ordinary least squares): same $ X^{(A)} $, $ y^{(A)} $, and $ L^{(A)} $, but\n$$\n\\lambda^{(B)} = 0.\n$$\n- Case C (edge, extreme regularization): same $ X^{(A)} $, $ y^{(A)} $, and $ L^{(A)} $, but\n$$\n\\lambda^{(C)} = 10^6.\n$$\n- Case D (general Tikhonov with first-difference operator): same $ X^{(A)} $ and $ y^{(A)} $, but with the first-difference operator\n$$\nL^{(D)} = \\begin{bmatrix}\n-1 & 1 & 0 \\\\\n0 & -1 & 1\n\\end{bmatrix}, \\quad\n\\lambda^{(D)} = 1.\n$$\n- Case E (underdetermined system, ridge stabilization): $ n = 3 $, $ p = 5 $, with\n$$\nX^{(E)} = \\begin{bmatrix}\n1 & 0 & -1 & 2 & 0 \\\\\n0 & 1 & 1 & -1 & 2 \\\\\n2 & -1 & 0 & 1 & 1\n\\end{bmatrix}, \\quad\ny^{(E)} = \\begin{bmatrix}\n1 \\\\ 0 \\\\ -1\n\\end{bmatrix}, \\quad\nL^{(E)} = I_5, \\quad\n\\lambda^{(E)} = 0.3.\n$$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is the list $ [ \\mathrm{df}_\\lambda, \\mathrm{LOOCV}_\\lambda, \\mathrm{GCV}_\\lambda ] $ for the corresponding test case, in the order A, B, C, D, E. Each real number must be rounded to exactly $ 8 $ decimal places. No units are involved in this problem, and no angles are used.", "solution": "The task is to derive and compute key quantities related to Tikhonov regularization. The objective is to find the estimator $ \\beta \\in \\mathbb{R}^p $ that minimizes the penalized least squares functional:\n$$\nJ(\\beta) = \\| y - X \\beta \\|_2^2 + \\lambda \\| L \\beta \\|_2^2\n$$\nwhere $ y \\in \\mathbb{R}^n $ is the response vector, $ X \\in \\mathbb{R}^{n \\times p} $ is the design matrix, $ L \\in \\mathbb{R}^{m \\times p} $ is a penalty operator, and $ \\lambda \\ge 0 $ is the regularization parameter.\n\n**1. Derivation of the Estimator and the Smoother Matrix**\nThe objective function can be written as $J(\\beta) = (y - X \\beta)^T (y - X \\beta) + \\lambda \\beta^T L^T L \\beta$. Expanding this and taking the gradient with respect to $\\beta$ gives:\n$$\n\\nabla_\\beta J(\\beta) = -2 X^T y + 2 (X^T X + \\lambda L^T L) \\beta\n$$\nSetting the gradient to zero yields the normal equations for the estimator $\\hat{\\beta}_\\lambda$:\n$$\n(X^T X + \\lambda L^T L) \\hat{\\beta}_\\lambda = X^T y\n$$\nAssuming the matrix on the left is invertible, we solve for the estimator:\n$$\n\\hat{\\beta}_\\lambda = (X^T X + \\lambda L^T L)^{-1} X^T y\n$$\nThe fitted response vector $ \\hat{y}_\\lambda = X \\hat{\\beta}_\\lambda $ is then:\n$$\n\\hat{y}_\\lambda = X (X^T X + \\lambda L^T L)^{-1} X^T y\n$$\nThis is a linear transformation of the observed response, $ \\hat{y}_\\lambda = S_\\lambda y $, where the linear smoother, or hat matrix, $ S_\\lambda $ is:\n$$\nS_\\lambda = X (X^T X + \\lambda L^T L)^{-1} X^T\n$$\n\n**2. Derivation of Effective Degrees of Freedom**\nThe effective degrees of freedom, $ \\mathrm{df}_\\lambda $, are defined as the trace of the Jacobian of the fitted values with respect to the observed values. Since $ \\hat{y}_\\lambda = S_\\lambda y $, the Jacobian matrix is simply $ S_\\lambda $:\n$$\n\\frac{\\partial \\hat{y}_\\lambda}{\\partial y} = S_\\lambda\n$$\nTherefore, the effective degrees of freedom are the trace of the smoother matrix:\n$$\n\\mathrm{df}_\\lambda = \\mathrm{trace}(S_\\lambda)\n$$\n\n**3. Derivation of the Leave-One-Out Cross-Validation (LOOCV) Formula**\nThe LOOCV residual for the $i$-th observation, $e_i^{\\mathrm{LOO}} = y_i - \\hat{y}_{\\lambda, i}^{(-i)}$, can be derived without refitting the model $n$ times. The key result, typically derived using the Sherman-Morrison formula to relate the leave-one-out fit to the full-data fit, is:\n$$\ne_i^{\\mathrm{LOO}} = \\frac{y_i - \\hat{y}_{\\lambda, i}}{1 - S_{\\lambda, ii}}\n$$\nwhere $y_i - \\hat{y}_{\\lambda, i}$ is the ordinary residual for the $i$-th observation from the full-data model, and $ S_{\\lambda, ii} $ is the $i$-th diagonal element of the hat matrix $ S_\\lambda $. The LOOCV mean squared error is the average of the squared LOO residuals:\n$$\n\\mathrm{LOOCV}_\\lambda = \\frac{1}{n} \\sum_{i=1}^n (e_i^{\\mathrm{LOO}})^2 = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{y_i - \\hat{y}_{\\lambda, i}}{1 - S_{\\lambda, ii}} \\right)^2\n$$\n\n**4. Derivation of the Generalized Cross-Validation (GCV) Formula**\nGeneralized Cross-Validation approximates LOOCV by replacing each individual leverage score $ S_{\\lambda, ii} $ in the denominator with their average value, $\\bar{S}_\\lambda = \\frac{1}{n} \\sum_i S_{\\lambda, ii} = \\frac{\\mathrm{trace}(S_\\lambda)}{n} = \\frac{\\mathrm{df}_\\lambda}{n}$. This substitution gives:\n$$\n\\mathrm{GCV}_\\lambda = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{y_i - \\hat{y}_{\\lambda, i}}{1 - \\mathrm{df}_\\lambda/n} \\right)^2\n$$\nFactoring out the constant denominator yields the standard computable expression for GCV:\n$$\n\\mathrm{GCV}_\\lambda = \\frac{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_{\\lambda, i})^2}{\\left(1 - \\mathrm{df}_\\lambda/n\\right)^2} = \\frac{\\mathrm{MSE}_\\lambda}{\\left(1 - \\frac{\\mathrm{df}_\\lambda}{n}\\right)^2}\n$$\nwhere $\\mathrm{MSE}_\\lambda$ is the training mean squared error.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes effective degrees of freedom, LOOCV score, and GCV score\n    for Tikhonov regularization for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    X_A = np.array([\n        [1, -1, 2],\n        [0, 2, -1],\n        [3, 0, 1],\n        [-2, 1, 0],\n        [1, 1, 1]\n    ])\n    y_A = np.array([[1], [-1], [3], [0], [2]])\n    L_A = np.identity(3)\n    lambda_A = 0.5\n    \n    lambda_B = 0.0\n    \n    lambda_C = 1e6\n    \n    L_D = np.array([\n        [-1, 1, 0],\n        [0, -1, 1]\n    ])\n    lambda_D = 1.0\n    \n    X_E = np.array([\n        [1, 0, -1, 2, 0],\n        [0, 1, 1, -1, 2],\n        [2, -1, 0, 1, 1]\n    ])\n    y_E = np.array([[1], [0], [-1]])\n    L_E = np.identity(5)\n    lambda_E = 0.3\n\n    test_cases = [\n        # Case A\n        {\"X\": X_A, \"y\": y_A, \"L\": L_A, \"lam\": lambda_A},\n        # Case B\n        {\"X\": X_A, \"y\": y_A, \"L\": L_A, \"lam\": lambda_B},\n        # Case C\n        {\"X\": X_A, \"y\": y_A, \"L\": L_A, \"lam\": lambda_C},\n        # Case D\n        {\"X\": X_A, \"y\": y_A, \"L\": L_D, \"lam\": lambda_D},\n        # Case E\n        {\"X\": X_E, \"y\": y_E, \"L\": L_E, \"lam\": lambda_E},\n    ]\n\n    results = []\n    for case in test_cases:\n        X, y, L, lam = case[\"X\"], case[\"y\"], case[\"L\"], case[\"lam\"]\n        n, p = X.shape\n\n        # 1. Compute the smoother matrix S_lambda\n        XTX = X.T @ X\n        # L may not be square, so L.T @ L is required\n        LTL = L.T @ L\n        A = XTX + lam * LTL\n        \n        A_inv = np.linalg.inv(A)\n        \n        S_lambda = X @ A_inv @ X.T\n        \n        # 2. Compute effective degrees of freedom df_lambda\n        df_lambda = np.trace(S_lambda)\n       \n        # 3. Compute LOOCV_lambda\n        y_hat = S_lambda @ y\n        residuals = y - y_hat\n        S_ii = np.diag(S_lambda)\n        \n        # Reshape S_ii to (n,1) for element-wise division with residuals (n,1)\n        loo_residuals = residuals / (1 - S_ii.reshape(-1, 1))\n        \n        loocv_lambda = np.mean(loo_residuals**2)\n        \n        # 4. Compute GCV_lambda\n        mse = np.mean(residuals**2)\n        \n        denominator = 1 - df_lambda / n\n        if np.isclose(denominator, 0):\n             # Handle potential division by zero if df_lambda is very close to n\n             gcv_lambda = np.inf\n        else:\n             gcv_lambda = mse / (denominator**2)\n\n        # Round results to 8 decimal places (ties to even) and append\n        # np.round uses \"round half to even\", which is the specified method.\n        results.append([\n            np.round(df_lambda, 8),\n            np.round(loocv_lambda, 8),\n            np.round(gcv_lambda, 8)\n        ])\n\n    # Format the output string to match the required format\n    output_parts = []\n    for res in results:\n        # Format each rounded number to a string with 8 decimal places\n        formatted_res = [f\"{num:.8f}\" for num in res]\n        output_parts.append(f\"[{','.join(formatted_res)}]\")\n    \n    final_output = f\"[{','.join(output_parts)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "3490562"}, {"introduction": "For large-scale applications, solving the ridge regression normal equations via direct matrix inversion is often computationally infeasible. This exercise explores the use of iterative methods like gradient descent and introduces the powerful concept of preconditioning to accelerate their convergence [@problem_id:3490601]. By designing a theoretically \"ideal\" preconditioner, you will uncover the upper bound of performance improvement and gain deep insight into why preconditioning is essential for efficient optimization.", "problem": "Consider ridge-regularized least squares (also known as Tikhonov regularization) with data matrix $A \\in \\mathbb{R}^{m \\times n}$, response $b \\in \\mathbb{R}^{m}$, and regularization parameter $\\lambda > 0$. The objective is\n$$\nf(x) \\;=\\; \\frac{1}{2}\\|A x - b\\|^{2} \\;+\\; \\frac{\\lambda}{2}\\|x\\|^{2},\n$$\nwhose Hessian is the symmetric positive definite (SPD) matrix $H \\equiv A^{\\mathsf{T}} A + \\lambda I$. A typical preconditioning strategy is to apply a change of variables $x = P y$ with an invertible matrix $P \\in \\mathbb{R}^{n \\times n}$, which transforms the Hessian to $\\widetilde{H} \\equiv P^{\\mathsf{T}} H P$. The spectral condition number of a SPD matrix $M$ is defined as $\\kappa(M) \\equiv \\lambda_{\\max}(M)/\\lambda_{\\min}(M)$, where $\\lambda_{\\max}(M)$ and $\\lambda_{\\min}(M)$ are its largest and smallest eigenvalues, respectively.\n\nSuppose gradient descent is applied to the preconditioned problem in the variable $y$ with a fixed step size chosen to minimize the worst-case linear convergence factor, using only the spectral bounds of the Hessian at hand. You may assume $m \\geq n$ but do not assume that $A$ has full column rank; the regularization ensures $H \\succ 0$. Work from first principles, beginning with the characterization of gradient descent on a quadratic objective and the relationship between the worst-case contraction factor, the step size, and the eigenvalue interval of the Hessian.\n\nDesign a preconditioner $P$ that minimizes the condition number $\\kappa(\\widetilde{H})$ over all invertible $P$, and, using spectral bounds, quantify the resulting worst-case per-iteration linear contraction factor of gradient descent with the optimal fixed step size on the preconditioned problem. Give your final answer as a two-entry row, whose first entry is the minimized condition number $\\kappa(\\widetilde{H})$ and whose second entry is the corresponding worst-case contraction factor. Provide the exact values; do not approximate or round.", "solution": "The solution is divided into two parts: first, we determine the optimal preconditioner by minimizing the condition number of the preconditioned Hessian, and second, we calculate the corresponding worst-case convergence factor for gradient descent.\n\n**1. Optimal Preconditioner and Minimized Condition Number**\n\nThe preconditioned Hessian is $\\widetilde{H} = P^{\\mathsf{T}} H P$, where $H = A^{\\mathsf{T}}A + \\lambda I$. Since $A^{\\mathsf{T}}A$ is symmetric positive semi-definite (SPSD) and $\\lambda > 0$, the Hessian $H$ is symmetric positive definite (SPD). Consequently, for any invertible $P$, the preconditioned Hessian $\\widetilde{H}$ is also SPD.\n\nThe spectral condition number of an SPD matrix $M$, $\\kappa(M) = \\lambda_{\\max}(M) / \\lambda_{\\min}(M)$, is always greater than or equal to $1$. The minimum possible value, $\\kappa(M)=1$, is achieved if and only if all eigenvalues of $M$ are equal. This implies that the matrix must be a scalar multiple of the identity, i.e., $M = cI$ for some constant $c > 0$.\n\nOur goal is to find an invertible matrix $P$ such that $\\widetilde{H} = P^{\\mathsf{T}} H P$ is a scalar multiple of the identity. The simplest target is the identity matrix itself: $P^{\\mathsf{T}} H P = I$.\nSince $H$ is SPD, it admits a unique SPD square root, $H^{1/2}$, such that $(H^{1/2})^2 = H$. This square root matrix is invertible. We can choose the preconditioner $P$ to be its inverse:\n$$P = (H^{1/2})^{-1} \\equiv H^{-1/2}$$\nSince $H^{1/2}$ is symmetric, its inverse $P$ is also symmetric, meaning $P^{\\mathsf{T}} = P$. Substituting this choice into the expression for $\\widetilde{H}$:\n$$\\widetilde{H} = P^{\\mathsf{T}} H P = H^{-1/2} H H^{-1/2} = H^{-1/2} (H^{1/2} H^{1/2}) H^{-1/2} = (H^{-1/2} H^{1/2}) (H^{1/2} H^{-1/2}) = I \\cdot I = I$$\nThe preconditioned Hessian becomes the identity matrix. The eigenvalues of $I$ are all $1$, so $\\lambda_{\\max}(\\widetilde{H}) = 1$ and $\\lambda_{\\min}(\\widetilde{H}) = 1$. The minimized condition number is:\n$$\\kappa(\\widetilde{H}) = \\frac{1}{1} = 1$$\n\n**2. Worst-Case Contraction Factor**\n\nGradient descent applied to the preconditioned quadratic objective has an error update rule $e_{k+1} = (I - \\alpha \\widetilde{H}) e_k$, where $\\alpha$ is the step size. The worst-case per-iteration linear contraction factor is the spectral radius of the iteration matrix $G \\equiv I - \\alpha \\widetilde{H}$, which is given by $\\rho(G) = \\max(|1 - \\alpha \\lambda_{\\min}(\\widetilde{H})|, |1 - \\alpha \\lambda_{\\max}(\\widetilde{H})|)$.\n\nThe step size $\\alpha^*$ that minimizes this factor is:\n$$\\alpha^* = \\frac{2}{\\lambda_{\\min}(\\widetilde{H}) + \\lambda_{\\max}(\\widetilde{H})}$$\nWith this optimal step size, the contraction factor becomes:\n$$\\text{Contraction factor} = \\frac{\\lambda_{\\max}(\\widetilde{H}) - \\lambda_{\\min}(\\widetilde{H})}{\\lambda_{\\max}(\\widetilde{H}) + \\lambda_{\\min}(\\widetilde{H})} = \\frac{\\kappa(\\widetilde{H}) - 1}{\\kappa(\\widetilde{H}) + 1}$$\nUsing the minimized condition number $\\kappa(\\widetilde{H}) = 1$ from the first part, we find the corresponding contraction factor:\n$$\\text{Contraction factor} = \\frac{1 - 1}{1 + 1} = \\frac{0}{2} = 0$$\nA contraction factor of zero implies that gradient descent converges in a single iteration.\n\nThus, the minimized condition number is $1$, and the corresponding worst-case contraction factor is $0$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 0\n\\end{pmatrix}\n}\n$$", "id": "3490601"}]}