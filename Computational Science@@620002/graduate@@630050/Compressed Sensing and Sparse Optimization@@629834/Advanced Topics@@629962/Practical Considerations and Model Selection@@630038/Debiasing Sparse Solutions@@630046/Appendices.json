{"hands_on_practices": [{"introduction": "This first practice provides a direct, hands-on demonstration of the bias inherent in the popular Lasso estimator. By first computing the Lasso solution and then performing a least-squares refit on its selected support, you will quantitatively measure the shrinkage effect and see how a simple, unpenalized second stage can correct for it [@problem_id:3442554]. This exercise forms the foundation for understanding why and how debiasing is a critical step in sparse modeling.", "problem": "Consider the canonical sparse recovery setting in compressed sensing and sparse optimization. Let a sensing matrix be denoted by $A \\in \\mathbb{R}^{m \\times n}$, and let a ground-truth sparse signal be denoted by $x^{\\star} \\in \\mathbb{R}^{n}$. Observed measurements are given by $y = A x^{\\star} + \\eta$, where $\\eta \\in \\mathbb{R}^{m}$ represents additive noise. The Least Absolute Shrinkage and Selection Operator (Lasso) estimate $\\widehat{x}_{\\text{Lasso}}$ is defined as the minimizer of the convex functional\n$$\n\\frac{1}{2} \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\nwhere $\\lambda \\ge 0$ is a regularization parameter and $\\|\\cdot\\|_{1}$ denotes the $\\ell_{1}$ norm. It is well understood that the $\\ell_{1}$ penalty induces shrinkage that creates coordinate-wise bias in the estimator relative to $x^{\\star}$.\n\nYou are required to implement an Iterative Soft-Thresholding Algorithm (ISTA) to compute $\\widehat{x}_{\\text{Lasso}}$, starting from first principles. Use the decomposition of the Lasso objective into a smooth term $g(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$ and a non-smooth term $h(x) = \\lambda \\|x\\|_{1}$. The gradient of $g$ is $\\nabla g(x) = A^{\\top}(A x - y)$, and the proximal operator of $h$ is the coordinate-wise soft-thresholding operator. Choose a constant step size $t$ satisfying $0 < t \\le \\frac{1}{L}$, where $L$ is the Lipschitz constant of $\\nabla g$, equal to the squared spectral norm of $A$, i.e., $L = \\|A\\|_{2}^{2}$. Starting from $x^{(0)} = 0$, generate iterates\n$$\nx^{(k+1)} = \\operatorname{soft}(x^{(k)} - t \\nabla g(x^{(k)}), \\lambda t),\n$$\nwhere for any $z \\in \\mathbb{R}^{n}$ and $\\tau \\ge 0$, the operator is defined coordinate-wise by\n$$\n\\operatorname{soft}(z_{i}, \\tau) = \\operatorname{sign}(z_{i}) \\cdot \\max(|z_{i}| - \\tau, 0).\n$$\n\nAfter computing $\\widehat{x}_{\\text{Lasso}}$, define the estimated support $\\widehat{S} = \\{ i \\in \\{1,\\dots,n\\} : |\\widehat{x}_{\\text{Lasso}, i}| > \\tau_{\\text{sup}}\\}$ using a fixed threshold $\\tau_{\\text{sup}} > 0$. Perform a least-squares refit restricted to $\\widehat{S}$ by solving\n$$\n\\min_{z \\in \\mathbb{R}^{|\\widehat{S}|}} \\|A_{\\widehat{S}} z - y\\|_{2}^{2},\n$$\nwhere $A_{\\widehat{S}}$ denotes the submatrix of $A$ containing the columns indexed by $\\widehat{S}$. Use the Moore–Penrose pseudoinverse to obtain $z^{\\text{LS}} = A_{\\widehat{S}}^{\\dagger} y$; then define the refitted estimate $\\widehat{x}_{\\text{refit}} \\in \\mathbb{R}^{n}$ by setting $(\\widehat{x}_{\\text{refit}})_{\\widehat{S}} = z^{\\text{LS}}$ and $(\\widehat{x}_{\\text{refit}})_{i} = 0$ for $i \\notin \\widehat{S}$.\n\nFor both $\\widehat{x}_{\\text{Lasso}}$ and $\\widehat{x}_{\\text{refit}}$, compute the coordinate-wise bias vector $b = \\widehat{x} - x^{\\star}$ and summarize bias using the mean absolute bias\n$$\n\\operatorname{MAB}(b) = \\frac{1}{n} \\sum_{i=1}^{n} |b_{i}|\n$$\nand the maximum absolute bias\n$$\n\\operatorname{MaxAB}(b) = \\max_{1 \\le i \\le n} |b_{i}|.\n$$\n\nImplement a program that performs the above for the following test suite. Each test case specifies $A$, $x^{\\star}$, $\\eta$, and $\\lambda$, and requires using $\\tau_{\\text{sup}} = 10^{-6}$.\n\nTest Case 1 (well-conditioned, moderate regularization):\n- Dimensions: $m = 8$, $n = 6$.\n- Matrix $A$:\n$$\n\\begin{bmatrix}\n0.36 & -0.07 & 0.22 & 0.10 & -0.31 & 0.41 \\\\\n-0.12 & 0.25 & 0.30 & -0.41 & 0.05 & -0.08 \\\\\n0.45 & 0.18 & -0.08 & 0.03 & 0.26 & -0.19 \\\\\n0.05 & -0.31 & 0.41 & 0.17 & -0.02 & 0.12 \\\\\n-0.27 & 0.11 & -0.36 & -0.28 & 0.44 & 0.06 \\\\\n0.14 & 0.39 & 0.07 & -0.02 & -0.23 & 0.28 \\\\\n0.31 & -0.22 & 0.18 & -0.35 & 0.09 & -0.27 \\\\\n-0.19 & 0.33 & -0.12 & 0.29 & 0.37 & -0.15\n\\end{bmatrix}\n$$\n- Ground truth $x^{\\star} = [0.0,\\, 1.5,\\, 0.0,\\, -2.0,\\, 0.0,\\, 0.5]$.\n- Noise $\\eta = [0.01,\\,-0.02,\\,0.015,\\,0.0,\\,-0.005,\\,0.008,\\,0.012,\\,-0.009]$.\n- Observations $y = A x^{\\star} + \\eta$.\n- Regularization $\\lambda = 0.1$.\n\nTest Case 2 (very large regularization; empty support edge case):\n- Use the same $A$ as in Test Case 1.\n- Ground truth $x^{\\star} = [0.0,\\, 0.0,\\, 2.0,\\, 0.0,\\, 0.0,\\, 0.0]$.\n- Noise $\\eta = [0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0]$.\n- Observations $y = A x^{\\star} + \\eta$.\n- Regularization $\\lambda = 100.0$.\n\nTest Case 3 (correlated columns; moderate regularization):\n- Dimensions: $m = 8$, $n = 6$.\n- Matrix $A$:\n$$\n\\begin{bmatrix}\n0.40 & 0.50 & 0.49 & -0.10 & 0.02 & 0.33 \\\\\n-0.20 & -0.25 & -0.24 & 0.12 & -0.18 & -0.31 \\\\\n0.35 & 0.42 & 0.41 & -0.22 & 0.27 & 0.05 \\\\\n-0.05 & -0.06 & -0.06 & 0.30 & 0.12 & -0.28 \\\\\n0.10 & 0.12 & 0.12 & -0.26 & -0.33 & 0.18 \\\\\n0.28 & 0.35 & 0.34 & 0.04 & 0.15 & -0.11 \\\\\n-0.17 & -0.21 & -0.21 & 0.09 & -0.07 & 0.24 \\\\\n0.22 & 0.27 & 0.26 & -0.19 & 0.31 & -0.09\n\\end{bmatrix}\n$$\n- Ground truth $x^{\\star} = [0.0,\\, 1.2,\\, 1.0,\\, 0.0,\\, 0.0,\\, 0.0]$.\n- Noise $\\eta = [0.003,\\,-0.004,\\,0.002,\\,0.006,\\,-0.005,\\,-0.001,\\,0.004,\\,-0.003]$.\n- Observations $y = A x^{\\star} + \\eta$.\n- Regularization $\\lambda = 0.15$.\n\nFor each test case, compute:\n1. The Lasso estimate $\\widehat{x}_{\\text{Lasso}}$ via ISTA with step size $t = 1/\\|A\\|_{2}^{2}$, initialized at zero, and iterate until convergence defined by $\\|x^{(k+1)} - x^{(k)}\\|_{2} \\le 10^{-8}$ or a maximum of $20000$ iterations.\n2. The estimated support $\\widehat{S}$ using $\\tau_{\\text{sup}} = 10^{-6}$.\n3. The refitted estimate $\\widehat{x}_{\\text{refit}}$ by least-squares over $\\widehat{S}$ using the Moore–Penrose pseudoinverse.\n4. The mean absolute bias and maximum absolute bias for both $\\widehat{x}_{\\text{Lasso}}$ and $\\widehat{x}_{\\text{refit}}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a list of four floats in the order $[\\operatorname{MAB}(\\widehat{x}_{\\text{Lasso}} - x^{\\star}), \\operatorname{MAB}(\\widehat{x}_{\\text{refit}} - x^{\\star}), \\operatorname{MaxAB}(\\widehat{x}_{\\text{Lasso}} - x^{\\star}), \\operatorname{MaxAB}(\\widehat{x}_{\\text{refit}} - x^{\\star})]$. Aggregate the three per-case lists in a single top-level list, so the final output format is\n$$\n\\big[ [r_{1,1}, r_{1,2}, r_{1,3}, r_{1,4}], [r_{2,1}, r_{2,2}, r_{2,3}, r_{2,4}], [r_{3,1}, r_{3,2}, r_{3,3}, r_{3,4}] \\big].\n$$\nNo physical units are involved, and all answers must be real numbers.", "solution": "The problem requires the implementation and comparison of two sparse signal estimation methods, Lasso and its debiased variant using least-squares refitting, within the context of compressed sensing. We are given the complete algorithmic and analytical framework to perform this task for three distinct test cases. The solution involves numerical optimization, linear algebra, and statistical evaluation.\n\nThe core problem is to find a sparse solution $x \\in \\mathbb{R}^{n}$ to a system of linear equations $y = Ax + \\eta$, where $A \\in \\mathbb{R}^{m \\times n}$ is a sensing matrix, $y \\in \\mathbb{R}^{m}$ are noisy measurements, and $\\eta \\in \\mathbb{R}^{m}$ is additive noise. The ground-truth signal $x^{\\star} \\in \\mathbb{R}^{n}$ is assumed to be sparse.\n\nFirst, we compute the Lasso estimate, $\\widehat{x}_{\\text{Lasso}}$, which is defined as the solution to the following convex optimization problem:\n$$\n\\widehat{x}_{\\text{Lasso}} = \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\right\\}.\n$$\nHere, $\\lambda \\ge 0$ is a regularization parameter that controls the trade-off between the data fidelity term $\\|A x - y\\|_{2}^{2}$ and the sparsity-inducing penalty term $\\|x\\|_{1}$. The objective function is a sum of a smooth, convex, differentiable function $g(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$ and a non-smooth, convex function $h(x) = \\lambda \\|x\\|_{1}$.\n\nThe problem specifies using the Iterative Soft-Thresholding Algorithm (ISTA), a proximal gradient method, to find $\\widehat{x}_{\\text{Lasso}}$. Starting with an initial guess $x^{(0)} = 0$, ISTA generates a sequence of estimates via the iterative update rule:\n$$\nx^{(k+1)} = \\operatorname{prox}_{t h}(x^{(k)} - t \\nabla g(x^{(k)})).\n$$\nThis update consists of two steps: a standard gradient descent step on the smooth part $g(x)$, and the application of the proximal operator of the non-smooth part $h(x)$. The gradient of a $g(x)$ is $\\nabla g(x) = A^{\\top}(A x - y)$. The proximal operator for $h(x) = \\lambda \\|x\\|_{1}$ is the soft-thresholding function, applied coordinate-wise:\n$$\n(\\operatorname{prox}_{t h}(z))_i = \\operatorname{soft}(z_i, \\lambda t) = \\operatorname{sign}(z_i) \\cdot \\max(|z_i| - \\lambda t, 0).\n$$\nThe step size $t$ must satisfy $0 < t \\le 1/L$ to guarantee convergence, where $L$ is the Lipschitz constant of the gradient $\\nabla g(x)$. For this problem, $L$ is the squared spectral norm of the matrix $A$, i.e., $L = \\|A\\|_{2}^{2} = \\sigma_{\\max}^2(A)$, where $\\sigma_{\\max}(A)$ is the largest singular value of $A$. We will use the upper bound for the step size, $t = 1/L = 1/\\|A\\|_{2}^{2}$. The iteration proceeds until the change in the estimate is sufficiently small, $\\|x^{(k+1)} - x^{(k)}\\|_{2} \\le 10^{-8}$, or a maximum of $20000$ iterations is reached.\n\nThe $\\ell_1$ penalty in Lasso is known to cause shrinkage, which introduces a bias in the non-zero coefficients of the estimate. To mitigate this bias, a second estimator, $\\widehat{x}_{\\text{refit}}$, is computed via a two-stage process. First, we identify the support (the set of indices of non-zero coefficients) of the Lasso solution:\n$$\n\\widehat{S} = \\{ i \\in \\{1,\\dots,n\\} : |\\widehat{x}_{\\text{Lasso}, i}| > \\tau_{\\text{sup}} \\},\n$$\nwhere $\\tau_{\\text{sup}} = 10^{-6}$ is a small threshold to account for numerical precision.\n\nSecond, we perform an unpenalized ordinary least-squares (OLS) fit restricted to this estimated support. This involves solving:\n$$\n\\min_{z \\in \\mathbb{R}^{|\\widehat{S}|}} \\|A_{\\widehat{S}} z - y\\|_{2}^{2},\n$$\nwhere $A_{\\widehat{S}}$ is the submatrix of $A$ formed by the columns whose indices are in $\\widehat{S}$. The solution to this OLS problem is given by $z^{\\text{LS}} = A_{\\widehat{S}}^{\\dagger} y$, where $A_{\\widehat{S}}^{\\dagger}$ is the Moore-Penrose pseudoinverse of $A_{\\widehat{S}}$. The use of the pseudoinverse ensures a unique, stable solution even if $A_{\\widehat{S}}$ is rank-deficient or ill-conditioned. The refitted estimate $\\widehat{x}_{\\text{refit}} \\in \\mathbb{R}^{n}$ is then constructed by setting its coefficients on the support $\\widehat{S}$ to $z^{\\text{LS}}$ and setting all other coefficients to zero:\n$$\n(\\widehat{x}_{\\text{refit}})_i = \\begin{cases} (z^{\\text{LS}})_j & \\text{if } i \\text{ is the } j\\text{-th index in } \\widehat{S} \\\\ 0 & \\text{if } i \\notin \\widehat{S} \\end{cases}.\n$$\nIf the estimated support $\\widehat{S}$ is empty, the refitted estimate is the zero vector, $\\widehat{x}_{\\text{refit}} = 0$.\n\nFinally, to evaluate the performance of both estimators, we compute the coordinate-wise bias vector $b = \\widehat{x} - x^{\\star}$ for each estimate $\\widehat{x} \\in \\{\\widehat{x}_{\\text{Lasso}}, \\widehat{x}_{\\text{refit}}\\}$. The bias is summarized using two metrics: the Mean Absolute Bias (MAB) and the Maximum Absolute Bias (MaxAB), defined as:\n$$\n\\operatorname{MAB}(b) = \\frac{1}{n} \\sum_{i=1}^{n} |b_{i}|, \\quad \\operatorname{MaxAB}(b) = \\max_{1 \\le i \\le n} |b_{i}|.\n$$\n\nWe will apply this entire procedure to each of the three test cases provided. For each case, we first construct the measurement vector $y = A x^{\\star} + \\eta$ from the given $A$, $x^{\\star}$, and $\\eta$. Then, we implement the ISTA algorithm to find $\\widehat{x}_{\\text{Lasso}}$, followed by the support identification and least-squares refitting to find $\\widehat{x}_{\\text{refit}}$. Subsequently, we compute the four specified bias metrics and report them in the requested format. This systematic process allows for a direct comparison of the bias properties of the Lasso and the debiased refitted estimators under varying conditions of the problem setup.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the debiasing sparse solutions problem for three test cases.\n    For each case, it computes the Lasso estimate via ISTA, performs\n    least-squares refitting, and calculates bias metrics for both estimates.\n    \"\"\"\n\n    # --- Helper Functions ---\n\n    def soft_threshold(z, tau):\n        \"\"\"\n        Soft-thresholding operator.\n        \"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - tau, 0)\n\n    def ista_solver(A, y, lambda_val, max_iter=20000, tol=1e-8):\n        \"\"\"\n        Iterative Soft-Thresholding Algorithm (ISTA) for solving Lasso.\n        \"\"\"\n        m, n = A.shape\n        # L = np.linalg.norm(A, ord=2)**2 is slow. Using SVD is faster.\n        # As L is the largest eigenvalue of A.T @ A, we compute it directly.\n        L = np.max(np.linalg.eigvalsh(A.T @ A))\n        t = 1.0 / L\n        \n        x_k = np.zeros(n)\n        for _ in range(max_iter):\n            grad = A.T @ (A @ x_k - y)\n            z = x_k - t * grad\n            x_k_plus_1 = soft_threshold(z, lambda_val * t)\n            \n            if np.linalg.norm(x_k_plus_1 - x_k) <= tol:\n                break\n            \n            x_k = x_k_plus_1\n        \n        return x_k\n\n    def refit_least_squares(A, y, x_lasso, tau_sup=1e-6):\n        \"\"\"\n        Performs least-squares refitting on the support of the Lasso estimate.\n        \"\"\"\n        n = A.shape[1]\n        support = np.where(np.abs(x_lasso) > tau_sup)[0]\n        \n        x_refit = np.zeros(n)\n        \n        if support.size > 0:\n            A_S = A[:, support]\n            try:\n                z_ls = np.linalg.pinv(A_S) @ y\n                x_refit[support] = z_ls\n            except np.linalg.LinAlgError:\n                # This case is unlikely with pseudoinverse but good practice.\n                pass\n                \n        return x_refit\n\n    def calculate_bias_metrics(x_est, x_star):\n        \"\"\"\n        Calculates MAB and MaxAB for a given estimate.\n        \"\"\"\n        bias = x_est - x_star\n        mab = np.mean(np.abs(bias))\n        max_ab = np.max(np.abs(bias))\n        return mab, max_ab\n\n    # --- Test Case Definitions ---\n\n    A1 = np.array([\n        [0.36, -0.07, 0.22, 0.10, -0.31, 0.41],\n        [-0.12, 0.25, 0.30, -0.41, 0.05, -0.08],\n        [0.45, 0.18, -0.08, 0.03, 0.26, -0.19],\n        [0.05, -0.31, 0.41, 0.17, -0.02, 0.12],\n        [-0.27, 0.11, -0.36, -0.28, 0.44, 0.06],\n        [0.14, 0.39, 0.07, -0.02, -0.23, 0.28],\n        [0.31, -0.22, 0.18, -0.35, 0.09, -0.27],\n        [-0.19, 0.33, -0.12, 0.29, 0.37, -0.15]\n    ])\n\n    A3 = np.array([\n        [0.40, 0.50, 0.49, -0.10, 0.02, 0.33],\n        [-0.20, -0.25, -0.24, 0.12, -0.18, -0.31],\n        [0.35, 0.42, 0.41, -0.22, 0.27, 0.05],\n        [-0.05, -0.06, -0.06, 0.30, 0.12, -0.28],\n        [0.10, 0.12, 0.12, -0.26, -0.33, 0.18],\n        [0.28, 0.35, 0.34, 0.04, 0.15, -0.11],\n        [-0.17, -0.21, -0.21, 0.09, -0.07, 0.24],\n        [0.22, 0.27, 0.26, -0.19, 0.31, -0.09]\n    ])\n\n    test_cases = [\n        {\n            \"A\": A1,\n            \"x_star\": np.array([0.0, 1.5, 0.0, -2.0, 0.0, 0.5]),\n            \"eta\": np.array([0.01, -0.02, 0.015, 0.0, -0.005, 0.008, 0.012, -0.009]),\n            \"lambda_val\": 0.1\n        },\n        {\n            \"A\": A1,\n            \"x_star\": np.array([0.0, 0.0, 2.0, 0.0, 0.0, 0.0]),\n            \"eta\": np.zeros(8),\n            \"lambda_val\": 100.0\n        },\n        {\n            \"A\": A3,\n            \"x_star\": np.array([0.0, 1.2, 1.0, 0.0, 0.0, 0.0]),\n            \"eta\": np.array([0.003, -0.004, 0.002, 0.006, -0.005, -0.001, 0.004, -0.003]),\n            \"lambda_val\": 0.15\n        }\n    ]\n\n    # --- Main Processing Loop ---\n\n    results = []\n    for case in test_cases:\n        A = case[\"A\"]\n        x_star = case[\"x_star\"]\n        eta = case[\"eta\"]\n        lambda_val = case[\"lambda_val\"]\n        \n        # 1. Compute measurements\n        y = A @ x_star + eta\n        \n        # 2. Compute Lasso estimate\n        x_lasso = ista_solver(A, y, lambda_val)\n        \n        # 3. Compute refitted estimate\n        x_refit = refit_least_squares(A, y, x_lasso)\n        \n        # 4. Calculate bias metrics\n        mab_lasso, maxab_lasso = calculate_bias_metrics(x_lasso, x_star)\n        mab_refit, maxab_refit = calculate_bias_metrics(x_refit, x_star)\n        \n        case_results = [mab_lasso, mab_refit, maxab_lasso, maxab_refit]\n        results.append(case_results)\n\n    # --- Final Output ---\n    # Convert list of lists to string representation as specified.\n    # The default str(list) includes spaces after commas, which is acceptable.\n    # The join then combines these string representations with a comma.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3442554"}, {"introduction": "Building upon the concept of post-hoc refitting, this exercise explores an iterative method for debiasing known as reweighted $\\ell_1$ minimization. You will implement a scheme that progressively adapts the penalties on coefficients, encouraging a sparser and more accurate solution than the standard Lasso before any final debiasing is applied [@problem_id:3442510]. This practice demonstrates how debiasing can be integrated into the optimization process itself to improve both model selection and coefficient accuracy.", "problem": "You are given a sparse recovery and debiasing task framed within the linear inverse problem. Let $A \\in \\mathbb{R}^{m \\times n}$, $x^\\star \\in \\mathbb{R}^n$ be a $k$-sparse vector (at most $k$ nonzero entries), and $y \\in \\mathbb{R}^m$ be the noisy observation modeled by $y = A x^\\star + \\eta$, where $\\eta$ is additive noise. The sparsity-inducing point estimation is formulated by the weighted $\\ell_1$-regularized least-squares objective, which in its basic unweighted form is a standard tool in compressed sensing for sparse optimization. The task is to implement two iterations of reweighted $\\ell_1$ minimization and then apply an ordinary least-squares refit on the final support to further reduce the shrinkage bias.\n\nFundamental base and given setup:\n- The data model is $y = A x^\\star + \\eta$, and the measurement matrix $A$ is known.\n- The initial sparse estimator is obtained by solving a convex optimization problem that penalizes the $\\ell_1$ norm of the coefficients, which is a widely accepted surrogate for the counting ($\\ell_0$) pseudo-norm in sparse recovery.\n- Reweighting reduces bias by iteratively assigning larger weights to small coefficients and smaller weights to large coefficients, and least-squares refitting on the selected support reduces shrinkage bias induced by the $\\ell_1$ penalty.\n\nMatrix and vector specification:\n- Use $m = 64$, $n = 128$, and $k = 10$.\n- Generate $A$ with entries sampled independently from a zero-mean normal distribution scaled by $1/\\sqrt{m}$, i.e., $A_{ij} \\sim \\mathcal{N}(0, 1/m)$.\n- Generate a ground-truth sparse vector $x^\\star$ with support size $k$ by selecting $k$ indices uniformly at random without replacement and setting those entries to independent draws from $\\mathcal{N}(0,1)$; set all other entries to zero.\n- Generate noise $\\eta$ with entries independently drawn from $\\mathcal{N}(0, \\sigma^2)$, and set $y = A x^\\star + \\eta$.\n- Use the fixed random seed $12345$ for all random number generation to ensure reproducibility.\n- Use a single fixed noise standard deviation $\\sigma = 0.02$ across all test cases to keep $y$ fixed.\n\nAlgorithmic tasks to implement:\n1. Solve the weighted $\\ell_1$-regularized least-squares problem twice in a reweighting scheme, starting from the unweighted case:\n   - Iteration $0$: Solve for $x^{(0)}$ with all weights equal to $1$.\n   - Iteration $1$: Compute new weights from $x^{(0)}$ using a strictly positive parameter $\\epsilon$, and solve for $x^{(1)}$.\n   - Iteration $2$: Compute new weights from $x^{(1)}$ and solve for $x^{(2)}$.\n   The weighted optimization problem has the form\n   $$\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\sum_{i=1}^n w_i |x_i|,$$\n   where $w_i$ are positive weights determined at each reweighting iteration. You must obtain the solution for each weighted problem using a principled algorithm derived from first-order optimality and proximal calculus (Iterative Soft Thresholding Algorithm), ensuring a step size chosen based on the Lipschitz constant of the gradient of the data fidelity term. The initial weights must be $w_i = 1$ for all $i$.\n2. Define the support of an estimate $x$ as the index set of entries whose magnitude exceeds a threshold $\\tau$: $S(x) = \\{ i \\in \\{1,\\dots,n\\} : |x_i| > \\tau \\}$.\n3. Perform a least-squares refit restricted to the final support $S(x^{(2)})$ by solving\n   $$\\min_{z \\in \\mathbb{R}^{|S(x^{(2)})|}} \\|A_{S(x^{(2)})} z - y\\|_2^2,$$\n   and then embed the solution back into $\\mathbb{R}^n$ by placing the coefficients on $S(x^{(2)})$ and zeros elsewhere.\n\nReporting requirements:\n- For each test case, report:\n  1. The size of the support after the initial unweighted solve, $|S(x^{(0)})|$ (an integer).\n  2. The size of the support after the second reweighting solve, $|S(x^{(2)})|$ (an integer).\n  3. The magnitude of the support change from iteration $0$ to $1$, defined as the size of the symmetric difference $|S(x^{(0)}) \\,\\triangle\\, S(x^{(1)})|$ (an integer).\n  4. The magnitude of the support change from iteration $1$ to $2$, defined as $|S(x^{(1)}) \\,\\triangle\\, S(x^{(2)})|$ (an integer).\n  5. The average coefficient magnitude bias on the true support $S(x^\\star)$ for $x^{(2)}$, defined as\n     $$b_{\\text{pre}} = \\frac{1}{|S(x^\\star)|} \\sum_{i \\in S(x^\\star)} \\left( |x^{(2)}_i| - |x^\\star_i| \\right) \\in \\mathbb{R}$$\n     (a float; negative values indicate shrinkage relative to the ground truth).\n  6. The average coefficient magnitude bias on the true support after least-squares refit, defined as\n     $$b_{\\text{post}} = \\frac{1}{|S(x^\\star)|} \\sum_{i \\in S(x^\\star)} \\left( |x^{\\text{LS}}_i| - |x^\\star_i| \\right) \\in \\mathbb{R},$$\n     where $x^{\\text{LS}}$ is the least-squares refit restricted to $S(x^{(2)})$ (a float).\n\nTest suite:\nRun your program on the following four test cases, all with the same $A$ and $y$ constructed as above using $\\sigma = 0.02$ and seed $12345$. Each test case specifies the regularization parameter $\\lambda$, the reweighting parameter $\\epsilon$, and the support threshold $\\tau$:\n- Test case $1$: $\\lambda = 0.05$, $\\epsilon = 10^{-3}$, $\\tau = 10^{-4}$.\n- Test case $2$: $\\lambda = 0.10$, $\\epsilon = 10^{-3}$, $\\tau = 10^{-4}$.\n- Test case $3$: $\\lambda = 0.05$, $\\epsilon = 10^{-6}$, $\\tau = 10^{-4}$.\n- Test case $4$: $\\lambda = 0.05$, $\\epsilon = 10^{-3}$, $\\tau = 10^{-2}$.\n\nOutput format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a sub-list in the order given, with elements in the exact order $[|S(x^{(0)})|, |S(x^{(2)})|, |S(x^{(0)}) \\,\\triangle\\, S(x^{(1)})|, |S(x^{(1)}) \\,\\triangle\\, S(x^{(2)})|, b_{\\text{pre}}, b_{\\text{post}}]$. For example, the overall output should look like:\n$$\\texttt{[[s0\\_1,s2\\_1,c01\\_1,c12\\_1,bpre\\_1,bpost\\_1],[s0\\_2,s2\\_2,c01\\_2,c12\\_2,bpre\\_2,bpost\\_2],[s0\\_3,s2\\_3,c01\\_3,c12\\_3,bpre\\_3,bpost\\_3],[s0\\_4,s2\\_4,c01\\_4,c12\\_4,bpre\\_4,bpost\\_4]]}$$\nNo physical units are involved in this problem, so you must report pure numbers. Angles are not involved. Percentages, if any arise, must be expressed as decimals, but this task does not require percentage outputs.", "solution": "The problem requires the implementation and evaluation of a reweighted $\\ell_1$-minimization algorithm for sparse signal recovery, followed by a least-squares debiasing step. The task is structured as a linear inverse problem, a standard framework in signal processing, statistics, and machine learning.\n\nThe underlying data generation model is given by the linear equation:\n$$ y = A x^\\star + \\eta $$\nwhere $x^\\star \\in \\mathbb{R}^n$ is the unknown $k$-sparse signal we aim to recover, $A \\in \\mathbb{R}^{m \\times n}$ is the measurement or sensing matrix, $\\eta \\in \\mathbb{R}^m$ represents additive noise, and $y \\in \\mathbb{R}^m$ is the observed measurement vector. The dimensions are specified as $m = 64$, $n = 128$, and the sparsity level is $k = 10$. The matrix $A$ has entries drawn independently from a normal distribution $\\mathcal{N}(0, 1/m)$, the non-zero entries of the true signal $x^\\star$ are drawn from $\\mathcal{N}(0,1)$, and the noise components are drawn from $\\mathcal{N}(0, \\sigma^2)$ with $\\sigma = 0.02$.\n\nTo recover an estimate of $x^\\star$ from $y$ and $A$, we solve the following weighted $\\ell_1$-regularized least-squares optimization problem:\n$$ \\min_{x \\in \\mathbb{R}^n} J(x) \\quad \\text{where} \\quad J(x) = \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\sum_{i=1}^n w_i |x_i| $$\nHere, $\\lambda > 0$ is a regularization parameter that balances data fidelity with sparsity, and $w_i > 0$ are weights that can be adapted to improve the solution. This objective function is a sum of two convex parts: a smooth, differentiable data-fidelity term $f(x) = \\frac{1}{2}\\|A x - y\\|_2^2$ and a non-smooth, convex regularization term $g(x) = \\lambda \\sum_{i=1}^n w_i |x_i|$.\n\nThis structure makes the problem amenable to proximal gradient methods. The specific algorithm employed is the Iterative Soft-Thresholding Algorithm (ISTA), which follows the update rule:\n$$ x_{t+1} = \\text{prox}_{\\alpha g} \\left( x_t - \\alpha \\nabla f(x_t) \\right) $$\nwhere $t$ is the iteration index and $\\alpha$ is the step size. The gradient of the data-fidelity term is $\\nabla f(x) = A^T(Ax - y)$. The proximal operator of the weighted $\\ell_1$-norm is the element-wise soft-thresholding function:\n$$ \\left( \\text{prox}_{\\alpha g}(z) \\right)_i = \\text{S}_{\\alpha \\lambda w_i}(z_i) = \\text{sign}(z_i) \\max(|z_i| - \\alpha \\lambda w_i, 0) $$\nFor ISTA to converge, the step size $\\alpha$ must be chosen such that $0 < \\alpha \\le 1/L$, where $L$ is the Lipschitz constant of the gradient $\\nabla f(x)$. This constant is the largest eigenvalue of $A^T A$, which is equal to the squared largest singular value of $A$, $L = \\sigma_{\\max}(A)^2$. We will use the step size $\\alpha = 1/L$.\n\nThe full ISTA update is therefore:\n$$ x_{t+1} = \\text{S}_{\\frac{\\lambda}{L} w} \\left( x_t - \\frac{1}{L} A^T(Ax_t - y) \\right) $$\nwhere the soft-thresholding operation is applied element-wise with the vector of thresholds constructed from the weights $w$.\n\nThe problem specifies a reweighting scheme to reduce the inherent bias of $\\ell_1$ regularization. This scheme involves three main solution steps:\n1.  **Iteration $0$**: Solve for $x^{(0)}$ using the unweighted problem, which corresponds to setting all weights $w_i^{(0)} = 1$.\n2.  **Iteration $1$**: Update the weights based on the result of the first step. A larger coefficient magnitude in $x^{(0)}$ suggests it is more likely to be part of the true support, so it should be penalized less. The weights are updated as $w_i^{(1)} = 1 / (|x_i^{(0)}| + \\epsilon)$, where $\\epsilon > 0$ is a small parameter to ensure stability. An ISTA solve is performed with these new weights to obtain $x^{(1)}$.\n3.  **Iteration $2$**: The process is repeated. New weights are computed from $x^{(1)}$ as $w_i^{(2)} = 1 / (|x_i^{(1)}| + \\epsilon)$, and the final regularized estimate $x^{(2)}$ is found by solving the corresponding weighted problem.\n\nThe solution vectors $x^{(j)}$ from ISTA are not perfectly sparse. A support set must be identified by thresholding. The support of an estimate $x$ is defined as $S(x) = \\{ i : |x_i| > \\tau \\}$, where $\\tau$ is a small threshold.\n\nFinally, to further mitigate the shrinkage-induced bias from the $\\ell_1$ penalty, a least-squares refitting step is performed. This involves solving an unregularized least-squares problem restricted to the final support set $S_{\\text{final}} = S(x^{(2)})$. Let $A_{S_{\\text{final}}}$ be the submatrix of $A$ containing only the columns indexed by $S_{\\text{final}}$. The debiased coefficients $z^{\\text{LS}}$ are found by solving:\n$$ \\min_{z \\in \\mathbb{R}^{|S_{\\text{final}}|}} \\|A_{S_{\\text{final}}} z - y\\|_2^2 $$\nThe closed-form solution is $z^{\\text{LS}} = (A_{S_{\\text{final}}}^T A_{S_{\\text{final}}})^\\dagger A_{S_{\\text{final}}}^T y$, where $\\dagger$ denotes the Moore-Penrose pseudoinverse, which simplifies to $(B^T B)^{-1} B^T$ for a full-rank matrix $B$. The final debiased estimate, $x^{\\text{LS}}$, is constructed by placing the coefficients of $z^{\\text{LS}}$ onto the support indices $S_{\\text{final}}$ and setting all other entries to zero.\n\nThe entire procedure is executed for four test cases, varying the parameters $\\lambda$, $\\epsilon$, and $\\tau$. For each case, we report six metrics: the support size of $x^{(0)}$ and $x^{(2)}$, the support change between iterations, and the average coefficient magnitude bias on the true support before and after the least-squares refit. All random processes are seeded with $12345$ for reproducibility.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a sparse recovery and debiasing problem using reweighted l1 minimization.\n    \"\"\"\n\n    # --- Problem Setup ---\n    m, n, k = 64, 128, 10\n    sigma = 0.02\n    seed = 12345\n\n    # --- Data Generation ---\n    rng = np.random.default_rng(seed)\n\n    # Generate matrix A\n    A = rng.normal(0, 1 / np.sqrt(m), size=(m, n))\n\n    # Generate sparse vector x_star\n    support_star_indices = rng.choice(n, k, replace=False)\n    x_star = np.zeros(n)\n    x_star[support_star_indices] = rng.normal(0, 1, size=k)\n    support_star_set = set(support_star_indices)\n\n    # Generate noise and observation y\n    eta = rng.normal(0, sigma, size=m)\n    y = A @ x_star + eta\n\n    # --- Algorithm Parameters ---\n    # Lipschitz constant of the gradient of the least-squares term\n    L = np.linalg.svd(A, compute_uv=False)[0] ** 2\n    ista_step_size = 1.0 / L\n    ista_iterations = 5000\n\n    # Test cases\n    test_cases = [\n        # (lambda, epsilon, tau)\n        (0.05, 1e-3, 1e-4),\n        (0.10, 1e-3, 1e-4),\n        (0.05, 1e-6, 1e-4),\n        (0.05, 1e-3, 1e-2),\n    ]\n\n    all_results = []\n\n    # --- Helper Functions ---\n    def soft_threshold(z, T):\n        \"\"\"Element-wise soft-thresholding operator.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - T, 0)\n\n    def ista_solve(A, y, lambda_val, weights):\n        \"\"\"\n        Solves the weighted l1-regularized least-squares problem using ISTA.\n        \"\"\"\n        x = np.zeros(A.shape[1])\n        At = A.T\n        thresholds = lambda_val * weights * ista_step_size\n        \n        for _ in range(ista_iterations):\n            gradient = At @ (A @ x - y)\n            z = x - ista_step_size * gradient\n            x = soft_threshold(z, thresholds)\n        return x\n\n    def get_support(x, tau):\n        \"\"\"Identifies the support of a vector based on a magnitude threshold.\"\"\"\n        return set(np.where(np.abs(x) > tau)[0])\n\n    def ls_refit(A, y, support_indices):\n        \"\"\"Performs least-squares refitting on the identified support.\"\"\"\n        x_ls = np.zeros(A.shape[1])\n        if not support_indices:\n            return x_ls\n        \n        support_list = sorted(list(support_indices))\n        A_S = A[:, support_list]\n        \n        # Solve the least-squares problem: min ||A_S z - y||_2^2\n        z, _, _, _ = np.linalg.lstsq(A_S, y, rcond=None)\n        \n        x_ls[support_list] = z\n        return x_ls\n\n    for lambda_val, epsilon, tau in test_cases:\n        case_results = []\n\n        # -- Iteration 0 (Unweighted l1) --\n        w0 = np.ones(n)\n        x0 = ista_solve(A, y, lambda_val, w0)\n        S0 = get_support(x0, tau)\n        case_results.append(len(S0))\n\n        # -- Iteration 1 (Reweighted) --\n        w1 = 1.0 / (np.abs(x0) + epsilon)\n        x1 = ista_solve(A, y, lambda_val, w1)\n        S1 = get_support(x1, tau)\n\n        # -- Iteration 2 (Reweighted) --\n        w2 = 1.0 / (np.abs(x1) + epsilon)\n        x2 = ista_solve(A, y, lambda_val, w2)\n        S2 = get_support(x2, tau)\n        case_results.append(len(S2))\n\n        # -- Support Change Metrics --\n        change01 = len(S0.symmetric_difference(S1))\n        case_results.append(change01)\n        change12 = len(S1.symmetric_difference(S2))\n        case_results.append(change12)\n\n        # -- Least-Squares Refit on final support S2 --\n        x_ls = ls_refit(A, y, S2)\n\n        # -- Bias Metrics on True Support --\n        x_star_on_support = x_star[support_star_indices]\n        x2_on_support = x2[support_star_indices]\n        x_ls_on_support = x_ls[support_star_indices]\n\n        # Bias before refit\n        b_pre = np.mean(np.abs(x2_on_support) - np.abs(x_star_on_support))\n        case_results.append(b_pre)\n        \n        # Bias after refit\n        b_post = np.mean(np.abs(x_ls_on_support) - np.abs(x_star_on_support))\n        case_results.append(b_post)\n\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # The output format is a list of lists.\n    # `repr` is used to get the string representation of floats without losing precision.\n    formatted_results = [\n        f\"[{res[0]},{res[1]},{res[2]},{res[3]},{repr(res[4])},{repr(res[5])}]\"\n        for res in all_results\n    ]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3442510"}, {"introduction": "The effectiveness of any debiasing procedure hinges on having first selected a good model, a choice heavily influenced by the regularization parameter $\\lambda$. This practice delves into a statistically principled method for selecting $\\lambda$ using Stein's Unbiased Risk Estimate (SURE) in a denoising context [@problem_id:3442574]. You will use SURE to find an optimal threshold from data and then apply least-squares refitting, illustrating a complete and robust workflow from data-driven tuning to final debiased estimation.", "problem": "You are given observations $y \\in \\mathbb{R}^n$ of an unknown sparse signal $x_0 \\in \\mathbb{R}^n$ corrupted by additive Gaussian noise with known variance $\\sigma^2$, that is, $y = x_0 + w$ where $w \\sim \\mathcal{N}(0, \\sigma^2 I_n)$. Consider the sparse estimation rule given by componentwise soft-thresholding with threshold $\\lambda \\ge 0$, defined by the estimator $x(\\lambda) = \\eta_{\\text{soft}}(y, \\lambda)$ applied entrywise via $\\eta_{\\text{soft}}(t, \\lambda) = \\operatorname{sign}(t) \\max(|t| - \\lambda, 0)$. Your task is to implement the following pipeline based on Stein’s Unbiased Risk Estimate (SURE) and least-squares debiasing:\n\n1. Starting from the principle of Stein’s Unbiased Risk Estimate (SURE) for Gaussian noise, construct an unbiased estimate of the mean-squared error risk of the estimator $x(\\lambda)$ as a function of $\\lambda$. The construction must rely only on the fundamental prerequisites: the differentiability requirements of the estimator, the distributional assumption on the noise, and the definition of mean-squared error risk. Do not import any specialized closed-form expressions from external sources; derive the required expression directly from the principle.\n\n2. For a given discrete grid of thresholds $\\Lambda = \\{\\lambda_0, \\lambda_1, \\ldots, \\lambda_m\\}$, evaluate the SURE for each $\\lambda \\in \\Lambda$, and select a minimizing $\\lambda^\\star \\in \\Lambda$. If multiple values of $\\lambda$ attain the same minimum SURE value within numerical precision, choose the smallest such $\\lambda$.\n\n3. Using the selected threshold $\\lambda^\\star$, define the support set $S(\\lambda^\\star) = \\{ i \\in \\{1,\\ldots,n\\} : |y_i| > \\lambda^\\star \\}$. Perform least-squares refitting constrained to this support in the identity-design denoising model, that is, compute the debiased estimator $x^{\\text{debias}}$ that minimizes $\\|y - x\\|_2^2$ subject to $\\operatorname{supp}(x) \\subseteq S(\\lambda^\\star)$. Report the resulting debiased estimate.\n\nFollow these algorithmic details precisely:\n- The divergence required by the SURE principle must be computed according to the weak derivative of the soft-thresholding operator with respect to $y$. For entries exactly at the threshold, treat them according to a measure-zero convention consistent with almost-everywhere differentiability.\n- The least-squares refitting on the selected support in the identity model is the Euclidean projection of $y$ onto the coordinate subspace indexed by $S(\\lambda^\\star)$.\n- For numerical reproducibility, when producing the final outputs, round all floating-point numbers to six digits after the decimal point.\n\nTest Suite:\nImplement your program to process the following four test cases. In each case, you are given $y$, $\\sigma^2$, and a uniform grid specification for $\\Lambda$. The grid should be constructed as all values $\\lambda$ from the given start to the given end, inclusive, with the specified step size. The angle unit is not applicable. No physical units are involved.\n\n- Case 1:\n  - $y = (3.2, -0.1, 0.0, 1.1, -2.7, 0.05)$\n  - $\\sigma^2 = 0.25$\n  - Grid: start $0.0$, end $2.5$, step $0.05$\n- Case 2:\n  - $y = (0.2, -0.15, 0.08, -0.05)$\n  - $\\sigma^2 = 0.04$\n  - Grid: start $0.0$, end $0.6$, step $0.01$\n- Case 3:\n  - $y = (5.0, 0.0, 0.0, -4.8, 0.1)$\n  - $\\sigma^2 = 1.0$\n  - Grid: start $0.0$, end $6.0$, step $0.1$\n- Case 4:\n  - $y = (1.0, -1.0, 1.0, -1.0)$\n  - $\\sigma^2 = 0.09$\n  - Grid: start $0.0$, end $1.5$, step $0.01$\n\nRequired Final Output:\nYour program should produce a single line of output containing the results for all four cases as a list of lists. For each case, output a list whose first element is the selected threshold $\\lambda^\\star$, followed by the components of the debiased estimator $x^{\\text{debias}}$ in the original coordinate order, all rounded to six decimal places. The overall output should therefore be a single string of the form\n[[\\lambda^\\star_{\\text{case1}}, x^{\\text{debias}}_{\\text{case1},1}, \\ldots, x^{\\text{debias}}_{\\text{case1},n_1}], [\\lambda^\\star_{\\text{case2}}, \\ldots], [\\lambda^\\star_{\\text{case3}}, \\ldots], [\\lambda^\\star_{\\text{case4}}, \\ldots]]\nprinted exactly on one line, with commas separating values and no additional text.", "solution": "The provided problem is a well-defined task in statistical signal processing, specifically in the domain of sparse signal denoising. It requires the implementation of a three-stage pipeline: first, the derivation and application of Stein's Unbiased Risk Estimate (SURE) to select an optimal threshold for soft-thresholding; second, the selection of this threshold from a discrete grid; and third, the application of least-squares debiasing (also known as refitting) to improve the estimate. The problem is scientifically grounded, mathematically consistent, and all necessary parameters are provided.\n\nThe algorithmic pipeline will be constructed as follows:\n\n1.  **Derivation and Implementation of SURE for Soft-Thresholding**\n\nThe goal is to find an unbiased estimate of the mean-squared error (MSE) risk, $R(x(\\lambda), x_0) = \\mathbb{E}[\\|x(\\lambda) - x_0\\|_2^2]$, for the soft-thresholding estimator $x(\\lambda) = \\eta_{\\text{soft}}(y, \\lambda)$, where the expectation is over the noise distribution. We are given the observation model $y = x_0 + w$, where $y, x_0 \\in \\mathbb{R}^n$ and $w \\sim \\mathcal{N}(0, \\sigma^2 I_n)$.\n\nSubstituting $x_0 = y - w$ into the risk definition, we have:\n$$R(x(\\lambda), x_0) = \\mathbb{E}[\\|x(\\lambda) - (y - w)\\|_2^2] = \\mathbb{E}[\\|(x(\\lambda) - y) + w\\|_2^2]$$\nExpanding the squared Euclidean norm gives:\n$$R(x(\\lambda), x_0) = \\mathbb{E}[\\|x(\\lambda) - y\\|_2^2 + 2w^T(x(\\lambda) - y) + \\|w\\|_2^2]$$\nBy linearity of expectation, this becomes:\n$$R(x(\\lambda), x_0) = \\mathbb{E}[\\|x(\\lambda) - y\\|_2^2] + 2\\mathbb{E}[w^T(x(\\lambda) - y)] + \\mathbb{E}[\\|w\\|_2^2]$$\nLet's analyze each term:\n-   $\\mathbb{E}[\\|x(\\lambda) - y\\|_2^2]$: This term involves the observation $y$ and the estimator $x(\\lambda)$, which is a function of $y$. The quantity $\\|x(\\lambda) - y\\|_2^2$ can be computed directly from the data.\n-   $\\mathbb{E}[\\|w\\|_2^2]$: Since $w_i \\sim \\mathcal{N}(0, \\sigma^2)$ are independent, $\\mathbb{E}[w_i^2] = \\sigma^2$. Thus, $\\mathbb{E}[\\|w\\|_2^2] = \\mathbb{E}[\\sum_{i=1}^n w_i^2] = \\sum_{i=1}^n \\mathbb{E}[w_i^2] = n\\sigma^2$.\n-   $2\\mathbb{E}[w^T(x(\\lambda) - y)]$: This cross-term involves the unknown noise $w$ and cannot be computed directly. We use Stein's Lemma, which states that for a random vector $w \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ and a weakly differentiable function $g: \\mathbb{R}^n \\to \\mathbb{R}^n$, $\\mathbb{E}[w^T g(y)] = \\sigma^2 \\mathbb{E}[\\nabla \\cdot g(y)]$, where $\\nabla \\cdot g(y) = \\sum_{i=1}^n \\frac{\\partial g_i}{\\partial y_i}$ is the divergence of $g$. Let $g(y) = x(\\lambda) - y$. The estimator $x(\\lambda)$ is the componentwise application of $\\eta_{\\text{soft}}(y_i, \\lambda)$, so $x_i(\\lambda)$ depends only on $y_i$. The function $g_i(y) = \\eta_{\\text{soft}}(y_i, \\lambda) - y_i$ is weakly differentiable. The divergence of $x(\\lambda)$ is:\n$$\\nabla \\cdot x(y, \\lambda) = \\sum_{i=1}^n \\frac{\\partial x_i(\\lambda)}{\\partial y_i} = \\sum_{i=1}^n \\frac{d}{dy_i} \\eta_{\\text{soft}}(y_i, \\lambda)$$\nThe soft-thresholding function is $\\eta_{\\text{soft}}(t, \\lambda) = \\operatorname{sign}(t)\\max(0, |t|-\\lambda)$. Its weak derivative with respect to $t$ is the indicator function $\\mathbb{I}(|t| > \\lambda)$. Therefore, the divergence is the number of components of $y$ whose magnitude exceeds the threshold $\\lambda$:\n$$\\nabla \\cdot x(y, \\lambda) = \\sum_{i=1}^n \\mathbb{I}(|y_i| > \\lambda)$$\nApplying Stein's Lemma to our cross-term:\n$$\\mathbb{E}[w^T(x(\\lambda) - y)] = \\sigma^2 \\mathbb{E}[\\nabla \\cdot (x(\\lambda) - y)] = \\sigma^2 \\mathbb{E}\\left[\\sum_{i=1}^n \\left(\\frac{\\partial x_i(\\lambda)}{\\partial y_i} - 1\\right)\\right] = \\sigma^2 \\mathbb{E}[\\nabla \\cdot x(y, \\lambda) - n]$$\nSubstituting these back into the risk expression:\n$$R(x(\\lambda), x_0) = \\mathbb{E}[\\|x(\\lambda) - y\\|_2^2 + 2\\sigma^2(\\nabla \\cdot x(y, \\lambda) - n) + n\\sigma^2] = \\mathbb{E}[\\|x(\\lambda) - y\\|_2^2 - n\\sigma^2 + 2\\sigma^2 \\nabla \\cdot x(y, \\lambda)]$$\nSURE is obtained by dropping the expectation, yielding an unbiased estimate of the risk based solely on the observed data $y$:\n$$\\text{SURE}(y, \\lambda) = \\sum_{i=1}^n \\min(y_i^2, \\lambda^2) - n\\sigma^2 + 2\\sigma^2 \\sum_{i=1}^n \\mathbb{I}(|y_i| > \\lambda)$$\nLet's analyze the term $\\|x(\\lambda) - y\\|_2^2 = \\sum_{i=1}^n (\\eta_{\\text{soft}}(y_i, \\lambda) - y_i)^2$.\n-   If $|y_i| > \\lambda$, $\\eta_{\\text{soft}}(y_i, \\lambda) - y_i = (y_i - \\operatorname{sign}(y_i)\\lambda) - y_i = -\\operatorname{sign}(y_i)\\lambda$. The squared difference is $\\lambda^2$.\n-   If $|y_i| \\le \\lambda$, $\\eta_{\\text{soft}}(y_i, \\lambda) - y_i = 0 - y_i = -y_i$. The squared difference is $y_i^2$.\nSo, we can write $\\|x(\\lambda) - y\\|_2^2 = \\sum_{i=1}^n \\min(y_i^2, \\lambda^2)$.\nThe final expression for SURE is:\n$$\\text{SURE}(y, \\lambda) = \\sum_{i=1}^n \\min(y_i^2, \\lambda^2) - n\\sigma^2 + 2\\sigma^2 \\sum_{i=1}^n \\mathbb{I}(|y_i| > \\lambda)$$\nThis formula depends only on $y$, $\\sigma^2$, and $\\lambda$, and will be implemented to evaluate thresholds.\n\n2.  **Optimal Threshold Selection**\n\nGiven a discrete grid of thresholds $\\Lambda = \\{\\lambda_0, \\lambda_1, \\ldots, \\lambda_m\\}$, the SURE value is computed for each $\\lambda \\in \\Lambda$. The optimal threshold $\\lambda^\\star$ is chosen to minimize this risk estimate:\n$$\\lambda^\\star = \\arg\\min_{\\lambda \\in \\Lambda} \\text{SURE}(y, \\lambda)$$\nThe problem specifies that if multiple $\\lambda$ values yield the same minimum SURE, the smallest such $\\lambda$ must be chosen. This tie-breaking rule ensures a unique solution.\n\n3.  **Least-Squares Debiasing**\n\nThe soft-thresholding estimator $x(\\lambda)$ is biased, as it shrinks the magnitudes of the non-zero coefficients. To correct this, we perform a least-squares refitting step. First, we identify the support set based on the optimal threshold $\\lambda^\\star$:\n$$S(\\lambda^\\star) = \\{ i \\in \\{1, \\ldots, n\\} : |y_i| > \\lambda^\\star \\}$$\nThe debiased estimator $x^{\\text{debias}}$ is found by minimizing the least-squares objective restricted to this support:\n$$\\text{minimize}_x \\quad \\|y - x\\|_2^2 \\quad \\text{subject to} \\quad \\operatorname{supp}(x) \\subseteq S(\\lambda^\\star)$$\nThe constraint $\\operatorname{supp}(x) \\subseteq S(\\lambda^\\star)$ is equivalent to setting $x_i = 0$ for all $i \\notin S(\\lambda^\\star)$. The objective function becomes:\n$$\\|y - x\\|_2^2 = \\sum_{i=1}^n (y_i - x_i)^2 = \\sum_{i \\in S(\\lambda^\\star)} (y_i - x_i)^2 + \\sum_{i \\notin S(\\lambda^\\star)} (y_i - 0)^2$$\nTo minimize this expression, we only need to minimize the first sum, as the second is constant with respect to the optimization variables. The minimum of $\\sum_{i \\in S(\\lambda^\\star)} (y_i - x_i)^2$ is achieved when $x_i = y_i$ for all $i \\in S(\\lambda^\\star)$.\nThus, the debiased estimator is:\n$$x^{\\text{debias}}_i = \\begin{cases} y_i & \\text{if } i \\in S(\\lambda^\\star) \\text{ (i.e., } |y_i| > \\lambda^\\star) \\\\ 0 & \\text{if } i \\notin S(\\lambda^\\star) \\text{ (i.e., } |y_i| \\le \\lambda^\\star) \\end{cases}$$\nThis corresponds to a Euclidean projection of the observation vector $y$ onto the coordinate subspace defined by the support set $S(\\lambda^\\star)$. The final algorithm will implement these three steps for each test case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        {\n            \"y\": np.array([3.2, -0.1, 0.0, 1.1, -2.7, 0.05]),\n            \"sigma2\": 0.25,\n            \"grid_spec\": (0.0, 2.5, 0.05)\n        },\n        {\n            \"y\": np.array([0.2, -0.15, 0.08, -0.05]),\n            \"sigma2\": 0.04,\n            \"grid_spec\": (0.0, 0.6, 0.01)\n        },\n        {\n            \"y\": np.array([5.0, 0.0, 0.0, -4.8, 0.1]),\n            \"sigma2\": 1.0,\n            \"grid_spec\": (0.0, 6.0, 0.1)\n        },\n        {\n            \"y\": np.array([1.0, -1.0, 1.0, -1.0]),\n            \"sigma2\": 0.09,\n            \"grid_spec\": (0.0, 1.5, 0.01)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _solve_case(case[\"y\"], case[\"sigma2\"], case[\"grid_spec\"])\n        results.append(result)\n\n    # Format the final output string as a list of lists.\n    # The default str() representation of a list is used, which is space-free.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _solve_case(y, sigma2, grid_spec):\n    \"\"\"\n    Solves a single denoising problem case.\n\n    Args:\n        y (np.ndarray): The observed noisy signal.\n        sigma2 (float): The variance of the Gaussian noise.\n        grid_spec (tuple): A tuple (start, end, step) for the lambda grid.\n\n    Returns:\n        list: A list containing the optimal lambda and the debiased signal,\n              all rounded to six decimal places.\n    \"\"\"\n    # Step 1: Generate the grid of thresholds Lambda.\n    # Use np.linspace for robust inclusion of the endpoint.\n    start, end, step = grid_spec\n    num_points = int(round((end - start) / step)) + 1\n    lambda_grid = np.linspace(start, end, num_points)\n\n    # Step 2: Evaluate SURE for each lambda in the grid.\n    n = len(y)\n    y_sq = y**2\n    abs_y = np.abs(y)\n    sure_values = []\n\n    for lambda_val in lambda_grid:\n        lambda_sq = lambda_val**2\n        \n        # Divergence term: sum(I(|y_i| > lambda))\n        divergence = np.sum(abs_y > lambda_val)\n        \n        # MSE term: ||x_hat - y||^2 = sum(min(y_i^2, lambda^2))\n        mse_term = np.sum(np.minimum(y_sq, lambda_sq))\n        \n        # SURE formula: ||x_hat - y||^2 - n*sigma^2 + 2*sigma^2*div(x_hat)\n        sure = mse_term - n * sigma2 + 2 * sigma2 * divergence\n        sure_values.append(sure)\n\n    # Step 3: Find the optimal lambda that minimizes SURE.\n    # If multiple lambdas yield the minimum SURE, the one with the smallest\n    # index (and thus smallest value) is chosen. np.argmin() does this.\n    sure_values = np.array(sure_values)\n    min_sure_val = np.min(sure_values)\n    \n    # To handle \"within numerical precision\", we use np.isclose.\n    # We find all indices that are close to the minimum and take the first one.\n    min_indices = np.where(np.isclose(sure_values, min_sure_val))[0]\n    best_lambda_index = min_indices[0]\n    lambda_star = lambda_grid[best_lambda_index]\n\n    # Step 4: Perform least-squares debiasing (refitting).\n    # The debiased estimate is the projection of y onto the support.\n    support_mask = np.abs(y) > lambda_star\n    x_debiased = np.zeros_like(y, dtype=float)\n    x_debiased[support_mask] = y[support_mask]\n\n    # Step 5: Format the output as specified.\n    # Round all floating-point numbers to six decimal places.\n    lambda_star_rounded = round(lambda_star, 6)\n    x_debiased_rounded = [round(val, 6) for val in x_debiased]\n\n    return [lambda_star_rounded] + x_debiased_rounded\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3442574"}]}