{"hands_on_practices": [{"introduction": "Comparing the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) is a cornerstone of classical model selection. This exercise provides a concrete setting to see their differing behaviors in action. By working within an orthonormal design, the complexities of solving for model coefficients are stripped away, allowing you to focus purely on how AIC and BIC trade goodness-of-fit against model complexity, and to observe how BIC's stronger penalty often favors more parsimonious models [@problem_id:3452903].", "problem": "Consider a linear observation model in compressed sensing with Gaussian noise given by $y \\in \\mathbb{R}^{n}$ and $X \\in \\mathbb{R}^{n \\times p}$, where the columns of $X$ are orthonormal so that $X^{\\top}X = I_{p}$. Assume $y = X \\beta^{\\star} + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$. Define the best-$k$ term approximation to be the $k$-term linear model that minimizes the empirical squared error over all subsets of $k$ columns of $X$. Let $z = X^{\\top} y \\in \\mathbb{R}^{p}$ and let $|z|_{(1)} \\geq |z|_{(2)} \\geq \\cdots \\geq |z|_{(p)}$ denote the order statistics of the magnitudes of the entries of $z$, with corresponding signed values $z_{(j)}$.\n\nStarting from the Gaussian log-likelihood and the orthonormality of $X$, derive the explicit expression for the residual sum of squares as a function of $k$ along the best-$k$ term path. Then, using the definitions of the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) as information criteria derived from the maximized Gaussian likelihood with a penalty proportional to the number of free parameters, obtain their expressions as functions of $k$ along this path. You may ignore additive constants that do not depend on $k$.\n\nNow specialize to the following data:\n- $n = 120$, $p = 10$,\n- $\\|y\\|^{2} = 300$,\n- $z = X^{\\top} y = [\\,5.2,\\,-3.1,\\,2.7,\\,-2.2,\\,1.9,\\,-1.5,\\,1.2,\\,0.9,\\,-0.6,\\,0.4\\,]$.\n\nCompute the values of $k$ that minimize AIC and BIC, respectively, along the best-$k$ term path. In case of ties, choose the smaller $k$. Report your final answer as the pair $(k_{\\mathrm{AIC}}, k_{\\mathrm{BIC}})$. The final answer must be given as a single entity and does not require rounding.", "solution": "The maximized Gaussian log-likelihood, which forms the basis for both AIC and BIC, depends on the residual sum of squares (RSS). For a model using a subset of predictors $S$ with $|S|=k$, the RSS is minimized when the coefficients $\\beta_S$ are chosen by least squares. Given the orthonormal design ($X_S^{\\top}X_S = I_k$), the least squares estimate is $\\hat{\\beta}_S = X_S^{\\top}y$, and the corresponding predicted values are $\\hat{y}_S = X_S X_S^{\\top}y$.\n\nBy the Pythagorean theorem on the vector space $\\mathbb{R}^n$, the RSS can be expressed as:\n$$ \\mathrm{RSS}(S) = \\|y - \\hat{y}_S\\|^2 = \\|y\\|^2 - \\|\\hat{y}_S\\|^2 $$\nThe squared norm of the projection is $\\|\\hat{y}_S\\|^2 = \\|X_S X_S^{\\top}y\\|^2 = (X_S^{\\top}y)^{\\top}(X_S^{\\top}X_S)(X_S^{\\top}y) = \\|X_S^{\\top}y\\|^2 = \\sum_{j \\in S} (X^{\\top}y)_j^2 = \\sum_{j \\in S} z_j^2$.\nThus, $\\mathrm{RSS}(S) = \\|y\\|^2 - \\sum_{j \\in S} z_j^2$.\n\nTo find the best-$k$ term model, we must choose the set $S$ of size $k$ that minimizes the RSS, which is equivalent to maximizing $\\sum_{j \\in S} z_j^2$. This is achieved by selecting the $k$ predictors corresponding to the $k$ largest values of $|z_j|$. Let $z_{(j)}^2$ be the $j$-th largest squared magnitude. The RSS for the best-$k$ term model is:\n$$ \\mathrm{RSS}_k = \\|y\\|^2 - \\sum_{j=1}^{k} z_{(j)}^2 $$\nFor a model with $k$ regression coefficients plus an estimated variance, we have $k+1$ parameters. The general forms for AIC and BIC are:\n$$ \\mathrm{AIC}(k) = n\\ln(\\mathrm{RSS}_k) + 2(k+1) $$\n$$ \\mathrm{BIC}(k) = n\\ln(\\mathrm{RSS}_k) + (k+1)\\ln(n) $$\nWe seek to minimize these functions over $k \\in \\{0, 1, \\dots, p\\}$. Ignoring additive constants that do not depend on $k$, the criteria to minimize are:\n$$ \\mathrm{AIC}(k) \\propto n\\ln(\\mathrm{RSS}_k) + 2k $$\n$$ \\mathrm{BIC}(k) \\propto n\\ln(\\mathrm{RSS}_k) + k\\ln(n) $$\nGiven data: $n = 120$, $p = 10$, $\\|y\\|^2 = 300$, and $z = [\\,5.2,\\,-3.1,\\,2.7,\\,-2.2,\\,1.9,\\,-1.5,\\,1.2,\\,0.9,\\,-0.6,\\,0.4\\,]$.\nThe squared magnitudes of $z$ are already ordered:\n$z_{(1)}^2=27.04, z_{(2)}^2=9.61, z_{(3)}^2=7.29, z_{(4)}^2=4.84, z_{(5)}^2=3.61, z_{(6)}^2=2.25, z_{(7)}^2=1.44, z_{(8)}^2=0.81, z_{(9)}^2=0.36, z_{(10)}^2=0.16$.\n\nWe compute $\\mathrm{RSS}_k = 300 - \\sum_{j=1}^{k} z_{(j)}^2$ for $k=0, \\dots, 10$:\n- $k=0: \\mathrm{RSS}_0 = 300$\n- $k=1: \\mathrm{RSS}_1 = 300 - 27.04 = 272.96$\n- $k=2: \\mathrm{RSS}_2 = 272.96 - 9.61 = 263.35$\n- $k=3: \\mathrm{RSS}_3 = 263.35 - 7.29 = 256.06$\n- $k=4: \\mathrm{RSS}_4 = 256.06 - 4.84 = 251.22$\n- $k=5: \\mathrm{RSS}_5 = 251.22 - 3.61 = 247.61$\n\nNow we evaluate the criteria (up to an additive constant):\n**For AIC:** Minimize $120\\ln(\\mathrm{RSS}_k) + 2k$\n- $k=0: 120\\ln(300) + 0 \\approx 684.46$\n- $k=1: 120\\ln(272.96) + 2 \\approx 673.12 + 2 = 675.12$\n- $k=2: 120\\ln(263.35) + 4 \\approx 668.82 + 4 = 672.82$\n- $k=3: 120\\ln(256.06) + 6 \\approx 665.45 + 6 = 671.45$\n- $k=4: 120\\ln(251.22) + 8 \\approx 663.16 + 8 = 671.16$\n- $k=5: 120\\ln(247.61) + 10 \\approx 661.42 + 10 = 671.42$\n- $k=6: 120\\ln(245.36) + 12 \\approx 660.32 + 12 = 672.32$\nThe minimum value occurs at $k=4$. Therefore, $k_{\\mathrm{AIC}} = 4$.\n\n**For BIC:** Minimize $120\\ln(\\mathrm{RSS}_k) + k\\ln(120)$. With $\\ln(120) \\approx 4.7875$.\n- $k=0: 120\\ln(300) + 0 \\approx 684.46$\n- $k=1: 120\\ln(272.96) + 1\\ln(120) \\approx 673.12 + 4.79 = 677.91$\n- $k=2: 120\\ln(263.35) + 2\\ln(120) \\approx 668.82 + 9.58 = 678.40$\n- $k=3: 120\\ln(256.06) + 3\\ln(120) \\approx 665.45 + 14.36 = 679.81$\nThe values increase after $k=1$, so the minimum is at $k=1$. Therefore, $k_{\\mathrm{BIC}} = 1$.\n\nThe optimal model sizes are $(k_{\\mathrm{AIC}}, k_{\\mathrm{BIC}}) = (4, 1)$.", "answer": "$$\n\\boxed{(4, 1)}\n$$", "id": "3452903"}, {"introduction": "Information criteria are not arbitrary rules but are derived from fundamental principles of statistical estimation. This practice challenges you to move beyond simply applying formulas and to engage with the theoretical underpinnings of AIC. By deriving the appropriate penalty term for a model with non-Gaussian (Laplace) noise, you will demonstrate that the familiar $2 \\times (\\text{number of parameters})$ penalty is a general asymptotic result of maximum likelihood theory, thereby deepening your understanding of the broad applicability of these powerful tools [@problem_id:3452916].", "problem": "Consider linear regression with independent and identically distributed noise modeled by the Laplace distribution. Specifically, suppose $y_{i} \\in \\mathbb{R}$ and $x_{i} \\in \\mathbb{R}^{p}$ satisfy $y_{i} = x_{i}^{\\top}\\beta + \\epsilon_{i}$ for $i=1,\\dots,n$, where $\\epsilon_{i}$ are independent and identically distributed Laplace random variables with mean $0$ and scale parameter $b>0$, having density $f(\\epsilon) = \\frac{1}{2b}\\exp\\!\\big(-|\\epsilon|/b\\big)$. Assume a fixed design matrix $X \\in \\mathbb{R}^{n \\times p}$ and consider a candidate sparse model that uses only an index set $S \\subset \\{1,\\dots,p\\}$ with $|S| = k$, with the corresponding parameter vector $\\beta_{S} \\in \\mathbb{R}^{k}$ (the remaining components are constrained to be zero). The parameters $(\\beta_{S}, b)$ are estimated by maximum likelihood, which coincides with minimizing the sum of absolute residuals with respect to $\\beta_{S}$ and estimating $b$ by maximizing the Laplace likelihood.\n\nDefine the training deviance by $D_{\\mathrm{train}}(S) = -2\\,\\ell\\big(\\hat{\\beta}_{S}, \\hat{b}; y\\big)$, where $\\ell(\\beta, b; y)$ is the Laplace log-likelihood for the observed sample under the model indexed by $S$, and $(\\hat{\\beta}_{S}, \\hat{b})$ are the maximum likelihood estimates under that model. Define the predictive deviance for a fresh, independent sample $y^{\\star}$ drawn from the same data-generating process, conditional on the training design $X$, by $D_{\\mathrm{pred}}(S) = -2\\,\\mathbb{E}_{y^{\\star}|X}\\!\\big[\\ell\\big(\\hat{\\beta}_{S}, \\hat{b}; y^{\\star}\\big)\\big]$, where the expectation is with respect to the sampling distribution of $y^{\\star}$ and conditioning is on $X$ and the training sample used to compute $(\\hat{\\beta}_{S}, \\hat{b})$.\n\nStarting from the definitions above and using asymptotic properties of maximum likelihood estimators under correctly specified parametric models, formulate an information criterion of the form $\\mathrm{IC}(S) = D_{\\mathrm{train}}(S) + \\mathrm{pen}(k,n)$ that yields an asymptotically unbiased estimator of the predictive deviance, in the sense that $\\mathbb{E}\\!\\big[\\mathrm{IC}(S)\\big] = \\mathbb{E}\\!\\big[D_{\\mathrm{pred}}(S)\\big] + o(1)$ as $n \\to \\infty$. Derive the explicit closed-form expression for the penalty $\\mathrm{pen}(k,n)$ in terms of $k$ and any other necessary quantities. Your final answer must be a single closed-form analytic expression. If you need to present a numerical constant, retain it exactly; do not approximate it numerically.", "solution": "The problem requires the derivation of a penalty term, $\\mathrm{pen}(k,n)$, for an information criterion of the form $\\mathrm{IC}(S) = D_{\\mathrm{train}}(S) + \\mathrm{pen}(k,n)$. This criterion must provide an asymptotically unbiased estimator of the predictive deviance, $D_{\\mathrm{pred}}(S)$, in the sense that $\\mathbb{E}[\\mathrm{IC}(S)] = \\mathbb{E}[D_{\\mathrm{pred}}(S)] + o(1)$ as the sample size $n \\to \\infty$. The expectation is taken over the distribution of the training data $y$.\n\nFrom the condition $\\mathbb{E}[D_{\\mathrm{train}}(S) + \\mathrm{pen}(k,n)] = \\mathbb{E}[D_{\\mathrm{pred}}(S)] + o(1)$, and assuming the penalty $\\mathrm{pen}(k,n)$ is non-random, we must derive an expression for the asymptotic value of the expected optimism, which is defined as the difference between the expected predictive deviance and the expected training deviance:\n$$\n\\mathrm{pen}(k,n) \\approx \\mathbb{E}[D_{\\mathrm{pred}}(S)] - \\mathbb{E}[D_{\\mathrm{train}}(S)]\n$$\nThis derivation follows the general logic used for Akaike's Information Criterion (AIC), relying on the asymptotic properties of Maximum Likelihood Estimators (MLEs). The problem statement specifies that a model indexed by a set $S$ with $|S|=k$ involves estimating a parameter vector $\\beta_S \\in \\mathbb{R}^k$ and the Laplace distribution's scale parameter $b$. Thus, the total number of parameters to be estimated for this model is $d = k+1$. Let $\\theta = (\\beta_S, b)$ be the $(k+1)$-dimensional parameter vector. Let $\\hat{\\theta} = (\\hat{\\beta}_S, \\hat{b})$ denote the MLE of $\\theta$ obtained from the training data $y$, and let $\\theta_0$ be the true, unknown parameter vector that generated the data. The assumption of a \"correctly specified\" model implies that such a $\\theta_0$ exists within the considered family of models.\n\nThe training deviance is $D_{\\mathrm{train}}(S) = -2\\ell(\\hat{\\theta}; y)$, where $\\ell(\\theta;y)$ is the log-likelihood function for the training data $y$. The predictive deviance is $D_{\\mathrm{pred}}(S) = -2\\mathbb{E}_{y^\\star|X}[\\ell(\\hat{\\theta}; y^\\star)]$, where the expectation is over a new, independent dataset $y^\\star$ from the same generating process.\n\nLet us analyze the relationship between the training deviance and the log-likelihood evaluated at the true parameter $\\theta_0$. We perform a second-order Taylor series expansion of $\\ell(\\theta_0; y)$ around the MLE $\\hat{\\theta}$:\n$$\n\\ell(\\theta_0; y) \\approx \\ell(\\hat{\\theta}; y) + (\\theta_0 - \\hat{\\theta})^{\\top} \\nabla\\ell(\\hat{\\theta}; y) + \\frac{1}{2}(\\theta_0 - \\hat{\\theta})^{\\top} \\nabla^2\\ell(\\hat{\\theta}; y) (\\theta_0 - \\hat{\\theta})\n$$\nBy definition of the MLE, the score (gradient) at $\\hat{\\theta}$ is zero, i.e., $\\nabla\\ell(\\hat{\\theta}; y) = 0$. Multiplying by $-2$, we get:\n$$\n-2\\ell(\\theta_0; y) \\approx -2\\ell(\\hat{\\theta}; y) - (\\theta_0 - \\hat{\\theta})^{\\top} \\nabla^2\\ell(\\hat{\\theta}; y) (\\theta_0 - \\hat{\\theta}) = D_{\\mathrm{train}}(S) + (\\hat{\\theta} - \\theta_0)^{\\top} [-\\nabla^2\\ell(\\hat{\\theta}; y)] (\\hat{\\theta} - \\theta_0)\n$$\nUnder standard asymptotic theory for MLEs, for large $n$, the MLE $\\hat{\\theta}$ is consistent, i.e., $\\hat{\\theta} \\to \\theta_0$ in probability. Also, the observed Fisher information, $-\\frac{1}{n}\\nabla^2\\ell(\\hat{\\theta}; y)$, converges to the Fisher information matrix per observation, $I(\\theta_0)$. Thus, $-\\nabla^2\\ell(\\hat{\\theta}; y) \\approx n I(\\theta_0)$. The asymptotic distribution of the MLE is given by $\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\xrightarrow{d} N(0, I(\\theta_0)^{-1})$.\nTaking the expectation over the training data $y$:\n$$\n\\mathbb{E}[-2\\ell(\\theta_0; y)] \\approx \\mathbb{E}[D_{\\mathrm{train}}(S)] + \\mathbb{E}\\left[(\\hat{\\theta} - \\theta_0)^{\\top} (n I(\\theta_0)) (\\hat{\\theta} - \\theta_0)\\right]\n$$\nThe quadratic form follows, asymptotically, the trace of the product of the matrices: $\\mathbb{E}[Z^\\top A Z] = \\mathbb{E}[Z]^\\top A \\mathbb{E}[Z] + \\mathrm{tr}(A \\cdot \\mathrm{Cov}(Z))$. Here $Z = (\\hat{\\theta} - \\theta_0)$ and $A = nI(\\theta_0)$. Asymptotically, $\\mathbb{E}[Z] \\approx 0$ and $\\mathrm{Cov}(Z) \\approx (nI(\\theta_0))^{-1}$. The expectation of the quadratic term is asymptotically $\\mathrm{tr}(nI(\\theta_0) \\cdot (nI(\\theta_0))^{-1}) = \\mathrm{tr}(I_d) = d$, where $d=k+1$ is the dimension of $\\theta$.\nSo, we have the first relation:\n$$\n\\mathbb{E}[-2\\ell(\\theta_0; y)] \\approx \\mathbb{E}[D_{\\mathrm{train}}(S)] + d\n$$\n\nNext, we analyze the predictive deviance. We expand the log-likelihood for the new data $y^\\star$, $\\ell(\\hat{\\theta}; y^\\star)$, around the true parameter $\\theta_0$:\n$$\n\\ell(\\hat{\\theta}; y^\\star) \\approx \\ell(\\theta_0; y^\\star) + (\\hat{\\theta} - \\theta_0)^{\\top} \\nabla\\ell(\\theta_0; y^\\star) + \\frac{1}{2}(\\hat{\\theta} - \\theta_0)^{\\top} \\nabla^2\\ell(\\theta_0; y^\\star) (\\hat{\\theta} - \\theta_0)\n$$\nThe predictive deviance involves an expectation over $y^\\star$. Noting that $\\hat{\\theta}$ is fixed with respect to $y^\\star$:\n$$\n\\mathbb{E}_{y^\\star}[\\ell(\\hat{\\theta}; y^\\star)] \\approx \\mathbb{E}_{y^\\star}[\\ell(\\theta_0; y^\\star)] + (\\hat{\\theta} - \\theta_0)^{\\top} \\mathbb{E}_{y^\\star}[\\nabla\\ell(\\theta_0; y^\\star)] + \\frac{1}{2}(\\hat{\\theta} - \\theta_0)^{\\top} \\mathbb{E}_{y^\\star}[\\nabla^2\\ell(\\theta_0; y^\\star)] (\\hat{\\theta} - \\theta_0)\n$$\nAt the true parameter $\\theta_0$, we have $\\mathbb{E}_{y^\\star}[\\nabla\\ell(\\theta_0; y^\\star)] = 0$ and $\\mathbb{E}_{y^\\star}[\\nabla^2\\ell(\\theta_0; y^\\star)] = -n I(\\theta_0)$. Thus:\n$$\n\\mathbb{E}_{y^\\star}[\\ell(\\hat{\\theta}; y^\\star)] \\approx \\mathbb{E}_{y^\\star}[\\ell(\\theta_0; y^\\star)] - \\frac{1}{2}(\\hat{\\theta} - \\theta_0)^{\\top} (n I(\\theta_0)) (\\hat{\\theta} - \\theta_0)\n$$\nMultiplying by $-2$ gives:\n$$\nD_{\\mathrm{pred}}(S) = -2\\mathbb{E}_{y^\\star}[\\ell(\\hat{\\theta}; y^\\star)] \\approx -2\\mathbb{E}_{y^\\star}[\\ell(\\theta_0; y^\\star)] + (\\hat{\\theta} - \\theta_0)^{\\top} (n I(\\theta_0)) (\\hat{\\theta} - \\theta_0)\n$$\nNow, we take the expectation over the training data $y$:\n$$\n\\mathbb{E}_y[D_{\\mathrm{pred}}(S)] \\approx \\mathbb{E}_y[-2\\mathbb{E}_{y^\\star}[\\ell(\\theta_0; y^\\star)]] + \\mathbb{E}_y\\left[(\\hat{\\theta} - \\theta_0)^{\\top} (n I(\\theta_0)) (\\hat{\\theta} - \\theta_0)\\right]\n$$\nSince $y$ and $y^\\star$ are i.i.d. samples, $\\mathbb{E}_y[-2\\mathbb{E}_{y^\\star}[\\ell(\\theta_0; y^\\star)]] = \\mathbb{E}_y[-2\\ell(\\theta_0; y)]$. The second term, as calculated before, has an expectation that is asymptotically $d$.\nThis gives the second relation:\n$$\n\\mathbb{E}[D_{\\mathrm{pred}}(S)] \\approx \\mathbb{E}[-2\\ell(\\theta_0; y)] + d\n$$\n\nCombining the two asymptotic relations:\n$$\n\\mathbb{E}[D_{\\mathrm{train}}(S)] \\approx \\mathbb{E}[-2\\ell(\\theta_0; y)] - d\n$$\n$$\n\\mathbb{E}[D_{\\mathrm{pred}}(S)] \\approx \\mathbb{E}[-2\\ell(\\theta_0; y)] + d\n$$\nSubtracting the first from the second, we find the expected optimism:\n$$\n\\mathbb{E}[D_{\\mathrm{pred}}(S)] - \\mathbb{E}[D_{\\mathrm{train}}(S)] \\approx (\\mathbb{E}[-2\\ell(\\theta_0; y)] + d) - (\\mathbb{E}[-2\\ell(\\theta_0; y)] - d) = 2d\n$$\nTherefore, the penalty that corrects for the optimistic bias of the training deviance is asymptotically $2d$. The problem specifies that a candidate model with $|S|=k$ estimates $k$ regression coefficients in $\\beta_S$ and the scale parameter $b$. The total number of estimated parameters is $d = k+1$.\nThe penalty term is:\n$$\n\\mathrm{pen}(k,n) = 2d = 2(k+1)\n$$\nThis is the penalty for the Akaike Information Criterion (AIC) generalized to this modeling context. Although the Laplace likelihood is not everywhere differentiable, which violates classical regularity conditions, the AIC result is known to hold under more general conditions that cover this case. The problem's direction to use standard asymptotic properties confirms this is the intended path. The resulting penalty depends on $k$ but not on $n$, which is consistent with the standard AIC formulation.", "answer": "$$ \\boxed{2(k+1)} $$", "id": "3452916"}, {"introduction": "Beyond likelihood-based penalties, a powerful alternative for model tuning is to directly estimate an estimator's predictive risk. This exercise introduces Stein’s Unbiased Risk Estimate (SURE), a remarkable analytical tool for estimating the mean-squared error in Gaussian denoising problems. You will derive the exact \"degrees of freedom\" for the widely used soft-thresholding estimator and use this to build the SURE criterion, providing a direct, data-driven method for selecting the optimal regularization threshold that serves as a potent alternative to cross-validation [@problem_id:3452918].", "problem": "Consider the orthogonal design setting in compressed sensing, where the observed response vector is modeled as $Y = \\theta + \\epsilon$ in $\\mathbb{R}^{n}$, with $\\epsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ and unknown mean vector $\\theta \\in \\mathbb{R}^{n}$. Define the soft-thresholding estimator at threshold $\\lambda \\ge 0$ coordinatewise by\n$$\n\\big(\\eta_{\\lambda}(y)\\big)_{i} = \\operatorname{sign}(y_{i}) \\cdot \\big(|y_{i}| - \\lambda\\big)_{+}, \\quad i = 1, \\dots, n,\n$$\nwhere $(t)_{+} = \\max\\{t, 0\\}$.\n\nUsing only first principles grounded in Gaussian calculus (specifically Stein’s identity for Gaussian expectations) and the definition of degrees of freedom\n$$\n\\mathrm{df}(g) = \\frac{1}{\\sigma^{2}} \\sum_{i=1}^{n} \\mathrm{Cov}\\big(g_{i}(Y), Y_{i}\\big),\n$$\nperform the following tasks:\n\n1. Derive the exact degrees of freedom of the soft-thresholding estimator $\\eta_{\\lambda}$ and show that it equals the expected number of nonzero coordinates under the sampling distribution of $Y$. Express this expectation explicitly in terms of $\\theta$, $\\sigma$, and $\\lambda$.\n\n2. Based on Stein’s Unbiased Risk Estimate (SURE), derive an explicit pathwise unbiased estimator of the prediction risk $\\mathbb{E}\\big[\\|\\eta_{\\lambda}(Y) - \\theta\\|_{2}^{2}\\big]$ in terms of the observed data vector $y$, the threshold $\\lambda$, and the noise level $\\sigma$.\n\n3. Use the structure you obtain to argue that the SURE as a function of $\\lambda$ is piecewise smooth with changes only at the absolute values of the coordinates of $y$. Conclude that any global minimizer of SURE over $\\lambda \\ge 0$ must lie in the finite set $\\{0, |y|_{(1)}, |y|_{(2)}, \\dots, |y|_{(n)}\\}$, where $|y|_{(1)} \\le |y|_{(2)} \\le \\dots \\le |y|_{(n)}$ are the order statistics of $\\{|y_{i}|\\}_{i=1}^{n}$.\n\nYour final answer must be a single closed-form analytic expression for the SURE-minimizing threshold $\\hat{\\lambda}$ as a function of $y$ and $\\sigma$. No numerical evaluation is required.", "solution": "The solution is presented in three parts as requested. The observed data vector is $Y \\in \\mathbb{R}^n$ with $Y \\sim \\mathcal{N}(\\theta, \\sigma^2 I_n)$, meaning the coordinates $Y_i$ are independent, with $Y_i \\sim \\mathcal{N}(\\theta_i, \\sigma^2)$. The soft-thresholding estimator is given by $\\eta_{\\lambda}(y)$, where its $i$-th coordinate is $\\eta_{\\lambda, i}(y_i) = \\operatorname{sign}(y_i) (|y_i|-\\lambda)_+$.\n\n### Part 1: Degrees of Freedom of the Soft-Thresholding Estimator\n\nThe degrees of freedom of an estimator $g(Y)$ is defined as $\\mathrm{df}(g) = \\frac{1}{\\sigma^2} \\sum_{i=1}^n \\mathrm{Cov}(g_i(Y), Y_i)$. For the soft-thresholding estimator $\\eta_{\\lambda}$, the coordinates are processed independently, so $g_i(Y) = \\eta_{\\lambda, i}(Y_i)$. The sum can be analyzed term by term.\n\nWe use Stein's identity for a Gaussian random variable $Z \\sim \\mathcal{N}(\\mu, \\sigma^2)$ and a weakly differentiable function $h$. The identity states $\\mathrm{Cov}(h(Z), Z) = \\sigma^2 \\mathbb{E}[h'(Z)]$. Applying this to each coordinate $Y_i \\sim \\mathcal{N}(\\theta_i, \\sigma^2)$ with the function $h(y_i) = \\eta_{\\lambda, i}(y_i)$, we get:\n$$\n\\mathrm{Cov}(\\eta_{\\lambda, i}(Y_i), Y_i) = \\sigma^2 \\mathbb{E}\\left[\\frac{d}{dY_i}\\eta_{\\lambda, i}(Y_i)\\right]\n$$\nThe function $\\eta_{\\lambda, i}(y_i)$ can be written piecewise:\n$$\n\\eta_{\\lambda, i}(y_i) =\n\\begin{cases}\ny_i - \\lambda & \\text{if } y_i > \\lambda \\\\\n0 & \\text{if } -\\lambda \\le y_i \\le \\lambda \\\\\ny_i + \\lambda & \\text{if } y_i < -\\lambda\n\\end{cases}\n$$\nThis function is continuous. Its weak derivative with respect to $y_i$ is given by:\n$$\n\\frac{d}{dy_i}\\eta_{\\lambda, i}(y_i) =\n\\begin{cases}\n1 & \\text{if } y_i > \\lambda \\\\\n0 & \\text{if } -\\lambda < y_i < \\lambda \\\\\n1 & \\text{if } y_i < -\\lambda\n\\end{cases}\n$$\nThis derivative can be written compactly using an indicator function as $\\mathbf{1}_{\\{|y_i| > \\lambda\\}}$.\nSubstituting this into the expectation, we have:\n$$\n\\mathrm{Cov}(\\eta_{\\lambda, i}(Y_i), Y_i) = \\sigma^2 \\mathbb{E}[\\mathbf{1}_{\\{|Y_i| > \\lambda\\}}] = \\sigma^2 P(|Y_i| > \\lambda)\n$$\nSumming over all coordinates $i=1, \\dots, n$, the total degrees of freedom are:\n$$\n\\mathrm{df}(\\eta_{\\lambda}) = \\frac{1}{\\sigma^2} \\sum_{i=1}^n \\sigma^2 P(|Y_i| > \\lambda) = \\sum_{i=1}^n P(|Y_i| > \\lambda)\n$$\nNow, we show this equals the expected number of nonzero coordinates of $\\eta_{\\lambda}(Y)$. A coordinate $\\eta_{\\lambda, i}(Y_i)$ is nonzero if and only if $|Y_i| > \\lambda$. Let $Z_i = \\mathbf{1}_{\\{\\eta_{\\lambda, i}(Y_i) \\neq 0\\}} = \\mathbf{1}_{\\{|Y_i| > \\lambda\\}}$ be an indicator variable for the $i$-th coordinate being nonzero. The total number of nonzero coordinates is $\\sum_{i=1}^n Z_i$. Its expectation is:\n$$\n\\mathbb{E}\\left[\\sum_{i=1}^n Z_i\\right] = \\sum_{i=1}^n \\mathbb{E}[Z_i] = \\sum_{i=1}^n P(\\eta_{\\lambda, i}(Y_i) \\neq 0) = \\sum_{i=1}^n P(|Y_i| > \\lambda)\n$$\nThis is identical to the expression for $\\mathrm{df}(\\eta_{\\lambda})$.\nTo express this explicitly, we use the fact that $Y_i \\sim \\mathcal{N}(\\theta_i, \\sigma^2)$. Let $\\Phi(\\cdot)$ be the cumulative distribution function of the standard normal distribution $\\mathcal{N}(0, 1)$.\n$$\nP(|Y_i| > \\lambda) = P(Y_i > \\lambda) + P(Y_i < -\\lambda) = P\\left(\\frac{Y_i - \\theta_i}{\\sigma} > \\frac{\\lambda - \\theta_i}{\\sigma}\\right) + P\\left(\\frac{Y_i - \\theta_i}{\\sigma} < \\frac{-\\lambda - \\theta_i}{\\sigma}\\right)\n$$\n$$\nP(|Y_i| > \\lambda) = \\left(1 - \\Phi\\left(\\frac{\\lambda - \\theta_i}{\\sigma}\\right)\\right) + \\Phi\\left(\\frac{-\\lambda - \\theta_i}{\\sigma}\\right) = \\Phi\\left(\\frac{\\theta_i - \\lambda}{\\sigma}\\right) + \\Phi\\left(\\frac{-\\theta_i - \\lambda}{\\sigma}\\right)\n$$\nSo, the degrees of freedom are $\\mathrm{df}(\\eta_{\\lambda}) = \\sum_{i=1}^n \\left[ \\Phi\\left(\\frac{\\theta_i - \\lambda}{\\sigma}\\right) + \\Phi\\left(\\frac{-\\theta_i - \\lambda}{\\sigma}\\right) \\right]$.\n\n### Part 2: Stein's Unbiased Risk Estimate (SURE)\n\nThe prediction risk is $R(\\lambda) = \\mathbb{E}[\\|\\eta_{\\lambda}(Y) - \\theta\\|_2^2]$. We can decompose the risk as:\n$$\nR(\\lambda) = \\mathbb{E}[\\|\\eta_{\\lambda}(Y) - Y + Y - \\theta\\|_2^2] = \\mathbb{E}[\\|\\eta_{\\lambda}(Y) - Y\\|_2^2] + \\mathbb{E}[\\|Y - \\theta\\|_2^2] + 2\\mathbb{E}[\\langle \\eta_{\\lambda}(Y) - Y, Y - \\theta \\rangle]\n$$\nThe middle term is $\\mathbb{E}[\\|\\epsilon\\|_2^2] = n\\sigma^2$. The cross-term can be analyzed using Stein's identity.\n$$\n\\mathbb{E}[\\langle \\eta_{\\lambda}(Y) - Y, Y - \\theta \\rangle] = \\sum_{i=1}^n \\mathbb{E}[(\\eta_{\\lambda, i}(Y_i) - Y_i)(Y_i - \\theta_i)]\n$$\nLet $h_i(y_i) = \\eta_{\\lambda, i}(y_i) - y_i$. Using Stein's identity $\\mathbb{E}[h_i(Y_i)(Y_i - \\theta_i)] = \\sigma^2 \\mathbb{E}[h_i'(Y_i)]$:\n$$\n\\mathbb{E}[h_i(Y_i)(Y_i - \\theta_i)] = \\sigma^2 \\mathbb{E}\\left[\\frac{d}{dY_i}(\\eta_{\\lambda, i}(Y_i) - Y_i)\\right] = \\sigma^2 \\mathbb{E}[\\mathbf{1}_{\\{|Y_i| > \\lambda\\}} - 1]\n$$\nSubstituting this back into the risk expansion:\n$$\nR(\\lambda) = \\mathbb{E}[\\|\\eta_{\\lambda}(Y) - Y\\|_2^2] + n\\sigma^2 + 2\\sum_{i=1}^n \\sigma^2 \\mathbb{E}[\\mathbf{1}_{\\{|Y_i| > \\lambda\\}} - 1]\n$$\n$$\nR(\\lambda) = \\mathbb{E}[\\|\\eta_{\\lambda}(Y) - Y\\|_2^2] + n\\sigma^2 + 2\\sigma^2\\mathbb{E}\\left[\\sum_{i=1}^n \\mathbf{1}_{\\{|Y_i| > \\lambda\\}}\\right] - 2n\\sigma^2\n$$\n$$\nR(\\lambda) = \\mathbb{E}\\left[ \\|\\eta_{\\lambda}(Y) - Y\\|_2^2 - n\\sigma^2 + 2\\sigma^2\\sum_{i=1}^n \\mathbf{1}_{\\{|Y_i| > \\lambda\\}} \\right]\n$$\nBy the law of total expectation, the expression inside the expectation is an unbiased estimator of the risk. This is SURE. For an observed data vector $y$, the estimate is:\n$$\n\\mathrm{SURE}(y, \\lambda) = \\|\\eta_{\\lambda}(y) - y\\|_2^2 - n\\sigma^2 + 2\\sigma^2\\sum_{i=1}^n \\mathbf{1}_{\\{|y_i| > \\lambda\\}}\n$$\nWe can write the first term more explicitly. For each coordinate $i$:\n$$\n(\\eta_{\\lambda, i}(y_i) - y_i)^2 =\n\\begin{cases}\n(y_i - \\lambda - y_i)^2 = \\lambda^2 & \\text{if } y_i > \\lambda \\\\\n(0 - y_i)^2 = y_i^2 & \\text{if } |y_i| \\le \\lambda \\\\\n(y_i + \\lambda - y_i)^2 = \\lambda^2 & \\text{if } y_i < -\\lambda\n\\end{cases}\n$$\nThis is equivalent to $\\min(y_i^2, \\lambda^2)$. Thus, the final pathwise unbiased risk estimator is:\n$$\n\\mathrm{SURE}(y, \\lambda) = \\sum_{i=1}^n \\min(y_i^2, \\lambda^2) - n\\sigma^2 + 2\\sigma^2\\sum_{i=1}^n \\mathbf{1}_{\\{|y_i| > \\lambda\\}}\n$$\n\n### Part 3: Minimization of SURE\n\nWe seek to find $\\hat{\\lambda} = \\operatorname{argmin}_{\\lambda \\ge 0} \\mathrm{SURE}(y, \\lambda)$. This is equivalent to minimizing the function $g(\\lambda) = \\mathrm{SURE}(y, \\lambda) + n\\sigma^2$, given by:\n$$\ng(\\lambda) = \\sum_{i=1}^n \\left( \\min(y_i^2, \\lambda^2) + 2\\sigma^2 \\mathbf{1}_{\\{|y_i| > \\lambda\\}} \\right)\n$$\nThe function $g(\\lambda)$ is a sum of functions $g_i(\\lambda) = \\min(y_i^2, \\lambda^2) + 2\\sigma^2 \\mathbf{1}_{\\{|y_i| > \\lambda\\}}$. For a fixed $i$, $g_i(\\lambda)$ is continuous for $\\lambda \\ge 0$ except at $\\lambda = |y_i|$. The derivative of $\\min(y_i^2, \\lambda^2)$ with respect to $\\lambda$ is $2\\lambda$ for $\\lambda < |y_i|$ and $0$ for $\\lambda > |y_i|$. The indicator function $\\mathbf{1}_{\\{|y_i| > \\lambda\\}}$ is piecewise constant.\nLet $|y|_{(0)} \\equiv 0$ and $|y|_{(1)} \\le |y|_{(2)} \\le \\dots \\le |y|_{(n)}$ be the order statistics of the absolute values of the coordinates of $y$. These values partition the domain $\\lambda \\ge 0$ into intervals $[|y|_{(k)}, |y|_{(k+1)})$ for $k=0, \\dots, n-1$, and $[|y|_{(n)}, \\infty)$.\n\nFor any $\\lambda$ in an open interval $(|y|_{(k)}, |y|_{(k+1)})$:\n- There are $k$ coordinates with $|y_i| \\le |y|_{(k)} < \\lambda$. For these, $\\min(y_i^2, \\lambda^2) = y_i^2$ and $\\mathbf{1}_{\\{|y_i|>\\lambda\\}} = 0$.\n- There are $n-k$ coordinates with $|y_i| \\ge |y|_{(k+1)} > \\lambda$. For these, $\\min(y_i^2, \\lambda^2) = \\lambda^2$ and $\\mathbf{1}_{\\{|y_i|>\\lambda\\}} = 1$.\nSo, for $\\lambda \\in (|y|_{(k)}, |y|_{(k+1)})$, the function is:\n$$\ng(\\lambda) = \\sum_{j=1}^k |y|_{(j)}^2 + (n-k)(\\lambda^2 + 2\\sigma^2)\n$$\nThe derivative with respect to $\\lambda$ on this interval is $\\frac{dg}{d\\lambda} = 2(n-k)\\lambda$. Since $\\lambda > 0$ and $n-k \\ge 1$ (for $k < n$), this derivative is strictly positive. This implies that $g(\\lambda)$ is strictly increasing on each open interval $(|y|_{(k)}, |y|_{(k+1)})$.\nFor $\\lambda > |y|_{(n)}$, all $|y_i| \\le \\lambda$, so $g(\\lambda) = \\sum_{i=1}^n y_i^2$, which is constant.\nSince the function is increasing on the intervals between the points $\\{0, |y|_{(1)}, \\dots, |y|_{(n)}\\}$ and constant beyond the last point, the global minimum of $g(\\lambda)$ for $\\lambda \\ge 0$ must be attained at one of these points.\n\nWe therefore need to find the value of $k \\in \\{0, 1, \\dots, n\\}$ that minimizes $g(|y|_{(k)})$. Let's define the criterion $S(k) = g(|y|_{(k)})$.\nAt $\\lambda = |y|_{(k)}$:\n- $\\sum_{i=1}^n \\min(y_i^2, |y|_{(k)}^2) = \\sum_{j=1}^k |y|_{(j)}^2 + (n-k)|y|_{(k)}^2$.\n- The term $\\sum_{i=1}^n \\mathbf{1}_{\\{|y_i| > |y|_{(k)}\\}}$ is the number of coordinates with absolute value strictly greater than $|y|_{(k)}$. This depends on whether the values $|y|_{(j)}$ are unique. However, the form of $g(\\lambda)$ on the interval $[|y|_{(k)}, |y|_{(k+1)})$ is $g(\\lambda) = \\sum_{j=1}^k |y|_{(j)}^2 + (n-k)(\\lambda^2+2\\sigma^2)$, which is continuous and increasing. Its minimum on this interval is at $\\lambda = |y|_{(k)}$. The value is $g(|y|_{(k)}) = \\sum_{j=1}^k |y|_{(j)}^2 + (n-k)(|y|_{(k)}^2+2\\sigma^2)$. This expression correctly handles ties and serves as the criterion to minimize over $k$.\nSo, we need to find $\\hat{k} = \\operatorname{argmin}_{k \\in \\{0, 1, \\dots, n\\}} S(k)$, where\n$$\nS(k) = \\sum_{j=1}^k |y|_{(j)}^2 + (n-k)\\left(|y|_{(k)}^2 + 2\\sigma^2\\right)\n$$\nwith the conventions $|y|_{(0)} = 0$ and $\\sum_{j=1}^0 (\\cdot) = 0$. The minimizing threshold is then $\\hat{\\lambda} = |y|_{(\\hat{k})}$. This constitutes a closed-form analytic expression as it specifies a direct computational procedure involving sorting and a finite number of comparisons, not an iterative optimization.\n\nThe final expression is therefore $\\hat{\\lambda}=|y|_{(\\hat{k})}$, where $\\hat{k}$ is the index that minimizes the explicit function $S(k)$.", "answer": "$$\n\\boxed{\\hat{\\lambda} = |y|_{(\\hat{k})}, \\quad \\text{where } \\hat{k} = \\underset{k \\in \\{0, 1, \\dots, n\\}}{\\operatorname{argmin}} \\left\\{ \\sum_{j=1}^{k} |y|_{(j)}^{2} + (n-k)\\left(|y|_{(k)}^{2} + 2\\sigma^{2}\\right) \\right\\}}\n$$", "id": "3452918"}]}