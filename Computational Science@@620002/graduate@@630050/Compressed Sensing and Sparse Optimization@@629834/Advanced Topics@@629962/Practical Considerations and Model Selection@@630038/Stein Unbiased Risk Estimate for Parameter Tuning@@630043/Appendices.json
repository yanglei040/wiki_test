{"hands_on_practices": [{"introduction": "To truly master a tool, we must first understand its inner workings from the ground up. This practice guides you through the fundamental derivation of the Generalized Stein Unbiased Risk Estimate (GSURE) for the LASSO estimator, a cornerstone of sparse recovery in linear inverse problems [@problem_id:3482264]. By starting with Stein's identity and applying implicit differentiation to the estimator's fixed-point optimality condition, you will construct a fully data-driven, unbiased estimate of the model's prediction risk, revealing the elegant connection between statistical estimation and optimization theory.", "problem": "Consider the underdetermined linear inverse problem of compressed sensing, where $y \\in \\mathbb{R}^{m}$ is observed according to $y = A x_{0} + w$ with $A \\in \\mathbb{R}^{m \\times n}$, $m < n$, and $w \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$. Let the estimator $x_{\\lambda}(y)$ be defined as the unique minimizer of the convex program $\\min_{x \\in \\mathbb{R}^{n}} \\frac{1}{2} \\|A x - y\\|^{2} + \\lambda \\|x\\|_{1}$ for some fixed $\\lambda > 0$. Assume that $x_{\\lambda}(y)$ is locally unique and differentiable almost everywhere with respect to $y$. Define the mean-squared prediction risk $R_{\\mathrm{pred}}(\\lambda) = \\mathbb{E}\\left[\\|A x_{\\lambda}(y) - A x_{0}\\|^{2}\\right]$.\n\nStarting from Stein's identity for Gaussian vectors as the only fundamental base, and without quoting any pre-established risk estimation formula, do the following:\n\n1. Derive an unbiased estimator for $R_{\\mathrm{pred}}(\\lambda)$ that depends only on the data $y$, the noise variance $\\sigma^{2}$, and the divergence of the mapping $y \\mapsto A x_{\\lambda}(y)$. Your derivation must proceed from the property that if $y = \\mu + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$ and $h:\\mathbb{R}^{m} \\to \\mathbb{R}^{m}$ is weakly differentiable with integrable divergence, then $\\mathbb{E}\\left[\\langle \\varepsilon, h(y) \\rangle\\right] = \\sigma^{2} \\mathbb{E}\\left[\\operatorname{div} h(y)\\right]$.\n\n2. Show that $x_{\\lambda}(y)$ satisfies the fixed-point relation $x_{\\lambda}(y) = \\operatorname{prox}_{\\tau \\lambda \\|\\cdot\\|_{1}}\\!\\left(x_{\\lambda}(y) - \\tau A^{\\top}(A x_{\\lambda}(y) - y)\\right)$ for any fixed step size $\\tau > 0$, where $\\operatorname{prox}_{\\gamma \\|\\cdot\\|_{1}}(u) = \\arg\\min_{x} \\frac{1}{2}\\|x - u\\|^{2} + \\gamma \\|x\\|_{1}$.\n\n3. Using implicit differentiation of the fixed-point equation in Part 2 and the fact that the Jacobian of the soft-thresholding operator is a diagonal selector $D = \\operatorname{diag}(d_{1},\\ldots,d_{n})$ with $d_{i} \\in \\{0,1\\}$ almost everywhere, where $d_{i} = 1$ if and only if the $i$th coordinate is active under soft-thresholding, derive an explicit expression for $\\operatorname{div}_{y}\\!\\left(A x_{\\lambda}(y)\\right)$ in terms of $A$, $\\tau$, and $D$ evaluated at the fixed point. You may assume that all inverses you use exist for almost every $y$.\n\n4. Combine your results to express the generalized Stein's unbiased risk estimate for prediction, $\\mathrm{GSURE}_{\\mathrm{pred}}(y; \\lambda, \\tau)$, as a single closed-form analytic expression depending on $A$, $y$, $\\lambda$, $\\tau$, $\\sigma^{2}$, and $D$ evaluated at $x_{\\lambda}(y)$. Express your final answer as a single analytic expression. No numerical evaluation is required, and no rounding is needed.\n\nYour final answer must be a single closed-form analytic expression.", "solution": "The user-provided problem is a well-posed and scientifically grounded exercise in statistical signal processing and optimization theory. It requests the derivation of the Generalized Stein's Unbiased Risk Estimate (GSURE) for the prediction risk of the LASSO estimator. All necessary data, models, and assumptions are provided, and there are no contradictions or ambiguities. The problem is valid, and a full solution is presented below.\n\nThe solution is organized into four parts, corresponding to the four tasks specified in the problem statement.\n\n**Part 1: Derivation of an Unbiased Estimator for Prediction Risk**\n\nThe mean-squared prediction risk is defined as $R_{\\mathrm{pred}}(\\lambda) = \\mathbb{E}\\left[\\|A x_{\\lambda}(y) - A x_{0}\\|^{2}\\right]$, where the expectation is over the distribution of the observation vector $y$. The observation model is $y = A x_{0} + w$, where $w \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$.\nLet us denote the true mean signal as $\\mu \\equiv A x_{0}$. The observation model is then $y = \\mu + w$, and the risk is $R_{\\mathrm{pred}}(\\lambda) = \\mathbb{E}\\left[\\|A x_{\\lambda}(y) - \\mu\\|^{2}\\right]$.\n\nWe can expand the squared norm inside the expectation as follows:\n$$\n\\|A x_{\\lambda}(y) - \\mu\\|^{2} = \\|(A x_{\\lambda}(y) - y) + (y - \\mu)\\|^{2}\n$$\nThe term $y - \\mu$ is the noise vector $w$. Let $\\hat{\\mu}(y) \\equiv A x_{\\lambda}(y)$ be the estimated signal. The expression becomes:\n$$\n\\|\\hat{\\mu}(y) - \\mu\\|^{2} = \\|\\hat{\\mu}(y) - y + w\\|^{2} = \\|\\hat{\\mu}(y) - y\\|^{2} + \\|w\\|^{2} + 2 \\langle \\hat{\\mu}(y) - y, w \\rangle\n$$\nTaking the expectation of each term:\n1.  The term $\\mathbb{E}\\left[\\|\\hat{\\mu}(y) - y\\|^{2}\\right]$ remains as is, since it involves an expectation of a function of $y$.\n2.  The term $\\mathbb{E}\\left[\\|w\\|^{2}\\right]$ is the expected squared norm of a Gaussian vector with i.i.d. components $w_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n    $$\n    \\mathbb{E}\\left[\\|w\\|^{2}\\right] = \\mathbb{E}\\left[\\sum_{i=1}^{m} w_i^2\\right] = \\sum_{i=1}^{m} \\mathbb{E}[w_i^2] = \\sum_{i=1}^{m} \\sigma^2 = m \\sigma^2\n    $$\n3.  For the cross-term $\\mathbb{E}\\left[2 \\langle \\hat{\\mu}(y) - y, w \\rangle\\right]$, we apply the given Stein's identity. Let $\\varepsilon = w = y - \\mu$. The identity states that for a weakly differentiable function $h(y)$, $\\mathbb{E}[\\langle \\varepsilon, h(y) \\rangle] = \\sigma^2 \\mathbb{E}[\\operatorname{div} h(y)]$.\n    Let $h(y) = \\hat{\\mu}(y) - y = A x_{\\lambda}(y) - y$. The divergence of $h(y)$ with respect to $y$ is:\n    $$\n    \\operatorname{div}_y h(y) = \\operatorname{div}_y (A x_{\\lambda}(y) - y) = \\operatorname{div}_y(A x_{\\lambda}(y)) - \\operatorname{div}_y(y)\n    $$\n    The divergence of the identity map $y \\mapsto y$ is $\\operatorname{div}_y(y) = \\sum_{i=1}^{m} \\frac{\\partial y_i}{\\partial y_i} = m$.\n    Therefore, $\\operatorname{div}_y h(y) = \\operatorname{div}_y(A x_{\\lambda}(y)) - m$.\n    Applying Stein's identity to the cross-term:\n    $$\n    \\mathbb{E}[\\langle \\hat{\\mu}(y) - y, w \\rangle] = \\sigma^2 \\mathbb{E}[\\operatorname{div}_y(A x_{\\lambda}(y)) - m]\n    $$\nCombining all terms, the risk is:\n$$\nR_{\\mathrm{pred}}(\\lambda) = \\mathbb{E}\\left[\\|\\hat{\\mu}(y) - y\\|^{2}\\right] + m \\sigma^2 + 2\\sigma^2 \\mathbb{E}\\left[\\operatorname{div}_y(A x_{\\lambda}(y)) - m\\right]\n$$\n$$\nR_{\\mathrm{pred}}(\\lambda) = \\mathbb{E}\\left[ \\|A x_{\\lambda}(y) - y\\|^{2} + m \\sigma^2 + 2\\sigma^2 \\operatorname{div}_y(A x_{\\lambda}(y)) - 2m \\sigma^2 \\right]\n$$\n$$\nR_{\\mathrm{pred}}(\\lambda) = \\mathbb{E}\\left[ \\|A x_{\\lambda}(y) - y\\|^{2} - m \\sigma^2 + 2\\sigma^2 \\operatorname{div}_y(A x_{\\lambda}(y)) \\right]\n$$\nSince the expectation of the quantity inside the brackets equals the risk, this quantity is an unbiased estimator of the risk. We denote it by $\\mathrm{SURE}_{\\mathrm{pred}}(y)$:\n$$\n\\mathrm{SURE}_{\\mathrm{pred}}(y) = \\|A x_{\\lambda}(y) - y\\|^{2} - m \\sigma^2 + 2\\sigma^2 \\operatorname{div}_y(A x_{\\lambda}(y))\n$$\nThis estimator depends on the data $y$, the noise variance $\\sigma^2$, and the divergence of the map $y \\mapsto A x_{\\lambda}(y)$, as required.\n\n**Part 2: Fixed-Point Relation for the Estimator**\n\nThe estimator $x_{\\lambda}(y)$ is the solution to the convex optimization problem:\n$$\nx_{\\lambda}(y) = \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ F(x) \\equiv \\frac{1}{2} \\|A x - y\\|^{2} + \\lambda \\|x\\|_{1} \\right\\}\n$$\nLet $f(x) = \\frac{1}{2} \\|A x - y\\|^{2}$ and $g(x) = \\lambda \\|x\\|_{1}$. The function $f(x)$ is differentiable with gradient $\\nabla f(x) = A^{\\top}(Ax-y)$. The function $g(x)$ is convex but not differentiable. The first-order optimality condition for the minimizer $x^* = x_{\\lambda}(y)$ states that the zero vector must belong to the subdifferential of the objective function at $x^*$:\n$$\n0 \\in \\partial F(x^*) = \\nabla f(x^*) + \\partial g(x^*)\n$$\nSubstituting the expressions for the gradient and subdifferential:\n$$\n0 \\in A^{\\top}(Ax^* - y) + \\lambda \\partial \\|x^*\\|_{1}\n$$\nThis can be rewritten as:\n$$\n-A^{\\top}(Ax^* - y) \\in \\lambda \\partial \\|x^*\\|_{1}\n$$\nNow, consider the fixed-point relation given in the problem:\n$$\nx^* = \\operatorname{prox}_{\\tau \\lambda \\|\\cdot\\|_{1}}\\!\\left(x^* - \\tau A^{\\top}(A x^* - y)\\right)\n$$\nfor some step size $\\tau > 0$. By definition, $z = \\operatorname{prox}_{\\gamma h}(v)$ is the unique minimizer of $\\frac{1}{2}\\|z-v\\|^2 + \\gamma h(z)$. The optimality condition for this proximal problem is $0 \\in (z-v) + \\gamma \\partial h(z)$, which is equivalent to $v-z \\in \\gamma \\partial h(z)$.\nApplying this definition to our fixed-point relation, we set:\n- $z = x^*$\n- $v = x^* - \\tau A^{\\top}(A x^* - y)$\n- $\\gamma h(\\cdot) = \\tau \\lambda \\|\\cdot\\|_{1}$\nThe optimality condition for the proximal map becomes:\n$$\n\\left(x^* - \\tau A^{\\top}(A x^* - y)\\right) - x^* \\in \\tau \\lambda \\partial \\|x^*\\|_{1}\n$$\n$$\n-\\tau A^{\\top}(A x^* - y) \\in \\tau \\lambda \\partial \\|x^*\\|_{1}\n$$\nSince $\\tau > 0$, we can divide by $\\tau$ to obtain:\n$$\n-A^{\\top}(A x^* - y) \\in \\lambda \\partial \\|x^*\\|_{1}\n$$\nThis is precisely the optimality condition for the original LASSO problem. Thus, the estimator $x_{\\lambda}(y)$ is a fixed point of the given iteration for any $\\tau > 0$.\n\n**Part 3: Derivation of the Divergence Term**\n\nWe need to compute $\\operatorname{div}_{y}\\!\\left(A x_{\\lambda}(y)\\right)$. This is given by the trace of the Jacobian matrix of $A x_{\\lambda}(y)$ with respect to $y$. Let $J_x \\equiv \\frac{\\partial x_{\\lambda}(y)}{\\partial y^{\\top}}$ be the $n \\times m$ Jacobian matrix of the estimator $x_{\\lambda}(y)$ with respect to $y$. Then the Jacobian of $A x_{\\lambda}(y)$ is $A J_x$, which is an $m \\times m$ matrix. The divergence is:\n$$\n\\operatorname{div}_{y}\\!\\left(A x_{\\lambda}(y)\\right) = \\operatorname{tr}(A J_x)\n$$\nTo find $J_x$, we perform implicit differentiation on the fixed-point equation from Part 2. Let $x = x_{\\lambda}(y)$ and denote the soft-thresholding operator as $S(u) \\equiv \\operatorname{prox}_{\\tau \\lambda \\|\\cdot\\|_{1}}(u)$. The fixed-point equation is:\n$$\nx = S\\left(x - \\tau A^{\\top}(A x - y)\\right)\n$$\nLet the argument of $S$ be $u(x,y) = x - \\tau A^{\\top} A x + \\tau A^{\\top} y$. We differentiate the equation $x = S(u(x,y))$ with respect to $y^{\\top}$ using the chain rule:\n$$\n\\frac{\\partial x}{\\partial y^{\\top}} = \\frac{\\partial S(u)}{\\partial u^{\\top}} \\left( \\frac{\\partial u}{\\partial x^{\\top}} \\frac{\\partial x}{\\partial y^{\\top}} + \\frac{\\partial u}{\\partial y^{\\top}} \\right)\n$$\nThe problem states that the Jacobian of the soft-thresholding operator, $\\frac{\\partial S(u)}{\\partial u^{\\top}}$, is a diagonal matrix $D \\in \\mathbb{R}^{n \\times n}$ where $d_{ii} \\in \\{0, 1\\}$.\nThe other Jacobians are:\n- $\\frac{\\partial x}{\\partial y^{\\top}} = J_x$\n- $\\frac{\\partial u}{\\partial x^{\\top}} = I_n - \\tau A^{\\top} A$\n- $\\frac{\\partial u}{\\partial y^{\\top}} = \\tau A^{\\top}$\nSubstituting these into the chain rule equation:\n$$\nJ_x = D \\left( (I_n - \\tau A^{\\top} A) J_x + \\tau A^{\\top} \\right)\n$$\n$$\nJ_x = D (I_n - \\tau A^{\\top} A) J_x + D \\tau A^{\\top}\n$$\nWe now solve for $J_x$:\n$$\nJ_x - D (I_n - \\tau A^{\\top} A) J_x = \\tau D A^{\\top}\n$$\n$$\n(I_n - D + \\tau D A^{\\top} A) J_x = \\tau D A^{\\top}\n$$\nAssuming the matrix on the left is invertible (as permitted by the problem statement), we find $J_x$:\n$$\nJ_x = \\tau (I_n - D + \\tau D A^{\\top} A)^{-1} D A^{\\top}\n$$\nThe divergence is the trace of $A J_x$. Using the cyclic property of the trace, $\\operatorname{tr}(XY) = \\operatorname{tr}(YX)$, we have $\\operatorname{tr}(A J_x) = \\operatorname{tr}(J_x A)$:\n$$\n\\operatorname{div}_{y}\\!\\left(A x_{\\lambda}(y)\\right) = \\operatorname{tr}(J_x A) = \\operatorname{tr}\\left( \\tau (I_n - D + \\tau D A^{\\top} A)^{-1} D A^{\\top} A \\right)\n$$\nThis provides the required explicit expression for the divergence in terms of $A$, $\\tau$, and $D$.\n\n**Part 4: The GSURE Formula**\n\nFinally, we combine the results from Part 1 and Part 3 to obtain the closed-form expression for the Generalized Stein's Unbiased Risk Estimate for prediction, $\\mathrm{GSURE}_{\\mathrm{pred}}(y; \\lambda, \\tau)$. We substitute the expression for the divergence into the SURE formula:\n$$\n\\mathrm{GSURE}_{\\mathrm{pred}}(y; \\lambda, \\tau) = \\|A x_{\\lambda}(y) - y\\|^{2} - m \\sigma^{2} + 2 \\sigma^{2} \\operatorname{div}_{y}\\!\\left(A x_{\\lambda}(y)\\right)\n$$\n$$\n\\mathrm{GSURE}_{\\mathrm{pred}}(y; \\lambda, \\tau) = \\|A x_{\\lambda}(y) - y\\|^{2} - m \\sigma^{2} + 2 \\sigma^{2} \\operatorname{tr}\\left(\\tau \\left(I_n - D + \\tau D A^{\\top} A\\right)^{-1} D A^{\\top} A \\right)\n$$\nThis expression depends on the specified quantities: the matrix $A$, the data $y$ (which determines $x_{\\lambda}(y)$ and $D$), the regularization parameter $\\lambda$ (implicit in $x_{\\lambda}(y)$ and $D$), the step-size parameter $\\tau$, the noise variance $\\sigma^2$, and the selector matrix $D$. This is the final analytical expression sought.", "answer": "$$\\boxed{\\|A x_{\\lambda}(y) - y\\|^{2} - m \\sigma^{2} + 2 \\sigma^{2} \\operatorname{tr}\\left(\\tau \\left(I_n - D + \\tau D A^{\\top} A\\right)^{-1} D A^{\\top} A \\right)}$$", "id": "3482264"}, {"introduction": "While exact formulas for the divergence term in SURE are powerful, their direct computation can be intractable for complex models. This exercise explores a practical and widely used solution: the finite-difference Monte Carlo method [@problem_id:3482321]. You will dissect the error sources in this approximation, analyzing the inherent trade-off between the systematic bias from the finite-difference step size and the stochastic variance from Monte Carlo sampling and floating-point errors, culminating in a principled strategy for selecting an optimal step size.", "problem": "Consider the standard Gaussian denoising model in compressed sensing and sparse optimization: an unknown deterministic signal vector $x_{0} \\in \\mathbb{R}^{n}$ is observed through noisy measurements $y = x_{0} + w$, where $w \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$. Let $f: \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$ be a predictor (e.g., a proximal or iterative reconstruction map) that is three times continuously differentiable in a neighborhood of a fixed input $y$, with Jacobian $J_{f}(y)$ and third derivative tensor $D^{3}f(y)$ having operator norm bounded by a constant $M_{3}$ in that neighborhood. The Stein's unbiased risk estimate (SURE) for the mean squared error risk of $f$ at $y$ is\n$$\n\\mathrm{SURE}(f;y) \\triangleq -n \\sigma^{2} + \\|f(y) - y\\|^{2} + 2 \\sigma^{2} \\,\\operatorname{div} f(y),\n$$\nwhere $\\operatorname{div} f(y) = \\operatorname{tr} J_{f}(y)$ denotes the divergence of $f$ at $y$.\n\nTo approximate $\\operatorname{div} f(y)$ without explicit Jacobian access, consider the finite-difference Monte Carlo (MC) estimator: draw $K$ independent probe vectors $z_{1}, \\dots, z_{K} \\overset{\\mathrm{i.i.d.}}{\\sim} \\mathcal{N}(0, I_{n})$ and define, for a step size $\\epsilon > 0$,\n$$\nd_{k}(\\epsilon) \\triangleq \\frac{1}{\\epsilon} \\, z_{k}^{\\top} \\big( f(y + \\epsilon z_{k}) - f(y) \\big), \\quad \\widehat{\\mathrm{div}}_{\\epsilon,K}(f;y) \\triangleq \\frac{1}{K} \\sum_{k=1}^{K} d_{k}(\\epsilon),\n$$\nand the corresponding MC-SURE estimator\n$$\n\\mathrm{SURE}_{\\epsilon,K}(f;y) \\triangleq -n \\sigma^{2} + \\|f(y) - y\\|^{2} + 2 \\sigma^{2} \\, \\widehat{\\mathrm{div}}_{\\epsilon,K}(f;y).\n$$\n\nAssume a standard floating-point evaluation model: each evaluation of $f$ returns $f(y) + \\eta$ for its argument $y$, where $\\eta$ is a zero-mean perturbation independent across calls and probes, with covariance bounded in operator norm by $u^{2} s_{f}(y)^{2} I_{n}$ for some known local scale $s_{f}(y) > 0$ and machine unit roundoff $u \\in (0,1)$. Under the stated smoothness, you may use Taylor expansions and Gaussian moment identities. Work in a regime where $\\epsilon$ is small enough that higher-order terms are dominated by those you retain.\n\n1. Show that the bias of $\\widehat{\\mathrm{div}}_{\\epsilon,K}(f;y)$ relative to $\\operatorname{div} f(y)$ is of order $\\epsilon^{2}$, and provide a bound of the form\n$$\n\\big| \\mathbb{E}[\\widehat{\\mathrm{div}}_{\\epsilon,K}(f;y)] - \\operatorname{div} f(y) \\big| \\leq c_{b} \\, \\epsilon^{2},\n$$\nwhere the constant $c_{b}$ should be expressed explicitly in terms of $M_{3}$ and Gaussian moments.\n\n2. Show that the variance of $\\widehat{\\mathrm{div}}_{\\epsilon,K}(f;y)$ decomposes into a probe-variance term that scales as $1/K$ and a floating-point induced term that scales as $1/(K \\epsilon^{2})$, i.e.,\n$$\n\\mathrm{Var}\\big( \\widehat{\\mathrm{div}}_{\\epsilon,K}(f;y) \\big) \\leq \\frac{c_{v}}{K} + \\frac{c_{r}}{K \\epsilon^{2}},\n$$\nfor some constants $c_{v}$ (independent of $\\epsilon$) and $c_{r}$ (depending on $u$ and $s_{f}(y)$) that you should define.\n\n3. Combine the above to obtain a leading-order mean-squared error (MSE) approximation for $\\mathrm{SURE}_{\\epsilon,K}(f;y)$ about $\\mathrm{SURE}(f;y)$. Ignoring $\\epsilon$-independent terms in this MSE, derive the step size $\\epsilon^{\\star}$ that minimizes the $\\epsilon$-dependent leading-order MSE as a function of $K$, $c_{b}$, and $c_{r}$. Provide your final $\\epsilon^{\\star}$ as a single closed-form analytic expression.\n\nYour final answer must be this single expression for $\\epsilon^{\\star}$. No numerical evaluation is required.", "solution": "The problem asks for the optimal step size $\\epsilon^{\\star}$ for a Monte Carlo Stein's unbiased risk estimate (SURE) procedure. This requires analyzing the mean-squared error (MSE) of the divergence estimator, which decomposes into its squared bias and variance. We will address the three parts of the problem sequentially.\n\nPart 1: Bias of the Divergence Estimator\n\nThe Monte Carlo estimator for the divergence is $\\widehat{\\mathrm{div}}_{\\epsilon,K}(f;y) = \\frac{1}{K} \\sum_{k=1}^{K} d_{k}(\\epsilon)$, where $d_{k}(\\epsilon) = \\frac{1}{\\epsilon} z_{k}^{\\top} ( f(y + \\epsilon z_{k}) - f(y) )$. Due to the i.i.d. nature of the probe vectors $z_k$, the expectation of the estimator is $\\mathbb{E}[\\widehat{\\mathrm{div}}_{\\epsilon,K}(f;y)] = \\mathbb{E}[d_1(\\epsilon)]$. Let's analyze the expectation of a single term $d(\\epsilon)$ for a generic probe $z \\sim \\mathcal{N}(0, I_n)$.\n\nThe floating-point model states that an evaluation of $f$ at a point $y_{arg}$ returns $\\tilde{f}(y_{arg}) = f(y_{arg}) + \\eta$, where $\\mathbb{E}[\\eta]=0$ and $\\eta$ is independent of other perturbations and the probe $z$.\nThe computed term is $d(\\epsilon) = \\frac{1}{\\epsilon} z^{\\top} (\\tilde{f}(y+\\epsilon z) - \\tilde{f}(y)) = \\frac{1}{\\epsilon} z^{\\top} (f(y+\\epsilon z) + \\eta_1 - f(y) - \\eta_2)$.\nTaking the expectation, we have:\n$$\n\\mathbb{E}[d(\\epsilon)] = \\mathbb{E}\\left[\\frac{1}{\\epsilon} z^{\\top} (f(y+\\epsilon z) - f(y))\\right] + \\frac{1}{\\epsilon} \\mathbb{E}[z^{\\top}(\\eta_1 - \\eta_2)]\n$$\nSince $\\eta_1, \\eta_2$ are zero-mean and independent of $z$, the second term is zero. Thus, the floating-point errors do not introduce bias.\nWe now analyze the first term using a Taylor expansion of $f$ around $y$. As $f$ is $C^3$, we can write:\n$$\nf(y + \\epsilon z) = f(y) + \\epsilon J_f(y) z + \\frac{\\epsilon^2}{2} D^2f(y)(z,z) + \\mathcal{R}_3(y, \\epsilon z)\n$$\nwhere $J_f(y)$ is the Jacobian of $f$ at $y$, $D^2f(y)(z,z)$ is a vector whose $i$-th component is $z^\\top H_i(y) z$ (with $H_i$ being the Hessian of $f_i$), and $\\mathcal{R}_3$ is the remainder term. By Taylor's theorem with Lagrange remainder, we can express the remainder using the third derivative:\n$f(y + \\epsilon z) - f(y) = \\epsilon J_f(y)z + \\frac{\\epsilon^2}{2} D^2f(y)(z,z) + \\frac{\\epsilon^3}{6} D^3f(y+\\xi \\epsilon z)(z,z,z)$ for some $\\xi \\in (0,1)$.\nSubstituting this into the expression for $d(\\epsilon)$ and taking the expectation:\n$$\n\\mathbb{E}[d(\\epsilon)] = \\mathbb{E}\\left[ z^{\\top}J_f(y)z + \\frac{\\epsilon}{2} z^{\\top}D^2f(y)(z,z) + \\frac{\\epsilon^2}{6} z^{\\top}D^3f(y+\\xi \\epsilon z)(z,z,z) \\right]\n$$\nWe evaluate the expectation of each term with respect to $z \\sim \\mathcal{N}(0, I_n)$:\n1.  $\\mathbb{E}[z^{\\top}J_f(y)z] = \\mathbb{E}[\\operatorname{tr}(z^{\\top}J_f(y)z)] = \\mathbb{E}[\\operatorname{tr}(J_f(y)zz^{\\top})] = \\operatorname{tr}(J_f(y)\\mathbb{E}[zz^{\\top}])$. Since $\\mathbb{E}[zz^{\\top}] = I_n$, this term equals $\\operatorname{tr}(J_f(y)) = \\operatorname{div}f(y)$.\n2.  The second term involves $z^{\\top}D^2f(y)(z,z) = \\sum_{i,j,k} z_i (H_i)_{jk} z_j z_k$, which is a cubic polynomial in the components of $z$. Since $z_i$ are independent standard normal variables, their distribution is symmetric. The expectation of any odd-degree monomial is zero, so $\\mathbb{E}[z_i z_j z_k]=0$ for all $i,j,k$. Thus, $\\mathbb{E}[z^{\\top}D^2f(y)(z,z)] = 0$.\nThe bias of the estimator is therefore:\n$$\n\\mathrm{Bias}(\\widehat{\\mathrm{div}}_{\\epsilon,K}) = \\mathbb{E}[d(\\epsilon)] - \\operatorname{div}f(y) = \\frac{\\epsilon^2}{6} \\mathbb{E}\\left[ z^{\\top}D^3f(y+\\xi \\epsilon z)(z,z,z) \\right]\n$$\nTo find the bound $c_b$, we take the absolute value and use the given bounds. For small $\\epsilon$, $D^3f(y+\\xi \\epsilon z) \\approx D^3f(y)$. The problem states the operator norm of the third derivative tensor is bounded by $M_3$.\n$$\n|\\mathrm{Bias}| \\leq \\frac{\\epsilon^2}{6} \\mathbb{E}\\left[ |z^{\\top}D^3f(y+\\xi \\epsilon z)(z,z,z)| \\right] \\leq \\frac{\\epsilon^2}{6} \\mathbb{E}\\left[ \\|z\\| \\|D^3f(y+\\xi \\epsilon z)(z,z,z)\\| \\right]\n$$\nUsing the operator norm definition, $\\|D^3f(\\cdot)(z,z,z)\\| \\leq \\|D^3f(\\cdot)\\|_{\\mathrm{op}} \\|z\\|^3 \\leq M_3 \\|z\\|^3$.\n$$\n|\\mathrm{Bias}| \\leq \\frac{\\epsilon^2 M_3}{6} \\mathbb{E}[\\|z\\|^4]\n$$\nWe compute the fourth moment of the norm of a standard Gaussian vector:\n$\\mathbb{E}[\\|z\\|^4] = \\mathbb{E}[(\\sum_{i=1}^n z_i^2)^2] = \\sum_{i=1}^n \\mathbb{E}[z_i^4] + \\sum_{i \\neq j} \\mathbb{E}[z_i^2 z_j^2]$.\nFor a standard normal variable $z_i$, $\\mathbb{E}[z_i^2]=1$ and $\\mathbb{E}[z_i^4]=3$. For $i \\neq j$, $z_i$ and $z_j$ are independent, so $\\mathbb{E}[z_i^2 z_j^2]=\\mathbb{E}[z_i^2]\\mathbb{E}[z_j^2]=1$.\nThere are $n$ diagonal terms and $n(n-1)$ off-diagonal terms.\n$\\mathbb{E}[\\|z\\|^4] = n \\cdot 3 + n(n-1) \\cdot 1 = 3n + n^2 - n = n^2 + 2n$.\nThe bias is bounded by $|\\mathbb{E}[\\widehat{\\mathrm{div}}_{\\epsilon,K}(f;y)] - \\operatorname{div} f(y)| \\leq \\frac{M_3(n^2+2n)}{6} \\epsilon^2$.\nThus, we identify the constant $c_b$ as:\n$$\nc_b = \\frac{M_3(n^2+2n)}{6}\n$$\n\nPart 2: Variance of the Divergence Estimator\n\nThe variance of the estimator is $\\mathrm{Var}(\\widehat{\\mathrm{div}}_{\\epsilon,K}) = \\frac{1}{K^2} \\sum_{k=1}^K \\mathrm{Var}(d_k(\\epsilon)) = \\frac{1}{K}\\mathrm{Var}(d(\\epsilon))$ due to the i.i.d. probes.\nWe decompose $d(\\epsilon)$ into a part from the exact function and a part from rounding error:\n$d(\\epsilon) = d_{\\mathrm{exact}}(\\epsilon) + d_{\\mathrm{round}}(\\epsilon)$, where $d_{\\mathrm{exact}}(\\epsilon) = \\frac{1}{\\epsilon} z^{\\top} (f(y+\\epsilon z) - f(y))$ and $d_{\\mathrm{round}}(\\epsilon) = \\frac{1}{\\epsilon} z^{\\top} (\\eta_1 - \\eta_2)$.\nThe probe vector $z$ is independent of the floating-point errors $\\eta_1, \\eta_2$. Therefore, $d_{\\mathrm{exact}}$ and $d_{\\mathrm{round}}$ are uncorrelated, and $\\mathrm{Var}(d(\\epsilon)) = \\mathrm{Var}(d_{\\mathrm{exact}}(\\epsilon)) + \\mathrm{Var}(d_{\\mathrm{round}}(\\epsilon))$.\n\nLet's first analyze the rounding error variance. Since $\\mathbb{E}[d_{\\mathrm{round}}(\\epsilon)]=0$:\n$$\n\\mathrm{Var}(d_{\\mathrm{round}}(\\epsilon)) = \\mathbb{E}[d_{\\mathrm{round}}(\\epsilon)^2] = \\frac{1}{\\epsilon^2} \\mathbb{E}[(z^{\\top}(\\eta_1-\\eta_2))^2] = \\frac{1}{\\epsilon^2} \\mathbb{E}[z^{\\top}(\\eta_1-\\eta_2)(\\eta_1-\\eta_2)^{\\top}z]\n$$\nUsing the trace identity and linearity of expectation:\n$$\n\\mathrm{Var}(d_{\\mathrm{round}}(\\epsilon)) = \\frac{1}{\\epsilon^2} \\operatorname{tr}\\left( \\mathbb{E}[(\\eta_1-\\eta_2)(\\eta_1-\\eta_2)^{\\top}] \\mathbb{E}[zz^{\\top}] \\right)\n$$\nSince $\\mathbb{E}[zz^{\\top}]=I_n$ and $\\eta_1, \\eta_2$ are independent and zero-mean, $\\mathbb{E}[(\\eta_1-\\eta_2)(\\eta_1-\\eta_2)^{\\top}] = \\mathrm{Cov}(\\eta_1) + \\mathrm{Cov}(\\eta_2)$. Let $C_1=\\mathrm{Cov}(\\eta_1)$ and $C_2=\\mathrm{Cov}(\\eta_2)$.\nThe problem states that the covariance is bounded such that $C_i \\preceq u^2 s_f(y_i)^2 I_n$ in the Loewner order. For small $\\epsilon$, we assume $s_f(y+\\epsilon z) \\approx s_f(y)$. This implies $\\operatorname{tr}(C_i) \\leq \\operatorname{tr}(u^2 s_f(y)^2 I_n) = n u^2 s_f(y)^2$.\n$\\mathrm{Var}(d_{\\mathrm{round}}(\\epsilon)) \\leq \\frac{1}{\\epsilon^2}(\\operatorname{tr}(C_1) + \\operatorname{tr}(C_2)) \\leq \\frac{2n u^2 s_f(y)^2}{\\epsilon^2}$.\nThe term $\\frac{c_r}{K\\epsilon^2}$ in the total variance implies $c_r = 2n u^2 s_f(y)^2$.\n\nNext, we analyze the probe-variance term, $\\mathrm{Var}(d_{\\mathrm{exact}}(\\epsilon))$. The problem asks for a constant $c_v$ independent of $\\epsilon$, which suggests we should consider the limit $\\epsilon \\to 0$.\n$d_{\\mathrm{exact}}(\\epsilon) = z^{\\top}J_f(y)z + O(\\epsilon)$.\nThe leading-order variance is $\\mathrm{Var}(z^{\\top}J_f(y)z)$. Let $J = J_f(y)$. We know $\\mathbb{E}[z^{\\top}Jz] = \\operatorname{tr}(J)$.\nThe variance is $\\mathrm{Var}(z^{\\top}Jz) = \\mathbb{E}[(z^{\\top}Jz)^2] - (\\operatorname{tr}(J))^2$.\nUsing Isserlis' theorem, $\\mathbb{E}[(\\sum_{i,j} J_{ij} z_i z_j)^2] = \\sum_{i,j,k,l} J_{ij} J_{kl} \\mathbb{E}[z_i z_j z_k z_l] = \\sum_{i,j,k,l} J_{ij} J_{kl} (\\delta_{ij}\\delta_{kl} + \\delta_{ik}\\delta_{jl} + \\delta_{il}\\delta_{jk})$.\nThis evaluates to $(\\operatorname{tr}(J))^2 + \\|J\\|_F^2 + \\operatorname{tr}(J^2)$, where $\\|J\\|_F$ is the Frobenius norm.\nSo, $\\mathrm{Var}(z^{\\top}Jz) = \\|J\\|_F^2 + \\operatorname{tr}(J^2)$. This is the constant $c_v$.\n$c_v = \\|J_f(y)\\|_F^2 + \\operatorname{tr}((J_f(y))^2)$.\nCombining the terms, the variance of the divergence estimator is bounded by:\n$$\n\\mathrm{Var}(\\widehat{\\mathrm{div}}_{\\epsilon,K}) \\leq \\frac{c_v}{K} + \\frac{c_r}{K\\epsilon^2}\n$$\nwith $c_v = \\|J_f(y)\\|_F^2 + \\operatorname{tr}((J_f(y))^2)$ and $c_r = 2n u^2 s_f(y)^2$.\n\nPart 3: Optimal Step Size $\\epsilon^{\\star}$\n\nThe MSE of the SURE estimator is $\\mathbb{E}[(\\mathrm{SURE}_{\\epsilon,K} - \\mathrm{SURE})^2]$.\n$\\mathrm{SURE}_{\\epsilon,K} - \\mathrm{SURE} = 2\\sigma^2(\\widehat{\\mathrm{div}}_{\\epsilon,K} - \\operatorname{div}f)$.\nThe MSE is $(2\\sigma^2)^2 \\mathbb{E}[(\\widehat{\\mathrm{div}}_{\\epsilon,K} - \\operatorname{div}f)^2] = (2\\sigma^2)^2 \\mathrm{MSE}(\\widehat{\\mathrm{div}}_{\\epsilon,K})$.\nThe MSE of the divergence estimator is the sum of its variance and squared bias:\n$$\n\\mathrm{MSE}(\\widehat{\\mathrm{div}}_{\\epsilon,K}) = \\mathrm{Var}(\\widehat{\\mathrm{div}}_{\\epsilon,K}) + (\\mathrm{Bias}(\\widehat{\\mathrm{div}}_{\\epsilon,K}))^2\n$$\nUsing the leading-order approximations and bounds from parts 1 and 2:\n$$\n\\mathrm{MSE}(\\widehat{\\mathrm{div}}_{\\epsilon,K}) \\approx \\left(\\frac{c_v}{K} + \\frac{c_r}{K\\epsilon^2}\\right) + (c_b\\epsilon^2)^2 = \\frac{c_v}{K} + \\frac{c_r}{K\\epsilon^2} + c_b^2 \\epsilon^4\n$$\nThe problem asks to minimize the $\\epsilon$-dependent leading-order MSE. The scaling factor $(2\\sigma^2)^2$ and the $\\epsilon$-independent term $\\frac{c_v}{K}$ do not affect the optimal $\\epsilon$. We minimize the function:\n$$\ng(\\epsilon) = c_b^2 \\epsilon^4 + \\frac{c_r}{K\\epsilon^2}\n$$\nTo find the minimum, we set its derivative with respect to $\\epsilon$ to zero:\n$$\n\\frac{dg}{d\\epsilon} = 4c_b^2\\epsilon^3 - \\frac{2c_r}{K\\epsilon^3} = 0\n$$\n$$\n4c_b^2(\\epsilon^{\\star})^3 = \\frac{2c_r}{K(\\epsilon^{\\star})^3} \\implies (\\epsilon^{\\star})^6 = \\frac{2c_r}{4Kc_b^2} = \\frac{c_r}{2Kc_b^2}\n$$\nThe second derivative $g''(\\epsilon) = 12c_b^2\\epsilon^2 + \\frac{6c_r}{K\\epsilon^4}$ is always positive for $\\epsilon > 0$, so this is a minimum.\nThe optimal step size is:\n$$\n\\epsilon^{\\star} = \\left(\\frac{c_r}{2Kc_b^2}\\right)^{1/6}\n$$\nSubstituting the expressions for $c_b$ and $c_r$:\n$$\n\\epsilon^{\\star} = \\left(\\frac{2n u^2 s_f(y)^2}{2K \\left(\\frac{M_3(n^2+2n)}{6}\\right)^2}\\right)^{1/6} = \\left(\\frac{n u^2 s_f(y)^2}{K \\frac{M_3^2 n^2(n+2)^2}{36}}\\right)^{1/6} = \\left(\\frac{36 u^2 s_f(y)^2}{K M_3^2 n (n+2)^2}\\right)^{1/6}\n$$\nThis is the final closed-form expression for the optimal step size $\\epsilon^{\\star}$.", "answer": "$$ \\boxed{ \\left( \\frac{c_r}{2 K c_b^2} \\right)^{1/6} } $$", "id": "3482321"}, {"introduction": "Building on our theoretical and practical foundations, this final practice applies these concepts to a sophisticated problem: dynamically tuning the regularization parameter for a nonconvex penalty like MCP [@problem_id:3482335]. You will implement the Stein's Unbiased Gradient Risk Estimator (SUGAR) to create a risk-minimizing continuation path for the parameter $\\lambda$, effectively using gradient descent on the risk landscape itself. This hands-on coding exercise demonstrates how the SURE framework extends beyond model assessment to active, gradient-based optimization of hyperparameters, connecting it to the broader field of homotopy methods.", "problem": "Consider the sparse denoising model in which a signal $x_0 \\in \\mathbb{R}^n$ is observed through additive Gaussian noise $w \\sim \\mathcal{N}(0,\\sigma^2 I_n)$, yielding $y = x_0 + w$. Let an estimator $\\mu(y; \\lambda, \\gamma)$ be defined as any minimizer of the objective $J(x; y, \\lambda, \\gamma) = \\tfrac{1}{2}\\lVert x - y \\rVert_2^2 + \\sum_{i=1}^n p_{\\lambda,\\gamma}(|x_i|)$, where $p_{\\lambda,\\gamma}(\\cdot)$ is the Minimax Concave Penalty (MCP) with regularization scale $\\lambda > 0$ and nonconvexity parameter $\\gamma > 1$. The MCP is defined by $p_{\\lambda,\\gamma}(t) = \\lambda \\int_0^t \\max\\{0, 1 - \\tfrac{s}{\\gamma \\lambda}\\} \\, ds$, which is a well-tested nonconvex penalty in sparse optimization.\n\nStarting from the fundamental facts of Gaussian noise modeling and the Stein's lemma for Gaussian divergence, derive an unbiased risk estimate that depends only on $y$, $\\sigma$, and the divergence of $\\mu(y; \\lambda, \\gamma)$ with respect to $y$. Then derive a computable expression for the gradient of this risk estimate with respect to $\\lambda$ using the Stein's Unbiased Gradient Risk Estimator (SUGAR), making use of Monte Carlo trace estimation of divergences through Gaussian probing. Treat $\\mu(y; \\lambda, \\gamma)$ as a separable estimator acting element-wise and include any required partial derivatives of the estimator with respect to the data $y$ and parameter $\\lambda$. You must not rely on any shortcut formulas for the final risk or gradient; instead, base your derivations on the stated definitions and properties.\n\nUsing these derivations, implement a continuation path $\\lambda(t)$, $t \\in [0,1]$, parameterized by discrete steps $t_k = \\tfrac{k}{T}$ for a fixed positive integer $T$. Initialize $\\lambda(0)$ at a data-dependent large value and evolve $\\lambda(t)$ via a gradient descent on the unbiased risk estimate with respect to $\\lambda$, using the SUGAR gradient. At each step, clip $\\lambda(t)$ into a positive interval to ensure well-defined estimators. Quantify the following path analysis quantities:\n- A regularity statistic defined as the maximum absolute step size $\\max_k |\\lambda(t_{k+1}) - \\lambda(t_k)|$ over the path.\n- The number of bifurcation events along the path, defined as the number of indices $k$ at which the support size of $\\mu(y; \\lambda(t_k), \\gamma)$ changes relative to $\\mu(y; \\lambda(t_{k-1}), \\gamma)$.\n- A boolean indicator of whether the support size is monotone nondecreasing along the path as $t$ increases (interpreting $\\lambda(t)$ as a decreasing homotopy parameter in sparse recovery).\n\nYour program must perform the following tasks for each test case:\n1. Generate a $k$-sparse ground truth $x_0 \\in \\mathbb{R}^n$ with nonzero entries drawn independently from the standard normal distribution, placed at uniformly random indices.\n2. Generate $y = x_0 + w$ with $w \\sim \\mathcal{N}(0,\\sigma^2 I_n)$ using a fixed pseudorandom seed provided in the test case for reproducibility.\n3. Implement the estimator $\\mu(y; \\lambda, \\gamma)$ via the element-wise MCP proximal mapping resulting from the stated optimization objective.\n4. Derive and implement Stein's unbiased risk estimate in terms of $y$, $\\sigma$, and the divergence of $\\mu$ with respect to $y$.\n5. Derive and implement the SUGAR gradient $\\tfrac{\\partial}{\\partial \\lambda} \\mathrm{SURE}(y; \\lambda, \\gamma)$ using Monte Carlo trace estimation with a Gaussian probe and a small finite-difference parameter.\n6. Track the continuation path $\\lambda(t)$ via gradient descent for $T$ steps with a specified step size and clipping bounds.\n7. Compute and return, for each test case, the tuple consisting of the final value $\\lambda(1)$, the number of bifurcation events, the regularity statistic, and the monotonicity indicator converted to an integer ($1$ for true, $0$ for false).\n\nYou must use radians if you introduce any angles, although none are expected here. No physical units are involved. All numeric outputs must be dimensionless numbers.\n\nTest Suite:\nFor each of the following test cases, your program must use the specified parameters and random seed.\n\n- Test Case A (happy path, moderate nonconvexity):\n  - $n = 80$, $k = 8$, $\\sigma = 0.15$, $\\gamma = 3.0$, $T = 120$, step size $\\alpha = 0.005$, seed $s = 1234$.\n- Test Case B (edge case, strong nonconvexity):\n  - $n = 80$, $k = 8$, $\\sigma = 0.15$, $\\gamma = 1.2$, $T = 120$, step size $\\alpha = 0.001$, seed $s = 5678$.\n- Test Case C (boundary case, near-convex penalty approximating the least absolute shrinkage and selection operator (LASSO)):\n  - $n = 80$, $k = 8$, $\\sigma = 0.05$, $\\gamma = 100.0$, $T = 120$, step size $\\alpha = 0.010$, seed $s = 9012$.\n\nInitialization and bounds:\n- Initialize $\\lambda(0)$ as $\\lambda_{\\mathrm{max}} = \\max_{i} |y_i|$.\n- Clip $\\lambda(t)$ to the interval $[\\lambda_{\\min}, \\lambda_{\\max}]$ with $\\lambda_{\\min} = 10^{-6}$ and $\\lambda_{\\max}$ as above.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated Python-style list of lists, each inner list containing four values in the order described above. For example: \"[[lambda_A,bifurcations_A,regularity_A,monotone_A],[lambda_B,bifurcations_B,regularity_B,monotone_B],[lambda_C,bifurcations_C,regularity_C,monotone_C]]\". Numerical values should be provided as floats for the first and third components, integers for the second and fourth components.", "solution": "The problem asks for the derivation and implementation of a parameter tuning scheme for the Minimax Concave Penalty (MCP) in a sparse signal denoising context. The core of the task is to use Stein's Unbiased Risk Estimate (SURE) and its gradient, estimated via the Stein's Unbiased Gradient Risk Estimator (SUGAR) with Monte Carlo techniques, to define a continuation path for the regularization parameter $\\lambda$.\n\nFirst, we formalize the estimator $\\mu(y; \\lambda, \\gamma)$. The objective function to minimize is $J(x; y, \\lambda, \\gamma) = \\tfrac{1}{2}\\lVert x - y \\rVert_2^2 + \\sum_{i=1}^n p_{\\lambda,\\gamma}(|x_i|)$. This objective is separable, meaning we can find the optimal $x = (x_1, \\dots, x_n)$ by minimizing each component $x_i$ independently:\n$$\n\\hat{x}_i = \\arg\\min_{x_i \\in \\mathbb{R}} \\left\\{ \\frac{1}{2}(x_i - y_i)^2 + p_{\\lambda,\\gamma}(|x_i|) \\right\\}\n$$\nThis is, by definition, the proximal operator of the function $p_{\\lambda,\\gamma}(|\\cdot|)$ applied to $y_i$. Let $\\mu_i(y_i; \\lambda, \\gamma)$ denote this solution. The derivative of the MCP penalty $p_{\\lambda,\\gamma}(t)$ for $t > 0$ is $p'_{\\lambda,\\gamma}(t) = \\lambda \\max\\{0, 1 - \\tfrac{t}{\\gamma \\lambda}\\}$. The first-order optimality condition is $0 \\in x_i - y_i + \\partial p_{\\lambda,\\gamma}(|x_i|)$, where $\\partial$ denotes the subgradient. Solving this for $x_i$ yields the element-wise estimator, which is a firm-thresholding function:\n$$\n\\mu_i(y_i; \\lambda, \\gamma) = \\begin{cases} 0 & \\text{if } |y_i| \\le \\lambda \\\\ \\mathrm{sgn}(y_i) \\frac{\\gamma(|y_i| - \\lambda)}{\\gamma - 1} & \\text{if } \\lambda < |y_i| \\le \\gamma \\lambda \\\\ y_i & \\text{if } |y_i| > \\gamma \\lambda \\end{cases}\n$$\nThis function is separable, i.e., $\\mu_i$ depends only on $y_i$.\n\nNext, we derive the unbiased risk estimate. The risk, or expected mean-squared error (MSE), is $R(\\lambda) = \\mathbb{E}[\\lVert \\mu(y; \\lambda) - x_0 \\rVert_2^2]$. We can expand this and use the fact that $y = x_0 + w$ with $w \\sim \\mathcal{N}(0, \\sigma^2 I_n)$:\n$$\n\\begin{aligned}\nR(\\lambda) &= \\mathbb{E}[\\lVert (\\mu(y) - y) + (y - x_0) \\rVert_2^2] \\\\\n&= \\mathbb{E}[\\lVert \\mu(y) - y \\rVert_2^2 + \\lVert y - x_0 \\rVert_2^2 + 2(y-x_0)^T(\\mu(y) - y)] \\\\\n&= \\mathbb{E}[\\lVert \\mu(y) - y \\rVert_2^2] + \\mathbb{E}[\\lVert w \\rVert_2^2] + 2\\mathbb{E}[w^T(\\mu(y) - y)]\n\\end{aligned}\n$$\nThe second term is $\\mathbb{E}[\\lVert w \\rVert_2^2] = n\\sigma^2$. For the third term, we apply Stein's Lemma, which states that for a random vector $w \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ and a weakly differentiable function $h: \\mathbb{R}^n \\to \\mathbb{R}^n$, $\\mathbb{E}[w^T h(y)] = \\sigma^2 \\mathbb{E}[\\nabla_y \\cdot h(y)]$. Let $h(y) = \\mu(y) - y$. The divergence is $\\nabla_y \\cdot h(y) = \\nabla_y \\cdot \\mu(y) - \\nabla_y \\cdot y = (\\sum_{i=1}^n \\frac{\\partial \\mu_i}{\\partial y_i}) - n$.\nSubstituting back, we get:\n$$\nR(\\lambda) = \\mathbb E\\left[\\lVert\\mu(y) - y\\rVert_2^2 + n\\sigma^2 + 2\\sigma^2 (\\nabla_y \\cdot \\mu(y) - n) \\right] = \\mathbb E\\left[\\lVert\\mu(y) - y\\rVert_2^2 - n\\sigma^2 + 2\\sigma^2 \\nabla_y \\cdot \\mu(y) \\right]\n$$\nAn unbiased estimator for $R(\\lambda)$ is obtained by dropping the expectation, yielding Stein's Unbiased Risk Estimate (SURE):\n$$\n\\mathrm{SURE}(y; \\lambda) = \\lVert\\mu(y) - y\\rVert_2^2 - n\\sigma^2 + 2\\sigma^2 \\nabla_y \\cdot \\mu(y)\n$$\nwhere the divergence term is $\\nabla_y \\cdot \\mu(y) = \\sum_{i=1}^n \\frac{\\partial \\mu_i(y_i)}{\\partial y_i}$. The derivative $\\frac{\\partial \\mu_i}{\\partial y_i}$ is piecewise constant:\n$$\n\\frac{\\partial \\mu_i(y_i)}{\\partial y_i} = \\begin{cases} 0 & \\text{if } |y_i| < \\lambda \\\\ \\frac{\\gamma}{\\gamma-1} & \\text{if } \\lambda < |y_i| < \\gamma\\lambda \\\\ 1 & \\text{if } |y_i| > \\gamma\\lambda \\end{cases}\n$$\nThe points of discontinuity have zero measure and do not affect the integration in Stein's Lemma.\n\nThe next step is to find the gradient of the risk with respect to $\\lambda$, $\\frac{\\partial R}{\\partial \\lambda}$, to perform gradient descent. A naive differentiation of the SURE formula is problematic, as it involves derivatives of the discontinuous function $\\frac{\\partial \\mu_i}{\\partial y_i}$. The SUGAR framework provides an unbiased estimator for the true risk gradient. The gradient of the risk is:\n$$\n\\frac{\\partial R}{\\partial \\lambda} = \\frac{\\partial}{\\partial \\lambda} \\mathbb{E}[\\lVert \\mu(y; \\lambda) - x_0 \\rVert_2^2] = \\mathbb{E}\\left[2(\\mu - x_0)^T \\frac{\\partial \\mu}{\\partial \\lambda}\\right]\n$$\nUsing $x_0 = y - w$, we have:\n$$\n\\frac{\\partial R}{\\partial \\lambda} = \\mathbb{E}\\left[2(\\mu - y)^T \\frac{\\partial \\mu}{\\partial \\lambda} + 2w^T \\frac{\\partial \\mu}{\\partial \\lambda}\\right]\n$$\nApplying Stein's Lemma to the second term with $h(y) = \\frac{\\partial \\mu}{\\partial \\lambda}(y; \\lambda)$ gives $\\mathbb{E}[w^T \\frac{\\partial \\mu}{\\partial \\lambda}] = \\sigma^2 \\mathbb{E}[\\nabla_y \\cdot \\frac{\\partial \\mu}{\\partial \\lambda}]$. Thus:\n$$\n\\frac{\\partial R}{\\partial \\lambda} = \\mathbb{E}\\left[2(\\mu - y)^T \\frac{\\partial \\mu}{\\partial \\lambda} + 2\\sigma^2 \\nabla_y \\cdot \\frac{\\partial \\mu}{\\partial \\lambda}\\right]\n$$\nThe SUGAR gradient estimator, $\\hat{g}(\\lambda)$, is the term inside the expectation:\n$$\n\\hat{g}(\\lambda) = 2(\\mu(y; \\lambda) - y)^T \\frac{\\partial \\mu}{\\partial \\lambda}(y; \\lambda) + 2\\sigma^2 \\left(\\nabla_y \\cdot \\frac{\\partial \\mu}{\\partial \\lambda}\\right)(y; \\lambda)\n$$\nTo create a computable expression, we approximate the partial derivatives and the divergence. The partial derivative with respect to $\\lambda$ is approximated using a central finite difference with a small step $\\epsilon_\\lambda$:\n$$\n\\frac{\\partial \\mu(y; \\lambda)}{\\partial \\lambda} \\approx \\frac{\\mu(y; \\lambda+\\epsilon_\\lambda) - \\mu(y; \\lambda-\\epsilon_\\lambda)}{2\\epsilon_\\lambda}\n$$\nThe divergence term, $\\nabla_y \\cdot f(y) = \\operatorname{tr}(J_f(y))$ where $f(y) = \\frac{\\partial \\mu}{\\partial \\lambda}$, is estimated using a Monte Carlo method with a Gaussian probe vector $\\delta \\sim \\mathcal{N}(0, I_n)$ and a small step $\\epsilon_y$. The divergence is estimated as:\n$$\n\\nabla_y \\cdot f(y) \\approx \\frac{1}{\\epsilon_y} \\delta^T (f(y+\\epsilon_y\\delta) - f(y))\n$$\nCombining these approximations, the SUGAR gradient estimator is implemented as:\nLet $f(z, \\lambda_{val}) = \\frac{\\mu(z; \\lambda_{val}+\\epsilon_\\lambda) - \\mu(z; \\lambda_{val}-\\epsilon_\\lambda)}{2\\epsilon_\\lambda}$.\nThe first term of $\\hat{g}(\\lambda)$ is $T_1 = 2(\\mu(y; \\lambda) - y)^T f(y, \\lambda)$.\nThe divergence is estimated as $\\nabla_y \\cdot f \\approx \\frac{1}{\\epsilon_y} \\delta^T (f(y+\\epsilon_y\\delta, \\lambda) - f(y, \\lambda))$.\nThe second term of $\\hat{g}(\\lambda)$ is $T_2 = 2\\sigma^2 \\frac{1}{\\epsilon_y} \\delta^T (f(y+\\epsilon_y\\delta, \\lambda) - f(y, \\lambda))$.\nThe final computable gradient is $\\hat{g}(\\lambda) = T_1 + T_2$. This requires evaluating the estimator $\\mu$ four times per gradient computation.\n\nThe overall algorithm proceeds as follows:\n1.  For a given test case, generate the sparse signal $x_0$ and the noisy observation $y$.\n2.  Initialize the regularization parameter $\\lambda_0 = \\max_i |y_i|$. This value ensures that the initial estimate $\\mu(y, \\lambda_0)$ is the zero vector, a common starting point in homotopy methods.\n3.  Pre-generate a single probe vector $\\delta \\sim \\mathcal{N}(0, I_n)$ for use in all SUGAR gradient calculations.\n4.  Iterate for $k=0, \\dots, T-1$:\n    a.  Compute the SUGAR gradient $\\hat{g}(\\lambda_k)$ at the current parameter value $\\lambda_k = \\lambda(t_k)$.\n    b.  Update the parameter via gradient descent: $\\lambda' = \\lambda_k - \\alpha \\hat{g}(\\lambda_k)$.\n    c.  Clip the new value to the interval $[\\lambda_{\\min}, \\lambda_{\\max}]$ to obtain $\\lambda_{k+1}$.\n5.  After $T$ steps, analyze the generated path $\\{\\lambda_k\\}_{k=0}^T$.\n    a.  Compute the support size $S_k = \\lVert\\mu(y; \\lambda_k)\\rVert_0$ for each $\\lambda_k$.\n    b.  The number of bifurcations is the count of $k \\in \\{1, \\dots, T\\}$ where $S_k \\neq S_{k-1}$.\n    c.  The regularity statistic is $\\max_{k \\in \\{0, \\dots, T-1\\}} |\\lambda_{k+1} - \\lambda_k|$.\n    d.  The monotonicity indicator is $1$ if $S_k \\ge S_{k-1}$ for all $k \\in \\{1, \\dots, T\\}$, and $0$ otherwise.\n6.  The final result for one test case is the tuple $(\\lambda_T, \\text{bifurcations}, \\text{regularity}, \\text{monotonicity})$.", "answer": "```python\nimport numpy as np\n\ndef mcp_prox(y, lam, gam):\n    \"\"\"\n    Computes the element-wise proximal operator of the MCP.\n    This corresponds to the estimator mu(y; lambda, gamma).\n    \"\"\"\n    if lam < 0:\n        return np.full_like(y, np.nan)\n        \n    y_abs = np.abs(y)\n    y_sign = np.sign(y)\n    \n    # Handle the gamma -> 1 case, which approaches hard thresholding\n    if np.isclose(gam, 1.0):\n        res = np.where(y_abs > lam, y, 0.0)\n        return res\n        \n    # Standard MCP for gamma > 1\n    term1 = np.zeros_like(y)\n    term2 = y_sign * gam * (y_abs - lam) / (gam - 1.0)\n    term3 = y\n    \n    # Piecewise application\n    res = np.where(y_abs <= lam, term1, term3)\n    res = np.where((y_abs > lam) & (y_abs <= gam * lam), term2, res)\n    \n    return res\n\ndef compute_sugar_gradient(y, lam, gam, sigma, delta, eps_y, eps_lam, prox_func):\n    \"\"\"\n    Computes the SUGAR gradient of the risk with respect to lambda.\n    \"\"\"\n    # Finite difference approximation of d(mu)/d(lambda)\n    def grad_lam_mu_approx(vec_y, val_lam):\n        # Clip lambda for finite difference to avoid negative values\n        lam_plus = max(0.0, val_lam + eps_lam)\n        lam_minus = max(0.0, val_lam - eps_lam)\n        mu_p = prox_func(vec_y, lam_plus, gam)\n        mu_m = prox_func(vec_y, lam_minus, gam)\n        return (mu_p - mu_m) / (2.0 * eps_lam)\n\n    # Compute d(mu)/d(lambda) at the current point y\n    grad_lam_mu_at_y = grad_lam_mu_approx(y, lam)\n    \n    # Compute mu at the current point\n    mu_at_y = prox_func(y, lam, gam)\n    \n    # First term of the SUGAR gradient\n    term1 = 2.0 * np.dot(mu_at_y - y, grad_lam_mu_at_y)\n    \n    # Second term (divergence) of the SUGAR gradient\n    # Compute d(mu)/d(lambda) at the perturbed point y + eps_y * delta\n    y_pert = y + eps_y * delta\n    grad_lam_mu_at_y_pert = grad_lam_mu_approx(y_pert, lam)\n    \n    # Estimate divergence using the Gaussian probe\n    div_est = (1.0 / eps_y) * np.dot(delta, grad_lam_mu_at_y_pert - grad_lam_mu_at_y)\n    term2 = 2.0 * sigma**2 * div_est\n    \n    return term1 + term2\n\ndef run_case(n, k, sigma, gam, T, alpha, seed):\n    \"\"\"\n    Runs a single test case for the MCP parameter path analysis.\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    \n    # 1. Generate data\n    x0 = np.zeros(n)\n    nonzero_indices = rng.choice(n, k, replace=False)\n    x0[nonzero_indices] = rng.randn(k)\n    \n    noise = rng.randn(n) * sigma\n    y = x0 + noise\n    \n    # 2. Initialization and bounds\n    lambda_max = np.max(np.abs(y))\n    lambda_min = 1e-6\n    \n    lambda_path = [lambda_max]\n    current_lambda = lambda_max\n    \n    # 3. Generate probe for SUGAR gradient\n    delta = rng.randn(n)\n    eps_y = 1e-6\n    eps_lam = 1e-6\n    \n    # 4. Track continuation path\n    for _ in range(T):\n        grad = compute_sugar_gradient(y, current_lambda, gam, sigma, delta, eps_y, eps_lam, mcp_prox)\n        \n        # Gradient descent step\n        current_lambda = current_lambda - alpha * grad\n        \n        # Clipping\n        current_lambda = np.clip(current_lambda, lambda_min, lambda_max)\n        \n        lambda_path.append(current_lambda)\n        \n    lambda_path = np.array(lambda_path)\n    \n    # 5. Path Analysis\n    # a. Final lambda\n    final_lambda = lambda_path[-1]\n    \n    # b. Regularity statistic\n    regularity_stat = np.max(np.abs(np.diff(lambda_path)))\n    \n    # c. Bifurcation events and Monotonicity\n    support_sizes = []\n    for lam in lambda_path:\n        mu = mcp_prox(y, lam, gam)\n        support_sizes.append(np.count_nonzero(mu))\n        \n    support_sizes = np.array(support_sizes)\n    support_diffs = np.diff(support_sizes)\n    \n    bifurcation_events = np.count_nonzero(support_diffs)\n    \n    # Monotonicity check: support must be non-decreasing as t increases\n    is_monotone = 1 if np.all(support_diffs >= 0) else 0\n\n    return final_lambda, bifurcation_events, regularity_stat, is_monotone\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (n, k, sigma, gamma, T, step_size_alpha, seed)\n        (80, 8, 0.15, 3.0, 120, 0.005, 1234),  # Test Case A\n        (80, 8, 0.15, 1.2, 120, 0.001, 5678),  # Test Case B\n        (80, 8, 0.05, 100.0, 120, 0.010, 9012) # Test Case C\n    ]\n\n    results = []\n    for case_params in test_cases:\n        final_lambda, bifurcations, regularity, monotone = run_case(*case_params)\n        results.append([final_lambda, bifurcations, regularity, int(monotone)])\n\n    # Format the final output string\n    inner_strings = [f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3482335"}]}