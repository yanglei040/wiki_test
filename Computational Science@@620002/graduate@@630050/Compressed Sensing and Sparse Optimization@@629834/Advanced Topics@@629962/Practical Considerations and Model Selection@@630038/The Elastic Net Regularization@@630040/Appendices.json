{"hands_on_practices": [{"introduction": "To build a strong intuition for the elastic net, it is often best to begin in an idealized setting. This first practice simplifies the general linear regression problem by assuming an orthonormal design matrix, a hypothetical scenario that decouples the parameter estimates. This simplification allows for the derivation of a clear, closed-form solution, revealing precisely how the elastic net estimator blends the soft-thresholding behavior of the LASSO with the continuous shrinkage of ridge regression [@problem_id:3487889].", "problem": "Consider a linear regression model with design matrix $X \\in \\mathbb{R}^{n \\times p}$ satisfying the orthonormal design condition $X^{\\top}X = I$, where $I$ is the identity matrix. Let $y \\in \\mathbb{R}^{n}$ be the response vector and define the vector $z = X^{\\top}y \\in \\mathbb{R}^{p}$. For a coefficient vector $\\beta \\in \\mathbb{R}^{p}$, consider the following three penalized least squares estimators:\n1. The least absolute shrinkage and selection operator (LASSO), defined as the minimizer of\n$$\n\\frac{1}{2}\\lVert y - X\\beta \\rVert_{2}^{2} + \\lambda_{1}\\lVert \\beta \\rVert_{1},\n$$\nwhere $\\lambda_{1} > 0$ and $\\lVert \\beta \\rVert_{1} = \\sum_{j=1}^{p} |\\beta_{j}|$.\n2. Ridge regression, defined as the minimizer of\n$$\n\\frac{1}{2}\\lVert y - X\\beta \\rVert_{2}^{2} + \\frac{\\lambda_{2}}{2}\\lVert \\beta \\rVert_{2}^{2},\n$$\nwhere $\\lambda_{2} > 0$ and $\\lVert \\beta \\rVert_{2}^{2} = \\sum_{j=1}^{p} \\beta_{j}^{2}$.\n3. The elastic net, defined as the minimizer of\n$$\n\\frac{1}{2}\\lVert y - X\\beta \\rVert_{2}^{2} + \\lambda_{1}\\lVert \\beta \\rVert_{1} + \\frac{\\lambda_{2}}{2}\\lVert \\beta \\rVert_{2}^{2},\n$$\nwith $\\lambda_{1} > 0$ and $\\lambda_{2} > 0$.\n\nStarting from first principles of convex optimization and the orthonormality condition $X^{\\top}X = I$, derive the coordinate-wise closed-form solution for the elastic net estimator in terms of $z_{j}$, $\\lambda_{1}$, and $\\lambda_{2}$. Then similarly obtain the coordinate-wise solutions for the LASSO and ridge regression. Finally, for a single coordinate $j$ with $z_{j} = 2.7$, $\\lambda_{1} = 1.5$, and $\\lambda_{2} = 0.5$, compute the three coordinate-wise estimates for the elastic net, LASSO, and ridge regression, respectively. Present your final numerical answer as a single row matrix in the order: elastic net, LASSO, ridge. No rounding is required.", "solution": "The core of the problem is to find the minimizers of three different objective functions. All three objective functions share a common least squares term, $\\frac{1}{2}\\lVert y - X\\beta \\rVert_{2}^{2}$. We can simplify this term using the given orthonormal design condition, $X^{\\top}X = I$, and the definition $z = X^{\\top}y$.\n\nLet's expand the squared $\\ell_2$-norm:\n$$\n\\lVert y - X\\beta \\rVert_{2}^{2} = (y - X\\beta)^{\\top}(y - X\\beta) = y^{\\top}y - y^{\\top}X\\beta - \\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta\n$$\nUsing $X^{\\top}X = I$ and $z = X^{\\top}y$ (which implies $z^{\\top} = y^{\\top}X$), the expression becomes:\n$$\ny^{\\top}y - z^{\\top}\\beta - \\beta^{\\top}z + \\beta^{\\top}I\\beta = y^{\\top}y - 2\\beta^{\\top}z + \\lVert \\beta \\rVert_{2}^{2}\n$$\nWe can rewrite this by completing the square with respect to $\\beta$:\n$$\ny^{\\top}y - 2\\beta^{\\top}z + \\lVert \\beta \\rVert_{2}^{2} = \\lVert \\beta \\rVert_{2}^{2} - 2\\beta^{\\top}z + \\lVert z \\rVert_{2}^{2} - \\lVert z \\rVert_{2}^{2} + y^{\\top}y = \\lVert z - \\beta \\rVert_{2}^{2} + y^{\\top}y - \\lVert z \\rVert_{2}^{2}\n$$\nSince $y^{\\top}y - \\lVert z \\rVert_{2}^{2}$ is a constant with respect to $\\beta$, minimizing an objective function of the form $\\frac{1}{2}\\lVert y - X\\beta \\rVert_{2}^{2} + P(\\beta)$ is equivalent to minimizing $\\frac{1}{2}\\lVert z - \\beta \\rVert_{2}^{2} + P(\\beta)$, where $P(\\beta)$ is the penalty term.\n\nThe simplified objective function is separable with respect to the coordinates of $\\beta$ because both the squared error term $\\frac{1}{2}\\lVert z - \\beta \\rVert_{2}^{2} = \\frac{1}{2}\\sum_{j=1}^{p}(z_j - \\beta_j)^2$ and the penalties ($\\lVert \\beta \\rVert_1 = \\sum_j |\\beta_j|$, $\\lVert \\beta \\rVert_2^2 = \\sum_j \\beta_j^2$) are sums over the individual coordinates. Thus, we can solve for each coefficient $\\hat{\\beta}_j$ independently by minimizing its corresponding coordinate-wise objective function.\n\nFirst, we derive the solution for the elastic net estimator. The objective function to minimize for a single coordinate $\\beta_j$ is:\n$$\nJ_{\\text{enet}}(\\beta_j) = \\frac{1}{2}(z_j - \\beta_j)^2 + \\lambda_1 |\\beta_j| + \\frac{\\lambda_2}{2}\\beta_j^2\n$$\nThis is a convex function. The minimum is found where its subgradient with respect to $\\beta_j$ contains $0$. The subgradient is:\n$$\n\\partial_{\\beta_j} J_{\\text{enet}}(\\beta_j) = -(z_j - \\beta_j) + \\lambda_1 \\partial(|\\beta_j|) + \\lambda_2 \\beta_j\n$$\nwhere $\\partial(|\\beta_j|)$ is the subgradient of the absolute value function: it is $\\text{sgn}(\\beta_j)$ for $\\beta_j \\neq 0$ and the interval $[-1, 1]$ for $\\beta_j = 0$.\n\nWe analyze three cases for the optimal value $\\hat{\\beta}_j$:\n1.  If $\\hat{\\beta}_j > 0$, then $\\partial(|\\hat{\\beta}_j|) = 1$. Setting the gradient to $0$:\n    $$\n    -\\left(z_j - \\hat{\\beta}_j\\right) + \\lambda_1 + \\lambda_2 \\hat{\\beta}_j = 0 \\implies \\hat{\\beta}_j - z_j + \\lambda_1 + \\lambda_2 \\hat{\\beta}_j = 0 \\implies \\hat{\\beta}_j(1 + \\lambda_2) = z_j - \\lambda_1\n    $$\n    $$\n    \\hat{\\beta}_j = \\frac{z_j - \\lambda_1}{1 + \\lambda_2}\n    $$\n    This solution is valid only if $\\hat{\\beta}_j > 0$, which implies $z_j - \\lambda_1 > 0$, or $z_j > \\lambda_1$.\n\n2.  If $\\hat{\\beta}_j < 0$, then $\\partial(|\\hat{\\beta}_j|) = -1$. Setting the gradient to $0$:\n    $$\n    -\\left(z_j - \\hat{\\beta}_j\\right) - \\lambda_1 + \\lambda_2 \\hat{\\beta}_j = 0 \\implies \\hat{\\beta}_j - z_j - \\lambda_1 + \\lambda_2 \\hat{\\beta}_j = 0 \\implies \\hat{\\beta}_j(1 + \\lambda_2) = z_j + \\lambda_1\n    $$\n    $$\n    \\hat{\\beta}_j = \\frac{z_j + \\lambda_1}{1 + \\lambda_2}\n    $$\n    This solution is valid only if $\\hat{\\beta}_j < 0$, which implies $z_j + \\lambda_1 < 0$, or $z_j < -\\lambda_1$.\n\n3.  If $\\hat{\\beta}_j = 0$, the subgradient condition is $0 \\in -(z_j - 0) + \\lambda_1[-1, 1] + 0$.\n    This simplifies to $0 \\in -z_j + [-\\lambda_1, \\lambda_1]$, which means $z_j \\in [-\\lambda_1, \\lambda_1]$, or $|z_j| \\le \\lambda_1$.\n\nCombining these three cases, we obtain the coordinate-wise solution for the elastic net estimator:\n$$\n\\hat{\\beta}_{j}^{\\text{enet}} = \\begin{cases} \\frac{z_j - \\lambda_1}{1 + \\lambda_2} & \\text{if } z_j > \\lambda_1 \\\\ \\frac{z_j + \\lambda_1}{1 + \\lambda_2} & \\text{if } z_j < -\\lambda_1 \\\\ 0 & \\text{if } |z_j| \\le \\lambda_1 \\end{cases}\n$$\nThis can be written compactly using the soft-thresholding operator, $S_{\\lambda}(x) = \\text{sgn}(x)\\max(0, |x|-\\lambda)$, as:\n$$\n\\hat{\\beta}_{j}^{\\text{enet}} = \\frac{S_{\\lambda_1}(z_j)}{1 + \\lambda_2}\n$$\n\nNext, we obtain the solution for the LASSO estimator. The LASSO is a special case of the elastic net with $\\lambda_2 = 0$. Substituting $\\lambda_2 = 0$ into the elastic net solution gives the coordinate-wise LASSO solution:\n$$\n\\hat{\\beta}_{j}^{\\text{lasso}} = \\frac{S_{\\lambda_1}(z_j)}{1 + 0} = S_{\\lambda_1}(z_j) = \\text{sgn}(z_j)\\max(0, |z_j|-\\lambda_1)\n$$\n\nFinally, we find the solution for the ridge regression estimator. Ridge regression corresponds to the elastic net with $\\lambda_1 = 0$. Substituting $\\lambda_1 = 0$ into the elastic net solution:\nThe condition $z_j > \\lambda_1$ becomes $z_j > 0$. The condition $z_j < -\\lambda_1$ becomes $z_j < 0$. The condition $|z_j| \\le \\lambda_1$ becomes $z_j=0$.\nSo, if $z_j > 0$, $\\hat{\\beta}_j = \\frac{z_j - 0}{1 + \\lambda_2} = \\frac{z_j}{1 + \\lambda_2}$.\nIf $z_j < 0$, $\\hat{\\beta}_j = \\frac{z_j + 0}{1 + \\lambda_2} = \\frac{z_j}{1 + \\lambda_2}$.\nIf $z_j = 0$, $\\hat{\\beta}_j = 0$, which is also given by the formula $\\frac{z_j}{1 + \\lambda_2}$.\nThus, the coordinate-wise ridge regression solution is:\n$$\n\\hat{\\beta}_{j}^{\\text{ridge}} = \\frac{z_j}{1 + \\lambda_2}\n$$\nThis can also be derived directly by minimizing the differentiable objective $J_{\\text{ridge}}(\\beta_j) = \\frac{1}{2}(z_j - \\beta_j)^2 + \\frac{\\lambda_2}{2}\\beta_j^2$. Setting the derivative to $0$ yields $\\beta_j(1 + \\lambda_2) - z_j = 0$, giving the same result.\n\nNow we compute the numerical estimates for a single coordinate $j$ with the given values: $z_j = 2.7$, $\\lambda_1 = 1.5$, and $\\lambda_2 = 0.5$.\n\nFor the elastic net estimate, we first check the threshold condition: $|z_j| = 2.7 > 1.5 = \\lambda_1$. Since $z_j > \\lambda_1$, we use the first case:\n$$\n\\hat{\\beta}_{j}^{\\text{enet}} = \\frac{z_j - \\lambda_1}{1 + \\lambda_2} = \\frac{2.7 - 1.5}{1 + 0.5} = \\frac{1.2}{1.5} = \\frac{12}{15} = \\frac{4}{5} = 0.8\n$$\n\nFor the LASSO estimate, we use the soft-thresholding function:\n$$\n\\hat{\\beta}_{j}^{\\text{lasso}} = S_{\\lambda_1}(z_j) = S_{1.5}(2.7) = \\text{sgn}(2.7)\\max(0, |2.7|-1.5) = 1 \\cdot (2.7 - 1.5) = 1.2\n$$\n\nFor the ridge regression estimate, we use its specific formula:\n$$\n\\hat{\\beta}_{j}^{\\text{ridge}} = \\frac{z_j}{1 + \\lambda_2} = \\frac{2.7}{1 + 0.5} = \\frac{2.7}{1.5} = \\frac{27}{15} = \\frac{9}{5} = 1.8\n$$\n\nThe three estimates for the coordinate $\\beta_j$ are $0.8$ for the elastic net, $1.2$ for LASSO, and $1.8$ for ridge regression.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.8 & 1.2 & 1.8 \\end{pmatrix}}\n$$", "id": "3487889"}, {"introduction": "While the orthonormal case provides key insights, real-world problems require a robust and efficient solver. This exercise guides you through the derivation of the coordinate descent algorithm, the workhorse for fitting elastic net models. By tackling the optimization one variable at a time and using clever residual caching, this method becomes highly scalable, and understanding its mechanics is crucial for any practitioner [@problem_id:3487939].", "problem": "Consider the elastic net regularization problem in the context of Compressed Sensing (CS) and sparse optimization. Let $X \\in \\mathbb{R}^{n \\times p}$ be a feature matrix with columns $x_{j} \\in \\mathbb{R}^{n}$ for $j \\in \\{1,\\dots,p\\}$ and let $y \\in \\mathbb{R}^{n}$ be a response vector. The elastic net objective is\n$$\nF(\\beta) \\;=\\; \\frac{1}{2n}\\,\\lVert y - X\\beta \\rVert_{2}^{2} \\;+\\; \\lambda_{1}\\,\\lVert \\beta \\rVert_{1} \\;+\\; \\frac{\\lambda_{2}}{2}\\,\\lVert \\beta \\rVert_{2}^{2},\n$$\nwhere $\\beta \\in \\mathbb{R}^{p}$, $\\lambda_{1} \\ge 0$, and $\\lambda_{2} \\ge 0$. Consider a cyclic coordinate descent algorithm that maintains the cached residual $r := y - X\\beta$ and the precomputed diagonal Gram entries $d_{j} := \\frac{1}{n}\\lVert x_{j} \\rVert_{2}^{2}$ for all $j \\in \\{1,\\dots,p\\}$. For a given coordinate $j$, define the partial residual $r^{(j)} := r + x_{j}\\beta_{j}$, and let $s_{j} := \\mathrm{nnz}(x_{j})$ denote the number of nonzero entries in the column $x_{j}$.\n\nStarting from the fundamental optimality conditions of convex functions and subgradient calculus for the $\\ell_{1}$ norm, derive an exact closed-form coordinate update for $\\beta_{j}$ expressed purely in terms of $x_{j}$, $r$, $d_{j}$, $\\lambda_{1}$, and $\\lambda_{2}$, together with the residual update that preserves $r = y - X\\beta$ after the change in $\\beta_{j}$. Then, analyze the per-iteration computational complexity in the extreme regime $p \\gg n$ with a sparse matrix $X$, assuming you reuse $r$ and $d_{j}$ and perform the residual update after each coordinate change. Express the cost of one full pass across all $p$ coordinates in terms of $\\{s_{j}\\}_{j=1}^{p}$ using asymptotic notation.\n\nYour final answer must be a two-entry row matrix whose first entry is the exact coordinate update for $\\beta_{j}$ and whose second entry is the asymptotic complexity per full pass across all $p$ coordinates, both as closed-form expressions. No numerical evaluation is required.", "solution": "We begin from the elastic net objective\n$$\nF(\\beta) \\;=\\; \\frac{1}{2n}\\,\\lVert y - X\\beta \\rVert_{2}^{2} \\;+\\; \\lambda_{1}\\,\\lVert \\beta \\rVert_{1} \\;+\\; \\frac{\\lambda_{2}}{2}\\,\\lVert \\beta \\rVert_{2}^{2}.\n$$\nFix all coordinates except $j$ and consider the one-dimensional subproblem in the variable $t \\in \\mathbb{R}$ that replaces $\\beta_{j}$. Let the current iterate be $\\beta$, the residual be $r := y - X\\beta$, and the partial residual be $r^{(j)} := r + x_{j}\\beta_{j} = y - \\sum_{k \\ne j} x_{k}\\beta_{k}$. The restricted objective in $t$ is\n$$\n\\phi_{j}(t)\n\\;=\\;\n\\frac{1}{2n}\\,\\lVert r^{(j)} - x_{j} t \\rVert_{2}^{2}\n\\;+\\;\n\\lambda_{1}\\,|t|\n\\;+\\;\n\\frac{\\lambda_{2}}{2}\\,t^{2}\n\\;+\\; \\text{const},\n$$\nwhere \"const\" does not depend on $t$. Expanding the quadratic term and collecting coefficients yields\n$$\n\\phi_{j}(t)\n\\;=\\;\n\\frac{1}{2}\\,(d_{j} + \\lambda_{2})\\,t^{2}\n\\;-\\;\n\\left(\\frac{1}{n}\\,x_{j}^{\\top} r^{(j)}\\right) t\n\\;+\\;\n\\lambda_{1}\\,|t|\n\\;+\\; \\text{const},\n$$\nwhere $d_{j} := \\frac{1}{n}\\lVert x_{j} \\rVert_{2}^{2}$. This is a strictly convex one-dimensional problem comprising a quadratic term plus an $\\ell_{1}$ penalty. The subgradient optimality condition for a minimizer $t^{\\star}$ is\n$$\n0 \\;\\in\\; (d_{j} + \\lambda_{2})\\,t^{\\star} \\;-\\; \\left(\\frac{1}{n}\\,x_{j}^{\\top} r^{(j)}\\right) \\;+\\; \\lambda_{1}\\,\\partial|t^{\\star}|,\n$$\nwhere $\\partial|t|$ denotes the subdifferential of the absolute value at $t$. The solution is characterized by soft-thresholding:\n$$\nt^{\\star} \\;=\\; \\frac{1}{d_{j} + \\lambda_{2}} \\; S\\!\\left(\\frac{1}{n}\\,x_{j}^{\\top} r^{(j)},\\, \\lambda_{1}\\right),\n$$\nwhere the soft-thresholding operator $S(a,\\tau)$ is defined by $S(a,\\tau) := \\mathrm{sign}(a)\\,\\max\\{|a| - \\tau,\\, 0\\}$.\n\nTo express the update in terms of the cached residual $r$ and $d_{j}$, note that\n$$\nx_{j}^{\\top} r^{(j)} \\;=\\; x_{j}^{\\top}(r + x_{j}\\beta_{j}) \\;=\\; x_{j}^{\\top} r \\;+\\; \\lVert x_{j} \\rVert_{2}^{2}\\,\\beta_{j},\n$$\nand therefore\n$$\n\\frac{1}{n}\\,x_{j}^{\\top} r^{(j)} \\;=\\; \\frac{1}{n}\\,x_{j}^{\\top} r \\;+\\; d_{j}\\,\\beta_{j}.\n$$\nSubstituting this into the soft-thresholding expression gives the exact coordinate update\n$$\n\\beta_{j}^{\\mathrm{new}}\n\\;=\\;\n\\frac{1}{d_{j} + \\lambda_{2}}\n\\; S\\!\\left(\\frac{1}{n}\\,x_{j}^{\\top} r \\;+\\; d_{j}\\,\\beta_{j},\\, \\lambda_{1}\\right).\n$$\nAfter computing $\\beta_{j}^{\\mathrm{new}}$, the cached residual is updated to maintain $r = y - X\\beta$ via\n$$\nr \\;\\leftarrow\\; r \\;-\\; x_{j}\\,\\big(\\beta_{j}^{\\mathrm{new}} - \\beta_{j}^{\\mathrm{old}}\\big).\n$$\n\nWe now analyze computational complexity in the regime $p \\gg n$ with sparse $X$. Let $s_{j} := \\mathrm{nnz}(x_{j})$ be the number of nonzeros in the $j$-th column. The operations per coordinate update are:\n- Computing $\\frac{1}{n}\\,x_{j}^{\\top} r$: this takes $\\mathcal{O}(s_{j})$ time because only the nonzero entries of $x_{j}$ contribute.\n- Forming $\\frac{1}{n}\\,x_{j}^{\\top} r + d_{j}\\,\\beta_{j}$: this takes $\\mathcal{O}(1)$ time since $d_{j}$ is precomputed.\n- Applying soft-thresholding and scaling by $(d_{j} + \\lambda_{2})^{-1}$: this is $\\mathcal{O}(1)$.\n- Updating the residual $r \\leftarrow r - x_{j}\\,(\\beta_{j}^{\\mathrm{new}} - \\beta_{j}^{\\mathrm{old}})$: this takes $\\mathcal{O}(s_{j})$ time because only positions where $x_{j}$ is nonzero must be changed.\n\nThus, the total cost per coordinate is $\\mathcal{O}(s_{j})$, and one full pass across all $p$ coordinates has cost\n$$\n\\mathcal{O}\\!\\left(\\sum_{j=1}^{p} s_{j}\\right),\n$$\nwhich is $\\mathcal{O}(\\mathrm{nnz}(X))$ where $\\mathrm{nnz}(X)$ denotes the total number of nonzeros in $X$. In the extreme $p \\gg n$ regime with sparse columns, $s_{j} \\le n$ while $\\sum_{j=1}^{p} s_{j}$ captures the aggregate sparsity; reusing the cached residual and precomputed $d_{j}$ ensures the per-pass complexity scales with the sparsity rather than with $np$.", "answer": "$$\\boxed{\\begin{pmatrix}\n\\dfrac{1}{d_{j} + \\lambda_{2}}\\, S\\!\\left(\\dfrac{1}{n}\\,x_{j}^{\\top} r + d_{j}\\,\\beta_{j},\\, \\lambda_{1}\\right) & \\mathcal{O}\\!\\left(\\displaystyle\\sum_{j=1}^{p} s_{j}\\right)\n\\end{pmatrix}}$$", "id": "3487939"}, {"introduction": "A theoretically sound algorithm is only as good as its numerical implementation. This final practice delves into the critical, real-world challenges of implementing coordinate descent with finite-precision floating-point arithmetic. You will analyze how issues like catastrophic cancellation and varying data scales can impact the accuracy of the updates and evaluate standard safeguarding techniques that ensure the solver is both robust and reliable [@problem_id:3487930].", "problem": "Consider the elastic net problem in compressed sensing and sparse optimization: minimize over $\\,\\beta \\in \\mathbb{R}^p\\,$ the objective \n$$\nF(\\beta) \\;=\\; \\frac{1}{2n}\\,\\lVert y - X\\beta \\rVert_2^2 \\;+\\; \\lambda_1 \\lVert \\beta \\rVert_1 \\;+\\; \\frac{\\lambda_2}{2}\\,\\lVert \\beta \\rVert_2^2,\n$$\nwhere $\\,X \\in \\mathbb{R}^{n \\times p}\\,$, $\\,y \\in \\mathbb{R}^n\\,$, and $\\,\\lambda_1, \\lambda_2 \\ge 0\\,$. A common solver uses cyclic coordinate descent with residual maintenance. Fix an index $\\,j \\in \\{1,\\dots,p\\}\\,$. Let $\\,X_j \\in \\mathbb{R}^n\\,$ denote the $\\,j$-th column of $\\,X\\,$, and define the partial residual $\\,r \\,=\\, y - X\\beta + \\beta_j X_j\\,$ so that the coordinate-wise subproblem (as a function of a scalar $\\,b\\,$ replacing $\\,\\beta_j\\,$) is\n$$\ng(b) \\;=\\; \\frac{1}{2n}\\,\\lVert r - b X_j \\rVert_2^2 \\;+\\; \\lambda_1 |b| \\;+\\; \\frac{\\lambda_2}{2}\\,b^2.\n$$\nFrom first principles (subgradient optimality and the derivative of a quadratic), the exact minimizer is given by the soft-threshold and ridge-shrink update\n$$\n\\beta_j^{\\text{new}} \\;=\\; \\frac{S\\!\\left( \\frac{1}{n} X_j^\\top r \\,,\\, \\lambda_1 \\right)}{\\frac{1}{n}\\,\\lVert X_j \\rVert_2^2 \\,+\\, \\lambda_2},\n\\qquad\\text{where}\\qquad\nS(z,\\tau) \\,=\\, \\operatorname{sign}(z)\\,\\max\\{ |z| - \\tau,\\, 0 \\}.\n$$\nIn high-dimensional regimes with ill-conditioned feature scales, practical implementations must confront finite precision arithmetic as specified by Institute of Electrical and Electronics Engineers (IEEE) $\\,754\\,$ floating-point, where the unit roundoff (machine epsilon) is approximately $\\,\\epsilon_{\\text{mach}} \\approx 2^{-53} \\approx 1.11 \\times 10^{-16}\\,$, the largest representable finite number is on the order of $\\,1.79 \\times 10^{308}\\,$, and the smallest positive normalized number is on the order of $\\,2.22 \\times 10^{-308}\\,$ (with subnormals smaller but less accurate). Suppose $\\,X\\,$ has columns with widely varying norms, for example some $\\,\\lVert X_j \\rVert_2 \\ll 1\\,$ and others $\\,\\lVert X_j \\rVert_2 \\gg 1\\,$, and $\\,n\\,$ is large so that dot products $\\,X_j^\\top r\\,$ may suffer overflow, underflow, or catastrophic cancellation when $\\,|X_j^\\top r|\\,$ and $\\,\\lambda_1\\,$ are both very large and nearly equal.\n\nSelect all statements that are correct about the impact of finite precision and scaling on the soft-thresholding and ridge shrinkage in coordinate descent, and about effective safeguards to prevent numerical underflow or overflow, while preserving the correctness of the computed solution up to floating-point rounding.\n\nChoices:\n\nA. Standardizing the features by centering and scaling each $\\,X_j\\,$ to have $\\,\\lVert X_j \\rVert_2 = \\sqrt{n}\\,$ and centering $\\,y\\,$ reduces dynamic range in the dot products and denominators; one can then use the precomputed diagonal $\\,v_j = \\frac{1}{n}\\lVert X_j \\rVert_2^2 + \\lambda_2\\,$ and guard it below by a small positive bound $\\,\\delta\\,$, i.e., use $\\,\\max\\{v_j,\\delta\\}\\,$ in the denominator to avoid division by a denormalized or zero value, and rescale the coefficients back to the original units after optimization. This improves numerical stability without changing the optimizer in exact arithmetic when $\\,v_j \\ge \\delta\\,$.\n\nB. When $\\,|z| \\approx \\lambda_1 \\gg 1\\,$ with $\\,z = \\frac{1}{n}X_j^\\top r\\,$, directly forming $\\,|z| - \\lambda_1\\,$ can lose all significant digits due to cancellation; branching first on the comparison $\\,|z| \\le \\lambda_1\\,$ and returning $\\,0\\,$ in that case, and only computing $\\,|z| - \\lambda_1\\,$ in the complementary branch, reduces cancellation risk and prevents an incorrect nonzero sign flip in the soft-threshold $\\,S(z,\\lambda_1)\\,$.\n\nC. Accumulating $\\,X_j^\\top r\\,$ and $\\,\\lVert X_j \\rVert_2^2\\,$ using compensated summation (for example, Kahan summation) or pairwise summation reduces roundoff error in the presence of large $\\,n\\,$ and heterogeneous magnitudes in $\\,X_j\\,$ and $\\,r\\,$, thereby stabilizing both the soft-threshold input $\\,z\\,$ and the ridge denominator.\n\nD. To avoid underflow and overflow in the soft-threshold, it is safe and accuracy-preserving to replace $\\,S(z,\\lambda_1)\\,$ by the rational shrinkage $\\,z/(1+\\lambda_1)\\,$, which avoids subtracting nearly equal numbers while retaining sparsity and the same minimizer as the elastic net.\n\nE. To prevent overflow or underflow in forming $\\,X_j^\\top r\\,$ when entries of $\\,X_j\\,$ or $\\,r\\,$ span extreme scales, one may compute a rescaled dot product by factoring out $\\,\\alpha = \\lVert X_j \\rVert_\\infty\\,$ and $\\,\\beta = \\lVert r \\rVert_\\infty\\,$, forming $\\,d = (X_j/\\alpha)^\\top (r/\\beta)\\,$ in floating-point (where $\\,|d| \\le n\\,$), and then returning $\\,\\alpha \\beta d\\,$; this algebraically equivalent rescaling avoids overflow in intermediate products and underflow in partial sums.\n\nF. Because a large $\\,\\lambda_2\\,$ can make the denominator very large, it is numerically safer and theoretically equivalent to drop the $\\,\\lambda_2\\,$ term from the per-coordinate denominator during iterations and instead apply a single global shrinkage by $\\,1/(1+\\lambda_2)\\,$ to all coordinates at the end; this preserves the elastic net solution while avoiding per-iteration divisions by large numbers.\n\nSelect all that apply and justify your choices from first principles, including the coordinate descent optimality condition and floating-point error considerations. Your safeguards must be consistent with the elastic net objective and not change the optimizer in exact arithmetic, except possibly through explicit, documented regularization when protecting against pathological scales.", "solution": "The coordinate-wise minimizer for $\\beta_j$ is found by solving for the root of the subgradient of the objective with respect to $\\beta_j$. The objective restricted to the $j$-th coordinate, considering other coefficients fixed, is:\n$$\nf(\\beta_j) = \\frac{1}{2n} \\lVert y - \\sum_{k \\neq j} X_k \\beta_k - X_j \\beta_j \\rVert_2^2 + \\lambda_1 \\sum_{k \\neq j}|\\beta_k| + \\lambda_1 |\\beta_j| + \\frac{\\lambda_2}{2} \\sum_{k \\neq j} \\beta_k^2 + \\frac{\\lambda_2}{2} \\beta_j^2\n$$\nLetting $r = y - \\sum_{k \\neq j} X_k \\beta_k$, the terms not depending on $\\beta_j$ are constant. We minimize:\n$$\ng(\\beta_j) = \\frac{1}{2n} \\lVert r - X_j \\beta_j \\rVert_2^2 + \\lambda_1 |\\beta_j| + \\frac{\\lambda_2}{2} \\beta_j^2\n$$\nExpanding the quadratic term gives:\n$$\ng(\\beta_j) = \\frac{1}{2n} (\\lVert r \\rVert_2^2 - 2 \\beta_j X_j^\\top r + \\beta_j^2 \\lVert X_j \\rVert_2^2) + \\lambda_1 |\\beta_j| + \\frac{\\lambda_2}{2} \\beta_j^2\n$$\nThe subgradient with respect to $\\beta_j$ is:\n$$\n\\partial g(\\beta_j) = \\frac{1}{n} ( \\beta_j \\lVert X_j \\rVert_2^2 - X_j^\\top r ) + \\lambda_1 \\partial|\\beta_j| + \\lambda_2 \\beta_j\n$$\nSetting the subgradient to $0$ gives the optimality condition:\n$$\n\\beta_j (\\frac{1}{n} \\lVert X_j \\rVert_2^2 + \\lambda_2) - \\frac{1}{n} X_j^\\top r + \\lambda_1 s = 0, \\quad \\text{where } s \\in \\partial|\\beta_j|\n$$\nIf we let $z = \\frac{1}{n} X_j^\\top r$ and $d = \\frac{1}{n} \\lVert X_j \\rVert_2^2 + \\lambda_2$, this simplifies to $\\beta_j d - z + \\lambda_1 s = 0$. Solving for $\\beta_j$ based on the cases for the subgradient $s$ yields the soft-thresholding solution $\\beta_j = S(z, \\lambda_1)/d$, confirming the formula in the problem statement.\n\nWe now evaluate each option.\n\n**Option A Evaluation**\nThis option proposes a multi-part strategy: feature standardization, denominator guarding, and rescaling.\n1. **Standardization**: Scaling each column $X_j$ to have $\\lVert X_j \\rVert_2 = \\sqrt{n}$ implies that the term $\\frac{1}{n}\\lVert X_j \\rVert_2^2$ in the update's denominator becomes $\\frac{1}{n}(\\sqrt{n})^2 = 1$. This homogenizes the denominators across all features (to $1 + \\lambda_2$), preventing issues where some denominators are huge and others tiny due to feature scaling. This is a standard and highly effective technique for improving stability and convergence speed.\n2. **Denominator Guarding**: The denominator is $v_j = \\frac{1}{n}\\lVert X_j \\rVert_2^2 + \\lambda_2$. Since $\\lambda_2 \\ge 0$, $v_j$ is always non-negative. However, it can be zero if $X_j=0$ and $\\lambda_2=0$, or a very small (denormalized) number in floating point, leading to division by zero or overflow. Using $\\max\\{v_j, \\delta\\}$ for a small positive constant $\\delta$ is a robust way to prevent this, effectively regularizing pathologically-scaled features.\n3. **Rescaling**: If the optimization is performed on transformed data, the resulting coefficients must be transformed back to the original scale to be meaningful for the original problem. This is a standard and necessary step.\nThe statement correctly claims this improves stability without changing the optimizer in exact arithmetic, as long as the guard $v_j \\ge \\delta$ is not active. The entire procedure is a cornerstone of robust implementations of regularized models.\n**Verdict: Correct**\n\n**Option B Evaluation**\nThis option addresses catastrophic cancellation in computing $S(z, \\lambda_1)$. The computation involves $|z| - \\lambda_1$. When $|z|$ and $\\lambda_1$ are large and nearly equal, their difference loses most of its significant digits. For example, if $|z| = 1.2345678901234567 \\times 10^{20}$ and $\\lambda_1 = 1.2345678901234560 \\times 10^{20}$, their difference is $7 \\times 10^3$, but a standard floating-point subtraction might retain only one or two correct digits. The proposed solution is to first perform the comparison $|z| \\le \\lambda_1$. This comparison is robust in floating point. If it is true, the result is exactly $0$. The subtraction $|z| - \\lambda_1$ is only performed if $|z| > \\lambda_1$, in which case the result is known to be positive. This branching avoids performing the subtraction when it would lead to a loss of precision that could even flip the sign of the result from negative (or zero) to a spurious positive value, thereby incorrectly yielding a non-zero update when it should be zero. This is a standard procedure in numerically-aware coding.\n**Verdict: Correct**\n\n**Option C Evaluation**\nThis option suggests using compensated or pairwise summation for computing the dot product $X_j^\\top r = \\sum_{i=1}^n (X_j)_i r_i$ and the squared norm $\\lVert X_j \\rVert_2^2 = \\sum_{i=1}^n (X_j)_i^2$. When $n$ is large, a naive loop `sum = sum + term[i]` can accumulate significant roundoff error, especially if the terms have a wide range of magnitudes or if the running sum grows much larger than the terms being added. Compensated summation (like Kahan's algorithm) maintains a correction term that accounts for the low-order bits lost in each addition, dramatically reducing the total error. Pairwise summation adds elements in a recursive tree structure, which tends to add numbers of similar magnitude, also reducing error. Both are standard, effective techniques in numerical linear algebra to increase the accuracy of summations. Applying them here would yield more accurate values for the numerator argument $z$ and the denominator term $\\lVert X_j \\rVert_2^2$, thus stabilizing the entire update.\n**Verdict: Correct**\n\n**Option D Evaluation**\nThis option proposes replacing the soft-thresholding function $S(z, \\lambda_1)$ with a rational function $z/(1+\\lambda_1)$. This proposal is fundamentally flawed. The minimizer of the elastic net objective is rigorously derived to be the soft-thresholding operator. The function $S(z, \\lambda_1)$ is non-linear and, critically, sets the result to exactly $0$ for $|z| \\le \\lambda_1$, which is how the $L_1$ penalty induces sparsity. The proposed function $z/(1+\\lambda_1)$ is a simple linear scaling of $z$. It is only zero if $z=0$ and does not induce sparsity. It corresponds to the minimizer of a different objective function, not the elastic net. The claim that it \"retains sparsity and the same minimizer\" is false.\n**Verdict: Incorrect**\n\n**Option E Evaluation**\nThis option presents a method for robustly computing the dot product $X_j^\\top r$. The method is to factor out the infinity norms of the vectors, compute the dot product of the normalized vectors, and then rescale the result. Algebraically, this is an identity: $\\alpha \\beta d = \\lVert X_j \\rVert_\\infty \\lVert r \\rVert_\\infty \\left( \\left(\\frac{X_j}{\\lVert X_j \\rVert_\\infty}\\right)^\\top \\left(\\frac{r}{\\lVert r \\rVert_\\infty}\\right) \\right) = X_j^\\top r$. Numerically, this is a powerful technique. Let $X'_j = X_j/\\alpha$ and $r' = r/\\beta$. All elements of $X'_j$ and $r'$ have magnitude at most $1$. When computing the sum of products $\\sum_i (X'_j)_i (r')_i$, the intermediate products $(X'_j)_i (r')_i$ are also bounded by $1$, which prevents overflow on intermediate steps that could occur if some $(X_j)_i$ and $r_i$ were both large. It also mitigates underflow issues during summation by scaling up small vector entries. This is a classic method for accurate dot product computation, often used in robust numerical libraries.\n**Verdict: Correct**\n\n**Option F Evaluation**\nThis option suggests modifying the algorithm by removing the $\\lambda_2$ term from the per-coordinate denominator and applying a \"global shrinkage\" factor of $1/(1+\\lambda_2)$ at the end. The proposed per-iteration update would be for a pure LASSO problem ($\\lambda_2 = 0$). The claim is that this is \"theoretically equivalent\". This is false. The correct update is $\\beta_j^{\\text{new}} \\propto S(z, \\lambda_1)$, with a denominator of $(\\frac{1}{n}\\lVert X_j \\rVert_2^2 + \\lambda_2)$. The proposed final update would be $\\beta_j^{\\text{final}} \\propto S(z, \\lambda_1) / (\\frac{1}{n}\\lVert X_j \\rVert_2^2 \\cdot (1+\\lambda_2))$. The two denominators are not equal:\n$$\n\\frac{1}{n}\\lVert X_j \\rVert_2^2 + \\lambda_2 \\neq \\left(\\frac{1}{n}\\lVert X_j \\rVert_2^2\\right) (1+\\lambda_2) = \\frac{1}{n}\\lVert X_j \\rVert_2^2 + \\lambda_2 \\frac{1}{n}\\lVert X_j \\rVert_2^2\n$$\nunless $\\frac{1}{n}\\lVert X_j \\rVert_2^2=1$ or $\\lambda_2=0$. The true elastic net shrinkage depends on the specific norm of each feature column $X_j$. The proposed method applies a uniform post-hoc shrinkage, which does not solve the original elastic net problem. It is a different, incorrect algorithm.\n**Verdict: Incorrect**", "answer": "$$\\boxed{ABCE}$$", "id": "3487930"}]}