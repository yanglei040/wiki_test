{"hands_on_practices": [{"introduction": "The journey to mastering Gordon's theorem begins with a firm grasp of its central quantity: the Gaussian width. This exercise provides a foundational, hands-on calculation of the Gaussian width for one of the most important sets in compressed sensing—the set of sparse vectors. By working from first principles, you will see how the abstract definition of width translates into a concrete analytical expression involving the Gamma function, a result that depends only on the sparsity level of the signal [@problem_id:3448590].", "problem": "Consider a measurement model in compressed sensing where random linear maps are analyzed via Gordon’s escape through a mesh theorem. Let $n \\in \\mathbb{N}$ and fix an index set $S \\subset \\{1,2,\\dots,n\\}$ with $|S| = s$. Define the subset $T \\subset \\mathbb{R}^{n}$ to be the collection of $s$-sparse vectors with support contained in $S$ and bounded Euclidean norm:\n$$\nT \\triangleq \\left\\{ x \\in \\mathbb{R}^{n} : \\operatorname{supp}(x) \\subseteq S,\\ \\|x\\|_{2} \\leq 1 \\right\\}.\n$$\nLet $g \\in \\mathbb{R}^{n}$ be a standard Gaussian vector whose entries are independent and identically distributed as $\\mathcal{N}(0,1)$, and recall the definition of the Gaussian width of a set $T$:\n$$\nw(T) \\triangleq \\mathbb{E}\\left[\\,\\sup_{x \\in T} \\langle g, x \\rangle\\,\\right],\n$$\nwhere $\\langle \\cdot, \\cdot \\rangle$ denotes the standard Euclidean inner product. Starting from fundamental definitions and well-tested facts about Gaussian vectors and radial distributions, derive an exact analytical expression for $w(T)$ in terms of $s$ only. Express your final answer as a single closed-form symbolic expression. No numerical rounding is required, and no physical units are involved.", "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It is a standard calculation in high-dimensional probability and compressed sensing theory. We will now proceed with the derivation of the analytical expression for the Gaussian width $w(T)$.\n\nThe Gaussian width of the set $T$ is defined as:\n$$\nw(T) \\triangleq \\mathbb{E}\\left[\\,\\sup_{x \\in T} \\langle g, x \\rangle\\,\\right]\n$$\nwhere $T \\triangleq \\left\\{ x \\in \\mathbb{R}^{n} : \\operatorname{supp}(x) \\subseteq S,\\ \\|x\\|_{2} \\leq 1 \\right\\}$, the set $S$ has cardinality $|S| = s$, and $g \\in \\mathbb{R}^{n}$ is a standard Gaussian vector with i.i.d. components $g_i \\sim \\mathcal{N}(0, 1)$.\n\nOur first step is to simplify the term $\\sup_{x \\in T} \\langle g, x \\rangle$ for a fixed realization of $g$. The condition $\\operatorname{supp}(x) \\subseteq S$ implies that the components $x_i$ of any vector $x \\in T$ are zero for any index $i \\notin S$. The inner product $\\langle g, x \\rangle$ can thus be written as a sum over the indices in $S$:\n$$\n\\langle g, x \\rangle = \\sum_{i=1}^{n} g_i x_i = \\sum_{i \\in S} g_i x_i.\n$$\nLet us define $g_S \\in \\mathbb{R}^{s}$ as the vector composed of the entries of $g$ whose indices are in $S$, and similarly, let $x_S \\in \\mathbb{R}^{s}$ be the vector of the corresponding entries of $x$. The inner product is then $\\langle g, x \\rangle = \\langle g_S, x_S \\rangle_{\\mathbb{R}^s}$. The constraint $\\|x\\|_{2} \\leq 1$ becomes $\\|x_S\\|_{2} \\leq 1$, since all other components of $x$ are zero.\n\nThe optimization problem inside the expectation is therefore:\n$$\n\\sup_{x \\in T} \\langle g, x \\rangle = \\sup_{x_S \\in \\mathbb{R}^s, \\|x_S\\|_2 \\leq 1} \\langle g_S, x_S \\rangle.\n$$\nThis expression is the definition of the dual norm of $g_S$. For the $\\ell_2$-norm, the dual norm is the $\\ell_2$-norm itself. By the Cauchy-Schwarz inequality, $\\langle g_S, x_S \\rangle \\leq \\|g_S\\|_2 \\|x_S\\|_2$. Given that $\\|x_S\\|_2 \\leq 1$, we have $\\langle g_S, x_S \\rangle \\leq \\|g_S\\|_2$. This maximum value is attained when $x_S$ is aligned with $g_S$ and has the maximum possible norm, i.e., $x_S = g_S / \\|g_S\\|_2$ (for $g_S \\neq 0$). Thus, we have:\n$$\n\\sup_{x \\in T} \\langle g, x \\rangle = \\|g_S\\|_2.\n$$\nNow, we can rewrite the Gaussian width as the expectation of this norm:\n$$\nw(T) = \\mathbb{E}\\left[ \\|g_S\\|_2 \\right].\n$$\nThe vector $g_S$ is a sub-vector of $g$ containing $s$ components, each being an independent standard normal random variable. Therefore, $g_S$ is a standard Gaussian random vector in $\\mathbb{R}^s$. The random variable we are interested in is the Euclidean norm of $g_S$.\n\nLet the random variable $Z$ be the squared norm of $g_S$:\n$$\nZ = \\|g_S\\|_2^2 = \\sum_{i \\in S} g_i^2.\n$$\nSince $Z$ is the sum of the squares of $s$ independent standard normal random variables, it follows a chi-squared distribution with $s$ degrees of freedom, denoted as $Z \\sim \\chi^2(s)$.\n\nThe probability density function (PDF) of $Z$ for $z  0$ is given by:\n$$\nf_Z(z) = \\frac{1}{2^{s/2} \\Gamma(s/2)} z^{s/2 - 1} \\exp\\left(-\\frac{z}{2}\\right),\n$$\nwhere $\\Gamma(\\cdot)$ is the Gamma function.\n\nWe need to compute the expectation of $\\|g_S\\|_2 = \\sqrt{Z}$:\n$$\nw(T) = \\mathbb{E}[\\sqrt{Z}] = \\int_{0}^{\\infty} \\sqrt{z} f_Z(z) \\, dz.\n$$\nSubstituting the PDF of the $\\chi^2(s)$ distribution into the integral:\n$$\nw(T) = \\int_{0}^{\\infty} z^{1/2} \\left( \\frac{1}{2^{s/2} \\Gamma(s/2)} z^{s/2 - 1} \\exp\\left(-\\frac{z}{2}\\right) \\right) \\, dz.\n$$\nWe can factor out the constants and combine the powers of $z$:\n$$\nw(T) = \\frac{1}{2^{s/2} \\Gamma(s/2)} \\int_{0}^{\\infty} z^{1/2 + s/2 - 1} \\exp\\left(-\\frac{z}{2}\\right) \\, dz = \\frac{1}{2^{s/2} \\Gamma(s/2)} \\int_{0}^{\\infty} z^{(s+1)/2 - 1} \\exp\\left(-\\frac{z}{2}\\right) \\, dz.\n$$\nThe integral is a standard form related to the Gamma function. For a shape parameter $k  0$ and a scale parameter $\\theta  0$, the following identity holds:\n$$\n\\int_{0}^{\\infty} t^{k-1} \\exp\\left(-\\frac{t}{\\theta}\\right) \\, dt = \\theta^k \\Gamma(k).\n$$\nIn our case, the variable of integration is $z$, the shape parameter is $k = \\frac{s+1}{2}$, and the scale parameter is $\\theta=2$. Applying this identity, the integral evaluates to:\n$$\n\\int_{0}^{\\infty} z^{(s+1)/2 - 1} \\exp\\left(-\\frac{z}{2}\\right) \\, dz = 2^{(s+1)/2} \\Gamma\\left(\\frac{s+1}{2}\\right).\n$$\nSubstituting this back into our expression for $w(T)$:\n$$\nw(T) = \\frac{1}{2^{s/2} \\Gamma(s/2)} \\left( 2^{(s+1)/2} \\Gamma\\left(\\frac{s+1}{2}\\right) \\right).\n$$\nFinally, we simplify the expression by combining the powers of $2$:\n$$\nw(T) = \\frac{2^{(s+1)/2}}{2^{s/2}} \\frac{\\Gamma\\left(\\frac{s+1}{2}\\right)}{\\Gamma\\left(\\frac{s}{2}\\right)} = 2^{((s+1)/2) - (s/2)} \\frac{\\Gamma\\left(\\frac{s+1}{2}\\right)}{\\Gamma\\left(\\frac{s}{2}\\right)} = 2^{1/2} \\frac{\\Gamma\\left(\\frac{s+1}{2}\\right)}{\\Gamma\\left(\\frac{s}{2}\\right)}.\n$$\nThis gives the exact analytical expression for the Gaussian width of the set $T$:\n$$\nw(T) = \\sqrt{2} \\frac{\\Gamma\\left(\\frac{s+1}{2}\\right)}{\\Gamma\\left(\\frac{s}{2}\\right)}.\n$$\nThe result depends only on the sparsity level $s$, as required. This is the mean of the chi distribution with $s$ degrees of freedom.", "answer": "$$\\boxed{\\sqrt{2} \\frac{\\Gamma\\left(\\frac{s+1}{2}\\right)}{\\Gamma\\left(\\frac{s}{2}\\right)}}$$", "id": "3448590"}, {"introduction": "This practice elevates our analysis from simple sets to the geometric objects that govern the success of convex optimization: descent cones. Here, you will bridge the gap between abstract conic geometry and tangible algorithmic parameters. You will use the principle of polarity to relate the descent cone of the $\\ell_1$ norm to its subdifferential, leading to a computable formula for its statistical dimension and revealing a deep connection to the optimal thresholding parameter used in advanced algorithms like Approximate Message Passing (AMP) [@problem_id:3448604].", "problem": "Let $x^{\\star} \\in \\mathbb{R}^{n}$ be a fixed $s$-sparse vector with support $S \\subset \\{1,\\dots,n\\}$ of size $|S| = s$, and suppose its nonzero entries have known signs $v_{i} = \\operatorname{sign}(x^{\\star}_{i})$ for $i \\in S$. Consider the descent cone $D$ of the $\\ell_{1}$ norm at $x^{\\star}$, defined by $D := \\{d \\in \\mathbb{R}^{n} : \\exists t  0 \\text{ with } \\|x^{\\star} + t d\\|_{1} \\le \\|x^{\\star}\\|_{1}\\}$. Let $g \\sim \\mathcal{N}(0, I_{n})$ be a standard Gaussian vector in $\\mathbb{R}^{n}$, and recall that the Gaussian width $w(D \\cap S^{n-1})$ of the intersection of the cone $D$ with the unit sphere $S^{n-1}$ is a central quantity in Gordon’s escape through a mesh theorem, which provides probabilistic guarantees for exact recovery in compressed sensing.\n\nStarting from:\n- The polarity relationship between a descent cone and the conic hull of the subdifferential,\n- The definition of the statistical dimension of a closed convex cone in terms of the expected squared distance of a standard Gaussian vector to the polar cone,\n- The characterization of the subdifferential of the $\\ell_{1}$ norm at $x^{\\star}$,\n\nderive an explicit optimization formula over a scalar $ \\tau  0$ for the statistical dimension of the descent cone $D$ at $x^{\\star}$, expressed as an expectation of a squared Euclidean distance of the form $\\mathbb{E}\\big[\\operatorname{dist}^{2}(g, \\tau \\, \\partial\\|x^{\\star}\\|_{1})\\big]$. Then, by analytical differentiation, obtain the first-order optimality condition that determines the minimizer $\\tau^{\\star}$. Finally, for the specific numerical instance $n = 1000$ and $s = 50$, solve this optimality condition for $\\tau^{\\star}$ and provide its value.\n\nIn your derivation, explicitly carry out the integral calculations needed to evaluate expectations involving the standard Gaussian density and its tail function. Briefly explain how the optimizer $\\tau^{\\star}$ connects to the data-driven threshold selection that emerges in Approximate Message Passing (AMP) for sparse recovery via soft-thresholding.\n\nRound your final numerical value of $\\tau^{\\star}$ to four significant figures. No physical units are involved.", "solution": "The problem requires the derivation of an optimization formula for the statistical dimension of a descent cone in the context of sparse recovery, solving for the optimizer, and explaining its connection to Approximate Message Passing (AMP) algorithms.\n\n### Step 1: Formulation of the Optimization Problem\n\nThe problem asks for an optimization formula for the statistical dimension, $\\delta(D)$, of the descent cone $D$ of the $\\ell_1$ norm at an $s$-sparse vector $x^\\star$. The descent cone is given by $D = \\{d \\in \\mathbb{R}^n : \\sup_{u \\in \\partial\\|x^\\star\\|_1} \\langle u, d \\rangle \\le 0\\}$, which means $D$ is the polar cone of the subdifferential set $\\partial\\|x^\\star\\|_1$. By the bipolar theorem for cones, the polar of $D$ is the closed conic hull of the subdifferential: $D^\\circ = \\overline{\\text{cone}}(\\partial\\|x^\\star\\|_1)$.\n\nThe statistical dimension of $D$ is defined as $\\delta(D) = \\mathbb{E}_g[\\text{dist}^2(g, D^\\circ)]$, where $g \\sim \\mathcal{N}(0, I_n)$.\nA key result in the analysis of compressed sensing, emerging from Gordon's theorem and its application to convex optimization, establishes an identity connecting this geometric quantity to an optimization problem involving the subdifferential itself. The identity states that the statistical dimension is the minimum value of a specific objective function $L(\\tau)$:\n$$\n\\delta(D) = \\min_{\\tau  0} L(\\tau) = \\min_{\\tau  0} \\mathbb{E}\\big[\\operatorname{dist}^{2}(g, \\tau \\, \\partial\\|x^{\\star}\\|_{1})\\big]\n$$\nWe proceed by deriving the explicit form of the objective function $L(\\tau)$.\n\nThe subdifferential of the $\\ell_1$ norm at the $s$-sparse vector $x^\\star$ with support $S$ and sign vector $v$ on $S$ is the set:\n$$\n\\partial\\|x^\\star\\|_1 = \\{u \\in \\mathbb{R}^n : u_i = v_i \\text{ for } i \\in S, \\text{ and } |u_i| \\le 1 \\text{ for } i \\in S^c\\}\n$$\nwhere $S^c = \\{1, \\dots, n\\} \\setminus S$. We denote this set by $C$. The set $\\tau C$ is $\\{\\tau u : u \\in C\\}$.\nThe squared distance from $g$ to $\\tau C$ is:\n$$\n\\operatorname{dist}^{2}(g, \\tau C) = \\min_{u \\in C} \\|g - \\tau u\\|_2^2\n$$\nThis minimization problem decouples over the support $S$ and its complement $S^c$:\n$$\n\\operatorname{dist}^{2}(g, \\tau C) = \\min_{u \\in C} \\left( \\sum_{i \\in S} (g_i - \\tau u_i)^2 + \\sum_{i \\in S^c} (g_i - \\tau u_i)^2 \\right)\n$$\nFor $i \\in S$, $u_i$ is fixed to $v_i$, so the first term is simply $\\sum_{i \\in S} (g_i - \\tau v_i)^2 = \\|g_S - \\tau v\\|_2^2$.\nFor $i \\in S^c$, we must minimize $(g_i - \\tau u_i)^2$ over $|u_i| \\le 1$. This is equivalent to finding the squared distance from $g_i$ to the interval $[-\\tau, \\tau]$. The projection of $g_i$ onto $[-\\tau, \\tau]$ is $\\text{sgn}(g_i)\\min(|g_i|, \\tau)$. The squared distance is:\n$$\n\\min_{|u_i| \\le 1} (g_i - \\tau u_i)^2 = \\operatorname{dist}^2(g_i, [-\\tau, \\tau]) = (|g_i| - \\tau)^2 \\mathbb{I}(|g_i|  \\tau)\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. This expression is also $\\max(0, |g_i|-\\tau)^2$.\n\nCombining these parts, the total squared distance is:\n$$\n\\operatorname{dist}^{2}(g, \\tau C) = \\sum_{i \\in S} (g_i - \\tau v_i)^2 + \\sum_{i \\in S^c} (|g_i| - \\tau)^2 \\mathbb{I}(|g_i|  \\tau)\n$$\nThe objective function $L(\\tau)$ is the expectation of this quantity. Since the components $g_i$ of $g$ are i.i.d. $\\mathcal{N}(0,1)$, we can compute the expectation of each term.\n\nFor $i \\in S$, since $v_i^2=1$:\n$$\n\\mathbb{E}[(g_i - \\tau v_i)^2] = \\mathbb{E}[g_i^2] - 2\\tau v_i \\mathbb{E}[g_i] + \\tau^2 v_i^2 = 1 - 0 + \\tau^2 = 1+\\tau^2\n$$\nSumming over the $s$ elements of $S$, this part contributes $s(1+\\tau^2)$ to $L(\\tau)$.\n\nFor $i \\in S^c$, let $Z \\sim \\mathcal{N}(0,1)$. We need to calculate $\\mathbb{E}[(|Z|-\\tau)^2 \\mathbb{I}(|Z|\\tau)]$. Let $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-z^2/2)$ be the standard normal PDF and $Q(\\tau) = \\int_\\tau^\\infty \\phi(z)dz$ be its tail probability.\n\\begin{align*}\n\\mathbb{E}[(|Z|-\\tau)^2 \\mathbb{I}(|Z|\\tau)] = \\int_{-\\infty}^\\infty (|z|-\\tau)^2 \\mathbb{I}(|z|\\tau) \\phi(z)dz \\\\\n= 2 \\int_\\tau^\\infty (z-\\tau)^2 \\phi(z)dz \\\\\n= 2 \\int_\\tau^\\infty (z^2 - 2\\tau z + \\tau^2) \\phi(z)dz \\\\\n= 2 \\int_\\tau^\\infty z^2 \\phi(z)dz - 4\\tau \\int_\\tau^\\infty z \\phi(z)dz + 2\\tau^2 \\int_\\tau^\\infty \\phi(z)dz\n\\end{align*}\nUsing the standard Gaussian integrals $\\int_\\tau^\\infty \\phi(z)dz = Q(\\tau)$, $\\int_\\tau^\\infty z \\phi(z)dz = \\phi(\\tau)$, and $\\int_\\tau^\\infty z^2 \\phi(z)dz = \\tau\\phi(\\tau) + Q(\\tau)$ (via integration by parts), we get:\n$$\n2(\\tau\\phi(\\tau) + Q(\\tau)) - 4\\tau\\phi(\\tau) + 2\\tau^2 Q(\\tau) = (2+2\\tau^2)Q(\\tau) - 2\\tau\\phi(\\tau) = 2(1+\\tau^2)Q(\\tau) - 2\\tau\\phi(\\tau)\n$$\nThere are $n-s$ such terms. Combining everything, the objective function is:\n$$\nL(\\tau) = s(1+\\tau^2) + (n-s) \\left[ 2(1+\\tau^2)Q(\\tau) - 2\\tau\\phi(\\tau) \\right]\n$$\n\n### Step 2: First-Order Optimality Condition\n\nTo find the minimizer $\\tau^\\star  0$, we differentiate $L(\\tau)$ with respect to $\\tau$ and set the derivative to zero. We use the derivatives $Q'(\\tau) = -\\phi(\\tau)$ and $\\phi'(\\tau) = -\\tau\\phi(\\tau)$.\n$$\n\\frac{dL}{d\\tau} = 2s\\tau + (n-s) \\frac{d}{d\\tau}\\left[ (2+2\\tau^2)Q(\\tau) - 2\\tau\\phi(\\tau) \\right]\n$$\nThe derivative of the bracketed term is:\n\\begin{align*}\n\\frac{d}{d\\tau} \\left[ (2+2\\tau^2)Q(\\tau) - 2\\tau\\phi(\\tau) \\right] = \\left(4\\tau Q(\\tau) + (2+2\\tau^2)(-\\phi(\\tau))\\right) - \\left(2\\phi(\\tau) + 2\\tau(-\\tau\\phi(\\tau))\\right) \\\\\n= 4\\tau Q(\\tau) - 2\\phi(\\tau) - 2\\tau^2\\phi(\\tau) - 2\\phi(\\tau) + 2\\tau^2\\phi(\\tau) \\\\\n= 4\\tau Q(\\tau) - 4\\phi(\\tau)\n\\end{align*}\nSetting $L'(\\tau) = 0$:\n$$\n2s\\tau + (n-s)[4\\tau Q(\\tau) - 4\\phi(\\tau)] = 0\n$$\nAssuming $\\tau  0$, we can divide by $2$:\n$$\ns\\tau + 2(n-s)\\tau Q(\\tau) - 2(n-s)\\phi(\\tau) = 0\n$$\nThis simplifies to the first-order optimality condition for $\\tau^\\star$:\n$$\n\\tau (s + 2(n-s)Q(\\tau)) = 2(n-s)\\phi(\\tau)\n$$\n\n### Step 3: Numerical Solution\n\nFor the specific instance $n=1000$ and $s=50$, we have $n-s=950$. The optimality condition becomes:\n$$\n\\tau (50 + 2(950)Q(\\tau)) = 2(950)\\phi(\\tau) \\implies \\tau(50 + 1900 Q(\\tau)) = 1900 \\phi(\\tau)\n$$\nDividing by $1900$:\n$$\n\\tau\\left(\\frac{50}{1900} + Q(\\tau)\\right) = \\phi(\\tau) \\implies \\tau\\left(\\frac{1}{38} + Q(\\tau)\\right) = \\phi(\\tau)\n$$\nThis is a transcendental equation for $\\tau$ that must be solved numerically. Let's rewrite the original FOC by dividing by $2(n-s)$:\n$$\n\\frac{s}{n-s}\\frac{\\tau}{2} + \\tau Q(\\tau) - \\phi(\\tau) = 0\n$$\nWith $\\rho = s/(n-s) = 50/950 = 1/19$, the equation is:\n$$\n\\frac{1}{19}\\frac{\\tau}{2} + \\tau Q(\\tau) - \\phi(\\tau) = 0 \\iff \\frac{\\tau}{38} + \\tau Q(\\tau) = \\phi(\\tau)\n$$\nSolving this equation numerically (e.g., using Newton's method or a root-finding solver) yields the solution. A numerical evaluation shows that the function $f(\\tau) = \\frac{\\tau}{38} + \\tau Q(\\tau) - \\phi(\\tau)$ changes sign between $\\tau=1.398$ and $\\tau=1.399$. More accurate root-finding places the solution at $\\tau^\\star \\approx 1.398$.\nRounding to four significant figures, we get $\\tau^\\star = 1.398$.\n\n### Step 4: Connection to Approximate Message Passing (AMP)\n\nThe derived optimality condition and its solution $\\tau^\\star$ have a deep connection to the analysis of AMP algorithms for sparse recovery.\nAMP is an iterative algorithm for solving systems like $y=Ax+w$. In the context of estimating a sparse vector $x^\\star$, a key component of AMP is a non-linear function applied at each iteration, which for $\\ell_1$ recovery is the soft-thresholding operator, $\\eta(z; \\lambda) = \\text{sgn}(z)\\max(|z|-\\lambda, 0)$. The performance of AMP in the high-dimensional limit can be precisely characterized by a scalar recursion called state evolution.\n\nThe state evolution tracks the effective noise variance $\\sigma_t^2$ in the estimate at iteration $t$. The update for this variance is given by an expectation over the effect of the thresholding function on the signal components. For a problem of estimating $x^\\star$ from observations $x^\\star + \\sigma_t Z$ where $Z \\sim \\mathcal{N}(0,1)$, the next state's variance is $\\sigma_{t+1}^2 = \\mathbb{E}[(\\eta(x^\\star+\\sigma_t Z; \\lambda_t) - x^\\star)^2]$.\n\nThe optimal choice of threshold $\\lambda_t$ is one that minimizes this expected error. The term we calculated, $\\mathbb{E}[(|Z|-\\tau)^2 \\mathbb{I}(|Z|\\tau)]$, corresponds exactly to the expected squared error $\\mathbb{E}[\\eta(Z; \\tau)^2]$ when estimating a zero coefficient ($x^\\star_i=0$) observed in unit-variance Gaussian noise.\n\nOur objective function $L(\\tau)$ can be viewed as a total risk, balancing the estimation error on the non-zero components ($s(1+\\tau^2)$) and the zero components ($(n-s)\\mathbb{E}[\\eta(Z;\\tau)^2]$). The minimizer $\\tau^\\star$ is the threshold that optimally balances this trade-off for a given sparsity $s$ and dimension $n$.\n\nCrucially, the optimality condition we derived, $\\tau (s + 2(n-s)Q(\\tau)) = 2(n-s)\\phi(\\tau)$, is identical to the equation that defines the fixed point of the state evolution for the LASSO problem in the large system limit. The solution $\\tau^\\star$ represents the asymptotic optimal thresholding parameter for a given problem configuration ($s, n$). This demonstrates a fundamental link between a geometric property of the problem (the statistical dimension of the descent cone) and the parameters of an efficient algorithm (the threshold in AMP). The optimizer $\\tau^\\star$ is precisely the data-driven threshold that AMP would converge to.", "answer": "$$\\boxed{1.398}$$", "id": "3448604"}, {"introduction": "Theory is powerful, but seeing it in action provides the ultimate confirmation. This final practice brings the \"escape through a mesh\" theorem to life through a computer simulation. You will implement the theoretical calculations to predict the precise phase transition for $\\ell_1$ recovery and then run empirical trials to witness this transition firsthand [@problem_id:3481865]. This exercise solidifies the connection between the abstract Gaussian width of a cone and the concrete, observable boundary between success and failure in sparse signal recovery.", "problem": "Let $n \\in \\mathbb{N}$ and let $x^\\star \\in \\mathbb{R}^n$ be a $k$-sparse signal, meaning that exactly $k$ coordinates of $x^\\star$ are nonzero. Consider linear measurements $y = A x^\\star$ with $A \\in \\mathbb{R}^{m \\times n}$ having independent and identically distributed standard normal entries. The cone of interest is the descent cone $K = \\mathcal{D}(\\|\\cdot\\|_1, x^\\star)$ of the convex function $\\|\\cdot\\|_1$ at $x^\\star$, defined as\n$$\n\\mathcal{D}(\\|\\cdot\\|_1, x^\\star) = \\operatorname{cone}\\left\\{h \\in \\mathbb{R}^n : \\|x^\\star + h\\|_1 \\le \\|x^\\star\\|_1 \\right\\}.\n$$\nThe Gaussian width $w(K)$ of a cone $K$ is defined as\n$$\nw(K) = \\mathbb{E}\\left[ \\sup_{u \\in K \\cap \\mathbb{S}^{n-1}} \\langle g, u \\rangle \\right],\n$$\nwhere $\\mathbb{S}^{n-1}$ is the unit sphere in $\\mathbb{R}^n$ and $g \\sim \\mathcal{N}(0, I_n)$ is a standard Gaussian vector. Gordon’s \"escape through a mesh\" theorem asserts that a random subspace of codimension $m$ is likely to avoid $K$ when $m$ is greater than a function of the Gaussian width of $K$; conversely, for smaller $m$, intersections are likely.\n\nYour task is to write a complete, standalone program to demonstrate this phenomenon for the descent cone $K = \\mathcal{D}(\\|\\cdot\\|_1, x^\\star)$ by:\n1. Computing an approximation of the squared Gaussian width $w(K)^2$ via the statistical dimension of the descent cone using first principles of conic geometry. The statistical dimension is defined as\n$$\n\\delta(K) = \\mathbb{E}\\left[ \\|\\Pi_K(g)\\|_2^2 \\right],\n$$\nwhere $\\Pi_K(g)$ denotes the Euclidean projection of $g$ onto $K$, and is closely related to the squared Gaussian width of $K \\cap \\mathbb{S}^{n-1}$.\n2. Empirically estimating whether the random nullspace $\\operatorname{Null}(A)$ intersects $K$ nontrivially, which equivalently indicates failure of $\\ell_1$-minimization recovery. Specifically, for each trial, solve the convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^n} \\|x\\|_1 \\quad \\text{subject to} \\quad A x = y,\n$$\nand declare recovery success if the minimizer equals $x^\\star$ within a numerical tolerance, and failure otherwise. The presence of a nonzero $h \\in \\operatorname{Null}(A) \\cap K$ indicates that recovery fails.\n3. Comparing the empirical success rate with the theoretical prediction based on whether $m$ is larger or smaller than the computed $\\delta(K)$.\n\nUse the following well-tested definitions and facts as your fundamental base:\n- The descent cone of a proper, convex, lower-semicontinuous function at a point collects directions that do not increase the function value.\n- The Gaussian width and statistical dimension are geometric measures that govern phase transitions in high-dimensional convex recovery.\n- For the $\\ell_1$ norm, the statistical dimension of the descent cone at a $k$-sparse point can be characterized via an expectation involving the subdifferential and a scalar parameter, and is numerically tractable using the distribution of the absolute value of a standard normal random variable.\n\nProgram requirements:\n- Implement a numerical procedure to approximate $\\delta\\!\\left(\\mathcal{D}(\\|\\cdot\\|_1, x^\\star)\\right)$ for given $(n,k)$ by minimizing over a nonnegative scalar parameter a suitable expectation derived from the half-normal distribution of $|g|$, where $g \\sim \\mathcal{N}(0,1)$.\n- For each test case, generate $A$ with independent and identically distributed standard normal entries and a random $k$-sparse $x^\\star$ with nonzero entries drawn from a continuous distribution, then solve the $\\ell_1$-minimization problem using a linear programming formulation and record whether recovery succeeds.\n- For each test case, output two numbers: the predicted indicator $(1$ if $m  \\delta(K)$, $0$ otherwise$)$, and the empirical recovery success fraction across the specified number of trials, rounded to three decimal places.\n\nTest suite:\n- Case 1 (happy path, above threshold): $(n,k,m,T) = (200,10,85,8)$.\n- Case 2 (below threshold): $(n,k,m,T) = (200,10,55,8)$.\n- Case 3 (near threshold): $(n,k,m,T) = (200,10,70,8)$.\n- Case 4 (boundary, zero-sparse): $(n,k,m,T) = (100,0,1,8)$.\n- Case 5 (dense relative sparsity): $(n,k,m,T) = (200,60,150,6)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a two-element list of the form $[\\text{indicator}, \\text{fraction}]$. For example, the output should have the form\n$$\n[[i_1,f_1],[i_2,f_2],\\dots,[i_5,f_5]],\n$$\nwhere each $i_j$ is an integer $0$ or $1$ and each $f_j$ is a decimal rounded to three places.", "solution": "The problem statement poses a valid and well-defined task in the field of compressed sensing and high-dimensional probability. We first validate the problem and then provide a complete solution.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Signal and Sparsity**: $x^\\star \\in \\mathbb{R}^n$ is a $k$-sparse signal, where $n \\in \\mathbb{N}$ and $k$ is the number of nonzero coordinates.\n- **Measurement Model**: $y = A x^\\star$, where $A \\in \\mathbb{R}^{m \\times n}$ has independent and identically distributed (i.i.d.) standard normal entries.\n- **Descent Cone**: $K = \\mathcal{D}(\\|\\cdot\\|_1, x^\\star) = \\operatorname{cone}\\left\\{h \\in \\mathbb{R}^n : \\|x^\\star + h\\|_1 \\le \\|x^\\star\\|_1 \\right\\}$.\n- **Gaussian Width**: $w(K) = \\mathbb{E}\\left[ \\sup_{u \\in K \\cap \\mathbb{S}^{n-1}} \\langle g, u \\rangle \\right]$, with $g \\sim \\mathcal{N}(0, I_n)$.\n- **Statistical Dimension**: $\\delta(K) = \\mathbb{E}\\left[ \\|\\Pi_K(g)\\|_2^2 \\right]$, where $\\Pi_K(g)$ is the Euclidean projection of $g$ onto $K$.\n- **Recovery Problem**: $\\min_{x \\in \\mathbb{R}^n} \\|x\\|_1$ subject to $A x = y$.\n- **Recovery Success Condition**: The minimizer of the recovery problem equals $x^\\star$. This occurs if and only if the nullspace of $A$, $\\operatorname{Null}(A)$, has no nontrivial intersection with the descent cone $K$.\n- **Task**: For given parameters $(n,k,m,T)$, compute a theoretical success indicator ($1$ if $m  \\delta(K)$, $0$ otherwise) and an empirical success fraction over $T$ trials.\n- **Test Cases**:\n    1. $(n,k,m,T) = (200,10,85,8)$\n    2. $(n,k,m,T) = (200,10,55,8)$\n    3. $(n,k,m,T) = (200,10,70,8)$\n    4. $(n,k,m,T) = (100,0,1,8)$\n    5. $(n,k,m,T) = (200,60,150,6)$\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded**: The problem is firmly rooted in the modern theory of compressed sensing. The concepts of descent cones, statistical dimension, Gaussian width, and their connection to phase transitions in convex optimization are central, established results in this field (cf. Amelunxen, Lotz, McCoy, Tropp, 2014, \"Living on the Edge: Phase Transitions in Convex Programs with Random Data\").\n- **Well-Posed**: The problem is well-posed. It asks for a numerical computation and simulation to demonstrate a known theoretical result. The tasks are specified clearly, and the parameters provided allow for a unique set of outputs.\n- **Objective**: The problem is stated in precise, mathematical language, free from subjectivity.\n- **Completeness**: The problem provides all necessary definitions and parameters. It implicitly relies on the known formula for the statistical dimension of the $\\ell_1$ descent cone, which is standard in this context.\n- **No Other Flaws**: The problem is not contradictory, unrealistic (within the mathematical framework), or ill-structured.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. We proceed with a full solution.\n\n### Solution\n\nThe problem explores the phase transition phenomenon in compressed sensing, where the success of recovering a sparse signal $x^\\star$ from linear measurements $y=Ax^\\star$ via $\\ell_1$-minimization undergoes a sharp change as the number of measurements $m$ varies. This transition is precisely characterized by the statistical dimension $\\delta(K)$ of the descent cone $K = \\mathcal{D}(\\|\\cdot\\|_1, x^\\star)$.\n\nThe fundamental principle is that for a random Gaussian matrix $A$, the recovery of $x^\\star$ is highly probable if $m  \\delta(K)$ and highly improbable if $m  \\delta(K)$. The point $m \\approx \\delta(K)$ marks the phase transition boundary. Our solution consists of two parts: first, computing the theoretical threshold $\\delta(K)$, and second, empirically verifying the prediction by simulating the recovery process.\n\n**1. Theoretical Prediction via Statistical Dimension**\n\nThe statistical dimension of the descent cone of the $\\ell_1$-norm at a $k$-sparse signal $x^\\star \\in \\mathbb{R}^n$, denoted $K=\\mathcal{D}(\\|\\cdot\\|_1, x^\\star)$, is independent of the support location and the signs of the nonzero entries of $x^\\star$. It is given by the formula:\n$$\n\\delta(K) = k + \\min_{\\lambda \\ge 0} \\left\\{ (n-k) \\mathbb{E}\\left[(\\lvert Z \\rvert - \\lambda)^2_+\\right] + k\\lambda^2 \\right\\}\n$$\nwhere $Z \\sim \\mathcal{N}(0,1)$ is a standard normal random variable, and $(t)_+ = \\max(t, 0)$ is the positive part function. The expectation term can be computed in a closed form involving the probability density function (PDF) $\\phi$ and the cumulative distribution function (CDF) $\\Phi$ of a standard normal variable:\n$$\n\\mathbb{E}\\left[(\\lvert Z \\rvert - \\lambda)^2_+\\right] = 2 \\int_{\\lambda}^{\\infty} (z-\\lambda)^2 \\phi(z) dz = 2 \\left[ (1+\\lambda^2)(1-\\Phi(\\lambda)) - \\lambda\\phi(\\lambda) \\right]\n$$\nfor $\\lambda \\ge 0$.\nThe problem of finding $\\delta(K)$ thus reduces to a one-dimensional convex optimization problem over the non-negative scalar $\\lambda$. We solve this numerically to find the value of $\\delta(K)$ for each pair of $(n,k)$ in the test suite. The theoretical prediction for recovery success is then given by an indicator function: $1$ if $m  \\delta(K)$ and $0$ otherwise.\n\nA special case is $k=0$, where $x^\\star = 0$. The descent cone is $K = \\{h : \\|h\\|_1 \\le 0\\} = \\{0\\}$. The projection of any vector onto $\\{0\\}$ is the zero vector, so $\\delta(\\{0\\}) = \\mathbb{E}[\\|\\Pi_{\\{0\\}}(g)\\|_2^2] = 0$.\n\n**2. Empirical Verification via Simulation**\n\nWe perform Monte Carlo simulations to estimate the empirical probability of successful recovery for each test case $(n,k,m,T)$. For each of the $T$ trials:\na. A random $k$-sparse signal $x^\\star \\in \\mathbb{R}^n$ is generated. A support of size $k$ is chosen uniformly at random, and the non-zero entries are drawn from a standard normal distribution.\nb. An $m \\times n$ measurement matrix $A$ is generated with i.i.d. $\\mathcal{N}(0,1)$ entries.\nc. The measurement vector is computed as $y = Ax^\\star$.\nd. The $\\ell_1$-minimization problem (also known as Basis Pursuit) is solved to find an estimate $\\hat{x}$:\n$$\n\\hat{x} = \\arg\\min_{x \\in \\mathbb{R}^n} \\|x\\|_1 \\quad \\text{subject to} \\quad Ax = y\n$$\nThis convex optimization problem is recast as a Linear Program (LP) by representing $x$ as the difference of two non-negative vectors, $x = u - v$, where $u_i, v_i \\ge 0$. The LP is:\n$$\n\\min_{u, v \\in \\mathbb{R}^n} \\sum_{i=1}^n (u_i + v_i) \\quad \\text{subject to} \\quad A(u-v) = y, \\quad u \\ge 0, \\quad v \\ge 0.\n$$\nThis LP is solved using standard numerical solvers.\ne. Recovery is declared a \"success\" if the solution $\\hat{x}$ is numerically close to the original signal $x^\\star$. We use a mixed absolute-relative error tolerance: for $k  0$, success is defined by $\\|\\hat{x} - x^\\star\\|_2 / \\|x^\\star\\|_2  10^{-6}$; for $k=0$, success is defined by $\\|\\hat{x}\\|_2  10^{-6}$.\nThe empirical success fraction is the number of successful trials divided by the total number of trials, $T$. This fraction is compared against the theoretical indicator.\n\nThe implementation of this procedure for the specified test cases will demonstrate the sharpness of the phase transition predicted by the statistical dimension.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog, minimize_scalar\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes theoretical and empirical results for l1 recovery phase transitions.\n    \"\"\"\n    TOLERANCE = 1e-6\n\n    def calculate_statistical_dimension(n, k):\n        \"\"\"\n        Calculates the statistical dimension delta(K) for the l1 descent cone.\n        \n        The formula is:\n        delta(K) = k + min_{lambda = 0} { (n-k) * E[(|Z|-lambda)^2_+] + k*lambda^2 }\n        where Z ~ N(0,1).\n        \"\"\"\n        if k == 0:\n            return 0.0\n        if k == n:\n            return float(n)\n\n        def expectation_term(lam):\n            \"\"\"Computes E[(|Z|-lambda)^2_+].\"\"\"\n            if lam  0:\n                # The minimization is over lambda = 0\n                return np.inf\n            # Using the closed-form expression\n            # 2 * [ (1+lam^2)*(1-Phi(lam)) - lam*phi(lam) ]\n            return 2 * ( (1 + lam**2) * (1 - norm.cdf(lam)) - lam * norm.pdf(lam) )\n\n        def objective(lam, n_val, k_val):\n            \"\"\"The function to be minimized over lambda.\"\"\"\n            return (n_val - k_val) * expectation_term(lam) + k_val * lam**2\n\n        # Numerically minimize the objective function to find the minimum value.\n        # The minimization is over lambda = 0.\n        # 'bounded' method is suitable for box constraints.\n        res = minimize_scalar(\n            objective, \n            args=(n, k), \n            bounds=(0, 10), # lambda is unlikely to be large, 10 is a safe upper bound.\n            method='bounded'\n        )\n        \n        min_val = res.fun\n        return k + min_val\n\n    def run_single_trial(n, k, m, rng):\n        \"\"\"\n        Runs a single trial of sparse recovery.\n        \"\"\"\n        # 1. Generate a random k-sparse signal x_star\n        x_star = np.zeros(n)\n        if k  0:\n            support = rng.choice(n, k, replace=False)\n            x_star[support] = rng.standard_normal(k)\n\n        # 2. Generate measurement matrix A and measurements y\n        A = rng.standard_normal((m, n))\n        y = A @ x_star\n        \n        # 3. Solve the l1 minimization problem (Basis Pursuit) via Linear Programming\n        # Problem: min ||x||_1 s.t. Ax = y\n        # LP form:   min c.T @ z s.t. A_eq @ z = b_eq, z = 0\n        # where z = [u, v], x = u - v, ||x||_1 = 1.T @ u + 1.T @ v\n        c_lp = np.ones(2 * n)\n        A_lp = np.hstack([A, -A])\n        b_lp = y\n        \n        # Using 'highs' solver, which is robust and efficient.\n        res = linprog(c=c_lp, A_eq=A_lp, b_eq=b_lp, bounds=(0, None), method='highs')\n\n        if not res.success:\n            return False\n\n        # 4. Reconstruct the solution and check for success\n        x_sol = res.x[:n] - res.x[n:]\n        \n        if k == 0:\n            # Absolute error for the zero vector\n            norm_x_star = 0\n            if np.linalg.norm(x_sol)  TOLERANCE:\n                return True\n        else:\n            # Relative error for non-zero vectors\n            norm_x_star = np.linalg.norm(x_star)\n            if norm_x_star  1e-9 and np.linalg.norm(x_sol - x_star) / norm_x_star  TOLERANCE:\n                return True\n        \n        return False\n\n    # Test cases: (n, k, m, T)\n    test_cases = [\n        (200, 10, 85, 8),\n        (200, 10, 55, 8),\n        (200, 10, 70, 8),\n        (100, 0, 1, 8),\n        (200, 60, 150, 6)\n    ]\n    \n    final_results = []\n    rng = np.random.default_rng(seed=42) # Seed for reproducibility\n    \n    # Cache for statistical dimension calculation\n    delta_cache = {}\n\n    for n, k, m, T in test_cases:\n        # Calculate theoretical prediction\n        if (n, k) not in delta_cache:\n            delta_cache[(n, k)] = calculate_statistical_dimension(n, k)\n        \n        delta_K = delta_cache[(n, k)]\n        theoretical_indicator = 1 if m  delta_K else 0\n        \n        # Run empirical simulations\n        success_count = 0\n        if T  0:\n            for _ in range(T):\n                if run_single_trial(n, k, m, rng):\n                    success_count += 1\n            empirical_fraction = success_count / T\n        else:\n            empirical_fraction = 0.0\n\n        final_results.append(f\"[{theoretical_indicator},{empirical_fraction:.3f}]\")\n\n    print(f\"[{','.join(final_results)}]\")\n\nsolve()\n```", "id": "3481865"}]}