{"hands_on_practices": [{"introduction": "Mastering State Evolution (SE) begins with proficiency in its core recursion. This first practice focuses on the fundamental mechanics of computing a single-step update of the SE variance, $\\tau_{t+1}^{2}$. By deriving the optimal threshold for a positive soft-thresholding denoiser, you will not only calculate the mean-squared error that drives the evolution but also see how SE serves as a powerful predictive tool for tuning algorithm parameters. [@problem_id:3481509]", "problem": "Consider the Approximate Message Passing (AMP) algorithm for compressed sensing with measurement ratio $\\delta \\in (0,1)$ and additive white Gaussian measurement noise of variance $\\sigma_{w}^{2}$. The state evolution (SE) tracks the effective scalar noise at iteration $t$ via\n$$\n\\tau_{t+1}^{2} \\;=\\; \\sigma_{w}^{2} \\;+\\; \\frac{1}{\\delta} \\,\\mathbb{E}\\!\\left[\\big(\\eta_{t}(X_{0}+\\tau_{t} Z) - X_{0}\\big)^{2}\\right],\n$$\nwhere $Z \\sim \\mathcal{N}(0,1)$ is independent of $X_{0}$, and $\\eta_{t}$ is a scalar denoiser applied componentwise.\n\nIn this problem you will determine the scalar denoising threshold that minimizes the mean-squared error (MSE) in the SE update and then use it to predict the next effective variance. Assume the following setup:\n\n- The iteration index $t$ is fixed and known quantities are $\\delta = 0.8$, $\\sigma_{w}^{2} = 1.0 \\times 10^{-2}$, and $\\tau_{t} = 0.5$.\n- The unknown signal entry is nonnegative and deterministic, $X_{0} = \\mu$, with $\\mu = 0.3989422804$.\n- The scalar denoiser is the positive soft-thresholding rule\n$$\n\\eta_{t}(u;\\lambda) \\;=\\; \\max(u-\\lambda,\\,0),\n$$\nwith scalar threshold $\\lambda \\geq 0$ to be optimized.\n\nStarting from the SE definition above and using the laws of conditional expectation for Gaussian random variables and first principles of calculus, derive the threshold $\\lambda^{\\star}$ that minimizes the MSE\n$$\n\\mathbb{E}\\!\\left[\\big(\\eta_{t}(X_{0}+\\tau_{t} Z;\\lambda) - X_{0}\\big)^{2}\\right]\n$$\nover $\\lambda \\geq 0$. Then, plug $\\lambda^{\\star}$ into the SE update to predict $\\tau_{t+1}^{2}$.\n\nCarry out all necessary derivations symbolically before substituting numerical values. Round all numerical results in your final answer to four significant figures. Provide your final answer as the two-entry row vector $\\big(\\lambda^{\\star},\\,\\tau_{t+1}^{2}\\big)$ with no units.", "solution": "We want to find the threshold $\\lambda \\geq 0$ that minimizes the Mean-Squared Error (MSE)\n$$\nR(\\lambda) = \\mathbb{E}\\!\\left[\\big(\\eta_{t}(X_{0}+\\tau_{t} Z;\\lambda) - X_{0}\\big)^{2}\\right],\n$$\nwhere $X_0 = \\mu$, $Z \\sim \\mathcal{N}(0,1)$, and $\\eta_{t}(u;\\lambda) = \\max(u-\\lambda, 0)$. Let $Y = \\mu + \\tau_t Z$, so that $Y \\sim \\mathcal{N}(\\mu,\\tau_{t}^{2})$. The MSE is\n$$\nR(\\lambda) = \\mathbb{E}\\!\\left[\\big(\\max(Y-\\lambda, 0) - \\mu\\big)^{2}\\right].\n$$\nTo find the minimum, we differentiate $R(\\lambda)$ with respect to $\\lambda$ and set the derivative to zero. Using the Leibniz integral rule:\n\\begin{align*}\n\\frac{dR}{d\\lambda} = \\frac{d}{d\\lambda} \\left( \\int_{-\\infty}^{\\lambda} (-\\mu)^2 f_Y(y)\\,dy + \\int_{\\lambda}^{\\infty} (y-\\lambda-\\mu)^2 f_Y(y)\\,dy \\right) \\\\\n= \\mu^2 f_Y(\\lambda) - (\\lambda-\\lambda-\\mu)^2 f_Y(\\lambda) + \\int_{\\lambda}^{\\infty} \\frac{\\partial}{\\partial\\lambda}(y-\\lambda-\\mu)^2 f_Y(y)\\,dy \\\\\n= -2 \\int_{\\lambda}^{\\infty} (y-\\lambda-\\mu) f_Y(y)\\,dy = -2\\,\\mathbb{E}[(Y-\\lambda-\\mu)\\mathbf{1}\\{Y  \\lambda\\}].\n\\end{align*}\nSetting the derivative to zero gives the optimality condition $\\mathbb{E}[(Y-\\lambda-\\mu)\\mathbf{1}\\{Y  \\lambda\\}] = 0$, which implies $\\mathbb{E}[Y\\mathbf{1}\\{Y  \\lambda\\}] = (\\lambda+\\mu)\\mathbb{P}(Y  \\lambda)$. Let $a = (\\lambda-\\mu)/\\tau_t$. The condition $Y  \\lambda$ is equivalent to $Z  a$. The expectation becomes\n$$\n\\mathbb{E}[(\\mu+\\tau_t Z)\\mathbf{1}\\{Z  a\\}] = \\mu\\,\\mathbb{P}(Za) + \\tau_t\\,\\mathbb{E}[Z\\mathbf{1}\\{Z  a\\}] = \\mu Q(a) + \\tau_t \\phi(a),\n$$\nwhere $\\phi(\\cdot)$ and $Q(\\cdot)$ are the standard normal PDF and tail probability function. The optimality condition simplifies to\n$$\n\\mu Q(a) + \\tau_t \\phi(a) = (\\lambda+\\mu) Q(a) \\quad\\implies\\quad \\tau_t \\phi(a) = \\lambda Q(a).\n$$\nWe can write this as a scalar fixed-point equation for $a = (\\lambda-\\mu)/\\tau_t$:\n$$\n\\frac{\\mu}{\\tau_{t}} = \\frac{\\phi(a)}{Q(a)} - a,\n$$\nwhere $\\phi(a)/Q(a)$ is the Mills ratio. In our numerical instance, $\\tau_{t} = 0.5$ and $\\mu = 0.3989422804 \\approx 1/\\sqrt{2\\pi}$. Thus,\n$$\n\\frac{\\mu}{\\tau_{t}} = \\frac{1/(0.5\\sqrt{2\\pi})}{0.5} = \\frac{2}{\\sqrt{2\\pi}} = \\sqrt{\\frac{2}{\\pi}}.\n$$\nAt $a = 0$, we have $\\phi(0) = 1/\\sqrt{2\\pi}$ and $Q(0) = 1/2$, so the right-hand side is $\\frac{\\phi(0)}{Q(0)} - 0 = \\frac{1/\\sqrt{2\\pi}}{1/2} = \\sqrt{\\frac{2}{\\pi}}$. The equation holds for $a^{\\star} = 0$, so the optimal threshold is $\\lambda^{\\star} = \\mu + a^{\\star}\\tau_{t} = \\mu = 0.3989422804$.\n\nNext, we compute the minimized MSE, $R(\\lambda^{\\star})$, by substituting $\\lambda^{\\star}=\\mu$:\n$$\nR(\\lambda^{\\star}) = \\mathbb{E}\\!\\left[\\big(\\max(\\mu+\\tau_t Z - \\mu, 0) - \\mu\\big)^{2}\\right] = \\mathbb{E}\\!\\left[\\big(\\max(\\tau_t Z, 0) - \\mu\\big)^{2}\\right].\n$$\nExpanding the square gives $R(\\lambda^{\\star}) = \\mathbb{E}[\\max(\\tau_t Z, 0)^2] - 2\\mu\\,\\mathbb{E}[\\max(\\tau_t Z, 0)] + \\mu^2$. The two expectations are:\n$$\n\\mathbb{E}[\\max(\\tau_t Z, 0)] = \\int_0^\\infty \\tau_t z\\,\\phi(z)\\,dz = \\tau_t \\phi(0) = \\frac{\\tau_t}{\\sqrt{2\\pi}}.\n$$\n$$\n\\mathbb{E}[\\max(\\tau_t Z, 0)^2] = \\int_0^\\infty (\\tau_t z)^2 \\phi(z)\\,dz = \\tau_t^2 \\int_0^\\infty z^2 \\phi(z)\\,dz = \\tau_t^2 \\left(\\frac{1}{2}\\mathbb{E}[Z^2]\\right) = \\frac{\\tau_t^2}{2}.\n$$\nSubstituting these back, and using $\\mu = \\tau_t \\sqrt{2/\\pi} = 2 \\tau_t \\phi(0)$, we have $2\\mu\\frac{\\tau_t}{\\sqrt{2\\pi}} = \\mu (2\\tau_t \\phi(0)) = \\mu^2$.\n$$\nR(\\lambda^{\\star}) = \\frac{\\tau_t^2}{2} - \\mu^2 + \\mu^2 = \\frac{\\tau_t^2}{2}.\n$$\nWith $\\tau_{t} = 0.5$, we have $\\tau_{t}^{2} = 0.25$ and hence\n$$\nR(\\lambda^{\\star}) = \\frac{0.25}{2} = 0.125.\n$$\nFinally, we plug this minimized MSE into the SE update:\n$$\n\\tau_{t+1}^{2} = \\sigma_{w}^{2} + \\frac{1}{\\delta}\\,R(\\lambda^{\\star}) = 1.0 \\times 10^{-2} + \\frac{1}{0.8}\\times 0.125 = 0.01 + 0.15625 = 0.16625.\n$$\nRounding to four significant figures,\n$$\n\\lambda^{\\star} \\approx 0.3989, \\qquad \\tau_{t+1}^{2} \\approx 0.1663.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}0.3989  0.1663\\end{pmatrix}}$$", "id": "3481509"}, {"introduction": "Beyond single-step updates, the true analytical power of State Evolution lies in predicting the long-term performance of the Approximate Message Passing (AMP) algorithm. This exercise introduces the concept of an SE fixed point, which represents the steady-state effective noise that the algorithm is predicted to reach. By analyzing a scenario where a complex denoiser is approximated by a simpler one, you will derive the equation for this fixed-point variance, $\\sigma_{*}^{2}$, and learn how to predict the ultimate performance limits of AMP. [@problem_id:3481529]", "problem": "Consider the linear measurement model $y = A x_{0} + w$ where $A \\in \\mathbb{R}^{m \\times n}$ has independent and identically distributed entries $A_{ij} \\sim \\mathcal{N}(0, 1/n)$, the signal is identically zero $x_{0} = 0 \\in \\mathbb{R}^{n}$, and the noise $w \\in \\mathbb{R}^{m}$ has independent and identically distributed entries $w_{i} \\sim \\mathcal{N}(0, \\sigma_{w}^{2})$. Assume the measurement ratio $m/n \\to \\delta \\in (0, \\infty)$ as $m, n \\to \\infty$. Consider an Approximate Message Passing (AMP) algorithm (Approximate Message Passing (AMP)) whose denoiser at iteration $t$ is the proximal operator for Sorted L-One Penalized Estimation (SLOPE) (Sorted L-One Penalized Estimation (SLOPE)), applied to its pseudo-data $r^{t} \\in \\mathbb{R}^{n}$. The SLOPE proximal is non-separable because it depends on the order statistics of $|r^{t}|$ and a non-increasing sequence of weights $\\{\\lambda_{i}\\}_{i=1}^{n}$. \n\nTo make State Evolution (SE) (State Evolution (SE)) tractable, approximate the non-separable denoiser by tracking the empirical distribution of the thresholds that act on the order statistics and suppose this empirical distribution concentrates at a single point. Specifically, assume that in the large-system limit, the empirical distribution of sorted thresholds converges to a degenerate distribution at the threshold level $c \\sigma_{t}$, where $c  0$ is a fixed regularization design parameter and $\\sigma_{t}^{2}$ is the current effective noise variance in SE. Under this approximation, the proximal reduces to componentwise soft-thresholding with threshold $c \\sigma_{t}$, so the scalar denoising function can be written as $\\eta(r; c \\sigma_{t}) = \\mathrm{sign}(r)\\max\\{|r| - c \\sigma_{t}, 0\\}$.\n\nStarting from first principles for AMP SE under i.i.d. Gaussian sensing and the above approximation, derive the fixed-point effective noise variance $\\sigma_{*}^{2}$ as a closed-form analytic expression in terms of $\\delta$, $\\sigma_{w}^{2}$, and $c$. Your derivation must:\n- Begin at the foundational SE description of AMP as an equivalent scalar Gaussian channel with variance tracked by SE.\n- Use the exact soft-thresholding rule to express the mean squared error per coordinate in terms of expectations over the standard normal distribution.\n- Reduce the mean squared error to standard functions of the normal distribution, introducing $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-z^{2}/2)$ and $Q(z) = \\int_{z}^{\\infty} \\phi(u)\\,\\mathrm{d}u$ only as needed.\n\nProvide your final answer as a single closed-form analytic expression for $\\sigma_{*}^{2}$ in terms of $\\delta$, $\\sigma_{w}^{2}$, $c$, $\\phi(c)$, and $Q(c)$. No numerical evaluation is required.", "solution": "The State Evolution (SE) for AMP characterizes the statistical properties of the algorithm's iterates in the large-system limit ($m, n \\to \\infty$, $m/n \\to \\delta$). The core principle of SE is that at each iteration $t$, the pseudo-data vector $r^t$ (the input to the denoiser) is statistically equivalent to the true signal $x_0$ corrupted by additive white Gaussian noise of variance $\\sigma_t^2$. That is, for any component $i$, $r_i^t$ behaves as a random variable drawn from the distribution of $x_{0,i} + \\sigma_t Z_i$, where $Z_i \\sim \\mathcal{N}(0,1)$ is a standard normal random variable.\n\nThe evolution of the effective noise variance from one iteration to the next is described by the SE recursion:\n$$ \\sigma_{t+1}^2 = \\sigma_w^2 + \\frac{1}{\\delta} \\mathbb{E}\\left[ \\left( \\eta(x_0 + \\sigma_t Z; \\tau_t) - x_0 \\right)^2 \\right] $$\nwhere the expectation is over the joint distribution of the signal $x_0$ and the standard normal variable $Z$, and $\\tau_t$ is the thresholding parameter at iteration $t$.\n\nIn this specific problem, we are given several simplifications:\n1.  The true signal is identically zero: $x_0 = 0$.\n2.  The denoising function is componentwise soft-thresholding: $\\eta(r; \\tau) = \\mathrm{sign}(r)\\max\\{|r| - \\tau, 0\\}$.\n3.  The threshold $\\tau_t$ is coupled to the effective noise standard deviation $\\sigma_t$ via $\\tau_t = c \\sigma_t$, where $c$ is a positive constant.\n\nSubstituting these into the general SE recursion, we get:\n$$ \\sigma_{t+1}^2 = \\sigma_w^2 + \\frac{1}{\\delta} \\mathbb{E}\\left[ \\left( \\eta(0 + \\sigma_t Z; c \\sigma_t) - 0 \\right)^2 \\right] $$\nwhere $Z \\sim \\mathcal{N}(0,1)$. This simplifies to:\n$$ \\sigma_{t+1}^2 = \\sigma_w^2 + \\frac{1}{\\delta} \\mathbb{E}\\left[ \\eta(\\sigma_t Z; c \\sigma_t)^2 \\right] $$\n\nOur next task is to evaluate the expectation term. Let $V = \\sigma_t Z$. Then $V$ is a Gaussian random variable with mean $0$ and variance $\\sigma_t^2$. The expectation is:\n$$ \\mathbb{E}\\left[ \\eta(\\sigma_t Z; c \\sigma_t)^2 \\right] = \\mathbb{E}\\left[ \\left( \\mathrm{sign}(\\sigma_t Z)\\max\\{|\\sigma_t Z| - c\\sigma_t, 0\\} \\right)^2 \\right] \\\\ = \\mathbb{E}\\left[ \\left( \\max\\{\\sigma_t|Z| - c\\sigma_t, 0\\} \\right)^2 \\right] \\\\ = \\sigma_t^2 \\, \\mathbb{E}\\left[ \\left( \\max\\{|Z| - c, 0\\} \\right)^2 \\right] $$\nThe expectation is over $Z \\sim \\mathcal{N}(0,1)$. We compute this expectation by integrating against the standard normal probability density function (PDF), $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$.\n$$ \\mathbb{E}\\left[ (\\max\\{|Z| - c, 0\\})^2 \\right] = \\int_{-\\infty}^{\\infty} (\\max\\{|z| - c, 0\\})^2 \\phi(z) \\, \\mathrm{d}z $$\nThe integrand is non-zero only for $|z| > c$. Due to the symmetry of the integrand $(|z|-c)^2 \\phi(z)$, we can write the integral as:\n$$ = 2 \\int_{c}^{\\infty} (z - c)^2 \\phi(z) \\, \\mathrm{d}z $$\nExpanding the squared term, $(z - c)^2 = z^2 - 2cz + c^2$, we get:\n$$ = 2 \\left[ \\int_{c}^{\\infty} z^2 \\phi(z) \\, \\mathrm{d}z - 2c \\int_{c}^{\\infty} z \\phi(z) \\, \\mathrm{d}z + c^2 \\int_{c}^{\\infty} \\phi(z) \\, \\mathrm{d}z \\right] $$\nWe evaluate each integral separately using standard truncated Gaussian moment identities:\n1.  $\\int_{c}^{\\infty} \\phi(z) \\, \\mathrm{d}z = Q(c)$\n2.  $\\int_{c}^{\\infty} z \\phi(z) \\, \\mathrm{d}z = \\phi(c)$\n3.  $\\int_{c}^{\\infty} z^2 \\phi(z) \\, \\mathrm{d}z = c\\phi(c) + Q(c)$\n\nSubstituting these results back into the expression for the expectation:\n$$ \\mathbb{E}\\left[ (\\max\\{|Z| - c, 0\\})^2 \\right] = 2 \\left[ (c\\phi(c) + Q(c)) - 2c(\\phi(c)) + c^2(Q(c)) \\right] \\\\ = 2 \\left[ Q(c) - c\\phi(c) + c^2 Q(c) \\right] \\\\ = 2 \\left[ (1+c^2)Q(c) - c\\phi(c) \\right] $$\nThe total mean squared error term is therefore:\n$$ \\mathbb{E}\\left[ \\eta(\\sigma_t Z; c \\sigma_t)^2 \\right] = \\sigma_t^2 \\cdot 2 \\left[ (1+c^2)Q(c) - c\\phi(c) \\right] $$\nLet us define a constant $K(c)$ that depends only on the parameter $c$:\n$$ K(c) = 2 \\left[ (1+c^2)Q(c) - c\\phi(c) \\right] $$\nThe SE recursion simplifies to a linear map on the variance:\n$$ \\sigma_{t+1}^2 = \\sigma_w^2 + \\frac{K(c)}{\\delta} \\sigma_t^2 $$\nTo find the fixed point $\\sigma_*^2$, we set $\\sigma_{t+1}^2 = \\sigma_t^2 = \\sigma_*^2$:\n$$ \\sigma_*^2 = \\sigma_w^2 + \\frac{K(c)}{\\delta} \\sigma_*^2 $$\nSolving for $\\sigma_*^2$:\n$$ \\sigma_*^2 \\left( 1 - \\frac{K(c)}{\\delta} \\right) = \\sigma_w^2 \\\\ \\sigma_*^2 \\left( \\frac{\\delta - K(c)}{\\delta} \\right) = \\sigma_w^2 \\\\ \\sigma_*^2 = \\frac{\\delta \\sigma_w^2}{\\delta - K(c)} $$\nFinally, substituting the expression for $K(c)$ yields the closed-form solution:\n$$ \\sigma_*^2 = \\frac{\\delta \\sigma_w^2}{\\delta - 2 \\left[ (1+c^2)Q(c) - c\\phi(c) \\right]} = \\frac{\\delta \\sigma_w^2}{\\delta - 2(1+c^2)Q(c) + 2c\\phi(c)} $$\nThis is the fixed-point effective noise variance as a function of the problem parameters $\\delta$, $\\sigma_w^2$, and $c$, and the standard normal functions $\\phi(c)$ and $Q(c)$.", "answer": "$$\\boxed{\\frac{\\delta \\sigma_{w}^{2}}{\\delta - 2(1+c^{2})Q(c) + 2c\\phi(c)}}$$", "id": "3481529"}, {"introduction": "State Evolution is not just a tool for performance prediction; it also offers profound insights into optimal algorithm design. This final practice moves from calculation to strategic thinking, challenging you to determine the best way to schedule denoisers over time to minimize the final error. By leveraging the fundamental properties of the SE recursion and the unique optimality of the Minimum Mean-Squared Error (MMSE) estimator, you will uncover a deep principle about designing the most effective recovery algorithms. [@problem_id:3481473]", "problem": "Consider the standard linear compressed sensing model with additive white Gaussian noise (AWGN): $y = A x_{0} + w$, where $A \\in \\mathbb{R}^{m \\times n}$ has independent and identically distributed entries $A_{ij} \\sim \\mathcal{N}(0, 1/m)$, the measurement ratio is $\\delta = m/n \\in (0, 1)$, the noise is $w \\sim \\mathcal{N}(0, \\sigma_{w}^{2} I_{m})$, and the signal entries are independent and identically distributed as $X_{0} \\sim P_{X}$ with $\\mathbb{E}[X_{0}^{2}]  \\infty$. Assume the prior $P_{X}$ is known.\n\nRun approximate message passing (AMP) for a total of $T_{f}$ iterations with a two-phase denoiser schedule. In phase one (warm start), use the minimum mean-squared error (MMSE) denoiser matched to $P_{X}$, defined as the posterior mean under the scalar additive white Gaussian noise channel, and in phase two, use the soft-thresholding denoiser. Specifically, let the switch time be an integer $t^{\\dagger} \\in \\{0, 1, \\dots, T_{f}\\}$ so that for iterations $t = 0, 1, \\dots, t^{\\dagger}-1$ the AMP denoiser is the MMSE denoiser, and for iterations $t = t^{\\dagger}, \\dots, T_{f}-1$ the AMP denoiser is the soft-thresholding map $v \\mapsto \\operatorname{sign}(v)\\max\\{|v| - \\theta_{t}, 0\\}$ with an arbitrary threshold schedule $\\{\\theta_{t}\\}_{t \\geq 0}$ (possibly depending on iteration index and any observable AMP quantities). Assume that the canonical state evolution (SE) holds for this AMP instance in the high-dimensional limit, with the usual initialization for the effective noise variance and the Onsager correction.\n\nUsing only the foundational facts that (i) the state evolution recursion for AMP with separable denoisers reduces each iteration to scalar denoising under an effective Gaussian noise and (ii) the MMSE estimator under a known prior minimizes the mean-squared error for scalar denoising under additive white Gaussian noise among all measurable estimators, determine the switch time $t^{\\dagger}$ that minimizes the final mean-squared error predicted by state evolution at iteration $T_{f}$.\n\nTake $T_{f} = 5$. Provide your answer as a single integer. No rounding is required.", "solution": "The problem asks for the optimal switch time $t^{\\dagger}$ in a two-phase denoiser schedule for an Approximate Message Passing (AMP) algorithm to minimize the final mean-squared error (MSE) predicted by State Evolution (SE).\n\nLet $\\tau_t^2$ denote the predicted MSE of the signal estimate at iteration $t$, and let $\\sigma_t^2$ denote the effective noise variance of the input to the denoiser at iteration $t$. According to the provided foundational facts, the SE recursion proceeds as follows:\n1. At iteration $t$, the input to the denoiser $\\eta_t$ is statistically equivalent to the true signal $X_0$ corrupted by independent Gaussian noise of variance $\\sigma_t^2$.\n2. The MSE of the resulting signal estimate is $\\tau_{t+1}^2 = \\mathbb{E}\\left[ (\\eta_t(X_0 + \\sigma_t Z) - X_0)^2 \\right]$, where $Z \\sim \\mathcal{N}(0, 1)$. Let's denote this function as $\\tau_{t+1}^2 = \\Psi(\\sigma_t^2; \\eta_t)$.\n3. The effective noise variance for the next iteration is then updated as $\\sigma_{t+1}^2 = \\sigma_w^2 + \\frac{1}{\\delta} \\tau_{t+1}^2$.\n\nOur goal is to choose the sequence of denoisers $\\{\\eta_t\\}_{t=0}^{T_f-1}$, subject to the two-phase schedule, to minimize the final MSE, $\\tau_{T_f}^2$. The schedule is:\n$$\n\\eta_t =\n\\begin{cases}\n\\eta_{\\text{MMSE}}  \\text{for } t = 0, 1, \\dots, t^{\\dagger}-1 \\\\\n\\eta_{\\text{ST}, \\theta_t}  \\text{for } t = t^{\\dagger}, \\dots, T_f-1\n\\end{cases}\n$$\nwhere $\\eta_{\\text{MMSE}}$ is the Minimum Mean-Squared Error denoiser and $\\eta_{\\text{ST}, \\theta_t}$ is the soft-thresholding operator.\n\nThe second foundational fact states that the MMSE estimator minimizes the MSE for any given input noise level. In our notation, this means for any given effective noise variance $\\sigma^2$ and any denoiser $\\eta$:\n$$\n\\Psi(\\sigma^2; \\eta_{\\text{MMSE}}) \\le \\Psi(\\sigma^2; \\eta)\n$$\nThis inequality is the key. It means that for a fixed $\\sigma_t^2$, choosing $\\eta_t = \\eta_{\\text{MMSE}}$ produces the smallest possible next-step MSE, $\\tau_{t+1}^2$.\n\nThe SE update $\\sigma_{t+1}^2 = \\sigma_w^2 + \\frac{1}{\\delta} \\tau_{t+1}^2$ shows that $\\sigma_{t+1}^2$ is a monotonically increasing function of $\\tau_{t+1}^2$. Therefore, to minimize $\\sigma_{t+1}^2$, we must minimize $\\tau_{t+1}^2$, which is achieved by choosing $\\eta_t = \\eta_{\\text{MMSE}}$.\n\nLet's apply this logic backwards from the final step, $t=T_f-1$. We want to minimize $\\tau_{T_f}^2 = \\Psi(\\sigma_{T_f-1}^2; \\eta_{T_f-1})$.\nFor any given value of $\\sigma_{T_f-1}^2$, this is minimized by choosing $\\eta_{T_f-1} = \\eta_{\\text{MMSE}}$.\n\nNow consider step $t=T_f-2$. We want to make $\\sigma_{T_f-1}^2$ as small as possible, because the function $\\Psi(\\sigma^2; \\eta)$ is non-decreasing in its first argument $\\sigma^2$ (a higher input noise cannot lead to a lower output error). To minimize $\\sigma_{T_f-1}^2 = \\sigma_w^2 + \\frac{1}{\\delta} \\Psi(\\sigma_{T_f-2}^2; \\eta_{T_f-2})$, we must choose $\\eta_{T_f-2}$ to minimize $\\Psi(\\sigma_{T_f-2}^2; \\eta_{T_f-2})$. The optimal choice is again $\\eta_{T_f-2} = \\eta_{\\text{MMSE}}$.\n\nThis reasoning applies recursively for all steps $t = T_f-1, T_f-2, \\dots, 0$. At every iteration, the optimal choice to minimize the final error is to use the MMSE denoiser. Any other choice at any step $t$ would produce a (potentially) larger $\\tau_{t+1}^2$, leading to a larger $\\sigma_{t+1}^2$, and this increase in error variance would propagate through all subsequent iterations, leading to a larger or equal final MSE.\n\nTherefore, the optimal strategy is to use the MMSE denoiser for all iterations, from $t=0$ to $t=T_f-1$.\nThe given two-phase schedule specifies using $\\eta_{\\text{MMSE}}$ for $t = 0, \\dots, t^{\\dagger}-1$. To make this schedule optimal, we must choose $t^{\\dagger}$ such that the MMSE denoiser is used for all $T_f$ iterations. This means the second phase (soft-thresholding) must never occur.\nThe soft-thresholding phase is for $t = t^{\\dagger}, \\dots, T_f-1$. This range of iterations is an empty set if and only if $t^{\\dagger} > T_f-1$.\nSince $t^{\\dagger}$ must be an integer, the smallest integer value that satisfies this is $t^{\\dagger} = T_f$.\n\nFor the given problem, $T_f=5$. The iterations run from $t=0$ to $4$.\nThe optimal strategy is to use $\\eta_{\\text{MMSE}}$ for $t=0, 1, 2, 3, 4$.\nThe schedule uses $\\eta_{\\text{MMSE}}$ for $t=0, \\dots, t^{\\dagger}-1$.\nSetting $t^{\\dagger}=5$ gives the MMSE phase for $t=0, \\dots, 4$. The soft-thresholding phase becomes $t=5, \\dots, 4$, which is empty. This corresponds to the optimal strategy.\nThus, the switch time $t^{\\dagger}$ that minimizes the final MSE is $T_f = 5$.", "answer": "$$\\boxed{5}$$", "id": "3481473"}]}