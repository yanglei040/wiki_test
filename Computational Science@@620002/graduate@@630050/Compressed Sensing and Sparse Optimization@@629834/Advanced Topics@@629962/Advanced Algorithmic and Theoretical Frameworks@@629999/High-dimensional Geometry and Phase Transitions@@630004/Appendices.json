{"hands_on_practices": [{"introduction": "The success of compressed sensing hinges on ensuring the true sparse signal is the unique solution to the recovery program. This exercise explores the Exact Recovery Condition (ERC), a powerful tool that uses a \"dual certificate\" to verify this uniqueness. You will construct such a certificate and connect its existence to a tangible property of the sensing matrix—its mutual coherence—thereby deriving a concrete condition for guaranteed sparse recovery. [@problem_id:3451442]", "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ be a sensing matrix with columns $\\{a_{j}\\}_{j=1}^{n}$ normalized so that $\\|a_{j}\\|_{2}=1$ for all $j$, and mutual coherence $\\mu = \\max_{i \\neq j} |a_{i}^{\\top} a_{j}|$. Let $x \\in \\mathbb{R}^{n}$ be a $k$-sparse vector with support $S \\subset \\{1,\\dots,n\\}$ of size $|S|=k$, and define the sign vector $s = \\operatorname{sign}(x_{S}) \\in \\{-1,0,1\\}^{k}$ restricted to the support $S$. Consider the candidate dual certificate $y \\in \\mathbb{R}^{m}$ defined as the minimum Euclidean norm solution of the linear system $A_{S}^{\\top} y = s$, i.e., choose\n$$\ny = A_{S} \\left(A_{S}^{\\top} A_{S}\\right)^{-1} s.\n$$\nUsing only fundamental definitions and well-tested matrix norm bounds, and without invoking any specialized compressed sensing theorems, do the following:\n- Derive an upper bound on $\\|A_{S^{c}}^{\\top} y\\|_{\\infty}$ in terms of $k$ and $\\mu$ by appropriately bounding $\\left\\|\\left(A_{S}^{\\top} A_{S}\\right)^{-1}\\right\\|_{\\infty}$.\n- Explain why the strict inequality $\\|A_{S^{c}}^{\\top} y\\|_{\\infty}  1$ constitutes a sufficient Exact Recovery Condition (ERC) for Basis Pursuit (BP), relating your bound to this condition.\n- From your bound, determine the largest mutual coherence $\\mu$ (as a function of $k$) that guarantees the existence of such a dual certificate $y$ satisfying $\\|A_{S^{c}}^{\\top} y\\|_{\\infty}  1$ for every sign pattern $s \\in \\{-1,1\\}^{k}$.\n\nYour final answer must be a single closed-form analytical expression in terms of $k$. Do not provide an inequality. No rounding is required.", "solution": "The problem requires a three-part analysis concerning the recovery of a sparse signal using Basis Pursuit. We will address each part in sequence after validating the problem statement. The validation confirms that the problem is well-posed, scientifically grounded, and contains all necessary information for a rigorous solution.\n\n### Part 1: Derivation of the Upper Bound\n\nWe are asked to derive an upper bound on $\\|A_{S^{c}}^{\\top} y\\|_{\\infty}$ where $y = A_{S} (A_{S}^{\\top} A_{S})^{-1} s$.\nThe expression to be bounded is $\\|A_{S^{c}}^{\\top} A_{S} (A_{S}^{\\top} A_{S})^{-1} s\\|_{\\infty}$.\nWe use the sub-multiplicative property of compatible matrix and vector norms:\n$$\n\\|A_{S^{c}}^{\\top} A_{S} (A_{S}^{\\top} A_{S})^{-1} s\\|_{\\infty} \\le \\|A_{S^{c}}^{\\top} A_{S}\\|_{\\infty} \\|(A_{S}^{\\top} A_{S})^{-1}\\|_{\\infty} \\|s\\|_{\\infty}\n$$\nWe proceed by bounding each term on the right-hand side.\n\n**Bounding $\\|s\\|_{\\infty}$**:\nThe problem specifies considering sign patterns $s \\in \\{-1, 1\\}^k$. The vector $s$ thus has entries with magnitude equal to $1$. The infinity norm is the maximum absolute value of the components, so $\\|s\\|_{\\infty} = 1$.\n\n**Bounding $\\|A_{S^{c}}^{\\top} A_{S}\\|_{\\infty}$**:\nLet the matrix $C = A_{S^{c}}^{\\top} A_{S}$. This is a matrix of size $(n-k) \\times k$, where $k = |S|$. An entry $C_{ij}$ of this matrix corresponds to $a_i^{\\top} a_j$, where the column index $j$ is in the support set $S$ and the row index $i$ is in its complement $S^c$. Since $i \\in S^c$ and $j \\in S$, we always have $i \\neq j$. By the definition of mutual coherence $\\mu$, we have $|C_{ij}| = |a_i^{\\top} a_j| \\le \\mu$.\nThe infinity norm of a matrix is its maximum absolute row sum. For any row $i$ of $C$, corresponding to an index $i \\in S^c$:\n$$\n\\sum_{j \\in S} |C_{ij}| = \\sum_{j \\in S} |a_i^{\\top} a_j| \\le \\sum_{j \\in S} \\mu = k \\mu\n$$\nSince this holds for any row, the maximum absolute row sum is also bounded by this value. Therefore,\n$$\n\\|A_{S^{c}}^{\\top} A_{S}\\|_{\\infty} \\le k \\mu.\n$$\n\n**Bounding $\\|(A_{S}^{\\top} A_{S})^{-1}\\|_{\\infty}$**:\nLet $G = A_{S}^{\\top} A_{S}$. This is a $k \\times k$ Gram matrix. Its diagonal entries are $G_{jj} = a_j^{\\top} a_j = \\|a_j\\|_2^2 = 1$ for $j \\in S$. Its off-diagonal entries are $G_{ij} = a_i^{\\top} a_j$ for $i, j \\in S, i \\neq j$. The magnitude of these off-diagonal entries is bounded by $\\mu$, i.e., $|G_{ij}| \\le \\mu$.\nWe can write $G = I + H$, where $I$ is the $k \\times k$ identity matrix and $H$ is a matrix with zeros on its diagonal ($H_{jj}=0$) and off-diagonal entries $H_{ij} = G_{ij}$ for $i \\neq j$.\nThe infinity norm of $H$ is its maximum absolute row sum:\n$$\n\\|H\\|_{\\infty} = \\max_{i \\in S} \\sum_{j \\in S, j \\neq i} |H_{ij}| \\le \\max_{i \\in S} \\sum_{j \\in S, j \\neq i} \\mu\n$$\nIn each row, there are $k-1$ off-diagonal terms. Thus, $\\|H\\|_{\\infty} \\le (k-1)\\mu$.\nIf $\\|H\\|_{\\infty}  1$, which requires $(k-1)\\mu  1$, the matrix $G = I+H$ is invertible. Its inverse can be bounded using the Neumann series expansion. A standard result for matrix norms states that if $\\|H\\|_{\\infty}  1$, then $\\|(I+H)^{-1}\\|_{\\infty} \\le \\frac{1}{1-\\|H\\|_{\\infty}}$.\nApplying this result, we obtain:\n$$\n\\|(A_{S}^{\\top} A_{S})^{-1}\\|_{\\infty} = \\|G^{-1}\\|_{\\infty} \\le \\frac{1}{1-\\|H\\|_{\\infty}} \\le \\frac{1}{1-(k-1)\\mu}\n$$\nThis bound is valid under the condition $\\mu  1/(k-1)$.\n\n**Combining the Bounds**:\nSubstituting the individual bounds back into the original inequality, we find:\n$$\n\\|A_{S^{c}}^{\\top} y\\|_{\\infty} \\le (k\\mu) \\left( \\frac{1}{1-(k-1)\\mu} \\right) (1) = \\frac{k\\mu}{1-(k-1)\\mu}\n$$\nThis is the derived upper bound for $\\|A_{S^{c}}^{\\top} y\\|_{\\infty}$ in terms of $k$ and $\\mu$.\n\n### Part 2: Explanation of the Exact Recovery Condition (ERC)\n\nBasis Pursuit (BP) finds the sparsest solution to an underdetermined system of linear equations by solving the $\\ell_1$-norm minimization problem:\n$$\n\\min_{z \\in \\mathbb{R}^n} \\|z\\|_1 \\quad \\text{subject to} \\quad Az = b\n$$\nwhere $b=Ax$ for some true $k$-sparse signal $x$. The goal is to show that $x$ is the unique solution to this problem.\nThe optimality conditions for this convex optimization problem (the Karush-Kuhn-Tucker or KKT conditions) state that a feasible point $x$ is optimal if and only if there exists a dual vector (Lagrange multiplier) $y^* \\in \\mathbb{R}^m$ such that the subgradient of the Lagrangian with respect to $z$ at $z=x$ contains the zero vector. The Lagrangian is $L(z, y^*) = \\|z\\|_1 + (y^*)^\\top(b-Az)$. The subgradient condition translates to $A^\\top y^* \\in \\partial \\|x\\|_1$, where $\\partial \\|x\\|_1$ is the subdifferential of the $\\ell_1$-norm at $x$.\nThe subdifferential of $\\|z\\|_1$ at $z=x$ is the set of vectors $v$ such that:\n$$\nv_j = \\begin{cases} \\operatorname{sign}(x_j)  \\text{if } j \\in S \\text{ (where } x_j \\neq 0) \\\\ v_j \\in [-1, 1]  \\text{if } j \\in S^c \\text{ (where } x_j = 0) \\end{cases}\n$$\nFor $x$ to be the unique solution, a stricter condition is required: the subgradient must be unique on the inactive set $S^c$. This leads to the Exact Recovery Condition (ERC), which is the existence of a dual vector $y$ that satisfies:\n1. $A_j^{\\top} y = \\operatorname{sign}(x_j)$ for all $j \\in S$. This can be written compactly as $A_S^\\top y = \\operatorname{sign}(x_S)$.\n2. $|A_j^{\\top} y|  1$ for all $j \\in S^c$. This is equivalent to $\\|A_{S^c}^\\top y\\|_{\\infty}  1$.\n\nThe problem defines a specific candidate dual certificate $y = A_{S} (A_{S}^{\\top} A_{S})^{-1} s$, with $s = \\operatorname{sign}(x_S)$. Let's verify the first condition:\n$$\nA_S^\\top y = A_S^\\top \\left( A_S (A_S^\\top A_S)^{-1} s \\right) = (A_S^\\top A_S) (A_S^\\top A_S)^{-1} s = s\n$$\nThe construction of $y$ is precisely to satisfy the first condition of the ERC. Therefore, the sufficiency of the ERC for unique recovery reduces to verifying that the second condition, $\\|A_{S^c}^\\top y\\|_{\\infty}  1$, holds for this choice of $y$.\n\n### Part 3: Determination of the Largest Mutual Coherence\n\nTo guarantee exact recovery for any $k$-sparse signal, the condition $\\|A_{S^{c}}^{\\top} y\\|_{\\infty}  1$ must hold for any support set $S$ of size $k$ and any sign pattern $s \\in \\{-1,1\\}^k$. Using the bound derived in Part 1, a sufficient condition to ensure this is:\n$$\n\\frac{k\\mu}{1-(k-1)\\mu}  1\n$$\nFor this inequality to be meaningful, the denominator must be positive, which implies $1 - (k-1)\\mu  0$, or $\\mu  \\frac{1}{k-1}$.\nAssuming the denominator is positive, we can multiply both sides by it without changing the direction of the inequality:\n$$\nk\\mu  1 - (k-1)\\mu\n$$\n$$\nk\\mu + (k-1)\\mu  1\n$$\n$$\n(2k-1)\\mu  1\n$$\n$$\n\\mu  \\frac{1}{2k-1}\n$$\nThis condition is stricter than the condition $\\mu  \\frac{1}{k-1}$ for any $k  1$. Thus, if $\\mu  \\frac{1}{2k-1}$, the bound holds and the ERC is satisfied.\nThe question asks for the largest mutual coherence $\\mu$ that guarantees the existence of the dual certificate $y$ satisfying the ERC. The derived condition, $\\mu  \\frac{1}{2k-1}$, provides a uniform guarantee for all $k$-sparse signals. The limit on $\\mu$ is the supremum of this interval. Therefore, the largest value of $\\mu$ is the boundary of this condition.", "answer": "$$\\boxed{\\frac{1}{2k-1}}$$", "id": "3451442"}, {"introduction": "While deterministic conditions like mutual coherence provide strong guarantees, they are often too pessimistic for the random matrices used in practice. This exercise transitions to the probabilistic viewpoint, where recovery is understood as a geometric phenomenon. You will derive the famous Donoho-Tanner phase transition curve from first principles by calculating the statistical dimension of the $\\ell_1$ descent cone, providing a precise prediction for when recovery succeeds in the high-dimensional limit. [@problem_id:3451344]", "problem": "Consider a compressed sensing model with measurement matrix $A \\in \\mathbb{R}^{m \\times n}$ whose entries are independent and identically distributed as $\\mathcal{N}(0,1/m)$. Let $x_{0} \\in \\mathbb{R}^{n}$ be a $k$-sparse vector, with sparsity fraction $\\rho = k/n \\in (0,1)$, and define the sampling ratio $\\delta = m/n \\in (0,1)$. Recovery is attempted by $\\ell_{1}$-minimization: minimize $\\|x\\|_{1}$ subject to $Ax = y$, where $y = Ax_{0}$. In the high-dimensional limit $n \\to \\infty$ with fixed $\\rho$, the Donoho–Tanner phase transition is characterized by a critical sampling ratio $\\delta_{c}(\\rho)$ such that exact recovery occurs with probability approaching one when $\\delta  \\delta_{c}(\\rho)$ and fails when $\\delta  \\delta_{c}(\\rho)$.\n\nUsing only foundational principles appropriate to convex geometry and random projections—namely, the conic kinematic formula for random subspaces, the definition of the descent cone of a convex function at a point, and the definition of statistical dimension—derive the phase transition curve $\\delta_{c}(\\rho)$ for $\\ell_{1}$-minimization at a $k$-sparse $x_{0}$. Your derivation should begin from first principles: characterize the descent cone of the $\\ell_{1}$ norm at $x_{0}$, connect recovery with the nullspace of $A$, and invoke the conic kinematic viewpoint to identify the threshold in terms of the statistical dimension of the descent cone. Then, compute the statistical dimension by reducing it to a one-dimensional optimization over a scalar parameter introduced through an appropriate scaling of the subdifferential, and perform the necessary Gaussian expectations explicitly.\n\nYour final answer must be a single closed-form analytic expression for $\\delta_{c}(\\rho)$ expressed in terms of a one-dimensional infimum involving the standard normal density $\\varphi(t) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-t^{2}/2)$ and the standard normal upper tail $Q(t) = \\int_{t}^{\\infty} \\varphi(u)\\,du$. No numerical approximation is required. Provide only the expression for $\\delta_{c}(\\rho)$ as your final answer.", "solution": "The problem is valid as it is a standard, well-posed problem in the mathematical theory of compressed sensing. It is scientifically grounded, objective, and contains all necessary information for a rigorous derivation.\n\nThe problem asks for the phase transition curve $\\delta_c(\\rho)$ for the successful recovery of a $k$-sparse vector $x_0 \\in \\mathbb{R}^n$ from measurements $y = A x_0$ via $\\ell_1$-minimization. The matrix $A \\in \\mathbb{R}^{m \\times n}$ has i.i.d. $\\mathcal{N}(0, 1/m)$ entries. The sparsity is $\\rho = k/n$ and the sampling ratio is $\\delta = m/n$.\n\nThe recovery algorithm solves:\n$$\n\\min_{x \\in \\mathbb{R}^n} \\|x\\|_1 \\quad \\text{subject to} \\quad Ax = y.\n$$\nSince $y = Ax_0$, the true signal $x_0$ is a feasible point. For $x_0$ to be the unique solution, it must be the case that for any other feasible point $x \\neq x_0$, we have $\\|x\\|_1  \\|x_0\\|_1$.\nAny feasible point $x$ can be written as $x = x_0 + h$ for some non-zero vector $h$. The constraint $Ax=Ax_0$ implies $A(x_0+h)=Ax_0$, which simplifies to $Ah=0$. Thus, the perturbation vector $h$ must lie in the nullspace of $A$, denoted $\\text{Null}(A)$.\n\nThe condition for successful recovery is that for all $h \\in \\text{Null}(A) \\setminus \\{0\\}$, we must have $\\|x_0+h\\|_1  \\|x_0\\|_1$. This means that no non-zero vector in the nullspace of $A$ can be a descent direction for the $\\ell_1$-norm at $x_0$.\n\nThe set of descent directions for a convex function $f(x)$ at a point $x_0$ is the cone $\\mathcal{D}(f, x_0) = \\{ h \\in \\mathbb{R}^n : f'(x_0; h)  0 \\}$, where $f'(x_0; h)$ is the directional derivative. For $f(x) = \\|x\\|_1$, the directional derivative at $x_0$ in the direction $h$ is given by\n$$\nf'(x_0; h) = \\sum_{i \\in S} h_i \\text{sign}((x_0)_i) + \\sum_{j \\in S^c} |h_j|,\n$$\nwhere $S = \\text{supp}(x_0)$ is the support of $x_0$ of size $k$, and $S^c$ is its complement of size $n-k$. Let $s \\in \\mathbb{R}^k$ be the vector of signs of the non-zero entries of $x_0$. Then the descent cone is:\n$$\n\\mathcal{D}(\\|\\cdot\\|_1, x_0) = \\left\\{ h \\in \\mathbb{R}^n : \\langle s, h_S \\rangle + \\|h_{S^c}\\|_1  0 \\right\\},\n$$\nwhere $h_S$ and $h_{S^c}$ are the subvectors of $h$ corresponding to the index sets $S$ and $S^c$.\n\nThe recovery condition is that the intersection of the descent cone and the nullspace of $A$ contains only the zero vector:\n$$\n\\overline{\\mathcal{D}}(\\|\\cdot\\|_1, x_0) \\cap \\text{Null}(A) = \\{0\\}.\n$$\nWe use the closure of the cone, $\\overline{\\mathcal{D}}$, as it does not change the probability of intersection for a random subspace.\n\nIn the high-dimensional limit ($n \\to \\infty$) with a random Gaussian matrix $A$, the theory of random projections and convex geometry (specifically, the conic kinematic formula or Gordon's escape-through-the-mesh theorem) states that this condition holds with high probability if and only if the sum of the dimensions of the cone and the subspace is less than the ambient dimension $n$. The \"dimension\" of the cone is its statistical dimension $\\delta(C)$. The dimension of $\\text{Null}(A)$ is $n-m$. The phase transition boundary is where the inequality becomes tight:\n$$\n\\delta(\\overline{\\mathcal{D}}) + \\dim(\\text{Null}(A)) = n \\implies \\delta(\\overline{\\mathcal{D}}) + (n-m) = n \\implies \\delta(\\overline{\\mathcal{D}}) = m.\n$$\nDividing by $n$, the critical sampling ratio $\\delta_c = m/n$ is given by the normalized statistical dimension of the descent cone:\n$$\n\\delta_c(\\rho) = \\lim_{n \\to \\infty} \\frac{\\delta(\\overline{\\mathcal{D}})}{n}.\n$$\nThe statistical dimension of a cone $C$ can be calculated using its polar cone $C^\\circ$ via the relation $\\delta(C) + \\delta(C^\\circ) = n$. Another key formula is $\\delta(C) = \\mathbb{E}_{g \\sim \\mathcal{N}(0, I_n)}[\\text{dist}(g, C^\\circ)^2]$. We will use the latter.\n\nFirst, we characterize the polar cone $\\mathcal{D}^\\circ$ (we drop the bar for simplicity).\n$$\n\\mathcal{D}^\\circ = \\{ v \\in \\mathbb{R}^n : \\langle v, h \\rangle \\le 0, \\forall h \\in \\overline{\\mathcal{D}} \\}.\n$$\nA vector $v$ belongs to $\\mathcal{D}^\\circ$ if and only if it can be written as $v_S = t s$ and $\\|v_{S^c}\\|_\\infty \\le t$ for some scalar $t \\ge 0$. Here, $v_S$ and $v_{S^c}$ are the subvectors of $v$.\n\nThe statistical dimension is $\\delta(\\mathcal{D}) = \\mathbb{E}_g[ \\text{dist}(g, \\mathcal{D}^\\circ)^2 ] = \\mathbb{E}_g[ \\min_{v \\in \\mathcal{D}^\\circ} \\|g - v\\|_2^2 ]$. Let $g=(g_S, g_{S^c})$ be a standard Gaussian vector. We solve the minimization problem:\n$$\n\\min_{t \\ge 0, v_{S^c}: \\|v_{S^c}\\|_\\infty \\le t} \\left( \\|g_S - ts\\|_2^2 + \\|g_{S^c} - v_{S^c}\\|_2^2 \\right).\n$$\nFor a fixed $t \\ge 0$, the minimization over $v_{S^c}$ decouples coordinate-wise. For each $j \\in S^c$, we solve $\\min_{|v_j| \\le t} (g_j - v_j)^2$. The solution is $v_j = \\text{sign}(g_j) \\min(|g_j|, t)$. The squared error is $(|g_j|-t)^2$ if $|g_j|t$ and $0$ otherwise.\nThe problem reduces to a one-dimensional minimization over $t \\ge 0$:\n$$\n\\text{dist}(g, \\mathcal{D}^\\circ)^2 = \\min_{t \\ge 0} \\left( \\|g_S - ts\\|_2^2 + \\sum_{j \\in S^c} (\\max\\{|g_j|-t, 0\\})^2 \\right).\n$$\nLet $L(t,g) = \\|g_S - ts\\|_2^2 + \\sum_{j \\in S^c, |g_j|t} (|g_j|-t)^2$. The normalized phase transition threshold is\n$$\n\\delta_c(\\rho) = \\lim_{n \\to \\infty} \\frac{1}{n} \\mathbb{E}_g[\\min_{t \\ge 0} L(t,g)].\n$$\nIn the high-dimensional limit, we can exchange the expectation and minimization:\n$$\n\\delta_c(\\rho) = \\min_{t \\ge 0} \\lim_{n \\to \\infty} \\frac{1}{n} \\mathbb{E}_g[L(t,g)].\n$$\nLet's compute the expectation. $\\|g_S - ts\\|_2^2 = \\|g_S\\|_2^2 - 2t\\langle g_S, s \\rangle + t^2 \\|s\\|_2^2$.\n$\\mathbb{E}[\\|g_S\\|_2^2] = k$. $\\mathbb{E}[\\langle g_S, s \\rangle] = 0$. $\\|s\\|_2^2 = k$.\nThus, $\\mathbb{E}[\\|g_S - ts\\|_2^2] = k + kt^2$.\nFor the second term, by linearity of expectation:\n$$\n\\mathbb{E}\\left[\\sum_{j \\in S^c, |g_j|t} (|g_j|-t)^2\\right] = (n-k) \\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)}[ (\\max\\{|Z|-t, 0\\})^2 ].\n$$\nLet's compute the expectation over a standard normal variable $Z$:\n$$\n\\mathbb{E}[ (\\max\\{|Z|-t, 0\\})^2 ] = \\int_{-\\infty}^\\infty (\\max\\{|z|-t, 0\\})^2 \\varphi(z) dz = 2 \\int_t^\\infty (z-t)^2 \\varphi(z) dz.\n$$\nWe compute this integral by parts:\n$2 \\int_t^\\infty (z^2 - 2zt + t^2) \\varphi(z) dz = 2 \\left( \\int_t^\\infty z^2\\varphi(z)dz - 2t\\int_t^\\infty z\\varphi(z)dz + t^2\\int_t^\\infty \\varphi(z)dz \\right)$.\nUsing the identities $\\int_t^\\infty \\varphi(z)dz = Q(t)$, $\\int_t^\\infty z\\varphi(z)dz = \\varphi(t)$, and $\\int_t^\\infty z^2\\varphi(z)dz = t\\varphi(t) + Q(t)$, we get:\n$2 \\left( (t\\varphi(t) + Q(t)) - 2t\\varphi(t) + t^2Q(t) \\right) = 2 \\left( (1+t^2)Q(t) - t\\varphi(t) \\right)$.\n\nNow, we assemble the normalized expectation of $L(t,g)$:\n$$\n\\frac{1}{n} \\mathbb{E}_g[L(t,g)] = \\frac{1}{n} \\left( k+kt^2 + (n-k) \\cdot 2 \\left( (1+t^2)Q(t) - t\\varphi(t) \\right) \\right).\n$$\nTaking the limit as $n\\to\\infty$, with $k/n \\to \\rho$ and $(n-k)/n \\to 1-\\rho$:\n$$\n\\lim_{n \\to \\infty} \\frac{1}{n} \\mathbb{E}_g[L(t,g)] = \\rho(1+t^2) + (1-\\rho) \\cdot 2 \\left( (1+t^2)Q(t) - t\\varphi(t) \\right).\n$$\nThe phase transition curve is the infimum of this expression over $t \\ge 0$.\n$$\n\\delta_c(\\rho) = \\inf_{t \\ge 0} \\left\\{ \\rho(1+t^2) + 2(1-\\rho) \\left( (1+t^2)Q(t) - t\\varphi(t) \\right) \\right\\}.\n$$\nThis expression matches the requirement of the problem statement. The parameter $t$ is the scalar over which the optimization is performed.", "answer": "$$\n\\boxed{\\inf_{t \\ge 0} \\left\\{ \\rho(1+t^{2}) + 2(1-\\rho) \\left( (1+t^{2})Q(t) - t\\varphi(t) \\right) \\right\\}}\n$$", "id": "3451344"}, {"introduction": "The theoretical phase transition curve is typically derived for Gaussian measurement matrices, but its remarkable accuracy extends to many other types of random matrices—a phenomenon known as universality. This hands-on coding practice challenges you to test this principle empirically. By simulating recovery experiments and comparing the phase transitions for Gaussian and structured random Fourier ensembles, you will gain insight into this deep concept and develop practical skills in computational verification. [@problem_id:3451296]", "problem": "Consider the compressed sensing model in which an unknown vector $x_0 \\in \\mathbb{R}^n$ is $k$-sparse, meaning it has at most $k$ nonzero entries. Measurements $y \\in \\mathbb{C}^m$ are obtained through a linear operator $A \\in \\mathbb{C}^{m \\times n}$, with $y = A x_0$. Recovery is performed via the basis pursuit program that minimizes the $\\ell_1$ norm subject to exact measurement constraints. The geometry of high-dimensional convex sets suggests that the success or failure of $\\ell_1$ recovery exhibits a sharp transition in the plane of undersampling ratio and sparsity ratio, commonly summarized by a phase transition curve $\\delta_c(\\rho)$, where $\\delta = m/n$ and $\\rho = k/n$. This curve is conjectured to be universal across a class of measurement ensembles after suitable randomization.\n\nStarting from fundamental definitions and well-tested facts:\n- Sparse recovery via $\\ell_1$ minimization is defined as the convex optimization problem $\\min \\|x\\|_1$ subject to $A x = y$, where $\\|\\cdot\\|_1$ denotes the sum of absolute values of components.\n- Universality in compressed sensing posits that the empirical phase transition behavior is largely determined by geometric properties of random polytopes and is largely independent of the specific distribution of the measurement matrix, provided certain incoherence and isotropy conditions hold.\n- The Discrete Fourier Transform (DFT) matrix $F \\in \\mathbb{C}^{n \\times n}$ is defined by $F_{j,k} = \\exp\\!\\left(-2\\pi i \\frac{jk}{n}\\right)/\\sqrt{n}$, with $i$ denoting the imaginary unit. A partial Fourier subsampling matrix is formed by selecting $m$ rows uniformly at random from $F$. Random modulation is applied via a diagonal matrix $R \\in \\mathbb{R}^{n \\times n}$ where diagonal entries are independent Rademacher variables (each entry equal to $+1$ or $-1$ with equal probability), yielding $A = P F R$, where $P$ denotes row selection.\n\nYour task is to implement a program that empirically tests universality by comparing the estimated phase transition between two measurement ensembles:\n1. A Gaussian ensemble where entries of $A$ are independent and identically distributed normal random variables with mean $0$ and variance $1/m$, denoted $A_{ij} \\sim \\mathcal{N}(0, 1/m)$.\n2. A randomly modulated partial Fourier ensemble where $A = P F R$ as defined above, with $F$ orthonormal.\n\nFor each test case, do the following:\n- Fix $n$ and $k$. For each candidate $m$ in a provided list, perform $T$ independent trials. In each trial:\n  - Sample a $k$-sparse $x_0 \\in \\mathbb{R}^n$ with support chosen uniformly at random and nonzero values drawn independently from a standard normal distribution $\\mathcal{N}(0,1)$.\n  - Form measurements $y = A x_0$ for each ensemble separately.\n  - Solve the basis pursuit problem $\\min \\|x\\|_1$ subject to $A x = y$ via an equivalent linear programming formulation using the variable split $x = u - v$ with $u \\ge 0$, $v \\ge 0$.\n  - Declare success if the relative $\\ell_2$-error $\\|x^\\star - x_0\\|_2 / \\max(\\|x_0\\|_2, \\varepsilon)$ is at most a specified tolerance, where $x^\\star$ is the recovered solution and $\\varepsilon$ is a small positive number to avoid division by zero.\n- For each ensemble, compute the empirical success rate for each $m$ as the fraction of trials that succeed.\n- For each ensemble, estimate $\\delta_c$ at the given $k$ by selecting the smallest $\\delta = m/n$ among the candidate $m$ values whose success rate is at least a specified threshold $q$. If no candidate $m$ reaches the threshold, declare that $\\delta_c$ is undefined.\n- Return a boolean result for each test case indicating whether the absolute difference between the estimated critical undersampling ratios for the two ensembles, $|\\delta_c^{\\mathrm{Gauss}} - \\delta_c^{\\mathrm{Fourier}}|$, is less than or equal to a specified tolerance $\\epsilon$. If one or both $\\delta_c$ values are undefined, return false for that test case.\n\nAlgorithmic constraints:\n- The basis pursuit problem must be solved via a linear program by introducing $u, v \\in \\mathbb{R}^n$, minimizing $\\mathbf{1}^\\top(u + v)$ subject to $A(u - v) = y$, $u \\ge 0$, $v \\ge 0$. For complex-valued $A$ and $y$, enforce the equality constraints by splitting into real and imaginary parts: $\\operatorname{Re}(A)(u - v) = \\operatorname{Re}(y)$ and $\\operatorname{Im}(A)(u - v) = \\operatorname{Im}(y)$.\n\nSuccess criterion:\n- Use the relative error tolerance $\\tau = 10^{-6}$ and the denominator stabilizer $\\varepsilon = 10^{-12}$.\n\nTest suite:\n- Case $1$: $n = 64$, $k = 4$, candidate $m$ values $[12, 14, 16, 18, 20, 22, 24]$, number of trials $T = 8$, success threshold $q = 0.75$, tolerance $\\epsilon = 0.15$.\n- Case $2$: $n = 96$, $k = 8$, candidate $m$ values $[24, 28, 32, 36, 40, 44]$, number of trials $T = 6$, success threshold $q = 0.67$, tolerance $\\epsilon = 0.20$.\n- Case $3$: $n = 128$, $k = 16$, candidate $m$ values $[36, 44, 52, 60, 68, 76, 84]$, number of trials $T = 5$, success threshold $q = 0.60$, tolerance $\\epsilon = 0.20$.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where each result is a boolean corresponding to the test cases in the order listed above.\n\nAll answers are unitless real numbers and booleans; no physical units are involved. Angles, if any, are to be interpreted in radians, but none are required in this task.", "solution": "The problem asks for an empirical investigation of the universality conjecture in compressed sensing. This conjecture posits that the phase transition behavior of sparse signal recovery via $\\ell_1$-minimization is largely independent of the specific distribution of the random measurement matrix, provided certain general conditions are met. We are to compare the phase transition points for two common matrix ensembles: a complex Gaussian ensemble and a randomly modulated partial Fourier ensemble.\n\n### Theoretical Framework\n\nThe core problem in compressed sensing is to recover a $k$-sparse signal $x_0 \\in \\mathbb{R}^n$ (a vector with at most $k$ non-zero elements) from a set of linear measurements $y = A x_0$. The measurement matrix $A$ is an $m \\times n$ matrix, where $m  n$, making the system of equations underdetermined.\n\nRecovery is often achieved by solving the basis pursuit convex optimization problem:\n$$ \\min_{x \\in \\mathbb{R}^n} \\|x\\|_1 \\quad \\text{subject to} \\quad Ax = y $$\nwhere $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$ is the $\\ell_1$-norm. This program succeeds in recovering $x_0$ exactly if the number of measurements $m$ is sufficiently large relative to the sparsity $k$ and the signal dimension $n$.\n\nThe success of recovery exhibits a sharp phase transition. For a fixed signal structure, as the undersampling ratio $\\delta = m/n$ increases, the probability of successful recovery transitions rapidly from near $0$ to near $1$. The critical value $\\delta_c$ at which this transition occurs depends on the sparsity ratio. The phase transition curve is a function $\\delta_c(\\rho)$ in the $(\\rho, \\delta)$ plane, where $\\rho=k/n$ and $\\delta=m/n$. The universality conjecture states that this curve is the same for a wide class of random matrix ensembles.\n\n### Methodology and Algorithmic Design\n\nOur task is to estimate and compare the critical undersampling ratio $\\delta_c$ for two ensembles.\n\n1.  **Gaussian Ensemble**: The matrix $A_{\\text{Gauss}} \\in \\mathbb{C}^{m \\times n}$ has entries that are independent and identically distributed. The problem specifies $A_{ij} \\sim \\mathcal{N}(0, 1/m)$. For a complex-valued matrix, this is standardly interpreted as entries being complex normal variables, $A_{ij} \\sim \\mathcal{CN}(0, 1/m)$. This means $A_{ij} = U + iV$ where $U, V$ are i.i.d. real Gaussian random variables $\\mathcal{N}(0, 1/(2m))$. The variance is $\\mathbb{E}[|A_{ij}|^2] = \\mathbb{E}[U^2 + V^2] = \\frac{1}{2m} + \\frac{1}{2m} = \\frac{1}{m}$, consistent with the problem statement.\n\n2.  **Randomly Modulated Partial Fourier Ensemble**: The matrix is constructed as $A_{\\text{Fourier}} = PFR$.\n    -   $F \\in \\mathbb{C}^{n \\times n}$ is the orthonormal Discrete Fourier Transform (DFT) matrix, with entries $F_{j,k} = \\frac{1}{\\sqrt{n}} \\exp(-2\\pi i \\frac{jk}{n})$ for $j, k \\in \\{0, \\dots, n-1\\}$.\n    -   $R \\in \\mathbb{R}^{n \\times n}$ is a diagonal matrix whose diagonal entries are independent Rademacher random variables (i.e., $\\pm 1$ with probability $1/2$). This random modulation is crucial for ensuring incoherence.\n    -   $P$ is a projection operator that selects $m$ rows of $FR$ uniformly at random.\n\nThe experimental procedure for a given test case $(n, k, \\{m\\}_{\\text{cand}}, T, q, \\epsilon)$ is as follows:\nFor each candidate number of measurements $m \\in \\{m\\}_{\\text{cand}}$, we perform $T$ independent trials. In each trial:\n- A $k$-sparse signal $x_0 \\in \\mathbb{R}^n$ is generated. Its support (the set of indices of non-zero entries) is chosen uniformly at random, and the non-zero values are drawn from a standard normal distribution $\\mathcal{N}(0,1)$.\n- For each ensemble, the measurement matrix $A$ is generated, and the measurement vector $y = Ax_0$ is computed.\n- The basis pursuit problem is solved to obtain a recovered signal $x^\\star$.\n- Success is declared if the relative $\\ell_2$-error is below a tolerance $\\tau = 10^{-6}$:\n  $$ \\frac{\\|x^\\star - x_0\\|_2}{\\max(\\|x_0\\|_2, \\varepsilon)} \\le \\tau $$\n  with $\\varepsilon = 10^{-12}$ to prevent division by zero.\n\nThe empirical success rate for each $m$ is the fraction of successful trials. The critical undersampling ratio $\\delta_c$ is estimated as $m_c/n$, where $m_c$ is the smallest candidate $m$ for which the success rate is at least the threshold $q$. If no $m$ achieves this rate, $\\delta_c$ is considered undefined for that ensemble.\n\nFinally, universality is tested by checking if the absolute difference between the estimated critical ratios is within a tolerance $\\epsilon$: $|\\delta_c^{\\mathrm{Gauss}} - \\delta_c^{\\mathrm{Fourier}}| \\le \\epsilon$. The test fails if this condition is not met or if either $\\delta_c$ is undefined.\n\n### Basis Pursuit as a Linear Program\n\nThe $\\ell_1$-minimization problem is solved by reformulating it as a linear program (LP). We use the variable splitting technique by representing $x \\in \\mathbb{R}^n$ as the difference of two non-negative vectors, $x = u - v$, where $u,v \\in \\mathbb{R}^n$ and $u,v \\ge 0$. The $\\ell_1$-norm $\\|x\\|_1$ is then equivalent to minimizing $\\mathbf{1}^\\top(u+v)$, where $\\mathbf{1}$ is a vector of ones. This holds because for any optimal solution, for each component $i$, at least one of $u_i$ or $v_i$ must be zero.\n\nThe basis pursuit problem becomes:\n$$ \\min_{u,v \\in \\mathbb{R}^n} \\mathbf{1}^\\top u + \\mathbf{1}^\\top v \\quad \\text{subject to} \\quad A(u-v) = y, \\ u \\ge 0, \\ v \\ge 0. $$\nSince the matrix $A$ and vector $y$ are complex, $A = \\operatorname{Re}(A) + i\\operatorname{Im}(A)$ and $y = \\operatorname{Re}(y) + i\\operatorname{Im}(y)$, the single complex equality constraint $A(u-v) = y$ is equivalent to two real equality constraints:\n$$ \\operatorname{Re}(A)(u-v) = \\operatorname{Re}(y) $$\n$$ \\operatorname{Im}(A)(u-v) = \\operatorname{Im}(y) $$\nThis system is ready to be solved by a standard LP solver. We define a new variable vector $z = [u^\\top, v^\\top]^\\top \\in \\mathbb{R}^{2n}$. The LP is then:\n- **Minimize:** $c^\\top z$, where $c = \\mathbf{1}_{2n} \\in \\mathbb{R}^{2n}$.\n- **Subject to:** $A_{eq} z = b_{eq}$ and $z \\ge 0$.\nThe equality constraint matrix $A_{eq} \\in \\mathbb{R}^{2m \\times 2n}$ and vector $b_{eq} \\in \\mathbb{R}^{2m}$ are constructed as:\n$$ A_{eq} = \\begin{pmatrix} \\operatorname{Re}(A)  -\\operatorname{Re}(A) \\\\ \\operatorname{Im}(A)  -\\operatorname{Im}(A) \\end{pmatrix}, \\quad b_{eq} = \\begin{pmatrix} \\operatorname{Re}(y) \\\\ \\operatorname{Im}(y) \\end{pmatrix} $$\nThe solution of the LP, $z^\\star = [(u^\\star)^\\top, (v^\\star)^\\top]^\\top$, gives the recovered signal $x^\\star = u^\\star - v^\\star$. This procedure is implemented for each trial to find $x^\\star$ and evaluate recovery success.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\nfrom scipy.fft import fft\n\ndef solve_bp_complex(A, y, n):\n    \"\"\"\n    Solves the basis pursuit problem min ||x||_1 s.t. Ax = y for complex A and y\n    using a linear programming formulation.\n    \"\"\"\n    m = A.shape[0]\n\n    # LP variable is z = [u, v] where x = u - v, u, v = 0. Size is 2n.\n    # Objective function: min 1_T * u + 1_T * v, so c = [1, 1, ..., 1].\n    c = np.ones(2 * n)\n\n    # Equality constraints: A(u - v) = y\n    # This splits into real and imaginary parts:\n    # Re(A)(u - v) = Re(y)\n    # Im(A)(u - v) = Im(y)\n    # A_eq * z = b_eq, where z = [u, v]^T\n    A_re = np.real(A)\n    A_im = np.imag(A)\n    \n    A_eq = np.block([\n        [A_re, -A_re],\n        [A_im, -A_im]\n    ])\n\n    b_eq = np.concatenate([np.real(y), np.imag(y)])\n    \n    # All variables are non-negative\n    bounds = (0, None)\n\n    # Solve the linear program\n    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs', options={'presolve': True})\n\n    if res.success:\n        z = res.x\n        u = z[:n]\n        v = z[n:]\n        x_rec = u - v\n        return x_rec\n    else:\n        # Return a vector of NaNs to indicate failure, which will fail the success check.\n        return np.full(n, np.nan)\n\ndef check_success(x_rec, x_0, tau, eps_denom):\n    \"\"\"\n    Checks if the recovery was successful based on relative l2 error.\n    \"\"\"\n    if np.any(np.isnan(x_rec)):\n        return False\n    norm_x0 = np.linalg.norm(x_0)\n    error_norm = np.linalg.norm(x_rec - x_0)\n    relative_error = error_norm / max(norm_x0, eps_denom)\n    return relative_error = tau\n\ndef estimate_critical_delta(ensemble_type, n, k, m_candidates, T, q, tau, eps_denom, rng):\n    \"\"\"\n    Estimates the critical undersampling ratio delta_c for a given ensemble.\n    \"\"\"\n    # Pre-compute the orthonormal DFT matrix for the Fourier case\n    if ensemble_type == 'fourier':\n        F = fft(np.eye(n), norm='ortho', axis=0)\n\n    success_rates = {}\n\n    for m in m_candidates:\n        success_count = 0\n        for _ in range(T):\n            # 1. Generate k-sparse signal x_0\n            x_0 = np.zeros(n)\n            support = rng.choice(n, k, replace=False)\n            x_0[support] = rng.standard_normal(k)\n\n            # 2. Generate measurement matrix A and measurements y\n            if ensemble_type == 'gaussian':\n                # A_ij ~ CN(0, 1/m). Real and imag parts are N(0, 1/(2m)).\n                A = (rng.standard_normal((m, n)) + 1j * rng.standard_normal((m, n))) / np.sqrt(2 * m)\n            elif ensemble_type == 'fourier':\n                # A = PFR\n                signs = rng.choice([-1.0, 1.0], size=n)\n                R_mult = signs # For efficient multiplication\n                \n                rows = rng.choice(n, m, replace=False)\n                # A = (F @ np.diag(signs))[rows, :] is inefficient\n                # A = F_rows @ diag(signs) = F_rows * signs (broadcast)\n                A = F[rows, :] * R_mult\n            \n            y = A @ x_0\n\n            # 3. Solve for x_rec\n            x_rec = solve_bp_complex(A, y, n)\n\n            # 4. Check for success\n            if check_success(x_rec, x_0, tau, eps_denom):\n                success_count += 1\n        \n        success_rates[m] = success_count / T\n    \n    # 5. Estimate delta_c\n    delta_c = float('nan') # Using NaN to represent 'undefined'\n    sorted_m = sorted(m_candidates)\n    for m in sorted_m:\n        if success_rates[m] = q:\n            delta_c = m / n\n            break # Found the smallest m that meets the threshold\n            \n    return delta_c\n\ndef run_single_test_case(n, k, m_candidates, T, q, epsilon, tau, eps_denom, seed):\n    \"\"\"\n    Runs the full comparison for a single test case.\n    \"\"\"\n    # Use a specific seed for each test case for reproducibility\n    rng = np.random.default_rng(seed)\n\n    delta_c_gauss = estimate_critical_delta(\n        'gaussian', n, k, m_candidates, T, q, tau, eps_denom, rng\n    )\n    delta_c_fourier = estimate_critical_delta(\n        'fourier', n, k, m_candidates, T, q, tau, eps_denom, rng\n    )\n\n    if np.isnan(delta_c_gauss) or np.isnan(delta_c_fourier):\n        return False\n        \n    return abs(delta_c_gauss - delta_c_fourier) = epsilon\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Success criterion parameters\n    tau = 1e-6\n    eps_denom = 1e-12\n\n    # Test suite from the problem statement.\n    test_cases = [\n        {'n': 64, 'k': 4, 'm_candidates': [12, 14, 16, 18, 20, 22, 24], 'T': 8, 'q': 0.75, 'epsilon': 0.15, 'seed': 0},\n        {'n': 96, 'k': 8, 'm_candidates': [24, 28, 32, 36, 40, 44], 'T': 6, 'q': 0.67, 'epsilon': 0.20, 'seed': 1},\n        {'n': 128, 'k': 16, 'm_candidates': [36, 44, 52, 60, 68, 76, 84], 'T': 5, 'q': 0.60, 'epsilon': 0.20, 'seed': 2},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_single_test_case(\n            case['n'], case['k'], case['m_candidates'], case['T'],\n            case['q'], case['epsilon'], tau, eps_denom, case['seed']\n        )\n        # Convert Python booleans (True/False) to JSON-style strings (true/false)\n        results.append(str(result).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3451296"}]}