{"hands_on_practices": [{"introduction": "To build a strong foundation, we first explore the core of Total Variation (TV) regularization in a two-dimensional setting, such as image processing. This exercise asks you to compute both anisotropic and isotropic TV for a small image, which will clarify how these penalties measure signal variation differently. By comparing the $\\|\\cdot\\|_1$-based anisotropic TV and the $\\|\\cdot\\|_2$-based isotropic TV, you will develop an intuition for why anisotropic regularization, which is directly related to the fused LASSO penalty, encourages solutions with specific \"blocky\" structures. [@problem_id:3447189]", "problem": "Consider a two-dimensional discrete image represented on a $2 \\times 2$ grid with homogeneous Neumann boundary conditions (zero normal gradient at the boundary). Let the grayscale intensity array $u$ be\n$$\nu \\;=\\;\n\\begin{pmatrix}\n0 & 2 \\\\\n3 & 1\n\\end{pmatrix}.\n$$\nUse the forward-difference discrete gradient: for each pixel $(i,j)$, the horizontal component is $D_{x}u_{i,j} = u_{i,j+1} - u_{i,j}$ if $j<2$ and $D_{x}u_{i,2} = 0$, and the vertical component is $D_{y}u_{i,j} = u_{i+1,j} - u_{i,j}$ if $i<2$ and $D_{y}u_{2,j} = 0$. The anisotropic total variation $\\mathrm{TV}_{\\mathrm{aniso}}$ uses the one-norm of the discrete gradient components at each pixel, whereas the isotropic total variation $\\mathrm{TV}_{\\mathrm{iso}}$ uses the two-norm (Euclidean norm) of the discrete gradient at each pixel. Starting from these core definitions of discrete gradients and norms, compute $\\mathrm{TV}_{\\mathrm{aniso}}$ and $\\mathrm{TV}_{\\mathrm{iso}}$ for the given image. Then, based on first principles of norm inequalities, determine which directional differences (horizontal-only, vertical-only, or simultaneous horizontal and vertical at a pixel) are penalized more strongly by $\\mathrm{TV}_{\\mathrm{aniso}}$ versus $\\mathrm{TV}_{\\mathrm{iso}}$, and explain why this is consistent with the structure of the fused Least Absolute Shrinkage and Selection Operator (LASSO) penalty in compressed sensing. Provide the values of $\\mathrm{TV}_{\\mathrm{aniso}}$ and $\\mathrm{TV}_{\\mathrm{iso}}$ in exact form without rounding.", "solution": "The problem is well-posed and scientifically grounded, providing all necessary definitions and data for a unique solution. We proceed with the solution.\n\nThe image is represented by a $2 \\times 2$ array of intensities $u$, where the indices $(i,j)$ refer to the row and column, respectively, starting from $1$. The given image is:\n$$\nu = \\begin{pmatrix} u_{1,1} & u_{1,2} \\\\ u_{2,1} & u_{2,2} \\end{pmatrix} = \\begin{pmatrix} 0 & 2 \\\\ 3 & 1 \\end{pmatrix}\n$$\nThe discrete gradient at each pixel $(i,j)$ is a vector $(\\nabla u)_{i,j} = (D_x u_{i,j}, D_y u_{i,j})$. The components are defined by the forward-difference scheme with homogeneous Neumann boundary conditions.\nFor a $2 \\times 2$ grid (where the maximum index is $2$), the definitions are:\n$D_x u_{i,j} = u_{i,j+1} - u_{i,j}$ for $j < 2$, and $D_x u_{i,2} = 0$.\n$D_y u_{i,j} = u_{i+1,j} - u_{i,j}$ for $i < 2$, and $D_y u_{2,j} = 0$.\n\nWe compute the discrete gradient vector at each of the four pixels:\n1.  For pixel $(1,1)$:\n    $D_x u_{1,1} = u_{1,2} - u_{1,1} = 2 - 0 = 2$.\n    $D_y u_{1,1} = u_{2,1} - u_{1,1} = 3 - 0 = 3$.\n    So, $(\\nabla u)_{1,1} = (2, 3)$.\n\n2.  For pixel $(1,2)$:\n    $D_x u_{1,2} = 0$ (boundary condition as $j=2$).\n    $D_y u_{1,2} = u_{2,2} - u_{1,2} = 1 - 2 = -1$.\n    So, $(\\nabla u)_{1,2} = (0, -1)$.\n\n3.  For pixel $(2,1)$:\n    $D_x u_{2,1} = u_{2,2} - u_{2,1} = 1 - 3 = -2$.\n    $D_y u_{2,1} = 0$ (boundary condition as $i=2$).\n    So, $(\\nabla u)_{2,1} = (-2, 0)$.\n\n4.  For pixel $(2,2)$:\n    $D_x u_{2,2} = 0$ (boundary condition as $j=2$).\n    $D_y u_{2,2} = 0$ (boundary condition as $i=2$).\n    So, $(\\nabla u)_{2,2} = (0, 0)$.\n\nThe anisotropic total variation, $\\mathrm{TV}_{\\mathrm{aniso}}$, is the sum of the $\\ell_1$-norms of the gradient vectors at each pixel:\n$$\n\\mathrm{TV}_{\\mathrm{aniso}}(u) = \\sum_{i=1}^2 \\sum_{j=1}^2 \\|(\\nabla u)_{i,j}\\|_1 = \\sum_{i=1}^2 \\sum_{j=1}^2 \\left( |D_x u_{i,j}| + |D_y u_{i,j}| \\right)\n$$\nSubstituting the computed values:\n$\\mathrm{TV}_{\\mathrm{aniso}}(u) = \\|(2, 3)\\|_1 + \\|(0, -1)\\|_1 + \\|(-2, 0)\\|_1 + \\|(0, 0)\\|_1$\n$\\mathrm{TV}_{\\mathrm{aniso}}(u) = (|2| + |3|) + (|0| + |-1|) + (|-2| + |0|) + (|0| + |0|)$\n$\\mathrm{TV}_{\\mathrm{aniso}}(u) = (2 + 3) + (0 + 1) + (2 + 0) + 0 = 5 + 1 + 2 = 8$.\n\nThe isotropic total variation, $\\mathrm{TV}_{\\mathrm{iso}}$, is the sum of the $\\ell_2$-norms (Euclidean norms) of the gradient vectors at each pixel:\n$$\n\\mathrm{TV}_{\\mathrm{iso}}(u) = \\sum_{i=1}^2 \\sum_{j=1}^2 \\|(\\nabla u)_{i,j}\\|_2 = \\sum_{i=1}^2 \\sum_{j=1}^2 \\sqrt{(D_x u_{i,j})^2 + (D_y u_{i,j})^2}\n$$\nSubstituting the computed values:\n$\\mathrm{TV}_{\\mathrm{iso}}(u) = \\|(2, 3)\\|_2 + \\|(0, -1)\\|_2 + \\|(-2, 0)\\|_2 + \\|(0, 0)\\|_2$\n$\\mathrm{TV}_{\\mathrm{iso}}(u) = \\sqrt{2^2 + 3^2} + \\sqrt{0^2 + (-1)^2} + \\sqrt{(-2)^2 + 0^2} + \\sqrt{0^2 + 0^2}$\n$\\mathrm{TV}_{\\mathrm{iso}}(u) = \\sqrt{4 + 9} + \\sqrt{1} + \\sqrt{4} + 0 = \\sqrt{13} + 1 + 2 = 3 + \\sqrt{13}$.\n\nNext, we analyze which directional differences are penalized more strongly by each TV variant. This comparison is rooted in the properties of the $\\ell_1$ and $\\ell_2$ norms in $\\mathbb{R}^2$. Let the gradient vector at a pixel be $\\mathbf{g} = (g_x, g_y)$. The anisotropic penalty is $\\|\\mathbf{g}\\|_1 = |g_x| + |g_y|$ and the isotropic penalty is $\\|\\mathbf{g}\\|_2 = \\sqrt{g_x^2 + g_y^2}$.\nThe fundamental relationship between these two norms is $\\|\\mathbf{g}\\|_2 \\le \\|\\mathbf{g}\\|_1$. Equality holds if and only if one of the components of $\\mathbf{g}$ is zero.\n\nCase 1: The gradient is aligned with a coordinate axis (horizontal-only or vertical-only difference). For example, $\\mathbf{g} = (g_x, 0)$ with $g_x \\neq 0$.\nThe anisotropic penalty is $|g_x| + |0| = |g_x|$.\nThe isotropic penalty is $\\sqrt{g_x^2 + 0^2} = |g_x|$.\nIn this case, the penalties are identical.\n\nCase 2: The gradient has both horizontal and vertical components (e.g., a \"diagonal\" difference), so $g_x \\neq 0$ and $g_y \\neq 0$.\nThe anisotropic penalty is $|g_x| + |g_y|$.\nThe isotropic penalty is $\\sqrt{g_x^2 + g_y^2}$.\nHere, the strict inequality $\\|\\mathbf{g}\\|_2 < \\|\\mathbf{g}\\|_1$ holds. For example, if $\\mathbf{g} = (c, c)$ for some constant $c \\neq 0$, then $\\|\\mathbf{g}\\|_1 = 2|c|$ while $\\|\\mathbf{g}\\|_2 = \\sqrt{c^2+c^2} = \\sqrt{2}|c|$. Clearly, $2|c| > \\sqrt{2}|c|$.\n\nThis demonstrates that for a gradient of a given Euclidean magnitude $\\|\\mathbf{g}\\|_2$, the $\\ell_1$ norm (anisotropic penalty) is maximized when the gradient is equally distributed between components (e.g., along the diagonal) and minimized when it is aligned with an axis. Therefore, $\\mathrm{TV}_{\\mathrm{aniso}}$ penalizes simultaneous horizontal and vertical differences more harshly relative to its penalization of axis-aligned differences. This property encourages solutions where gradients are sparse and aligned with the coordinate axes, which can lead to \"blocky\" or \"staircase\" artifacts in image reconstruction problems. In contrast, $\\mathrm{TV}_{\\mathrm{iso}}$ is rotationally invariant, penalizing only the magnitude of the gradient, not its direction.\n\nThe connection to the fused LASSO is direct. The fused LASSO penalty for a one-dimensional signal $\\mathbf{x} = (x_1, \\ldots, x_n)$ includes a term of the form $\\lambda \\sum_{i=2}^n |x_i - x_{i-1}|$. This term is precisely the one-dimensional anisotropic total variation of the signal $\\mathbf{x}$. Its purpose is to promote piecewise-constant solutions by penalizing differences between adjacent coefficients.\n\nA direct generalization of this penalty to a two-dimensional image $u$ is to penalize differences along both dimensions separately:\n$$\nP(u) = \\lambda_v \\sum_{i,j} |u_{i+1,j} - u_{i,j}| + \\lambda_h \\sum_{i,j} |u_{i,j+1} - u_{i,j}|\n$$\nIf $\\lambda_v = \\lambda_h = \\lambda$, this becomes $\\lambda \\sum_{i,j} (|u_{i+1,j} - u_{i,j}| + |u_{i,j+1} - u_{i,j}|)$, which is exactly $\\lambda \\cdot \\mathrm{TV}_{\\mathrm{aniso}}(u)$. The structure of this penalty is a sum of absolute values of individual difference components, which is characteristic of the LASSO's $\\ell_1$-norm penalty. This separable structure is precisely what leads to the anisotropic behavior analyzed above: it penalizes diagonal gradients more strongly than axis-aligned gradients, a behavior inherited from the properties of the $\\ell_1$ norm. Thus, the fused LASSO is structurally an application of anisotropic total variation regularization.", "answer": "$$\n\\boxed{\\begin{pmatrix} 8 & 3 + \\sqrt{13} \\end{pmatrix}}\n$$", "id": "3447189"}, {"introduction": "Having built an intuition for TV, we now turn to the mathematical underpinnings of the one-dimensional fused LASSO. This practice uses the formal language of convex analysis, specifically subdifferentials, to derive the first-order optimality conditions for the fused LASSO objective. By working through these principles, you will uncover the precise mechanism by which the fused LASSO simultaneously promotes sparsity and piecewise-constant structure, revealing the solution as a soft-thresholding operation on a datum shifted by the total variation term. [@problem_id:3447208]", "problem": "Consider the fused Least Absolute Shrinkage and Selection Operator (LASSO) signal approximator, also known as one-dimensional Total Variation (TV) denoising with an additional coordinate-wise sparsity penalty. Let $y \\in \\mathbb{R}^{n}$ be given, and define the optimization problem\n$$\n\\min_{\\beta \\in \\mathbb{R}^{n}} \\; \\frac{1}{2}\\|\\beta - y\\|_{2}^{2} + \\lambda_{1}\\|\\beta\\|_{1} + \\lambda_{2}\\|D\\beta\\|_{1},\n$$\nwhere $\\lambda_{1} \\ge 0$, $\\lambda_{2} \\ge 0$, and $D \\in \\mathbb{R}^{(n-1)\\times n}$ is the first-order forward difference operator whose rows satisfy $(D\\beta)_{i} = \\beta_{i+1} - \\beta_{i}$ for $i \\in \\{1,\\dots,n-1\\}$. Throughout, use the definition of the subdifferential of a proper closed convex function $g:\\mathbb{R}^{p}\\to \\mathbb{R}\\cup\\{+\\infty\\}$ at a point $x \\in \\mathbb{R}^{p}$,\n$$\n\\partial g(x) := \\left\\{\\, v \\in \\mathbb{R}^{p} \\; : \\; g(z) \\ge g(x) + \\langle v, z - x\\rangle \\;\\; \\text{for all } z \\in \\mathbb{R}^{p} \\,\\right\\},\n$$\nand the necessary and sufficient optimality condition for unconstrained convex minimization, namely $0 \\in \\partial f(x^{\\star})$ where $f$ is convex and $x^{\\star}$ is a minimizer.\n\n1) Starting from the subdifferential definition above, write the subdifferential of the $\\ell_{1}$ norm $\\|\\beta\\|_{1}$ at an arbitrary point $\\beta \\in \\mathbb{R}^{n}$, in a coordinate-wise form.\n\n2) Using the optimality condition for the convex objective and properties of subdifferentials of sums, derive a coordinate-wise inclusion that must hold at any minimizer $\\beta^{\\star}$:\n$$\n0 \\in \\beta_{i}^{\\star} - y_{i} + \\lambda_{1} u_{i}^{\\star} + \\lambda_{2}(D^{\\top}v^{\\star})_{i} \\quad \\text{for each } i \\in \\{1,\\dots,n\\},\n$$\nfor some $u^{\\star} \\in \\partial \\|\\beta^{\\star}\\|_{1}$ and some $v^{\\star} \\in \\partial \\|D\\beta^{\\star}\\|_{1}$. Use this to show that, for any fixed $v \\in \\mathbb{R}^{n-1}$, the coordinate $\\beta_{i}^{\\star}$ can be written as the solution of a one-dimensional strongly convex problem with a unique closed form depending on $y_{i}$, $\\lambda_{1}$, and $(D^{\\top}v)_{i}$, and conclude a necessary and sufficient condition under which $\\beta_{i}^{\\star} = 0$ holds exactly.\n\n3) Specialize to $n=4$ and suppose that at an optimal solution the TV subgradient components satisfy $v_{1}^{\\star} = \\frac{1}{2}$, $v_{2}^{\\star} = -\\frac{1}{3}$, and $v_{0}^{\\star} = 0$, $v_{4}^{\\star} = 0$ as boundary conditions for $D^{\\top}$. Provide a single closed-form analytic expression for $\\beta_{2}^{\\star}$ in terms of $y_{2}$, $\\lambda_{1}$, and $\\lambda_{2}$, using only elementary functions such as $\\operatorname{sign}$, $\\max$, and absolute value. Your final answer must be a single analytic expression depending on $y_{2}$, $\\lambda_{1}$, and $\\lambda_{2}$, with no inequalities and no piecewise cases. No rounding is required.", "solution": "We begin from foundational principles in convex analysis. The subdifferential of a proper closed convex function $g:\\mathbb{R}^{p}\\to \\mathbb{R}\\cup\\{+\\infty\\}$ at a point $x$ is defined by\n$$\n\\partial g(x) := \\{\\, v \\in \\mathbb{R}^{p} : g(z) \\ge g(x) + \\langle v, z-x\\rangle \\text{ for all } z \\in \\mathbb{R}^{p} \\,\\}.\n$$\nFor the sum $f = f_{1}+f_{2}$ of proper closed convex functions, a standard well-tested subdifferential sum rule states that\n$$\n\\partial f(x) \\subseteq \\partial f_{1}(x) + \\partial f_{2}(x),\n$$\nwith equality holding under mild regularity such as continuity of one summand at $x$. For unconstrained convex minimization, a point $x^{\\star}$ minimizes $f$ if and only if $0 \\in \\partial f(x^{\\star})$.\n\nStep 1: Subdifferential of the $\\ell_{1}$ norm. Consider $g(\\beta) = \\|\\beta\\|_{1} = \\sum_{i=1}^{n} |\\beta_{i}|$. The subdifferential is separable across coordinates:\n$$\n\\partial \\|\\beta\\|_{1} = \\prod_{i=1}^{n} \\partial |\\beta_{i}|,\n$$\nwhere, for a scalar $t \\in \\mathbb{R}$, the subdifferential of the absolute value is\n$$\n\\partial |t| = \n\\begin{cases}\n\\{ \\operatorname{sign}(t) \\}, & \\text{if } t \\neq 0,\\\\\n[-1,1], & \\text{if } t = 0.\n\\end{cases}\n$$\nTherefore, in coordinate-wise form,\n$$\n\\partial \\|\\beta\\|_{1} = \\left\\{\\, u \\in \\mathbb{R}^{n} : u_{i} =\n\\begin{cases}\n\\operatorname{sign}(\\beta_{i}), & \\text{if } \\beta_{i} \\neq 0,\\\\\n\\xi \\text{ with } \\xi \\in [-1,1], & \\text{if } \\beta_{i} = 0,\n\\end{cases}\n\\text{ for all } i \\in \\{1,\\dots,n\\}\\,\\right\\}.\n$$\n\nStep 2: Optimality for the fused Least Absolute Shrinkage and Selection Operator (LASSO) objective. Define the objective\n$$\nF(\\beta) := \\frac{1}{2}\\|\\beta - y\\|_{2}^{2} + \\lambda_{1}\\|\\beta\\|_{1} + \\lambda_{2}\\|D\\beta\\|_{1}.\n$$\nThe first term is smooth with gradient $\\nabla \\left(\\frac{1}{2}\\|\\beta - y\\|_{2}^{2}\\right) = \\beta - y$. The second and third terms are convex but generally nonsmooth. By the subdifferential sum rule,\n$$\n\\partial F(\\beta) = (\\beta - y) + \\lambda_{1}\\,\\partial \\|\\beta\\|_{1} + \\lambda_{2}\\, D^{\\top} \\partial \\|D\\beta\\|_{1},\n$$\nwhere we have used the chain rule for subdifferentials of linear mappings: $\\partial \\|D\\beta\\|_{1} = D^{\\top} \\partial \\|z\\|_{1}\\big|_{z = D\\beta}$. Thus, at a minimizer $\\beta^{\\star}$, there exist $u^{\\star} \\in \\partial \\|\\beta^{\\star}\\|_{1}$ and $v^{\\star} \\in \\partial \\|D\\beta^{\\star}\\|_{1}$ such that the necessary and sufficient optimality condition holds:\n$$\n0 \\in \\beta^{\\star} - y + \\lambda_{1} u^{\\star} + \\lambda_{2} D^{\\top} v^{\\star}.\n$$\nEquivalently, in coordinates, for each $i \\in \\{1,\\dots,n\\}$,\n$$\n0 \\in \\beta_{i}^{\\star} - y_{i} + \\lambda_{1} u_{i}^{\\star} + \\lambda_{2} (D^{\\top} v^{\\star})_{i}.\n$$\nBecause $D$ is the first-order forward difference, $(D\\beta)_{i} = \\beta_{i+1} - \\beta_{i}$ for $i \\in \\{1,\\dots,n-1\\}$. A direct calculation shows that\n$$\nD^{\\top} v = \\begin{bmatrix}\n- v_{1} \\\\\nv_{1} - v_{2} \\\\\n\\vdots \\\\\nv_{n-2} - v_{n-1} \\\\\nv_{n-1}\n\\end{bmatrix},\n$$\nso that for interior indices $i \\in \\{2,\\dots,n-1\\}$ one has $(D^{\\top} v)_{i} = v_{i-1} - v_{i}$, and at boundaries $(D^{\\top} v)_{1} = -v_{1}$, $(D^{\\top} v)_{n} = v_{n-1}$.\n\nFix any $v \\in \\mathbb{R}^{n-1}$ and define the shifted data\n$$\nz_{i} := y_{i} - \\lambda_{2} (D^{\\top} v)_{i}, \\quad \\text{for } i \\in \\{1,\\dots,n\\}.\n$$\nThen the coordinate-wise optimality inclusion becomes\n$$\n0 \\in \\beta_{i}^{\\star} - z_{i} + \\lambda_{1} u_{i}^{\\star}, \\quad \\text{with } u_{i}^{\\star} \\in \\partial |\\beta_{i}^{\\star}|.\n$$\nThis is exactly the first-order optimality condition for the scalar problem in the single variable $b$,\n$$\n\\min_{b \\in \\mathbb{R}} \\; \\frac{1}{2}(b - z_{i})^{2} + \\lambda_{1} |b|.\n$$\nThis problem has a unique closed-form solution given by the soft-thresholding operation,\n$$\nb^{\\star} = \\operatorname{sign}(z_{i}) \\max\\left(|z_{i}| - \\lambda_{1}, 0\\right).\n$$\nTherefore, for any fixed $v$,\n$$\n\\beta_{i}^{\\star} = \\operatorname{sign}\\!\\left(y_{i} - \\lambda_{2}(D^{\\top} v)_{i}\\right) \\max\\!\\left(\\left|y_{i} - \\lambda_{2}(D^{\\top} v)_{i}\\right| - \\lambda_{1}, 0\\right).\n$$\nFrom this representation, $\\beta_{i}^{\\star} = 0$ if and only if the thresholding condition\n$$\n\\left|y_{i} - \\lambda_{2}(D^{\\top} v^{\\star})_{i}\\right| \\le \\lambda_{1}\n$$\nholds with $v^{\\star} \\in \\partial \\|D\\beta^{\\star}\\|_{1}$. This precisely exhibits how exact zeros can occur even in the presence of the TV coupling: the neighboring-difference subgradient $(D^{\\top} v^{\\star})_{i}$ shifts the effective datum $y_{i}$ before the $\\ell_{1}$ shrinkage, and if the shifted magnitude is at most $\\lambda_{1}$, the coordinate is zero.\n\nStep 3: Specialization to $n=4$ and the requested expression. For $n=4$, suppose the optimal TV subgradient satisfies $v_{1}^{\\star} = \\frac{1}{2}$, $v_{2}^{\\star} = -\\frac{1}{3}$, and boundary conditions $v_{0}^{\\star} = 0$, $v_{4}^{\\star} = 0$. For $i=2$ (an interior index), we have\n$$\n(D^{\\top} v^{\\star})_{2} = v_{1}^{\\star} - v_{2}^{\\star} = \\frac{1}{2} - \\left(-\\frac{1}{3}\\right) = \\frac{5}{6}.\n$$\nTherefore,\n$$\n\\beta_{2}^{\\star} = \\operatorname{sign}\\!\\left(y_{2} - \\lambda_{2}\\cdot \\frac{5}{6}\\right) \\max\\!\\left(\\left|y_{2} - \\lambda_{2}\\cdot \\frac{5}{6}\\right| - \\lambda_{1}, 0\\right).\n$$\nThis is a single closed-form analytic expression in the variables $y_{2}$, $\\lambda_{1}$, and $\\lambda_{2}$, and it encodes the exact-zero condition implicitly via the outer $\\max$.", "answer": "$$\\boxed{\\operatorname{sign}\\!\\left(y_{2} - \\frac{5}{6}\\lambda_{2}\\right)\\,\\max\\!\\left(\\left|y_{2} - \\frac{5}{6}\\lambda_{2}\\right| - \\lambda_{1}, \\, 0\\right)}$$", "id": "3447208"}, {"introduction": "While understanding the optimality conditions is crucial, solving fused LASSO problems in practice, especially for general design matrices $X$, requires sophisticated algorithms. This exercise introduces the Alternating Direction Method of Multipliers (ADMM), a powerful and widely-used framework for splitting complex optimization problems into simpler subproblems. By performing one full numerical iteration, you will demystify the abstract steps of the algorithm and gain hands-on experience with the interplay between primal and dual variable updates. [@problem_id:3447147]", "problem": "Consider the fused least absolute shrinkage and selection operator (LASSO) problem that augments the least-squares data fidelity with both elementwise sparsity and one-dimensional total variation sparsity. The fused LASSO objective is\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda_{1} \\|\\beta\\|_{1} + \\lambda_{2} \\|D \\beta\\|_{1},\n$$\nwhere $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $y \\in \\mathbb{R}^{n}$ is the observed data, and $D \\in \\mathbb{R}^{(p-1) \\times p}$ is the first-order forward difference operator that encodes the one-dimensional total variation. The Alternating Direction Method of Multipliers (ADMM) is a decomposition method for structured convex optimization. Introduce auxiliary variables $z \\in \\mathbb{R}^{p}$ and $s \\in \\mathbb{R}^{p-1}$ to split the nonsmooth terms, with linear constraints $z = \\beta$ and $s = D \\beta$. Starting from the scaled augmented Lagrangian principle, construct the ADMM updates for the primal variables $\\beta$, $z$, $s$ and the scaled dual variables $u$, $v$, and then perform one full ADMM iteration numerically for the following specified instance.\n\nUse $n=p=4$, set\n$$\nX = I_{4}, \\quad y = \\begin{pmatrix} 3 \\\\ -3 \\\\ 9 \\\\ -5 \\end{pmatrix}, \\quad \\lambda_{1} = 1, \\quad \\lambda_{2} = 1, \\quad \\rho = 1,\n$$\nand let the forward difference matrix be\n$$\nD = \\begin{pmatrix}\n-1 & 1 & 0 & 0 \\\\\n0 & -1 & 1 & 0 \\\\\n0 & 0 & -1 & 1\n\\end{pmatrix}.\n$$\nInitialize the auxiliary and scaled dual variables at zero,\n$$\nz^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad s^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad u^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad v^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nStarting from the augmented Lagrangian and first-order optimality principles, derive the ADMM update equations and compute the first-iteration values $\\beta^{(1)}$, $z^{(1)}$, $s^{(1)}$, $u^{(1)}$, $v^{(1)}$ numerically for the given instance. Then form the stacked primal constraint-violation vector\n$$\nr^{(1)} = \\begin{pmatrix} \\beta^{(1)} - z^{(1)} \\\\ D \\beta^{(1)} - s^{(1)} \\end{pmatrix} \\in \\mathbb{R}^{7},\n$$\nand compute its squared Euclidean norm $\\|r^{(1)}\\|_{2}^{2}$. Provide the exact value of $\\|r^{(1)}\\|_{2}^{2}$ as your final answer. No rounding is required.", "solution": "The problem requires the derivation and application of the Alternating Direction Method of Multipliers (ADMM) to a fused LASSO objective function. We will first validate the problem statement, then derive the general ADMM updates, and finally apply them for one iteration with the given numerical data to compute the required quantity.\n\n### Problem Validation\n**Step 1: Extract Givens**\n- Objective: $\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda_{1} \\|\\beta\\|_{1} + \\lambda_{2} \\|D \\beta\\|_{1}$\n- Dimensions: $n=p=4$\n- Data: $y = \\begin{pmatrix} 3 \\\\ -3 \\\\ 9 \\\\ -5 \\end{pmatrix}$, $X = I_{4}$\n- Regularization parameters: $\\lambda_{1} = 1$, $\\lambda_{2} = 1$\n- ADMM penalty parameter: $\\rho = 1$\n- Difference operator: $D = \\begin{pmatrix} -1 & 1 & 0 & 0 \\\\ 0 & -1 & 1 & 0 \\\\ 0 & 0 & -1 & 1 \\end{pmatrix} \\in \\mathbb{R}^{3 \\times 4}$\n- Initial conditions: $z^{(0)} = \\mathbf{0} \\in \\mathbb{R}^4$, $s^{(0)} = \\mathbf{0} \\in \\mathbb{R}^3$, $u^{(0)} = \\mathbf{0} \\in \\mathbb{R}^4$, $v^{(0)} = \\mathbf{0} \\in \\mathbb{R}^3$\n- Constraints for ADMM splitting: $z = \\beta$, $s = D \\beta$\n- Target quantity: $\\|r^{(1)}\\|_{2}^{2}$ where $r^{(1)} = \\begin{pmatrix} \\beta^{(1)} - z^{(1)} \\\\ D \\beta^{(1)} - s^{(1)} \\end{pmatrix}$\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, objective, and complete. It describes a standard fused LASSO regularized regression problem, a fundamental topic in sparse optimization and compressed sensing. The chosen solution method, ADMM, is a standard and appropriate algorithm for this problem structure. The provided data is mathematically consistent and sufficient to perform the requested calculations. The problem is valid.\n\n### ADMM Formulation and Derivation\n\nThe original problem is equivalent to the constrained optimization problem:\n$$\n\\min_{\\beta, z, s} \\ \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda_{1} \\|z\\|_{1} + \\lambda_{2} \\|s\\|_{1} \\quad \\text{subject to} \\quad \\beta - z = 0, \\ D\\beta - s = 0.\n$$\nThe scaled augmented Lagrangian, $\\mathcal{L}_{\\rho}$, for this problem is:\n$$\n\\mathcal{L}_{\\rho}(\\beta, z, s, u, v) = \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda_{1}\\|z\\|_{1} + \\lambda_{2}\\|s\\|_{1} + \\frac{\\rho}{2}\\|\\beta - z + u\\|_{2}^{2} + \\frac{\\rho}{2}\\|D\\beta - s + v\\|_{2}^{2} - \\frac{\\rho}{2}\\|u\\|_{2}^{2} - \\frac{\\rho}{2}\\|v\\|_{2}^{2}\n$$\nwhere $u$ and $v$ are the scaled dual variables. The ADMM algorithm iteratively minimizes $\\mathcal{L}_{\\rho}$ with respect to the primal variables $\\beta, z, s$ and then updates the dual variables $u, v$.\n\nThe $(k+1)$-th iteration consists of the following steps:\n1.  **$\\beta$-minimization:**\n    $\\beta^{(k+1)} = \\arg\\min_{\\beta} \\left( \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\frac{\\rho}{2}\\|\\beta - z^{(k)} + u^{(k)}\\|_{2}^{2} + \\frac{\\rho}{2}\\|D\\beta - s^{(k)} + v^{(k)}\\|_{2}^{2} \\right)$\n    This is a quadratic objective in $\\beta$. The first-order optimality condition (setting the gradient with respect to $\\beta$ to zero) yields a linear system:\n    $$\n    X^T(X\\beta - y) + \\rho(\\beta - z^{(k)} + u^{(k)}) + D^T\\rho(D\\beta - s^{(k)} + v^{(k)}) = 0\n    $$\n    $$\n    (X^TX + \\rho I + \\rho D^TD)\\beta = X^Ty + \\rho(z^{(k)} - u^{(k)}) + \\rho D^T(s^{(k)} - v^{(k)})\n    $$\n    The update is $\\beta^{(k+1)} = (X^TX + \\rho(I + D^TD))^{-1} (X^Ty + \\rho(z^{(k)} - u^{(k)}) + \\rho D^T(s^{(k)} - v^{(k)}))$.\n\n2.  **$z$-minimization:**\n    $z^{(k+1)} = \\arg\\min_{z} \\left( \\lambda_{1}\\|z\\|_{1} + \\frac{\\rho}{2}\\|\\beta^{(k+1)} - z + u^{(k)}\\|_{2}^{2} \\right) = \\arg\\min_{z} \\left( \\lambda_{1}\\|z\\|_{1} + \\frac{\\rho}{2}\\|z - (\\beta^{(k+1)} + u^{(k)})\\|_{2}^{2} \\right)$\n    This is the proximal operator of the $\\ell_1$-norm, which is the soft-thresholding operator $S_{\\kappa}(\\cdot)$:\n    $z^{(k+1)} = S_{\\lambda_1/\\rho}(\\beta^{(k+1)} + u^{(k)})$, where $(S_{\\kappa}(a))_i = \\text{sign}(a_i) \\max(|a_i| - \\kappa, 0)$.\n\n3.  **$s$-minimization:**\n    $s^{(k+1)} = \\arg\\min_{s} \\left( \\lambda_{2}\\|s\\|_{1} + \\frac{\\rho}{2}\\|D\\beta^{(k+1)} - s + v^{(k)}\\|_{2}^{2} \\right) = \\arg\\min_{s} \\left( \\lambda_{2}\\|s\\|_{1} + \\frac{\\rho}{2}\\|s - (D\\beta^{(k+1)} + v^{(k)})\\|_{2}^{2} \\right)$\n    Similarly, this is solved by soft-thresholding:\n    $s^{(k+1)} = S_{\\lambda_2/\\rho}(D\\beta^{(k+1)} + v^{(k)})$.\n\n4.  **Dual variable updates:**\n    $u^{(k+1)} = u^{(k)} + \\beta^{(k+1)} - z^{(k+1)}$\n    $v^{(k+1)} = v^{(k)} + D\\beta^{(k+1)} - s^{(k+1)}$\n\n### First ADMM Iteration (k=0)\n\nWe perform one iteration with the given initial conditions $z^{(0)}=\\mathbf{0}, s^{(0)}=\\mathbf{0}, u^{(0)}=\\mathbf{0}, v^{(0)}=\\mathbf{0}$ and parameters $\\lambda_1=\\lambda_2=\\rho=1$, $X=I_4$.\n\n**1. Compute $\\beta^{(1)}$**\nThe update equation simplifies to:\n$$\n\\beta^{(1)} = (I_4^T I_4 + 1(I_4 + D^TD))^{-1} (I_4^T y + 1(\\mathbf{0} - \\mathbf{0}) + 1D^T(\\mathbf{0} - \\mathbf{0})) = (2I_4 + D^TD)^{-1} y\n$$\nFirst, we compute the matrix $D^TD$:\n$$\nD^T = \\begin{pmatrix} -1 & 0 & 0 \\\\ 1 & -1 & 0 \\\\ 0 & 1 & -1 \\\\ 0 & 0 & 1 \\end{pmatrix}, \\quad D^TD = \\begin{pmatrix} 1 & -1 & 0 & 0 \\\\ -1 & 2 & -1 & 0 \\\\ 0 & -1 & 2 & -1 \\\\ 0 & 0 & -1 & 1 \\end{pmatrix}\n$$\nThe matrix to be inverted is $A = 2I_4 + D^TD$:\n$$\nA = 2\\begin{pmatrix} 1&0&0&0\\\\0&1&0&0\\\\0&0&1&0\\\\0&0&0&1 \\end{pmatrix} + \\begin{pmatrix} 1 & -1 & 0 & 0 \\\\ -1 & 2 & -1 & 0 \\\\ 0 & -1 & 2 & -1 \\\\ 0 & 0 & -1 & 1 \\end{pmatrix} = \\begin{pmatrix} 3 & -1 & 0 & 0 \\\\ -1 & 4 & -1 & 0 \\\\ 0 & -1 & 4 & -1 \\\\ 0 & 0 & -1 & 3 \\end{pmatrix}\n$$\nWe solve the linear system $A\\beta^{(1)} = y$:\n$$\n\\begin{pmatrix} 3 & -1 & 0 & 0 \\\\ -1 & 4 & -1 & 0 \\\\ 0 & -1 & 4 & -1 \\\\ 0 & 0 & -1 & 3 \\end{pmatrix} \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -3 \\\\ 9 \\\\ -5 \\end{pmatrix}\n$$\nSolving this system yields:\n$\\beta^{(1)} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\\\ -1 \\end{pmatrix}$.\n\n**2. Compute $z^{(1)}$**\n$z^{(1)} = S_{\\lambda_1/\\rho}(\\beta^{(1)} + u^{(0)}) = S_{1/1}(\\beta^{(1)} + \\mathbf{0}) = S_1(\\beta^{(1)})$.\nWith $\\beta^{(1)} = (1, 0, 2, -1)^T$ and a threshold of $\\kappa=1$:\n- $z_1^{(1)} = S_1(1) = \\text{sign}(1)\\max(|1|-1, 0) = 0$\n- $z_2^{(1)} = S_1(0) = \\text{sign}(0)\\max(|0|-1, 0) = 0$\n- $z_3^{(1)} = S_1(2) = \\text{sign}(2)\\max(|2|-1, 0) = 1$\n- $z_4^{(1)} = S_1(-1) = \\text{sign}(-1)\\max(|-1|-1, 0) = 0$\nSo, $z^{(1)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\n\n**3. Compute $s^{(1)}$**\n$s^{(1)} = S_{\\lambda_2/\\rho}(D\\beta^{(1)} + v^{(0)}) = S_{1/1}(D\\beta^{(1)} + \\mathbf{0}) = S_1(D\\beta^{(1)})$.\nFirst, compute the argument $D\\beta^{(1)}$:\n$$\nD\\beta^{(1)} = \\begin{pmatrix} -1 & 1 & 0 & 0 \\\\ 0 & -1 & 1 & 0 \\\\ 0 & 0 & -1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -1(1) + 1(0) \\\\ 0(1) - 1(0) + 1(2) \\\\ 0(1) + 0(0) -1(2) + 1(-1) \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 2 \\\\ -3 \\end{pmatrix}\n$$\nNow, apply soft-thresholding with $\\kappa=1$:\n- $s_1^{(1)} = S_1(-1) = 0$\n- $s_2^{(1)} = S_1(2) = 1$\n- $s_3^{(1)} = S_1(-3) = -2$\nSo, $s^{(1)} = \\begin{pmatrix} 0 \\\\ 1 \\\\ -2 \\end{pmatrix}$.\n\nThe problem also asks to compute $u^{(1)}$ and $v^{(1)}$ implicitly as part of the full iteration description.\n**4. Compute $u^{(1)}$ and $v^{(1)}$**\n$u^{(1)} = u^{(0)} + \\beta^{(1)} - z^{(1)} = \\mathbf{0} + \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix}$.\n$v^{(1)} = v^{(0)} + D\\beta^{(1)} - s^{(1)} = \\mathbf{0} + \\begin{pmatrix} -1 \\\\ 2 \\\\ -3 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\\\ -1 \\end{pmatrix}$.\n\n### Compute the Final Quantity\n\nThe problem asks for the squared Euclidean norm of the stacked primal constraint-violation vector, $\\|r^{(1)}\\|_{2}^{2}$. The vector $r^{(1)}$ is defined as:\n$$\nr^{(1)} = \\begin{pmatrix} \\beta^{(1)} - z^{(1)} \\\\ D\\beta^{(1)} - s^{(1)} \\end{pmatrix}\n$$\nThe two components of $r^{(1)}$ are exactly the updates for the dual variables (since initial duals were zero).\n$$\n\\beta^{(1)} - z^{(1)} = u^{(1)} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix}\n$$\n$$\nD\\beta^{(1)} - s^{(1)} = v^{(1)} = \\begin{pmatrix} -1 \\\\ 1 \\\\ -1 \\end{pmatrix}\n$$\nThus, the stacked vector is:\n$$\nr^{(1)} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ -1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{pmatrix}\n$$\nIts squared Euclidean norm is the sum of the squares of its components:\n$$\n\\|r^{(1)}\\|_{2}^{2} = 1^2 + 0^2 + 1^2 + (-1)^2 + (-1)^2 + 1^2 + (-1)^2 = 1 + 0 + 1 + 1 + 1 + 1 + 1 = 6.\n$$", "answer": "$$\n\\boxed{6}\n$$", "id": "3447147"}]}