{"hands_on_practices": [{"introduction": "To begin, we will solidify our understanding of the latent variable formulation, which is a cornerstone for both theoretical analysis and algorithmic design in overlapping group LASSO. This exercise asks you to manually construct the key components of this formulation for a small, concrete example. By building the latent variables and the duplication matrix from scratch, you will demystify the abstract definitions and gain a practical grasp of the algebraic structure that enables the method's flexibility [@problem_id:3465480].", "problem": "Consider the overlapping group least absolute shrinkage and selection operator (LASSO) penalty in a latent-variable formulation used in compressed sensing and sparse optimization. Let there be a parameter vector $\\beta \\in \\mathbb{R}^{p}$ with $p=5$ and overlapping groups $g_1=\\{1,2,3\\}$, $g_2=\\{3,4\\}$, and $g_3=\\{4,5\\}$. In the latent-variable reformulation, for each group $g$, introduce a group-restricted latent variable $\\beta^{(g)} \\in \\mathbb{R}^{|g|}$ that stores a copy of the components of $\\beta$ indexed by $g$, and define the stacked latent vector $z \\in \\mathbb{R}^{\\sum_{g} |g|}$ by concatenating the $\\beta^{(g)}$ in the order $(g_1,g_2,g_3)$. The duplication mapping $D \\in \\mathbb{R}^{(\\sum_{g} |g|)\\times p}$ is defined by the linear relation $z = D \\beta$, where each row of $D$ selects the appropriate component of $\\beta$ that corresponds to the latent copy indexed by that row.\n\nStarting from the basic definition of the duplication mapping and the latent variable construction described above, do the following:\n- Explicitly write the latent vectors $\\beta^{(g_1)}$, $\\beta^{(g_2)}$, and $\\beta^{(g_3)}$ in terms of the entries of $\\beta$.\n- Explicitly construct the matrix $D$.\n- Write the consensus constraint in matrix form linking $z$ and $\\beta$.\n\nThen, using only linear algebra operations and without appealing to any pre-stated shortcut formulas, derive $D^{\\top} D$ and compute its determinant. Provide the determinant as your final answer. If your final result is a number, give it exactly; no rounding is required.", "solution": "We begin from the latent-variable construction for overlapping groups: for each group $g \\subset \\{1,\\dots,p\\}$, we introduce a latent vector $\\beta^{(g)} \\in \\mathbb{R}^{|g|}$ that stores a copy of the components of $\\beta$ indexed by $g$. The stacked latent vector is formed as $z = \\big(\\beta^{(g_1)};\\beta^{(g_2)};\\beta^{(g_3)}\\big) \\in \\mathbb{R}^{|g_1|+|g_2|+|g_3|}$, where we use the semicolon to denote vertical concatenation.\n\nGiven $p=5$ and the groups $g_1=\\{1,2,3\\}$, $g_2=\\{3,4\\}$, and $g_3=\\{4,5\\}$, the latent variables are the group-restricted copies of $\\beta = (\\beta_1,\\beta_2,\\beta_3,\\beta_4,\\beta_5)^{\\top}$:\n- For $g_1=\\{1,2,3\\}$, the latent vector is\n$$\n\\beta^{(g_1)} = \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{pmatrix}.\n$$\n- For $g_2=\\{3,4\\}$, the latent vector is\n$$\n\\beta^{(g_2)} = \\begin{pmatrix} \\beta_3 \\\\ \\beta_4 \\end{pmatrix}.\n$$\n- For $g_3=\\{4,5\\}$, the latent vector is\n$$\n\\beta^{(g_3)} = \\begin{pmatrix} \\beta_4 \\\\ \\beta_5 \\end{pmatrix}.\n$$\n\nConcatenating in the order $(g_1,g_2,g_3)$ gives the stacked latent vector $z \\in \\mathbb{R}^{7}$:\n$$\nz \\;=\\; \\begin{pmatrix}\n\\beta^{(g_1)} \\\\ \\beta^{(g_2)} \\\\ \\beta^{(g_3)}\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_3 \\\\ \\beta_4 \\\\ \\beta_4 \\\\ \\beta_5\n\\end{pmatrix}.\n$$\n\nBy definition of the duplication mapping, there exists a matrix $D \\in \\mathbb{R}^{7 \\times 5}$ such that $z = D \\beta$. Each row of $D$ places a $1$ in the column corresponding to the index of $\\beta$ that is being copied and zeros elsewhere. Following the order of entries in $z$ written above, the seven rows of $D$ are:\n- Row $1$ corresponds to $\\beta_1$,\n- Row $2$ corresponds to $\\beta_2$,\n- Rows $3$ and $4$ both correspond to $\\beta_3$ (due to the overlap between $g_1$ and $g_2$),\n- Rows $5$ and $6$ both correspond to $\\beta_4$ (due to the overlap between $g_2$ and $g_3$),\n- Row $7$ corresponds to $\\beta_5$.\n\nThus,\n$$\nD \\;=\\;\n\\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1\n\\end{pmatrix}.\n$$\n\nThe consensus constraint that links the latent copies to the original variable is precisely the linear relation\n$$\nz - D \\beta \\;=\\; 0,\n$$\nequivalently $z = D \\beta$. This ensures that all group-wise copies of a shared coordinate are equal to the corresponding original coordinate. In coordinate form, this enforces equalities such as the two copies for index $3$ being equal to $\\beta_3$, and the two copies for index $4$ being equal to $\\beta_4$.\n\nNext, we compute $D^{\\top} D$. By construction, each row of $D$ has exactly one entry equal to $1$ and all other entries $0$, and duplicates occur exactly when an original index appears in multiple groups. Consequently, the product $D^{\\top} D$ is diagonal, with the $(i,i)$ entry equal to the number of times index $i$ appears across all groups. We verify this explicitly.\n\nLet $e_i \\in \\mathbb{R}^{5}$ be the $i$-th standard basis vector. Each row of $D$ is one of the $e_i^{\\top}$. If an index $i$ appears $c_i$ times in the stacked latent vector $z$ (i.e., in the groups), then $D$ contains $c_i$ copies of the row $e_i^{\\top}$. Therefore,\n$$\nD^{\\top} D \\;=\\; \\sum_{k=1}^{7} r_k^{\\top} r_k \\;=\\; \\sum_{i=1}^{5} c_i \\, e_i e_i^{\\top} \\;=\\; \\operatorname{diag}(c_1, c_2, c_3, c_4, c_5),\n$$\nwhere $r_k$ denotes the $k$-th row of $D$.\n\nCounting occurrences from the group structure:\n- Index $1$ appears only in $g_1$, so $c_1 = 1$.\n- Index $2$ appears only in $g_1$, so $c_2 = 1$.\n- Index $3$ appears in $g_1$ and $g_2$, so $c_3 = 2$.\n- Index $4$ appears in $g_2$ and $g_3$, so $c_4 = 2$.\n- Index $5$ appears only in $g_3$, so $c_5 = 1$.\n\nHence,\n$$\nD^{\\top} D \\;=\\; \\operatorname{diag}(1,\\,1,\\,2,\\,2,\\,1).\n$$\n\nFinally, the determinant of a diagonal matrix is the product of its diagonal entries. Therefore,\n$$\n\\det(D^{\\top} D) \\;=\\; 1 \\times 1 \\times 2 \\times 2 \\times 1 \\;=\\; 4.\n$$\n\nThis is the requested scalar quantity.", "answer": "$$\\boxed{4}$$", "id": "3465480"}, {"introduction": "Having established the mechanics of the latent variable view, we now explore the unique statistical behavior it enables. This practice challenges you to solve a carefully designed problem from first principles, using subgradient optimality conditions to reveal how overlapping group LASSO can identify sparse patterns that do not conform to the initial group structure. This exercise is crucial for developing intuition about the estimator's solutions and honing your skills in subgradient calculus [@problem_id:3465473].", "problem": "Consider an overlapping group Least Absolute Shrinkage and Selection Operator (LASSO) problem in dimension $p=4$ with squared-error data fit and two overlapping groups. Let the decision variable be $x \\in \\mathbb{R}^{4}$, and consider the convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{4}} \\;\\; \\frac{1}{2}\\|x - b\\|_{2}^{2} + \\lambda\\Big(\\|x_{G_{1}}\\|_{2} + \\|x_{G_{2}}\\|_{2}\\Big),\n$$\nwhere the groups are $G_{1}=\\{1,2\\}$ and $G_{2}=\\{2,3,4\\}$, and $\\lambda>0$. The vector $x_{G}$ denotes the subvector of $x$ restricted to indices in $G$. Take $\\lambda=1$ and the data vector $b \\in \\mathbb{R}^{4}$ to be\n$$\nb_{1}=0, \\quad b_{2}=3+\\frac{2}{\\sqrt{5}}, \\quad b_{3}=1+\\frac{1}{\\sqrt{5}}, \\quad b_{4}=0.\n$$\nStart from first principles of convex optimality and subgradient calculus for the overlapping group penalty to analyze this instance. Establish that the unique minimizer $x^{\\star}$ has support equal to the union of features $\\{2,3\\}$, which is not expressible as a union of disjoint whole groups with the same group system $\\{G_{1},G_{2}\\}$, and determine the exact optimal subgradient coordinate at the intersection feature $2$, that is, the value of the second component of a subgradient of the penalty at $x^{\\star}$ that participates in the optimality condition.\n\nReport as your final answer the exact value of this subgradient coordinate at feature $2$ at the optimizer. Do not round; provide an exact closed-form expression.", "solution": "The optimization problem is given by\n$$\n\\min_{x \\in \\mathbb{R}^{4}} \\;\\; f(x) := \\frac{1}{2}\\|x - b\\|_{2}^{2} + \\lambda\\Omega(x),\n$$\nwhere the penalty term is $\\Omega(x) = \\|x_{G_{1}}\\|_{2} + \\|x_{G_{2}}\\|_{2}$. The problem parameters are specified as:\n- Dimension: $p=4$.\n- Groups: $G_{1}=\\{1,2\\}$ and $G_{2}=\\{2,3,4\\}$.\n- Regularization parameter: $\\lambda=1$.\n- Data vector $b \\in \\mathbb{R}^{4}$: $b_{1}=0$, $b_{2}=3+\\frac{2}{\\sqrt{5}}$, $b_{3}=1+\\frac{1}{\\sqrt{5}}$, $b_{4}=0$.\n\nThe objective function $f(x)$ is strictly convex, being the sum of a strictly convex function (the squared Euclidean norm) and a convex function (the group LASSO penalty). Therefore, a unique minimizer $x^{\\star}$ exists. The first-order necessary and sufficient condition for optimality is that the zero vector must be an element of the subdifferential of $f(x)$ at $x = x^{\\star}$:\n$$\n0 \\in \\partial f(x^{\\star}) = (x^{\\star} - b) + \\lambda \\partial\\Omega(x^{\\star}).\n$$\nThis condition can be rewritten as $b - x^{\\star} \\in \\lambda \\partial\\Omega(x^{\\star})$. With $\\lambda=1$, this simplifies to $b - x^{\\star} \\in \\partial\\Omega(x^{\\star})$. This means that the vector $b - x^{\\star}$ must be a subgradient of the penalty $\\Omega(x)$ at the optimal point $x^{\\star}$.\n\nThe subdifferential of the penalty term $\\Omega(x)$ is given by the sum of the subdifferentials of its constituent norm terms: $\\partial\\Omega(x) = \\partial_x (\\|x_{G_1}\\|_2) + \\partial_x (\\|x_{G_2}\\|_2)$. An element $g \\in \\partial\\Omega(x)$ is a vector $g \\in \\mathbb{R}^4$ of the form $g = u + v$, where $u \\in \\partial_x (\\|x_{G_1}\\|_2)$ and $v \\in \\partial_x (\\|x_{G_2}\\|_2)$. The vectors $u$ and $v$ have support restricted to $G_1$ and $G_2$, respectively.\nThe subgradient of the Euclidean norm of a subvector $x_G$ is:\n- If $x_G \\neq 0$, $\\partial_x(\\|x_G\\|_2)$ is the singleton set containing the vector $u$ where $u_G = x_G/\\|x_G\\|_2$ and $u_{G^c}=0$.\n- If $x_G = 0$, $\\partial_x(\\|x_G\\|_2)$ is the set of vectors $u$ where $\\|u_G\\|_2 \\le 1$ and $u_{G^c}=0$.\n\nThe problem requires us to establish that the support of the minimizer is $\\{2, 3\\}$. Let us hypothesize that this is true, i.e., $x^{\\star}_1=0$, $x^{\\star}_4=0$, and $x^{\\star}_2 \\neq 0, x^{\\star}_3 \\neq 0$.\nUnder this hypothesis, the subvectors corresponding to the groups are $x^{\\star}_{G_1} = (0, x^{\\star}_2)$ and $x^{\\star}_{G_2} = (x^{\\star}_2, x^{\\star}_3, 0)$. Both are non-zero vectors. Therefore, the penalty $\\Omega(x)$ is differentiable at $x^{\\star}$, and its subdifferential $\\partial\\Omega(x^{\\star})$ is a singleton set containing only the gradient $\\nabla\\Omega(x^{\\star})$.\n\nLet's compute the unique subgradient $g^{\\star} \\in \\partial\\Omega(x^{\\star})$.\nFor $G_1 = \\{1,2\\}$, we have $x^{\\star}_{G_1}=(0, x^{\\star}_2)$. The corresponding subgradient component vector $u^{\\star}$ has its components over $G_1$ given by $x^{\\star}_{G_1} / \\|x^{\\star}_{G_1}\\|_2 = (0, x^{\\star}_2) / |x^{\\star}_2| = (0, \\text{sgn}(x^{\\star}_2))$. So $u^{\\star}_1=0$ and $u^{\\star}_2=\\text{sgn}(x^{\\star}_2)$.\nFor $G_2 = \\{2,3,4\\}$, we have $x^{\\star}_{G_2}=(x^{\\star}_2, x^{\\star}_3, 0)$. The corresponding subgradient component vector $v^{\\star}$ has its components over $G_2$ given by $x^{\\star}_{G_2} / \\|x^{\\star}_{G_2}\\|_2 = (x^{\\star}_2, x^{\\star}_3, 0) / \\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}$. So, $v^{\\star}_2 = x^{\\star}_2 / \\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}$, $v^{\\star}_3 = x^{\\star}_3 / \\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}$, and $v^{\\star}_4=0$.\n\nThe components of the total subgradient $g^{\\star} = u^{\\star} + v^{\\star}$ are:\n$g^{\\star}_1 = u^{\\star}_1 = 0$\n$g^{\\star}_2 = u^{\\star}_2 + v^{\\star}_2 = \\text{sgn}(x^{\\star}_2) + \\frac{x^{\\star}_2}{\\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}}$\n$g^{\\star}_3 = v^{\\star}_3 = \\frac{x^{\\star}_3}{\\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}}$\n$g^{\\star}_4 = v^{\\star}_4 = 0$\n\nThe optimality condition $b - x^{\\star} = g^{\\star}$ provides a system of equations. Since $b_2 > 0$ and $b_3 > 0$, we can reasonably assume $x^{\\star}_2 > 0$ and $x^{\\star}_3 > 0$. Then $\\text{sgn}(x^{\\star}_2)=1$.\nFor the active indices $j \\in \\{2,3\\}$:\n1) $b_2 - x^{\\star}_2 = 1 + \\frac{x^{\\star}_2}{\\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}}$\n2) $b_3 - x^{\\star}_3 = \\frac{x^{\\star}_3}{\\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}}$\n\nFrom equation (2), we can express $\\frac{1}{\\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}} = \\frac{b_3 - x^{\\star}_3}{x^{\\star}_3}$. Substituting this into (1):\n$b_2 - x^{\\star}_2 = 1 + x^{\\star}_2 \\left( \\frac{b_3 - x^{\\star}_3}{x^{\\star}_3} \\right) = 1 + \\frac{b_3 x^{\\star}_2}{x^{\\star}_3} - x^{\\star}_2$\n$b_2 - 1 = \\frac{b_3 x^{\\star}_2}{x^{\\star}_3} \\implies x^{\\star}_3 = \\frac{b_3}{b_2 - 1} x^{\\star}_2$.\n\nLet's substitute the given values for $b_2$ and $b_3$:\n$b_2 - 1 = \\left(3 + \\frac{2}{\\sqrt{5}}\\right) - 1 = 2 + \\frac{2}{\\sqrt{5}} = \\frac{2\\sqrt{5}+2}{\\sqrt{5}}$.\n$b_3 = 1 + \\frac{1}{\\sqrt{5}} = \\frac{\\sqrt{5}+1}{\\sqrt{5}}$.\nThe ratio is $\\frac{b_3}{b_2 - 1} = \\frac{(\\sqrt{5}+1)/\\sqrt{5}}{2(\\sqrt{5}+1)/\\sqrt{5}} = \\frac{1}{2}$.\nThus, we find the linear relationship $x^{\\star}_3 = \\frac{1}{2} x^{\\star}_2$.\n\nNow, substitute this relation back into equation (2):\n$\\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2} = \\sqrt{(2x^{\\star}_3)^2 + (x^{\\star}_3)^2} = \\sqrt{5(x^{\\star}_3)^2} = \\sqrt{5}x^{\\star}_3$ (since we assumed $x^{\\star}_3>0$).\nEquation (2) becomes $b_3 - x^{\\star}_3 = \\frac{x^{\\star}_3}{\\sqrt{5}x^{\\star}_3} = \\frac{1}{\\sqrt{5}}$.\nSolving for $x^{\\star}_3$:\n$x^{\\star}_3 = b_3 - \\frac{1}{\\sqrt{5}} = \\left(1 + \\frac{1}{\\sqrt{5}}\\right) - \\frac{1}{\\sqrt{5}} = 1$.\nFrom this, we find $x^{\\star}_2 = 2x^{\\star}_3 = 2(1)=2$.\nOur candidate solution is $x^{\\star} = (0, 2, 1, 0)$. Both $x^{\\star}_2$ and $x^{\\star}_3$ are positive, which is consistent with our earlier assumption.\n\nWe must verify that this solution satisfies the optimality conditions for the inactive indices $j \\in \\{1,4\\}$.\nFor $j=1$: $b_1 - x^{\\star}_1 = 0 - 0 = 0$. The subgradient component is $g^{\\star}_1 = 0$, as calculated before. The condition $b_1 - x^{\\star}_1 = g^{\\star}_1$ is satisfied.\nFor $j=4$: $b_4 - x^{\\star}_4 = 0 - 0 = 0$. The subgradient component is $g^{\\star}_4 = 0$, as calculated before. The condition $b_4 - x^{\\star}_4 = g^{\\star}_4$ is satisfied.\nSince all optimality conditions are met, and the minimizer is unique, we have confirmed that $x^{\\star} = (0, 2, 1, 0)$ is the unique solution. Its support is indeed $\\{2, 3\\}$.\n\nThe problem states that this support is not expressible as a union of groups from the system $\\{G_1, G_2\\}$. The possible unions of groups are $\\emptyset$, $G_1=\\{1,2\\}$, $G_2=\\{2,3,4\\}$, and $G_1 \\cup G_2 = \\{1,2,3,4\\}$. The support of $x^{\\star}$, which is $\\{2,3\\}$, does not match any of these sets. This demonstrates how overlapping group LASSO can select feature sets that do not conform to the predefined group structure, instead selecting features that participate in multiple important groups.\n\nFinally, we need to find the value of the second component of the subgradient of the penalty $\\Omega(x)$ at $x^{\\star}$ that participates in the optimality condition. This is the component $g^{\\star}_2$ of the vector $g^{\\star} = b - x^{\\star}$ (since $\\lambda=1$).\nThis value can be calculated in two ways.\nFirst, using the formula we derived for the subgradient components:\n$g^{\\star}_2 = \\text{sgn}(x^{\\star}_2) + \\frac{x^{\\star}_2}{\\sqrt{(x^{\\star}_2)^2 + (x^{\\star}_3)^2}} = 1 + \\frac{2}{\\sqrt{2^2 + 1^2}} = 1 + \\frac{2}{\\sqrt{5}}$.\nSecond, using the optimality condition directly:\n$g^{\\star}_2 = b_2 - x^{\\star}_2 = \\left(3 + \\frac{2}{\\sqrt{5}}\\right) - 2 = 1 + \\frac{2}{\\sqrt{5}}$.\nBoth methods yield the same result. The value is an exact expression as required.", "answer": "$$\n\\boxed{1 + \\frac{2}{\\sqrt{5}}}\n$$", "id": "3465473"}, {"introduction": "This advanced exercise will guide you through the derivation of the dual problem for the overlapping group LASSO, a key step in understanding many state-of-the-art solvers. Every convex optimization problem has a corresponding dual problem, and the duality gap provides a powerful, computable measure of suboptimality. By constructing a dual certificate and computing the duality gap, you will learn a fundamental technique for monitoring algorithm convergence and certifying the quality of a solution [@problem_id:3465446].", "problem": "Consider a linear inverse problem with design matrix $X \\in \\mathbb{R}^{n \\times p}$ and observations $y \\in \\mathbb{R}^{n}$. The overlapping group Least Absolute Shrinkage and Selection Operator (LASSO) penalizes the parameter vector $\\beta \\in \\mathbb{R}^{p}$ by the latent overlapping group norm constructed from a family of index sets $\\{g_k\\}_{k=1}^{K}$, where each $g_k \\subset \\{1,\\dots,p\\}$ may overlap. Define selection operators $E_{g_k}: \\mathbb{R}^{p} \\to \\mathbb{R}^{|g_k|}$ that restrict a vector to the coordinates in group $g_k$. The latent overlapping group norm is\n$$\n\\Omega(\\beta) \\;=\\; \\inf_{\\{v^{(k)}\\}_{k=1}^{K}} \\left\\{ \\sum_{k=1}^{K} w_k \\,\\|v^{(k)}\\|_{2} \\;:\\; \\sum_{k=1}^{K} E_{g_k}^{\\top} v^{(k)} \\;=\\; \\beta \\right\\},\n$$\nwith given positive weights $w_k$. The regularized least-squares estimator solves\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\frac{1}{2}\\,\\|y - X\\beta\\|_{2}^{2} \\;+\\; \\lambda\\,\\Omega(\\beta),\n$$\nfor a regularization parameter $\\lambda > 0$.\n\nStarting from first principles of Fenchel-Rockafellar duality for convex functions and the definition of convex conjugates, derive the dual problem and the dual feasibility conditions in terms of the dual norm $\\Omega^{*}$. Using these conditions, specify how a dual certificate can be constructed from a given primal iterate by using only quantities available from that iterate. Then, for the concrete instance below, compute a valid dual certificate and evaluate the duality gap to monitor convergence.\n\nData, groups, and parameters:\n- $n=3$, $p=4$, $X$ and $y$ are\n$$\nX \\;=\\; \\begin{pmatrix}\n1 & 0 & 1 & 0 \\\\\n0 & 1 & 1 & 1 \\\\\n1 & 1 & 0 & 1\n\\end{pmatrix}, \\qquad\ny \\;=\\; \\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\n- Overlapping groups $g_1 = \\{1,2\\}$, $g_2 = \\{2,3\\}$, $g_3 = \\{3,4\\}$ with weights $w_1 = w_2 = w_3 = 1$ and regularization parameter $\\lambda = 1$.\n- Primal iterate\n$$\n\\beta \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 2 \\end{pmatrix}.\n$$\n\nTasks:\n1. Derive the dual problem and show that the dual feasible set can be expressed as $\\{u \\in \\mathbb{R}^{n} : \\|X_{g_k}^{\\top} u\\|_{2} \\leq \\lambda w_k \\text{ for all } k = 1,2,3\\}$, where $X_{g_k}$ denotes the submatrix of $X$ formed by the columns indexed by $g_k$.\n2. From the given $\\beta$, construct a dual certificate $u$ using only residual information and adjust it, if necessary, to satisfy the dual feasibility conditions you derived.\n3. Compute the primal objective value $P(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\Omega(\\beta)$ using the latent formulation of $\\Omega$ above, and compute the dual objective value $D(u)$ at your dual certificate.\n4. Report the duality gap $G(\\beta,u) = P(\\beta) - D(u)$ as a single real number. If you choose to round, round your final answer to four significant figures; otherwise, provide an exact value.\n\nYour final answer must be a single number or a single closed-form analytic expression.", "solution": "The problem asks for the derivation of the dual of the overlapping group LASSO problem, the construction of a dual certificate for a given primal iterate, and the computation of the duality gap. We address each task in sequence.\n\n### Task 1: Derivation of the Dual Problem\n\nThe primal problem is given by:\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\frac{1}{2}\\,\\|y - X\\beta\\|_{2}^{2} \\;+\\; \\lambda\\,\\Omega(\\beta)\n$$\nThis is a convex optimization problem of the form $\\min_{\\beta} f(\\beta) + g(\\beta)$, where $f(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2}$ and $g(\\beta) = \\lambda\\Omega(\\beta)$. To derive the dual problem using Fenchel-Rockafellar duality, we can introduce an auxiliary variable $z \\in \\mathbb{R}^{n}$ and rewrite the problem as:\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}, z \\in \\mathbb{R}^{n}} \\;\\frac{1}{2}\\,\\|y - z\\|_{2}^{2} \\;+\\; \\lambda\\,\\Omega(\\beta) \\quad \\text{subject to} \\quad z = X\\beta\n$$\nThe Lagrangian for this problem, with a dual variable $u \\in \\mathbb{R}^{n}$ for the constraint $X\\beta - z = 0$, is:\n$$\nL(\\beta, z; u) = \\frac{1}{2}\\|y - z\\|_{2}^{2} + \\lambda\\,\\Omega(\\beta) + u^{\\top}(X\\beta - z)\n$$\nThe Lagrange dual function $D(u)$ is the infimum of the Lagrangian over the primal variables $\\beta$ and $z$:\n$$\nD(u) = \\inf_{\\beta \\in \\mathbb{R}^{p}, z \\in \\mathbb{R}^{n}} L(\\beta, z; u) = \\inf_{z \\in \\mathbb{R}^{n}} \\left( \\frac{1}{2}\\|y - z\\|_{2}^{2} - u^{\\top}z \\right) + \\inf_{\\beta \\in \\mathbb{R}^{p}} \\left( \\lambda\\,\\Omega(\\beta) + u^{\\top}X\\beta \\right)\n$$\nWe compute the two infima separately.\n\nFor the first term involving $z$, the objective is a quadratic function of $z$. We find the minimum by setting the gradient with respect to $z$ to zero:\n$$\n\\nabla_{z} \\left( \\frac{1}{2}\\|y - z\\|_{2}^{2} - u^{\\top}z \\right) = -(y - z) - u = 0 \\implies z = y + u\n$$\nSubstituting this back gives the minimum value:\n$$\n\\frac{1}{2}\\|y - (y+u)\\|_{2}^{2} - u^{\\top}(y+u) = \\frac{1}{2}\\|-u\\|_{2}^{2} - u^{\\top}y - \\|u\\|_{2}^{2} = -y^{\\top}u - \\frac{1}{2}\\|u\\|_{2}^{2}\n$$\nFor the second term involving $\\beta$, we use the definition of the convex conjugate. The conjugate of a function $h(\\beta)$ is $h^{*}(\\alpha) = \\sup_{\\beta} (\\alpha^{\\top}\\beta - h(\\beta))$. Thus,\n$$\n\\inf_{\\beta \\in \\mathbb{R}^{p}} \\left( \\lambda\\,\\Omega(\\beta) + u^{\\top}X\\beta \\right) = \\inf_{\\beta} \\left( \\lambda\\,\\Omega(\\beta) - (-X^{\\top}u)^{\\top}\\beta \\right) = - \\sup_{\\beta} \\left( (-X^{\\top}u)^{\\top}\\beta - \\lambda\\,\\Omega(\\beta) \\right) = - (\\lambda\\Omega)^{*}(-X^{\\top}u)\n$$\nUsing the scaling property of conjugates, $(c h)^{*}(\\alpha) = c h^{*}(\\alpha/c)$ for $c > 0$, we have $(\\lambda\\Omega)^{*}(\\alpha) = \\lambda\\Omega^{*}(\\alpha/\\lambda)$. Therefore:\n$$\n- (\\lambda\\Omega)^{*}(-X^{\\top}u) = - \\lambda\\Omega^{*}\\left(-\\frac{1}{\\lambda}X^{\\top}u\\right)\n$$\nNow, we must find the conjugate of the latent overlapping group norm, $\\Omega(\\beta)$.\n$$\n\\Omega^{*}(\\alpha) = \\sup_{\\beta} \\left( \\alpha^{\\top}\\beta - \\Omega(\\beta) \\right) = \\sup_{\\beta} \\left( \\alpha^{\\top}\\beta - \\inf_{\\{v^{(k)}\\}} \\left\\{ \\sum_{k=1}^{K} w_k \\|v^{(k)}\\|_{2} \\mid \\sum_{k=1}^{K} E_{g_k}^{\\top} v^{(k)} = \\beta \\right\\} \\right)\n$$\nBy swapping the supremum and infimum, we get:\n$$\n\\Omega^{*}(\\alpha) = \\sup_{\\beta, \\{v^{(k)}\\}} \\left\\{ \\alpha^{\\top}\\beta - \\sum_{k=1}^{K} w_k \\|v^{(k)}\\|_{2} \\mid \\sum_{k=1}^{K} E_{g_k}^{\\top} v^{(k)} = \\beta \\right\\}\n$$\nSubstituting for $\\beta$:\n$$\n\\Omega^{*}(\\alpha) = \\sup_{\\{v^{(k)}\\}} \\left\\{ \\alpha^{\\top}\\left(\\sum_{k=1}^{K} E_{g_k}^{\\top} v^{(k)}\\right) - \\sum_{k=1}^{K} w_k \\|v^{(k)}\\|_{2} \\right\\} = \\sum_{k=1}^{K} \\sup_{v^{(k)}} \\left( (E_{g_k}\\alpha)^{\\top}v^{(k)} - w_k \\|v^{(k)}\\|_{2} \\right)\n$$\nThe term $\\sup_{v} (s^{\\top}v - w\\|v\\|_{2})$ is the conjugate of the function $w\\|\\cdot\\|_{2}$. This conjugate is $0$ if $\\|s\\|_{2} \\le w$ and $+\\infty$ otherwise. It is the indicator function of the Euclidean ball of radius $w$.\nThus, $\\Omega^{*}(\\alpha)$ is $0$ if $\\|E_{g_k}\\alpha\\|_{2} \\le w_k$ for all $k=1, \\dots, K$, and $+\\infty$ otherwise. This is the indicator function of the set $\\mathcal{C} = \\{\\alpha \\in \\mathbb{R}^{p} \\mid \\|E_{g_k}\\alpha\\|_{2} \\le w_k, \\forall k \\}$.\n\nThe term $- \\lambda\\Omega^{*}\\left(-\\frac{1}{\\lambda}X^{\\top}u\\right)$ is finite (and equal to $0$) if and only if $-\\frac{1}{\\lambda}X^{\\top}u \\in \\mathcal{C}$, which means:\n$$\n\\left\\| E_{g_k}\\left(-\\frac{1}{\\lambda}X^{\\top}u\\right) \\right\\|_{2} \\le w_k \\quad \\text{for all } k=1, \\dots, K\n$$\nSince $E_{g_k}$ is a linear operator that selects coordinates, this is equivalent to:\n$$\n\\frac{1}{\\lambda} \\| (X^{\\top}u)_{g_k} \\|_{2} \\le w_k \\iff \\|X_{g_k}^{\\top}u\\|_{2} \\le \\lambda w_k\n$$\nThis is the dual feasibility condition.\n\nCombining the parts, the dual function is:\n$$\nD(u) = -y^{\\top}u - \\frac{1}{2}\\|u\\|_{2}^{2}\n$$\nThe dual problem is to maximize $D(u)$ subject to the feasibility conditions:\n$$\n\\max_{u \\in \\mathbb{R}^{n}} \\left\\{ -y^{\\top}u - \\frac{1}{2}\\|u\\|_{2}^{2} \\right\\} \\quad \\text{subject to} \\quad \\|X_{g_k}^{\\top}u\\|_{2} \\le \\lambda w_k \\quad \\text{for } k=1, \\dots, K\n$$\nThe dual feasible set is therefore $\\{u \\in \\mathbb{R}^{n} : \\|X_{g_k}^{\\top} u\\|_{2} \\leq \\lambda w_k \\text{ for all } k = 1,2,3\\}$. This completes the first task.\n\n### Task 2: Construction of a Dual Certificate\n\nWe are given $n=3$, $p=4$, $\\lambda=1$, $w_1=w_2=w_3=1$, and\n$$\nX = \\begin{pmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 1 & 1 \\\\ 1 & 1 & 0 & 1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\end{pmatrix}, \\quad \\beta = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 2 \\end{pmatrix}\n$$\nA natural candidate for the dual variable is related to the primal residual $r = y - X\\beta$. From the KKT conditions, at optimality, $u = y - X\\beta$. We use this as a starting point.\nFirst, compute the residual:\n$$\nX\\beta = \\begin{pmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 1 & 1 \\\\ 1 & 1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\n$$\n$$\nr = y - X\\beta = \\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -2 \\\\ -2 \\end{pmatrix}\n$$\nLet's call this candidate $u_0 = r$. Now we check if $u_0$ is dual feasible. The condition is $\\|X_{g_k}^{\\top}u_0\\|_{2} \\le 1$ for $k=1,2,3$.\nThe groups are $g_1 = \\{1,2\\}$, $g_2 = \\{2,3\\}$, $g_3 = \\{3,4\\}$. The submatrices are:\n$$\nX_{g_1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}, \\quad X_{g_2} = \\begin{pmatrix} 0 & 1 \\\\ 1 & 1 \\\\ 1 & 0 \\end{pmatrix}, \\quad X_{g_3} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \\end{pmatrix}\n$$\nNow check the norms:\nFor $k=1$: $X_{g_1}^{\\top}u_0 = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ -2 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -4 \\end{pmatrix}$. $\\|X_{g_1}^{\\top}u_0\\|_{2} = \\sqrt{(-1)^2 + (-4)^2} = \\sqrt{17}$.\nFor $k=2$: $X_{g_2}^{\\top}u_0 = \\begin{pmatrix} 0 & 1 & 1 \\\\ 1 & 1 & 0 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ -2 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} -4 \\\\ -1 \\end{pmatrix}$. $\\|X_{g_2}^{\\top}u_0\\|_{2} = \\sqrt{(-4)^2 + (-1)^2} = \\sqrt{17}$.\nFor $k=3$: $X_{g_3}^{\\top}u_0 = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ -2 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -4 \\end{pmatrix}$. $\\|X_{g_3}^{\\top}u_0\\|_{2} = \\sqrt{(-1)^2 + (-4)^2} = \\sqrt{17}$.\n\nSince $\\sqrt{17} > 1$, $u_0$ is not a feasible dual certificate. To make it feasible, we scale it by a factor $c > 0$. Let $u = c u_0$. The feasibility condition becomes $c \\|X_{g_k}^{\\top}u_0\\|_{2} \\le 1$. This must hold for all $k$. We choose the largest possible $c$:\n$$\nc = \\frac{1}{\\max_{k} \\|X_{g_k}^{\\top}u_0\\|_{2}} = \\frac{1}{\\sqrt{17}}\n$$\nOur adjusted dual certificate is $u = \\frac{1}{\\sqrt{17}}u_0 = \\frac{1}{\\sqrt{17}}\\begin{pmatrix} 1 \\\\ -2 \\\\ -2 \\end{pmatrix}$.\n\n### Task 3: Compute Primal and Dual Objective Values\n\nThe primal objective is $P(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\Omega(\\beta)$.\nThe first term is $\\frac{1}{2}\\|r\\|_{2}^{2} = \\frac{1}{2}(1^2 + (-2)^2 + (-2)^2) = \\frac{1}{2}(1+4+4) = \\frac{9}{2}$.\nThe second term requires computing $\\Omega(\\beta)$ with $\\lambda=1$.\n$$\n\\Omega(\\beta) = \\inf \\left\\{ \\|v^{(1)}\\|_{2} + \\|v^{(2)}\\|_{2} + \\|v^{(3)}\\|_{2} \\mid \\sum_{k=1}^{3} E_{g_k}^{\\top} v^{(k)} = \\beta \\right\\}\n$$\nThe constraint $\\sum E_{g_k}^{\\top}v^{(k)} = \\beta$ expands to a system of equations for the components of $v^{(k)}=(v_1^{(k)}, v_2^{(k)})^\\top$:\n$$\n\\begin{pmatrix} v_1^{(1)} \\\\ v_2^{(1)} + v_1^{(2)} \\\\ v_2^{(2)} + v_1^{(3)} \\\\ v_2^{(3)} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 2 \\end{pmatrix}\n$$\nThis gives $v_1^{(1)}=1$, $v_2^{(3)}=2$, $v_1^{(2)} = -v_2^{(1)}$, and $v_1^{(3)} = -v_2^{(2)}$. We need to minimize:\n$$\nf(v_2^{(1)}, v_2^{(2)}) = \\sqrt{1^2 + (v_2^{(1)})^2} + \\sqrt{(-v_2^{(1)})^2 + (v_2^{(2)})^2} + \\sqrt{(-v_2^{(2)})^2 + 2^2}\n$$\nThis is a convex function. Its minimum occurs where the gradient is zero. Let $a=v_2^{(1)}$ and $b=v_2^{(2)}$:\n$$\n\\frac{\\partial f}{\\partial a} = \\frac{a}{\\sqrt{1+a^2}} + \\frac{a}{\\sqrt{a^2+b^2}} = a\\left(\\frac{1}{\\sqrt{1+a^2}} + \\frac{1}{\\sqrt{a^2+b^2}}\\right) = 0 \\implies a=0\n$$\n$$\n\\frac{\\partial f}{\\partial b} = \\frac{b}{\\sqrt{a^2+b^2}} + \\frac{b}{\\sqrt{b^2+4}} = b\\left(\\frac{1}{\\sqrt{a^2+b^2}} + \\frac{1}{\\sqrt{b^2+4}}\\right) = 0 \\implies b=0\n$$\nThe minimum is at $v_2^{(1)}=0$ and $v_2^{(2)}=0$.\nThe optimal decomposition is $v^{(1)}=(1,0)^\\top$, $v^{(2)}=(0,0)^\\top$, $v^{(3)}=(0,2)^\\top$.\nThen $\\Omega(\\beta) = \\|(1,0)\\|_2 + \\|(0,0)\\|_2 + \\|(0,2)\\|_2 = 1 + 0 + 2 = 3$.\nThe primal objective value is $P(\\beta) = \\frac{9}{2} + 1 \\cdot 3 = \\frac{15}{2} = 7.5$.\n\nThe dual objective is $D(u) = -y^{\\top}u - \\frac{1}{2}\\|u\\|_{2}^{2}$. Using our certificate $u = \\frac{1}{\\sqrt{17}}(1, -2, -2)^\\top$:\n$$\ny^{\\top}u = \\begin{pmatrix} 2 & 0 & 1 \\end{pmatrix} \\frac{1}{\\sqrt{17}}\\begin{pmatrix} 1 \\\\ -2 \\\\ -2 \\end{pmatrix} = \\frac{1}{\\sqrt{17}}(2 \\cdot 1 + 0 \\cdot (-2) + 1 \\cdot (-2)) = \\frac{0}{\\sqrt{17}} = 0\n$$\n$$\n\\|u\\|_{2}^{2} = \\left(\\frac{1}{\\sqrt{17}}\\right)^2 \\|(1, -2, -2)\\|_{2}^{2} = \\frac{1}{17}(1^2 + (-2)^2 + (-2)^2) = \\frac{9}{17}\n$$\nSo, the dual objective value is $D(u) = -0 - \\frac{1}{2}\\left(\\frac{9}{17}\\right) = -\\frac{9}{34}$.\n\n### Task 4: Duality Gap\n\nThe duality gap is $G(\\beta, u) = P(\\beta) - D(u)$.\n$$\nG(\\beta, u) = \\frac{15}{2} - \\left(-\\frac{9}{34}\\right) = \\frac{15}{2} + \\frac{9}{34} = \\frac{15 \\cdot 17}{34} + \\frac{9}{34} = \\frac{255+9}{34} = \\frac{264}{34} = \\frac{132}{17}\n$$\nThe duality gap is a measure of the suboptimality of the pair $(\\beta, u)$. Since it is non-zero, the given $\\beta$ is not the optimal solution to the primal problem.\nThe exact value of the duality gap is $\\frac{132}{17}$.", "answer": "$$\\boxed{\\frac{132}{17}}$$", "id": "3465446"}]}