{"hands_on_practices": [{"introduction": "Before we can perform complex tasks like signal recovery or structure learning on graphs, we must first learn how to represent them mathematically. This exercise focuses on constructing the fundamental operators that encode a graph's topology and edge weights: the incidence matrix $B$ and the weighted graph Laplacian $L$. By working through a simple, concrete example [@problem_id:3478305], you will gain hands-on experience in building these matrices from scratch and exploring their spectral properties, which are central to many graph-based algorithms.", "problem": "Consider an undirected weighted graph with three nodes labeled $1$, $2$, and $3$, and edges $(1,2)$ of weight $2$ and $(2,3)$ of weight $1$. Choose any orientation of the edges to define the incidence matrix, for instance orient $(1,2)$ as $1 \\to 2$ and $(2,3)$ as $2 \\to 3$. Let $B \\in \\mathbb{R}^{2 \\times 3}$ denote the oriented incidence matrix whose $e$-th row has a $+1$ at the source node, a $-1$ at the sink node, and zeros elsewhere. Let $W \\in \\mathbb{R}^{2 \\times 2}$ be the diagonal matrix of edge weights, with the entry $W_{ee}$ equal to the weight of edge $e$. The weighted graph Laplacian is defined by the fundamental relation $L = B^{\\top} W B$.\n\nThis graph-based operator plays a central role in sparse recovery on graphs via total variation regularization, where the graph difference operator $B$ encodes sparsity in edge differences, and in the Graphical Least Absolute Shrinkage and Selection Operator (Graphical LASSO), where Laplacian-structured precision matrices lead to likelihood terms involving the pseudo-determinant of $L$ (the product of its nonzero eigenvalues).\n\nTasks:\n- Explicitly construct $B$ and $L$ from the given graph and orientation using the above fundamental definitions.\n- Compute all eigenvalues and corresponding eigenvectors of $L$.\n- Using the spectral decomposition of $L$, compute the pseudo-determinant of $L$, defined as the product of all nonzero eigenvalues.\n\nProvide your final answer as the value of the pseudo-determinant of $L$. No rounding is required, and no units are to be reported. The intermediate steps must be fully justified from first principles as outlined above.", "solution": "The problem requires the calculation of the pseudo-determinant of a weighted graph Laplacian matrix $L$. The solution proceeds in three steps: first, constructing the Laplacian matrix $L$ from the given graph structure and definitions; second, computing the eigenvalues of $L$; and third, calculating the pseudo-determinant as the product of the nonzero eigenvalues.\n\nFirst, we construct the necessary matrices based on the problem statement. The graph has three nodes, which we label as $\\{1, 2, 3\\}$, and two edges. The edges are $e_1 = (1,2)$ with weight $w_1 = 2$ and $e_2 = (2,3)$ with weight $w_2 = 1$. The problem specifies an orientation for the edges to define the incidence matrix: $1 \\to 2$ for the first edge and $2 \\to 3$ for the second edge.\n\nThe oriented incidence matrix $B \\in \\mathbb{R}^{2 \\times 3}$ has rows corresponding to edges and columns to nodes. For an edge oriented from node $u$ to node $v$, the corresponding row has a $+1$ in the column for $u$ and a $-1$ in the column for $v$.\nFor edge $e_1: 1 \\to 2$, the first row of $B$ is $\\begin{pmatrix} 1 & -1 & 0 \\end{pmatrix}$.\nFor edge $e_2: 2 \\to 3$, the second row of $B$ is $\\begin{pmatrix} 0 & 1 & -1 \\end{pmatrix}$.\nCombining these rows gives the incidence matrix:\n$$\nB = \\begin{pmatrix} 1 & -1 & 0 \\\\ 0 & 1 & -1 \\end{pmatrix}\n$$\nThe transpose of $B$ is:\n$$\nB^{\\top} = \\begin{pmatrix} 1 & 0 \\\\ -1 & 1 \\\\ 0 & -1 \\end{pmatrix}\n$$\nThe weight matrix $W \\in \\mathbb{R}^{2 \\times 2}$ is a diagonal matrix containing the edge weights. The weights are $w_1 = 2$ and $w_2 = 1$.\n$$\nW = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe weighted graph Laplacian $L$ is defined as $L = B^{\\top} W B$. We compute this product:\nFirst, we compute $W B$:\n$$\nW B = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & -1 & 0 \\\\ 0 & 1 & -1 \\end{pmatrix} = \\begin{pmatrix} 2 & -2 & 0 \\\\ 0 & 1 & -1 \\end{pmatrix}\n$$\nNext, we compute $L = B^{\\top} (W B)$:\n$$\nL = B^{\\top} (W B) = \\begin{pmatrix} 1 & 0 \\\\ -1 & 1 \\\\ 0 & -1 \\end{pmatrix} \\begin{pmatrix} 2 & -2 & 0 \\\\ 0 & 1 & -1 \\end{pmatrix} = \\begin{pmatrix} (1)(2) + (0)(0) & (1)(-2) + (0)(1) & (1)(0) + (0)(-1) \\\\ (-1)(2) + (1)(0) & (-1)(-2) + (1)(1) & (-1)(0) + (1)(-1) \\\\ (0)(2) + (-1)(0) & (0)(-2) + (-1)(1) & (0)(0) + (-1)(-1) \\end{pmatrix}\n$$\n$$\nL = \\begin{pmatrix} 2 & -2 & 0 \\\\ -2 & 3 & -1 \\\\ 0 & -1 & 1 \\end{pmatrix}\n$$\nThis completes the first task of constructing $B$ and $L$.\n\nThe second task is to compute the eigenvalues of $L$. The eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(L - \\lambda I) = 0$, where $I$ is the $3 \\times 3$ identity matrix.\n$$\nL - \\lambda I = \\begin{pmatrix} 2-\\lambda & -2 & 0 \\\\ -2 & 3-\\lambda & -1 \\\\ 0 & -1 & 1-\\lambda \\end{pmatrix}\n$$\nWe compute the determinant:\n$$\n\\det(L - \\lambda I) = (2-\\lambda) \\begin{vmatrix} 3-\\lambda & -1 \\\\ -1 & 1-\\lambda \\end{vmatrix} - (-2) \\begin{vmatrix} -2 & -1 \\\\ 0 & 1-\\lambda \\end{vmatrix}\n$$\n$$\n= (2-\\lambda)((3-\\lambda)(1-\\lambda) - 1) + 2((-2)(1-\\lambda) - 0)\n$$\n$$\n= (2-\\lambda)(\\lambda^2 - 4\\lambda + 3 - 1) + 2(-2 + 2\\lambda)\n$$\n$$\n= (2-\\lambda)(\\lambda^2 - 4\\lambda + 2) - 4 + 4\\lambda\n$$\n$$\n= 2\\lambda^2 - 8\\lambda + 4 - \\lambda^3 + 4\\lambda^2 - 2\\lambda - 4 + 4\\lambda\n$$\n$$\n= -\\lambda^3 + 6\\lambda^2 - 6\\lambda\n$$\nSetting the characteristic polynomial to zero to find the eigenvalues:\n$$\n-\\lambda^3 + 6\\lambda^2 - 6\\lambda = 0\n$$\n$$\n-\\lambda(\\lambda^2 - 6\\lambda + 6) = 0\n$$\nOne eigenvalue is immediately identified as $\\lambda_1 = 0$. This is expected for the Laplacian of a connected graph; the corresponding eigenvector is the vector of all ones, $v_1 = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix}^{\\top}$, since the row sums of $L$ are not all zero, but the sum of elements in each row of an unweighted Laplacian is zero. For a weighted Laplacian, $L\\mathbf{1}=0$ still holds.\n\nThe other two eigenvalues are the roots of the quadratic equation $\\lambda^2 - 6\\lambda + 6 = 0$. Using the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$:\n$$\n\\lambda = \\frac{6 \\pm \\sqrt{(-6)^2 - 4(1)(6)}}{2(1)} = \\frac{6 \\pm \\sqrt{36 - 24}}{2} = \\frac{6 \\pm \\sqrt{12}}{2}\n$$\nSince $\\sqrt{12} = \\sqrt{4 \\times 3} = 2\\sqrt{3}$, the eigenvalues are:\n$$\n\\lambda = \\frac{6 \\pm 2\\sqrt{3}}{2} = 3 \\pm \\sqrt{3}\n$$\nSo, the three eigenvalues of $L$ are $\\lambda_1 = 0$, $\\lambda_2 = 3 - \\sqrt{3}$, and $\\lambda_3 = 3 + \\sqrt{3}$.\n\nThe final task is to compute the pseudo-determinant of $L$, which is defined as the product of all its nonzero eigenvalues. The nonzero eigenvalues are $\\lambda_2 = 3 - \\sqrt{3}$ and $\\lambda_3 = 3 + \\sqrt{3}$.\nThe pseudo-determinant, denoted $\\det^+(L)$, is:\n$$\n\\det^+(L) = \\lambda_2 \\times \\lambda_3 = (3 - \\sqrt{3})(3 + \\sqrt{3})\n$$\nUsing the difference of squares identity $(a-b)(a+b) = a^2 - b^2$:\n$$\n\\det^+(L) = 3^2 - (\\sqrt{3})^2 = 9 - 3 = 6\n$$\nThe pseudo-determinant of the graph Laplacian $L$ is $6$.", "answer": "$$\\boxed{6}$$", "id": "3478305"}, {"introduction": "The Graphical LASSO is a cornerstone technique for inferring conditional independence relationships—and thus a sparse graph structure—from data. While the general problem requires sophisticated numerical solvers, analyzing the simplest non-trivial case for a two-node graph provides profound insight into the mechanism of sparsity-induction. This practice [@problem_id:3478302] guides you through a first-principles derivation using subgradient calculus to reveal the precise relationship between the regularization parameter $\\lambda$ and the sample covariance matrix $S$ that determines whether an edge exists in the estimated graph.", "problem": "Consider a zero-mean Gaussian graphical model with precision matrix $\\Theta \\in \\mathbb{S}_{++}^{2}$ and sample covariance matrix $S \\in \\mathbb{S}_{++}^{2}$ given by\n$$\nS \\;=\\; \\begin{pmatrix} s_{11} & s_{12} \\\\ s_{12} & s_{22} \\end{pmatrix},\n$$\nwhere $s_{11} > 0$, $s_{22} > 0$, and $s_{11} s_{22} - s_{12}^{2} > 0$. The Graphical Least Absolute Shrinkage and Selection Operator (LASSO) estimator $\\hat{\\Theta}$ is defined as the solution of the convex optimization problem\n$$\n\\min_{\\Theta \\in \\mathbb{S}_{++}^{2}} \\; \\Big\\{ -\\ln \\det(\\Theta) + \\operatorname{tr}(S \\Theta) + \\lambda \\sum_{i \\neq j} |\\theta_{ij}| \\Big\\},\n$$\nwhere the $\\ell_{1}$ penalty is applied to off-diagonal elements only, $\\lambda \\geq 0$, and $\\mathbb{S}_{++}^{2}$ denotes the cone of $2 \\times 2$ symmetric positive definite matrices.\n\nUsing only first principles, including the definition of the objective, basic properties of the matrix logarithm and trace, and subgradient optimality conditions for nonsmooth convex functions, perform the following for the case $p=2$:\n- Parametrize $\\Theta$ as $\\Theta = \\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix}$ with $a>0$, $c>0$, and $ac - b^{2} > 0$.\n- Derive the first-order optimality and subgradient conditions and solve explicitly for the optimizer $\\hat{\\Theta}$ in terms of $s_{11}$, $s_{12}$, $s_{22}$, and $\\lambda$.\n- Determine the smallest penalty level $\\lambda_{\\mathrm{th}}$ (as a function of $S$) such that for all $\\lambda \\geq \\lambda_{\\mathrm{th}}$, the optimal off-diagonal entry of $\\hat{\\Theta}$ is exactly zero.\n\nReport your final answer as a single closed-form analytic expression for $\\lambda_{\\mathrm{th}}$ in terms of the entries of $S$. No rounding is required. No units are involved.", "solution": "The goal is to find the smallest penalty level $\\lambda_{\\mathrm{th}}$ such that for all $\\lambda \\geq \\lambda_{\\mathrm{th}}$, the off-diagonal entry of the estimated precision matrix $\\hat{\\Theta}$ is zero. We start by writing the objective function and deriving its subgradient optimality conditions.\n\nUsing the specified parametrization $\\Theta = \\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix}$, the objective function is:\n$$ f(a, b, c) = -\\ln(ac - b^2) + s_{11}a + 2s_{12}b + s_{22}c + 2\\lambda|b| $$\nThis is a convex optimization problem over the constraints $a>0$, $c>0$, and $ac - b^2 > 0$. The minimum is achieved when the zero vector is in the subdifferential of the objective function. The subgradient optimality conditions at the optimum $(\\hat{a}, \\hat{b}, \\hat{c})$ are:\n$$ \\frac{\\partial f}{\\partial a} = -\\frac{c}{ac - b^2} + s_{11} = 0 $$\n$$ \\frac{\\partial f}{\\partial c} = -\\frac{a}{ac - b^2} + s_{22} = 0 $$\n$$ 0 \\in \\frac{\\partial f}{\\partial b} = \\frac{2b}{ac - b^2} + 2s_{12} + 2\\lambda \\partial|b| $$\nwhere $\\partial|b|$ is the subdifferential of the absolute value function. Letting $z \\in \\partial|b|$, the third condition becomes:\n$$ \\frac{b}{ac - b^2} + s_{12} + \\lambda z = 0 $$\nThese conditions can be compactly expressed in matrix form. Recognizing that $\\Theta^{-1} = \\frac{1}{ac-b^2}\\begin{pmatrix} c & -b \\\\ -b & a \\end{pmatrix}$, the conditions are equivalent to $\\hat{\\Theta}^{-1} = S + \\Lambda$, where $\\Lambda = \\begin{pmatrix} 0 & \\lambda z \\\\ \\lambda z & 0 \\end{pmatrix}$.\n\nWe are interested in the case where the optimal off-diagonal element is zero, i.e., $\\hat{b} = 0$. If $\\hat{b} = 0$, the precision matrix is diagonal, $\\hat{\\Theta} = \\begin{pmatrix} \\hat{a} & 0 \\\\ 0 & \\hat{c} \\end{pmatrix}$, and its inverse is $\\hat{\\Theta}^{-1} = \\begin{pmatrix} 1/\\hat{a} & 0 \\\\ 0 & 1/\\hat{c} \\end{pmatrix}$.\nThe optimality conditions for the diagonal entries give:\n- $(\\hat{\\Theta}^{-1})_{11} = 1/\\hat{a} = s_{11} \\implies \\hat{a} = 1/s_{11}$\n- $(\\hat{\\Theta}^{-1})_{22} = 1/\\hat{c} = s_{22} \\implies \\hat{c} = 1/s_{22}$\nSince $S$ is positive definite, $s_{11} > 0$ and $s_{22} > 0$, so $\\hat{a}$ and $\\hat{c}$ are positive, and the resulting $\\hat{\\Theta}$ is positive definite.\n\nNow we check the off-diagonal condition. With $\\hat{b} = 0$, $(\\hat{\\Theta}^{-1})_{12} = 0$. The condition becomes:\n$0 = s_{12} + \\lambda z$, which implies $z = -s_{12}/\\lambda$.\n\nFor $\\hat{b} = 0$, the subgradient element $z$ must be in the interval $[-1, 1]$. Therefore, a solution with $\\hat{b}=0$ exists if and only if:\n$$ -1 \\leq -\\frac{s_{12}}{\\lambda} \\leq 1 $$\nThis is equivalent to $|s_{12}/\\lambda| \\leq 1$, or, for $\\lambda > 0$:\n$$ |s_{12}| \\leq \\lambda $$\nThis inequality defines the range of $\\lambda$ values for which the optimal solution $\\hat{\\Theta}$ is diagonal. If $\\lambda  |s_{12}|$, this condition is violated, and the solution must have $\\hat{b} \\neq 0$.\n\nThe threshold $\\lambda_{\\mathrm{th}}$ is the smallest value of $\\lambda$ for which the off-diagonal entry becomes zero. This occurs precisely at the boundary of the condition.\n$$ \\lambda_{\\mathrm{th}} = |s_{12}| $$\nFor all $\\lambda \\geq |s_{12}|$, the optimal off-diagonal entry $\\hat{\\theta}_{12}$ will be zero. This result shows that the decision to include an edge is determined by comparing the regularization parameter to the magnitude of the corresponding sample covariance.", "answer": "$$\\boxed{|s_{12}|}$$", "id": "3478302"}, {"introduction": "Moving from analytical solutions to practical applications requires scalable algorithms that can handle large, arbitrary graphs. This practice demonstrates a powerful modern approach: solving the problem in the dual domain. You will learn how to transform the graph total variation denoising problem—a key tool in graph signal processing—into its convex dual, which can then be solved efficiently using an accelerated projected gradient method [@problem_id:3478308]. This exercise bridges the gap between optimization theory and practical implementation, a crucial skill for any researcher in the field.", "problem": "You are given a finite, simple, undirected graph specified by a node set $\\mathcal{V}$ of size $n$ and an edge set $\\mathcal{E}$ of size $m$, together with an arbitrary fixed orientation of each edge. Let $B \\in \\mathbb{R}^{m \\times n}$ denote the oriented node-edge incidence matrix, where for edge $e = (i,j)$ oriented from $i$ to $j$, the $e$-th row of $B$ has $-1$ at column $i$, $+1$ at column $j$, and $0$ elsewhere. Let $W \\in \\mathbb{R}^{m \\times m}$ be a diagonal matrix of nonnegative edge weights with diagonal entries $w_e  0$. For a signal $x \\in \\mathbb{R}^n$ on the nodes, the anisotropic graph total variation (graph-TV) is defined by $\\|W B x\\|_1 = \\sum_{e \\in \\mathcal{E}} w_e |(B x)_e|$.\n\nFor a given input vector $y \\in \\mathbb{R}^n$ and a regularization parameter $\\lambda \\ge 0$, consider the proximal map of the graph-TV functional:\n$$\n\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{\\mathrm{graph-TV}}}(y) \\triangleq \\underset{x \\in \\mathbb{R}^n}{\\arg\\min}\\ \\frac{1}{2}\\|x - y\\|_2^2 + \\lambda \\|W B x\\|_1.\n$$\nYou are asked to:\n\n- Derive a dual formulation whose maximization variable $q \\in \\mathbb{R}^m$ is constrained to lie in the $\\ell_{\\infty}$ ball of radius $1$, that is, $\\|q\\|_{\\infty} \\le 1$, starting from standard convex duality and the definition of the $\\ell_1$ norm as a support function. Then explain how a projected gradient method on the dual can be implemented with a simple coordinate-wise projection onto the $\\ell_{\\infty}$ ball, and how this leads to an efficient evaluation of the proximal map via $x^{\\star} = y - \\lambda B^{\\top} W q^{\\star}$.\n\n- Implement the projection of an arbitrary vector of edge differences $v \\in \\mathbb{R}^m$ onto an $\\ell_{\\infty}$ ball of radius $1$, i.e., implement the projection map $P_{\\infty}(v) = \\underset{\\|q\\|_{\\infty} \\le 1}{\\arg\\min}\\ \\|q - v\\|_2^2$, and use it inside an accelerated projected gradient method on the dual problem to compute $x^{\\star} = \\operatorname{prox}_{\\lambda \\|\\cdot\\|_{\\mathrm{graph-TV}}}(y)$.\n\n- Base your derivation on the following fundamental facts: the Fenchel conjugate of the $\\ell_1$ norm is the indicator of the unit $\\ell_{\\infty}$ ball; the Fenchel–Rockafellar duality for convex functions composed with linear operators; and the fact that the projection onto an $\\ell_{\\infty}$ ball is done by coordinate-wise clipping.\n\nYour program must implement the described algorithm and evaluate it on the following test suite. For each test case, compute the primal solution $x^{\\star}$ via the dual projected gradient method with an automatically selected constant step size based on a power iteration estimate of the Lipschitz constant of the dual gradient. Then report, for each test case, two floats:\n- The duality gap $g = f(x^{\\star}) - d(q)$, where $f(x) = \\frac{1}{2}\\|x - y\\|_2^2 + \\lambda \\|W B x\\|_1$ and $d(q)$ is the dual objective value at the final dual iterate $q$, which must satisfy $g \\ge 0$.\n- A Karush–Kuhn–Tucker (KKT) violation metric defined as the maximum of two quantities: the $\\ell_{\\infty}$-ball feasibility violation $\\max(0, \\|q\\|_{\\infty} - 1)$ and the subgradient consistency violation on the support of $W B x^{\\star}$, namely $\\max_{e:\\ |(W B x^{\\star})_e|  \\varepsilon} |q_e - \\operatorname{sign}((W B x^{\\star})_e)|$ for a fixed threshold $\\varepsilon = 10^{-8}$.\n\nTest suite (all edges are undirected but assume the given orientation from the first node to the second in each pair, all weights are on the edges in the same order as listed):\n- Case $1$ (happy path): $n = 5$, $\\mathcal{E} = \\{(0,1),(1,2),(2,3),(3,4)\\}$, $w = [1,1,1,1]$, $y = [1.0, -0.5, 0.3, 2.0, -1.2]$, $\\lambda = 0.7$.\n- Case $2$ (boundary $\\lambda = 0$): $n = 5$, $\\mathcal{E} = \\{(0,1),(1,2),(2,3),(3,4)\\}$, $w = [1,1,1,1]$, $y = [0.2, -0.1, 0.0, 0.3, -0.4]$, $\\lambda = 0.0$.\n- Case $3$ (disconnected components with heterogeneous weights): $n = 6$, $\\mathcal{E} = \\{(0,1),(2,3),(3,4)\\}$, $w = [1.0, 0.5, 2.0]$, $y = [-1.0, 0.0, 2.5, -2.0, 0.7, 1.3]$, $\\lambda = 0.6$.\n- Case $4$ (star graph, strong regularization): $n = 6$, $\\mathcal{E} = \\{(0,1),(0,2),(0,3),(0,4),(0,5)\\}$, $w = [1,1,1,1,1]$, $y = [3.0, -1.0, 0.5, -0.5, 2.0, -2.0]$, $\\lambda = 5.0$.\n\nImplementation details:\n- Use the dual formulation with variable $q \\in \\mathbb{R}^m$ constrained by $\\|q\\|_{\\infty} \\le 1$.\n- Let $A = W B B^{\\top} W \\in \\mathbb{R}^{m \\times m}$ and $b = W B y \\in \\mathbb{R}^m$. The smooth dual objective for minimization is $h(q) = \\frac{1}{2}\\lambda^2 q^{\\top} A q - \\lambda b^{\\top} q$. Its gradient is $\\nabla h(q) = \\lambda^2 A q - \\lambda b$. Use an accelerated projected gradient method with constant step size $1/L$, where $L$ is an estimate of the Lipschitz constant of $\\nabla h$, equal to $\\lambda^2 \\|A\\|_2$. Estimate $\\|A\\|_2$ using $50$ steps of power iteration with a relative tolerance of $10^{-10}$.\n- The projection onto the $\\ell_{\\infty}$ ball of radius $1$ is the coordinate-wise clipping $P_{\\infty}(v)_e = \\min\\{\\max\\{v_e,-1\\},1\\}$ for each coordinate $e \\in \\{1,\\dots,m\\}$.\n- After computing $q$, recover $x^{\\star} = y - \\lambda B^{\\top} W q$.\n\nYour program should output, for the four test cases in the order specified, a single line containing a flat list of $8$ floating point numbers in the following order: $[g_1, k_1, g_2, k_2, g_3, k_3, g_4, k_4]$, where $g_i$ is the duality gap and $k_i$ is the KKT violation for case $i$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[0.001,0.0,0.0,0.0,0.002,0.0001,0.0,0.0]$). All numerical values are unitless real numbers.", "solution": "The solution involves deriving the convex dual of the primal problem, which can then be solved efficiently with a projected gradient method.\n\nThe primal problem is to find the proximal map of the graph-TV functional:\n$$\nx^{\\star} = \\underset{x \\in \\mathbb{R}^n}{\\arg\\min}\\ \\frac{1}{2}\\|x - y\\|_2^2 + \\lambda \\|W B x\\|_1\n$$\nWe use the fact that the $\\ell_1$-norm is the support function of the unit $\\ell_{\\infty}$-ball: $\\|z\\|_1 = \\sup_{\\|q\\|_{\\infty} \\le 1} q^{\\top} z$. This allows us to rewrite the problem as a minimax objective:\n$$\n\\min_{x \\in \\mathbb{R}^n} \\sup_{\\|q\\|_{\\infty} \\le 1} \\left( \\frac{1}{2}\\|x - y\\|_2^2 + \\lambda q^{\\top} W B x \\right)\n$$\nBy strong duality, we can exchange the `min` and `max` operators:\n$$\n\\max_{\\|q\\|_{\\infty} \\le 1} \\min_{x \\in \\mathbb{R}^n} \\mathcal{L}(x, q) \\quad \\text{where} \\quad \\mathcal{L}(x, q) = \\frac{1}{2}\\|x - y\\|_2^2 + \\lambda q^{\\top} W B x\n$$\nTo find the dual objective, we first solve the inner minimization problem with respect to $x$ for a fixed $q$. We find the minimum by setting the gradient of the Lagrangian $\\mathcal{L}(x, q)$ with respect to $x$ to zero:\n$$\n\\nabla_x \\mathcal{L}(x, q) = (x - y) + \\lambda B^{\\top} W q = 0\n$$\nThis gives the primal-dual relationship $x^{\\star}(q) = y - \\lambda B^{\\top} W q$. Substituting this back into $\\mathcal{L}(x, q)$ yields the dual objective function $d(q)$:\n$$\nd(q) = \\frac{1}{2}\\|(y - \\lambda B^{\\top} W q) - y\\|_2^2 + \\lambda q^{\\top} W B (y - \\lambda B^{\\top} W q)\n$$\n$$\nd(q) = \\lambda (W B y)^{\\top} q - \\frac{\\lambda^2}{2} q^{\\top} (W B B^{\\top} W) q\n$$\nThe dual problem is to maximize this concave quadratic function over the hypercube $\\|q\\|_{\\infty} \\le 1$. For implementation, it is conventional to solve an equivalent minimization problem for the convex function $h(q) = -d(q)$:\n$$\nq^{\\star} = \\underset{\\|q\\|_{\\infty} \\le 1}{\\arg\\min}\\ h(q) = \\frac{\\lambda^2}{2} q^{\\top} (W B B^{\\top} W) q - \\lambda (W B y)^{\\top} q\n$$\nThis dual problem is ideal for a projected gradient method. It involves minimizing a smooth convex function over a simple convex set. The algorithm iteratively applies a gradient step followed by a projection onto the constraint set $\\mathcal{C} = \\{q : \\|q\\|_{\\infty} \\le 1\\}$. The projection is a simple coordinate-wise clipping function: $(P_{\\mathcal{C}}(v))_e = \\min(\\max(v_e, -1), 1)$. After running an accelerated projected gradient algorithm (FISTA) to find an approximate solution $q^{\\star}$, the optimal primal solution is recovered using the relationship $x^{\\star} = y - \\lambda B^{\\top} W q^{\\star}$.", "answer": "```python\nimport numpy as np\n\ndef build_incidence_matrix(n, edges):\n    \"\"\"Constructs the oriented node-edge incidence matrix B.\"\"\"\n    m = len(edges)\n    B = np.zeros((m, n))\n    for i, edge in enumerate(edges):\n        u, v = edge\n        B[i, u] = -1\n        B[i, v] = 1\n    return B\n\ndef power_iteration(A, num_iter=50, tol=1e-10):\n    \"\"\"Estimates the spectral norm of a symmetric matrix A.\"\"\"\n    if A.shape[0] == 0:\n        return 0.0\n    v = np.random.rand(A.shape[1])\n    v = v / np.linalg.norm(v)\n    \n    eigenval_old = 0.0\n    for _ in range(num_iter):\n        Av = A @ v\n        v = Av / np.linalg.norm(Av)\n        eigenval = np.dot(v, A @ v)\n        if abs(eigenval - eigenval_old)  tol * (eigenval + 1e-12):\n            break\n        eigenval_old = eigenval\n        \n    return eigenval\n\ndef solve_single_case(n, edges, w, y, lambda_, num_fista_iter=5000, kkt_eps=1e-8):\n    \"\"\"Solves the graph-TV prox problem for a single case.\"\"\"\n    m = len(edges)\n    B = build_incidence_matrix(n, edges)\n    W = np.diag(w)\n    \n    # Handle the special case lambda = 0\n    if lambda_ == 0.0:\n        x_star = y\n        # Choose a dual-optimal q that satisfies KKT\n        WBy = W @ B @ y\n        q = np.clip(WBy, -1.0, 1.0) # A valid subgradient\n        \n        # Primal objective value f(x*) = 0\n        # Dual objective value d(q) = 0\n        gap = 0.0\n        \n        # KKT violation\n        feasibility_viol = max(0.0, np.max(np.abs(q)) - 1.0)\n        \n        active_set = np.abs(WBy)  kkt_eps\n        if np.any(active_set):\n            consistency_viol = np.max(np.abs(q[active_set] - np.sign(WBy[active_set])))\n        else:\n            consistency_viol = 0.0\n        \n        kkt_violation = max(feasibility_viol, consistency_viol)\n        \n        return gap, kkt_violation\n\n    # FISTA for lambda  0\n    A = W @ B @ B.T @ W\n    b = W @ B @ y\n    lambda_b = lambda_ * b\n\n    # Lipschitz constant of the dual gradient\n    L_A = power_iteration(A)\n    L = lambda_**2 * L_A\n    alpha = 1.0 / L if L  0 else 1.0\n\n    # FISTA initialization\n    q = np.zeros(m)\n    z = np.zeros(m)\n    t = 1.0\n    \n    # FISTA iterations\n    for _ in range(num_fista_iter):\n        q_prev = q\n        grad_h_z = lambda_**2 * (A @ z) - lambda_b\n        q_next = z - alpha * grad_h_z\n        q = np.clip(q_next, -1.0, 1.0)  # Projection P_inf\n\n        t_next = (1.0 + np.sqrt(1.0 + 4.0 * t**2)) / 2.0\n        z = q + ((t - 1.0) / t_next) * (q - q_prev)\n        t = t_next\n    \n    # Recover primal solution\n    x_star = y - lambda_ * B.T @ W @ q\n    \n    # Calculate metrics\n    # Primal objective f(x*)\n    WBx_star = W @ B @ x_star\n    f_x_star = 0.5 * np.linalg.norm(x_star - y)**2 + lambda_ * np.linalg.norm(WBx_star, 1)\n\n    # Dual maximization objective d(q)\n    d_q = lambda_b.T @ q - 0.5 * lambda_**2 * q.T @ A @ q\n    \n    # Duality gap g = f(x*) - d(q)\n    gap = f_x_star - d_q\n    \n    # KKT violation\n    # 1. Feasibility violation\n    feasibility_viol = max(0.0, np.max(np.abs(q)) - 1.0)\n    \n    # 2. Subgradient consistency violation\n    active_set = np.abs(WBx_star)  kkt_eps\n    if np.any(active_set):\n        consistency_viol = np.max(np.abs(q[active_set] - np.sign(WBx_star[active_set])))\n    else:\n        consistency_viol = 0.0\n        \n    kkt_violation = max(feasibility_viol, consistency_viol)\n    \n    return gap, kkt_violation\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    test_cases = [\n        # Case 1 (happy path)\n        (5, [(0, 1), (1, 2), (2, 3), (3, 4)], np.array([1.0, 1.0, 1.0, 1.0]), np.array([1.0, -0.5, 0.3, 2.0, -1.2]), 0.7),\n        # Case 2 (boundary lambda = 0)\n        (5, [(0, 1), (1, 2), (2, 3), (3, 4)], np.array([1.0, 1.0, 1.0, 1.0]), np.array([0.2, -0.1, 0.0, 0.3, -0.4]), 0.0),\n        # Case 3 (disconnected components)\n        (6, [(0, 1), (2, 3), (3, 4)], np.array([1.0, 0.5, 2.0]), np.array([-1.0, 0.0, 2.5, -2.0, 0.7, 1.3]), 0.6),\n        # Case 4 (star graph, strong regularization)\n        (6, [(0, 1), (0, 2), (0, 3), (0, 4), (0, 5)], np.array([1.0, 1.0, 1.0, 1.0, 1.0]), np.array([3.0, -1.0, 0.5, -0.5, 2.0, -2.0]), 5.0)\n    ]\n\n    results = []\n    for case in test_cases:\n        n, edges, w, y, lambda_ = case\n        gap, kkt_viol = solve_single_case(n, edges, w, y, lambda_)\n        results.extend([gap, kkt_viol])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "3478308"}]}