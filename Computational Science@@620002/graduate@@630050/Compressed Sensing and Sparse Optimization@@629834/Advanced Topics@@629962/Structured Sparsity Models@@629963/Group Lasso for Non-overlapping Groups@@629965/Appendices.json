{"hands_on_practices": [{"introduction": "This first practice is a foundational exercise that goes to the heart of many algorithms used to solve the group LASSO. We will derive the proximal operator, also known as the block soft-thresholding operator, directly from first principles of convex analysis. Mastering this building block is essential, as it forms the core update step in methods like proximal gradient descent and coordinate descent for group-sparse problems.", "problem": "Consider the proximal mapping associated with the group Least Absolute Shrinkage and Selection Operator (LASSO), where the penalty is the weighted sum of Euclidean norms over non-overlapping groups. Let the groups be non-overlapping and defined by $G_1=\\{1,2\\}$ and $G_2=\\{3\\}$. Let the data vector be $v \\in \\mathbb{R}^3$ with blocks $v_{G_1}=(3,4)$ and $v_{G_2}=(3)$, and let the positive weights be $w_1=2$ and $w_2=1$. Consider the convex optimization problem to compute the proximal point at step size $\\lambda=1$:\n$$\n\\min_{x \\in \\mathbb{R}^3}\\ \\frac{1}{2}\\|x-v\\|_2^2 + \\lambda \\sum_{g=1}^2 w_g \\|x_{G_g}\\|_2.\n$$\nStarting from foundational facts in convex analysis and optimization, namely separability of sums over disjoint groups, first-order optimality via subdifferentials, and the subdifferential of the Euclidean norm, derive the form of the unique minimizer block by block without invoking any pre-stated shrinkage formula. Then compute the explicit block outputs for $x_{G_1}$ and $x_{G_2}$, and identify the group support, defined as the set of group indices $g$ such that $\\|x_{G_g}\\|_2 \\neq 0$. Provide as your final answer the cardinality of this support set. No rounding is required.", "solution": "The user wants me to solve a proximal mapping problem for the group LASSO.\n\n### Step 1: Extract Givens\nThe problem provides the following information:\n-   The objective function to minimize: $\\frac{1}{2}\\|x-v\\|_2^2 + \\lambda \\sum_{g=1}^2 w_g \\|x_{G_g}\\|_2$\n-   The vector $x \\in \\mathbb{R}^3$.\n-   The data vector $v \\in \\mathbb{R}^3$.\n-   Non-overlapping groups: $G_1=\\{1,2\\}$ and $G_2=\\{3\\}$.\n-   Blocks of the data vector: $v_{G_1}=(3,4)$ and $v_{G_2}=(3)$.\n-   Positive weights: $w_1=2$ and $w_2=1$.\n-   Step size: $\\lambda=1$.\n-   The task is to find the minimizer $x$, identify the group support (indices $g$ for which $\\|x_{G_g}\\|_2 \\neq 0$), and provide the cardinality of this support.\n-   The derivation must start from foundational facts: separability, first-order optimality via subdifferentials, and the subdifferential of the Euclidean norm.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard convex optimization problem, specifically the computation of a proximal operator for the group LASSO penalty.\n-   **Scientifically Grounded:** The group LASSO and its proximal mapping are fundamental concepts in high-dimensional statistics, machine learning, and signal processing. The problem is based on well-established mathematical principles of convex analysis. It is scientifically sound.\n-   **Well-Posed:** The objective function is the sum of a strictly convex function ($\\frac{1}{2}\\|x-v\\|_2^2$) and a convex function ($\\lambda \\sum w_g \\|x_{G_g}\\|_2$). This ensures that a unique minimizer exists. The problem is well-posed.\n-   **Objective:** The problem is stated in precise mathematical language, free from ambiguity or subjective claims.\n-   **Completeness and Consistency:** All necessary parameters ($v$, groups, $w$, $\\lambda$) are provided. The dimensions are consistent: $v \\in \\mathbb{R}^3$ and the groups $G_1, G_2$ partition the index set $\\{1, 2, 3\\}$.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nThe optimization problem is:\n$$ \\min_{x \\in \\mathbb{R}^3} F(x) = \\min_{x \\in \\mathbb{R}^3} \\left( \\frac{1}{2}\\|x-v\\|_2^2 + \\lambda \\sum_{g=1}^2 w_g \\|x_{G_g}\\|_2 \\right) $$\nThe groups $G_1 = \\{1,2\\}$ and $G_2 = \\{3\\}$ are non-overlapping and their union is $\\{1,2,3\\}$. This means the problem is separable. The squared Euclidean norm term can be decomposed across the groups:\n$$ \\|x-v\\|_2^2 = \\sum_{i=1}^3 (x_i - v_i)^2 = \\sum_{i \\in G_1} (x_i - v_i)^2 + \\sum_{i \\in G_2} (x_i - v_i)^2 = \\|x_{G_1}-v_{G_1}\\|_2^2 + \\|x_{G_2}-v_{G_2}\\|_2^2 $$\nThe objective function can thus be written as a sum of functions of disjoint sets of variables:\n$$ F(x) = \\left( \\frac{1}{2}\\|x_{G_1}-v_{G_1}\\|_2^2 + \\lambda w_1 \\|x_{G_1}\\|_2 \\right) + \\left( \\frac{1}{2}\\|x_{G_2}-v_{G_2}\\|_2^2 + \\lambda w_2 \\|x_{G_2}\\|_2 \\right) $$\n$$ F(x) = F_1(x_{G_1}) + F_2(x_{G_2}) $$\nTo minimize $F(x)$, we can minimize $F_1(x_{G_1})$ and $F_2(x_{G_2})$ independently. Let's derive the solution for a generic group subproblem:\n$$ \\min_{x_g} F_g(x_g) = \\min_{x_g} \\left( \\frac{1}{2}\\|x_g-v_g\\|_2^2 + \\alpha_g \\|x_g\\|_2 \\right) $$\nwhere $x_g \\equiv x_{G_g}$, $v_g \\equiv v_{G_g}$, and $\\alpha_g = \\lambda w_g$.\n\nAccording to Fermat's rule for convex functions, a point $x_g^*$ is a minimizer if and only if the zero vector is in the subdifferential of the objective function at $x_g^*$:\n$$ 0 \\in \\partial F_g(x_g^*) $$\nThe subdifferential of $F_g$ is given by the sum of the subdifferentials of its terms:\n$$ \\partial F_g(x_g) = (x_g - v_g) + \\alpha_g \\partial \\|x_g\\|_2 $$\nThe subdifferential of the Euclidean norm $\\|z\\|_2$ is:\n$$ \\partial \\|z\\|_2 = \\begin{cases} \\{z / \\|z\\|_2\\} & \\text{if } z \\neq 0 \\\\ \\{u : \\|u\\|_2 \\le 1\\} & \\text{if } z = 0 \\end{cases} $$\nThe optimality condition is $0 \\in (x_g^* - v_g) + \\alpha_g \\partial \\|x_g^*\\|_2$, which can be rewritten as:\n$$ v_g - x_g^* \\in \\alpha_g \\partial \\|x_g^*\\|_2 $$\nWe analyze two cases for the solution $x_g^*$.\n\nCase 1: $x_g^* \\neq 0$.\nIn this case, $\\partial \\|x_g^*\\|_2 = \\{x_g^* / \\|x_g^*\\|_2\\}$. The optimality condition becomes:\n$$ v_g - x_g^* = \\alpha_g \\frac{x_g^*}{\\|x_g^*\\|_2} $$\nRearranging for $v_g$:\n$$ v_g = x_g^* + \\alpha_g \\frac{x_g^*}{\\|x_g^*\\|_2} = x_g^* \\left(1 + \\frac{\\alpha_g}{\\|x_g^*\\|_2}\\right) $$\nThis shows that $v_g$ and $x_g^*$ must be collinear and point in the same direction. Therefore, $\\frac{x_g^*}{\\|x_g^*\\|_2} = \\frac{v_g}{\\|v_g\\|_2}$.\nTaking the Euclidean norm of both sides of the equation for $v_g$:\n$$ \\|v_g\\|_2 = \\left\\| x_g^* \\left(1 + \\frac{\\alpha_g}{\\|x_g^*\\|_2}\\right) \\right\\|_2 = \\|x_g^*\\|_2 \\left(1 + \\frac{\\alpha_g}{\\|x_g^*\\|_2}\\right) = \\|x_g^*\\|_2 + \\alpha_g $$\nFrom this, we find the norm of the solution: $\\|x_g^*\\|_2 = \\|v_g\\|_2 - \\alpha_g$.\nSince we assumed $x_g^* \\neq 0$, we must have $\\|x_g^*\\|_2 > 0$, which implies $\\|v_g\\|_2 - \\alpha_g > 0$, or $\\|v_g\\|_2 > \\alpha_g$.\nIf this condition holds, the solution is:\n$$ x_g^* = \\|x_g^*\\|_2 \\frac{v_g}{\\|v_g\\|_2} = (\\|v_g\\|_2 - \\alpha_g) \\frac{v_g}{\\|v_g\\|_2} = \\left(1 - \\frac{\\alpha_g}{\\|v_g\\|_2}\\right) v_g $$\n\nCase 2: $x_g^* = 0$.\nIn this case, $\\partial \\|x_g^*\\|_2$ is the closed unit ball $\\{u : \\|u\\|_2 \\le 1\\}$. The optimality condition becomes:\n$$ v_g - 0 \\in \\alpha_g \\{u : \\|u\\|_2 \\le 1\\} $$\nThis is equivalent to $\\|v_g\\|_2 \\le \\alpha_g$.\n\nCombining both cases, the unique minimizer $x_g^*$ is:\n$$ x_g^* = \\begin{cases} \\left(1 - \\frac{\\alpha_g}{\\|v_g\\|_2}\\right) v_g & \\text{if } \\|v_g\\|_2 > \\alpha_g \\\\ 0 & \\text{if } \\|v_g\\|_2 \\le \\alpha_g \\end{cases} $$\nThis is the block soft-thresholding operator.\n\n### Computation for the Specific Problem\nNow we apply this derived formula to each group using the given values: $\\lambda=1$, $v_{G_1}=(3,4)$, $v_{G_2}=(3)$, $w_1=2$, $w_2=1$.\n\n**For Group 1:**\n-   $g=1$, $x_{G_1} \\in \\mathbb{R}^2$.\n-   $v_{G_1} = (3,4)$.\n-   $\\alpha_1 = \\lambda w_1 = 1 \\times 2 = 2$.\n-   The norm of $v_{G_1}$ is $\\|v_{G_1}\\|_2 = \\sqrt{3^2 + 4^2} = \\sqrt{9+16} = \\sqrt{25} = 5$.\n-   We compare $\\|v_{G_1}\\|_2$ with $\\alpha_1$: $5 > 2$.\n-   Since $\\|v_{G_1}\\|_2 > \\alpha_1$, we are in the first case. The solution $x_{G_1}$ is non-zero.\n-   $x_{G_1} = \\left(1 - \\frac{\\alpha_1}{\\|v_{G_1}\\|_2}\\right) v_{G_1} = \\left(1 - \\frac{2}{5}\\right) (3,4) = \\frac{3}{5} (3,4) = \\left(\\frac{9}{5}, \\frac{12}{5}\\right)$.\n-   The norm of the solution block is $\\|x_{G_1}\\|_2 = \\sqrt{(\\frac{9}{5})^2 + (\\frac{12}{5})^2} = \\frac{1}{5}\\sqrt{81+144} = \\frac{\\sqrt{225}}{5} = \\frac{15}{5} = 3$.\n-   Since $\\|x_{G_1}\\|_2 = 3 \\neq 0$, group $1$ is in the support.\n\n**For Group 2:**\n-   $g=2$, $x_{G_2} \\in \\mathbb{R}^1$.\n-   $v_{G_2} = (3)$.\n-   $\\alpha_2 = \\lambda w_2 = 1 \\times 1 = 1$.\n-   The norm of $v_{G_2}$ is $\\|v_{G_2}\\|_2 = |3| = 3$.\n-   We compare $\\|v_{G_2}\\|_2$ with $\\alpha_2$: $3 > 1$.\n-   Since $\\|v_{G_2}\\|_2 > \\alpha_2$, we are again in the first case. The solution $x_{G_2}$ is non-zero.\n-   $x_{G_2} = \\left(1 - \\frac{\\alpha_2}{\\|v_{G_2}\\|_2}\\right) v_{G_2} = \\left(1 - \\frac{1}{3}\\right) (3) = \\frac{2}{3} \\times 3 = 2$.\n-   The norm of the solution block is $\\|x_{G_2}\\|_2 = |2| = 2$.\n-   Since $\\|x_{G_2}\\|_2 = 2 \\neq 0$, group $2$ is also in the support.\n\n### Group Support\nThe group support is defined as the set of group indices $g$ such that $\\|x_{G_g}\\|_2 \\neq 0$.\n-   For $g=1$, we found $\\|x_{G_1}\\|_2 = 3 \\neq 0$.\n-   For $g=2$, we found $\\|x_{G_2}\\|_2 = 2 \\neq 0$.\nBoth groups are active. The group support set is $\\{1, 2\\}$.\nThe cardinality of this support set is the number of elements in it, which is $2$.", "answer": "$$\\boxed{2}$$", "id": "3449689"}, {"introduction": "Building on the proximal operator, this exercise demonstrates how to solve a full group LASSO problem in a special but insightful case where the design matrix has orthonormal blocks. This structure decouples the problem, allowing it to be solved group by group using the block soft-thresholding logic we previously derived [@problem_id:3449689]. This practice illuminates the concept of the regularization path, revealing how the set of active groups changes as the penalty parameter $\\lambda$ is varied.", "problem": "Consider the Group Least Absolute Shrinkage and Selection Operator (Group LASSO) for non-overlapping groups, defined as the solution to\n$$\n\\min_{x \\in \\mathbb{R}^{p}} \\; \\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\sum_{g=1}^{G} w_{g} \\|x_{G_{g}}\\|_{2},\n$$\nwhere $A \\in \\mathbb{R}^{m \\times p}$ is a design matrix whose columns are partitioned into $G$ disjoint index sets $G_{1},\\dots,G_{G}$, $x_{G_{g}}$ denotes the subvector of $x$ restricted to indices in group $G_{g}$, $w_{g} > 0$ are group weights, and $\\lambda \\geq 0$ is a regularization parameter. A group $G_{g}$ is said to be active if $\\|x_{G_{g}}\\|_{2} > 0$ in the minimizer, and inactive otherwise.\n\nWork with the following concrete instance:\n- Two groups $G_{1} = \\{1,2\\}$ and $G_{2} = \\{3,4\\}$, so $G = 2$ and $p = 4$.\n- The matrix $A \\in \\mathbb{R}^{4 \\times 4}$ has orthonormal columns within each group and the groups are mutually orthogonal subspaces; specifically, let\n$$\nA = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{pmatrix},\n$$\nso the columns are the standard basis vectors in $\\mathbb{R}^{4}$ and $A^{\\top}A = I_{4}$.\n- The weights are $w_{1} = \\frac{3}{2}$ and $w_{2} = 1$.\n- The observation vector is $y = \\begin{pmatrix} 3 \\\\ 4 \\\\ 2 \\\\ 2 \\end{pmatrix}$.\n\nStarting only from core definitions of convex optimization, subdifferentials of norms, and orthonormality, derive the exact characterization of the minimizer $x^{\\star}(\\lambda)$ and determine, as a function of $\\lambda$, which group(s) are active. Then compute the two critical values of $\\lambda$ at which the active set changes (i.e., the values of $\\lambda$ where a group switches between active and inactive). Report these two critical values in increasing order as your final answer. Express the final values exactly; do not round.", "solution": "The user wants to solve a Group LASSO optimization problem and find the critical values of the regularization parameter $\\lambda$ where the set of active groups changes.\n\nThe optimization problem is given by:\n$$\n\\min_{x \\in \\mathbb{R}^{p}} \\; L(x) = \\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\sum_{g=1}^{G} w_{g} \\|x_{G_{g}}\\|_{2}\n$$\nThe function $L(x)$ is convex as it is the sum of a convex quadratic function (the least-squares term) and a sum of convex functions (Euclidean norms, which are scaled by non-negative weights). Therefore, a vector $x^{\\star}$ is a global minimizer if and only if the zero vector is an element of the subdifferential of $L(x)$ evaluated at $x^{\\star}$. This is the first-order optimality condition:\n$$\n0 \\in \\partial L(x^{\\star})\n$$\nUsing the sum rule for subdifferentials, we have:\n$$\n\\partial L(x) = \\nabla \\left( \\frac{1}{2}\\|y - A x\\|_{2}^{2} \\right) + \\lambda \\partial \\left( \\sum_{g=1}^{G} w_{g} \\|x_{G_{g}}\\|_{2} \\right)\n$$\nThe gradient of the differentiable least-squares term is:\n$$\n\\nabla \\left( \\frac{1}{2}\\|y - A x\\|_{2}^{2} \\right) = A^{\\top}(Ax - y)\n$$\nThe penalty term $\\sum_{g=1}^{G} w_{g} \\|x_{G_{g}}\\|_{2}$ is separable with respect to the disjoint groups of variables $x_{G_g}$. Its subdifferential is the Cartesian product of the subdifferentials of each term. This allows us to write the optimality condition for each block of variables $x_{G_g}$ separately:\n$$\n0_{|G_g|} \\in [A^{\\top}(Ax - y)]_{G_g} + \\lambda w_g \\partial \\|x_{G_g}\\|_{2}\n$$\nwhere $0_{|G_g|}$ is the zero vector of appropriate dimension and $[v]_{G_g}$ denotes the subvector of $v$ corresponding to the indices in group $G_g$.\n\nThe problem states that the columns of $A$ are orthonormal within each group and the groups are mutually orthogonal. This means that if $A_{G_g}$ is the submatrix of $A$ with columns indexed by $G_g$, then $A_{G_g}^{\\top}A_{G_h} = \\delta_{gh}I_{|G_g|}$, where $\\delta_{gh}$ is the Kronecker delta and $I$ is the identity matrix. This property greatly simplifies the gradient term:\n$$\n[A^{\\top}(Ax - y)]_{G_g} = [A^{\\top}Ax]_{G_g} - [A^{\\top}y]_{G_g}\n$$\nThe term $[A^{\\top}Ax]_{G_g}$ becomes:\n$$\n[A^{\\top}Ax]_{G_g} = \\sum_{h=1}^{G} (A_{G_g}^{\\top}A_{G_h}) x_{G_h} = (A_{G_g}^{\\top}A_{G_g}) x_{G_g} = I_{|G_g|} x_{G_g} = x_{G_g}\n$$\nThe term $[A^{\\top}y]_{G_g}$ is $A_{G_g}^{\\top}y$.\nSubstituting these back into the block-wise optimality condition, we find that the problem decouples for each group:\n$$\n0 \\in x_{G_g} - A_{G_g}^{\\top}y + \\lambda w_g \\partial \\|x_{G_g}\\|_{2}\n$$\nLet's define $z_g \\equiv A_{G_g}^{\\top}y$. The condition becomes:\n$$\nz_g - x_{G_g} \\in \\lambda w_g \\partial \\|x_{G_g}\\|_{2}\n$$\nTo solve for $x_{G_g}$, we analyze the subdifferential of the Euclidean norm, $\\partial \\|u\\|_2$:\n- If $u \\neq 0$, $\\partial \\|u\\|_2 = \\{ \\frac{u}{\\|u\\|_2} \\}$.\n- If $u = 0$, $\\partial \\|u\\|_2 = \\{ v \\in \\mathbb{R}^{|G_g|} : \\|v\\|_2 \\leq 1 \\}$.\n\nWe consider two cases for the solution $x_{G_g}^\\star$.\n\nCase 1: The group is active, i.e., $x_{G_g}^\\star \\neq 0$.\nThe subdifferential is a singleton, and the optimality condition is an equation:\n$$\nz_g - x_{G_g}^\\star = \\lambda w_g \\frac{x_{G_g}^\\star}{\\|x_{G_g}^\\star\\|_2}\n$$\nRearranging for $z_g$:\n$$\nz_g = x_{G_g}^\\star \\left( 1 + \\frac{\\lambda w_g}{\\|x_{G_g}^\\star\\|_2} \\right)\n$$\nThis implies that $x_{G_g}^\\star$ is a positive scalar multiple of $z_g$. Taking the norm of both sides:\n$$\n\\|z_g\\|_2 = \\|x_{G_g}^\\star\\|_2 \\left( 1 + \\frac{\\lambda w_g}{\\|x_{G_g}^\\star\\|_2} \\right) = \\|x_{G_g}^\\star\\|_2 + \\lambda w_g\n$$\nThis gives the norm of the solution: $\\|x_{G_g}^\\star\\|_2 = \\|z_g\\|_2 - \\lambda w_g$. For the norm to be positive, we must have $\\|z_g\\|_2 > \\lambda w_g$. If this condition holds, we can find $x_{G_g}^\\star$:\n$$\nx_{G_g}^\\star = z_g \\left( \\frac{\\|x_{G_g}^\\star\\|_2}{\\|z_g\\|_2} \\right) = z_g \\left( \\frac{\\|z_g\\|_2 - \\lambda w_g}{\\|z_g\\|_2} \\right) = \\left( 1 - \\frac{\\lambda w_g}{\\|z_g\\|_2} \\right) z_g\n$$\n\nCase 2: The group is inactive, i.e., $x_{G_g}^\\star = 0$.\nThe optimality condition becomes an inclusion:\n$$\nz_g - 0 \\in \\lambda w_g \\{ v : \\|v\\|_2 \\leq 1 \\}\n$$\nThis is equivalent to $\\|z_g\\|_2 \\leq \\lambda w_g$. When this condition holds, $x_{G_g}^\\star = 0$ is the solution.\n\nA group $G_g$ is active if $\\|z_g\\|_2 > \\lambda w_g$ and inactive if $\\|z_g\\|_2 \\le \\lambda w_g$. The active set changes when $\\lambda$ crosses a critical value $\\lambda_g^c$ where a group transitions between active and inactive. This occurs precisely when $\\|z_g\\|_2 = \\lambda_g^c w_g$. The critical value for group $g$ is therefore:\n$$\n\\lambda_g^c = \\frac{\\|z_g\\|_2}{w_g}\n$$\n\nNow we apply this general result to the specific instance provided:\n- Groups: $G_{1} = \\{1,2\\}$ and $G_{2} = \\{3,4\\}$.\n- Matrix: $A = I_{4}$, the $4 \\times 4$ identity matrix. This matrix satisfies the orthogonality conditions.\n- Weights: $w_{1} = \\frac{3}{2}$ and $w_{2} = 1$.\n- Observation vector: $y = \\begin{pmatrix} 3 \\\\ 4 \\\\ 2 \\\\ 2 \\end{pmatrix}$.\n\nFor Group $G_1 = \\{1,2\\}$:\nThe submatrix $A_{G_1}$ consists of the first two columns of $I_4$. We compute $z_1 = A_{G_1}^{\\top}y$:\n$$\nz_1 = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 4 \\\\ 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}\n$$\nThe norm is $\\|z_1\\|_2 = \\sqrt{3^2 + 4^2} = \\sqrt{9+16} = \\sqrt{25} = 5$.\nThe critical value for Group 1 is:\n$$\n\\lambda_1^c = \\frac{\\|z_1\\|_2}{w_1} = \\frac{5}{3/2} = \\frac{10}{3}\n$$\n\nFor Group $G_2 = \\{3,4\\}$:\nThe submatrix $A_{G_2}$ consists of the last two columns of $I_4$. We compute $z_2 = A_{G_2}^{\\top}y$:\n$$\nz_2 = \\begin{pmatrix} 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 4 \\\\ 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n$$\nThe norm is $\\|z_2\\|_2 = \\sqrt{2^2 + 2^2} = \\sqrt{4+4} = \\sqrt{8} = 2\\sqrt{2}$.\nThe critical value for Group 2 is:\n$$\n\\lambda_2^c = \\frac{\\|z_2\\|_2}{w_2} = \\frac{2\\sqrt{2}}{1} = 2\\sqrt{2}\n$$\n\nThe two critical values of $\\lambda$ at which the active set changes are $\\lambda_1^c = \\frac{10}{3}$ and $\\lambda_2^c = 2\\sqrt{2}$. We must report them in increasing order.\n$\\frac{10}{3} \\approx 3.333$\n$2\\sqrt{2} \\approx 2 \\times 1.414 = 2.828$\nThus, $2\\sqrt{2} < \\frac{10}{3}$.\n\nThe active set as a function of $\\lambda \\ge 0$:\n- For $0 \\le \\lambda < 2\\sqrt{2}$: Both groups are active.\n- For $2\\sqrt{2} \\le \\lambda < \\frac{10}{3}$: Group 2 becomes inactive, Group 1 remains active.\n- For $\\lambda \\ge \\frac{10}{3}$: Both groups are inactive.\n\nThe two critical values where the active set composition changes are $2\\sqrt{2}$ and $\\frac{10}{3}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2\\sqrt{2} & \\frac{10}{3}\n\\end{pmatrix}\n}\n$$", "id": "3449709"}, {"introduction": "Instead of solving for the optimal solution, this practice challenges you to verify if a given solution is indeed optimal. By working backward from a proposed solution, you will use the Karush-Kuhn-Tucker (KKT) conditions to determine the precise value of the regularization parameter $\\lambda$ that makes the solution valid. This exercise provides a deeper intuition for the subgradient optimality conditions, clarifying the distinct roles they play for both active and inactive groups.", "problem": "Consider the group Least Absolute Shrinkage and Selection Operator (group LASSO) with non-overlapping groups, defined for a response vector $y \\in \\mathbb{R}^n$, a design matrix $X \\in \\mathbb{R}^{n \\times p}$ with columns $\\{x_j\\}_{j=1}^p$, a grouping $\\{G_1,\\dots,G_m\\}$ that partitions $\\{1,\\dots,p\\}$, positive weights $\\{w_g\\}_{g=1}^m$, and a regularization parameter $\\lambda > 0$, by the objective\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\ \\frac{1}{2} \\|y - X \\beta\\|_2^2 + \\lambda \\sum_{g=1}^m w_g \\|\\beta_g\\|_2,\n$$\nwhere $\\beta_g \\in \\mathbb{R}^{|G_g|}$ denotes the subvector of $\\beta$ indexed by $G_g$.\n\nUse fundamental first-order optimality for convex optimization and the subdifferential of the Euclidean norm (without invoking any pre-packaged formulas) to verify the Karush-Kuhn-Tucker (KKT) conditions for the following specific instance by checking block correlations and subgradient membership at the claimed solution, and determine the unique value of $\\lambda$ for which the claimed solution is optimal.\n\nThe instance is specified as follows:\n- Dimensions: $n = 5$, $p = 6$, with three non-overlapping groups $G_1 = \\{1,2\\}$, $G_2 = \\{3,4\\}$, and $G_3 = \\{5,6\\}$.\n- Weights: $w_1 = 1$, $w_2 = 2$, $w_3 = 1$.\n- Columns of $X$: \n$$\nx_1 = (1,0,0,0,0)^\\top,\\quad\nx_2 = (0,1,0,0,0)^\\top,\\quad\nx_3 = (0,0,1,0,0)^\\top,\\quad\nx_4 = (0,0,0,1,0)^\\top,\\quad\nx_5 = (0,0,0,0,1)^\\top,\\quad\nx_6 = \\frac{1}{\\sqrt{2}}(0,0,1,1,0)^\\top.\n$$\n- Response: $y = (3,3,2,0,0)^\\top$.\n- Claimed optimal solution: \n$$\n\\beta^\\star = (2,2,0,0,0,0)^\\top,\n$$\nwhich has the active block $G_1$ and inactive blocks $G_2$ and $G_3$.\n\nTasks:\n1. Starting only from the definition of the objective and basic properties of convex functions and subgradients, write down the necessary and sufficient first-order optimality condition for $\\beta^\\star$ to be optimal.\n2. Evaluate the block-wise correlation vectors $X_{G_g}^\\top r$ for $g \\in \\{1,2,3\\}$, where $r = y - X \\beta^\\star$ is the residual at the claimed solution, and use subgradient membership for the Euclidean norm to test the KKT conditions for each block.\n3. Determine the unique $\\lambda > 0$ consistent with the active block condition for $G_1$, and verify that the inactive blocks satisfy their respective conditions at this $\\lambda$.\n\nExpress the final value of $\\lambda$ as an exact closed-form expression. No rounding is required.", "solution": "The problem statement is valid as it is scientifically grounded, well-posed, objective, and provides a complete and consistent setup for a standard problem in convex optimization.\n\nThe group LASSO objective function is given by a sum of two convex functions:\n$$\nF(\\beta) = f(\\beta) + h(\\beta) = \\frac{1}{2} \\|y - X \\beta\\|_2^2 + \\lambda \\sum_{g=1}^m w_g \\|\\beta_g\\|_2\n$$\nwhere $f(\\beta) = \\frac{1}{2} \\|y - X \\beta\\|_2^2$ is differentiable and $h(\\beta) = \\lambda \\sum_{g=1}^m w_g \\|\\beta_g\\|_2$ is non-differentiable. Since $F(\\beta)$ is convex, a vector $\\beta^\\star \\in \\mathbb{R}^p$ is a global minimizer of $F(\\beta)$ if and only if the zero vector is an element of its subdifferential, i.e., $0 \\in \\partial F(\\beta^\\star)$.\n\nUsing the sum rule for subdifferentials, this condition becomes:\n$$\n0 \\in \\nabla f(\\beta^\\star) + \\partial h(\\beta^\\star)\n$$\nwhere $\\nabla f(\\beta^\\star)$ is the gradient of $f$ at $\\beta^\\star$ and $\\partial h(\\beta^\\star)$ is the subdifferential of $h$ at $\\beta^\\star$. The gradient of the least-squares term $f(\\beta)$ is $\\nabla f(\\beta) = -X^\\top(y - X\\beta)$. The optimality condition can thus be written as:\n$$\nX^\\top(y - X\\beta^\\star) \\in \\partial h(\\beta^\\star)\n$$\nThe penalty term $h(\\beta)$ is separable across the non-overlapping groups $G_g$, so its subdifferential is the Cartesian product of the subdifferentials of each term:\n$$\n\\partial h(\\beta^\\star) = \\lambda w_1 \\partial \\|\\beta_{G_1}^\\star\\|_2 \\times \\lambda w_2 \\partial \\|\\beta_{G_2}^\\star\\|_2 \\times \\dots \\times \\lambda w_m \\partial \\|\\beta_{G_m}^\\star\\|_2\n$$\nHere, $\\partial \\|\\cdot\\|_2$ denotes the subdifferential of the Euclidean norm. For a vector $u$,\n$$\n\\partial \\|u\\|_2 = \\begin{cases} \\{u / \\|u\\|_2\\} & \\text{if } u \\neq 0 \\\\ \\{v \\mid \\|v\\|_2 \\le 1\\} & \\text{if } u = 0 \\end{cases}\n$$\nLet $r = y - X\\beta^\\star$ be the residual vector at the solution $\\beta^\\star$. The optimality condition can be decomposed into block-wise Karush-Kuhn-Tucker (KKT) conditions. For each group $g \\in \\{1, \\dots, m\\}$:\n$$\nX_{G_g}^\\top r \\in \\lambda w_g \\partial \\|\\beta_{G_g}^\\star\\|_2\n$$\nThis leads to two distinct conditions:\n1.  For an **active block** where $\\beta_{G_g}^\\star \\neq 0$:\n    $$\n    X_{G_g}^\\top r = \\lambda w_g \\frac{\\beta_{G_g}^\\star}{\\|\\beta_{G_g}^\\star\\|_2}\n    $$\n2.  For an **inactive block** where $\\beta_{G_g}^\\star = 0$:\n    $$\n    \\|X_{G_g}^\\top r\\|_2 \\le \\lambda w_g\n    $$\n\nNow, we apply these conditions to the specific instance provided.\nThe design matrix $X$ is constructed from its columns:\n$$\nX = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 & \\frac{1}{\\sqrt{2}} \\\\ 0 & 0 & 0 & 1 & 0 & \\frac{1}{\\sqrt{2}} \\\\ 0 & 0 & 0 & 0 & 1 & 0 \\end{pmatrix}\n$$\nThe response is $y = (3, 3, 2, 0, 0)^\\top$ and the claimed solution is $\\beta^\\star = (2, 2, 0, 0, 0, 0)^\\top$.\nFirst, we compute the predicted response $X\\beta^\\star$:\n$$\nX\\beta^\\star = \\beta_1^\\star x_1 + \\beta_2^\\star x_2 = 2 \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + 2 \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe residual vector $r$ is:\n$$\nr = y - X\\beta^\\star = \\begin{pmatrix} 3 \\\\ 3 \\\\ 2 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nNext, we compute the block-wise correlation vectors $X_{G_g}^\\top r$.\n-   **Group $G_1 = \\{1, 2\\}$** (active, $\\beta_{G_1}^\\star = (2, 2)^\\top \\neq 0$):\n    $$\n    X_{G_1}^\\top r = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n    $$\n-   **Group $G_2 = \\{3, 4\\}$** (inactive, $\\beta_{G_2}^\\star = (0, 0)^\\top = 0$):\n    $$\n    X_{G_2}^\\top r = \\begin{pmatrix} 0 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}\n    $$\n-   **Group $G_3 = \\{5, 6\\}$** (inactive, $\\beta_{G_3}^\\star = (0, 0)^\\top = 0$):\n    $$\n    X_{G_3}^\\top r = \\begin{pmatrix} 0 & 0 & 0 & 0 & 1 \\\\ 0 & 0 & \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\frac{2}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\sqrt{2} \\end{pmatrix}\n    $$\n\nTo find the unique value of $\\lambda$, we use the equality condition for the active block $G_1$, with $w_1 = 1$.\nThe subvector is $\\beta_{G_1}^\\star = (2, 2)^\\top$. Its Euclidean norm is $\\|\\beta_{G_1}^\\star\\|_2 = \\sqrt{2^2 + 2^2} = \\sqrt{8} = 2\\sqrt{2}$.\nThe KKT condition for $G_1$ is:\n$$\n\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\lambda w_1 \\frac{\\beta_{G_1}^\\star}{\\|\\beta_{G_1}^\\star\\|_2} = \\lambda (1) \\frac{\\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}}{2\\sqrt{2}} = \\frac{\\lambda}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nComparing the components of the vectors, we get the equation $1 = \\frac{\\lambda}{\\sqrt{2}}$, which uniquely determines $\\lambda$:\n$$\n\\lambda = \\sqrt{2}\n$$\nFinally, we must verify that this value of $\\lambda$ satisfies the KKT conditions for the inactive blocks $G_2$ and $G_3$.\n-   **Verification for Group $G_2$**:\n    The condition is $\\|X_{G_2}^\\top r\\|_2 \\le \\lambda w_2$. With $w_2 = 2$:\n    $$\n    \\|X_{G_2}^\\top r\\|_2 = \\left\\| \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\sqrt{2^2 + 0^2} = 2\n    $$\n    We check if $2 \\le \\lambda w_2 = (\\sqrt{2})(2) = 2\\sqrt{2}$. This is true, as $1 \\le \\sqrt{2}$. The condition is satisfied.\n-   **Verification for Group $G_3$**:\n    The condition is $\\|X_{G_3}^\\top r\\|_2 \\le \\lambda w_3$. With $w_3 = 1$:\n    $$\n    \\|X_{G_3}^\\top r\\|_2 = \\left\\| \\begin{pmatrix} 0 \\\\ \\sqrt{2} \\end{pmatrix} \\right\\|_2 = \\sqrt{0^2 + (\\sqrt{2})^2} = \\sqrt{2}\n    $$\n    We check if $\\sqrt{2} \\le \\lambda w_3 = (\\sqrt{2})(1) = \\sqrt{2}$. This is also true. The condition is satisfied.\n\nAll KKT conditions are satisfied for $\\lambda = \\sqrt{2}$. Therefore, the unique value of $\\lambda$ for which the claimed solution $\\beta^\\star$ is optimal is $\\sqrt{2}$.", "answer": "$$\\boxed{\\sqrt{2}}$$", "id": "3449680"}]}