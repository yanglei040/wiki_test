{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we will implement the Simultaneous Orthogonal Matching Pursuit (SOMP) algorithm, a cornerstone greedy method for solving the Multiple Measurement Vector (MMV) problem. This exercise requires translating the theoretical steps of SOMP—iteratively selecting the dictionary atom that best explains the collective residual and updating the solution via orthogonal projection—into functional code. By working through concrete numerical examples, you will gain practical experience with a foundational recovery algorithm and develop intuition for its behavior in both noisy and noise-free scenarios [@problem_id:3460799].", "problem": "Consider the Multiple Measurement Vector (MMV) model in compressed sensing. The MMV model assumes that $L$ measurement vectors share a common sparse support across rows of the unknown coefficient matrix. Formally, let $A \\in \\mathbb{R}^{m \\times n}$ be a measurement matrix (also called a dictionary), $X \\in \\mathbb{R}^{n \\times L}$ be a row-sparse coefficient matrix with at most $k$ nonzero rows (joint sparsity), and $Y \\in \\mathbb{R}^{m \\times L}$ be the observed measurements satisfying\n$$\nY = A X + E,\n$$\nwhere $E \\in \\mathbb{R}^{m \\times L}$ is additive noise. The Simultaneous Orthogonal Matching Pursuit (SOMP) algorithm greedily selects columns of $A$ (called atoms) that best explain the residual across the $L$ measurement vectors, updating the residual by orthogonal projection onto the span of selected atoms and stopping after a prescribed number of iterations.\n\nStarting from fundamental principles:\n- The residual at iteration $t$ is defined as $R^{(t)} = Y - A_{S^{(t)}} X_{S^{(t)}}$, where $S^{(t)}$ is the set of selected indices at iteration $t$ and $A_{S^{(t)}}$ is the submatrix of $A$ containing columns indexed by $S^{(t)}$.\n- The least-squares update enforces $X_{S^{(t)}}$ to minimize the Frobenius norm of the residual, i.e., $X_{S^{(t)}} = \\operatorname*{arg\\,min}_{Z \\in \\mathbb{R}^{|S^{(t)}| \\times L}} \\| Y - A_{S^{(t)}} Z \\|_F$, with the orthogonal projector onto the span of $A_{S^{(t)}}$ expressed (when $A_{S^{(t)}}$ has full column rank) as $P_{A_{S^{(t)}}} = A_{S^{(t)}} \\left( A_{S^{(t)}}^\\top A_{S^{(t)}} \\right)^{-1} A_{S^{(t)}}^\\top$, yielding the residual $R^{(t)} = (I - P_{A_{S^{(t)}}}) Y$.\n- The greedy selection of an index $j \\notin S^{(t-1)}$ at iteration $t$ is obtained by maximizing the energy captured by projecting the current residual $R^{(t-1)}$ onto the atom $A_j$. With unit-norm columns, the energy captured is $\\| A_j^\\top R^{(t-1)} \\|_2^2$, so the SOMP selection rule chooses\n$$\nj^{\\star} \\in \\operatorname*{arg\\,max}_{j \\notin S^{(t-1)}} \\| A_j^\\top R^{(t-1)} \\|_2.\n$$\n\nYour task is to implement SOMP and run it for exactly two iterations, tracking the evolution of the residual and the selected indices on the following small, fully specified instances. All indices in this problem use zero-based indexing.\n\nImplement the following test suite with parameters $(m,n,L) = (5,10,3)$ and joint sparsity level $k = 2$:\n\n- Test Case 1 (Noise-free, general dictionary):\n  - Construct $A$ by drawing entries independently from a standard normal distribution with a fixed seed $s_A = 0$ and normalizing each column to unit $\\ell_2$ norm.\n  - Choose a known support $S^\\mathrm{true} = [2, 7]$ and set nonzero rows of $X$ at indices $2$ and $7$ as $X_{2,:} = [1.0, -0.5, 0.8]$ and $X_{7,:} = [0.9, 0.3, -1.2]$. All other rows of $X$ are zero.\n  - Set $E = 0$ and $Y = A X$.\n\n- Test Case 2 (Small noise, same dictionary and support):\n  - Use the same $A$ and $X$ as in Test Case 1.\n  - Add Gaussian noise $E$ with entries drawn independently from a standard normal distribution scaled by $\\sigma = 0.01$ with seed $s_E = 1$, i.e., $E = \\sigma \\cdot W$ where $W$ has independent standard normal entries.\n  - Set $Y = A X + E$.\n\n- Test Case 3 (Tie in first selection, orthonormal atoms):\n  - Construct $A$ as $A = [I_5 \\,|\\, B]$, where $I_5 \\in \\mathbb{R}^{5 \\times 5}$ is the identity (its columns are already unit norm) and $B \\in \\mathbb{R}^{5 \\times 5}$ is formed by drawing entries from a standard normal distribution with fixed seed $s_B = 2$ and normalizing each column of $B$ to unit $\\ell_2$ norm.\n  - Choose a known support $S^\\mathrm{true} = [1, 2]$ and set nonzero rows of $X$ at indices $1$ and $2$ as $X_{1,:} = [0.6, 0.8, 0.0]$ and $X_{2,:} = [0.0, 0.6, 0.8]$. All other rows of $X$ are zero.\n  - Set $E = 0$ and $Y = A X$.\n  - Because $\\|X_{1,:}\\|_2 = \\|X_{2,:}\\|_2$, and the first five atoms are orthonormal, the first SOMP iteration induces a tie between indices $1$ and $2$. Resolve ties by choosing the smallest index.\n\nAlgorithmic requirements:\n- Normalize every column of $A$ to unit $\\ell_2$ norm before running SOMP.\n- At each iteration $t = 1,2$, compute the correlation matrix $C^{(t)} = A^\\top R^{(t-1)} \\in \\mathbb{R}^{n \\times L}$, then the selection scores $g_j^{(t)} = \\| C^{(t)}_{j,:} \\|_2$ for all $j \\notin S^{(t-1)}$, select $j^{\\star}$ as the index achieving the maximum score (breaking ties by choosing the smallest index), update $S^{(t)} = S^{(t-1)} \\cup \\{ j^{\\star} \\}$, and update the residual by least squares with $R^{(t)} = Y - A_{S^{(t)}} X_{S^{(t)}}$, where $X_{S^{(t)}}$ is the least-squares solution minimizing $\\| Y - A_{S^{(t)}} Z \\|_F$.\n- Record the sequence of selected indices $[j^{\\star}_1, j^{\\star}_2]$ and the Frobenius norms of the residual $\\| R^{(1)} \\|_F$ and $\\| R^{(2)} \\|_F$.\n\nFinal output format:\n- For each test case, output a list containing two elements: the list of selected indices over the two iterations, and the list of the two residual Frobenius norms (rounded to six decimal places).\n- Aggregate the results from all test cases in order into a single list, and print a single line containing this aggregated list as a comma-separated list enclosed in square brackets, with no additional text. For example, the output should look like\n$$\n[[[j_1,j_2],[r_1,r_2]],[[\\ldots],[\\ldots]],[[\\ldots],[\\ldots]]]\n$$\nwhere $j_1, j_2$ are integers and $r_1, r_2$ are floats rounded to six decimal places.\n\nYour program must be a complete, runnable implementation that produces the specified single-line output for the above three test cases.", "solution": "The problem is valid. It is a well-posed, scientifically grounded, and objective task rooted in the established principles of compressed sensing and sparse signal recovery. All parameters, conditions, and procedures are specified with sufficient clarity and precision to permit a unique and verifiable solution.\n\nThe task is to implement the Simultaneous Orthogonal Matching Pursuit (SOMP) algorithm to recover a row-sparse matrix $X$ from a set of linear measurements $Y = AX + E$. SOMP is an iterative greedy algorithm designed for the Multiple Measurement Vector (MMV) problem, where multiple signal vectors share a common sparse support. The algorithm iteratively identifies the support of $X$ by selecting columns (atoms) from the dictionary $A$ that best explain the observed measurements $Y$.\n\nThe implementation proceeds by faithfully executing the steps defined by the SOMP algorithm for exactly two iterations.\n\n**Algorithm Principle and Implementation**\n\nThe core of the SOMP algorithm lies in its iterative two-stage process: a selection stage and an update stage.\n\n**1. Initialization:**\nThe algorithm begins with an empty support set, $S^{(0)} = \\emptyset$, and the initial residual equal to the measurement matrix, $R^{(0)} = Y$.\n\n**2. Iterative Process (for iterations $t=1, 2, \\dots$):**\n\n**a. Atom Selection:**\nThe fundamental principle of the selection step is to identify the atom $A_j$ from the dictionary that has the highest correlation with the current residual, $R^{(t-1)}$. This correlation is measured across all $L$ measurement channels. For each atom $A_j$ (where $j$ is not already in the support set $S^{(t-1)}$), we compute the projection of the multi-channel residual onto it. The energy of this projection is given by $\\| A_j^\\top R^{(t-1)} \\|_2^2$. SOMP selects the atom that maximizes this energy, which is equivalent to maximizing the $\\ell_2$-norm of the correlation vector:\n$$\nj^{\\star} = \\operatorname*{arg\\,max}_{j \\notin S^{(t-1)}} \\| A_j^\\top R^{(t-1)} \\|_2\n$$\nIn the implementation, this is achieved by first computing the correlation matrix $C^{(t)} = A^\\top R^{(t-1)}$. The selection score for each atom $j$ is then the $\\ell_2$-norm of the $j$-th row of $C^{(t)}$. The `numpy.linalg.norm` function with `axis=1` is used for this purpose. Indices already selected are masked out, and `numpy.argmax` finds the index of the atom with the highest score. The problem specifies that ties are broken by choosing the smallest index, a behavior inherent to `numpy.argmax`. The support set is then updated: $S^{(t)} = S^{(t-1)} \\cup \\{ j^{\\star} \\}$.\n\n**b. Least-Squares Update and Residual Calculation:**\nOnce the support set $S^{(t)}$ is updated, the coefficient matrix $X$ is re-estimated to best fit the measurements $Y$ using only the selected atoms. This is a classic least-squares problem:\n$$\nX_{S^{(t)}} = \\operatorname*{arg\\,min}_{Z \\in \\mathbb{R}^{|S^{(t)}| \\times L}} \\| Y - A_{S^{(t)}} Z \\|_F\n$$\nwhere $A_{S^{(t)}}$ is the sub-matrix of $A$ containing the columns indexed by $S^{(t)}$, and $X_{S^{(t)}}$ are the corresponding non-zero rows of the estimate of $X$. The solution to this minimization problem is given by projecting $Y$ onto the subspace spanned by the columns of $A_{S^{(t)}}$. When the columns of $A_{S^{(t)}}$ are linearly independent, the solution is unique and given by $X_{S^{(t)}} = (A_{S^{(t)}}^\\top A_{S^{(t)}})^{-1} A_{S^{(t)}}^\\top Y$.\nThe implementation uses `numpy.linalg.lstsq(A_S, Y)`, which provides a numerically stable and accurate solution to this least-squares problem, effectively computing the product of the Moore-Penrose pseudoinverse of $A_{S^{(t)}}$ with $Y$.\n\nThe new residual, $R^{(t)}$, is the part of $Y$ not explained by this projection:\n$$\nR^{(t)} = Y - A_{S^{(t)}} X_{S^{(t)}}\n$$\nThe Frobenius norm of this residual, $\\|R^{(t)}\\|_F$, is computed and recorded at each iteration. This norm quantifies the remaining error and its decrease indicates the algorithm's progress in explaining the data.\n\nThis process is repeated for the required two iterations for each of the three test cases specified. The setup for each test case involves constructing the matrices $A$, $X$, and $Y$ according to the given parameters, including random seeds for reproducibility and specific noise conditions. The final output aggregates the list of selected indices and the list of residual Frobenius norms for each case.", "answer": "```python\nimport numpy as np\n\ndef run_somp(A, Y, num_iterations):\n    \"\"\"\n    Implements the Simultaneous Orthogonal Matching Pursuit (SOMP) algorithm.\n\n    Args:\n        A (np.ndarray): The measurement matrix (m x n).\n        Y (np.ndarray): The measurement vectors (m x L).\n        num_iterations (int): The number of iterations to run.\n\n    Returns:\n        tuple: A tuple containing:\n            - list: The list of selected indices.\n            - list: The list of residual Frobenius norms at each iteration.\n    \"\"\"\n    m, n = A.shape\n    \n    # Initial residual is the measurement matrix itself\n    R = Y.copy()\n    \n    # Set of selected indices\n    S = []\n    \n    # Mask for selected indices to handle argmax efficiently\n    selected_mask = np.zeros(n, dtype=bool)\n    \n    # List to store residual norms\n    residual_norms = []\n\n    for _ in range(num_iterations):\n        # 1. Selection Step: Find the atom most correlated with the residual\n        # Compute correlations: A^T * R\n        C = A.T @ R\n        \n        # Compute scores as the l2-norm of each row of the correlation matrix\n        scores = np.linalg.norm(C, axis=1)\n        \n        # Mask out already selected columns by setting their score to a negative value\n        scores[selected_mask] = -1.0\n        \n        # Select the index with the maximum score. numpy.argmax breaks ties\n        # by choosing the smallest index, as required.\n        j_star = np.argmax(scores)\n        \n        # 2. Update Step\n        # Add the new index to the support set\n        S.append(j_star)\n        selected_mask[j_star] = True\n        \n        # Form the submatrix of A with currently selected columns\n        A_S = A[:, S]\n        \n        # Solve the least-squares problem: min ||Y - A_S * Z||_F\n        # np.linalg.lstsq is a numerically stable way to do this.\n        X_S, _, _, _ = np.linalg.lstsq(A_S, Y, rcond=None)\n        \n        # Update the residual\n        R = Y - A_S @ X_S\n        \n        # Calculate and store the Frobenius norm of the new residual\n        norm_R = np.linalg.norm(R, 'fro')\n        residual_norms.append(norm_R)\n        \n    # Round norms to six decimal places as per the requirement\n    rounded_norms = [round(norm, 6) for norm in residual_norms]\n    \n    return S, rounded_norms\n\ndef solve():\n    \"\"\"\n    Sets up and runs the three test cases for the SOMP algorithm,\n    then prints the formatted results.\n    \"\"\"\n    results = []\n    m, n, L = 5, 10, 3\n    num_iterations = 2\n\n    # --- Test Case 1: Noise-free, general dictionary ---\n    rng_A = np.random.default_rng(seed=0)\n    A1 = rng_A.standard_normal((m, n))\n    A1 /= np.linalg.norm(A1, axis=0)  # Normalize each column to unit l2-norm\n    \n    X1 = np.zeros((n, L))\n    X1[2, :] = [1.0, -0.5, 0.8]\n    X1[7, :] = [0.9, 0.3, -1.2]\n    \n    Y1 = A1 @ X1\n    \n    indices1, norms1 = run_somp(A1, Y1, num_iterations)\n    results.append([indices1, norms1])\n\n    # --- Test Case 2: Small noise, same dictionary and support ---\n    # Use the same A1 and X1\n    rng_E = np.random.default_rng(seed=1)\n    sigma = 0.01\n    E = sigma * rng_E.standard_normal((m, L))\n    \n    Y2 = A1 @ X1 + E\n    \n    indices2, norms2 = run_somp(A1, Y2, num_iterations)\n    results.append([indices2, norms2])\n\n    # --- Test Case 3: Tie in first selection, orthonormal atoms ---\n    rng_B = np.random.default_rng(seed=2)\n    I5 = np.identity(5)\n    B = rng_B.standard_normal((5, 5))\n    B /= np.linalg.norm(B, axis=0)\n    A3 = np.hstack((I5, B))\n    \n    X3 = np.zeros((n, L))\n    X3[1, :] = [0.6, 0.8, 0.0]\n    X3[2, :] = [0.0, 0.6, 0.8]\n    \n    Y3 = A3 @ X3\n    \n    indices3, norms3 = run_somp(A3, Y3, num_iterations)\n    results.append([indices3, norms3])\n\n    # Format the final output string to remove spaces for a compact representation\n    final_output_str = str(results).replace(\" \", \"\")\n    print(final_output_str)\n\nsolve()\n```", "id": "3460799"}, {"introduction": "Moving from greedy algorithms to the powerful framework of convex optimization, we now focus on the key building block for solving $\\ell_{2,1}$-regularized problems. This practice involves deriving the proximal operator for the mixed $\\ell_{2,1}$ norm, a crucial component in algorithms like ISTA and FISTA. By working through the derivation and applying it to a numerical instance, you will gain a deep understanding of the \"group soft-thresholding\" mechanism that enforces joint sparsity [@problem_id:3460758].", "problem": "Consider the Multiple Measurement Vector (MMV) model for joint-sparse recovery, where the unknown coefficient matrix $X \\in \\mathbb{R}^{n \\times L}$ has row-sparse structure and the data fidelity is modeled via a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ and measurements $Y \\in \\mathbb{R}^{m \\times L}$. A widely used convex objective for estimating $X$ is the sum of a quadratic data term and the mixed $\\ell_{2,1}$ norm, defined by $\\|X\\|_{2,1} = \\sum_{i=1}^{n} \\|X_{i,:}\\|_2$. In proximal splitting methods, the proximal operator of the function $g(X) = \\tau \\|X\\|_{2,1}$, for $\\tau  0$, is required. Starting from the definition of the proximal operator and the mixed $\\ell_{2,1}$ norm, derive the closed-form expression of $\\operatorname{prox}_{\\tau \\|\\cdot\\|_{2,1}}(V)$ for a given matrix $V \\in \\mathbb{R}^{n \\times L}$ by analyzing the optimization problem row-wise and establishing the optimality conditions. Then apply your derivation to the specific matrix\n$$\nV = \\begin{pmatrix}\n3  4  0  0 \\\\\n1  2  2  1 \\\\\n1  1  0  0\n\\end{pmatrix}\n$$\nwith parameter $\\tau = 2$, and compute the squared Frobenius norm of the proximal output $X = \\operatorname{prox}_{\\tau \\|\\cdot\\|_{2,1}}(V)$. Provide your final answer as an exact closed-form expression. No rounding is required.", "solution": "The problem requires the derivation of the proximal operator for the mixed $\\ell_{2,1}$ norm and its application to a specific matrix $V$ and parameter $\\tau$.\n\nFirst, we establish the formal definition of the proximal operator. For a function $g: \\mathbb{R}^{n \\times L} \\to \\mathbb{R}$, its proximal operator evaluated at a point $V \\in \\mathbb{R}^{n \\times L}$ is defined as the unique solution to the optimization problem:\n$$ \\operatorname{prox}_{g}(V) = \\arg\\min_{X \\in \\mathbb{R}^{n \\times L}} \\left( g(X) + \\frac{1}{2} \\|X-V\\|_F^2 \\right) $$\nwhere $\\|\\cdot\\|_F$ denotes the Frobenius norm.\n\nIn this problem, the function is $g(X) = \\tau \\|X\\|_{2,1}$, where $\\tau  0$ is a scalar parameter and $\\|X\\|_{2,1}$ is the mixed $\\ell_{2,1}$ norm, defined as the sum of the Euclidean norms of the rows of $X$:\n$$ \\|X\\|_{2,1} = \\sum_{i=1}^{n} \\|X_{i,:}\\|_2 $$\nHere, $X_{i,:}$ denotes the $i$-th row vector of the matrix $X$.\n\nThe optimization problem for the proximal operator of $g(X)$ is therefore:\n$$ X^* = \\operatorname{prox}_{\\tau \\|\\cdot\\|_{2,1}}(V) = \\arg\\min_{X} \\left( \\tau \\sum_{i=1}^{n} \\|X_{i,:}\\|_2 + \\frac{1}{2} \\|X-V\\|_F^2 \\right) $$\nThe squared Frobenius norm is the sum of the squared Euclidean norms of the rows of its matrix argument:\n$$ \\|X-V\\|_F^2 = \\sum_{i=1}^{n} \\|X_{i,:} - V_{i,:}\\|_2^2 $$\nSubstituting this decomposition into the objective function, we get:\n$$ X^* = \\arg\\min_{X} \\sum_{i=1}^{n} \\left( \\tau \\|X_{i,:}\\|_2 + \\frac{1}{2} \\|X_{i,:} - V_{i,:}\\|_2^2 \\right) $$\nThe objective function is a sum of terms, where the $i$-th term depends only on the $i$-th row of $X$, which is $X_{i,:}$. This separability allows us to minimize the function by minimizing it for each row independently. For each row $i \\in \\{1, \\dots, n\\}$, we solve:\n$$ X_{i,:}^* = \\arg\\min_{x_i \\in \\mathbb{R}^{1 \\times L}} \\left( \\tau \\|x_i\\|_2 + \\frac{1}{2} \\|x_i - v_i\\|_2^2 \\right) $$\nwhere we have let $x_i = X_{i,:}$ and $v_i = V_{i,:}$ for notational simplicity. This subproblem is the proximal operator of the scaled Euclidean norm, $\\tau \\|\\cdot\\|_2$.\n\nTo solve this subproblem, we use subdifferential calculus. Let $J(x_i) = \\tau \\|x_i\\|_2 + \\frac{1}{2} \\|x_i - v_i\\|_2^2$. The first-order necessary and sufficient condition for optimality is $0 \\in \\partial J(x_i^*)$. The subdifferential of $J$ is $\\partial J(x_i) = \\tau \\partial \\|x_i\\|_2 + \\nabla \\left(\\frac{1}{2} \\|x_i - v_i\\|_2^2\\right) = \\tau \\partial \\|x_i\\|_2 + (x_i - v_i)$.\nThe subdifferential of the Euclidean norm is:\n$$ \\partial \\|x_i\\|_2 = \\begin{cases} \\{ u \\in \\mathbb{R}^{1 \\times L} \\mid \\|u\\|_2 \\le 1 \\}  \\text{if } x_i = 0 \\\\ \\{ \\frac{x_i}{\\|x_i\\|_2} \\}  \\text{if } x_i \\ne 0 \\end{cases} $$\nWe consider two cases for the solution $x_i^*$:\n\nCase 1: $x_i^* \\ne 0$.\nThe optimality condition is $0 = \\tau \\frac{x_i^*}{\\|x_i^*\\|_2} + x_i^* - v_i$.\nRearranging this equation gives $v_i = x_i^* \\left(1 + \\frac{\\tau}{\\|x_i^*\\|_2}\\right)$. This shows that $v_i$ and $x_i^*$ are collinear. Taking the Euclidean norm of both sides yields $\\|v_i\\|_2 = \\|x_i^*\\|_2 \\left(1 + \\frac{\\tau}{\\|x_i^*\\|_2}\\right) = \\|x_i^*\\|_2 + \\tau$. Thus, $\\|x_i^*\\|_2 = \\|v_i\\|_2 - \\tau$. For this to be a non-zero solution, we must have $\\|v_i\\|_2 - \\tau  0$, or $\\|v_i\\|_2  \\tau$. If this condition holds, we can find $x_i^*$ from the collinearity: $x_i^* = \\frac{\\|x_i^*\\|_2}{\\|v_i\\|_2} v_i = \\frac{\\|v_i\\|_2 - \\tau}{\\|v_i\\|_2} v_i = \\left(1 - \\frac{\\tau}{\\|v_i\\|_2}\\right) v_i$.\n\nCase 2: $x_i^* = 0$.\nThe optimality condition becomes $0 \\in \\tau \\partial \\|0\\|_2 + (0 - v_i)$, which implies $v_i \\in \\tau \\partial \\|0\\|_2$. This means $v_i$ must belong to the set $\\{ u \\mid \\|u\\|_2 \\le \\tau \\}$, so $\\|v_i\\|_2 \\le \\tau$.\n\nCombining both cases, the closed-form solution for each row $X_{i,:}^*$ is a block soft-thresholding operation:\n$$ X_{i,:}^* = \\begin{cases} \\left(1 - \\frac{\\tau}{\\|V_{i,:}\\|_2}\\right) V_{i,:}  \\text{if } \\|V_{i,:}\\|_2  \\tau \\\\ 0  \\text{if } \\|V_{i,:}\\|_2 \\le \\tau \\end{cases} $$\nThis can be written compactly as $X_{i,:}^* = \\max\\left(0, 1 - \\frac{\\tau}{\\|V_{i,:}\\|_2}\\right)V_{i,:}$.\n\nWe now apply this formula to the given matrix $V$ with $\\tau = 2$:\n$$ V = \\begin{pmatrix} 3  4  0  0 \\\\ 1  2  2  1 \\\\ 1  1  0  0 \\end{pmatrix} $$\nLet $X = \\operatorname{prox}_{2 \\|\\cdot\\|_{2,1}}(V)$. We compute $X$ row by row.\n\nFor the first row, $V_{1,:} = (3, 4, 0, 0)$:\nThe $\\ell_2$-norm is $\\|V_{1,:}\\|_2 = \\sqrt{3^2 + 4^2 + 0^2 + 0^2} = \\sqrt{25} = 5$.\nSince $\\|V_{1,:}\\|_2 = 5  \\tau = 2$, we apply the shrinkage formula:\n$X_{1,:} = \\left(1 - \\frac{2}{5}\\right) V_{1,:} = \\frac{3}{5} (3, 4, 0, 0)$.\n\nFor the second row, $V_{2,:} = (1, 2, 2, 1)$:\nThe $\\ell_2$-norm is $\\|V_{2,:}\\|_2 = \\sqrt{1^2 + 2^2 + 2^2 + 1^2} = \\sqrt{10}$.\nSince $\\|V_{2,:}\\|_2 = \\sqrt{10} \\approx 3.162  \\tau = 2$, we again apply the shrinkage formula:\n$X_{2,:} = \\left(1 - \\frac{2}{\\sqrt{10}}\\right) V_{2,:}$.\n\nFor the third row, $V_{3,:} = (1, 1, 0, 0)$:\nThe $\\ell_2$-norm is $\\|V_{3,:}\\|_2 = \\sqrt{1^2 + 1^2 + 0^2 + 0^2} = \\sqrt{2}$.\nSince $\\|V_{3,:}\\|_2 = \\sqrt{2} \\approx 1.414  \\tau = 2$, this row is set to zero:\n$X_{3,:} = (0, 0, 0, 0)$.\n\nThe problem asks for the squared Frobenius norm of the resulting matrix $X$, which is $\\|X\\|_F^2 = \\sum_{i=1}^{3} \\|X_{i,:}\\|_2^2$.\nWe compute the squared norms of the rows of $X$. If $\\|V_{i,:}\\|_2  \\tau$, then $\\|X_{i,:}\\|_2 = \\left(1 - \\frac{\\tau}{\\|V_{i,:}\\|_2}\\right) \\|V_{i,:}\\|_2 = \\|V_{i,:}\\|_2 - \\tau$. Consequently, $\\|X_{i,:}\\|_2^2 = (\\|V_{i,:}\\|_2 - \\tau)^2$. If $\\|V_{i,:}\\|_2 \\le \\tau$, then $\\|X_{i,:}\\|_2^2 = 0$.\n\nFor the first row:\n$\\|X_{1,:}\\|_2^2 = (\\|V_{1,:}\\|_2 - \\tau)^2 = (5 - 2)^2 = 3^2 = 9$.\n\nFor the second row:\n$\\|X_{2,:}\\|_2^2 = (\\|V_{2,:}\\|_2 - \\tau)^2 = (\\sqrt{10} - 2)^2 = (\\sqrt{10})^2 - 2(2)\\sqrt{10} + 2^2 = 10 - 4\\sqrt{10} + 4 = 14 - 4\\sqrt{10}$.\n\nFor the third row:\nSince $\\|V_{3,:}\\|_2 \\le \\tau$, we have $\\|X_{3,:}\\|_2^2 = 0$.\n\nThe squared Frobenius norm of $X$ is the sum of these values:\n$\\|X\\|_F^2 = \\|X_{1,:}\\|_2^2 + \\|X_{2,:}\\|_2^2 + \\|X_{3,:}\\|_2^2 = 9 + (14 - 4\\sqrt{10}) + 0 = 23 - 4\\sqrt{10}$.", "answer": "$$\\boxed{23 - 4\\sqrt{10}}$$", "id": "3460758"}, {"introduction": "Having explored both greedy and optimization-based algorithms, we now turn to a foundational theoretical question: what properties of the MMV model enable unique support identification? This exercise guides you through a probabilistic analysis to show that the submatrix of true coefficients, $X_S$, is full-rank with probability one, provided its entries are drawn from a continuous distribution. Understanding this result reveals the profound implication that the subspace spanned by the measurements, $\\mathcal{C}(Y)$, becomes identical to the subspace of the true dictionary atoms, $\\mathcal{C}(A_S)$, which underpins the success of many advanced MMV algorithms [@problem_id:3460795].", "problem": "Consider the Multiple Measurement Vector (MMV) model in compressed sensing, where the noiseless measurements are given by $Y = A X$, with $A \\in \\mathbb{R}^{m \\times n}$, $X \\in \\mathbb{R}^{n \\times L}$, and $Y \\in \\mathbb{R}^{m \\times L}$. The matrix $X$ is jointly $k$-sparse, meaning there exists an index set $S \\subset \\{1,2,\\dots,n\\}$ with $|S| = k$ such that the rows of $X$ with indices in $S$ may be nonzero while all other rows are identically zero. Let $X_S \\in \\mathbb{R}^{k \\times L}$ denote the submatrix formed by the $k$ active rows of $X$. Assume the entries of $X_S$ are drawn independently and identically from a continuous Gaussian distribution with zero mean and finite variance (that is, $X_S$ has independent and identically distributed (i.i.d.) Gaussian entries).\n\nUsing only foundational principles from linear algebra and probability, compute the probability (with respect to the randomness in $X_S$) that $\\operatorname{rank}(X_S) = \\min(k,L)$. Then, using this conclusion and the definitions of rank and column space, discuss the implications of this rank property for joint support identifiability in the MMV model as $Y = A_S X_S$ with $A_S \\in \\mathbb{R}^{m \\times k}$ the submatrix of $A$ formed by columns indexed by $S$. Provide your final answer for the probability as an exact value. No rounding is required, and no physical units are involved.", "solution": "The problem will first be validated according to the specified criteria.\n\n### Step 1: Extract Givens\n-   Model: Multiple Measurement Vector (MMV) model.\n-   Noiseless measurements: $Y = A X$.\n-   Matrix dimensions: $A \\in \\mathbb{R}^{m \\times n}$, $X \\in \\mathbb{R}^{n \\times L}$, $Y \\in \\mathbb{R}^{m \\times L}$.\n-   Sparsity of $X$: $X$ is jointly $k$-sparse.\n-   Support set: $S \\subset \\{1,2,\\dots,n\\}$ with $|S| = k$.\n-   Definition of joint sparsity: Rows of $X$ with indices not in $S$ are identically zero.\n-   Active submatrix: $X_S \\in \\mathbb{R}^{k \\times L}$ is formed by the $k$ active rows of $X$.\n-   Distribution of active entries: The entries of $X_S$ are drawn independently and identically from a continuous Gaussian distribution with zero mean and finite variance.\n-   Task 1: Compute the probability that $\\operatorname{rank}(X_S) = \\min(k,L)$.\n-   Task 2: Discuss the implications of this rank property for joint support identifiability in the model $Y = A_S X_S$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated for validity.\n-   **Scientifically Grounded**: The problem is set within the well-established framework of compressed sensing, specifically addressing the Multiple Measurement Vector (MMV) model. The concepts of joint sparsity, rank of random matrices, and support identifiability are central to this field. The assumption of i.i.d. Gaussian entries is a standard and fundamental model for theoretical analysis in signal processing and random matrix theory. The problem is based on established mathematical principles.\n-   **Well-Posed**: The problem asks for a specific probability and a subsequent discussion of its implications. The calculation of the rank probability of a random matrix with entries from a continuous distribution is a standard problem in probability and linear algebra. The discussion is framed within the specific context of the MMV model, making it well-defined.\n-   **Objective**: The problem is stated using precise, unambiguous mathematical terminology. There are no subjective or opinion-based elements.\n-   **Complete and Consistent**: All necessary information is provided. The dimensions of the matrices ($k$, $L$), the nature of the random entries (i.i.d. continuous Gaussian), and the specific quantities to be computed and discussed are clearly defined. There are no contradictions in the given information.\n\n### Step 3: Verdict and Action\nThe problem is scientifically sound, well-posed, and objective. It is deemed **valid**. A full solution will be provided.\n\n### Solution\n\nThe solution is presented in two parts as requested by the problem statement.\n\n**Part 1: Probability of Full Rank for $X_S$**\n\nWe are asked to compute the probability that $\\operatorname{rank}(X_S) = \\min(k,L)$, where $X_S$ is a $k \\times L$ matrix whose entries are independent and identically distributed (i.i.d.) random variables from a continuous Gaussian distribution. The rank of a matrix is the dimension of the vector space spanned by its columns (or rows). The maximum possible rank of a $k \\times L$ matrix is $\\min(k,L)$. The condition $\\operatorname{rank}(X_S) = \\min(k,L)$ is equivalent to the matrix having full rank (either full row rank or full column rank).\n\nA matrix is rank-deficient if its rank is strictly less than the maximum possible rank. We will show that the event of $X_S$ being rank-deficient has a probability of $0$.\n\nLet's consider two cases.\n\nCase 1: $k \\le L$.\nIn this case, $\\min(k,L) = k$. The matrix $X_S$ has at most $k$ linearly independent rows. The matrix is rank-deficient if $\\operatorname{rank}(X_S)  k$. This occurs if and only if the $k$ rows of $X_S$ are linearly dependent. Let the rows be $\\mathbf{r}_1, \\mathbf{r}_2, \\dots, \\mathbf{r}_k$, where each $\\mathbf{r}_i \\in \\mathbb{R}^L$.\n\nThe rows are linearly dependent if there exists a set of scalars $c_1, c_2, \\dots, c_k$, not all zero, such that $\\sum_{i=1}^k c_i \\mathbf{r}_i = \\mathbf{0}$. This is equivalent to at least one row vector being a linear combination of the others.\n\nLet $E$ be the event that the rows are linearly dependent. Let $E_i$ be the event that the $i$-th row $\\mathbf{r}_i$ is in the span of the preceding $i-1$ rows, $\\operatorname{span}(\\mathbf{r}_1, \\dots, \\mathbf{r}_{i-1})$. The event $E$ is the union of these events: $E = \\bigcup_{i=1}^k E_i$.\n\n-   For $i=1$, $E_1$ is the event that $\\mathbf{r}_1 \\in \\operatorname{span}(\\emptyset)$, which means $\\mathbf{r}_1 = \\mathbf{0}$. Since each of the $L$ entries of $\\mathbf{r}_1$ is from a continuous distribution, the probability of any single entry being exactly $0$ is $0$. Thus, $P(\\mathbf{r}_1 = \\mathbf{0}) = 0$.\n-   For $i  1$, consider the event $E_i$. Let $W_{i-1} = \\operatorname{span}(\\mathbf{r}_1, \\dots, \\mathbf{r}_{i-1})$. The dimension of this subspace, $\\dim(W_{i-1})$, is at most $i-1$. Since $i-1  k \\le L$, $W_{i-1}$ is a proper subspace of $\\mathbb{R}^L$. As a subspace of dimension less than $L$, $W_{i-1}$ has an $L$-dimensional Lebesgue measure of zero.\n-   The vector $\\mathbf{r}_i$ is a random vector in $\\mathbb{R}^L$ whose entries are i.i.d. from a continuous Gaussian distribution. The joint probability distribution of its entries is absolutely continuous with respect to the Lebesgue measure on $\\mathbb{R}^L$. The probability that such a random vector falls into a set of measure zero is zero.\n-   Therefore, for any given set of linearly independent vectors $\\mathbf{r}_1, \\dots, \\mathbf{r}_{i-1}$, the conditional probability $P(\\mathbf{r}_i \\in W_{i-1} | \\mathbf{r}_1, \\dots, \\mathbf{r}_{i-1}) = 0$. If the preceding vectors are linearly dependent, the dimension of their span is even smaller, and the probability remains $0$. Averaging over all possibilities for the preceding vectors, we find that the unconditional probability $P(E_i) = P(\\mathbf{r}_i \\in \\operatorname{span}(\\mathbf{r}_1, \\dots, \\mathbf{r}_{i-1})) = 0$ for all $i=2, \\dots, k$.\n\nBy the union bound, the probability of rank deficiency is $P(\\operatorname{rank}(X_S)  k) = P(E) = P(\\bigcup_{i=1}^k E_i) \\le \\sum_{i=1}^k P(E_i) = \\sum_{i=1}^k 0 = 0$.\nSince the probability must be non-negative, we have $P(\\operatorname{rank}(X_S)  k) = 0$.\nTherefore, the probability of the complementary event is $P(\\operatorname{rank}(X_S) = k) = 1 - 0 = 1$.\n\nCase 2: $L  k$.\nIn this case, $\\min(k,L) = L$. The argument is symmetric to Case 1, but applied to the $L$ columns of $X_S$, which are vectors in $\\mathbb{R}^k$. Let the columns be $\\mathbf{v}_1, \\dots, \\mathbf{v}_L \\in \\mathbb{R}^k$. The matrix is rank-deficient if $\\operatorname{rank}(X_S)  L$, which occurs if and only if the columns are linearly dependent.\nFollowing the same logic, the subspace spanned by the first $j-1$ columns, $\\operatorname{span}(\\mathbf{v}_1, \\dots, \\mathbf{v}_{j-1})$, has dimension at most $j-1$ in $\\mathbb{R}^k$. Since $j \\le L  k$, this is a proper subspace of $\\mathbb{R}^k$ and has $k$-dimensional Lebesgue measure zero. The probability that the next independent random column $\\mathbf{v}_j$ (drawn from a continuous distribution on $\\mathbb{R}^k$) falls into this subspace is $0$.\nBy the union bound, the probability that the set of columns is linearly dependent is $0$. Thus, $P(\\operatorname{rank}(X_S)  L) = 0$.\nThis implies $P(\\operatorname{rank}(X_S) = L) = 1$.\n\nCombining both cases, the probability that $\\operatorname{rank}(X_S) = \\min(k,L)$ is $1$.\n\n**Part 2: Implications for Joint Support Identifiability**\n\nThe fact that $\\operatorname{rank}(X_S) = \\min(k,L)$ with probability $1$ has a profound implication for identifying the support set $S$ in the MMV model $Y = A_S X_S$. The key insight arises from analyzing the relationship between the column space of the measurements, $\\mathcal{C}(Y)$, and the column space of the active dictionary atoms, $\\mathcal{C}(A_S)$.\n\nThe equation $Y = A_S X_S$ implies that every column of $Y$ is a linear combination of the columns of $A_S$. This means that the column space of $Y$ is a subspace of the column space of $A_S$, i.e., $\\mathcal{C}(Y) \\subseteq \\mathcal{C}(A_S)$.\n\nLet us consider the typical MMV scenario where the number of measurement vectors $L$ is greater than or equal to the sparsity level $k$, i.e., $L \\ge k$.\nIn this case, our result from Part 1 states that with probability $1$, $\\operatorname{rank}(X_S) = \\min(k,L) = k$.\nThe matrix $X_S \\in \\mathbb{R}^{k \\times L}$ having rank $k$ means it has full row rank. Geometrically, this implies that its $L$ columns, which are vectors in $\\mathbb{R}^k$, span the entire space $\\mathbb{R}^k$. That is, the column space of $X_S$ is $\\mathcal{C}(X_S) = \\mathbb{R}^k$.\n\nNow, let's examine the column space of the measurements, $\\mathcal{C}(Y)$. By definition, $\\mathcal{C}(Y)$ is the set of all linear combinations of the columns of $Y$. The columns of $Y$ are given by $A_S \\mathbf{x}_j$, where $\\mathbf{x}_j$ are the columns of $X_S$. So, $\\mathcal{C}(Y)$ is the image of the column space of $X_S$ under the linear transformation represented by $A_S$.\nMathematically:\n$$\n\\mathcal{C}(Y) = \\{A_S \\mathbf{v} \\mid \\mathbf{v} \\in \\mathcal{C}(X_S)\\}\n$$\nSince we established that $\\mathcal{C}(X_S) = \\mathbb{R}^k$ with probability $1$ (for $L \\ge k$), we can substitute this into the expression for $\\mathcal{C}(Y)$:\n$$\n\\mathcal{C}(Y) = \\{A_S \\mathbf{v} \\mid \\mathbf{v} \\in \\mathbb{R}^k\\}\n$$\nThis is precisely the definition of the column space of the matrix $A_S$, which is $\\mathcal{C}(A_S)$.\nTherefore, with probability $1$, we have the equality:\n$$\n\\mathcal{C}(Y) = \\mathcal{C}(A_S)\n$$\nThis equality is the central implication for joint support identifiability. It signifies that the subspace spanned by the measurement vectors is identical to the subspace spanned by the correct, active columns of the dictionary $A$. This transforms the problem of identifying the unknown support $S$. Instead of solving a difficult bilinear inverse problem for both $S$ and $X_S$, one can adopt a two-stage approach:\n1.  **Subspace Estimation**: Estimate the $k$-dimensional subspace $\\mathcal{C}(Y)$ directly from the measurement data $Y$ (e.g., via Singular Value Decomposition).\n2.  **Support Identification**: Search for the subset $S$ of $k$ columns from the full dictionary A whose span, $\\mathcal{C}(A_S)$, best matches the estimated subspace.\n\nThe uniqueness of this identification hinges on a property of the sensing matrix $A$: that no two distinct subsets of $k$ columns span the same $k$-dimensional subspace. This is a stronger condition than the spark of $A$ being greater than $2k$, but is often implicitly assumed to hold for random matrices $A$.\n\nIn summary, the full rank property of $X_S$ ensures that the coefficient matrix does not cause a loss of dimensional information. When $L \\ge k$, it guarantees that the subspace signature of the active columns $A_S$ is perfectly preserved in the measurements $Y$, thereby enabling support recovery via subspace methods.\n\nThe final answer requested is the probability computed in the first part.", "answer": "$$\n\\boxed{1}\n$$", "id": "3460795"}]}