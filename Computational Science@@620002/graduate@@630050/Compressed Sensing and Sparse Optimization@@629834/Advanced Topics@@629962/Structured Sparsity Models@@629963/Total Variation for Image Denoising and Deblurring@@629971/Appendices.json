{"hands_on_practices": [{"introduction": "To begin, we must ground our understanding in the fundamental definition of Total Variation (TV). This first exercise requires a direct, hands-on calculation of both anisotropic and isotropic TV for a small image patch. By working through this problem [@problem_id:3491268], you will gain a concrete feel for how TV measures image complexity and, crucially, understand the numerical and structural differences between the $\\ell_1$ (anisotropic) and $\\ell_2$ (isotropic) formulations, which have distinct effects on features like corners in denoised images.", "problem": "Consider a discrete image model on a Cartesian grid with unit spacing. Let the discrete gradient at pixel $(i,j)$ be defined by the forward-difference operator with homogeneous Neumann boundary conditions (i.e., any forward difference that would access an out-of-bounds pixel is set to zero). Specifically, the discrete gradient at $(i,j)$ is the two-vector consisting of the forward horizontal and vertical differences at $(i,j)$. The anisotropic total variation (TV) of a discrete image is the sum, over all pixels, of the $\\ell_{1}$ norm of the discrete gradient vector at each pixel. The isotropic total variation is the sum, over all pixels, of the $\\ell_{2}$ norm of the discrete gradient vector at each pixel.\n\nYou are given the $3\\times 3$ image patch with pixel values\n$$\nU \\;=\\;\n\\begin{pmatrix}\n1 & 2 & 2 \\\\\n1 & 3 & 5 \\\\\n2 & 3 & 6\n\\end{pmatrix}.\n$$\nUsing only the above fundamental definitions and the stated boundary model, do the following:\n- Compute explicitly the anisotropic total variation and the isotropic total variation of $U$.\n- Explain concisely, from first principles of vector norms, why these two values differ numerically for this patch and identify the structural source of the difference at the pixel level.\n\nLet $D$ denote the exact difference between the anisotropic and isotropic total variations,\n$$\nD \\;=\\; \\mathrm{TV}_{\\mathrm{anisotropic}}(U) \\;-\\; \\mathrm{TV}_{\\mathrm{isotropic}}(U).\n$$\nReport $D$ as a single closed-form analytic expression. Do not round your answer.", "solution": "The problem statement is internally consistent, scientifically grounded in the principles of numerical image analysis and vector norms, and provides all necessary information to compute a unique solution. Therefore, the problem is deemed valid. We proceed with the solution.\n\nLet the discrete image be represented by the matrix $U$, where $U_{i,j}$ is the pixel value at row $i$ and column $j$. The grid is a $3 \\times 3$ Cartesian grid, so the indices $(i,j)$ range from $(1,1)$ to $(3,3)$. The given image is:\n$$\nU =\n\\begin{pmatrix}\n1 & 2 & 2 \\\\\n1 & 3 & 5 \\\\\n2 & 3 & 6\n\\end{pmatrix}\n$$\nThe discrete gradient at a pixel $(i,j)$ is a vector $\\nabla U_{i,j} = \\begin{pmatrix} (\\nabla_x U)_{i,j} \\\\ (\\nabla_y U)_{i,j} \\end{pmatrix}$. The components are defined by forward differences:\n$$\n(\\nabla_x U)_{i,j} = U_{i,j+1} - U_{i,j}\n$$\n$$\n(\\nabla_y U)_{i,j} = U_{i+1,j} - U_{i,j}\n$$\nThe homogeneous Neumann boundary conditions imply that any difference that requires an out-of-bounds pixel is set to zero. For a $3 \\times 3$ image, this means $(\\nabla_x U)_{i,3} = 0$ for $i \\in \\{1,2,3\\}$ and $(\\nabla_y U)_{3,j} = 0$ for $j \\in \\{1,2,3\\}$.\n\nFirst, we compute the discrete gradient vector for each of the $9$ pixels:\n- At $(1,1)$: $\\nabla U_{1,1} = \\begin{pmatrix} U_{1,2} - U_{1,1} \\\\ U_{2,1} - U_{1,1} \\end{pmatrix} = \\begin{pmatrix} 2 - 1 \\\\ 1 - 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n- At $(1,2)$: $\\nabla U_{1,2} = \\begin{pmatrix} U_{1,3} - U_{1,2} \\\\ U_{2,2} - U_{1,2} \\end{pmatrix} = \\begin{pmatrix} 2 - 2 \\\\ 3 - 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$\n- At $(1,3)$: $\\nabla U_{1,3} = \\begin{pmatrix} 0 \\\\ U_{2,3} - U_{1,3} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 5 - 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}$\n- At $(2,1)$: $\\nabla U_{2,1} = \\begin{pmatrix} U_{2,2} - U_{2,1} \\\\ U_{3,1} - U_{2,1} \\end{pmatrix} = \\begin{pmatrix} 3 - 1 \\\\ 2 - 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$\n- At $(2,2)$: $\\nabla U_{2,2} = \\begin{pmatrix} U_{2,3} - U_{2,2} \\\\ U_{3,2} - U_{2,2} \\end{pmatrix} = \\begin{pmatrix} 5 - 3 \\\\ 3 - 3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$\n- At $(2,3)$: $\\nabla U_{2,3} = \\begin{pmatrix} 0 \\\\ U_{3,3} - U_{2,3} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 6 - 5 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$\n- At $(3,1)$: $\\nabla U_{3,1} = \\begin{pmatrix} U_{3,2} - U_{3,1} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 - 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n- At $(3,2)$: $\\nabla U_{3,2} = \\begin{pmatrix} U_{3,3} - U_{3,2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 6 - 3 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}$\n- At $(3,3)$: $\\nabla U_{3,3} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n\nNext, we compute the anisotropic total variation, $\\mathrm{TV}_{\\mathrm{anisotropic}}(U)$, which is the sum of the $\\ell_1$ norms of these gradient vectors. For a vector $v = \\begin{pmatrix} v_x \\\\ v_y \\end{pmatrix}$, its $\\ell_1$ norm is $\\|v\\|_1 = |v_x| + |v_y|$.\n$$\n\\mathrm{TV}_{\\mathrm{anisotropic}}(U) = \\sum_{i=1}^{3} \\sum_{j=1}^{3} \\|\\nabla U_{i,j}\\|_1\n$$\n$$\n\\mathrm{TV}_{\\mathrm{anisotropic}}(U) = (|1|+|0|) + (|0|+|1|) + (|0|+|3|) + (|2|+|1|) + (|2|+|0|) + (|0|+|1|) + (|1|+|0|) + (|3|+|0|) + (|0|+|0|)\n$$\n$$\n\\mathrm{TV}_{\\mathrm{anisotropic}}(U) = 1 + 1 + 3 + 3 + 2 + 1 + 1 + 3 + 0 = 15\n$$\n\nThen, we compute the isotropic total variation, $\\mathrm{TV}_{\\mathrm{isotropic}}(U)$, which is the sum of the $\\ell_2$ norms. For a vector $v = \\begin{pmatrix} v_x \\\\ v_y \\end{pmatrix}$, its $\\ell_2$ norm is $\\|v\\|_2 = \\sqrt{v_x^2 + v_y^2}$.\n$$\n\\mathrm{TV}_{\\mathrm{isotropic}}(U) = \\sum_{i=1}^{3} \\sum_{j=1}^{3} \\|\\nabla U_{i,j}\\|_2\n$$\n$$\n\\mathrm{TV}_{\\mathrm{isotropic}}(U) = \\sqrt{1^2+0^2} + \\sqrt{0^2+1^2} + \\sqrt{0^2+3^2} + \\sqrt{2^2+1^2} + \\sqrt{2^2+0^2} + \\sqrt{0^2+1^2} + \\sqrt{1^2+0^2} + \\sqrt{3^2+0^2} + \\sqrt{0^2+0^2}\n$$\n$$\n\\mathrm{TV}_{\\mathrm{isotropic}}(U) = 1 + 1 + 3 + \\sqrt{5} + 2 + 1 + 1 + 3 + 0 = 12 + \\sqrt{5}\n$$\n\nThe two values differ because of the fundamental properties of the $\\ell_1$ and $\\ell_2$ norms. For any vector $v$ in $\\mathbb{R}^n$, the inequality $\\|v\\|_1 \\ge \\|v\\|_2$ holds. In our case, for a $2$-dimensional gradient vector $v = \\begin{pmatrix} v_x \\\\ v_y \\end{pmatrix}$, we have $\\|v\\|_1 = |v_x|+|v_y|$ and $\\|v\\|_2 = \\sqrt{v_x^2+v_y^2}$. Equality, $|v_x|+|v_y| = \\sqrt{v_x^2+v_y^2}$, holds if and only if at least one component, $v_x$ or $v_y$, is zero. This corresponds to a gradient that is purely horizontal or purely vertical.\n\nThe numerical difference between $\\mathrm{TV}_{\\mathrm{anisotropic}}(U)$ and $\\mathrm{TV}_{\\mathrm{isotropic}}(U)$ arises from the sum of the differences $\\|\\nabla U_{i,j}\\|_1 - \\|\\nabla U_{i,j}\\|_2$ over all pixels. This difference is non-zero only at pixels where both components of the gradient vector are non-zero.\nExamining our computed gradients, only the pixel at $(2,1)$ has a gradient vector with two non-zero components: $\\nabla U_{2,1} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$. For all other pixels, the gradient vector has at least one zero component, meaning $\\|\\nabla U_{i,j}\\|_1 = \\|\\nabla U_{i,j}\\|_2$ for all $(i,j) \\neq (2,1)$.\nThe structural source of the difference is therefore located entirely at pixel $(2,1)$, which represents a corner or a point where the image intensity changes in both the horizontal and vertical directions simultaneously.\n\nFinally, we compute the exact difference $D$:\n$$\nD = \\mathrm{TV}_{\\mathrm{anisotropic}}(U) - \\mathrm{TV}_{\\mathrm{isotropic}}(U)\n$$\nThis difference is the sum of the term-by-term differences, which simplifies to the difference at the single contributing pixel:\n$$\nD = (\\|\\nabla U_{2,1}\\|_1 - \\|\\nabla U_{2,1}\\|_2) = (|2|+|1|) - \\sqrt{2^2+1^2} = 3 - \\sqrt{5}\n$$\nAlternatively, using the total computed values:\n$$\nD = 15 - (12 + \\sqrt{5}) = 3 - \\sqrt{5}\n$$\nThe result is consistent.", "answer": "$$\n\\boxed{3 - \\sqrt{5}}\n$$", "id": "3491268"}, {"introduction": "Having established the definition of TV, we now explore *why* it is so effective for denoising. This problem [@problem_id:3491235] simplifies the classic Rudin–Osher–Fatemi (ROF) model to a one-dimensional, two-pixel signal, allowing for a full analytical solution. By deriving the minimizer from first principles, you will uncover the core mechanism of TV regularization: it acts as a soft-thresholding operator on the signal's gradient, effectively eliminating small variations (noise) while preserving large ones (edges).", "problem": "Consider the Rudin–Osher–Fatemi (ROF) model for image denoising, which seeks a signal $u$ that balances fidelity to a given noisy observation $f$ with a regularization that penalizes the total variation. In discrete one-dimensional form with two pixels, the ROF energy is defined by\n$$\nE(u) \\;=\\; \\frac{1}{2}\\sum_{i=1}^{2} (u_{i} - f_{i})^{2} \\;+\\; \\lambda\\,\\mathrm{TV}(u),\n$$\nwhere $\\lambda>0$ is the regularization parameter and $\\mathrm{TV}(u)$ denotes the total variation. For anisotropic total variation in one dimension, the total variation is the sum of absolute forward differences, so for two pixels it reduces to\n$$\n\\mathrm{TV}(u) \\;=\\; |u_{2} - u_{1}|.\n$$\nLet the observed two-pixel signal be $f=(a,b)$, where $a \\in \\mathbb{R}$ and $b \\in \\mathbb{R}$. Starting from the definitions above and using first principles of convex optimization and subgradient optimality, derive the exact minimizer $u^{\\star}=(u_{1}^{\\star},u_{2}^{\\star})$ of $E(u)$ as a closed-form analytic expression in terms of $a$, $b$, and $\\lambda$. Furthermore, determine the explicit threshold on $|a-b|$ at which the minimizer becomes piecewise constant, meaning $u_{1}^{\\star} = u_{2}^{\\star}$.\n\nExpress your final answers as a single row vector containing three entries $(u_{1}^{\\star},\\,u_{2}^{\\star},\\,T)$, where $T$ is the threshold in terms of $\\lambda$. No numerical evaluation is required.", "solution": "The user has provided a problem statement for validation and solution.\n\n### Step 1: Extract Givens\n- **Model**: Rudin–Osher–Fatemi (ROF) model for a $1$-dimensional signal with $2$ pixels.\n- **Energy Function**: $E(u) = \\frac{1}{2}\\sum_{i=1}^{2} (u_{i} - f_{i})^{2} + \\lambda\\,\\mathrm{TV}(u)$.\n- **Signal**: $u = (u_1, u_2)$, where $u_1, u_2 \\in \\mathbb{R}$.\n- **Observation**: $f = (a, b)$, where $a, b \\in \\mathbb{R}$.\n- **Regularization Parameter**: $\\lambda > 0$.\n- **Total Variation (TV)**: $\\mathrm{TV}(u) = |u_{2} - u_{1}|$.\n- **Objective**:\n    1. Find the exact minimizer $u^{\\star}=(u_{1}^{\\star},u_{2}^{\\star})$ of $E(u)$ as a closed-form analytic expression in terms of $a$, $b$, and $\\lambda$.\n    2. Determine the explicit threshold $T$ on $|a-b|$ at which the minimizer becomes piecewise constant, i.e., $u_{1}^{\\star} = u_{2}^{\\star}$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is based on the Rudin–Osher–Fatemi (ROF) model, a fundamental and widely-used model in image processing, inverse problems, and sparse optimization. The total variation (TV) definition is standard for the $1$-dimensional anisotropic case. The problem is a correct and well-established application of convex optimization principles. It is not based on any false premises or pseudoscience.\n2.  **Well-Posed**: The energy function $E(u)$ is the sum of two convex functions: a strictly convex quadratic term, $\\frac{1}{2}\\|u-f\\|_2^2$, and a convex regularization term, $\\lambda|u_2-u_1|$. The sum, $E(u)$, is therefore strictly convex. A strictly convex function has a unique minimizer. Thus, a unique, stable, and meaningful solution exists.\n3.  **Objective**: The problem is stated using precise mathematical language, free of ambiguity, subjectivity, or opinion.\n4.  **Completeness and Consistency**: All necessary variables ($a, b, \\lambda$) and definitions are provided. The problem is self-contained and free of contradictions.\n5.  **Relevance**: The problem is directly relevant to the topic of total variation for image denoising, which is a core concept in compressed sensing and sparse optimization.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with a full, reasoned solution.\n\nThe energy function to be minimized is given by:\n$$\nE(u_1, u_2) = \\frac{1}{2}((u_1 - a)^2 + (u_2 - b)^2) + \\lambda|u_2 - u_1|\n$$\nThis function is convex, but not everywhere differentiable due to the absolute value term. The unique minimizer $u^{\\star}=(u_1^{\\star}, u_2^{\\star})$ can be found by setting the subgradient of $E$ to zero. The subgradient optimality condition is $0 \\in \\partial E(u^{\\star})$.\n\nThe subgradient of $E$ with respect to the vector $u=(u_1, u_2)$ is the sum of the gradient of the differentiable part and the subgradient of the non-differentiable part.\nThe gradient of the fidelity term $F(u_1, u_2) = \\frac{1}{2}((u_1 - a)^2 + (u_2 - b)^2)$ is:\n$$\n\\nabla F(u_1, u_2) = \\begin{pmatrix} u_1 - a \\\\ u_2 - b \\end{pmatrix}\n$$\nThe subgradient of the total variation term $R(u_1, u_2) = \\lambda|u_2 - u_1|$ is:\n$$\n\\partial R(u_1, u_2) = \\lambda \\partial |u_2 - u_1| = \\lambda \\begin{pmatrix} -\\partial_z |z| \\\\ \\partial_z |z| \\end{pmatrix}_{z=u_2-u_1} = \\lambda \\begin{pmatrix} -\\mathrm{sgn}(u_2-u_1) \\\\ \\mathrm{sgn}(u_2-u_1) \\end{pmatrix}\n$$\nwhere $\\mathrm{sgn}(z)$ is the set-valued signum function:\n$$\n\\mathrm{sgn}(z) = \\begin{cases} \\{1\\} & \\text{if } z > 0 \\\\ \\{-1\\} & \\text{if } z < 0 \\\\ [-1, 1] & \\text{if } z = 0 \\end{cases}\n$$\nThe optimality condition $0 \\in \\partial E(u_1, u_2) = \\nabla F(u_1, u_2) + \\partial R(u_1, u_2)$ implies that there exists a scalar $s \\in \\mathrm{sgn}(u_2 - u_1)$ such that:\n$$\n\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} u_1 - a \\\\ u_2 - b \\end{pmatrix} + \\lambda \\begin{pmatrix} -s \\\\ s \\end{pmatrix}\n$$\nThis gives us a system of two equations:\n1. $u_1 - a - \\lambda s = 0 \\implies u_1 = a + \\lambda s$\n2. $u_2 - b + \\lambda s = 0 \\implies u_2 = b - \\lambda s$\n\nThe value of $s$ depends on the sign of $u_2 - u_1$. Let's analyze the cases.\n\n**Case 1: $u_2 - u_1 > 0$**\nIn this case, $s = \\mathrm{sgn}(u_2 - u_1) = 1$. Substituting $s=1$ into the equations for $u_1$ and $u_2$:\n$u_1^{\\star} = a + \\lambda$\n$u_2^{\\star} = b - \\lambda$\nFor this solution to be valid, the initial assumption $u_2 - u_1 > 0$ must hold:\n$(b - \\lambda) - (a + \\lambda) > 0 \\implies b - a - 2\\lambda > 0 \\implies b - a > 2\\lambda$.\n\n**Case 2: $u_2 - u_1 < 0$**\nIn this case, $s = \\mathrm{sgn}(u_2 - u_1) = -1$. Substituting $s=-1$:\n$u_1^{\\star} = a - \\lambda$\n$u_2^{\\star} = b + \\lambda$\nThe initial assumption $u_2 - u_1 < 0$ must hold:\n$(b + \\lambda) - (a - \\lambda) < 0 \\implies b - a + 2\\lambda < 0 \\implies a - b > 2\\lambda$.\n\n**Case 3: $u_2 - u_1 = 0$**\nIn this case, $s \\in [-1, 1]$. The condition $u_1=u_2$ implies that the solution is piecewise constant. Let $u_1^{\\star} = u_2^{\\star} = c$. The energy function becomes:\n$E(c, c) = \\frac{1}{2}((c - a)^2 + (c - b)^2)$.\nTo find the minimum, we differentiate with respect to $c$ and set to zero:\n$\\frac{dE}{dc} = (c-a) + (c-b) = 2c - (a+b) = 0 \\implies c = \\frac{a+b}{2}$.\nSo, if $u_1^{\\star} = u_2^{\\star}$, then $u_1^{\\star} = u_2^{\\star} = \\frac{a+b}{2}$.\nNow we check for which values of $a$ and $b$ this solution is valid. We need to find an $s \\in [-1, 1]$ that satisfies the optimality equations:\n$u_1 = a + \\lambda s \\implies \\frac{a+b}{2} = a + \\lambda s \\implies \\lambda s = \\frac{b-a}{2} \\implies s = \\frac{b-a}{2\\lambda}$.\nThe condition $s \\in [-1, 1]$ becomes:\n$-1 \\le \\frac{b-a}{2\\lambda} \\le 1 \\implies -2\\lambda \\le b-a \\le 2\\lambda \\implies |a-b| \\le 2\\lambda$.\n\nSummary of the minimizer $u^{\\star}=(u_{1}^{\\star},u_{2}^{\\star})$:\n- If $a-b > 2\\lambda$, then $u_1^{\\star} = a - \\lambda$ and $u_2^{\\star} = b + \\lambda$.\n- If $b-a > 2\\lambda$, then $u_1^{\\star} = a + \\lambda$ and $u_2^{\\star} = b - \\lambda$.\n- If $|a-b| \\le 2\\lambda$, then $u_1^{\\star} = u_2^{\\star} = \\frac{a+b}{2}$.\n\nThe minimizer becomes piecewise constant ($u_1^{\\star} = u_2^{\\star}$) if and only if $|a-b| \\le 2\\lambda$. The problem asks for the explicit threshold on $|a-b|$ at which this transition occurs. This threshold is the boundary of the region, which is $T = 2\\lambda$.\n\nTo express the solution in a single closed form, we can use the soft-thresholding function, $\\mathrm{soft}(z, \\gamma) = \\operatorname{sgn}(z) \\max(0, |z|-\\gamma)$.\nFrom our optimality conditions, we have $u_1^{\\star}+u_2^{\\star} = a+b$ and $u_1^{\\star}-u_2^{\\star} = a-b+2\\lambda s$. The case analysis is equivalent to finding the value of $u_1^{\\star}-u_2^{\\star}$. This is a classic proximal operator problem. The solution for $d' = u_1^{\\star}-u_2^{\\star}$ is the solution to minimizing $\\frac{1}{2}(d' - (a-b))^2 + 2\\lambda|d'|$, which is $d' = \\mathrm{soft}(a-b, 2\\lambda)$.\nSo, we have:\n$u_1^{\\star} + u_2^{\\star} = a+b$\n$u_1^{\\star} - u_2^{\\star} = \\operatorname{sgn}(a-b)\\max(|a-b|-2\\lambda, 0)$\nSolving this system for $u_1^{\\star}$ and $u_2^{\\star}$:\n$u_1^{\\star} = \\frac{1}{2} \\left( (a+b) + \\operatorname{sgn}(a-b)\\max(|a-b|-2\\lambda, 0) \\right)$\n$u_2^{\\star} = \\frac{1}{2} \\left( (a+b) - \\operatorname{sgn}(a-b)\\max(|a-b|-2\\lambda, 0) \\right)$\n\nThe three required quantities are $u_1^{\\star}$, $u_2^{\\star}$, and the threshold $T=2\\lambda$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{2}\\left(a+b + \\operatorname{sgn}(a-b)\\max(|a-b|-2\\lambda, 0)\\right) & \\frac{1}{2}\\left(a+b - \\operatorname{sgn}(a-b)\\max(|a-b|-2\\lambda, 0)\\right) & 2\\lambda \\end{pmatrix}}\n$$", "id": "3491235"}, {"introduction": "While analytical solutions are insightful, most real-world TV optimization problems require iterative algorithms. This practice problem [@problem_id:3491252] provides a window into this computational reality by guiding you through one full iteration of the Alternating Direction Method of Multipliers (ADMM). Tackling a TV-regularized deblurring problem, you will see how ADMM decomposes a complex task into a sequence of simpler subproblems—a least-squares inversion and a proximal mapping—which is the key to its power and widespread use.", "problem": "Consider a one-dimensional length-$3$ signal denoising and deblurring problem with Total Variation (TV) regularization. The forward model is a blur operator acting on a signal $u \\in \\mathbb{R}^{3}$, defined by a circular-averaging blur over a three-point window. Under periodic boundary conditions on a length-$3$ grid, the blur matrix is $K = \\frac{1}{3} J$, where $J \\in \\mathbb{R}^{3 \\times 3}$ is the all-ones matrix. The observed data is $f \\in \\mathbb{R}^{3}$, and the reconstruction solves the convex optimization problem\n$$\n\\min_{u \\in \\mathbb{R}^{3}} \\ \\frac{1}{2} \\| K u - f \\|_{2}^{2} + \\lambda \\| D u \\|_{1},\n$$\nwhere $D \\in \\mathbb{R}^{3 \\times 3}$ is the discrete forward difference operator with periodic boundary conditions,\n$$\nD = \\begin{pmatrix}\n-1 & 1 & 0 \\\\\n0 & -1 & 1 \\\\\n1 & 0 & -1\n\\end{pmatrix},\n$$\nand $\\| D u \\|_{1}$ is the anisotropic TV seminorm.\n\nIntroduce the auxiliary variable $w \\in \\mathbb{R}^{3}$ to split $\\| D u \\|_{1}$ as $\\| w \\|_{1}$ with the constraint $w = D u$. Consider the Alternating Direction Method of Multipliers (ADMM) in its scaled form with scaled dual variable $y \\in \\mathbb{R}^{3}$ and penalty parameter $\\rho > 0$. The scaled ADMM iteration alternates between minimizing the augmented Lagrangian with respect to $u$ and $w$, followed by a dual ascent on $y$.\n\nTake the specific instance with\n- data $f = \\begin{pmatrix} 1 \\\\ 1 \\\\ 4 \\end{pmatrix}$,\n- regularization parameter $\\lambda = \\frac{1}{2}$,\n- penalty parameter $\\rho = 1$,\n- initial iterates $u^{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$, $w^{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$, $y^{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\nStarting from these initial iterates, perform one full ADMM iteration to compute the updated $u^{1}$ and $w^{1}$. In the $w$-update, the proximal operator should be applied componentwise as the soft-thresholding mapping $S_{\\tau}(x) = \\operatorname{sign}(x) \\max\\{ |x| - \\tau, 0 \\}$.\n\nExpress your final answer as a single row vector formed by concatenating $u^{1}$ followed by $w^{1}$, in the order\n$$\n\\begin{pmatrix}\nu^{1}_{1} & u^{1}_{2} & u^{1}_{3} & w^{1}_{1} & w^{1}_{2} & w^{1}_{3}\n\\end{pmatrix}.\n$$\nNo rounding is required and no units are involved. Your answer must be exact.", "solution": "The problem asks to perform one full iteration of the Alternating Direction Method of Multipliers (ADMM) for a specific one-dimensional total variation (TV) regularized deblurring problem. The optimization problem is given by\n$$ \\min_{u \\in \\mathbb{R}^{3}} \\ \\frac{1}{2} \\| K u - f \\|_{2}^{2} + \\lambda \\| D u \\|_{1} $$\nWe introduce an auxiliary variable $w = Du$ to split the problem as\n$$ \\min_{u, w} \\ \\frac{1}{2} \\| K u - f \\|_{2}^{2} + \\lambda \\| w \\|_{1} \\quad \\text{subject to} \\quad Du - w = 0 $$\nThe scaled augmented Lagrangian $L_{\\rho}(u, w, y)$ for this problem is\n$$ L_{\\rho}(u, w, y) = \\frac{1}{2} \\| K u - f \\|_{2}^{2} + \\lambda \\| w \\|_{1} + \\frac{\\rho}{2} \\| Du - w + y \\|_{2}^{2} $$\nwhere terms not dependent on $u$ or $w$ are omitted. An ADMM iteration consists of three steps:\n1. $u$-update: $u^{k+1} = \\arg\\min_{u} L_{\\rho}(u, w^k, y^k)$\n2. $w$-update: $w^{k+1} = \\arg\\min_{w} L_{\\rho}(u^{k+1}, w, y^k)$\n3. $y$-update: $y^{k+1} = y^k + Du^{k+1} - w^{k+1}$\n\nWe are given the initial iterates $u^{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$, $w^{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$, and $y^{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$. We will compute $u^1$ and $w^1$ using the given parameters $\\lambda = \\frac{1}{2}$ and $\\rho = 1$.\n\n**Step 1: The $u$-update**\nWe compute $u^1$ by minimizing the Lagrangian with respect to $u$, holding $w$ and $y$ at their $k=0$ values:\n$$ u^1 = \\arg\\min_{u} \\left( \\frac{1}{2} \\| K u - f \\|_{2}^{2} + \\frac{\\rho}{2} \\| Du - w^0 + y^0 \\|_{2}^{2} \\right) $$\nSubstituting $w^0=0$ and $y^0=0$:\n$$ u^1 = \\arg\\min_{u} \\left( \\frac{1}{2} \\| K u - f \\|_{2}^{2} + \\frac{\\rho}{2} \\| Du \\|_{2}^{2} \\right) $$\nThis is a standard least-squares problem. The objective function is quadratic in $u$. To find the minimum, we set the gradient with respect to $u$ to zero:\n$$ \\nabla_u \\left( \\frac{1}{2} (Ku-f)^T(Ku-f) + \\frac{\\rho}{2} (Du)^T(Du) \\right) = 0 $$\n$$ K^T(Ku - f) + \\rho D^T D u = 0 $$\nThis gives the normal equations:\n$$ (K^T K + \\rho D^T D) u^1 = K^T f $$\nWe need to compute the matrices and vectors involved.\nThe blur matrix is $K = \\frac{1}{3} J = \\frac{1}{3}\\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{pmatrix}$.\nThus, $K^T=K$. We compute $K^T K = K^2 = \\left(\\frac{1}{3}J\\right)^2 = \\frac{1}{9}J^2$. For a $3 \\times 3$ all-ones matrix, $J^2=3J$. Therefore, $K^T K = \\frac{1}{9}(3J) = \\frac{1}{3}J = K$.\nThe forward difference matrix is $D = \\begin{pmatrix} -1 & 1 & 0 \\\\ 0 & -1 & 1 \\\\ 1 & 0 & -1 \\end{pmatrix}$. Its transpose is $D^T = \\begin{pmatrix} -1 & 0 & 1 \\\\ 1 & -1 & 0 \\\\ 0 & 1 & -1 \\end{pmatrix}$.\nThe product $D^T D$ is:\n$$ D^T D = \\begin{pmatrix} -1 & 0 & 1 \\\\ 1 & -1 & 0 \\\\ 0 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} -1 & 1 & 0 \\\\ 0 & -1 & 1 \\\\ 1 & 0 & -1 \\end{pmatrix} = \\begin{pmatrix} 2 & -1 & -1 \\\\ -1 & 2 & -1 \\\\ -1 & -1 & 2 \\end{pmatrix} $$\nWith $\\rho=1$, the system matrix is $A = K^T K + \\rho D^T D$:\n$$ A = \\frac{1}{3}\\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{pmatrix} + \\begin{pmatrix} 2 & -1 & -1 \\\\ -1 & 2 & -1 \\\\ -1 & -1 & 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{3} & -\\frac{2}{3} & -\\frac{2}{3} \\\\ -\\frac{2}{3} & \\frac{7}{3} & -\\frac{2}{3} \\\\ -\\frac{2}{3} & -\\frac{2}{3} & \\frac{7}{3} \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 7 & -2 & -2 \\\\ -2 & 7 & -2 \\\\ -2 & -2 & 7 \\end{pmatrix} $$\nThe right-hand side is $b = K^T f = K f$. With $f = \\begin{pmatrix} 1 \\\\ 1 \\\\ 4 \\end{pmatrix}$:\n$$ b = \\frac{1}{3}\\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 4 \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 1+1+4 \\\\ 1+1+4 \\\\ 1+1+4 \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 6 \\\\ 6 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix} $$\nWe solve the linear system $A u^1 = b$:\n$$ \\frac{1}{3}\\begin{pmatrix} 7 & -2 & -2 \\\\ -2 & 7 & -2 \\\\ -2 & -2 & 7 \\end{pmatrix} u^1 = \\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix} $$\nBy inspection, let's test a constant solution $u^1 = \\begin{pmatrix} c \\\\ c \\\\ c \\end{pmatrix}$. The left side becomes:\n$$ \\frac{1}{3}\\begin{pmatrix} 7c - 2c - 2c \\\\ -2c + 7c - 2c \\\\ -2c - 2c + 7c \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 3c \\\\ 3c \\\\ 3c \\end{pmatrix} = \\begin{pmatrix} c \\\\ c \\\\ c \\end{pmatrix} $$\nEquating this to the right side $\\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix}$, we find $c=2$.\nThus, the solution is $u^1 = \\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix}$.\n\n**Step 2: The $w$-update**\nWe compute $w^1$ by minimizing the Lagrangian with respect to $w$, using the newly computed $u^1$:\n$$ w^1 = \\arg\\min_{w} \\left( \\lambda \\| w \\|_{1} + \\frac{\\rho}{2} \\| Du^1 - w + y^0 \\|_{2}^{2} \\right) $$\nSubstituting $y^0=0$, the problem is equivalent to finding the proximal operator of the $\\ell_1$-norm:\n$$ w^1 = \\arg\\min_{w} \\left( \\lambda \\| w \\|_{1} + \\frac{\\rho}{2} \\| w - Du^1 \\|_{2}^{2} \\right) = \\text{prox}_{\\frac{\\lambda}{\\rho}\\|\\cdot\\|_1}(Du^1) $$\nThis proximal operator is the soft-thresholding function $S_{\\tau}(x)$ applied element-wise, with the threshold $\\tau = \\frac{\\lambda}{\\rho}$.\nGiven $\\lambda = \\frac{1}{2}$ and $\\rho = 1$, the threshold is $\\tau = \\frac{1/2}{1} = \\frac{1}{2}$.\nFirst, we compute the argument $Du^1$:\n$$ Du^1 = \\begin{pmatrix} -1 & 1 & 0 \\\\ 0 & -1 & 1 \\\\ 1 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} -2+2 \\\\ -2+2 \\\\ 2-2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nNow we apply the soft-thresholding operator $S_{1/2}$ to each component of $Du^1$:\n$$ w^1_i = S_{1/2}((Du^1)_i) = \\text{sign}((Du^1)_i) \\max\\{ |(Du^1)_i| - \\frac{1}{2}, 0 \\} $$\nFor each component, $(Du^1)_i=0$, so:\n$$ w^1_i = \\text{sign}(0) \\max\\{ |0| - \\frac{1}{2}, 0 \\} = 0 \\cdot 0 = 0 $$\nTherefore, $w^1 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\n**Conclusion of the first iteration**\nAfter one full ADMM iteration, the updated variables are:\n$u^1 = \\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix}$ and $w^1 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\nThe problem asks for the final answer as a single row vector concatenating the components of $u^1$ and $w^1$.\nThe vector is $\\begin{pmatrix} u^1_1 & u^1_2 & u^1_3 & w^1_1 & w^1_2 & w^1_3 \\end{pmatrix}$.\nSubstituting the computed values, we get $\\begin{pmatrix} 2 & 2 & 2 & 0 & 0 & 0 \\end{pmatrix}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2 & 2 & 2 & 0 & 0 & 0\n\\end{pmatrix}\n}\n$$", "id": "3491252"}]}