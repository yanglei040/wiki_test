{"hands_on_practices": [{"introduction": "To truly grasp the mechanics of primal-dual methods, there is no substitute for working through a problem by hand. This exercise strips away the complexity of large-scale implementations and focuses on the core optimality conditions for a small, one-dimensional Total Variation (TV) denoising problem. By explicitly calculating the primal and dual solutions from first principles, you will build a concrete intuition for the primal-dual relationship and how the dual variable encodes the structure of the optimal solution. [@problem_id:3466896]", "problem": "Consider the one-dimensional Rudin-Osher-Fatemi (ROF) total variation denoising problem on a grid of length $n=5$. Let the discrete forward-difference operator $K:\\mathbb{R}^{5}\\to\\mathbb{R}^{4}$ be defined by $(Kx)_{i}=x_{i+1}-x_{i}$ for $i\\in\\{1,2,3,4\\}$, and let $K^{\\top}$ denote its adjoint, which satisfies $K^{\\top}p=\\left[-p_{1},\\,p_{1}-p_{2},\\,p_{2}-p_{3},\\,p_{3}-p_{4},\\,p_{4}\\right]^{\\top}$ for any $p\\in\\mathbb{R}^{4}$. Given the data vector $b\\in\\mathbb{R}^{5}$ defined by\n$$\nb=\\begin{pmatrix}1\\\\2\\\\-1\\\\0\\\\0\\end{pmatrix},\n$$\nand the regularization parameter $\\lambda=1$, consider the convex optimization problem\n$$\n\\min_{x\\in\\mathbb{R}^{5}}\\;\\frac{1}{2}\\|x-b\\|_{2}^{2}+\\lambda\\|Kx\\|_{1}.\n$$\nUsing only fundamental principles of convex analysis and duality (for example, the definitions of convex conjugates and subgradients), and without invoking any pre-packaged algorithm, do the following:\n- Derive the necessary and sufficient optimality conditions for a primal-dual pair $(x^{\\star},p^{\\star})$ associated with the above problem, where the dual variable $p$ must satisfy the box constraint $\\|p\\|_{\\infty}\\leq \\lambda$.\n- Compute explicitly the unique primal solution $x^{\\star}$ and a dual solution $p^{\\star}$ that satisfy those conditions.\n- Verify the primal-dual relation $x^{\\star}=b-K^{\\top}p^{\\star}$.\n\nReport your final answer as a single row matrix whose first five entries are the components of $x^{\\star}$ in order, and whose next four entries are the components of $p^{\\star}$ in order. No rounding is required, and no units are involved. The final answer must be a single matrix expression as specified above.", "solution": "The problem is to solve the one-dimensional Rudin-Osher-Fatemi (ROF) total variation denoising problem for a given data vector.\n\n### Step 1: Extract Givens\n- Grid length: $n=5$.\n- Primal variable: $x \\in \\mathbb{R}^{5}$.\n- Data vector: $b=\\begin{pmatrix}1\\\\2\\\\-1\\\\0\\\\0\\end{pmatrix} \\in \\mathbb{R}^{5}$.\n- Regularization parameter: $\\lambda=1$.\n- Discrete forward-difference operator $K:\\mathbb{R}^{5}\\to\\mathbb{R}^{4}$ defined by $(Kx)_{i}=x_{i+1}-x_{i}$ for $i\\in\\{1,2,3,4\\}$.\n- Adjoint operator $K^{\\top}:\\mathbb{R}^{4}\\to\\mathbb{R}^{5}$ defined by $K^{\\top}p=\\left(-p_{1},\\,p_{1}-p_{2},\\,p_{2}-p_{3},\\,p_{3}-p_{4},\\,p_{4}\\right)^{\\top}$ for $p\\in\\mathbb{R}^{4}$.\n- Optimization problem: $\\min_{x\\in\\mathbb{R}^{5}}\\;\\frac{1}{2}\\|x-b\\|_{2}^{2}+\\lambda\\|Kx\\|_{1}$.\n- Dual variable constraint: $\\|p\\|_{\\infty}\\leq \\lambda$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. It is a standard instance of the ROF model, a well-established method in signal processing and inverse problems. The objective function is the sum of a strictly convex, differentiable term ($\\frac{1}{2}\\|x-b\\|_{2}^{2}$) and a convex, non-differentiable term ($\\lambda\\|Kx\\|_{1}$). This structure ensures the existence of a unique primal solution $x^{\\star}$. The problem is well-posed, self-contained, and all terms are precisely defined. The provided definitions for the operator $K$ and its adjoint $K^{\\top}$ are consistent with standard finite difference operators on a discrete grid. The problem is valid.\n\n### Step 3: Proceed to Solution\n\nThe optimization problem can be written as:\n$$\n\\min_{x\\in\\mathbb{R}^{5}} f(x) + g(Kx)\n$$\nwhere $f(x) = \\frac{1}{2}\\|x-b\\|_{2}^{2}$ and $g(z) = \\lambda\\|z\\|_{1}$. Both $f(x)$ and $g(z)$ are convex functions. Since $f(x)$ is strictly convex, the primal problem has a unique solution $x^\\star$.\n\nThe necessary and sufficient condition for $x^\\star$ to be the minimizer is that the zero vector must belong to the subdifferential of the objective function evaluated at $x^\\star$. Using the sum rule and the chain rule for subdifferentials:\n$$\n0 \\in \\partial \\left( f(x^\\star) + g(Kx^\\star) \\right) = \\nabla f(x^\\star) + K^{\\top} \\partial g(Kx^\\star)\n$$\nThe gradient of $f(x)$ is $\\nabla f(x) = x-b$.\nThe subdifferential of $g(z) = \\lambda \\|z\\|_{1}$ at a point $z_0=Kx^\\star$ is the set $\\partial g(z_0) = \\{ p \\in \\mathbb{R}^4 \\mid p_i \\in \\lambda \\partial|z_{0,i}| \\}$. The subgradient of the absolute value function is $\\text{sign}(t)$ for $t\\ne 0$ and the interval $[-1, 1]$ for $t=0$.\n\nThe optimality condition becomes:\n$$\n0 \\in (x^\\star - b) + K^{\\top} \\left( \\lambda \\partial \\|Kx^\\star\\|_{1} \\right)\n$$\nThis means there exists a dual vector $p^\\star \\in \\mathbb{R}^4$ such that:\n$$\np^\\star \\in \\lambda \\partial \\|Kx^\\star\\|_{1} \\quad \\text{and} \\quad x^\\star - b + K^{\\top} p^\\star = 0\n$$\nThe first condition, $p^\\star \\in \\lambda \\partial \\|Kx^\\star\\|_{1}$, is equivalent to the following for each component $i \\in \\{1,2,3,4\\}$:\n1. $|p^\\star_i| \\leq \\lambda$. This is the box constraint $\\|p^\\star\\|_{\\infty} \\leq \\lambda$.\n2. If $(Kx^\\star)_i \\neq 0$, then $p^\\star_i = \\lambda \\cdot \\text{sign}((Kx^\\star)_i)$.\nThis is equivalent to the complementary slackness condition $\\langle p^\\star, Kx^\\star \\rangle = \\lambda \\|Kx^\\star\\|_1$.\n\nThe second condition is the primal-dual relationship:\n$$\nx^\\star = b - K^{\\top}p^\\star\n$$\nWith $\\lambda=1$, the complete set of optimality conditions for a pair $(x^\\star, p^\\star)$ is:\n(A) $x^\\star = b - K^{\\top}p^\\star$\n(B) $\\|p^\\star\\|_{\\infty} \\leq 1$\n(C) For $i \\in \\{1,2,3,4\\}$, $p^\\star_i = \\text{sign}((Kx^\\star)_i)$ if $(Kx^\\star)_i \\neq 0$.\n\nNow, we must find a pair $(x^\\star, p^\\star)$ that satisfies these conditions. We can substitute (A) into (C) to solve for $p^\\star$.\n$$\nKx^\\star = K(b - K^{\\top}p^\\star) = Kb - (KK^{\\top})p^\\star\n$$\nLet's compute the matrices $Kb$ and $KK^{\\top}$.\n$$\nKb = \\begin{pmatrix} b_2 - b_1 \\\\ b_3 - b_2 \\\\ b_4 - b_3 \\\\ b_5 - b_4 \\end{pmatrix} = \\begin{pmatrix} 2 - 1 \\\\ -1 - 2 \\\\ 0 - (-1) \\\\ 0 - 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -3 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nThe matrix $L = KK^{\\top}$ is a $4 \\times 4$ matrix. Its entries are $L_{ij} = \\langle K_i, K_j \\rangle$ where $K_i$ are the rows of $K$. One can compute it directly:\n$$\nKK^{\\top} = \\begin{pmatrix}\n2  -1  0  0 \\\\\n-1  2  -1  0 \\\\\n0  -1  2  -1 \\\\\n0  0  -1  2\n\\end{pmatrix}\n$$\nLet $z^\\star = Kx^\\star$. The conditions become finding $p^\\star$ such that $\\|p^\\star\\|_\\infty \\le 1$ and for $i=1,2,3,4$:\n- If $z^\\star_i = (Kb - Lp^\\star)_i  0$, then $p^\\star_i = 1$.\n- If $z^\\star_i = (Kb - Lp^\\star)_i  0$, then $p^\\star_i = -1$.\n- If $z^\\star_i = (Kb - Lp^\\star)_i = 0$, then $|p^\\star_i| \\le 1$.\n\nTotal variation regularization encourages piecewise-constant solutions. Let us hypothesize that the solution $x^\\star$ has one jump, located between $x_2^\\star$ and $x_3^\\star$. This is suggested by the large drop in value in $b$ between $b_2=2$ and $b_3=-1$.\nThis hypothesis implies:\n$$\n(Kx^\\star)_1 = x_2^\\star-x_1^\\star = 0\n$$\n$$\n(Kx^\\star)_2 = x_3^\\star-x_2^\\star \\neq 0\n$$\n$$\n(Kx^\\star)_3 = x_4^\\star-x_3^\\star = 0\n$$\n$$\n(Kx^\\star)_4 = x_5^\\star-x_4^\\star = 0\n$$\nFrom condition (C), this structure implies for the dual variable $p^\\star$:\n- $z^\\star_1 = 0 \\implies (Kb - Lp^\\star)_1 = 0 \\implies 1 - (2p^\\star_1 - p^\\star_2) = 0$.\n- $z^\\star_2 \\ne 0 \\implies p^\\star_2 = \\text{sign}(z^\\star_2)$, so $p^\\star_2 = 1$ or $p^\\star_2 = -1$.\n- $z^\\star_3 = 0 \\implies (Kb - Lp^\\star)_3 = 0 \\implies 1 - (-p^\\star_2 + 2p^\\star_3 - p^\\star_4) = 0$.\n- $z^\\star_4 = 0 \\implies (Kb - Lp^\\star)_4 = 0 \\implies 0 - (-p^\\star_3 + 2p^\\star_4) = 0 \\implies p^\\star_3 = 2p^\\star_4$.\n\nFrom $p^\\star_3 = 2p^\\star_4$ and the constraint $|p_i^\\star| \\le 1$, we must have $|2p^\\star_4| \\le 1$, so $|p^\\star_4| \\le 1/2$.\n\nLet's test the two cases for $p^\\star_2$:\nCase 1: Assume $p^\\star_2 = 1$.\n- From $1 - (2p^\\star_1 - p^\\star_2) = 0 \\implies 1 - (2p^\\star_1 - 1) = 0 \\implies 2p^\\star_1=2 \\implies p^\\star_1 = 1$.\n- From $1 - (-p^\\star_2 + 2p^\\star_3 - p^\\star_4) = 0 \\implies 1 - (-1 + 2p^\\star_3 - p^\\star_4) = 0 \\implies 2 - 2p^\\star_3 + p^\\star_4 = 0$.\n- Substitute $p^\\star_3 = 2p^\\star_4$: $2 - 2(2p^\\star_4) + p^\\star_4 = 0 \\implies 2 - 3p^\\star_4 = 0 \\implies p^\\star_4 = 2/3$.\n- This violates the condition $|p^\\star_4| \\le 1/2$. Thus, $p^\\star_2=1$ is not a valid solution.\n\nCase 2: Assume $p^\\star_2 = -1$.\n- From $1 - (2p^\\star_1 - p^\\star_2) = 0 \\implies 1 - (2p^\\star_1 - (-1)) = 0 \\implies 1 - 2p^\\star_1 - 1 = 0 \\implies p^\\star_1 = 0$.\n- From $1 - (-p^\\star_2 + 2p^\\star_3 - p^\\star_4) = 0 \\implies 1 - (-(-1) + 2p^\\star_3 - p^\\star_4) = 0 \\implies -2p^\\star_3 + p^\\star_4 = 0$.\n- Substitute $p^\\star_3 = 2p^\\star_4$: $-2(2p^\\star_4) + p^\\star_4 = 0 \\implies -3p^\\star_4 = 0 \\implies p^\\star_4 = 0$.\n- From $p^\\star_3 = 2p^\\star_4$, we get $p^\\star_3 = 0$.\n- The resulting candidate for the dual solution is $p^\\star = (0, -1, 0, 0)^{\\top}$. All components satisfy $|p_i^\\star| \\le 1$.\n\nWe must verify the consistency of this solution. We assumed $z^\\star_2 \\ne 0$ and $p^\\star_2 = \\text{sign}(z^\\star_2) = -1$, which requires $z^\\star_2  0$. Let's calculate $z^\\star_2$:\n$$\nz^\\star_2 = (Kb - Lp^\\star)_2 = -3 - (-p^\\star_1 + 2p^\\star_2 - p^\\star_3) = -3 - (0 + 2(-1) - 0) = -3 - (-2) = -1.\n$$\nSince $z^\\star_2 = -1  0$, the condition $p^\\star_2 = -1$ is consistent.\nAll optimality conditions are satisfied. Thus, we have found a valid dual solution $p^\\star = (0, -1, 0, 0)^{\\top}$.\n\nNow we compute the unique primal solution $x^\\star$ using the relation $x^\\star = b - K^{\\top}p^\\star$.\n$$\nK^{\\top}p^\\star = K^{\\top} \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -p^\\star_1 \\\\ p^\\star_1 - p^\\star_2 \\\\ p^\\star_2 - p^\\star_3 \\\\ p^\\star_3 - p^\\star_4 \\\\ p^\\star_4 \\end{pmatrix} = \\begin{pmatrix} -0 \\\\ 0 - (-1) \\\\ -1 - 0 \\\\ 0 - 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nSo, the primal solution is:\n$$\nx^\\star = b - K^{\\top}p^\\star = \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nSo, $x^{\\star} = (1, 1, 0, 0, 0)^{\\top}$.\n\nFinally, we verify the primal-dual relation as requested. With $x^{\\star}=(1, 1, 0, 0, 0)^{\\top}$ and $p^{\\star}=(0, -1, 0, 0)^{\\top}$, we have already computed $b - K^{\\top} p^{\\star}$ and shown it equals $x^{\\star}$. The verification is complete.\n\nThe components of $x^{\\star}$ are $(1, 1, 0, 0, 0)$, and the components of $p^{\\star}$ are $(0, -1, 0, 0)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  1  0  0  0  0  -1  0  0\n\\end{pmatrix}\n}\n$$", "id": "3466896"}, {"introduction": "While TV regularization is powerful, it is well-known for introducing 'staircasing' artifacts, where sloped regions in a signal are approximated by flat pieces. This hands-on coding exercise allows you to observe this phenomenon directly and explore a powerful alternative: second-order Total Generalized Variation (TGV). By implementing primal-dual solvers for both TV and TGV and comparing their performance on a signal containing a linear ramp, you will gain practical insight into the benefits of higher-order regularization for preserving more complex structures. [@problem_id:3478968]", "problem": "You are asked to implement a primal-dual algorithm to denoise a small one-dimensional signal using second-order Total Generalized Variation (TGV) regularization and to compare its ramp bias against first-order Total Variation (TV) denoising. The task must be completed by building on the following fundamental base: convex optimization of a sum of a data fidelity term and a sparsity-inducing regularizer, the definition of the discrete derivative operator on one-dimensional signals, and the proximal operator for a squared error data fidelity. You must not assume any result that is not directly derivable from these bases, and you should derive your algorithmic updates from first principles of convex analysis, duality, and proximal splitting.\n\nConsider a signal with $n$ samples represented as a vector $u \\in \\mathbb{R}^n$. Let $f \\in \\mathbb{R}^n$ denote the observed noisy data. The discrete forward difference operator $D : \\mathbb{R}^n \\to \\mathbb{R}^n$ is defined using a one-sided difference with homogeneous Neumann boundary conditions. The data fidelity term is the squared error $F(u) = \\frac{1}{2}\\lVert u - f \\rVert_2^2$. The first-order Total Variation (TV) regularizer is the anisotropic $\\ell_1$ norm of $D u$. The second-order Total Generalized Variation of order $2$ (TGV$^2$) introduces an auxiliary field $w \\in \\mathbb{R}^n$ and penalizes the anisotropic $\\ell_1$ norms of $D u - w$ and $D w$, with positive weights. Your implementation must derive and use a primal-dual scheme based on these definitions to compute both TV- and TGV$^2$-regularized denoised solutions.\n\nDefine the bias at ramps to be the difference between the estimated slope of the reconstructed signal and the true slope of the underlying noiseless ramp. For each reconstruction, estimate the slope by least squares linear regression over all samples of the reconstructed signal. Compute the absolute ramp bias for TV and TGV$^2$ reconstructions separately and report the difference between these absolute biases, defined as $\\Delta = \\lvert b_{\\mathrm{TV}} \\rvert - \\lvert b_{\\mathrm{TGV}} \\rvert$, where $b_{\\mathrm{TV}}$ and $b_{\\mathrm{TGV}}$ are the signed slope errors with respect to the true slope. A positive value of $\\Delta$ indicates that TGV$^2$ exhibits lower absolute bias than TV.\n\nImplement the algorithms in a single self-contained program that produces results for the following test suite. In all cases, use fixed parameters for the algorithms: number of samples $n = 32$, primal step size $\\tau = 0.1$, dual step size $\\sigma = 0.1$, over-relaxation parameter $\\theta = 1$, number of iterations $N_{\\mathrm{iter}} = 1000$, TV regularization weight $\\lambda_{\\mathrm{TV}} = 0.12$, and TGV$^2$ weights $\\alpha_1 = 0.8$ and $\\alpha_2 = 1.6$. Use a fixed random seed set to $42$ for reproducibility of the noise. All quantities are dimensionless. Angles are not involved. There are no physical units.\n\nTest suite:\n- Case $1$ (happy path): A linear ramp signal $u^\\star[i] = i/(n-1)$ for $i \\in \\{0, \\dots, n-1\\}$, observed with additive independent and identically distributed Gaussian noise of zero mean and standard deviation $\\sigma_{\\mathrm{noise}} = 0.05$. Compute $\\Delta$ as specified.\n- Case $2$ (boundary condition coverage): A constant signal $u^\\star[i] = 0.5$, observed with additive independent and identically distributed Gaussian noise of zero mean and standard deviation $\\sigma_{\\mathrm{noise}} = 0.05$. Compute $\\Delta$ as specified.\n- Case $3$ (edge case): A noise-free linear ramp signal $u^\\star[i] = i/(n-1)$ with $\\sigma_{\\mathrm{noise}} = 0$. Compute $\\Delta$ as specified.\n\nYour program should produce a single line of output containing the results as a comma-separated list of floating-point numbers enclosed in square brackets, ordered as $[\\Delta_{\\text{Case }1}, \\Delta_{\\text{Case }2}, \\Delta_{\\text{Case }3}]$, with each number rounded to six decimal places (e.g., $[0.123456,0.000001,-0.045000]$).", "solution": "The problem requires the implementation and comparison of first-order Total Variation (TV) and second-order Total Generalized Variation (TGV) regularization for one-dimensional signal denoising. The comparison is based on the ramp bias, which quantifies how well each method preserves linear slopes. The solution is derived from first principles of convex optimization using a primal-dual algorithm.\n\nThe general form of the denoising problem is to find a signal $u \\in \\mathbb{R}^n$ that minimizes a composite objective function:\n$$ u^* = \\arg\\min_{u \\in \\mathbb{R}^n} \\left( F(u) + R(u) \\right) $$\nwhere $F(u)$ is a data fidelity term and $R(u)$ is a regularizer. The problem specifies the data fidelity term as the squared $\\ell_2$-norm distance to the observed noisy signal $f \\in \\mathbb{R}^n$, given by $F(u) = \\frac{1}{2}\\lVert u - f \\rVert_2^2$.\n\nThe problem is solved using the Primal-Dual Hybrid Gradient (PDHG) method, also known as the Chambolle-Pock algorithm, which is well-suited for problems of the form:\n$$ \\min_{x} \\mathcal{F}(x) + \\mathcal{G}(Kx) $$\nwhere $\\mathcal{F}$ and $\\mathcal{G}$ are proper, convex, lower semi-continuous functions, and $K$ is a continuous linear operator. The iterative scheme for this problem is defined by:\n$$\n\\begin{cases}\ny^{k+1} = \\mathrm{prox}_{\\sigma \\mathcal{G}^*}(y^k + \\sigma K \\bar{x}^k) \\\\\nx^{k+1} = \\mathrm{prox}_{\\tau \\mathcal{F}}(x^k - \\tau K^* y^{k+1}) \\\\\n\\bar{x}^{k+1} = x^{k+1} + \\theta(x^{k+1} - x^k)\n\\end{cases}\n$$\nHere, $x$ is the primal variable, $y$ is the dual variable, $K^*$ is the adjoint of $K$, $\\mathcal{G}^*$ is the convex conjugate of $\\mathcal{G}$, and $\\mathrm{prox}_{\\gamma H}(z) = \\arg\\min_v ( H(v) + \\frac{1}{2\\gamma} \\lVert v-z \\rVert_2^2 )$ is the proximal operator. The algorithm parameters are the primal step size $\\tau  0$, the dual step size $\\sigma  0$, and the over-relaxation parameter $\\theta \\in [0, 1]$. The step sizes must satisfy $\\tau \\sigma \\lVert K \\rVert^2  1$. The problem specifies $\\tau=0.1$, $\\sigma=0.1$, and $\\theta=1$.\n\nThe discrete forward difference operator $D: \\mathbb{R}^n \\to \\mathbb{R}^n$ with homogeneous Neumann boundary conditions is defined as $(Du)_i = u_{i+1} - u_i$ for $i \\in \\{0, \\dots, n-2\\}$, and $(Du)_{n-1} = 0$. Its adjoint operator $D^*: \\mathbb{R}^n \\to \\mathbb{R}^n$ must satisfy $\\langle Du, p \\rangle = \\langle u, D^*p \\rangle$ for all $u, p \\in \\mathbb{R}^n$. This yields $(D^*p)_0 = -p_0$, $(D^*p)_i = p_{i-1} - p_i$ for $i \\in \\{1, \\dots, n-2\\}$, and $(D^*p)_{n-1} = p_{n-2}$.\n\nFor both TV and TGV models, the data fidelity component $\\mathcal{F}$ and its proximal operator are common. Let $\\mathcal{F}(u) = \\frac{1}{2}\\lVert u - f \\rVert_2^2$. Its proximal operator is:\n$$ \\mathrm{prox}_{\\tau \\mathcal{F}}(v) = \\arg\\min_u \\left( \\frac{1}{2}\\lVert u - f \\rVert_2^2 + \\frac{1}{2\\tau}\\lVert u - v \\rVert_2^2 \\right) = \\frac{v + \\tau f}{1 + \\tau} $$\n\n**First-Order Total Variation (TV) Denoising**\nThe TV-regularized problem is:\n$$ \\min_{u \\in \\mathbb{R}^n} \\frac{1}{2}\\lVert u - f \\rVert_2^2 + \\lambda_{\\mathrm{TV}} \\lVert Du \\rVert_1 $$\nThis fits the PDHG framework with the following assignments:\n- Primal variable $x = u \\in \\mathbb{R}^n$.\n- $\\mathcal{F}(u) = \\frac{1}{2}\\lVert u - f \\rVert_2^2$.\n- $\\mathcal{G}(y) = \\lambda_{\\mathrm{TV}} \\lVert y \\rVert_1$, where $y \\in \\mathbb{R}^n$ is a generic variable.\n- Linear operator $K = D$.\n\nThe convex conjugate of $\\mathcal{G}$ is $\\mathcal{G}^*(p) = I_{\\lVert \\cdot \\rVert_\\infty \\le \\lambda_{\\mathrm{TV}}}(p)$, which is the indicator function of the $\\ell_\\infty$-ball of radius $\\lambda_{\\mathrm{TV}}$. The proximal operator of $\\mathcal{G}^*$ is the projection onto this ball:\n$$ \\mathrm{prox}_{\\sigma \\mathcal{G}^*}(v) = \\mathrm{proj}_{\\lVert \\cdot \\rVert_\\infty \\le \\lambda_{\\mathrm{TV}}}(v) $$\nThis projection is computed element-wise as $(v_i)_{\\text{proj}} = v_i / \\max(1, |v_i|/\\lambda_{\\mathrm{TV}})$.\nThe complete TV denoising algorithm is:\n1. Initialize $u^0$, $\\bar{u}^0 = u^0$, and the dual variable $y^0$.\n2. For $k=0, 1, \\dots, N_{\\mathrm{iter}}-1$:\n   $y^{k+1} = \\mathrm{proj}_{\\lVert \\cdot \\rVert_\\infty \\le \\lambda_{\\mathrm{TV}}}(y^k + \\sigma D \\bar{u}^k)$\n   $u^{k+1} = \\frac{(u^k - \\tau D^* y^{k+1}) + \\tau f}{1 + \\tau}$\n   $\\bar{u}^{k+1} = u^{k+1} + \\theta(u^{k+1} - u^k)$\n\n**Second-Order Total Generalized Variation (TGV$^2$) Denoising**\nThe TGV$^2$-regularized problem is a joint minimization over the signal $u$ and an auxiliary field $w \\in \\mathbb{R}^n$:\n$$ \\min_{u, w} \\frac{1}{2}\\lVert u - f \\rVert_2^2 + \\alpha_1 \\lVert Du - w \\rVert_1 + \\alpha_2 \\lVert Dw \\rVert_1 $$\nTo fit this into the PDHG framework, we define a composite primal variable $x = (u, w) \\in \\mathbb{R}^{2n}$.\n- $\\mathcal{F}(u, w) = \\frac{1}{2}\\lVert u - f \\rVert_2^2 + 0 \\cdot \\lVert w \\rVert_2^2$.\n- $\\mathcal{G}(z_1, z_2) = \\alpha_1 \\lVert z_1 \\rVert_1 + \\alpha_2 \\lVert z_2 \\rVert_1$, for $(z_1, z_2) \\in \\mathbb{R}^{2n}$.\n- Linear operator $K(u,w) = \\begin{pmatrix} Du - w \\\\ Dw \\end{pmatrix} = \\begin{pmatrix} D  -I \\\\ 0  D \\end{pmatrix} \\begin{pmatrix} u \\\\ w \\end{pmatrix}$.\n\nThe adjoint operator is $K^* = \\begin{pmatrix} D^*  0 \\\\ -I  D^* \\end{pmatrix}$. The dual variable is $y=(p,q) \\in \\mathbb{R}^{2n}$.\nThe proximal operator of $\\mathcal{F}$ separates for $u$ and $w$:\n$$ \\mathrm{prox}_{\\tau\\mathcal{F}}((v_u, v_w)) = \\left(\\frac{v_u + \\tau f}{1+\\tau}, v_w\\right) $$\nThe conjugate $\\mathcal{G}^*$ is the indicator function of the set $\\{ (p,q) \\mid \\lVert p \\rVert_\\infty \\le \\alpha_1, \\lVert q \\rVert_\\infty \\le \\alpha_2 \\}$. Its proximal operator is the projection onto this set, which separates for $p$ and $q$.\nThe TGV$^2$ denoising algorithm is:\n1. Initialize $u^0, w^0$, $\\bar{u}^0=u^0, \\bar{w}^0=w^0$, and dual variables $p^0, q^0$.\n2. For $k=0, 1, \\dots, N_{\\mathrm{iter}}-1$:\n   $p^{k+1} = \\mathrm{proj}_{\\lVert \\cdot \\rVert_\\infty \\le \\alpha_1}(p^k + \\sigma(D\\bar{u}^k - \\bar{w}^k))$\n   $q^{k+1} = \\mathrm{proj}_{\\lVert \\cdot \\rVert_\\infty \\le \\alpha_2}(q^k + \\sigma D\\bar{w}^k)$\n   $u_{v} = u^k - \\tau D^*p^{k+1}$\n   $w_{v} = w^k - \\tau(-p^{k+1} + D^*q^{k+1})$\n   $u^{k+1} = \\frac{u_v + \\tau f}{1+\\tau}$\n   $w^{k+1} = w_v$\n   $\\bar{u}^{k+1} = u^{k+1} + \\theta(u^{k+1} - u^k)$\n   $\\bar{w}^{k+1} = w^{k+1} + \\theta(w^{k+1} - w^k)$\n\n**Ramp Bias Calculation**\nFor each reconstructed signal $u_{\\text{recon}}$, the slope is estimated by finding the coefficient $m$ that minimizes $\\sum_{i=0}^{n-1} ( (m \\cdot i + c) - u_{\\text{recon}}[i] )^2$ via least squares regression. The signed slope error, or bias, is $b = m - m^\\star$, where $m^\\star$ is the slope of the true underlying signal. For a linear ramp $u^\\star[i] = i/(n-1)$, $m^\\star = 1/(n-1)$. For a constant signal, $m^\\star = 0$. The final metric is the difference in absolute biases: $\\Delta = |b_{\\mathrm{TV}}| - |b_{\\mathrm{TGV}}|$. A positive $\\Delta$ indicates that TGV$^2$ has a smaller absolute ramp bias, and thus better preserves linear structures, a known theoretical advantage of TGV$^2$ over TV. The implementation will execute these derived algorithms for the specified test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the denoising experiments and compute the ramp bias difference.\n    Implements primal-dual algorithms for TV and TGV denoising from first principles.\n    \"\"\"\n    # --- Global Problem Parameters ---\n    N = 32\n    TAU = 0.1\n    SIGMA = 0.1\n    THETA = 1.0\n    N_ITER = 1000\n    LAMBDA_TV = 0.12\n    ALPHA1 = 0.8\n    ALPHA2 = 1.6\n    SEED = 42\n\n    # --- Operator Definitions ---\n    def D_op(u):\n        \"\"\"Discrete forward difference operator with homogeneous Neumann boundary.\"\"\"\n        n = len(u)\n        res = np.zeros(n, dtype=np.float64)\n        res[:-1] = u[1:] - u[:-1]\n        # res[-1] is 0 by initialization\n        return res\n\n    def D_adj_op(p):\n        \"\"\"Adjoint of the discrete forward difference operator.\"\"\"\n        n = len(p)\n        res = np.zeros(n, dtype=np.float64)\n        res[0] = -p[0]\n        if n > 2:\n            res[1:-1] = p[:-2] - p[1:-1]\n        if n > 1:\n            res[-1] = p[-2]\n        return res\n\n    # --- Denoising Algorithms ---\n    def denoise_tv(f):\n        \"\"\"\n        TV denoising using a primal-dual algorithm.\n        min_u 0.5 * ||u - f||^2 + LAMBDA_TV * ||Du||_1\n        \"\"\"\n        u = np.copy(f)\n        u_bar = np.copy(u)\n        y = np.zeros_like(f)\n\n        for _ in range(N_ITER):\n            u_old = np.copy(u)\n            \n            # Dual update using prox of the conjugate\n            y_update_arg = y + SIGMA * D_op(u_bar)\n            denom = np.maximum(1.0, np.abs(y_update_arg) / LAMBDA_TV)\n            y = y_update_arg / denom\n\n            # Primal update using prox of the data fidelity term\n            prox_arg = u_old - TAU * D_adj_op(y)\n            u = (prox_arg + TAU * f) / (1.0 + TAU)\n            \n            # Over-relaxation step\n            u_bar = u + THETA * (u - u_old)\n        \n        return u\n\n    def denoise_tgv(f):\n        \"\"\"\n        TGV denoising using a primal-dual algorithm.\n        min_{u,w} 0.5*||u-f||^2 + ALPHA1*||Du-w||_1 + ALPHA2*||Dw||_1\n        \"\"\"\n        u = np.copy(f)\n        u_bar = np.copy(u)\n        w = np.zeros_like(f)\n        w_bar = np.zeros_like(f)\n        p = np.zeros_like(f)\n        q = np.zeros_like(f)\n        \n        for _ in range(N_ITER):\n            u_old = np.copy(u)\n            w_old = np.copy(w)\n            \n            # Dual update for (p, q)\n            v_p = p + SIGMA * (D_op(u_bar) - w_bar)\n            v_q = q + SIGMA * D_op(w_bar)\n            \n            p = v_p / np.maximum(1.0, np.abs(v_p) / ALPHA1)\n            q = v_q / np.maximum(1.0, np.abs(q) / ALPHA2)\n            \n            # Primal update for (u, w)\n            v_u = u_old - TAU * D_adj_op(p)\n            v_w = w_old - TAU * (-p + D_adj_op(q))\n            \n            u = (v_u + TAU * f) / (1.0 + TAU)\n            w = v_w\n            \n            # Over-relaxation step\n            u_bar = u + THETA * (u - u_old)\n            w_bar = w + THETA * (w - w_old)\n            \n        return u\n\n    # --- Bias Calculation ---\n    def get_slope(u):\n        \"\"\"Estimate the slope of a signal using linear least squares.\"\"\"\n        n = len(u)\n        x = np.arange(n, dtype=np.float64)\n        A = np.vstack([x, np.ones(n)]).T\n        m, _ = np.linalg.lstsq(A, u, rcond=None)[0]\n        return m\n\n    # --- Test Suite Execution ---\n    rng = np.random.default_rng(SEED)\n    test_specs = [\n        {'type': 'ramp', 'noise_std': 0.05},\n        {'type': 'const', 'noise_std': 0.05},\n        {'type': 'ramp', 'noise_std': 0.0},\n    ]\n    \n    results = []\n    for spec in test_specs:\n        if spec['type'] == 'ramp':\n            u_star = np.arange(N, dtype=np.float64) / (N - 1)\n            true_slope = 1.0 / (N - 1)\n        else: # 'const'\n            u_star = np.full(N, 0.5, dtype=np.float64)\n            true_slope = 0.0\n\n        if spec['noise_std'] > 0:\n            noise = rng.normal(0, spec['noise_std'], N)\n            f = u_star + noise\n        else:\n            f = u_star\n            \n        # Perform denoising\n        u_tv = denoise_tv(f)\n        u_tgv = denoise_tgv(f)\n        \n        # Calculate slope errors (bias)\n        slope_tv = get_slope(u_tv)\n        slope_tgv = get_slope(u_tgv)\n        \n        bias_tv = slope_tv - true_slope\n        bias_tgv = slope_tgv - true_slope\n        \n        # Calculate delta of absolute biases\n        delta = np.abs(bias_tv) - np.abs(bias_tgv)\n        results.append(delta)\n\n    # --- Format and Print Output according to problem specification ---\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3478968"}, {"introduction": "In practice, a crucial choice is how to balance data fidelity and regularization, often framed as either a constrained problem (e.g., using the Morozov discrepancy principle) or an unconstrained Lagrangian problem. This advanced practice explores the deep connection between these two views through the lens of iterative optimization. You will implement a primal-dual algorithm with an early-stopping strategy for the constrained problem and discover how the iteration count can act as an implicit regularization parameter, approximating the solution from an unconstrained problem with a perfectly chosen 'oracle' parameter $\\lambda$. [@problem_id:3466854]", "problem": "Implement a complete numerical study of early stopping for the Primal-Dual Hybrid Gradient (PDHG) method applied to one-dimensional anisotropic Total Variation (TV) constrained reconstruction under the Morozov discrepancy principle, and compare it to an oracle parameter selection for the unconstrained TV-regularized least squares. The goal is to derive from first principles, implement, and test when early-stopped PDHG for the constrained problem matches a Lagrangian formulation with an oracle regularization parameter selected to satisfy the same discrepancy.\n\nConsider the following setting. Let $A \\in \\mathbb{R}^{m \\times n}$ be a linear sensing operator, let $b \\in \\mathbb{R}^{m}$ be observed data, and let $\\epsilon \\in \\mathbb{R}_{+}$ be a noise level. Let the anisotropic one-dimensional Total Variation (TV) semi-norm be defined for $x \\in \\mathbb{R}^{n}$ by $TV(x) = \\sum_{i=1}^{n-1} |x_{i+1} - x_{i}|$, which corresponds to the $\\ell_{1}$-norm of the forward differences. The Morozov discrepancy principle requires the residual norm to satisfy $\\|A x - b\\|_{2} \\approx \\epsilon$.\n\nYou must start from the following fundamental base.\n- Convex constrained TV minimization: minimize $TV(x)$ subject to $\\|A x - b\\|_{2} \\le \\epsilon$.\n- Lagrangian TV-regularized least squares: minimize $\\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda\\, TV(x)$ for some $\\lambda \\ge 0$.\n- Convex duality and Karush–Kuhn–Tucker (KKT) conditions for the constrained problem, and the existence of a Lagrange multiplier $\\lambda$ connecting constrained and unconstrained problems under Slater’s condition.\n- The PDHG method as a first-order primal-dual algorithm for min-max saddle problems of the form $\\min_{x} g(x) + f(K x)$ with step sizes constrained by $\\tau \\sigma \\|K\\|^{2}  1$.\n- Proximal mappings and convex conjugates: for $f(z) = \\frac{1}{2}\\|z - b\\|_{2}^{2}$, $f^{*}(p) = \\frac{1}{2}\\|p\\|_{2}^{2} + \\langle p, b\\rangle$ with proximal $\\operatorname{prox}_{\\sigma f^{*}}(v) = \\frac{v - \\sigma b}{1 + \\sigma}$; for $h(u) = \\|u\\|_{1}$, $h^{*}(q) = \\iota_{\\|q\\|_{\\infty} \\le 1}(q)$ with proximal given by projection onto the $\\ell_{\\infty}$-unit ball; for $f(z) = \\iota_{\\|z - b\\|_{2} \\le \\epsilon}(z)$, $f^{*}(p) = \\langle p, b\\rangle + \\epsilon \\|p\\|_{2}$ with proximal $\\operatorname{prox}_{\\sigma f^{*}}(v) = \\operatorname{shrink}_{\\ell_{2}}(v - \\sigma b, \\sigma \\epsilon)$ where $\\operatorname{shrink}_{\\ell_{2}}(w, t) = \\max\\{0, 1 - t/\\|w\\|_{2}\\}\\, w$.\n\nYour tasks are:\n- Derive the PDHG updates for the constrained problem $\\min_{x} TV(x)$ subject to $\\|A x - b\\|_{2} \\le \\epsilon$ by writing it in the form $\\min_{x} g(x) + f(K x)$ with $g(x) = 0$, $K x = \\begin{bmatrix} D x \\\\ A x \\end{bmatrix}$ where $D$ is the forward-difference operator, and $f(u, v) = \\|u\\|_{1} + \\iota_{\\|v - b\\|_{2} \\le \\epsilon}(v)$. Provide explicit dual updates using proximal mappings of $f^{*}$ and the primal update using $K^{\\top}$.\n- Specify a computable early stopping rule guided by primal-dual optimality residuals and the discrepancy principle. The rule must check both a discrepancy closeness condition $|\\|A x^{k} - b\\|_{2} - \\epsilon| \\le \\rho\\, \\epsilon$ for a chosen tolerance $\\rho \\in (0, 1)$ and a primal-dual stationarity residual $\\|K^{\\top} y^{k}\\|_{2} \\le \\eta$ for a chosen $\\eta \\in \\mathbb{R}_{+}$, where $y^{k}$ stacks the dual variables. Explain why these conditions approximate KKT feasibility and complementary slackness.\n- Derive the PDHG updates for the unconstrained Lagrangian problem with $f(Ax) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ and $h(Dx) = \\lambda \\|D x\\|_{1}$, using a product-space splitting with the same $K$ and separable dual updates. Then implement a bisection on $\\lambda$ to find an oracle $\\lambda_{\\star}$ such that the solution $x_{\\lambda_{\\star}}$ satisfies $\\|A x_{\\lambda_{\\star}} - b\\|_{2} \\approx \\epsilon$ within a prescribed relative tolerance.\n- Implement an estimator of $\\|K\\|$ using a power iteration for $K^{\\top} K = D^{\\top} D + A^{\\top} A$ and select PDHG step sizes $\\tau$ and $\\sigma$ that satisfy $\\tau \\sigma \\|K\\|^{2}  1$.\n- For each test case described below, generate a piecewise-constant ground truth $x_{0}$, simulate data $b = A x_{0} + \\xi$ with $\\xi \\sim \\mathcal{N}(0, \\sigma^{2} I)$, set $\\epsilon = \\|\\xi\\|_{2}$, run the constrained PDHG with the early stopping rule to obtain $x_{\\mathrm{ES}}$, compute the oracle $\\lambda_{\\star}$ by bisection and solve the unconstrained PDHG to obtain $x_{\\lambda_{\\star}}$, and then return a boolean that is true if both of the following hold:\n  - Residual match: $|\\|A x_{\\mathrm{ES}} - b\\|_{2} - \\epsilon| \\le \\alpha\\, \\epsilon$,\n  - Solution match: $\\|x_{\\mathrm{ES}} - x_{\\lambda_{\\star}}\\|_{2} \\le \\beta\\, \\|x_{\\lambda_{\\star}}\\|_{2}$,\n  for given tolerances $\\alpha \\in (0, 1)$ and $\\beta \\in (0, 1)$.\n- Your program must implement everything from scratch using only the stated runtime environment and produce the specified output.\n\nUse the following test suite that specifies different operator conditions, noise levels, and stopping budgets. In all cases, angles do not appear, and there are no physical units required. All thresholds must be treated as dimensionless numbers.\n\nTest suite (each tuple describes $(n, m, \\text{seed}, \\sigma, \\text{ill}, \\text{max\\_iter\\_ES}, \\rho, \\eta, \\alpha, \\beta)$):\n- Case $1$: $(64, 40, 0, 0.01, \\text{False}, 3000, 0.05, 10^{-3}, 0.05, 0.10)$.\n- Case $2$: $(64, 40, 1, 10^{-4}, \\text{False}, 3000, 0.05, 10^{-3}, 0.05, 0.10)$.\n- Case $3$: $(64, 40, 2, 0.05, \\text{False}, 3000, 0.05, 10^{-3}, 0.05, 0.10)$.\n- Case $4$: $(64, 40, 3, 0.01, \\text{True}, 50, 0.05, 10^{-3}, 0.05, 0.10)$, where the flag $\\text{ill} = \\text{True}$ indicates that $A$ is made ill-conditioned by post-multiplication with a poorly conditioned square matrix on the domain.\n\nImplementation details to respect:\n- Construct $D$ as the forward-difference operator $(D x)_{i} = x_{i+1} - x_{i}$ for $i \\in \\{1,\\dots,n-1\\}$ with $(D x) \\in \\mathbb{R}^{n-1}$, and its adjoint $D^{\\top}$ given by $(D^{\\top} q)_{1} = -q_{1}$, $(D^{\\top} q)_{i} = q_{i-1} - q_{i}$ for $i \\in \\{2,\\dots,n-1\\}$, and $(D^{\\top} q)_{n} = q_{n-1}$.\n- For the constrained problem, use dual updates with projections $q \\leftarrow \\operatorname{proj}_{\\|\\cdot\\|_{\\infty} \\le 1}(q)$ and $p \\leftarrow \\operatorname{shrink}_{\\ell_{2}}(p - \\sigma b, \\sigma \\epsilon)$, where $q$ is the gradient dual and $p$ is the data-fit dual.\n- For the unconstrained problem with parameter $\\lambda$, use dual updates $q \\leftarrow \\operatorname{proj}_{\\|\\cdot\\|_{\\infty} \\le \\lambda}(q)$ and $p \\leftarrow \\frac{p - \\sigma b}{1 + \\sigma}$.\n- Select $\\tau = \\sigma = \\frac{0.99}{\\|K\\|}$ and $\\theta = 1$, where $\\|K\\|$ is estimated by power iteration on $K^{\\top} K$.\n- Use bisection on $\\lambda$ with a bracket updated to ensure that the residual at the upper bracket is at least $\\epsilon$, and run a fixed-budget PDHG solve per $\\lambda$ value with warm starts to find $\\lambda_{\\star}$ such that $|\\|A x_{\\lambda_{\\star}} - b\\|_{2} - \\epsilon| \\le \\alpha\\, \\epsilon$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets (for example, $[r_{1},r_{2},r_{3},r_{4}]$), where each $r_{i}$ is a boolean returned by the match test above.\n\nYour implementation must be fully self-contained and runnable as is, using only the allowed libraries, and must not require any user input or external files or network access. The final output must be exactly one line in the specified format.", "solution": "The objective is to conduct a numerical study on the relationship between an early-stopped primal-dual algorithm for a constrained Total Variation (TV) minimization problem and the solution of its corresponding unconstrained Lagrangian formulation with an oracle-chosen regularization parameter. We will derive the necessary algorithms from first principles and implement a test to verify when their respective solutions coincide under the Morozov discrepancy principle.\n\nLet the linear inverse problem be modeled as $b = Ax_0 + \\xi$, where $A \\in \\mathbb{R}^{m \\times n}$ is a sensing operator, $x_0 \\in \\mathbb{R}^n$ is the unknown true signal (assumed to be piecewise-constant), $b \\in \\mathbb{R}^m$ is the measured data, and $\\xi \\in \\mathbb{R}^m$ is additive noise with known energy $\\|\\xi\\|_2 = \\epsilon$. The one-dimensional anisotropic TV semi-norm is $TV(x) = \\|Dx\\|_1$, where $D \\in \\mathbb{R}^{(n-1) \\times n}$ is the forward-difference operator.\n\nWe compare two standard formulations for reconstructing $x$ from $b$:\n1.  **Constrained (Basis Pursuit Denoising-style) formulation**: $\\min_{x \\in \\mathbb{R}^n} TV(x)$ subject to $\\|Ax - b\\|_2 \\le \\epsilon$.\n2.  **Unconstrained (Lagrangian) formulation**: $\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda TV(x)$ for a regularization parameter $\\lambda \\ge 0$.\n\nUnder suitable conditions (e.g., Slater's condition), there exists a Lagrange multiplier $\\lambda_\\star \\ge 0$ such that the solution to the unconstrained problem with $\\lambda=\\lambda_\\star$ is also a solution to the constrained problem. The Morozov discrepancy principle suggests that $\\lambda_\\star$ should be chosen such that the solution $x_{\\lambda_\\star}$ satisfies $\\|Ax_{\\lambda_\\star} - b\\|_2 = \\epsilon$. This study investigates if an early-stopped iterative algorithm for the constrained problem can produce a solution similar to $x_{\\lambda_\\star}$. We employ the Primal-Dual Hybrid Gradient (PDHG) method for both problems.\n\n**1. PDHG Algorithm for the Constrained Problem**\n\nThe constrained problem is $\\min_{x} \\|Dx\\|_1$ subject to $\\|Ax-b\\|_2 \\le \\epsilon$. This can be written in the generic saddle-point form $\\min_x \\max_y \\langle Kx, y \\rangle + G(x) - F^*(y)$, which PDHG is designed to solve.\nLet $G(x) = 0$. We define a composite linear operator $K: \\mathbb{R}^n \\to \\mathbb{R}^{n-1} \\times \\mathbb{R}^m$ as $Kx = \\begin{bmatrix} Dx \\\\ Ax \\end{bmatrix}$. The objective function can be expressed as $F(Kx)$, where $F: \\mathbb{R}^{n-1} \\times \\mathbb{R}^m \\to \\mathbb{R} \\cup \\{+\\infty\\}$ is given by $F(u, v) = \\|u\\|_1 + \\iota_{\\|v-b\\|_2 \\le \\epsilon}(v)$. Here, $(u,v)$ is a point in the range of $K$, and $\\iota_C$ is the indicator function of set $C$.\nThe function $F$ is separable: $F(u,v) = F_1(u) + F_2(v)$, with $F_1(u) = \\|u\\|_1$ and $F_2(v) = \\iota_{\\|v-b\\|_2 \\le \\epsilon}(v)$.\nThe dual variable $y$ is split accordingly as $y = \\begin{bmatrix} q \\\\ p \\end{bmatrix}$, where $q \\in \\mathbb{R}^{n-1}$ and $p \\in \\mathbb{R}^m$.\nThe convex conjugate $F^*(y)$ is also separable: $F^*(q,p) = F_1^*(q) + F_2^*(p)$.\nThe conjugates are:\n-   $F_1^*(q) = \\sup_u (\\langle q, u \\rangle - \\|u\\|_1) = \\iota_{\\|q\\|_\\infty \\le 1}(q)$.\n-   $F_2^*(p) = \\sup_v (\\langle p, v \\rangle - \\iota_{\\|v-b\\|_2 \\le \\epsilon}(v)) = \\sup_{\\|w\\|_2 \\le \\epsilon} \\langle p, w+b \\rangle = \\langle p, b \\rangle + \\epsilon\\|p\\|_2$.\n\nThe PDHG algorithm with extrapolation parameter $\\theta=1$ iterates as follows:\n$y^{k+1} = \\operatorname{prox}_{\\sigma F^*} (y^k + \\sigma K \\bar{x}^k)$\n$x^{k+1} = \\operatorname{prox}_{\\tau G} (x^k - \\tau K^\\top y^{k+1})$\n$\\bar{x}^{k+1} = 2x^{k+1} - x^k$\n\nSince $G(x)=0$, its proximal operator is the identity, $\\operatorname{prox}_{\\tau G}(z)=z$. The primal update is a simple gradient descent step on the dual variables: $x^{k+1} = x^k - \\tau K^\\top y^{k+1}$. The adjoint operator $K^\\top$ is given by $K^\\top y = D^\\top q + A^\\top p$.\nThe dual update on $y$ is separable due to the separability of $F^*$:\n-   $q^{k+1} = \\operatorname{prox}_{\\sigma F_1^*}(q^k + \\sigma D\\bar{x}^k) = \\operatorname{proj}_{\\|\\cdot\\|_\\infty \\le 1}(q^k + \\sigma D\\bar{x}^k)$.\n-   $p^{k+1} = \\operatorname{prox}_{\\sigma F_2^*}(p^k + \\sigma A\\bar{x}^k)$. The proximal operator for $\\sigma F_2^*$ is $\\operatorname{prox}_{\\sigma F_2^*}(v) = \\operatorname{argmin}_p \\{\\frac{1}{2}\\|p-v\\|_2^2 + \\sigma(\\langle p, b \\rangle + \\epsilon \\|p\\|_2)\\}$. The solution is $p = \\operatorname{prox}_{\\sigma\\epsilon\\|\\cdot\\|_2}(v-\\sigma b) = \\operatorname{shrink}_{\\ell_2}(v-\\sigma b, \\sigma\\epsilon)$.\nSubstituting $v=p^k + \\sigma A\\bar{x}^k$, we get $p^{k+1} = \\operatorname{shrink}_{\\ell_2}(p^k + \\sigma(A\\bar{x}^k - b), \\sigma\\epsilon)$.\n\nThe complete updates are:\n1.  $q^{k+1} = \\operatorname{proj}_{\\|\\cdot\\|_\\infty \\le 1}(q^k + \\sigma D\\bar{x}^k)$\n2.  $p^{k+1} = \\operatorname{shrink}_{\\ell_2}(p^k + \\sigma(A\\bar{x}^k-b), \\sigma\\epsilon)$\n3.  $x^{k+1} = x^k - \\tau (D^\\top q^{k+1} + A^\\top p^{k+1})$\n4.  $\\bar{x}^{k+1} = 2 x^{k+1} - x^k$\n\n**2. Early Stopping Rule and KKT Conditions**\n\nThe Karush-Kuhn-Tucker (KKT) conditions for the constrained problem are:\n1.  **Stationarity**: $0 \\in \\partial(\\|Dx^*\\|_1) + \\nu^* A^\\top(Ax^*-b)$, where we assume the solution lies on the boundary, so $\\|Ax^*-b\\|_2=\\epsilon$.\n2.  **Primal Feasibility**: $\\|Ax^*-b\\|_2 \\le \\epsilon$.\n3.  **Dual Feasibility**: $\\nu^* \\ge 0$.\n4.  **Complementary Slackness**: $\\nu^* (\\|Ax^*-b\\|_2 - \\epsilon) = 0$.\n\nThe optimality conditions for the saddle-point problem are $K^\\top y^* = 0$ (i.e., $D^\\top q^* + A^\\top p^* = 0$) and $Kx^* \\in \\partial F^*(y^*)$. These are equivalent to the KKT conditions.\nThe proposed early stopping rule consists of two parts:\n-   $|\\|A x^{k} - b\\|_{2} - \\epsilon| \\le \\rho\\, \\epsilon$: This condition enforces that the iterate $x^k$ is close to satisfying the discrepancy principle $\\|Ax-b\\|_2 = \\epsilon$. This corresponds to ensuring the primal feasibility and active complementary slackness conditions are approximately met.\n-   $\\|K^{\\top} y^{k}\\|_{2} = \\|D^\\top q^k + A^\\top p^k\\|_2 \\le \\eta$: This condition directly measures the proximity to satisfying the primal-dual optimality condition $K^\\top y^* = 0$, which is related to the KKT stationarity condition.\nThus, when both conditions are met, the iterate $(x^k, y^k)$ is considered a good approximation of an optimal primal-dual solution.\n\n**3. PDHG Algorithm for the Unconstrained Problem**\n\nThe Lagrangian formulation is $\\min_x \\frac{1}{2}\\|Ax-b\\|_2^2 + \\lambda \\|Dx\\|_1$.\nWe use the same saddle-point structure with $G(x)=0$ and $Kx = \\begin{bmatrix} Dx \\\\ Ax \\end{bmatrix}$. The function $F$ is now $F(u, v) = \\lambda\\|u\\|_1 + \\frac{1}{2}\\|v-b\\|_2^2$.\n$F$ is separable: $F(u,v) = F_1(u) + F_2(v)$ with $F_1(u) = \\lambda\\|u\\|_1$ and $F_2(v) = \\frac{1}{2}\\|v-b\\|_2^2$.\nThe conjugates are:\n-   $F_1^*(q) = (\\lambda\\|\\cdot\\|_1)^*(q) = \\iota_{\\|q\\|_\\infty \\le \\lambda}(q)$.\n-   $F_2^*(p) = (\\frac{1}{2}\\|\\cdot-b\\|_2^2)^*(p) = \\frac{1}{2}\\|p\\|_2^2 + \\langle p, b \\rangle$.\n\nThe PDHG updates for the dual variables $y=\\begin{bmatrix} q \\\\ p \\end{bmatrix}$ are:\n-   $q^{k+1} = \\operatorname{prox}_{\\sigma F_1^*}(q^k + \\sigma D\\bar{x}^k) = \\operatorname{proj}_{\\|\\cdot\\|_\\infty \\le \\lambda}(q^k + \\sigma D\\bar{x}^k)$.\n-   $p^{k+1} = \\operatorname{prox}_{\\sigma F_2^*}(p^k + \\sigma A\\bar{x}^k)$. The proximal operator for $\\sigma F_2^*$ is $\\operatorname{prox}_{\\sigma F_2^*}(v) = \\operatorname{argmin}_p \\{\\frac{1}{2}\\|p-v\\|_2^2 + \\sigma(\\frac{1}{2}\\|p\\|_2^2 + \\langle p, b \\rangle) \\}$. This is a quadratic in $p$ with solution $p = \\frac{v-\\sigma b}{1+\\sigma}$. Substituting $v=p^k + \\sigma A\\bar{x}^k$, we get $p^{k+1} = \\frac{p^k + \\sigma(A\\bar{x}^k-b)}{1+\\sigma}$.\nThe primal and extrapolation updates are identical in form to the constrained case.\n\n**4. Parameter Selection**\n\nThe PDHG algorithm's convergence is guaranteed if the step sizes $\\tau, \\sigma  0$ satisfy $\\tau\\sigma\\|K\\|_2^2  1$. We estimate $\\|K\\|_2$ by finding the largest eigenvalue of $K^\\top K = D^\\top D + A^\\top A$ via power iteration. We then set $\\tau = \\sigma = 0.99 / \\|K\\|_2$.\nTo find the oracle parameter $\\lambda_\\star$ for the unconstrained problem, we use bisection. The residual norm $R(\\lambda) = \\|Ax_\\lambda - b\\|_2$ is a monotonically increasing function of $\\lambda$. We search for $\\lambda_\\star$ such that $|R(\\lambda_\\star) - \\epsilon| \\le \\alpha \\epsilon$. We establish a search bracket $[\\lambda_{low}, \\lambda_{high}]$ and iteratively narrow it by solving the unconstrained problem for $\\lambda_{mid}$ and comparing its residual to $\\epsilon$.\n\nThe following implementation performs the described numerical study for the given test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs the numerical study comparing early-stopped PDHG for constrained TV\n    reconstruction with an oracle-parameter unconstrained TV reconstruction.\n    \"\"\"\n\n    def get_operators(n, m, ill, seed):\n        \"\"\"Constructs the linear operators A and D.\"\"\"\n        rng = np.random.default_rng(seed)\n        A_rand = rng.standard_normal((m, n))\n\n        if ill:\n            # Create an ill-conditioned matrix C\n            U, _, Vh = np.linalg.svd(rng.standard_normal((n, n)))\n            s = np.logspace(0, -5, n)  # Condition number ~1e5\n            C = U @ np.diag(s) @ Vh\n            A = A_rand @ C\n        else:\n            A = A_rand\n        \n        # Normalize A to have spectral norm approx 1 for stability\n        A /= np.linalg.norm(A, 2)\n\n        def D_op(x):\n            return x[1:] - x[:-1]\n\n        def DT_op(q):\n            res = np.zeros(n)\n            res[0] = -q[0]\n            res[1:-1] = q[:-1] - q[1:]\n            res[-1] = q[-1]\n            return res\n\n        return A, D_op, DT_op\n\n    def estimate_norm_K(A, D_op, DT_op, n, iters=20):\n        \"\"\"Estimates the spectral norm of K using power iteration on K^T K.\"\"\"\n        x = np.random.randn(n)\n        x /= np.linalg.norm(x)\n        \n        for _ in range(iters):\n            x = DT_op(D_op(x)) + A.T @ (A @ x)\n            norm_val = np.linalg.norm(x)\n            x /= norm_val\n        \n        return np.sqrt(norm_val)\n\n    def proj_linf(v, radius):\n        \"\"\"Projection onto the L-infinity ball.\"\"\"\n        return np.clip(v, -radius, radius)\n\n    def shrink_l2(v, t):\n        \"\"\"L2-norm soft-thresholding (proximal of L2 norm).\"\"\"\n        norm_v = np.linalg.norm(v)\n        if norm_v > t:\n            return v * (1.0 - t / norm_v)\n        else:\n            return np.zeros_like(v)\n\n    def pdhg_solver(\n        mode, A, D_op, DT_op, b, n, n_m1, m,\n        tau, sigma, max_iter,\n        lambda_reg=None, epsilon=None, # Mode-specific parameters\n        x_init=None, q_init=None, p_init=None,\n        early_stop_params=None\n    ):\n        \"\"\"A generic PDHG solver for both constrained and unconstrained problems.\"\"\"\n        x = np.zeros(n) if x_init is None else np.copy(x_init)\n        q = np.zeros(n_m1) if q_init is None else np.copy(q_init)\n        p = np.zeros(m) if p_init is None else np.copy(p_init)\n        x_bar = np.copy(x)\n\n        for k in range(max_iter):\n            # Dual updates\n            Dx_bar = D_op(x_bar)\n            Ax_bar = A @ x_bar\n            \n            if mode == 'constrained':\n                q_next = proj_linf(q + sigma * Dx_bar, 1.0)\n                p_next = shrink_l2(p + sigma * (Ax_bar - b), sigma * epsilon)\n            elif mode == 'unconstrained':\n                q_next = proj_linf(q + sigma * Dx_bar, lambda_reg)\n                p_next = (p + sigma * (Ax_bar - b)) / (1.0 + sigma)\n            else:\n                raise ValueError(\"Invalid PDHG mode\")\n\n            # Primal update\n            x_next = x - tau * (DT_op(q_next) + A.T @ p_next)\n            \n            # Extrapolation\n            x_bar_next = 2 * x_next - x\n            \n            # Update variables\n            x, q, p, x_bar = x_next, q_next, p_next, x_bar_next\n            \n            if early_stop_params and mode == 'constrained':\n                rho, eta = early_stop_params\n                residual_norm = np.linalg.norm(A @ x - b)\n                stationarity_res = np.linalg.norm(DT_op(q) + A.T @ p)\n                if abs(residual_norm - epsilon) = rho * epsilon and stationarity_res = eta:\n                    break\n\n        return x, q, p\n\n    def solve_constrained_es(A, D_op, DT_op, b, n, n_m1, m, tau, sigma, max_iter_es, epsilon, rho, eta):\n        x_es, _, _ = pdhg_solver(\n            mode='constrained', A=A, D_op=D_op, DT_op=DT_op, b=b, n=n, n_m1=n_m1, m=m,\n            tau=tau, sigma=sigma, max_iter=max_iter_es,\n            epsilon=epsilon, early_stop_params=(rho, eta)\n        )\n        return x_es\n\n    def solve_unconstrained_bisection(A, D_op, DT_op, b, n, n_m1, m, tau, sigma, epsilon, alpha):\n        # Bisection to find lambda_star\n        lambda_low = 0.0\n        lambda_high = 1.0\n\n        pdhg_iters_per_lambda = 2000\n        \n        # Find a valid upper bound for lambda\n        x_init, q_init, p_init = None, None, None\n        while True:\n            x_lambda, q_lambda, p_lambda = pdhg_solver(\n                'unconstrained', A, D_op, DT_op, b, n, n_m1, m,\n                tau, sigma, pdhg_iters_per_lambda,\n                lambda_reg=lambda_high, x_init=x_init, q_init=q_init, p_init=p_init\n            )\n            x_init, q_init, p_init = x_lambda, q_lambda, p_lambda # Warm start\n            residual = np.linalg.norm(A @ x_lambda - b)\n            if residual > epsilon:\n                break\n            lambda_high *= 2.0\n            if lambda_high > 1e6: # Safety break\n                print(\"Warning: Could not find lambda_high\")\n                break\n\n        # Bisection loop\n        x_lambda_star = x_lambda\n        for _ in range(50): # Max 50 bisection iterations\n            lambda_mid = (lambda_low + lambda_high) / 2.0\n            if lambda_mid == lambda_low or lambda_mid == lambda_high:\n                break\n            \n            x_lambda_star, q_init, p_init = pdhg_solver(\n                'unconstrained', A, D_op, DT_op, b, n, n_m1, m, \n                tau, sigma, pdhg_iters_per_lambda,\n                lambda_reg=lambda_mid, x_init=x_init, q_init=q_init, p_init=p_init\n            )\n            \n            residual = np.linalg.norm(A @ x_lambda_star - b)\n\n            if abs(residual - epsilon) = alpha * epsilon:\n                break # Found a good lambda\n            \n            if residual  epsilon:\n                lambda_low = lambda_mid\n            else:\n                lambda_high = lambda_mid\n                \n        return x_lambda_star\n\n    test_cases = [\n        # (n, m, seed, noise_sigma, ill, max_iter_ES, rho, eta, alpha, beta)\n        (64, 40, 0, 0.01, False, 3000, 0.05, 1e-3, 0.05, 0.10),\n        (64, 40, 1, 1e-4, False, 3000, 0.05, 1e-3, 0.05, 0.10),\n        (64, 40, 2, 0.05, False, 3000, 0.05, 1e-3, 0.05, 0.10),\n        (64, 40, 3, 0.01, True, 50, 0.05, 1e-3, 0.05, 0.10),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        n, m, seed, noise_sigma, ill, max_iter_es, rho, eta, alpha, beta = case\n        n_m1 = n - 1\n\n        # --- Setup problem instance ---\n        rng = np.random.default_rng(seed)\n        \n        # Ground truth piecewise-constant signal\n        x0 = np.zeros(n)\n        x0[n//4 : n//2] = 1.0\n        x0[2*n//3 : 5*n//6] = -0.5\n\n        A, D_op, DT_op = get_operators(n, m, ill, seed)\n        noise = noise_sigma * rng.standard_normal(m)\n        b = A @ x0 + noise\n        epsilon = np.linalg.norm(noise)\n        \n        # --- Parameter Setup ---\n        norm_K = estimate_norm_K(A, D_op, DT_op, n)\n        tau = sigma = 0.99 / norm_K\n\n        # --- Solve and Compare ---\n        # 1. Constrained problem with early stopping\n        x_es = solve_constrained_es(A, D_op, DT_op, b, n, n_m1, m, tau, sigma, max_iter_es, epsilon, rho, eta)\n        \n        # 2. Unconstrained problem with oracle lambda\n        x_lambda_star = solve_unconstrained_bisection(A, D_op, DT_op, b, n, n_m1, m, tau, sigma, epsilon, alpha)\n        \n        # 3. Final comparison\n        res_es = np.linalg.norm(A @ x_es - b)\n        \n        residual_match = abs(res_es - epsilon) = alpha * epsilon\n        \n        norm_x_lambda_star = np.linalg.norm(x_lambda_star)\n        if norm_x_lambda_star == 0:\n            # Handle case where oracle solution is zero\n            solution_match = np.linalg.norm(x_es) = beta\n        else:\n            solution_match = np.linalg.norm(x_es - x_lambda_star) = beta * norm_x_lambda_star\n        \n        results.append(residual_match and solution_match)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3466854"}]}