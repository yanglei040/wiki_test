{"hands_on_practices": [{"introduction": "To effectively use submodular functions in optimization, it is crucial to have a concrete grasp of their defining properties and the convex penalties they generate. This first exercise takes us back to fundamentals, guiding you through the process of verifying submodularity for a classic overlapping group penalty, deriving its Lovász extension from first principles, and using these concepts to formulate the optimality conditions for a sparse recovery problem [@problem_id:3483776]. This foundational work is essential for building intuition and developing more advanced algorithms.", "problem": "Consider the ground set $\\{1,2,3,4\\}$ and the overlapping groups $G_{1}=\\{1,2,3\\}$ and $G_{2}=\\{2,4\\}$. Define the set function $F:2^{\\{1,2,3,4\\}}\\to\\mathbb{R}_{+}$ by\n$$\nF(S)=3\\,\\mathbf{1}\\{S\\cap G_{1}\\neq\\emptyset\\}+2\\,\\mathbf{1}\\{S\\cap G_{2}\\neq\\emptyset\\},\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ denotes the indicator function. Using only the core definitions of submodularity and the Lovász extension, proceed as follows:\n\n1. Starting from the definition of submodularity, establish that $F$ is a nondecreasing submodular function with $F(\\emptyset)=0$.\n\n2. Starting from the definition of the Lovász extension of a submodular function, derive the Lovász extension $f:\\mathbb{R}^{4}\\to\\mathbb{R}$ of $F$ and then its symmetrized norm $\\|x\\|_{F}=f(|x|)$, expressed directly in terms of the coordinates of $x$. Your derivation must not rely on pre-stated formulas; derive the expression purely from the Lovász extension definition applied to $F$.\n\n3. Using the definition of the dual norm $\\|\\cdot\\|_{F}^{*}$ as $\\|z\\|_{F}^{*}=\\sup\\{z^{\\top}x:\\|x\\|_{F}\\leq 1\\}$, derive the dual norm $\\|z\\|_{F}^{*}$ for the special case $z=(0,1,0,0)^{\\top}$.\n\n4. Write the Karush-Kuhn-Tucker (KKT) conditions for the unconstrained convex optimization problem\n$$\n\\min_{x\\in\\mathbb{R}^{4}} \\frac{1}{2}\\|y-Ax\\|_{2}^{2}+\\lambda\\|x\\|_{F},\n$$\nwhere $A\\in\\mathbb{R}^{m\\times 4}$, $y\\in\\mathbb{R}^{m}$, and $\\lambda > 0$; specify the stationarity condition using the subdifferential of $\\|x\\|_{F}$.\n\nRound no numerical quantities; provide exact expressions. Express the final answer as a single analytic expression for the dual norm value obtained in step 3.", "solution": "The problem is valid as it is mathematically well-posed, self-contained, and grounded in the established theory of submodular functions and convex optimization. We will address the four parts of the problem in sequence.\n\nThe ground set is $V = \\{1, 2, 3, 4\\}$, with groups $G_1 = \\{1, 2, 3\\}$ and $G_2 = \\{2, 4\\}$. The set function is $F: 2^V \\to \\mathbb{R}_{+}$ defined by $F(S) = 3\\,\\mathbf{1}\\{S\\cap G_1 \\neq\\emptyset\\} + 2\\,\\mathbf{1}\\{S\\cap G_2 \\neq\\emptyset\\}$.\n\n1.  We must establish that $F$ is a nondecreasing submodular function and that $F(\\emptyset) = 0$.\n\nFirst, we evaluate $F$ on the empty set $\\emptyset$:\n$$\nF(\\emptyset) = 3\\,\\mathbf{1}\\{\\emptyset \\cap G_1 \\neq \\emptyset\\} + 2\\,\\mathbf{1}\\{\\emptyset \\cap G_2 \\neq \\emptyset\\} = 3\\,\\mathbf{1}\\{\\emptyset \\neq \\emptyset\\} + 2\\,\\mathbf{1}\\{\\emptyset \\neq \\emptyset\\} = 3(0) + 2(0) = 0\n$$\nSo, $F(\\emptyset)=0$ is confirmed.\n\nNext, we establish that $F$ is nondecreasing. A set function $F$ is nondecreasing if for any two sets $S \\subseteq T \\subseteq V$, we have $F(S) \\leq F(T)$.\nLet $S \\subseteq T$. For any group $G_i$, if $S \\cap G_i \\neq \\emptyset$, then it must be that $T \\cap G_i \\neq \\emptyset$, because $S \\cap G_i \\subseteq T \\cap G_i$. This implies that the indicator function value cannot decrease: $\\mathbf{1}\\{S \\cap G_i \\neq \\emptyset\\} \\leq \\mathbf{1}\\{T \\cap G_i \\neq \\emptyset\\}$ for $i \\in \\{1, 2\\}$. Since the weights $w_1 = 3$ and $w_2 = 2$ are positive, we can write:\n$$\nF(S) = 3\\,\\mathbf{1}\\{S \\cap G_1 \\neq \\emptyset\\} + 2\\,\\mathbf{1}\\{S \\cap G_2 \\neq \\emptyset\\} \\leq 3\\,\\mathbf{1}\\{T \\cap G_1 \\neq \\emptyset\\} + 2\\,\\mathbf{1}\\{T \\cap G_2 \\neq \\emptyset\\} = F(T)\n$$\nThus, $F$ is a nondecreasing function.\n\nFinally, we establish submodularity. A set function $F$ is submodular if for any two sets $S, T \\subseteq V$, the following inequality holds: $F(S) + F(T) \\geq F(S \\cup T) + F(S \\cap T)$.\nThe function $F$ is a conic combination of two simpler set functions, $F(S) = 3 F_1(S) + 2 F_2(S)$, where $F_i(S) = \\mathbf{1}\\{S \\cap G_i \\neq \\emptyset\\}$. Since the set of submodular functions is a convex cone, if we can show that $F_1$ and $F_2$ are submodular, then $F$ must also be submodular.\nLet's prove that a general function of the form $g(S) = \\mathbf{1}\\{S \\cap G \\neq \\emptyset\\}$ is submodular for any fixed group $G \\subseteq V$. We need to show $g(S) + g(T) \\geq g(S \\cup T) + g(S \\cap T)$.\nLet $A = S \\cap G$ and $B = T \\cap G$. The inequality becomes:\n$$\n\\mathbf{1}\\{A \\neq \\emptyset\\} + \\mathbf{1}\\{B \\neq \\emptyset\\} \\geq \\mathbf{1}\\{(S \\cup T) \\cap G \\neq \\emptyset\\} + \\mathbf{1}\\{(S \\cap T) \\cap G \\neq \\emptyset\\}\n$$\nUsing the distributive property of set operations, $(S \\cup T) \\cap G = (S \\cap G) \\cup (T \\cap G) = A \\cup B$, and $(S \\cap T) \\cap G = (S \\cap G) \\cap (T \\cap G) = A \\cap B$. The inequality is thus:\n$$\n\\mathbf{1}\\{A \\neq \\emptyset\\} + \\mathbf{1}\\{B \\neq \\emptyset\\} \\geq \\mathbf{1}\\{A \\cup B \\neq \\emptyset\\} + \\mathbf{1}\\{A \\cap B \\neq \\emptyset\\}\n$$\nWe check this by cases:\n- Case 1: $A = \\emptyset$ and $B = \\emptyset$. Then $A \\cup B = \\emptyset$ and $A \\cap B = \\emptyset$. The inequality is $0 + 0 \\geq 0 + 0$, which is true.\n- Case 2: $A \\neq \\emptyset$ and $B = \\emptyset$. Then $A \\cup B = A \\neq \\emptyset$ and $A \\cap B = \\emptyset$. The inequality is $1 + 0 \\geq 1 + 0$, which is true. The case $A = \\emptyset$ and $B \\neq \\emptyset$ is symmetric.\n- Case 3: $A \\neq \\emptyset$ and $B \\neq \\emptyset$. Then $A \\cup B \\neq \\emptyset$. The inequality is $1 + 1 \\geq 1 + \\mathbf{1}\\{A \\cap B \\neq \\emptyset\\}$. This simplifies to $1 \\geq \\mathbf{1}\\{A \\cap B \\neq \\emptyset\\}$, which is always true since the indicator function is at most $1$.\nSince the inequality holds in all cases, the function $g(S)$ is submodular. It follows that $F_1(S)$ and $F_2(S)$ are submodular, and therefore $F(S)$ is submodular.\n\n2.  We derive the Lovász extension $f:\\mathbb{R}^4 \\to \\mathbb{R}$ of $F$, and the corresponding symmetrized norm $\\|x\\|_F = f(|x|)$.\n\nThe Lovász extension of a submodular function $F$ with $F(\\emptyset)=0$ can be defined for a vector $u \\in \\mathbb{R}^n_+$ by ordering its components $u_{\\pi(1)} \\ge u_{\\pi(2)} \\ge \\dots \\ge u_{\\pi(n)} \\ge u_{\\pi(n+1)} = 0$. The extension is given by:\n$$\nf(u) = \\sum_{i=1}^{n} (u_{\\pi(i)} - u_{\\pi(i+1)}) F(S_i)\n$$\nwhere $S_i = \\{\\pi(1), \\dots, \\pi(i)\\}$. The Lovász extension is linear in $F$, so for $F = 3F_1 + 2F_2$, its extension is $f(u) = 3f_1(u) + 2f_2(u)$, where $f_i$ is the Lovász extension of $F_i(S) = \\mathbf{1}\\{S \\cap G_i \\neq \\emptyset\\}$.\n\nLet's derive the form of the Lovász extension $f_G$ for a general function $F_G(S) = \\mathbf{1}\\{S \\cap G \\neq \\emptyset\\}$ and $u \\in \\mathbb{R}^n_+$.\n$$\nf_G(u) = \\sum_{i=1}^{n} (u_{\\pi(i)} - u_{\\pi(i+1)}) \\mathbf{1}\\{S_i \\cap G \\neq \\emptyset\\}\n$$\nLet $k$ be the first index $i$ such that $S_i$ has a non-empty intersection with $G$. This means that $\\pi(k) \\in G$, but for all $j < k$, $\\pi(j) \\notin G$. For all $i \\ge k$, $S_i \\supseteq S_k$, so $S_i \\cap G \\neq \\emptyset$ and $\\mathbf{1}\\{S_i \\cap G \\neq \\emptyset\\} = 1$. For $i < k$, the indicator is $0$. The sum becomes a telescoping series:\n$$\nf_G(u) = \\sum_{i=k}^{n} (u_{\\pi(i)} - u_{\\pi(i+1)}) = (u_{\\pi(k)} - u_{\\pi(k+1)}) + \\dots + (u_{\\pi(n)} - u_{\\pi(n+1)}) = u_{\\pi(k)} - u_{\\pi(n+1)} = u_{\\pi(k)}\n$$\nThe component $u_{\\pi(k)}$ is the largest value among all components $u_j$ whose indices $j$ are in the group $G$. Therefore, $f_G(u) = \\max_{j \\in G} u_j$.\n\nApplying this result to our function $F$ for a vector $u \\in \\mathbb{R}^4_+$:\n$f(u) = 3 f_1(u) + 2 f_2(u) = 3 \\max_{j \\in G_1} u_j + 2 \\max_{j \\in G_2} u_j$.\nWith $G_1 = \\{1, 2, 3\\}$ and $G_2 = \\{2, 4\\}$, we have:\n$f(u) = 3 \\max(u_1, u_2, u_3) + 2 \\max(u_2, u_4)$.\n\nThe symmetrized norm $\\|x\\|_F$ is defined as the Lovász extension evaluated on the vector of absolute values of the components of $x$, i.e., $\\|x\\|_F = f(|x|)$, where $|x| = (|x_1|, |x_2|, |x_3|, |x_4|)^\\top$.\n$$\n\\|x\\|_F = 3 \\max(|x_1|, |x_2|, |x_3|) + 2 \\max(|x_2|, |x_4|)\n$$\n\n3.  We derive the dual norm $\\|z\\|_F^*$ for $z = (0, 1, 0, 0)^\\top$.\n\nThe dual norm is defined as $\\|z\\|_F^* = \\sup\\{z^\\top x : \\|x\\|_F \\leq 1\\}$. For the given $z$, this becomes:\n$$\n\\|z\\|_F^* = \\sup \\{ x_2 \\ : \\ 3 \\max(|x_1|, |x_2|, |x_3|) + 2 \\max(|x_2|, |x_4|) \\le 1 \\}\n$$\nWe want to maximize $x_2$ subject to the norm constraint. To do so, we can assume $x_2 > 0$, so $|x_2| = x_2$.\nLet's find an upper bound on $x_2$. The norm expression involves two terms containing $|x_2|$. We have the following inequalities:\n$\\max(|x_1|, |x_2|, |x_3|) \\ge |x_2|$\n$\\max(|x_2|, |x_4|) \\ge |x_2|$\nUsing these, we can establish a lower bound on the norm $\\|x\\|_F$:\n$$\n\\|x\\|_F = 3 \\max(|x_1|, |x_2|, |x_3|) + 2 \\max(|x_2|, |x_4|) \\ge 3|x_2| + 2|x_2| = 5|x_2|\n$$\nThe constraint $\\|x\\|_F \\le 1$ thus implies $5|x_2| \\le 1$, which means $|x_2| \\le \\frac{1}{5}$. This provides an upper bound for $x_2$ of $\\frac{1}{5}$.\n\nWe now check if this upper bound is attainable. To maximize $x_2$, we should make the other components $|x_1|, |x_3|, |x_4|$ as small as possible to relax the norm constraint. Let's set $x_1 = 0$, $x_3 = 0$, and $x_4 = 0$. The constraint becomes:\n$$\n3 \\max(0, |x_2|, 0) + 2 \\max(|x_2|, 0) \\le 1\n$$\nAssuming $x_2 > 0$:\n$$\n3x_2 + 2x_2 \\le 1 \\implies 5x_2 \\le 1 \\implies x_2 \\le \\frac{1}{5}\n$$\nThe maximum value of $x_2$ is $\\frac{1}{5}$, which is achieved at the point $x = (0, \\frac{1}{5}, 0, 0)^\\top$. This value matches the upper bound we derived.\nTherefore, the supremum is $\\frac{1}{5}$.\n\n4.  We write the Karush-Kuhn-Tucker (KKT) conditions for the optimization problem.\n\nThe problem is $\\min_{x\\in\\mathbb{R}^{4}} \\frac{1}{2}\\|y-Ax\\|_{2}^{2}+\\lambda\\|x\\|_{F}$.\nLet the objective function be $L(x) = g(x) + h(x)$, where $g(x) = \\frac{1}{2}\\|y-Ax\\|_{2}^{2}$ is smooth and convex, and $h(x) = \\lambda\\|x\\|_{F}$ is convex but non-smooth. This is an unconstrained convex optimization problem. The necessary and sufficient optimality condition (stationarity) is that the zero vector must belong to the subdifferential of the objective function at a minimizer $x^*$:\n$$\n0 \\in \\partial L(x^*)\n$$\nUsing the sum rule for subdifferentials, which applies here as $g(x)$ is differentiable:\n$$\n\\partial L(x^*) = \\nabla g(x^*) + \\partial h(x^*)\n$$\nThe gradient of the least-squares term $g(x) = \\frac{1}{2}(y-Ax)^\\top(y-Ax)$ is:\n$$\n\\nabla g(x) = -A^\\top(y-Ax) = A^\\top(Ax-y)\n$$\nThe subdifferential of the norm term $h(x) = \\lambda\\|x\\|_{F}$ is:\n$$\n\\partial h(x) = \\lambda \\partial \\|x\\|_F\n$$\nCombining these, the optimality condition at a solution $x^*$ is:\n$$\n0 \\in A^\\top(Ax^*-y) + \\lambda \\partial \\|x^*\\|_F\n$$\nThis can be rewritten as $A^\\top(y-Ax^*) \\in \\lambda \\partial \\|x^*\\|_F$. Let $s$ be a subgradient vector, $s \\in \\partial \\|x^*\\|_F$. Then the condition is $A^\\top(y-Ax^*) = \\lambda s$.\nThe subdifferential $\\partial \\|x^*\\|_F$ is defined in terms of the dual norm $\\|\\cdot\\|_F^*$:\n$$\n\\partial \\|x^*\\|_F = \\{ v \\in \\mathbb{R}^4 \\mid \\|v\\|_F^* \\le 1 \\text{ and } v^\\top x^* = \\|x^*\\|_F \\}\n$$\nTherefore, the complete KKT conditions are that there exists a vector $s \\in \\mathbb{R}^4$ such that at the minimizer $x^*$:\n1. Stationarity: $A^\\top(y-Ax^*) = \\lambda s$\n2. Subgradient conditions: $\\|s\\|_F^* \\leq 1$ and $s^\\top x^* = \\|x^*\\|_F$.\nThese conditions fully characterize the solution $x^*$.", "answer": "$$\n\\boxed{\\frac{1}{5}}\n$$", "id": "3483776"}, {"introduction": "With the theoretical groundwork in place, we now turn to solving a full-fledged optimization problem involving a submodular regularizer. This practice focuses on deriving and implementing proximal gradient algorithms, the workhorses of modern large-scale optimization. You will see how the proximal operator of the Lovász extension, a seemingly complex operation, can be elegantly reduced to a Euclidean projection onto the associated base polytope, making algorithms like ISTA and FISTA practical [@problem_id:3483796]. This exercise bridges the gap between the abstract theory of submodular functions and their concrete application in iterative signal recovery algorithms.", "problem": "Consider the composite convex minimization problem with a structured sparsity-inducing regularizer arising from a submodular function. Let $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^{m}$ be given, and let $f:2^{\\{1,\\dots,n\\}} \\to \\mathbb{R}_{+}$ be a normalized ($f(\\varnothing)=0$), nondecreasing (monotone) submodular set function with Lovász extension $\\hat{f}:\\mathbb{R}^{n} \\to \\mathbb{R}_{+}$. Consider the objective\n$$\nF(x) \\triangleq \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\,\\hat{f}(x),\n$$\nwith $\\lambda > 0$. \n\nTasks:\n1. Using only the definitions of convex differentiability, Lipschitz continuity of gradients, and the proximal operator, derive from first principles the Iterative Shrinkage-Thresholding Algorithm (ISTA) and the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) update rules for minimizing $F(x)$. Your derivation must start from the quadratic upper bound on a smooth function with $L$-Lipschitz gradient and the definition of the proximal mapping for a convex function.\n2. For a general monotone submodular function $f$, specify how to evaluate the proximal operator of the Lovász extension efficiently, expressed in terms of a projection onto a polyhedron associated with $f$. State the key geometric relation that reduces the proximal evaluation to a Euclidean projection, and identify the polyhedron explicitly.\n3. Now specialize to a cardinality-based monotone submodular function $f$ defined by $f(S) = \\sum_{k=1}^{|S|} v_{k}$, with nonincreasing weights $v_{1} \\geq v_{2} \\geq \\dots \\geq v_{n} \\geq 0$. In this case, the Lovász extension is the ordered weighted $\\ell_{1}$ norm $\\hat{f}(x) = \\sum_{k=1}^{n} v_{k} |x|_{(k)}$, where $|x|_{(1)} \\geq \\dots \\geq |x|_{(n)}$ denotes the decreasing rearrangement of the absolute entries of $x$. Explain the sorting-and-isotonic-regression procedure that yields the proximal operator $\\mathrm{prox}_{\\lambda \\hat{f}}(y)$ for any $y \\in \\mathbb{R}^{n}$ in $\\mathcal{O}(n \\log n)$ time, detailing the role of the pool-adjacent-violators algorithm.\n\nFinally, consider the concrete instance with $n=m=3$, $A=I_{3}$, $\\lambda=1$, weights $v=(2,1,0.5)$, and $b=(3,1,-2)^{\\top}$. Starting from $x^{0}=0$, perform one ISTA iteration with step size equal to the inverse of the Lipschitz constant of $\\nabla \\left(\\frac{1}{2}\\|Ax-b\\|_{2}^{2}\\right)$. Express the result $x^{1}$ as a row vector using a $3 \\times 1$ row matrix. No rounding is required, and no physical units are involved.", "solution": "The problem is well-posed, scientifically grounded in the field of convex optimization, and provides all necessary information for a unique solution. It is therefore deemed valid.\n\nThe objective function to minimize is of the composite form $F(x) = g(x) + h(x)$, where:\n- $g(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ is a smooth, convex, and differentiable function.\n- $h(x) = \\lambda \\,\\hat{f}(x)$ is a convex function, but generally non-differentiable. Since $f$ is a normalized, nondecreasing submodular set function, its Lovász extension $\\hat{f}$ is convex. As $\\lambda > 0$, $h(x)$ is also convex.\n\n**1. Derivation of ISTA and FISTA**\n\nThe core principle behind proximal gradient methods is to iteratively minimize the objective function by forming a local quadratic approximation of the smooth part, $g(x)$, while keeping the non-smooth part, $h(x)$, exact.\n\nThe gradient of $g(x)$ is $\\nabla g(x) = A^{\\top}(Ax - b)$. For $g(x)$ to have an $L$-Lipschitz continuous gradient, the following must hold for some constant $L > 0$:\n$$\n\\|\\nabla g(x) - \\nabla g(y)\\|_{2} \\leq L\\|x-y\\|_{2} \\quad \\forall x, y \\in \\mathbb{R}^{n}\n$$\nCalculating the difference of gradients:\n$$\n\\|\\nabla g(x) - \\nabla g(y)\\|_{2} = \\|A^{\\top}(Ax - b) - A^{\\top}(Ay - b)\\|_{2} = \\|A^{\\top}A(x-y)\\|_{2} \\leq \\|A^{\\top}A\\|_{2} \\|x-y\\|_{2}\n$$\nwhere $\\|A^{\\top}A\\|_{2}$ is the spectral norm of $A^{\\top}A$. Thus, the smallest possible Lipschitz constant is $L = \\|A^{\\top}A\\|_{2} = \\sigma_{\\max}(A)^2$, where $\\sigma_{\\max}(A)$ is the largest singular value of $A$.\n\nA fundamental property of a function with an $L$-Lipschitz gradient is the descent lemma, which provides a quadratic upper bound (a majorant) on the function:\n$$\ng(x) \\leq g(x^{k}) + \\langle \\nabla g(x^{k}), x - x^{k} \\rangle + \\frac{L}{2}\\|x - x^{k}\\|_{2}^{2}\n$$\nAt each iteration $k$, we find the next iterate $x^{k+1}$ by minimizing this upper bound on $g(x)$ plus the non-smooth term $h(x)$:\n$$\nx^{k+1} = \\underset{x}{\\arg\\min} \\left( g(x^{k}) + \\langle \\nabla g(x^{k}), x - x^{k} \\rangle + \\frac{L}{2}\\|x - x^{k}\\|_{2}^{2} + h(x) \\right)\n$$\nWe can discard terms that are constant with respect to $x$ (i.e., $g(x^k)$ and terms involving only $x^k$) and complete the square for the remaining terms involving $x$:\n\\begin{align*}\nx^{k+1} &= \\underset{x}{\\arg\\min} \\left( \\langle \\nabla g(x^{k}), x \\rangle + \\frac{L}{2}\\|x - x^{k}\\|_{2}^{2} + h(x) \\right) \\\\\n&= \\underset{x}{\\arg\\min} \\left( \\frac{L}{2} \\|x\\|_{2}^{2} - L \\langle x, x^{k} \\rangle + \\langle \\nabla g(x^{k}), x \\rangle + h(x) \\right) \\\\\n&= \\underset{x}{\\arg\\min} \\left( \\frac{L}{2} \\|x\\|_{2}^{2} - \\langle x, L x^{k} - \\nabla g(x^{k}) \\rangle + h(x) \\right) \\\\\n&= \\underset{x}{\\arg\\min} \\left( \\frac{L}{2} \\left\\| x - \\left(x^{k} - \\frac{1}{L}\\nabla g(x^{k})\\right) \\right\\|_{2}^{2} + h(x) \\right)\n\\end{align*}\nThis minimization problem is the definition of the proximal operator of $h(x)$ with parameter $1/L$. The proximal operator of a convex function $\\phi$ is defined as $\\mathrm{prox}_{\\phi}(y) \\triangleq \\underset{x}{\\arg\\min} \\left( \\frac{1}{2}\\|x-y\\|_{2}^{2} + \\phi(x) \\right)$.\nIn our case, the objective is equivalent to $\\underset{x}{\\arg\\min} \\left( \\frac{1}{2}\\left\\| x - \\left(x^{k} - \\frac{1}{L}\\nabla g(x^{k})\\right) \\right\\|_{2}^{2} + \\frac{1}{L}h(x) \\right)$.\nLetting $\\eta = 1/L$ be the step size, the update is:\n$$\nx^{k+1} = \\mathrm{prox}_{\\eta h}\\left(x^{k} - \\eta \\nabla g(x^{k})\\right)\n$$\nSubstituting $h(x) = \\lambda \\hat{f}(x)$, we get the **Iterative Shrinkage-Thresholding Algorithm (ISTA)** update rule:\n$$\nx^{k+1} = \\mathrm{prox}_{\\eta \\lambda \\hat{f}}\\left(x^{k} - \\eta A^{\\top}(Ax^{k} - b)\\right)\n$$\nThe **Fast Iterative Shrinkage-Thresholding Algorithm (FISTA)** introduces a momentum term to accelerate convergence. It applies the same proximal gradient step but to an extrapolated point $y^k$ rather than the previous iterate $x^{k-1}$. The update rules are:\nInitialize $x^0$, let $y^1 = x^0$, $t_1 = 1$. For $k \\geq 1$:\n\\begin{enumerate}\n    \\item Perform a proximal gradient step at $y^{k}$: $x^{k} = \\mathrm{prox}_{\\eta \\lambda \\hat{f}}\\left(y^{k} - \\eta \\nabla g(y^{k})\\right)$\n    \\item Update the momentum parameter: $t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2}$\n    \\item Form the next extrapolated point: $y^{k+1} = x^{k} + \\frac{t_k - 1}{t_{k+1}}(x^{k} - x^{k-1})$\n\\end{enumerate}\n\n**2. Proximal Operator of the Lovász Extension**\n\nThe task is to evaluate $\\mathrm{prox}_{\\gamma \\hat{f}}(y) = \\underset{x}{\\arg\\min} \\left( \\frac{1}{2}\\|x-y\\|_{2}^{2} + \\gamma \\hat{f}(x) \\right)$, where $\\gamma = \\eta\\lambda$.\nThe Lovász extension $\\hat{f}(x)$ of a submodular function $f$ can be expressed as the support function of the associated base polyhedron $B(f)$, i.e., $\\hat{f}(x) = \\max_{s \\in B(f)} s^{\\top}x$. The base polyhedron is defined as:\n$$\nB(f) = \\left\\{ s \\in \\mathbb{R}^n \\mid \\sum_{i=1}^n s_i = f(\\{1,\\dots,n\\}), \\text{ and } \\forall S \\subseteq \\{1,\\dots,n\\}, \\sum_{i \\in S} s_i \\le f(S) \\right\\}\n$$\nSubstituting this into the proximal problem, we get a minimax problem:\n$$\n\\mathrm{prox}_{\\gamma \\hat{f}}(y) = \\underset{x}{\\arg\\min} \\left( \\frac{1}{2}\\|x-y\\|_{2}^{2} + \\gamma \\max_{s \\in B(f)} s^{\\top}x \\right)\n$$\nSince the objective is convex in $x$ and concave (linear) in $s$, and the domain $B(f)$ is compact, we can swap the min and max operators (Sion's Minimax Theorem):\n$$\n\\max_{s \\in B(f)} \\min_{x} \\left( \\frac{1}{2}\\|x-y\\|_{2}^{2} + \\gamma s^{\\top}x \\right)\n$$\nThe inner minimization with respect to $x$ is unconstrained. Setting the gradient to zero gives $x-y+\\gamma s = 0$, which implies the minimizer is $x^*(s) = y - \\gamma s$. Substituting this back into the expression:\n$$\n\\max_{s \\in B(f)} \\left( \\frac{1}{2}\\|(y - \\gamma s) - y\\|_{2}^{2} + \\gamma s^{\\top}(y - \\gamma s) \\right) = \\max_{s \\in B(f)} \\left( \\frac{\\gamma^2}{2}\\|s\\|_{2}^{2} + \\gamma s^{\\top}y - \\gamma^2 \\|s\\|_{2}^{2} \\right) = \\max_{s \\in B(f)} \\left( \\gamma s^{\\top}y - \\frac{\\gamma^2}{2}\\|s\\|_{2}^{2} \\right)\n$$\nFinding the maximizing $s$ is equivalent to minimizing its negative:\n$$\n\\underset{s \\in B(f)}{\\arg\\min} \\left( \\frac{\\gamma^2}{2}\\|s\\|_{2}^{2} - \\gamma s^{\\top}y \\right) = \\underset{s \\in B(f)}{\\arg\\min} \\frac{\\gamma^2}{2} \\left\\| s - \\frac{y}{\\gamma} \\right\\|_{2}^{2}\n$$\nThe solution, let's call it $s^*$, is the Euclidean projection of the vector $y/\\gamma$ onto the base polyhedron $B(f)$:\n$$\ns^* = \\mathrm{proj}_{B(f)}\\left(\\frac{y}{\\gamma}\\right)\n$$\nThe key geometric relation is that the solution to the proximal problem, $x^*$, is then given by the primal-dual relationship we found earlier:\n$$\nx^* = y - \\gamma s^* \\quad \\implies \\quad \\mathrm{prox}_{\\gamma \\hat{f}}(y) = y - \\gamma \\, \\mathrm{proj}_{B(f)}\\left(\\frac{y}{\\gamma}\\right)\n$$\nThus, evaluating the proximal operator of the Lovász extension is reduced to a Euclidean projection onto the corresponding base polyhedron.\n\n**3. Proximal Operator for Cardinality-Based Functions**\n\nFor the specific case $f(S) = \\sum_{k=1}^{|S|} v_{k}$ with $v_{1} \\geq v_{2} \\geq \\dots \\geq v_{n} \\geq 0$, the Lovász extension is the ordered weighted $\\ell_1$ norm (also known as the OWL norm): $\\hat{f}(x) = \\sum_{k=1}^{n} v_{k} |x|_{(k)}$, where $|x|_{(k)}$ is the $k$-th largest absolute value of the entries in $x$.\n\nWe need to compute $x^* = \\mathrm{prox}_{\\lambda \\hat{f}}(y) = \\underset{x}{\\arg\\min} \\frac{1}{2}\\|x-y\\|_2^2 + \\lambda \\sum_{k=1}^n v_k |x|_{(k)}$.\nDue to the symmetry of the regularizer, the solution $x^*$ must satisfy $\\mathrm{sign}(x^*_i) = \\mathrm{sign}(y_i)$ for $y_i \\ne 0$, and the ordering of the absolute values of the solution must match the ordering of the absolute values of $y$. That is, if $|y_i| \\ge |y_j|$, then $|x^*_i| \\ge |x^*_j|$.\n\nThis allows us to solve the problem on the absolute values of $y$ after sorting them, and then reconstruct the solution. The procedure is as follows:\n1.  Let $u = |y|$ be the vector of absolute values of $y$. Let $\\sigma = \\mathrm{sign}(y)$ be the vector of signs.\n2.  Find the permutation $\\pi$ that sorts $u$ in descending order, such that $u_{\\pi(1)} \\ge u_{\\pi(2)} \\ge \\dots \\ge u_{\\pi(n)}$. Let $u_{\\text{sorted}} = (u_{\\pi(1)}, \\dots, u_{\\pi(n)})$.\n3.  The optimization problem over the sorted non-negative solution values $z=(z_1, \\dots, z_n)$ becomes:\n    $$\n    \\underset{z_1 \\ge \\dots \\ge z_n \\ge 0}{\\min} \\frac{1}{2}\\sum_{k=1}^n (z_k - u_{\\pi(k)})^2 + \\lambda \\sum_{k=1}^n v_k z_k\n    $$\n    This is equivalent to finding the projection of a vector onto the cone of non-negative, nonincreasing vectors. The objective can be rewritten by completing the square:\n    $$\n    \\underset{z_1 \\ge \\dots \\ge z_n \\ge 0}{\\min} \\frac{1}{2}\\sum_{k=1}^n (z_k - (u_{\\pi(k)} - \\lambda v_k))^2\n    $$\n4.  Let $c_k = u_{\\pi(k)} - \\lambda v_k$. We want to find the vector $z$ that is closest to $c=(c_1, \\dots, c_n)$ under the constraints $z_1 \\ge \\dots \\ge z_n \\ge 0$. This is a problem of isotonic regression with non-negativity constraints.\n5.  This is solved in two steps. First, we find the nonincreasing vector $z'$ closest to $c$. This is a standard isotonic regression problem that can be solved in $\\mathcal{O}(n)$ time using the Pool-Adjacent-Violators Algorithm (PAVA). PAVA iterates through the vector $c$. If it finds a component $c_k$ that violates the nonincreasing order (i.e., $c_k < c_{k+1}$), it averages the block of components involved in the violation and replaces each component in that block with the average. This process is repeated until no violations remain.\n6.  Second, the non-negativity constraint $z_k \\ge 0$ is imposed. Since the projection onto the non-negative orthant of a vector that is already nonincreasing preserves the order, the final solution for the sorted values is $z_k = \\max(0, z'_k)$.\n7.  The final solution $x^*$ is reconstructed by re-applying the permutation and signs: create a vector $x'_{\\text{abs}}$ such that $(x'_{\\text{abs}})_{\\pi(k)} = z_k$ for $k=1, \\dots, n$. Then the final result is $x^*_i = \\sigma_i (x'_{\\text{abs}})_i$.\n\nThe dominant computational cost is the initial sorting of $|y|$, which takes $\\mathcal{O}(n \\log n)$ time.\n\n**4. Concrete Instance Calculation**\n\nGiven: $n=m=3$, $A=I_{3}$, $\\lambda=1$, $v=(2,1,0.5)$, $b=(3,1,-2)^{\\top}$, and $x^{0}=(0,0,0)^{\\top}$. We perform one ISTA iteration.\n\nThe objective is $F(x) = \\frac{1}{2}\\|x-b\\|_2^2 + \\hat{f}(x)$.\nThe smooth part is $g(x) = \\frac{1}{2}\\|x-b\\|_2^2$, with gradient $\\nabla g(x) = x-b$.\nThe Lipschitz constant of $\\nabla g(x)$ is $L = \\|I_3\\|_2 = 1$.\nThe step size is $\\eta = 1/L = 1$.\n\nThe first ISTA iteration is:\n$x^{1} = \\mathrm{prox}_{\\eta \\lambda \\hat{f}}(x^0 - \\eta \\nabla g(x^0))$\n$x^{1} = \\mathrm{prox}_{1 \\cdot 1 \\cdot \\hat{f}}(x^0 - 1 \\cdot (x^0 - b)) = \\mathrm{prox}_{\\hat{f}}(b)$.\nWe need to calculate $\\mathrm{prox}_{\\hat{f}}((3,1,-2)^{\\top})$. Let $y=(3,1,-2)^{\\top}$.\n\nWe use the procedure from part 3:\n1.  Absolute values: $u = |y| = (|3|, |1|, |-2|) = (3, 1, 2)$.\n    Signs: $\\sigma = \\mathrm{sign}(y) = (1, 1, -1)$.\n2.  Sorted absolute values: The sorted values of $u$ are $(3, 2, 1)$. The corresponding original indices are $(1, 3, 2)$. So, $|y|_{(1)} = 3$, $|y|_{(2)} = 2$, $|y|_{(3)} = 1$.\n3.  The weights vector is $v=(v_1, v_2, v_3) = (2, 1, 0.5)$. $\\lambda=1$.\n4.  Form the vector $c = (|y|_{(k)} - \\lambda v_k)_{k=1,2,3}$:\n    $c_1 = |y|_{(1)} - \\lambda v_1 = 3 - 1 \\cdot 2 = 1$.\n    $c_2 = |y|_{(2)} - \\lambda v_2 = 2 - 1 \\cdot 1 = 1$.\n    $c_3 = |y|_{(3)} - \\lambda v_3 = 1 - 1 \\cdot 0.5 = 0.5$.\n    So, $c = (1, 1, 0.5)$.\n5.  Perform isotonic regression. The vector $c$ is already nonincreasing ($1 \\ge 1 \\ge 0.5$), so PAVA returns $c$ itself. Let this be $z' = (1, 1, 0.5)$.\n6.  Project onto non-negative cone. All entries of $z'$ are non-negative, so $z=z'=(1, 1, 0.5)$. This is the vector of sorted absolute values of the solution.\n7.  Reconstruct the solution. The sorted values $z$ correspond to the original indices $(1, 3, 2)$.\n    - The largest absolute value, $z_1=1$, corresponds to index $1$.\n    - The second largest, $z_2=1$, corresponds to index $3$.\n    - The smallest, $z_3=0.5$, corresponds to index $2$.\n    So, the vector of absolute values of the solution is $|x^1| = (1, 0.5, 1)$.\n8.  Apply the signs $\\sigma=(1, 1, -1)$:\n    $x^1_1 = 1 \\cdot 1 = 1$.\n    $x^1_2 = 1 \\cdot 0.5 = 0.5$.\n    $x^1_3 = -1 \\cdot 1 = -1$.\nThe result of one ISTA iteration is $x^1 = (1, 0.5, -1)^{\\top}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 0.5 & -1\n\\end{pmatrix}\n}\n$$", "id": "3483796"}, {"introduction": "Proximal methods on the primal problem are powerful, but an alternative and equally insightful approach is to work in the dual space. This practice explores the Fenchel dual of the structured sparsity problem, revealing it to be a constrained quadratic program over the base polytope. You will implement the Frank-Wolfe algorithm—a classic method that capitalizes on the geometry of the constraint set by iteratively solving a much simpler linear minimization oracle—to solve this dual problem [@problem_id:3483789]. This provides a different algorithmic perspective and deepens the understanding of the central role played by the base polytope's geometry and extreme points.", "problem": "Consider a finite ground set with $n$ elements and a normalized, monotone submodular set function $F:2^{\\{1,\\dots,n\\}}\\to\\mathbb{R}_{+}$ satisfying $F(\\emptyset)=0$. The base polytope of $F$ is defined as the convex set\n$$\nB(F)=\\left\\{ s\\in\\mathbb{R}^{n} \\mid \\sum_{i\\in S} s_i \\le F(S) \\ \\text{for all} \\ S\\subseteq\\{1,\\dots,n\\}, \\ \\sum_{i=1}^{n} s_i = F(\\{1,\\dots,n\\}) \\right\\}.\n$$\nThe Lovász extension $\\Omega_F$ of $F$ is the convex function on $\\mathbb{R}^{n}$ given by\n$$\n\\Omega_F(x)=\\max_{s\\in B(F)} s^\\top x.\n$$\nIn structured sparsity for compressed sensing, a widely used regularizer is $\\lambda \\,\\Omega_F(|x|)$, where $|x|$ denotes the componentwise absolute value and $\\lambda > 0$. Consider the convex optimization problem\n$$\n\\min_{x\\in\\mathbb{R}^{n}} \\ \\frac{1}{2}\\,\\|y-Ax\\|_2^2 + \\lambda \\,\\Omega_F(|x|),\n$$\nwhere $y\\in\\mathbb{R}^{m}$ is a given observation, $A\\in\\mathbb{R}^{m\\times n}$ is a measurement matrix, and $\\|\\cdot\\|_2$ denotes the Euclidean norm.\n\nTasks:\n1) Starting from first principles of convex analysis (Fenchel conjugates and Fenchel duality) and the definition of the base polytope as the support of the Lovász extension, derive the Fenchel dual of the above problem. Show that the dual problem can be written as\n$$\n\\min_{u\\in\\mathbb{R}^{m}} \\ \\frac{1}{2}\\,\\|u-y\\|_2^2 \\quad \\text{subject to} \\quad A^\\top u \\in \\lambda\\, B(F).\n$$\nThen, under the additional structural assumption that $A$ has orthonormal rows, that is, $A A^\\top = I_m$, reduce the dual to the following equivalent projection problem on the base polytope:\n$$\n\\min_{s\\in \\lambda\\, B(F)} \\ \\frac{1}{2}\\,\\|s - A^\\top y\\|_2^2,\n$$\nand explain why solving this projection yields $A^\\top u^\\star = s^\\star$, where $u^\\star$ is dual-optimal and $s^\\star$ is the projection.\n\n2) Specialize $F$ to the concave cardinality-based function $F(S) = \\alpha \\,\\sqrt{|S|}$ with $\\alpha>0$. For this $F$, the extreme points of $B(F)$ are obtained by the greedy algorithm: for any weight vector $w\\in\\mathbb{R}^n$, sort indices by non-increasing $w$, and assign extreme-point components $s_{\\pi(k)} = \\alpha\\,(\\sqrt{k}-\\sqrt{k-1})$ for $k=1,\\dots,n$, where $\\pi$ is the sorting permutation and $\\sqrt{0}:=0$. Use this greedy extreme-point oracle to implement a Frank–Wolfe method to solve the projection problem\n$$\n\\min_{s\\in \\lambda\\, B(F)} \\ \\frac{1}{2}\\,\\|s - v\\|_2^2,\n$$\nwith $v=A^\\top y$. Use exact line search at each Frank–Wolfe iteration and the greedy oracle for the linear minimization subproblem over $B(F)$.\n\n3) After computing the projection $s^\\star$, recover a primal candidate $x^\\star$ using the proximal identity for the Lovász extension in the special case $A A^\\top = I_m$ and $A=I_n$ (the identity): $x^\\star = y - s^\\star$. Evaluate the primal objective value\n$$\n\\frac{1}{2}\\,\\|y - x^\\star\\|_2^2 + \\lambda\\,\\Omega_F(|x^\\star|),\n$$\nwhere $\\Omega_F(|x|)$ for cardinality-based $F$ can be computed as\n$$\n\\Omega_F(|x|) \\ = \\ \\alpha \\sum_{k=1}^{n} \\big(\\sqrt{k}-\\sqrt{k-1}\\big) \\, |x|_{(k)},\n$$\nwith $|x|_{(k)}$ denoting the $k$-th largest component of $|x|$.\n\nImplement the described algorithm in a complete, runnable program as specified below. Use $A=I_n$ and $\\alpha=1$ in all tests. Your program must:\n- Implement the Frank–Wolfe projection onto $\\lambda B(F)$ for $F(S)=\\sqrt{|S|}$ using the greedy extreme-point oracle.\n- Compute $x^\\star$ and the primal objective value for each test case.\n\nTest suite:\n- Case 1 (general “happy path”): $n=8$, $A=I_8$, $\\lambda=0.9$, $y=[1.5,0.8,0.3,2.0,0.1,1.2,0.5,0.7]$.\n- Case 2 (boundary condition with zero observation): $n=8$, $A=I_8$, $\\lambda=1.0$, $y=[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]$.\n- Case 3 (extreme-point input): $n=8$, $A=I_8$, $\\lambda=0.8$, $y$ equals the extreme point of $\\lambda B(F)$ produced by the greedy oracle when the weight vector is $w=[8,7,6,5,4,3,2,1]$.\n- Case 4 (small dimension with varied magnitudes): $n=4$, $A=I_4$, $\\lambda=0.7$, $y=[0.2,3.0,1.0,0.4]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each result must be the primal objective value (a float) for the corresponding test case, computed to full precision of Python’s default float type. For example, the output format must be exactly like: “[result1,result2,result3,result4]”. No physical units are involved. Angles are not used. Percentages are not used.", "solution": "The problem will be addressed in three parts as requested. First, a derivation of the dual problem and its reduction under an orthonormality assumption will be presented. Second, the Frank-Wolfe algorithm for solving the resulting projection problem will be detailed. Third, the process for recovering the primal solution and evaluating the objective function will be described.\n\n### Part 1: Duality and Problem Reduction\n\nThe primal problem is a convex optimization problem of the form:\n$$\n\\min_{x\\in\\mathbb{R}^{n}} \\ \\frac{1}{2}\\,\\|y-Ax\\|_2^2 + \\lambda \\,\\Omega_F(|x|)\n$$\nThis problem can be formulated as $\\min_x g(Ax) + f(x)$, where $g(z) = \\frac{1}{2}\\|y-z\\|_2^2$ and $f(x) = \\lambda \\Omega_F(|x|)$. We can derive its Fenchel dual, which is given by $\\min_u g^*(u) + f^*(-A^\\top u)$.\n\nLet's compute the Fenchel conjugates $g^*$ and $f^*$.\n\nThe conjugate of $g(z) = \\frac{1}{2}\\|y-z\\|_2^2$ is:\n$$\ng^*(u) = \\sup_{z \\in \\mathbb{R}^m} \\left( u^\\top z - \\frac{1}{2}\\|y-z\\|_2^2 \\right)\n$$\nThe maximizer is found by setting the gradient with respect to $z$ to zero: $u - (-(y-z)) = u + y - z = 0$, which gives $z = y+u$. Substituting this back into the expression yields:\n$$\ng^*(u) = u^\\top(y+u) - \\frac{1}{2}\\|y-(y+u)\\|_2^2 = u^\\top y + \\|u\\|_2^2 - \\frac{1}{2}\\|u\\|_2^2 = \\frac{1}{2}\\|u\\|_2^2 + u^\\top y\n$$\nThis can be written as $\\frac{1}{2}\\|y+u\\|_2^2 - \\frac{1}{2}\\|y\\|_2^2$.\n\nThe conjugate of $f(x) = \\lambda \\Omega_F(|x|)$ is more subtle. The Lovász extension is defined as $\\Omega_F(|x|) = \\max_{s \\in B(F)} s^\\top |x|$. Since vectors $s \\in B(F)$ have non-negative components for a monotone function $F$, this is $\\max_{s \\in B(F)} \\sum_i s_i |x_i|$. Thus, $\\lambda \\Omega_F(|x|) = \\max_{s \\in \\lambda B(F)} s^\\top |x|$.\nThe derivation of the dual constraint $A^\\top u \\in \\lambda B(F)$ from the regularizer $\\lambda \\Omega_F(|x|)$ is non-trivial. A rigorous derivation shows that the corresponding dual constraint involves a set related to the base polytope, but not the base polytope itself. Specifically, the conjugate of $x \\mapsto \\lambda \\Omega_F(|x|)$ is the indicator function of the set $\\{v \\mid \\sum_{i \\in S} |v_i| \\le \\lambda F(S) \\text{ for all } S\\}$. However, the problem statement implies a direct duality where the constraint set is $\\lambda B(F)$. This specific duality holds if the regularizer is $\\lambda \\Omega_F(x)$ and the variable $x$ is constrained to be non-negative. It is a common simplification in the literature to use this duality pair. We will follow the problem's prompt and demonstrate that the dual of the stated dual problem recovers a primal form closely related to the original.\n\nLet's start from the proposed dual problem and derive its dual, which by strong duality (the problem is convex and satisfies Slater's condition) will be the primal problem.\nThe dual problem is:\n$$\n\\min_{u\\in\\mathbb{R}^{m}} \\ \\frac{1}{2}\\,\\|u-y\\|_2^2 \\quad \\text{subject to} \\quad A^\\top u \\in \\lambda\\, B(F)\n$$\nWe introduce a variable $s = A^\\top u$ and use an indicator function $\\iota_{\\lambda B(F)}(s)$ for the constraint set.\n$$\n\\min_{u,s} \\ \\frac{1}{2}\\,\\|u-y\\|_2^2 + \\iota_{\\lambda B(F)}(s) \\quad \\text{subject to} \\quad s = A^\\top u\n$$\nThe Lagrangian for this problem, with Lagrange multiplier $-x \\in \\mathbb{R}^n$, is:\n$$\nL(u,s;x) = \\frac{1}{2}\\|u-y\\|_2^2 + \\iota_{\\lambda B(F)}(s) - x^\\top(s-A^\\top u)\n$$\nThe dual function is $q(x) = \\inf_{u,s} L(u,s;x)$. We can separate the infima over $u$ and $s$:\n$$\nq(x) = \\inf_{u} \\left(\\frac{1}{2}\\|u-y\\|_2^2 + (Ax)^\\top u\\right) + \\inf_{s} \\left(\\iota_{\\lambda B(F)}(s) - x^\\top s\\right)\n$$\nThe infimum over $u$ is found by setting the gradient to zero: $u-y+Ax = 0 \\implies u=y-Ax$. The minimum value is $\\frac{1}{2}\\|-Ax\\|_2^2 + (Ax)^\\top(y-Ax) = (Ax)^\\top y - \\frac{1}{2}\\|Ax\\|_2^2$.\nThe infimum over $s$ is $-\\sup_{s \\in \\lambda B(F)} x^\\top s = -\\lambda \\sup_{s' \\in B(F)} x^\\top s'$. By definition, this is $-\\lambda \\Omega_F(x)$.\nThe resulting dual function is $q(x) = (Ax)^\\top y - \\frac{1}{2}\\|Ax\\|_2^2 - \\lambda \\Omega_F(x)$.\nThe dual-of-the-dual problem is $\\max_x q(x)$, which is equivalent to $\\min_x -q(x)$:\n$$\n\\min_x \\ \\frac{1}{2}\\|Ax\\|_2^2 - (Ax)^\\top y + \\lambda \\Omega_F(x)\n$$\nCompleting the square for the quadratic term gives $\\frac{1}{2}\\|Ax-y\\|_2^2 - \\frac{1}{2}\\|y\\|_2^2$. Dropping the constant term $-\\frac{1}{2}\\|y\\|_2^2$, we recover the primal problem $\\min_x \\frac{1}{2}\\|y-Ax\\|_2^2 + \\lambda \\Omega_F(x)$. This matches the original problem up to the absolute value inside $\\Omega_F$. We proceed assuming this derivation is what was intended.\n\nNow, we reduce the dual problem under the assumption that $A$ has orthonormal rows, i.e., $AA^\\top = I_m$.\nThe dual problem is $\\min_{u} \\frac{1}{2}\\|u-y\\|_2^2$ subject to $A^\\top u \\in \\lambda B(F)$.\nThis can be viewed as a nested optimization:\n$$\n\\min_{s \\in \\lambda B(F)} \\left( \\min_{u \\text{ s.t. } A^\\top u=s} \\frac{1}{2}\\|u-y\\|_2^2 \\right)\n$$\nThe inner problem is the projection of $y$ onto the affine subspace $\\{u \\in \\mathbb{R}^m \\mid A^\\top u = s\\}$. The optimal solution $u^*$ is characterized by the condition that $u^*-y$ is orthogonal to the subspace's parallel component, which is $\\ker(A^\\top)$. Thus, $u^*-y \\in (\\ker(A^\\top))^\\perp = \\text{range}(A)$.\nTherefore, $u^* - y = Az$ for some $z \\in \\mathbb{R}^n$. So, $u^* = y + Az$.\nPlugging this into the constraint $A^\\top u^* = s$:\n$$\nA^\\top(y+Az) = s \\implies A^\\top y + A^\\top A z = s\n$$\nThe objective value at this optimum is $\\frac{1}{2}\\|u^*-y\\|_2^2 = \\frac{1}{2}\\|Az\\|_2^2 = \\frac{1}{2} z^\\top A^\\top A z$.\nThe condition $AA^\\top=I_m$ implies that $A$ is an isometry from the row space of $A$ (which is range$(A^\\top)$) to $\\mathbb{R}^m$.\nLet's consider the objective of the equivalent problem proposed: $\\frac{1}{2}\\|s-A^\\top y\\|_2^2$.\n$$\n\\frac{1}{2}\\|s - A^\\top y\\|_2^2 = \\frac{1}{2}\\|A^\\top u - A^\\top y\\|_2^2 = \\frac{1}{2}\\|A^\\top(u-y)\\|_2^2\n$$\nSince $AA^\\top=I_m$, the matrix $A^\\top A$ is an orthogonal projector onto the row space of $A$, range$(A^\\top)$.\nAny vector $v \\in \\mathbb{R}^m$ can be decomposed as $v = Av_1 + v_2$ where $v_1 \\in \\text{range}(A^\\top)$ and $v_2 \\in \\ker(A)$. Then $A^\\top v = A^\\top A v_1$. $\\|A^\\top v\\|_2^2 = \\|A^\\top A v_1\\|_2^2 = v_1^\\top A^\\top A A^\\top A v_1 = v_1^\\top A^\\top A v_1 = \\|Av_1\\|_2^2$.\nLet $u-y = w$. For any $u$ such that $A^\\top u = s$, we have $\\|A^\\top(u-y)\\|_2^2 = \\|s - A^\\top y\\|_2^2$.\nThe objective $\\|u-y\\|_2^2$ can be decomposed using the projection onto range$(A)$: since $AA^\\top=I_m$, range$(A)=\\mathbb{R}^m$, so this decomposition is trivial.\nA more direct proof: Let $s^\\star = \\text{argmin}_{s \\in \\lambda B(F)} \\|s - A^\\top y\\|_2^2$. Let $u^\\star_c = y - A(A^\\top y - s^\\star)$.\nLet's check feasibility of $u^\\star_c$: $A^\\top u^\\star_c = A^\\top y - A^\\top A(A^\\top y - s^\\star)$. This does not trivially simplify to $s^\\star$.\nThe equivalence is a known result. The intuition is that since $A$ with orthonormal rows preserves norms for vectors in its row space, minimizing $\\|u-y\\|$ subject to a constraint on $A^\\top u$ is equivalent to minimizing the distance between the constrained vector $s=A^\\top u$ and the transformed target $A^\\top y$. Let $u_{opt}$ solve the first problem and $s_{opt} = A^\\top u_{opt}$. Then $(u_{opt}-y)$ is orthogonal to $\\ker(A^\\top)$. So $u_{opt}-y \\in \\text{range}(A)$. So $u_{opt}-y = Az$ for some $z$. Then $A^\\top(u_{opt}-y) = A^\\top A z$. Hence $s_{opt}-A^\\top y = A^\\top A z$. Also $\\|u_{opt}-y\\|^2 = \\|Az\\|^2 = z^\\top A^\\top A z$. This line of reasoning is intricate. We accept the stated equivalence. Solving $\\min_{s\\in \\lambda\\, B(F)} \\ \\frac{1}{2}\\,\\|s - A^\\top y\\|_2^2$ yields a solution $s^\\star$. The corresponding dual-optimal $u^\\star$ satisfies $A^\\top u^\\star = s^\\star$.\n\n### Part 2: Frank-Wolfe Algorithm for Projection\n\nWe solve the convex problem $\\min_{s\\in \\mathcal{C}} f(s)$, where $f(s) = \\frac{1}{2}\\|s - v\\|_2^2$ and $\\mathcal{C} = \\lambda B(F)$, with $v = A^\\top y$. The Frank-Wolfe (FW) algorithm is well-suited for this, as projection onto $\\mathcal{C}$ is hard, but linear optimization over $\\mathcal{C}$ is easy via the provided greedy oracle.\n\nThe FW algorithm proceeds as follows:\n1.  **Initialization**: Choose an initial point $s^{(0)} \\in \\mathcal{C}$. A canonical choice is to solve the linear oracle with $v$ as the weight vector, i.e., $s^{(0)} = \\text{argmin}_{s \\in \\mathcal{C}} -s^\\top v = \\text{argmax}_{s \\in \\mathcal{C}} s^\\top v$.\n2.  **Iteration**: For $k=0, 1, 2, \\ldots$:\n    a.  **Compute Gradient**: The gradient of $f(s)$ at $s^{(k)}$ is $\\nabla f(s^{(k)}) = s^{(k)} - v$.\n    b.  **Solve Linear Subproblem**: Find the FW direction by finding the element in $\\mathcal{C}$ that is most aligned with the negative gradient. This involves solving a Linear Minimization Oracle (LMO):\n        $$\n        p^{(k)} = \\text{argmin}_{p \\in \\mathcal{C}} \\, p^\\top \\nabla f(s^{(k)}) = \\text{argmin}_{p \\in \\mathcal{C}} \\, p^\\top (s^{(k)}-v) = \\text{argmax}_{p \\in \\mathcal{C}} \\, p^\\top (v-s^{(k)})\n        $$\n        Since $\\mathcal{C} = \\lambda B(F)$, the optimum is achieved at an extreme point. We use the greedy oracle for $F(S)=\\alpha\\sqrt{|S|}$ with weights $w = v-s^{(k)}$. The oracle for $B(F)$ returns an extreme point $p'_{ext}$. We then have $p^{(k)} = \\lambda p'_{ext}$. The oracle works by sorting indices $\\pi$ such that $w_{\\pi(1)} \\ge w_{\\pi(2)} \\ge \\dots \\ge w_{\\pi(n)}$, and defining the components of $p'_{ext}$ as $p'_{\\pi(j)} = \\alpha(\\sqrt{j} - \\sqrt{j-1})$ for $j=1,\\dots,n$.\n    c.  **Exact Line Search**: Find the optimal step size $\\gamma_k \\in [0,1]$ that minimizes $f(s^{(k)} + \\gamma(p^{(k)}-s^{(k)}))$. Let $d^{(k)} = p^{(k)}-s^{(k)}$. We minimize with respect to $\\gamma$:\n        $$\n        \\min_{\\gamma \\in [0,1]} \\frac{1}{2} \\| s^{(k)} + \\gamma d^{(k)} - v \\|_2^2\n        $$\n        This is a simple quadratic in $\\gamma$. Taking the derivative and setting to zero gives:\n        $$\n        (s^{(k)} + \\gamma d^{(k)} - v)^\\top d^{(k)} = 0 \\implies \\gamma (d^{(k)})^\\top d^{(k)} + (s^{(k)}-v)^\\top d^{(k)} = 0\n        $$\n        The unconstrained optimal $\\gamma$ is $\\gamma^* = \\frac{(v-s^{(k)})^\\top d^{(k)}}{\\|d^{(k)}\\|_2^2} = \\frac{(v-s^{(k)})^\\top (p^{(k)}-s^{(k)})}{\\|p^{(k)}-s^{(k)}\\|_2^2}$. The step size for the update is $\\gamma_k = \\text{max}(0, \\text{min}(1, \\gamma^*))$. In practice, since $p^{(k)}$ maximizes $p^\\top(v-s^{(k)})$, the numerator is non-negative and $\\gamma^* \\ge 0$, so clipping at $0$ is not necessary unless $p^{(k)}=s^{(k)}$.\n    d.  **Update**: Update the current solution: $s^{(k+1)} = s^{(k)} + \\gamma_k d^{(k)} = (1-\\gamma_k)s^{(k)} + \\gamma_k p^{(k)}$.\n3.  **Termination**: The algorithm can be terminated after a fixed number of iterations or when the Frank-Wolfe duality gap, $g_k = (s^{(k)}-p^{(k)})^\\top \\nabla f(s^{(k)}) = (s^{(k)}-v)^\\top(s^{(k)}-p^{(k)})$, falls below a threshold.\n\n### Part 3: Primal Recovery and Objective Computation\n\nFor the special case $A=I_n$, the projection target is $v=A^\\top y = I_n y = y$. The Frank-Wolfe algorithm computes $s^\\star = \\text{argmin}_{s \\in \\lambda B(F)} \\frac{1}{2}\\|s-y\\|_2^2$. This is the projection of $y$ onto $\\lambda B(F)$, i.e., $s^\\star = P_{\\lambda B(F)}(y)$.\n\nThe problem states that the primal candidate solution $x^\\star$ can be recovered via the identity $x^\\star = y - s^\\star$. This identity corresponds to the proximal operator of $\\lambda \\Omega_F(x)$, which is $\\text{prox}_{\\lambda\\Omega_F}(y) = y - P_{\\lambda B(F)}(y)$. We will use this prescribed formula.\n\nOnce $x^\\star$ is computed, we evaluate the primal objective function:\n$$\n\\frac{1}{2}\\|y - x^\\star\\|_2^2 + \\lambda\\Omega_F(|x^\\star|)\n$$\nSubstituting $x^\\star = y - s^\\star$, the first term simplifies to $\\frac{1}{2}\\|y - (y-s^\\star)\\|_2^2 = \\frac{1}{2}\\|s^\\star\\|_2^2$.\n\nThe second term, the regularization penalty $\\lambda\\Omega_F(|x^\\star|)$, is computed using the given formula for cardinality-based functions. For $F(S)=\\alpha\\sqrt{|S|}$, the Lovász extension applied to a non-negative vector $z$ (here $z=|x^\\star|$) is:\n$$\n\\Omega_F(z) = \\alpha \\sum_{k=1}^n (\\sqrt{k}-\\sqrt{k-1}) z_{(k)}\n$$\nwhere $z_{(k)}$ is the $k$-th largest component of $z$.\nThe total primal objective value is therefore $\\frac{1}{2}\\|s^\\star\\|_2^2 + \\lambda \\alpha \\sum_{k=1}^n (\\sqrt{k}-\\sqrt{k-1}) |x^\\star|_{(k)}$. This value is computed for each test case.", "answer": "```python\nimport numpy as np\n\ndef greedy_oracle_sqrt_card(weights, alpha):\n    \"\"\"\n    Computes an extreme point of the base polytope B(F) for F(S) = alpha * sqrt(|S|).\n\n    Args:\n        weights (np.ndarray): The weight vector for the linear objective.\n        alpha (float): The scaling factor for the submodular function.\n\n    Returns:\n        np.ndarray: An extreme point of B(F).\n    \"\"\"\n    n = len(weights)\n    pi = np.argsort(weights)[::-1]  # Get permutation for sorting weights descending\n    \n    # Precompute sqrt values\n    sqrt_vals = np.sqrt(np.arange(n + 1))\n    \n    s = np.zeros(n)\n    inv_pi = np.argsort(pi)\n    \n    # Greedy assignment based on sorted weights\n    s_sorted = alpha * (sqrt_vals[1:] - sqrt_vals[:-1])\n    \n    # Reorder s back to original index order\n    s = s_sorted[inv_pi]\n    \n    return s\n\ndef frank_wolfe_projection(v, lamb, alpha, n, max_iter=200, tol=1e-12):\n    \"\"\"\n    Solves the projection problem min_{s in lambda*B(F)} 0.5*||s - v||^2\n    using the Frank-Wolfe algorithm.\n\n    Args:\n        v (np.ndarray): The vector to project.\n        lamb (float): The regularization parameter lambda.\n        alpha (float): The parameter alpha of the submodular function.\n        n (int): The dimension of the space.\n        max_iter (int): Maximum number of iterations.\n        tol (float): Tolerance for termination based on the duality gap.\n\n    Returns:\n        np.ndarray: The projection s_star.\n    \"\"\"\n    # Initialization\n    # s0 is the argmax of s^T v over lambda*B(F)\n    s_k = lamb * greedy_oracle_sqrt_card(v, alpha)\n\n    for k in range(max_iter):\n        # Compute gradient\n        grad_f = s_k - v\n        \n        # Solve linear subproblem to find the next extreme point\n        # p_k = argmin p^T grad_f = argmax p^T (-grad_f)\n        p_k = lamb * greedy_oracle_sqrt_card(-grad_f, alpha)\n        \n        # Duality gap\n        gap = -grad_f.T @ (p_k - s_k)\n        if gap  tol:\n            break\n            \n        # Exact line search for step size gamma\n        d_k = p_k - s_k\n        \n        # Avoid division by zero if s_k is already optimal\n        d_k_norm_sq = np.dot(d_k, d_k)\n        if d_k_norm_sq  1e-20:\n            gamma_k = 0.0\n        else:\n            gamma_k = gap / d_k_norm_sq\n        \n        gamma_k = max(0, min(1, gamma_k))\n        \n        # Update\n        s_k = s_k + gamma_k * d_k\n        \n    return s_k\n\ndef lovasz_extension_eval(x, alpha):\n    \"\"\"\n    Computes the Lovasz extension Omega_F(|x|) for F(S) = alpha * sqrt(|S|).\n    \"\"\"\n    n = len(x)\n    abs_x_sorted = np.sort(np.abs(x))[::-1]  # Sort |x| in descending order\n    \n    sqrt_vals = np.sqrt(np.arange(n + 1))\n    coeffs = sqrt_vals[1:] - sqrt_vals[:-1]\n    \n    return alpha * np.dot(coeffs, abs_x_sorted)\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        {'n': 8, 'A': np.eye(8), 'lambda': 0.9, 'y': np.array([1.5, 0.8, 0.3, 2.0, 0.1, 1.2, 0.5, 0.7])},\n        {'n': 8, 'A': np.eye(8), 'lambda': 1.0, 'y': np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])},\n        {'n': 8, 'A': np.eye(8), 'lambda': 0.8, 'y': None}, # y to be constructed\n        {'n': 4, 'A': np.eye(4), 'lambda': 0.7, 'y': np.array([0.2, 3.0, 1.0, 0.4])}\n    ]\n    \n    # Construct y for Case 3\n    alpha_case3 = 1.0\n    lambda_case3 = 0.8\n    w_case3 = np.array([8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0])\n    y_case3 = lambda_case3 * greedy_oracle_sqrt_card(w_case3, alpha_case3)\n    test_cases[2]['y'] = y_case3\n\n    results = []\n    alpha = 1.0  # Fixed as per problem for all tests\n    \n    for case in test_cases:\n        n, A, lamb, y = case['n'], case['A'], case['lambda'], case['y']\n        \n        # Since A=I_n, v = A^T y = y\n        v = y\n        \n        # 1. Compute the projection s_star\n        s_star = frank_wolfe_projection(v, lamb, alpha, n)\n        \n        # 2. Recover primal candidate x_star\n        x_star = y - s_star\n        \n        # 3. Evaluate the primal objective value\n        # Data fitting term: 0.5 * ||y - x_star||^2 = 0.5 * ||s_star||^2\n        data_term = 0.5 * np.dot(s_star, s_star)\n        \n        # Regularization term: lambda * Omega_F(|x_star|)\n        reg_term = lamb * lovasz_extension_eval(x_star, alpha)\n        \n        primal_obj_value = data_term + reg_term\n        results.append(primal_obj_value)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3483789"}]}