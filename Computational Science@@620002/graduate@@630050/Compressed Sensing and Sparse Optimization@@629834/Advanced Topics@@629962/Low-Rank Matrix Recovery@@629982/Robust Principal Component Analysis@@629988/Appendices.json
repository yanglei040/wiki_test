{"hands_on_practices": [{"introduction": "Before building full algorithms for Robust Principal Component Analysis (RPCA), we must master its core mathematical engine. This first exercise focuses on the Singular Value Thresholding (SVT) operator, which is the proximal operator for the nuclear norm and the workhorse for promoting low-rank solutions. By deriving the SVT operator from first principles and applying it to a concrete example, you will gain a deep understanding of how it manipulates singular values to enforce a low-rank structure [@problem_id:3474854].", "problem": "Consider the canonical Robust Principal Component Analysis (RPCA) decomposition model, which seeks a low-rank matrix and a sparse matrix via convex optimization. A standard proximal step for updating the low-rank component uses the proximal operator of the nuclear norm. Let the nuclear norm be defined by $\\|X\\|_{*} = \\sum_{i} \\sigma_{i}(X)$, where $\\sigma_{i}(X)$ are the singular values of the matrix $X$. The proximal operator of a proper, lower semicontinuous, convex function $f$ at a point $Y$ is defined by $\\operatorname{prox}_{f}(Y) = \\arg\\min_{X} \\left\\{ f(X) + \\tfrac{1}{2}\\|X - Y\\|_{F}^{2} \\right\\}$, where $\\|\\cdot\\|_{F}$ denotes the Frobenius norm. In the RPCA low-rank update, one uses the proximal operator of the scaled nuclear norm $f(X) = \\tau \\|X\\|_{*}$, where $\\tau > 0$ is a threshold parameter. \n\nStarting only from these base definitions, together with the unitary invariance of the Frobenius norm and the nuclear norm, derive the transformation induced on the singular values by $\\operatorname{prox}_{\\tau \\|\\cdot\\|_{*}}(Y)$, also known as the Singular Value Thresholding (SVT) operator (define this acronym on first use). Then apply your derivation to a matrix $Y$ whose Singular Value Decomposition (SVD) is $Y = U \\Sigma V^{\\top}$ with singular values $(\\sigma_{1}, \\sigma_{2}, \\sigma_{3}) = (5, 3, 0)$, using the threshold $\\tau = 2$. Compute the singular values of the proximal point $L^{\\star} = \\operatorname{prox}_{\\tau \\|\\cdot\\|_{*}}(Y)$ and the rank of $L^{\\star}$. \n\nExpress your final answer as a single expression listing the singular values of $L^{\\star}$ in nonincreasing order followed by the rank of $L^{\\star}$. No rounding is required and no physical units are involved.", "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It consists of a standard derivation and application within the field of convex optimization and matrix analysis. All necessary definitions and data are provided.\n\nThe task is to derive the proximal operator of the scaled nuclear norm and apply it to a specific matrix. The proximal operator of the function $f(X) = \\tau \\|X\\|_{*}$ is defined as the solution to the following optimization problem:\n$$\nL^{\\star} = \\operatorname{prox}_{\\tau \\|\\cdot\\|_{*}}(Y) = \\arg\\min_{X} \\left\\{ \\tau \\|X\\|_{*} + \\tfrac{1}{2}\\|X - Y\\|_{F}^{2} \\right\\}\n$$\nwhere $\\|X\\|_{*} = \\sum_{i} \\sigma_{i}(X)$ is the nuclear norm, with $\\sigma_i(X)$ being the singular values of $X$, and $\\|A\\|_{F} = \\sqrt{\\sum_{i,j} |A_{ij}|^2} = \\sqrt{\\operatorname{Tr}(A^{\\top}A)}$ is the Frobenius norm.\n\nLet the Singular Value Decomposition (SVD) of the matrix $Y \\in \\mathbb{R}^{m \\times n}$ be $Y = U \\Sigma_Y V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are unitary matrices, and $\\Sigma_Y$ is a rectangular diagonal matrix of size $m \\times n$ with non-negative diagonal entries $\\sigma_i(Y)$ (the singular values of $Y$) sorted in non-increasing order.\n\nThe objective function can be expanded as:\n$$\nJ(X) = \\tau \\|X\\|_{*} + \\tfrac{1}{2} \\left( \\|X\\|_{F}^{2} - 2\\langle X, Y \\rangle_F + \\|Y\\|_{F}^{2} \\right)\n$$\nwhere $\\langle X, Y \\rangle_F = \\operatorname{Tr}(X^{\\top}Y)$ is the Frobenius inner product. The term $\\|Y\\|_{F}^{2}$ is a constant with respect to $X$ and can be ignored in the minimization. The norms can be expressed in terms of singular values: $\\|X\\|_{*} = \\sum_i \\sigma_i(X)$ and $\\|X\\|_{F}^{2} = \\sum_i \\sigma_i(X)^2$. The problem is equivalent to minimizing:\n$$\n\\tilde{J}(X) = \\tau \\sum_i \\sigma_i(X) + \\tfrac{1}{2} \\sum_i \\sigma_i(X)^2 - \\operatorname{Tr}(X^{\\top}Y)\n$$\nThe von Neumann trace inequality states that for any two matrices $A$ and $B$, $\\operatorname{Tr}(A^{\\top}B) \\le \\sum_i \\sigma_i(A)\\sigma_i(B)$. Equality is achieved if $A$ and $B$ share the same singular vectors, i.e., $A = U\\Sigma_A V^{\\top}$ and $B = U\\Sigma_B V^{\\top}$ for some unitary matrices $U$ and $V$. To minimize $\\tilde{J}(X)$, we must maximize the term $\\operatorname{Tr}(X^{\\top}Y)$. Based on the von Neumann inequality, this maximum is achieved when $X$ has the same singular vectors as $Y$. Let us therefore seek a solution of the form $X = U \\Sigma_X V^{\\top}$, where $U$ and $V$ are the singular vector matrices from the SVD of $Y$.\n\nUsing the unitary invariance of the Frobenius norm, the term $\\|X - Y\\|_{F}^{2}$ simplifies:\n$$\n\\|X - Y\\|_{F}^{2} = \\|U \\Sigma_X V^{\\top} - U \\Sigma_Y V^{\\top}\\|_{F}^{2} = \\|U(\\Sigma_X - \\Sigma_Y)V^{\\top}\\|_{F}^{2} = \\|\\Sigma_X - \\Sigma_Y\\|_{F}^{2}\n$$\nSince $\\Sigma_X$ and $\\Sigma_Y$ are diagonal matrices with diagonal entries $\\sigma_i(X)$ and $\\sigma_i(Y)$ respectively, this becomes:\n$$\n\\|\\Sigma_X - \\Sigma_Y\\|_{F}^{2} = \\sum_i (\\sigma_i(X) - \\sigma_i(Y))^2\n$$\nThe nuclear norm of $X$ is $\\|X\\|_{*} = \\sum_i \\sigma_i(X)$.\nSubstituting these into the original optimization problem, we now minimize over the singular values $\\sigma_i(X)$:\n$$\n\\min_{\\{\\sigma_i(X) \\ge 0\\}} \\left\\{ \\tau \\sum_i \\sigma_i(X) + \\tfrac{1}{2} \\sum_i (\\sigma_i(X) - \\sigma_i(Y))^2 \\right\\}\n$$\nThis problem is separable, meaning we can solve for each $\\sigma_i(X)$ independently. For each $i$, we solve:\n$$\n\\min_{\\sigma \\ge 0} \\left\\{ \\tau \\sigma + \\tfrac{1}{2} (\\sigma - \\sigma_i(Y))^2 \\right\\}\n$$\nLet $g(\\sigma) = \\tau \\sigma + \\tfrac{1}{2} (\\sigma - \\sigma_i(Y))^2$. This is a convex quadratic function. To find the minimum, we compute the derivative with respect to $\\sigma$ and set it to zero (ignoring the non-negativity constraint for a moment):\n$$\n\\frac{dg}{d\\sigma} = \\tau + (\\sigma - \\sigma_i(Y)) = 0 \\implies \\sigma = \\sigma_i(Y) - \\tau\n$$\nNow, we must enforce the constraint $\\sigma \\ge 0$.\n1. If $\\sigma_i(Y) - \\tau > 0$, the unconstrained minimizer is positive, so it is the valid solution. The optimal value is $\\sigma^{\\star} = \\sigma_i(Y) - \\tau$.\n2. If $\\sigma_i(Y) - \\tau \\le 0$, the unconstrained minimizer is non-positive. Since $g(\\sigma)$ is a parabola opening upwards with its vertex at $\\sigma_i(Y) - \\tau$, the function is increasing for $\\sigma > \\sigma_i(Y) - \\tau$. Therefore, on the domain $\\sigma \\ge 0$, the minimum is achieved at the boundary, i.e., at $\\sigma^{\\star} = 0$.\n\nCombining these two cases, the solution for each singular value is:\n$$\n\\sigma_i(L^{\\star}) = \\max(0, \\sigma_i(Y) - \\tau) = (\\sigma_i(Y) - \\tau)_+\n$$\nThis operation is known as soft-thresholding. The resulting matrix $L^{\\star}$ is constructed by applying this transformation to the singular values of $Y$ while keeping the singular vectors the same:\n$$\nL^{\\star} = U \\mathcal{S}_{\\tau}(\\Sigma_Y) V^{\\top}\n$$\nwhere $\\mathcal{S}_{\\tau}(\\Sigma_Y)$ is the diagonal matrix with diagonal entries $\\max(0, \\sigma_i(Y)-\\tau)$. This transformation is called the Singular Value Thresholding (SVT) operator.\n\nNow we apply this result to the given problem. We have a matrix $Y$ with singular values $(\\sigma_1, \\sigma_2, \\sigma_3) = (5, 3, 0)$ and a threshold $\\tau = 2$.\nWe compute the singular values of $L^{\\star} = \\operatorname{prox}_{\\tau \\|\\cdot\\|_{*}}(Y)$ using the derived rule. Let's denote the singular values of $L^{\\star}$ as $\\sigma_i^{\\star}$.\n\nFor $\\sigma_1 = 5$:\n$$\n\\sigma_1^{\\star} = \\max(0, 5 - 2) = \\max(0, 3) = 3\n$$\nFor $\\sigma_2 = 3$:\n$$\n\\sigma_2^{\\star} = \\max(0, 3 - 2) = \\max(0, 1) = 1\n$$\nFor $\\sigma_3 = 0$:\n$$\n\\sigma_3^{\\star} = \\max(0, 0 - 2) = \\max(0, -2) = 0\n$$\nThe singular values of $L^{\\star}$, in nonincreasing order, are $(3, 1, 0)$.\n\nThe rank of a matrix is the number of its non-zero singular values. For $L^{\\star}$, the non-zero singular values are $3$ and $1$. There are two such values.\nTherefore, the rank of $L^{\\star}$ is $2$.\n\nThe final answer requires the singular values of $L^{\\star}$ and the rank of $L^{\\star}$. These are $(3, 1, 0)$ and $2$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3 & 1 & 0 & 2\n\\end{pmatrix}\n}\n$$", "id": "3474854"}, {"introduction": "RPCA algorithms typically operate by iteratively applying operators that separately promote low-rank and sparse structures. This practice simulates one fundamental step within such an algorithm, where you will apply both the Singular Value Thresholding operator to estimate the low-rank matrix $L$ and the element-wise soft-thresholding operator to estimate the sparse matrix $S$. This exercise [@problem_id:3474825] provides a concrete look at the two key building blocks of Principal Component Pursuit (PCP) working in tandem to decompose a data matrix.", "problem": "Consider the convex decomposition model known as Principal Component Pursuit (PCP), which seeks a low-rank matrix and a sparse matrix from data. PCP solves the optimization problem that minimizes the sum of the nuclear norm and the scaled entrywise one-norm. Specifically, given a data matrix $D \\in \\mathbb{R}^{m \\times n}$ and a parameter $\\lambda > 0$, the PCP objective is to minimize $\\|L\\|_{*} + \\lambda \\|S\\|_{1}$ with respect to $L$ and $S$ subject to $L + S = D$, where $\\|L\\|_{*}$ denotes the nuclear norm (sum of singular values) and $\\|S\\|_{1}$ denotes the entrywise one-norm (sum of absolute values).\n\nA proximal step toward the PCP solution can be defined by separately applying the proximal operator of the nuclear norm to update $L$ and the proximal operator of the entrywise one-norm to update $S$. Let the proximal parameter for both updates be $1$. That is, define\n$$\nL := \\operatorname{prox}_{\\|\\cdot\\|_{*}}(D) = \\arg\\min_{X} \\left\\{ \\|X\\|_{*} + \\frac{1}{2}\\|X - D\\|_{F}^{2} \\right\\},\n$$\nand\n$$\nS := \\operatorname{prox}_{\\lambda \\|\\cdot\\|_{1}}(D) = \\arg\\min_{Y} \\left\\{ \\lambda \\|Y\\|_{1} + \\frac{1}{2}\\|Y - D\\|_{F}^{2} \\right\\},\n$$\nwhere $\\|\\cdot\\|_{F}$ denotes the Frobenius norm. It is known that $\\operatorname{prox}_{\\|\\cdot\\|_{*}}$ acts by singular value thresholding and $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{1}}$ acts by entrywise soft-thresholding, but the computation should follow from first principles of proximal operators and orthogonal invariance.\n\nGiven the specific matrix\n$$\nD = \\begin{bmatrix} 1 & 0 \\\\ 0 & 10 \\end{bmatrix}\n$$\nand the parameter $\\lambda = 1$, perform the above proximal updates for $L$ and $S$. Report the updated pair $(L,S)$ by listing the eight entries in the order $\\big(L_{11}, L_{12}, L_{21}, L_{22}, S_{11}, S_{12}, S_{21}, S_{22}\\big)$ as a single row vector. The final answer must be a single analytical expression as specified and does not require rounding.", "solution": "The problem statement has been validated and is deemed sound. It presents a well-posed mathematical problem in the field of convex optimization, specifically concerning the proximal operators associated with the Principal Component Pursuit (PCP) model. All definitions and parameters are provided, and the task is unambiguous. We shall proceed with the solution.\n\nThe problem asks for the computation of two matrices, $L$ and $S$, resulting from the application of two distinct proximal operators to a given data matrix $D$. The inputs are the matrix $D = \\begin{bmatrix} 1 & 0 \\\\ 0 & 10 \\end{bmatrix}$ and the parameter $\\lambda = 1$.\n\nFirst, we compute the matrix $L$, defined as:\n$$\nL := \\operatorname{prox}_{\\|\\cdot\\|_{*}}(D) = \\arg\\min_{X \\in \\mathbb{R}^{2 \\times 2}} \\left\\{ \\|X\\|_{*} + \\frac{1}{2}\\|X - D\\|_{F}^{2} \\right\\}\n$$\nThis is the proximal operator of the nuclear norm, $\\|\\cdot\\|_{*}$, with a step-size parameter of $\\tau=1$. The solution to this optimization problem is given by the Singular Value Thresholding (SVT) operator. The SVT operator acts on the singular values of the input matrix. Let the Singular Value Decomposition (SVD) of $D$ be $D = U \\Sigma V^{\\top}$. The solution $L$ is then given by $L = U S_{1}(\\Sigma) V^{\\top}$, where $S_{1}(\\cdot)$ is the soft-thresholding operator with threshold $1$, applied to the diagonal entries of $\\Sigma$. The soft-thresholding function for a non-negative value $x$ and threshold $\\tau$ is $S_{\\tau}(x) = \\max(x - \\tau, 0)$.\n\nThe derivation of this result relies on the properties of unitarily invariant norms. The Frobenius norm $\\|\\cdot\\|_F$ is unitarily invariant, meaning $\\|A\\|_F = \\|UAV^{\\top}\\|_F$ for any unitary matrices $U, V$. The nuclear norm is also unitarily invariant. Using the SVD of $D$, the optimization problem can be rewritten as:\n$$\n\\arg\\min_{X} \\left\\{ \\|X\\|_{*} + \\frac{1}{2}\\|X - U\\Sigma V^{\\top}\\|_{F}^{2} \\right\\} = \\arg\\min_{X} \\left\\{ \\|U^{\\top} X V\\|_{*} + \\frac{1}{2}\\|U^{\\top} X V - \\Sigma\\|_{F}^{2} \\right\\}\n$$\nLetting $X' = U^{\\top} X V$, the problem becomes $\\arg\\min_{X'} \\left\\{ \\|X'\\|_{*} + \\frac{1}{2}\\|X' - \\Sigma\\|_{F}^{2} \\right\\}$. Due to Von Neumann's trace inequality, the minimum is achieved when $X'$ is a diagonal matrix. The problem then decouples for each diagonal entry of $X'$, which corresponds to a singular value. For each singular value $\\sigma_i$ of $D$, we solve $\\min_{x_i \\ge 0} \\{ x_i + \\frac{1}{2}(x_i - \\sigma_i)^2 \\}$, whose solution is $x_i = \\max(\\sigma_i - 1, 0)$.\n\nWe now apply this procedure to the given matrix $D = \\begin{bmatrix} 1 & 0 \\\\ 0 & 10 \\end{bmatrix}$. Since $D$ is a diagonal matrix, its singular values are the absolute values of its diagonal entries, which are $\\sigma_1 = 10$ and $\\sigma_2 = 1$. The corresponding singular vectors in $U$ and $V$ can be arranged. To align with the conventional ordering $\\sigma_1 \\ge \\sigma_2$, we have:\n$\\Sigma = \\begin{bmatrix} 10 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $U = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}$, and $V = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}$.\nWe verify this SVD: $U \\Sigma V^{\\top} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\begin{bmatrix} 10 & 0 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ 10 & 0 \\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 10 \\end{bmatrix} = D$.\n\nNext, we apply soft-thresholding with threshold $\\tau=1$ to the singular values:\n$\\hat{\\sigma}_1 = S_{1}(10) = \\max(10 - 1, 0) = 9$.\n$\\hat{\\sigma}_2 = S_{1}(1) = \\max(1 - 1, 0) = 0$.\nThe thresholded singular value matrix is $\\hat{\\Sigma} = \\begin{bmatrix} 9 & 0 \\\\ 0 & 0 \\end{bmatrix}$.\n\nFinally, we reconstruct $L$:\n$L = U \\hat{\\Sigma} V^{\\top} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\begin{bmatrix} 9 & 0 \\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} = \\begin{bmatrix} 0 & 0 \\\\ 9 & 0 \\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} = \\begin{bmatrix} 0 & 0 \\\\ 0 & 9 \\end{bmatrix}$.\nThus, the entries of $L$ are $L_{11}=0, L_{12}=0, L_{21}=0, L_{22}=9$.\n\nSecond, we compute the matrix $S$, defined as:\n$$\nS := \\operatorname{prox}_{\\lambda \\|\\cdot\\|_{1}}(D) = \\arg\\min_{Y \\in \\mathbb{R}^{2 \\times 2}} \\left\\{ \\lambda \\|Y\\|_{1} + \\frac{1}{2}\\|Y - D\\|_{F}^{2} \\right\\}\n$$\nWith $\\lambda=1$, the objective is $\\sum_{i,j} \\left(|Y_{ij}| + \\frac{1}{2}(Y_{ij} - D_{ij})^2\\right)$. Since the objective function is separable over the entries of the matrix $Y$, we can solve for each entry $S_{ij}$ independently by solving the one-dimensional problem:\n$S_{ij} = \\arg\\min_{y \\in \\mathbb{R}} \\left\\{ |y| + \\frac{1}{2}(y - D_{ij})^2 \\right\\}$.\nThe solution to this is the scalar soft-thresholding operator, $S_{1}(D_{ij}) = \\operatorname{sgn}(D_{ij}) \\max(|D_{ij}| - 1, 0)$.\n\nWe apply this operator to each entry of $D = \\begin{bmatrix} 1 & 0 \\\\ 0 & 10 \\end{bmatrix}$:\n$S_{11} = S_{1}(1) = \\operatorname{sgn}(1) \\max(|1| - 1, 0) = 1 \\cdot 0 = 0$.\n$S_{12} = S_{1}(0) = \\operatorname{sgn}(0) \\max(|0| - 1, 0) = 0 \\cdot 0 = 0$.\n$S_{21} = S_{1}(0) = \\operatorname{sgn}(0) \\max(|0| - 1, 0) = 0 \\cdot 0 = 0$.\n$S_{22} = S_{1}(10) = \\operatorname{sgn}(10) \\max(|10| - 1, 0) = 1 \\cdot 9 = 9$.\nThus, the matrix $S$ is $S = \\begin{bmatrix} 0 & 0 \\\\ 0 & 9 \\end{bmatrix}$.\nThe entries of $S$ are $S_{11}=0, S_{12}=0, S_{21}=0, S_{22}=9$.\n\nThe problem requires reporting the updated pair $(L, S)$ as a single row vector with entries in the order $\\big(L_{11}, L_{12}, L_{21}, L_{22}, S_{11}, S_{12}, S_{21}, S_{22}\\big)$.\nSubstituting the computed values, we have the vector $(0, 0, 0, 9, 0, 0, 0, 9)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & 0 & 0 & 9 & 0 & 0 & 0 & 9\n\\end{pmatrix}\n}\n$$", "id": "3474825"}, {"introduction": "The ultimate test of understanding an algorithm is to translate its theory into a working implementation. This final practice guides you through building a complete alternating proximal minimization solver for RPCA, integrating the SVT and soft-thresholding operators explored in the previous exercises [@problem_id:3474854] [@problem_id:3474825]. By coding the algorithm from scratch and testing it on synthetic data, you will witness how these mathematical tools converge to perform the powerful task of separating a low-rank matrix from sparse corruption [@problem_id:3474831].", "problem": "You are asked to implement and analyze a simple alternating proximal minimization algorithm for Robust Principal Component Analysis (RPCA), a canonical problem in compressed sensing and sparse optimization. Consider a data matrix $M \\in \\mathbb{R}^{m \\times n}$ that decomposes as $M = L_{\\star} + S_{\\star}$, where $L_{\\star}$ is low-rank and $S_{\\star}$ is sparse. Starting from the foundational base of convex regularization with the nuclear norm and the elementwise absolute-value norm, consider the unconstrained penalized objective\n$$\n\\min_{L, S \\in \\mathbb{R}^{m \\times n}} \\; \\frac{1}{2} \\lVert M - L - S \\rVert_{F}^{2} + \\mu \\lVert L \\rVert_{\\ast} + \\lambda \\lVert S \\rVert_{1},\n$$\nwhere $\\lVert \\cdot \\rVert_{F}$ is the Frobenius norm, $\\lVert \\cdot \\rVert_{\\ast}$ is the nuclear norm (sum of singular values), and $\\lVert \\cdot \\rVert_{1}$ is the elementwise $\\ell_{1}$ norm. Your program must implement an alternating proximal minimization scheme that alternates between minimizing the objective with respect to $L$ (holding $S$ fixed) and minimizing with respect to $S$ (holding $L$ fixed). The proximal operator for the nuclear norm corresponds to singular value thresholding, and the proximal operator for the elementwise $\\ell_{1}$ norm corresponds to elementwise soft-thresholding.\n\nFrom first principles, base your derivation on the definitions of the proximal operator, the nuclear norm, and the $\\ell_{1}$ norm, together with the decomposition of a matrix by the singular value decomposition. Do not assume any special algorithmic template beyond alternating minimization and proximal updates defined by these norms.\n\nYour program must:\n- Construct synthetic test matrices $M = L_{\\star} + S_{\\star}$ for a small test suite where $L_{\\star}$ has a prescribed rank and $S_{\\star}$ has a prescribed sparse support. Use orthonormal factors for $L_{\\star}$ (obtained from a random Gaussian matrix followed by a QR factorization) and fixed singular values, and select a random support for $S_{\\star}$ with nonzero entries of fixed magnitude and random signs. Use fixed pseudorandom seeds for reproducibility.\n- Run the alternating proximal minimization updates\n  - Update $L$ by the proximal map of $\\mu \\lVert \\cdot \\rVert_{\\ast}$ applied to $M - S$.\n  - Update $S$ by the proximal map of $\\lambda \\lVert \\cdot \\rVert_{1}$ applied to $M - L$.\n- Initialize $L$ and $S$ as the zero matrices.\n- At each iteration, check whether the estimated low-rank matrix has the correct rank (that is, the count of singular values greater than a fixed threshold matches the true rank) and whether the estimated sparse matrix has exactly the correct support (the set of indices with magnitude greater than a fixed threshold matches the true support). Define a small absolute threshold for both checks as $\\tau_{\\text{rank}} = 10^{-6}$ and $\\tau_{\\text{supp}} = 10^{-6}$.\n- Report the iteration count at which both the correct rank and the correct support are first simultaneously achieved. If this does not occur within a maximum of $1000$ iterations, report $-1$ for that test case. Use a convergence safeguard to update iterates until either the maximum iteration cap is reached or the relative change in $(L,S)$ drops below $10^{-8}$.\n\nTest Suite:\nImplement four test cases with the following parameters. For each case, generate $L_{\\star}$ and $S_{\\star}$ as specified, set $M = L_{\\star} + S_{\\star}$, and run the algorithm with the stated $(\\mu,\\lambda)$.\n\n- Case A (happy path, mixed low-rank and sparse):\n  - Dimensions: $m = 12$, $n = 10$.\n  - Rank of $L_{\\star}$: $r = 2$ with singular values $[8.0, 6.0]$.\n  - Sparse support size: $12$ nonzeros with magnitude $5.0$ and random $\\pm 1$ signs.\n  - Parameters: $\\mu = 1.0$, $\\lambda = 0.35$.\n  - Random seed: $0$.\n\n- Case B (boundary condition with no sparse component):\n  - Dimensions: $m = 15$, $n = 10$.\n  - Rank of $L_{\\star}$: $r = 3$ with singular values $[5.0, 4.0, 3.0]$.\n  - Sparse support size: $0$.\n  - Parameters: $\\mu = 0.8$, $\\lambda = 0.30$.\n  - Random seed: $1$.\n\n- Case C (boundary condition with no low-rank component):\n  - Dimensions: $m = 10$, $n = 10$.\n  - Rank of $L_{\\star}$: $r = 0$.\n  - Sparse support size: $15$ nonzeros with magnitude $4.0$ and random $\\pm 1$ signs.\n  - Parameters: $\\mu = 1.0$, $\\lambda = 0.50$.\n  - Random seed: $2$.\n\n- Case D (near-threshold harder case):\n  - Dimensions: $m = 10$, $n = 10$.\n  - Rank of $L_{\\star}$: $r = 2$ with singular values $[1.2, 1.1]$.\n  - Sparse support size: $8$ nonzeros with magnitude $0.6$ and random $\\pm 1$ signs.\n  - Parameters: $\\mu = 0.8$, $\\lambda = 0.45$.\n  - Random seed: $3$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the iteration counts for the four cases as a comma-separated list enclosed in square brackets (for example, `[7,4,5,-1]`). No additional text should be printed. No physical units are involved in this problem, and no angles must be specified. All numerical outputs are pure integers. The use of any language is permitted in principle, but your final answer must be a complete and runnable program that adheres to the provided execution environment constraints.", "solution": "The problem requires the implementation of an alternating proximal minimization algorithm to solve the Robust Principal Component Analysis (RPCA) problem. The solution seeks to decompose a given data matrix $M \\in \\mathbb{R}^{m \\times n}$ into a low-rank component $L$ and a sparse component $S$. This is achieved by solving the following convex optimization problem:\n$$\n\\min_{L, S \\in \\mathbb{R}^{m \\times n}} \\; f(L, S) = \\frac{1}{2} \\lVert M - L - S \\rVert_{F}^{2} + \\mu \\lVert L \\rVert_{\\ast} + \\lambda \\lVert S \\rVert_{1}\n$$\nHere, $\\lVert \\cdot \\rVert_{F}$ is the Frobenius norm, which measures the data fidelity, $\\lVert \\cdot \\rVert_{\\ast}$ is the nuclear norm (the sum of the singular values), which serves as a convex surrogate for the rank of $L$, and $\\lVert \\cdot \\rVert_{1}$ is the elementwise $\\ell_{1}$ norm (the sum of the absolute values of the entries), which promotes sparsity in $S$. The parameters $\\mu > 0$ and $\\lambda > 0$ control the trade-off between data fidelity and the low-rank and sparse regularizers.\n\nThe chosen solution method is an alternating minimization scheme, a form of block coordinate descent. In each iteration, we minimize the objective function with respect to one variable while keeping the other fixed. This breaks the joint minimization problem into two simpler subproblems.\n\n**Step 1: Minimization with respect to $L$**\n\nFixing $S$ to its value from the previous iteration, $S_k$, the subproblem for updating $L$ is:\n$$\nL_{k+1} = \\arg\\min_{L} \\frac{1}{2} \\lVert M - L - S_k \\rVert_{F}^{2} + \\mu \\lVert L \\rVert_{\\ast}\n$$\nThis can be rewritten as:\n$$\nL_{k+1} = \\arg\\min_{L} \\frac{1}{2} \\lVert (M - S_k) - L \\rVert_{F}^{2} + \\mu \\lVert L \\rVert_{\\ast}\n$$\nThis is the definition of the proximal operator of the nuclear norm, scaled by $\\mu$, applied to the matrix $M - S_k$. The proximal operator, denoted $\\text{prox}_{\\mu \\lVert \\cdot \\rVert_{\\ast}}$, is given by the Singular Value Thresholding (SVT) operator. For any matrix $X \\in \\mathbb{R}^{m \\times n}$ with singular value decomposition (SVD) $X = U \\Sigma V^{\\top}$, where $\\Sigma = \\text{diag}(\\{\\sigma_i\\}_{i=1}^{\\min(m,n)})$, the SVT operator is defined as:\n$$\n\\text{prox}_{\\mu \\lVert \\cdot \\rVert_{\\ast}}(X) = U \\mathcal{S}_{\\mu}(\\Sigma) V^{\\top}\n$$\nwhere $\\mathcal{S}_{\\mu}(\\Sigma)$ is a diagonal matrix with entries $(\\mathcal{S}_{\\mu}(\\Sigma))_{ii} = \\max(0, \\sigma_i - \\mu)$. This operation effectively shrinks the singular values of $X$ towards zero by $\\mu$ and sets any singular values smaller than $\\mu$ to zero, thereby promoting a low-rank structure.\n\n**Step 2: Minimization with respect to $S$**\n\nNext, we fix $L$ to its newly computed value, $L_{k+1}$, and solve for $S$:\n$$\nS_{k+1} = \\arg\\min_{S} \\frac{1}{2} \\lVert M - L_{k+1} - S \\rVert_{F}^{2} + \\lambda \\lVert S \\rVert_{1}\n$$\nThis can be rewritten as:\n$$\nS_{k+1} = \\arg\\min_{S} \\frac{1}{2} \\lVert (M - L_{k+1}) - S \\rVert_{F}^{2} + \\lambda \\lVert S \\rVert_{1}\n$$\nThis is the definition of the proximal operator of the $\\ell_1$ norm, scaled by $\\lambda$, applied to the matrix $M - L_{k+1}$. Due to the separability of the Frobenius norm and the $\\ell_1$ norm over the matrix elements, this problem can be solved elementwise. For each entry $(i,j)$, the problem is:\n$$\n(S_{k+1})_{ij} = \\arg\\min_{s_{ij}} \\frac{1}{2} ((M-L_{k+1})_{ij} - s_{ij})^2 + \\lambda |s_{ij}|\n$$\nThe solution is given by the elementwise soft-thresholding operator, $\\mathcal{S}_{\\lambda}$:\n$$\n(S_{k+1})_{ij} = \\mathcal{S}_{\\lambda}((M-L_{k+1})_{ij}) = \\text{sign}((M-L_{k+1})_{ij}) \\max(0, |(M-L_{k+1})_{ij}| - \\lambda)\n$$\nThis operation shrinks each element's magnitude towards zero by $\\lambda$, promoting sparsity.\n\n**Algorithm and Implementation**\n\nThe complete algorithm proceeds as follows:\n1. Initialize $L_0 = 0$ and $S_0 = 0$.\n2. For $k=0, 1, 2, \\dots$ until convergence or maximum iterations:\n   a. Update $L$: $L_{k+1} = \\text{prox}_{\\mu \\lVert \\cdot \\rVert_{\\ast}}(M - S_k)$.\n   b. Update $S$: $S_{k+1} = \\text{prox}_{\\lambda \\lVert \\cdot \\rVert_{1}}(M - L_{k+1})$.\n\nThe implementation creates synthetic data $M = L_{\\star} + S_{\\star}$ according to the specified parameters for each test case. $L_{\\star}$ is constructed from random orthonormal bases and prescribed singular values. $S_{\\star}$ is constructed by populating a random support with entries of a fixed magnitude and random signs. Fixed random seeds ensure reproducibility.\n\nThe algorithm iterates, and at each step $k$, it checks if the estimated $L_k$ has the correct rank $r$ (number of singular values $> \\tau_{\\text{rank}} = 10^{-6}$) and if the estimated $S_k$ has the exactly correct support (the set of indices where $|(S_k)_{ij}| > \\tau_{\\text{supp}} = 10^{-6}$ matches the true support of $S_{\\star}$). The iteration count at which both conditions are first met is recorded. The process stops if these conditions are met and the algorithm has subsequently converged (relative change in $(L,S)$ below $10^{-8}$), or if the maximum iteration limit of $1000$ is reached. If the success conditions are not met within the iteration limit, $-1$ is reported.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# from scipy import ... # No scipy needed for this implementation.\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for Robust PCA and print results.\n    \"\"\"\n\n    def generate_data(m, n, r, singular_values, sparse_size, sparse_mag, seed):\n        \"\"\"Generates synthetic data matrix M = L_star + S_star.\"\"\"\n        rng = np.random.default_rng(seed)\n\n        # Generate L_star (low-rank matrix)\n        if r > 0:\n            U_rand = rng.standard_normal(size=(m, r))\n            U, _ = np.linalg.qr(U_rand)\n            \n            V_rand = rng.standard_normal(size=(n, r))\n            V, _ = np.linalg.qr(V_rand)\n            \n            Sigma_star = np.diag(singular_values)\n            L_star = U @ Sigma_star @ V.T\n        else:\n            L_star = np.zeros((m, n))\n\n        # Generate S_star (sparse matrix)\n        S_star = np.zeros((m, n))\n        if sparse_size > 0:\n            indices = rng.choice(m * n, size=sparse_size, replace=False)\n            row_indices, col_indices = np.unravel_index(indices, (m, n))\n            \n            signs = rng.choice([-1.0, 1.0], size=sparse_size)\n            \n            S_star[row_indices, col_indices] = signs * sparse_mag\n        \n        true_support_indices = np.where(S_star != 0)\n        \n        M = L_star + S_star\n        \n        return M, true_support_indices\n\n    def prox_nuclear(X, mu):\n        \"\"\"Singular Value Thresholding (proximal operator for nuclear norm).\"\"\"\n        U, s, Vt = np.linalg.svd(X, full_matrices=False, compute_uv=True)\n        s_thresh = np.maximum(0, s - mu)\n        return U @ np.diag(s_thresh) @ Vt\n\n    def prox_l1(X, lam):\n        \"\"\"Element-wise soft-thresholding (proximal operator for l1 norm).\"\"\"\n        return np.sign(X) * np.maximum(np.abs(X) - lam, 0)\n\n    def run_rpca_instance(m, n, true_rank, sv, sp_size, sp_mag, mu, lam, seed):\n        \"\"\"Runs the alternating proximal minimization for one RPCA instance.\"\"\"\n        M, true_support_indices = generate_data(m, n, true_rank, sv, sp_size, sp_mag, seed)\n        \n        L = np.zeros((m, n))\n        S = np.zeros((m, n))\n        \n        max_iter = 1000\n        conv_tol = 1e-8\n        rank_tol = 1e-6\n        supp_tol = 1e-6\n        \n        found_iter = -1\n        \n        true_support_mask = np.zeros((m, n), dtype=bool)\n        if sp_size > 0:\n            true_support_mask[true_support_indices] = True\n        \n        for k in range(1, max_iter + 1):\n            L_old, S_old = L.copy(), S.copy()\n            \n            # L-update\n            L = prox_nuclear(M - S, mu)\n            \n            # S-update\n            S = prox_l1(M - L, lam)\n            \n            # Check for correct rank and support (if not already found)\n            if found_iter == -1:\n                singular_values_L = np.linalg.svd(L, compute_uv=False)\n                estimated_rank = np.sum(singular_values_L > rank_tol)\n                rank_ok = (estimated_rank == true_rank)\n                \n                estimated_support_mask = np.abs(S) > supp_tol\n                support_ok = np.array_equal(estimated_support_mask, true_support_mask)\n                \n                if rank_ok and support_ok:\n                    found_iter = k\n                    \n            # Check for convergence\n            norm_L_fro_sq = np.linalg.norm(L_old, 'fro')**2\n            norm_S_fro_sq = np.linalg.norm(S_old, 'fro')**2\n            norm_val_sq = norm_L_fro_sq + norm_S_fro_sq\n\n            norm_diff_L_fro_sq = np.linalg.norm(L - L_old, 'fro')**2\n            norm_diff_S_fro_sq = np.linalg.norm(S - S_old, 'fro')**2\n            norm_diff_sq = norm_diff_L_fro_sq + norm_diff_S_fro_sq\n            \n            if norm_val_sq > 0: # Avoid division by zero, especially at first iteration\n                rel_change = np.sqrt(norm_diff_sq / norm_val_sq)\n                if rel_change < conv_tol:\n                    break\n        return found_iter\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: Happy path\n        {'m': 12, 'n': 10, 'true_rank': 2, 'sv': [8.0, 6.0], 'sp_size': 12, 'sp_mag': 5.0, 'mu': 1.0, 'lam': 0.35, 'seed': 0},\n        # Case B: No sparse component\n        {'m': 15, 'n': 10, 'true_rank': 3, 'sv': [5.0, 4.0, 3.0], 'sp_size': 0, 'sp_mag': 0.0, 'mu': 0.8, 'lam': 0.30, 'seed': 1},\n        # Case C: No low-rank component\n        {'m': 10, 'n': 10, 'true_rank': 0, 'sv': [], 'sp_size': 15, 'sp_mag': 4.0, 'mu': 1.0, 'lam': 0.50, 'seed': 2},\n        # Case D: Harder case\n        {'m': 10, 'n': 10, 'true_rank': 2, 'sv': [1.2, 1.1], 'sp_size': 8, 'sp_mag': 0.6, 'mu': 0.8, 'lam': 0.45, 'seed': 3},\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = run_rpca_instance(**case_params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3474831"}]}