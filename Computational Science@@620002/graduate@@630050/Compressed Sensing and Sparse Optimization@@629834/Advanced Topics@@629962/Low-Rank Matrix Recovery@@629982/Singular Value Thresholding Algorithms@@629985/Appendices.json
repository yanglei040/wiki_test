{"hands_on_practices": [{"introduction": "This first exercise grounds the theory of singular value thresholding in the practical application of matrix denoising. You will see how the process of shrinking singular values is not an ad-hoc heuristic, but rather the exact solution to a well-posed constrained optimization problem. This practice will solidify your understanding of how a noise constraint, defined by the Frobenius norm, directly determines the optimal thresholding level [@problem_id:3476275].", "problem": "Consider a noisy data matrix $Y \\in \\mathbb{R}^{m \\times n}$ obtained as $Y = X_{0} + W$, where $X_{0}$ is a low-rank signal matrix and $W$ has independent, zero-mean Gaussian entries with variance $\\sigma^{2}$. Let the singular value decomposition (SVD) of $Y$ be $Y = U \\Sigma V^{\\top}$, with singular values $\\{s_{i}\\}$ contained in the diagonal matrix $\\Sigma$. To denoise $Y$, we solve the constrained nuclear norm problem\n$$\n\\min_{X \\in \\mathbb{R}^{m \\times n}} \\|X\\|_{*} \\quad \\text{subject to} \\quad \\|Y - X\\|_{F} \\leq \\delta,\n$$\nwhere $\\|\\cdot\\|_{*}$ denotes the nuclear norm (sum of singular values), $\\|\\cdot\\|_{F}$ denotes the Frobenius norm, and the fidelity radius is chosen by the discrepancy principle $\\delta = \\sqrt{m n}\\,\\sigma$.\n\nStarting from the unitary invariance of both the nuclear norm and the Frobenius norm, and the characterization of proximity operators for spectral functions, derive the singular value shrinkage structure of the optimizer $X^{\\star}$ for the above constrained problem. Then, using the residual constraint, determine the threshold level that is applied to the singular values of $Y$.\n\nLet $m = 6$, $n = 7$, the noise level be $\\sigma = 1$, and the singular values of $Y$ be given by $\\{s_{i}\\} = \\{9, 5, 3, 1\\}$ (with $s_{1} \\geq s_{2} \\geq s_{3} \\geq s_{4} \\geq 0$). After applying the derived threshold, the denoised singular values are the shrunk values $\\{\\max(s_{i} - \\lambda, 0)\\}$ for a threshold $\\lambda$ determined by the fidelity constraint.\n\nReport the sum of the denoised singular values, that is, the nuclear norm $\\|X^{\\star}\\|_{*}$ of the denoised matrix. Express your final answer as a single real number. No rounding is required.", "solution": "The problem requires us to find the nuclear norm of a denoised matrix $X^{\\star}$, which is the solution to the constrained optimization problem:\n$$\n\\min_{X \\in \\mathbb{R}^{m \\times n}} \\|X\\|_{*} \\quad \\text{subject to} \\quad \\|Y - X\\|_{F} \\leq \\delta\n$$\nHere, $Y \\in \\mathbb{R}^{m \\times n}$ is the noisy data matrix, $\\|X\\|_{*}$ is the nuclear norm of $X$ (the sum of its singular values), $\\| \\cdot \\|_{F}$ is the Frobenius norm, and $\\delta$ is a given fidelity radius.\n\nFirst, we establish the structure of the optimal solution $X^{\\star}$. This is a convex optimization problem, as the nuclear norm is a convex function and the constraint defines a convex set (a ball in the Frobenius norm). We can analyze this problem using the method of Lagrange multipliers. The Lagrangian is:\n$$\nL(X, \\mu) = \\|X\\|_{*} + \\mu (\\|Y - X\\|_{F}^2 - \\delta^2)\n$$\nfor a Lagrange multiplier $\\mu \\geq 0$. Minimizing $L(X, \\mu)$ with respect to $X$ for a fixed $\\mu > 0$ is equivalent to solving:\n$$\n\\min_{X} \\|X\\|_{*} + \\mu \\|Y - X\\|_{F}^2\n$$\nThis can be rewritten as:\n$$\n\\min_{X} \\frac{1}{2}\\|X - Y\\|_{F}^2 + \\frac{1}{2\\mu}\\|X\\|_{*}\n$$\nLetting $\\lambda = \\frac{1}{2\\mu}$, the problem is to find the proximal operator of the nuclear norm:\n$$\nX^{\\star} = \\text{prox}_{\\lambda\\|\\cdot\\|_{*}}(Y) = \\arg\\min_{X} \\frac{1}{2}\\|X - Y\\|_{F}^2 + \\lambda\\|X\\|_{*}\n$$\nThe solution to this problem is given by the singular value soft-thresholding operator. If the singular value decomposition (SVD) of the noisy matrix $Y$ is $Y = U \\Sigma V^{\\top}$, where $U$ and $V$ are unitary matrices and $\\Sigma = \\text{diag}(s_1, s_2, \\dots, s_r)$ with $s_1 \\ge s_2 \\ge \\dots \\ge s_r \\ge 0$ being the singular values of $Y$, the optimal solution $X^{\\star}$ is given by:\n$$\nX^{\\star} = U \\Sigma_{\\lambda} V^{\\top} \\quad \\text{where} \\quad \\Sigma_{\\lambda} = \\text{diag}(\\max(s_i - \\lambda, 0))\n$$\nThis establishes that the optimal solution $X^{\\star}$ shares the same singular vectors as $Y$, and its singular values, which we denote as $s_i^{\\star}$, are obtained by soft-thresholding the singular values of $Y$ by a value $\\lambda$.\n\nNext, we must determine the value of the threshold $\\lambda$. For a non-trivial solution (where $X^{\\star} \\neq Y$), the KKT conditions imply that the constraint must be active, meaning $\\|Y - X^{\\star}\\|_{F} = \\delta$, or equivalently, $\\|Y - X^{\\star}\\|_{F}^2 = \\delta^2$. We can express the squared Frobenius norm of the residual using the SVD:\n$$\n\\|Y - X^{\\star}\\|_{F}^2 = \\|U \\Sigma V^{\\top} - U \\Sigma_{\\lambda} V^{\\top}\\|_{F}^2\n$$\nSince the Frobenius norm is unitarily invariant, this simplifies to:\n$$\n\\|Y - X^{\\star}\\|_{F}^2 = \\|\\Sigma - \\Sigma_{\\lambda}\\|_{F}^2 = \\sum_{i=1}^{\\min(m,n)} (s_i - s_i^{\\star})^2 = \\sum_{i=1}^{\\min(m,n)} (s_i - \\max(s_i - \\lambda, 0))^2\n$$\nWe analyze the term $(s_i - \\max(s_i - \\lambda, 0))^2$:\n- If $s_i > \\lambda$, then $\\max(s_i - \\lambda, 0) = s_i - \\lambda$, and the term becomes $(s_i - (s_i - \\lambda))^2 = \\lambda^2$.\n- If $s_i \\leq \\lambda$, then $\\max(s_i - \\lambda, 0) = 0$, and the term becomes $(s_i - 0)^2 = s_i^2$.\n\nThus, the condition $\\|Y - X^{\\star}\\|_{F}^2 = \\delta^2$ becomes the following equation for $\\lambda$:\n$$\n\\sum_{i: s_i > \\lambda} \\lambda^2 + \\sum_{i: s_i \\leq \\lambda} s_i^2 = \\delta^2\n$$\n\nNow, we apply this to the specific values given in the problem:\n- Dimensions: $m = 6$, $n = 7$.\n- Noise standard deviation: $\\sigma = 1$.\n- Fidelity radius: $\\delta = \\sqrt{m n}\\sigma = \\sqrt{6 \\times 7} \\times 1 = \\sqrt{42}$.\n- Squared fidelity radius: $\\delta^2 = 42$.\n- The given singular values of $Y$ are $\\{9, 5, 3, 1\\}$. Since $Y \\in \\mathbb{R}^{6 \\times 7}$, it has $\\min(6,7)=6$ singular values. The remaining singular values are zero. So, the full ordered set of singular values is $\\{s_i\\}_{i=1}^6 = \\{9, 5, 3, 1, 0, 0\\}$.\n\nWe search for $\\lambda$ by testing intervals defined by the singular values:\n1.  If $\\lambda \\geq 9$: All $s_i \\leq \\lambda$. The equation is $\\sum_{i=1}^6 s_i^2 = 9^2 + 5^2 + 3^2 + 1^2 + 0^2 + 0^2 = 81 + 25 + 9 + 1 = 116$. Since $116 \\neq 42$, this range is incorrect.\n2.  If $5 \\leq \\lambda < 9$: Only $s_1 > \\lambda$. The equation is $1 \\cdot \\lambda^2 + \\sum_{i=2}^6 s_i^2 = 42$.\n    $\\lambda^2 + (5^2 + 3^2 + 1^2 + 0^2 + 0^2) = 42 \\implies \\lambda^2 + 35 = 42 \\implies \\lambda^2 = 7 \\implies \\lambda = \\sqrt{7}$.\n    Since $\\sqrt{7} \\approx 2.65$, it does not fall within the assumed range $[5, 9)$.\n3.  If $3 \\leq \\lambda < 5$: $s_1 > \\lambda$ and $s_2 > \\lambda$. The equation is $2 \\cdot \\lambda^2 + \\sum_{i=3}^6 s_i^2 = 42$.\n    $2\\lambda^2 + (3^2 + 1^2 + 0^2 + 0^2) = 42 \\implies 2\\lambda^2 + 10 = 42 \\implies 2\\lambda^2 = 32 \\implies \\lambda^2 = 16 \\implies \\lambda = 4$.\n    This value $\\lambda=4$ falls within the assumed range $[3, 5)$. This is the correct threshold.\n\nHaving found the threshold $\\lambda = 4$, we can compute the denoised singular values $s_i^{\\star} = \\max(s_i - \\lambda, 0)$:\n- $s_1^{\\star} = \\max(9 - 4, 0) = 5$\n- $s_2^{\\star} = \\max(5 - 4, 0) = 1$\n- $s_3^{\\star} = \\max(3 - 4, 0) = 0$\n- $s_4^{\\star} = \\max(1 - 4, 0) = 0$\n- $s_5^{\\star} = \\max(0 - 4, 0) = 0$\n- $s_6^{\\star} = \\max(0 - 4, 0) = 0$\n\nThe question asks for the sum of the denoised singular values, which is the nuclear norm of the optimal solution $X^{\\star}$:\n$$\n\\|X^{\\star}\\|_{*} = \\sum_{i=1}^6 s_i^{\\star} = 5 + 1 + 0 + 0 + 0 + 0 = 6\n$$", "answer": "$$\n\\boxed{6}\n$$", "id": "3476275"}, {"introduction": "Moving from constrained optimization to a regularized formulation, this problem tackles the classic matrix completion task. You will perform a single step of the proximal gradient method, a foundational first-order algorithm for composite optimization problems. This exercise provides direct, hands-on experience with how the singular value thresholding (SVT) operator acts as the proximal mapping for the nuclear norm, forming the core non-smooth step of the algorithm [@problem_id:3476324].", "problem": "Consider the matrix completion objective in the setting of compressed sensing and sparse optimization, where the goal is to recover a low-rank matrix by minimizing a sum of a smooth data fidelity term and a nonsmooth nuclear norm regularizer. Let the optimization variable be a $3\\times 3$ real matrix $X$, and define the smooth term by the projection of the residual onto an index set of observed entries $\\Omega$. Specifically, let $\\Omega=\\{(1,1),(2,2),(3,3)\\}$ and define the linear projection operator $P_{\\Omega}:\\mathbb{R}^{3\\times 3}\\to\\mathbb{R}^{3\\times 3}$ by\n$$\n\\left(P_{\\Omega}(Z)\\right)_{ij}=\n\\begin{cases}\nZ_{ij}, & (i,j)\\in\\Omega,\\\\\n0, & (i,j)\\notin\\Omega,\n\\end{cases}\n$$\nfor any $Z\\in\\mathbb{R}^{3\\times 3}$. Let the observed data matrix be\n$$\nB=\\begin{pmatrix}\n3 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & \\frac{1}{2}\n\\end{pmatrix},\n$$\nand consider the composite objective\n$$\nF(X)=\\frac{1}{2}\\left\\|P_{\\Omega}(X-B)\\right\\|_{F}^{2}+\\lambda\\left\\|X\\right\\|_{*},\n$$\nwhere $\\|\\cdot\\|_{F}$ denotes the Frobenius norm and $\\|\\cdot\\|_{*}$ denotes the nuclear norm. Starting from the initial iterate $X^{(0)}=0$, perform one proximal gradient step with step size $\\tau=1$ and regularization parameter $\\lambda=\\frac{7}{10}$ to obtain the next iterate $X^{(1)}$. Using only standard definitions of the projection operator, the gradient of the smooth term, and the proximal operator of the nuclear norm, carry out this single step and then determine the nuclear norm $\\left\\|X^{(1)}\\right\\|_{*}$. Express your final answer as a single real number with no units. No rounding is required.", "solution": "The problem asks to perform one step of the proximal gradient method for the composite objective function $F(X) = f(X) + g(X)$, where $f(X) = \\frac{1}{2}\\left\\|P_{\\Omega}(X-B)\\right\\|_{F}^{2}$ is the smooth data fidelity term and $g(X) = \\lambda\\left\\|X\\right\\|_{*}$ is the nonsmooth nuclear norm regularizer.\n\nThe general update rule for the proximal gradient method is given by:\n$$\nX^{(k+1)} = \\text{prox}_{\\tau g}\\left(X^{(k)} - \\tau \\nabla f(X^{(k)})\\right)\n$$\nIn our case, $g(X) = \\lambda\\|X\\|_*$, so the proximal operator is $\\text{prox}_{\\tau \\lambda \\|\\cdot\\|_*}(\\cdot)$. We need to compute $X^{(1)}$ starting from the initial iterate $X^{(0)}=0$, using step size $\\tau=1$ and regularization parameter $\\lambda=\\frac{7}{10}$.\n\nThe update step is:\n$$\nX^{(1)} = \\text{prox}_{\\tau \\lambda \\|\\cdot\\|_*}\\left(X^{(0)} - \\tau \\nabla f(X^{(0)})\\right)\n$$\n\n**Step 1: Compute the gradient of the smooth term.**\nThe smooth term is $f(X) = \\frac{1}{2}\\left\\|P_{\\Omega}(X-B)\\right\\|_{F}^{2} = \\frac{1}{2} \\sum_{(i,j) \\in \\Omega} (X_{ij} - B_{ij})^2$. The gradient with respect to $X$ is the matrix whose elements are the partial derivatives $\\frac{\\partial f}{\\partial X_{kl}}$. This results in:\n$$\n\\nabla f(X) = P_{\\Omega}(X - B)\n$$\n\n**Step 2: Evaluate the gradient at the initial iterate $X^{(0)}$.**\nAt $X^{(0)}=0$ (the $3\\times 3$ zero matrix), the gradient is:\n$$\n\\nabla f(X^{(0)}) = P_{\\Omega}(0 - B) = -P_{\\Omega}(B)\n$$\nGiven that $B$ is a diagonal matrix and the observation set $\\Omega$ consists of the diagonal indices, $P_{\\Omega}(B) = B$. Thus:\n$$\n\\nabla f(X^{(0)}) = -B = \\begin{pmatrix} -3 & 0 & 0 \\\\ 0 & -1 & 0 \\\\ 0 & 0 & -1/2 \\end{pmatrix}\n$$\n\n**Step 3: Perform the gradient descent step.**\nWe compute the argument for the proximal operator, which we denote by $Y$:\n$$\nY = X^{(0)} - \\tau \\nabla f(X^{(0)})\n$$\nUsing the given values $X^{(0)}=0$ and $\\tau=1$:\n$$\nY = 0 - 1 \\cdot (-B) = B = \\begin{pmatrix} 3 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1/2 \\end{pmatrix}\n$$\n\n**Step 4: Apply the proximal operator (Singular Value Thresholding).**\nThe next iterate $X^{(1)}$ is found by applying the proximal operator for the nuclear norm to $Y$. This is the Singular Value Thresholding (SVT) operator, $S_{\\gamma}(\\cdot)$, with a threshold parameter $\\gamma = \\tau \\lambda$.\nWith $\\tau=1$ and $\\lambda=7/10$, the threshold is $\\gamma = 7/10$. So, we must compute $X^{(1)} = S_{7/10}(Y)$.\n\nThe matrix $Y=B$ is a diagonal matrix, so its singular values are the absolute values of its diagonal entries: $\\sigma_1 = 3$, $\\sigma_2 = 1$, and $\\sigma_3 = 1/2$.\nThe SVT operator applies soft-thresholding to these singular values: $\\hat{\\sigma}_i = \\max(0, \\sigma_i - \\gamma)$.\n$$\n\\hat{\\sigma}_1 = \\max\\left(0, 3 - \\frac{7}{10}\\right) = \\max\\left(0, \\frac{30-7}{10}\\right) = \\frac{23}{10}\n$$\n$$\n\\hat{\\sigma}_2 = \\max\\left(0, 1 - \\frac{7}{10}\\right) = \\max\\left(0, \\frac{10-7}{10}\\right) = \\frac{3}{10}\n$$\n$$\n\\hat{\\sigma}_3 = \\max\\left(0, \\frac{1}{2} - \\frac{7}{10}\\right) = \\max\\left(0, \\frac{5-7}{10}\\right) = \\max\\left(0, -\\frac{2}{10}\\right) = 0\n$$\n\n**Step 5: Calculate the nuclear norm of the resulting iterate.**\nThe resulting iterate $X^{(1)}$ has the thresholded singular values $\\{\\hat{\\sigma}_i\\}$. The problem asks for the nuclear norm of $X^{(1)}$, which is the sum of its singular values.\n$$\n\\|X^{(1)}\\|_* = \\sum_{i=1}^3 \\hat{\\sigma}_i = \\frac{23}{10} + \\frac{3}{10} + 0 = \\frac{26}{10} = \\frac{13}{5}\n$$", "answer": "$$\\boxed{\\frac{13}{5}}$$", "id": "3476324"}, {"introduction": "This final practice introduces a more powerful and versatile optimization framework: the Alternating Direction Method of Multipliers (ADMM). By splitting the problem into sub-problems for different variables, ADMM can handle complex constraints and regularizers efficiently. This exercise demonstrates how SVT is used as a key building block within an ADMM iteration, showcasing its modular role in state-of-the-art algorithms for low-rank matrix recovery [@problem_id:3476276].", "problem": "Consider the convex optimization problem that seeks a low-rank solution by minimizing a data-fitting term with an identity forward model and a nuclear norm regularizer: minimize $X \\in \\mathbb{R}^{2 \\times 2}$ \n$$\\frac{1}{2}\\|X - B\\|_{F}^{2} + \\lambda \\|Z\\|_{*} \\quad \\text{subject to} \\quad X = Z,$$\nwhere $\\|\\cdot\\|_{F}$ denotes the Frobenius norm and $\\|\\cdot\\|_{*}$ denotes the nuclear norm. Assume the Alternating Direction Method of Multipliers (ADMM) in its scaled dual form is used to solve this problem, where the scaled dual variable is denoted by $U$. Let the Alternating Direction Method of Multipliers (ADMM) penalty parameter be $\\rho$, and let the initial iterates be $Z^{0} = 0$, $U^{0} = 0$. The forward model is the identity $A = I$, the regularization weight is $\\lambda = 1$, the penalty parameter is $\\rho = 2$, and the input data matrix is \n$$B = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}.$$\n\nStarting from the fundamental definitions of the augmented Lagrangian and the proximal operator, perform one full ADMM iteration to compute $X^{1}$, $Z^{1}$, and the scaled dual update $U^{1}$. In particular:\n- Derive the $X$-update from the first-order optimality condition of the quadratic subproblem for $X$.\n- Derive the $Z$-update as the proximal map of the nuclear norm applied to $X^{1} + U^{0}$, using the definition that the proximal operator of the nuclear norm acts by soft-thresholding the singular values of its matrix argument.\n- Derive the scaled dual update $U^{1}$ from the ADMM scaled dual recursion.\n\nFinally, compute the squared Frobenius norm of the scaled dual variable after this single iteration, namely $\\|U^{1}\\|_{F}^{2}$. Express the final value as an exact number. No rounding is required.", "solution": "The problem requires performing one full iteration of the Alternating Direction Method of Multipliers (ADMM) for the given optimization problem. The problem is stated as:\n$$ \\underset{X, Z}{\\text{minimize}} \\quad \\frac{1}{2}\\|X - B\\|_{F}^{2} + \\lambda \\|Z\\|_{*} \\quad \\text{subject to} \\quad X = Z $$\nThe scaled augmented Lagrangian $L_{\\rho}$ for this problem is:\n$$ L_{\\rho}(X, Z, U) = \\frac{1}{2}\\|X - B\\|_{F}^{2} + \\lambda \\|Z\\|_{*} + \\frac{\\rho}{2}\\|X - Z + U\\|_{F}^{2} - \\frac{\\rho}{2}\\|U\\|_{F}^{2} $$\nwhere $U$ is the scaled dual variable. We are given $\\lambda=1$, $\\rho=2$, $B = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}$, and initial iterates $Z^0 = 0$, $U^0 = 0$.\n\nThe ADMM algorithm consists of three updates:\n\n**1. $X$-update:**\nThe first step is to minimize $L_{\\rho}(X, Z^0, U^0)$ with respect to $X$:\n$$ X^1 = \\underset{X}{\\operatorname{argmin}} \\left( \\frac{1}{2}\\|X - B\\|_{F}^{2} + \\frac{\\rho}{2}\\|X - Z^0 + U^0\\|_{F}^{2} \\right) $$\nThis is an unconstrained quadratic problem. We find the solution by setting the gradient with respect to $X$ to zero:\n$$ \\nabla_X L_{\\rho} = (X - B) + \\rho(X - Z^0 + U^0) = 0 $$\nSolving for $X$:\n$$ (1+\\rho)X = B + \\rho Z^0 - \\rho U^0 $$\n$$ X^1 = \\frac{1}{1+\\rho} (B + \\rho Z^0 - \\rho U^0) $$\nSubstituting the given values ($\\rho=2$, $Z^0=0$, $U^0=0$):\n$$ X^1 = \\frac{1}{1+2} (B + 0 - 0) = \\frac{1}{3}B = \\frac{1}{3} \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 2/3 & 0 \\\\ 0 & 1/3 \\end{pmatrix} $$\n\n**2. $Z$-update:**\nThe second step is to minimize $L_{\\rho}(X^1, Z, U^0)$ with respect to $Z$:\n$$ Z^1 = \\underset{Z}{\\operatorname{argmin}} \\left( \\lambda\\|Z\\|_{*} + \\frac{\\rho}{2}\\|X^1 - Z + U^0\\|_{F}^{2} \\right) $$\nThis is equivalent to solving for the proximal operator of the nuclear norm:\n$$ Z^1 = \\underset{Z}{\\operatorname{argmin}} \\left( \\frac{\\lambda}{\\rho}\\|Z\\|_{*} + \\frac{1}{2}\\|Z - (X^1 + U^0)\\|_{F}^{2} \\right) = \\operatorname{prox}_{\\lambda/\\rho, \\|\\cdot\\|_{*}}(X^1 + U^0) $$\nThe threshold for the singular value thresholding (SVT) operator is $\\tau = \\lambda/\\rho = 1/2$. The argument is $X^1 + U^0 = X^1$. The singular values of the diagonal matrix $X^1$ are $\\sigma_1 = 2/3$ and $\\sigma_2 = 1/3$.\nApplying soft-thresholding:\n$$ \\sigma'_1 = \\max(0, \\sigma_1 - \\tau) = \\max(0, 2/3 - 1/2) = \\max(0, 4/6 - 3/6) = 1/6 $$\n$$ \\sigma'_2 = \\max(0, \\sigma_2 - \\tau) = \\max(0, 1/3 - 1/2) = \\max(0, 2/6 - 3/6) = 0 $$\nThe matrix $Z^1$ is reconstructed with these new singular values:\n$$ Z^1 = \\begin{pmatrix} 1/6 & 0 \\\\ 0 & 0 \\end{pmatrix} $$\n\n**3. Dual variable $U$-update:**\nThe scaled dual variable is updated according to the recursion:\n$$ U^1 = U^0 + X^1 - Z^1 $$\nSubstituting the computed matrices:\n$$ U^1 = 0 + \\begin{pmatrix} 2/3 & 0 \\\\ 0 & 1/3 \\end{pmatrix} - \\begin{pmatrix} 1/6 & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 2/3 - 1/6 & 0 \\\\ 0 & 1/3 \\end{pmatrix} = \\begin{pmatrix} 3/6 & 0 \\\\ 0 & 1/3 \\end{pmatrix} $$\n$$ U^1 = \\begin{pmatrix} 1/2 & 0 \\\\ 0 & 1/3 \\end{pmatrix} $$\n\n**4. Final Calculation:**\nThe problem asks for the squared Frobenius norm of $U^1$:\n$$ \\|U^1\\|_{F}^{2} = \\sum_{i,j} (U^1_{ij})^2 = \\left(\\frac{1}{2}\\right)^2 + 0^2 + 0^2 + \\left(\\frac{1}{3}\\right)^2 $$\n$$ \\|U^1\\|_{F}^{2} = \\frac{1}{4} + \\frac{1}{9} = \\frac{9}{36} + \\frac{4}{36} = \\frac{13}{36} $$", "answer": "$$\\boxed{\\frac{13}{36}}$$", "id": "3476276"}]}