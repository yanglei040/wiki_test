{"hands_on_practices": [{"introduction": "The theory of matrix completion is deeply rooted in the geometry of low-rank matrices. The set of all matrices of a fixed rank forms a non-linear manifold, and understanding its local structure is the first step toward analyzing recovery guarantees. This exercise will guide you through a concrete calculation of the tangent space to this manifold at a specific point, demystifying this fundamental concept and providing a solid foundation for the more advanced topics that follow [@problem_id:3450073].", "problem": "Let $u\\in\\mathbb{R}^{3}$ and $v\\in\\mathbb{R}^{3}$ be given by $u=\\begin{pmatrix}1\\\\ 1\\\\ 2\\end{pmatrix}$ and $v=\\begin{pmatrix}1\\\\ 2\\\\ 3\\end{pmatrix}$, and let $M=uv^{\\top}\\in\\mathbb{R}^{3\\times 3}$, which has rank $1$. Let $E_{ij}\\in\\mathbb{R}^{3\\times 3}$ denote the standard basis matrix with a $1$ in position $(i,j)$ and zeros elsewhere. Equip $\\mathbb{R}^{3\\times 3}$ with the Frobenius inner product $\\langle A,B\\rangle=\\operatorname{trace}(A^{\\top}B)$ and its induced norm $\\|A\\|_{F}=\\sqrt{\\langle A,A\\rangle}$. Define the tangent space $T$ at $M$ to the manifold of real $3\\times 3$ matrices of rank exactly $1$ by the set of all velocity matrices at $t=0$ of smooth curves $t\\mapsto M(t)$ lying in that manifold with $M(0)=M$.\n\nStarting from the definitions above (in particular, the definition of tangent space via velocities of curves and the definition of orthogonal projection in an inner product space), carry out the following steps:\n\n1. Derive an explicit description of the tangent space $T$ at $M$ as a linear subspace of $\\mathbb{R}^{3\\times 3}$, expressed in terms of $u$ and $v$.\n2. Using only the definition of orthogonal projection with respect to the Frobenius inner product and your description of $T$, derive a closed-form expression for the orthogonal projector $\\mathcal{P}_{T}:\\mathbb{R}^{3\\times 3}\\to T$ in terms of the orthogonal projectors onto the column space $\\operatorname{span}\\{u\\}$ and the row space $\\operatorname{span}\\{v\\}$. Do not assume any pre-existing projection formula; derive it from first principles.\n3. Compute the orthogonal projectors $\\mathcal{P}_{U}$ and $\\mathcal{P}_{V}$ onto $U=\\operatorname{span}\\{u\\}$ and $V=\\operatorname{span}\\{v\\}$, respectively, as $3\\times 3$ matrices.\n4. Compute $\\mathcal{P}_{T}(E_{23})$ explicitly as a $3\\times 3$ matrix and verify directly from the Frobenius inner product that $E_{23}-\\mathcal{P}_{T}(E_{23})$ is orthogonal to all elements of $T$.\n5. Finally, compute the single scalar quantity $\\|\\mathcal{P}_{T}(E_{23})\\|_{F}^{2}$ and report this value as your final answer. No rounding is required; give the exact value as a reduced fraction.", "solution": "The problem as stated is a well-defined exercise in linear algebra and differential geometry, concerning the tangent space of a matrix manifold and orthogonal projection. All provided data and definitions are standard and mathematically consistent. The problem is valid.\n\nThe solution proceeds in five steps as outlined in the problem statement.\n\nStep 1: Characterization of the tangent space $T$.\nThe manifold of rank-$1$ matrices in $\\mathbb{R}^{3 \\times 3}$ consists of matrices that can be written as $ab^{\\top}$ for non-zero vectors $a, b \\in \\mathbb{R}^{3}$. The tangent space $T$ at the point $M = uv^{\\top}$ is defined as the set of all velocity vectors $\\dot{M}(0)$ of smooth curves $t \\mapsto M(t)$ in the manifold, where $M(0) = M$.\nLet $M(t) = a(t)b(t)^{\\top}$ be such a smooth curve, with $a(0) = u$ and $b(0) = v$. The velocity vector at $t=0$ is obtained by the product rule:\n$$\n\\dot{M}(0) = \\left. \\frac{d}{dt} \\left( a(t)b(t)^{\\top} \\right) \\right|_{t=0} = \\dot{a}(0)b(0)^{\\top} + a(0)\\dot{b}(0)^{\\top}\n$$\nLet $x = \\dot{a}(0) \\in \\mathbb{R}^{3}$ and $y = \\dot{b}(0) \\in \\mathbb{R}^{3}$. Since $a(t)$ and $b(t)$ can be any smooth curves in $\\mathbb{R}^3$ starting at $u$ and $v$ respectively, their initial velocities $x$ and $y$ can be any vectors in $\\mathbb{R}^{3}$. Thus, any element $Z \\in T$ can be expressed in the form $Z = xv^{\\top} + uy^{\\top}$ for some $x, y \\in \\mathbb{R}^{3}$.\nThe set of all such matrices forms a linear subspace of $\\mathbb{R}^{3 \\times 3}$:\n$$\nT = \\{ xv^{\\top} + uy^{\\top} \\mid x \\in \\mathbb{R}^3, y \\in \\mathbb{R}^3 \\}\n$$\nThis is the sum of two subspaces: $T_v = \\{xv^{\\top} \\mid x \\in \\mathbb{R}^3\\}$, the space of matrices whose row space is contained in $\\operatorname{span}\\{v\\}$, and $T_u = \\{uy^{\\top} \\mid y \\in \\mathbb{R}^3\\}$, the space of matrices whose column space is contained in $\\operatorname{span}\\{u\\}$. So, $T = T_v + T_u$.\n\nStep 2: Derivation of the orthogonal projector $\\mathcal{P}_T$.\nThe orthogonal projector $\\mathcal{P}_T: \\mathbb{R}^{3\\times 3} \\to T$ maps a matrix $Z$ to its unique best approximation in $T$. This projection is characterized by the property that $Z - \\mathcal{P}_T(Z)$ is orthogonal to every element of $T$ with respect to the Frobenius inner product.\nThe representation $W = xv^{\\top} + uy^{\\top}$ for $W \\in T$ is not unique, as the subspaces $T_u$ and $T_v$ have a non-trivial intersection, $\\operatorname{span}\\{uv^\\top\\}$. To obtain a unique representation, we decompose $T$ into a direct sum of orthogonal subspaces. We can write $T = T' \\oplus T_u$, where $T' = \\{xv^{\\top} \\mid x \\in (\\operatorname{span}\\{u\\})^\\perp\\}$.\nLet's verify that $T'$ and $T_u$ are orthogonal. Let $W_1 = xv^{\\top} \\in T'$ (so $u^{\\top}x = 0$) and $W_2 = uy^{\\top} \\in T_u$. Their inner product is:\n$$\n\\langle W_1, W_2 \\rangle = \\operatorname{trace}(W_1^{\\top}W_2) = \\operatorname{trace}((xv^{\\top})^{\\top}(uy^{\\top})) = \\operatorname{trace}(vx^{\\top}uy^{\\top})\n$$\nUsing the cyclic property of the trace, $\\operatorname{trace}(ABCD) = \\operatorname{trace}(DABC)$, we get:\n$$\n\\langle W_1, W_2 \\rangle = \\operatorname{trace}(y^{\\top}vx^{\\top}u) = (y^{\\top}v)(x^{\\top}u)\n$$\nSince $x^{\\top}u=0$ by definition of $T'$, we have $\\langle W_1, W_2 \\rangle = 0$. Thus, $T'$ and $T_u$ are orthogonal subspaces.\nThe projector onto the direct sum is the sum of the projectors: $\\mathcal{P}_T = \\mathcal{P}_{T'} + \\mathcal{P}_{T_u}$.\nLet us derive $\\mathcal{P}_{T_u}(Z)$. We seek $uy^{\\top}$ that minimizes $\\|Z - uy^{\\top}\\|_F^2$. The normal equations are found by setting the derivative with respect to $y$ to zero, which gives $y = \\frac{Z^{\\top}u}{\\|u\\|^2}$. Thus,\n$$\n\\mathcal{P}_{T_u}(Z) = u \\left( \\frac{Z^{\\top}u}{\\|u\\|^2} \\right)^{\\top} = u \\frac{u^{\\top}Z}{\\|u\\|^2} = \\frac{uu^{\\top}}{\\|u\\|^2} Z = \\mathcal{P}_U Z\n$$\nwhere $\\mathcal{P}_U = \\frac{uu^\\top}{\\|u\\|^2}$ is the matrix for orthogonal projection onto $\\operatorname{span}\\{u\\}$.\nBy a completely symmetric argument for the subspace $T_v = \\{xv^\\top \\mid x \\in \\mathbb{R}^3\\}$, the projector is $\\mathcal{P}_{T_v}(Z) = Z \\frac{vv^\\top}{\\|v\\|^2} = Z \\mathcal{P}_V$.\nNow, to find $\\mathcal{P}_{T'}(Z)$, we note that $T' = \\{X \\in T_v \\mid X \\text{ is orthogonal to } T_u \\cap T_v \\text{ in } T_v \\}$. A more direct way is to notice that any $x \\in \\mathbb{R}^3$ can be uniquely decomposed as $x = \\mathcal{P}_U x + (I-\\mathcal{P}_U)x$. An element of $T_v$ is $( \\mathcal{P}_U x + (I - \\mathcal{P}_U)x ) v^\\top = (\\mathcal{P}_U x)v^\\top + ((I - \\mathcal{P}_U)x)v^\\top$. The first term belongs to $\\operatorname{span}\\{uv^\\top\\}$, which is $T_u \\cap T_v$. The second term represents an element of $T'$, because $u^\\top(I - \\mathcal{P}_U)x=0$.\nSo, $\\mathcal{P}_{T'}(Z)$ is the projection of $Z$ onto the elements of $T_v$ that are built from vectors orthogonal to $u$. This corresponds to first projecting $Z$ onto $T_v$, yielding $Z\\mathcal{P}_V$, and then removing the component that lies in $T_u$. This is achieved by applying $(I - \\mathcal{P}_U)$ on the left. So, $\\mathcal{P}_{T'}(Z) = (I-\\mathcal{P}_U) (Z \\mathcal{P}_V) = Z\\mathcal{P}_V - \\mathcal{P}_U Z \\mathcal{P}_V$.\nCombining the projectors:\n$$\n\\mathcal{P}_T(Z) = \\mathcal{P}_{T'}(Z) + \\mathcal{P}_{T_u}(Z) = (Z\\mathcal{P}_V - \\mathcal{P}_U Z \\mathcal{P}_V) + \\mathcal{P}_U Z\n$$\nThe resulting closed-form expression for the projector is:\n$$\n\\mathcal{P}_T(Z) = \\mathcal{P}_U Z + Z \\mathcal{P}_V - \\mathcal{P}_U Z \\mathcal{P}_V\n$$\n\nStep 3: Computation of the projectors $\\mathcal{P}_U$ and $\\mathcal{P}_V$.\nThe given vectors are $u=\\begin{pmatrix}1\\\\ 1\\\\ 2\\end{pmatrix}$ and $v=\\begin{pmatrix}1\\\\ 2\\\\ 3\\end{pmatrix}$.\nFirst, we compute the squared norms:\n$$\n\\|u\\|^2 = u^\\top u = 1^2 + 1^2 + 2^2 = 1+1+4 = 6\n$$\n$$\n\\|v\\|^2 = v^\\top v = 1^2 + 2^2 + 3^2 = 1+4+9 = 14\n$$\nThe projector matrices are:\n$$\n\\mathcal{P}_U = \\frac{uu^\\top}{\\|u\\|^2} = \\frac{1}{6} \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\end{pmatrix} \\begin{pmatrix} 1 & 1 & 2 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 1 & 1 & 2 \\\\ 1 & 1 & 2 \\\\ 2 & 2 & 4 \\end{pmatrix}\n$$\n$$\n\\mathcal{P}_V = \\frac{vv^\\top}{\\|v\\|^2} = \\frac{1}{14} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix} = \\frac{1}{14} \\begin{pmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 6 \\\\ 3 & 6 & 9 \\end{pmatrix}\n$$\n\nStep 4: Computation of $\\mathcal{P}_T(E_{23})$ and orthogonality verification.\nWe compute $\\mathcal{P}_T(E_{23})$ using the formula from Step 2 with $Z=E_{23} = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix}$.\nTerm 1: $\\mathcal{P}_U E_{23} = \\frac{1}{6} \\begin{pmatrix} 1 & 1 & 2 \\\\ 1 & 1 & 2 \\\\ 2 & 2 & 4 \\end{pmatrix} \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 2 \\end{pmatrix}$.\nTerm 2: $E_{23} \\mathcal{P}_V = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix} \\frac{1}{14} \\begin{pmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 6 \\\\ 3 & 6 & 9 \\end{pmatrix} = \\frac{1}{14} \\begin{pmatrix} 0 & 0 & 0 \\\\ 3 & 6 & 9 \\\\ 0 & 0 & 0 \\end{pmatrix}$.\nTerm 3: $\\mathcal{P}_U E_{23} \\mathcal{P}_V = \\frac{1}{\\|u\\|^2 \\|v\\|^2} u (u^\\top E_{23} v) v^\\top$.\nThe scalar term is $u^\\top E_{23} v = \\begin{pmatrix} 1 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3 \\\\ 0 \\end{pmatrix} = 3$.\nSo, $\\mathcal{P}_U E_{23} \\mathcal{P}_V = \\frac{3}{6 \\cdot 14} uv^\\top = \\frac{1}{28} uv^\\top = \\frac{1}{28} \\begin{pmatrix} 1 & 2 & 3 \\\\ 1 & 2 & 3 \\\\ 2 & 4 & 6 \\end{pmatrix}$.\nCombining the terms with common denominator $84$:\n$$\n\\mathcal{P}_T(E_{23}) = \\frac{14}{84} \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 2 \\end{pmatrix} + \\frac{6}{84} \\begin{pmatrix} 0 & 0 & 0 \\\\ 3 & 6 & 9 \\\\ 0 & 0 & 0 \\end{pmatrix} - \\frac{3}{84} \\begin{pmatrix} 1 & 2 & 3 \\\\ 1 & 2 & 3 \\\\ 2 & 4 & 6 \\end{pmatrix}\n$$\n$$\n\\mathcal{P}_T(E_{23}) = \\frac{1}{84} \\left( \\begin{pmatrix} 0 & 0 & 14 \\\\ 0 & 0 & 14 \\\\ 0 & 0 & 28 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 & 0 \\\\ 18 & 36 & 54 \\\\ 0 & 0 & 0 \\end{pmatrix} - \\begin{pmatrix} 3 & 6 & 9 \\\\ 3 & 6 & 9 \\\\ 6 & 12 & 18 \\end{pmatrix} \\right) = \\frac{1}{84} \\begin{pmatrix} -3 & -6 & 5 \\\\ 15 & 30 & 59 \\\\ -6 & -12 & 10 \\end{pmatrix}\n$$\nNow we compute the error $Q = E_{23} - \\mathcal{P}_T(E_{23})$:\n$$\nQ = \\frac{1}{84}\\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 84 \\\\ 0 & 0 & 0 \\end{pmatrix} - \\frac{1}{84} \\begin{pmatrix} -3 & -6 & 5 \\\\ 15 & 30 & 59 \\\\ -6 & -12 & 10 \\end{pmatrix} = \\frac{1}{84} \\begin{pmatrix} 3 & 6 & -5 \\\\ -15 & -30 & 25 \\\\ 6 & 12 & -10 \\end{pmatrix}\n$$\nTo verify orthogonality, we must show $\\langle Q, W \\rangle = 0$ for all $W \\in T$. It is sufficient to show this for the generating elements, i.e., show that $Q$ is orthogonal to all matrices of the form $uy^{\\top}$ and $xv^{\\top}$. This is equivalent to showing $Q^\\top u = 0$ and $Qv = 0$.\n$$\n84 Q v = \\begin{pmatrix} 3 & 6 & -5 \\\\ -15 & -30 & 25 \\\\ 6 & 12 & -10 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 3+12-15 \\\\ -15-60+75 \\\\ 6+24-30 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\implies Qv=0.\n$$\n$$\n84 Q^\\top u = \\begin{pmatrix} 3 & -15 & 6 \\\\ 6 & -30 & 12 \\\\ -5 & 25 & -10 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 3-15+12 \\\\ 6-30+24 \\\\ -5+25-20 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\implies Q^\\top u=0.\n$$\nThe orthogonality is verified.\n\nStep 5: Computation of $\\|\\mathcal{P}_{T}(E_{23})\\|_{F}^{2}$.\nFor any orthogonal projector $\\mathcal{P}$, it is self-adjoint, so $\\|\\mathcal{P}(Z)\\|_F^2 = \\langle \\mathcal{P}(Z), \\mathcal{P}(Z) \\rangle = \\langle Z, \\mathcal{P}(Z) \\rangle$.\nWith $Z = E_{23}$ and for any matrix $A$, the inner product is $\\langle E_{23}, A \\rangle = \\operatorname{trace}(E_{23}^\\top A) = A_{23}$. Therefore,\n$$\n\\|\\mathcal{P}_{T}(E_{23})\\|_{F}^{2} = \\langle E_{23}, \\mathcal{P}_{T}(E_{23}) \\rangle = (\\mathcal{P}_T(E_{23}))_{23}\n$$\nFrom our calculation in Step 4, the $(2,3)$ entry of $\\mathcal{P}_T(E_{23})$ is $\\frac{59}{84}$. Alternatively, we can calculate this entry directly:\n$$\n(\\mathcal{P}_T(E_{23}))_{23} = (\\mathcal{P}_U E_{23})_{23} + (E_{23} \\mathcal{P}_V)_{23} - (\\mathcal{P}_U E_{23} \\mathcal{P}_V)_{23}\n$$\nThese quantities were:\n$(\\mathcal{P}_U E_{23})_{23} = \\frac{1}{6}$ (this is $\\frac{u_2^2}{\\|u\\|^2} = \\frac{1^2}{6}$).\n$(E_{23} \\mathcal{P}_V)_{23} = \\frac{9}{14}$ (this is $\\frac{v_3^2}{\\|v\\|^2} = \\frac{3^2}{14}$).\n$(\\mathcal{P}_U E_{23} \\mathcal{P}_V)_{23} = \\frac{3}{28}$ (this is $\\frac{(u^\\top E_{23} v) u_2 v_3}{\\|u\\|^2 \\|v\\|^2} = \\frac{3 \\cdot 1 \\cdot 3}{6 \\cdot 14} = \\frac{9}{84} = \\frac{3}{28}$).\nSumming these gives:\n$$\n\\|\\mathcal{P}_{T}(E_{23})\\|_{F}^{2} = \\frac{1}{6} + \\frac{9}{14} - \\frac{3}{28} = \\frac{14}{84} + \\frac{54}{84} - \\frac{9}{84} = \\frac{14+54-9}{84} = \\frac{59}{84}\n$$\nThe value $59$ is prime and does not divide $84$, so the fraction is irreducible.", "answer": "$$\n\\boxed{\\frac{59}{84}}\n$$", "id": "3450073"}, {"introduction": "How can we be certain that the solution found by nuclear norm minimization is the true low-rank matrix we seek? The answer often lies in constructing a \"dual certificate,\" a mathematical witness that validates the solution's uniqueness and correctness. This practice moves from the abstract geometry of tangent spaces to the powerful mechanics of dual certification, showing how the properties of this witness are tied to the observed data and the structure of the underlying matrix [@problem_id:3450067]. By explicitly building a dual certificate for a small-scale problem, you will gain direct insight into this core proof technique.", "problem": "Consider the nuclear norm minimization problem for matrix completion: minimize the nuclear norm of a matrix $X \\in \\mathbb{R}^{2 \\times 2}$ subject to agreement with a partially observed matrix on an index set $\\Omega$. A sufficient condition for exact recovery of a rank-$1$ matrix $M$ by this program is the existence of a dual certificate $Y \\in \\mathbb{R}^{2 \\times 2}$ such that $P_{\\Omega}(Y) = Y$, the projection of $Y$ onto the tangent space $T$ at $M$ equals $U V^{\\top}$ (where $M = U \\Sigma V^{\\top}$ is the singular value decomposition with $U \\in \\mathbb{R}^{2 \\times 1}$ and $V \\in \\mathbb{R}^{2 \\times 1}$), and the operator norm $\\| P_{T^{\\perp}}(Y) \\|$ is strictly less than $1$. The tangent space at a rank-$1$ matrix $M = \\sigma u v^{\\top}$ is the set $T = \\{ u x^{\\top} + y v^{\\top} : x \\in \\mathbb{R}^{2}, y \\in \\mathbb{R}^{2} \\}$, and $T^{\\perp}$ denotes its orthogonal complement with respect to the Frobenius inner product.\n\nLet $M = a b^{\\top}$ with $a = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$ and $b = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$, so that\n$$\nM \\,=\\, \\begin{pmatrix} 3 & 1 \\\\ 6 & 2 \\end{pmatrix}.\n$$\nLet the observed index set be $\\Omega = \\{ (1,1), (1,2), (2,1) \\}$, and define the sampling projection $P_{\\Omega}$ by $(P_{\\Omega}(Z))_{ij} = Z_{ij}$ if $(i,j) \\in \\Omega$ and $(P_{\\Omega}(Z))_{ij} = 0$ otherwise.\n\nUsing only the core definitions above and standard properties of singular value decomposition and orthogonal projections, explicitly construct a dual certificate $Y \\in \\mathbb{R}^{2 \\times 2}$ satisfying $P_{\\Omega}(Y) = Y$ and $P_T(Y) = u v^{\\top}$, where $u = a / \\| a \\|_{2}$ and $v = b / \\| b \\|_{2}$. Work in an orthonormal basis $\\{ u, u_{\\perp} \\}$ for $\\mathbb{R}^{2}$ and $\\{ v, v_{\\perp} \\}$ for $\\mathbb{R}^{2}$, where $u_{\\perp}$ and $v_{\\perp}$ are unit vectors orthogonal to $u$ and $v$, respectively. Then compute the exact value of the operator norm $\\| P_{T^{\\perp}}(Y) \\|$ for your constructed $Y$.\n\nYour final answer must be a single real number in exact form (no rounding).", "solution": "The objective is to construct a dual certificate matrix $Y \\in \\mathbb{R}^{2 \\times 2}$ satisfying the given conditions and then to compute the operator norm of its projection onto the orthogonal complement of the tangent space $T$.\n\nFirst, we establish the orthonormal vectors that define the coordinate system for our space of matrices. The given vectors are $a = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$ and $b = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$.\nThe corresponding unit vectors $u$ and $v$ are:\n$$u = \\frac{a}{\\|a\\|_2} = \\frac{1}{\\sqrt{1^2 + 2^2}} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$$\n$$v = \\frac{b}{\\|b\\|_2} = \\frac{1}{\\sqrt{3^2 + 1^2}} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{10}} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$$\nNext, we construct unit vectors $u_{\\perp}$ and $v_{\\perp}$ that are orthogonal to $u$ and $v$, respectively.\nFor $u = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$, an orthogonal vector is $\\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix}$. Normalizing it gives:\n$$u_{\\perp} = \\frac{1}{\\sqrt{(-2)^2 + 1^2}} \\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix}$$\nFor $v = \\frac{1}{\\sqrt{10}} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$, an orthogonal vector is $\\begin{pmatrix} -1 \\\\ 3 \\end{pmatrix}$. Normalizing it gives:\n$$v_{\\perp} = \\frac{1}{\\sqrt{(-1)^2 + 3^2}} \\begin{pmatrix} -1 \\\\ 3 \\end{pmatrix} = \\frac{1}{\\sqrt{10}} \\begin{pmatrix} -1 \\\\ 3 \\end{pmatrix}$$\nThe set of matrices $\\{u v^{\\top}, u v_{\\perp}^{\\top}, u_{\\perp} v^{\\top}, u_{\\perp} v_{\\perp}^{\\top}\\}$ forms an orthonormal basis for the space $\\mathbb{R}^{2 \\times 2}$ under the Frobenius inner product $\\langle A, B \\rangle_F = \\mathrm{tr}(A^{\\top}B)$.\n\nThe tangent space $T$ at the rank-$1$ matrix $M$ is defined as $T = \\{ u x^{\\top} + y v^{\\top} : x \\in \\mathbb{R}^{2}, y \\in \\mathbb{R}^{2} \\}$.\nAny vector $x \\in \\mathbb{R}^2$ can be written as a linear combination of $v$ and $v_{\\perp}$, and any vector $y \\in \\mathbb{R}^2$ can be written as a linear combination of $u$ and $u_{\\perp}$. Therefore, any element in $T$ is a linear combination of the matrices $u v^{\\top}$, $u v_{\\perp}^{\\top}$, and $u_{\\perp} v^{\\top}$. Thus, $T$ is the subspace spanned by these three matrices:\n$$T = \\mathrm{span}\\{ u v^{\\top}, u v_{\\perp}^{\\top}, u_{\\perp} v^{\\top} \\}$$\nThe orthogonal complement $T^{\\perp}$ is the subspace spanned by the remaining basis matrix:\n$$T^{\\perp} = \\mathrm{span}\\{ u_{\\perp} v_{\\perp}^{\\top} \\}$$\nAny matrix $Z \\in \\mathbb{R}^{2 \\times 2}$ can be decomposed as $Z = P_T(Z) + P_{T^{\\perp}}(Z)$, where $P_T(Z)$ is the projection onto $T$ and $P_{T^{\\perp}}(Z)$ is the projection onto $T^{\\perp}$. The projection onto $T^{\\perp}$ is given by:\n$$P_{T^{\\perp}}(Z) = \\langle Z, u_{\\perp} v_{\\perp}^{\\top} \\rangle_F \\, u_{\\perp} v_{\\perp}^{\\top}$$\n\nWe are asked to construct a dual certificate $Y$ that satisfies two conditions:\n1. $P_{\\Omega}(Y) = Y$. Since the observed index set is $\\Omega = \\{ (1,1), (1,2), (2,1) \\}$, this condition implies that the entry of $Y$ at the unobserved index $(2,2)$ must be zero. Let $Y = \\begin{pmatrix} y_{11} & y_{12} \\\\ y_{21} & y_{22} \\end{pmatrix}$, then $y_{22} = 0$.\n2. $P_T(Y) = u v^{\\top}$.\n\nUsing the decomposition $Y = P_T(Y) + P_{T^{\\perp}}(Y)$ and the given condition, we can write $Y$ as:\n$$Y = u v^{\\top} + P_{T^{\\perp}}(Y)$$\nLet's define a scalar $c = \\langle Y, u_{\\perp} v_{\\perp}^{\\top} \\rangle_F$. Then $P_{T^{\\perp}}(Y) = c \\, u_{\\perp} v_{\\perp}^{\\top}$.\nSo, the matrix $Y$ has the form:\n$$Y = u v^{\\top} + c \\, u_{\\perp} v_{\\perp}^{\\top}$$\nWe can determine the value of the scalar $c$ by using the condition $y_{22} = 0$. The $(2,2)$-entry of $Y$ is given by:\n$$y_{22} = (u v^{\\top})_{22} + c(u_{\\perp} v_{\\perp}^{\\top})_{22}$$\nThe elements of the matrices are products of the corresponding vector components. Let $u = \\begin{pmatrix} u_1 \\\\ u_2 \\end{pmatrix}$, $v = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}$, etc.\n$$y_{22} = u_2 v_2 + c \\, u_{\\perp,2} v_{\\perp,2}$$\nFrom our definitions of the vectors:\n$u_2 = \\frac{2}{\\sqrt{5}}$, $v_2 = \\frac{1}{\\sqrt{10}}$\n$u_{\\perp,2} = \\frac{1}{\\sqrt{5}}$, $v_{\\perp,2} = \\frac{3}{\\sqrt{10}}$\nSubstituting these values into the equation for $y_{22}$:\n$$y_{22} = \\left(\\frac{2}{\\sqrt{5}}\\right)\\left(\\frac{1}{\\sqrt{10}}\\right) + c \\left(\\frac{1}{\\sqrt{5}}\\right)\\left(\\frac{3}{\\sqrt{10}}\\right) = \\frac{2}{\\sqrt{50}} + c \\frac{3}{\\sqrt{50}} = \\frac{1}{\\sqrt{50}}(2 + 3c)$$\nSetting $y_{22} = 0$:\n$$\\frac{1}{\\sqrt{50}}(2 + 3c) = 0 \\implies 2 + 3c = 0 \\implies c = -\\frac{2}{3}$$\nNow we have found the scalar $c$ that defines the projection of our constructed $Y$ onto $T^{\\perp}$.\n$$P_{T^{\\perp}}(Y) = -\\frac{2}{3} u_{\\perp} v_{\\perp}^{\\top}$$\nThe final step is to compute the operator norm of this matrix. The operator norm of a rank-$1$ matrix $x y^{\\top}$ is given by $\\|x y^{\\top}\\| = \\|x\\|_2 \\|y\\|_2$.\n$$\\| P_{T^{\\perp}}(Y) \\| = \\left\\| -\\frac{2}{3} u_{\\perp} v_{\\perp}^{\\top} \\right\\| = \\left|-\\frac{2}{3}\\right| \\|u_{\\perp}\\|_2 \\|v_{\\perp}\\|_2$$\nBy construction, $u_{\\perp}$ and $v_{\\perp}$ are unit vectors, so $\\|u_{\\perp}\\|_2 = 1$ and $\\|v_{\\perp}\\|_2 = 1$.\nTherefore, the operator norm is:\n$$\\| P_{T^{\\perp}}(Y) \\| = \\frac{2}{3} \\times 1 \\times 1 = \\frac{2}{3}$$\nThis value is strictly less than $1$, satisfying the final condition for a dual certificate, although we were only asked to compute the value itself.", "answer": "$$\\boxed{\\frac{2}{3}}$$", "id": "3450067"}, {"introduction": "Theoretical guarantees are powerful, but their conditions can be abstract. This final practice challenges you to bridge the gap between theory and computation by designing an algorithm that attempts to construct and verify a dual certificate for a given problem instance [@problem_id:3450116]. You will translate the mathematical conditions of dual feasibility into a system of linear equations and a norm check, creating a practical tool that can certify, post-recovery, whether a found solution is indeed the ground truth. This exercise highlights how theoretical concepts become actionable computational routines.", "problem": "Consider the convex relaxation of matrix completion via nuclear norm minimization: minimize the nuclear norm of a matrix subject to equality constraints on observed entries. Let a target low-rank matrix be given by the thin Singular Value Decomposition (SVD) as $X_\\star = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{n \\times r}$ and $V \\in \\mathbb{R}^{m \\times r}$ have orthonormal columns and $\\Sigma \\in \\mathbb{R}^{r \\times r}$ is invertible. Let $\\Omega \\subset \\{1,\\dots,n\\} \\times \\{1,\\dots,m\\}$ be the index set of observed entries, and let $P_{\\Omega}$ denote the orthogonal projection that zeroes out entries outside $\\Omega$. Define the tangent space at $X_\\star$ as $T = \\{ U B^{\\top} + C V^{\\top} : B \\in \\mathbb{R}^{m \\times r}, C \\in \\mathbb{R}^{n \\times r} \\}$ and its orthogonal complement $T^{\\perp}$. The orthogonal projection onto $T$ is given by $P_T(M) = U U^{\\top} M + M V V^{\\top} - U U^{\\top} M V V^{\\top}$ for any $M \\in \\mathbb{R}^{n \\times m}$, and $P_{T^{\\perp}}(M) = M - P_T(M)$.\n\nA well-tested fact from convex duality for nuclear norm minimization asserts the following: if there exists a matrix $Y$ in the range of $P_{\\Omega}$ such that $P_T(Y) = U V^{\\top}$ and $\\|P_{T^{\\perp}}(Y)\\|_{2} < 1$ where $\\|\\cdot\\|_{2}$ denotes the operator norm (largest singular value), then $X_\\star$ is the unique minimizer of the nuclear norm among all matrices $X$ satisfying $P_{\\Omega}(X) = P_{\\Omega}(X_\\star)$.\n\nStarting from these core definitions and facts, design an algorithmic routine that, given $n$, $m$, $r$, an index set $\\Omega$, and orthonormal matrices $U \\in \\mathbb{R}^{n \\times r}$ and $V \\in \\mathbb{R}^{m \\times r}$, attempts to construct a dual certificate $Y$ with the properties:\n- $Y \\in \\operatorname{range}(P_{\\Omega})$, that is, $Y_{ij} = 0$ for all $(i,j) \\notin \\Omega$,\n- $P_T(Y) = U V^{\\top}$,\n- $\\|P_{T^{\\perp}}(Y)\\|_{2} < 1$.\n\nYour algorithm should proceed from first principles as follows:\n- Recognize that the condition $Y \\in \\operatorname{range}(P_{\\Omega})$ allows parameterizing $Y$ entirely by entries indexed by $\\Omega$.\n- Formulate the linear equation $P_T(Y) = U V^{\\top}$ in terms of the unknown entries of $Y$ indexed by $\\Omega$, viewing $P_T$ as a linear operator on matrices.\n- Solve this linear system in the least-squares sense. If the residual is below a small tolerance, treat the system as feasible and proceed to compute $\\|P_{T^{\\perp}}(Y)\\|_{2}$.\n- Certify success if both the residual is sufficiently small and $\\|P_{T^{\\perp}}(Y)\\|_{2} < 1$.\n\nYour program must implement this routine and evaluate it on the following test suite, where in each case $U$ and $V$ are any orthonormal-column matrices generated deterministically by your code for reproducibility, and the target matrix is $X_\\star = U \\Sigma V^{\\top}$ (only $U$ and $V$ are required by the routine):\n\n- Test case $1$ (happy path): $n = 5$, $m = 4$, $r = 2$, and $\\Omega = \\{1,\\dots,5\\} \\times \\{1,\\dots,4\\}$ (the full set of indices).\n- Test case $2$ (infeasible due to severely undersampled $\\Omega$): $n = 6$, $m = 6$, $r = 2$, and $\\Omega = \\{(i,i) : i \\in \\{1,2,3,4,5,6\\}\\}$ (the main diagonal only).\n- Test case $3$ (boundary case): $n = 4$, $m = 6$, $r = 1$, and $\\Omega = \\varnothing$ (the empty set).\n\nFor each test case, your program must output a boolean value: $1$ for success if the dual certificate is constructed with both $P_T(Y) = U V^{\\top}$ (within numerical tolerance) and $\\|P_{T^{\\perp}}(Y)\\|_{2} < 1$, and $0$ for failure otherwise. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[1,0,1]$.\n\nNo physical units or angles are involved. All numerical comparisons must use a clear, specified tolerance, and all singular value computations must use the standard operator norm on $\\mathbb{R}^{n \\times m}$.", "solution": "The problem asks for the design and implementation of an algorithm to construct a dual certificate $Y \\in \\mathbb{R}^{n \\times m}$ for the matrix completion problem. The existence of such a certificate guarantees that a specific low-rank matrix $X_\\star = U \\Sigma V^{\\top}$ is the unique solution to the nuclear norm minimization problem. The conditions for $Y$ are:\n1.  $Y \\in \\operatorname{range}(P_{\\Omega})$, which means $Y_{ij} = 0$ for any index pair $(i,j)$ not in the observed set $\\Omega$.\n2.  $P_T(Y) = U V^{\\top}$, where $P_T$ is the orthogonal projection onto the tangent space $T$ at $X_\\star$.\n3.  $\\|P_{T^{\\perp}}(Y)\\|_{2} < 1$, where $P_{T^{\\perp}}$ is the projection onto the orthogonal complement of $T$ and $\\|\\cdot\\|_{2}$ is the operator norm (largest singular value).\n\nThe algorithm is developed by translating these conditions into a solvable numerical problem, following the principles outlined in the problem statement.\n\nFirst, we address the constraint $Y \\in \\operatorname{range}(P_{\\Omega})$. This implies that the matrix $Y$ is sparse, with non-zero entries only at positions specified by the index set $\\Omega$. Let $d = |\\Omega|$ be the number of observed entries. We can represent $Y$ as a linear combination of elementary matrices $\\{E^{(k)}\\}_{k=1}^d$, where $E^{(k)}$ is a matrix of zeros with a $1$ at the position of the $k$-th index in $\\Omega$. If we denote the coordinates corresponding to these non-zero entries as a vector $\\mathbf{y} \\in \\mathbb{R}^d$, we can write $Y$ as a linear function of $\\mathbf{y}$: $Y(\\mathbf{y})$.\n\nSecond, we formulate the linear equation $P_T(Y) = U V^{\\top}$. Substituting the parameterized form of $Y$, we get:\n$$ P_T(Y(\\mathbf{y})) = U V^{\\top} $$\nSince both $Y(\\mathbf{y})$ and the operator $P_T$ are linear, this constitutes a system of linear equations in the unknowns $\\mathbf{y}$. To express this in the standard form $A\\mathbf{x} = \\mathbf{b}$, we vectorize the matrices. Let $\\operatorname{vec}(M)$ be the operator that transforms an $n \\times m$ matrix $M$ into an $nm \\times 1$ column vector by stacking its columns. Applying this to our equation yields:\n$$ \\operatorname{vec}(P_T(Y(\\mathbf{y}))) = \\operatorname{vec}(U V^{\\top}) $$\nThis can be written as $A\\mathbf{y} = \\mathbf{b}$, where:\n- $\\mathbf{y} \\in \\mathbb{R}^d$ is the vector of unknown entries of $Y$. This is our $\\mathbf{x}$ in $A\\mathbf{x}=\\mathbf{b}$.\n- $\\mathbf{b} = \\operatorname{vec}(U V^{\\top}) \\in \\mathbb{R}^{nm}$ is the constant vector on the right-hand side.\n- $A \\in \\mathbb{R}^{nm \\times d}$ is the matrix representation of the linear operator $\\mathcal{L}(\\mathbf{y}) = \\operatorname{vec}(P_T(Y(\\mathbf{y})))$. The $k$-th column of $A$ is $\\operatorname{vec}(P_T(E^{(k)}))$, where $E^{(k)}$ is the basis matrix corresponding to the $k$-th observed entry.\n\nThird, we solve this linear system. As specified, we find the solution in the least-squares sense, which minimizes the Euclidean norm of the residual, $\\|A\\mathbf{y} - \\mathbf{b}\\|_2$. This yields an optimal vector of entries $\\mathbf{y}^*$.\n\nFourth, we verify the two required conditions for the resulting candidate matrix $Y^* = Y(\\mathbf{y}^*)$:\n1.  **Feasibility of the linear system**: We check if the residual of the least-squares solution is close to zero. The problem is considered feasible if $\\|A\\mathbf{y}^* - \\mathbf{b}\\|_2 < \\epsilon$, which is equivalent to $\\|P_T(Y^*) - U V^{\\top}\\|_F < \\epsilon$ for a given small tolerance $\\epsilon > 0$. We use $\\epsilon = 10^{-9}$. A non-negligible residual indicates that no matrix $Y$ supported on $\\Omega$ can satisfy $P_T(Y) = U V^{\\top}$, so the certificate construction fails.\n2.  **Operator norm constraint**: If the first condition holds, we proceed to compute the matrix $P_{T^{\\perp}}(Y^*)$ using the definition $P_{T^{\\perp}}(M) = M - P_T(M)$. We then calculate its operator norm, $\\|P_{T^{\\perp}}(Y^*)\\|_2$, by finding the largest singular value of the matrix. The construction is successful if this norm is strictly less than $1$.\n\nThe final algorithm for a given test case $(n, m, r, \\Omega)$ is as follows:\n1.  Deterministically generate matrices $U \\in \\mathbb{R}^{n \\times r}$ and $V \\in \\mathbb{R}^{m \\times r}$ with orthonormal columns.\n2.  If $\\Omega$ is empty, the only possible candidate is $Y=0$. We check if $P_T(0) = U V^{\\top}$. Since $P_T(0)=0$ and $U V^{\\top}$ is generally non-zero for $r \\geq 1$, this condition fails, resulting in failure.\n3.  If $\\Omega$ is not empty, construct the matrix $A$ and vector $\\mathbf{b}$ for the linear system $A\\mathbf{y}=\\mathbf{b}$ as described above.\n4.  Solve for $\\mathbf{y}^*$ using a numerical least-squares solver.\n5.  Check if the Frobenius norm of the residual, $\\|P_T(Y(\\mathbf{y}^*)) - UV^{\\top}\\|_F$, is less than $\\epsilon=10^{-9}$. If not, the test fails.\n6.  If the residual is small, compute the operator norm $\\|P_{T^{\\perp}}(Y(\\mathbf{y}^*))\\|_2$. If this norm is less than $1$, the test succeeds. Otherwise, it fails.\n\nThis procedure is applied to each test case to determine whether a valid dual certificate can be constructed.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for dual certificate construction.\n    \"\"\"\n    \n    TOL = 1e-9  # Numerical tolerance for residual check\n\n    def P_T(M, U, V):\n        \"\"\"\n        Orthogonal projection of matrix M onto the tangent space T.\n        T = { U B^T + C V^T : B in R^{m x r}, C in R^{n x r} }\n        P_T(M) = U U^T M + M V V^T - U U^T M V V^T\n        \"\"\"\n        UTM = U.T @ M\n        MV = M @ V\n        UTMV = UTM @ V\n        return U @ UTM + MV @ V.T - U @ UTMV @ V.T\n\n    def P_T_perp(M, U, V):\n        \"\"\"\n        Orthogonal projection onto the orthogonal complement of T.\n        P_{T^perp}(M) = M - P_T(M)\n        \"\"\"\n        return M - P_T(M, U, V)\n\n    def generate_orthonormal_matrix(rows, cols, seed):\n        \"\"\"\n        Deterministically generates a matrix with orthonormal columns.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        A = rng.standard_normal((rows, cols))\n        Q, _ = np.linalg.qr(A)\n        return Q\n\n    def check_certificate(n, m, r, Omega, u_seed, v_seed):\n        \"\"\"\n        Attempts to construct a dual certificate Y for given parameters.\n\n        Returns:\n            1 if a valid certificate is constructed, 0 otherwise.\n        \"\"\"\n        # Step 1: Deterministically generate U and V with orthonormal columns\n        U = generate_orthonormal_matrix(n, r, u_seed)\n        V = generate_orthonormal_matrix(m, r, v_seed)\n\n        # The right-hand side of the linear system P_T(Y) = UV^T\n        target = U @ V.T\n        \n        d = len(Omega)\n\n        # Handle the edge case of no observations\n        if d == 0:\n            # The only possible Y is the zero matrix.\n            # Check if P_T(0) = UV^T. Since P_T(0) = 0, this means checking if UV^T = 0.\n            residual_norm = np.linalg.norm(target)\n            return 1 if residual_norm < TOL else 0\n\n        # Step 2: Formulate the linear system A*y = b\n        # b is the vectorized target matrix\n        b = target.flatten(order='F')\n        \n        # A is the matrix representation of the linear operator.\n        # Its columns are vec(P_T(E_k)) for each basis matrix E_k.\n        A = np.zeros((n * m, d))\n        Omega_list = sorted(list(Omega))  # Use a sorted list for fixed ordering\n\n        for k in range(d):\n            i, j = Omega_list[k]\n            # Create a standard basis matrix E_{i,j} (with 1-based indexing from problem)\n            E_k = np.zeros((n, m))\n            E_k[i - 1, j - 1] = 1.0\n            \n            # Apply the projector P_T to the basis matrix\n            P_T_E_k = P_T(E_k, U, V)\n            \n            # The k-th column of A is the vectorized result\n            A[:, k] = P_T_E_k.flatten(order='F')\n            \n        # Step 3: Solve the system in the least-squares sense\n        y, residuals, _, _ = np.linalg.lstsq(A, b, rcond=None)\n        \n        # Step 4: Certify success or failure\n        # Condition 1: The residual must be within tolerance.\n        # `residuals` is an array containing the sum of squared residuals ||Ay-b||_2^2.\n        # It's empty if the system is underdetermined and an exact solution is found.\n        residual_is_small = (len(residuals) == 0) or (residuals[0] < TOL**2)\n        \n        if not residual_is_small:\n            return 0  # Fails feasibility condition\n\n        # Reconstruct the dual certificate candidate Y from the solution vector y\n        Y = np.zeros((n, m))\n        for k in range(d):\n            i, j = Omega_list[k]\n            Y[i - 1, j - 1] = y[k]\n        \n        # Condition 2: The operator norm of P_{T^perp}(Y) must be less than 1.\n        Y_perp = P_T_perp(Y, U, V)\n        \n        try:\n            singular_values = np.linalg.svd(Y_perp, compute_uv=False)\n            operator_norm = singular_values[0] if len(singular_values) > 0 else 0.0\n        except np.linalg.LinAlgError:\n            # In case of SVD convergence failure, treat as failure\n            return 0\n\n        if operator_norm < 1.0:\n            return 1  # Success\n        else:\n            return 0  # Fails operator norm condition\n\n    # --- Test Suite ---\n    \n    # Test case 1: n=5, m=4, r=2, Omega = full set (happy path)\n    omega1 = set((i, j) for i in range(1, 5 + 1) for j in range(1, 4 + 1))\n    \n    # Test case 2: n=6, m=6, r=2, Omega = diagonal (undersampled)\n    omega2 = set((i, i) for i in range(1, 6 + 1))\n    \n    # Test case 3: n=4, m=6, r=1, Omega = empty set (boundary case)\n    omega3 = set()\n    \n    test_cases = [\n        # (n, m, r, Omega, u_seed, v_seed)\n        (5, 4, 2, omega1, 0, 1),\n        (6, 6, 2, omega2, 2, 3),\n        (4, 6, 1, omega3, 4, 5),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = check_certificate(*case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3450116"}]}