{"hands_on_practices": [{"introduction": "A deep understanding of the nuclear norm's subdifferential begins with the ability to construct its elements from first principles. This practice guides you through the concrete construction of a subgradient for a rank-1 matrix, starting from the dual definition of the nuclear norm. By building the matrix and verifying its properties, you will demystify the abstract structure of the subdifferential and solidify your grasp of the necessary orthogonality and norm constraints. [@problem_id:3469357]", "problem": "Let $X \\in \\mathbb{R}^{3 \\times 2}$ be a rank-$1$ matrix with Singular Value Decomposition (SVD) $X = \\sigma\\, u v^{\\top}$ where $\\sigma > 0$, $u \\in \\mathbb{R}^{3}$, and $v \\in \\mathbb{R}^{2}$ are unit-norm singular vectors. Consider the nuclear norm $\\|\\cdot\\|_{*}$ and recall that it is the dual norm of the spectral (operator) norm $\\|\\cdot\\|$. Using only the facts that (i) $\\|X\\|_{*} = \\sup_{\\|Y\\| \\le 1} \\langle Y, X \\rangle$ and (ii) for a closed, convex function $f$, the subdifferential at $x$ is the set of maximizers of the support functional $f(x) = \\sup_{y \\in \\mathcal{C}} \\langle y, x \\rangle$ over the polar set $\\mathcal{C}$, derive the structural form of a subgradient of the nuclear norm at a rank-$1$ point. Then, with the concrete data below, construct a specific subgradient and compute a scalar summary of it.\n\nTake\n$$\nu \\;=\\; \\begin{pmatrix} \\tfrac{2}{3} \\\\[4pt] \\tfrac{1}{3} \\\\[4pt] \\tfrac{2}{3} \\end{pmatrix}, \n\\qquad\nv \\;=\\; \\begin{pmatrix} \\tfrac{3}{5} \\\\[4pt] \\tfrac{4}{5} \\end{pmatrix},\n$$\nand fix orthonormal complements\n$$\nU_{\\perp} \\;=\\; \\begin{pmatrix}\n\\tfrac{1}{\\sqrt{5}} & \\tfrac{4}{3\\sqrt{5}} \\\\\n[6pt]\n-\\tfrac{2}{\\sqrt{5}} & \\tfrac{2}{3\\sqrt{5}} \\\\\n[6pt]\n0 & -\\tfrac{5}{3\\sqrt{5}}\n\\end{pmatrix} \\in \\mathbb{R}^{3 \\times 2},\n\\qquad\nV_{\\perp} \\;=\\; \\begin{pmatrix} -\\tfrac{4}{5} \\\\[4pt] \\tfrac{3}{5} \\end{pmatrix} \\in \\mathbb{R}^{2 \\times 1},\n$$\nso that the columns of $U_{\\perp}$ form an orthonormal basis for the orthogonal complement of $u$ in $\\mathbb{R}^{3}$ and $V_{\\perp}$ is a unit vector spanning the orthogonal complement of $v$ in $\\mathbb{R}^{2}$. Define\n$$\nM \\;=\\; \\begin{pmatrix} \\tfrac{1}{2} \\\\[4pt] \\tfrac{1}{2} \\end{pmatrix} \\in \\mathbb{R}^{2 \\times 1},\n\\qquad\nW \\;=\\; U_{\\perp}\\, M\\, V_{\\perp}^{\\top} \\in \\mathbb{R}^{3 \\times 2},\n\\qquad\nG \\;=\\; u v^{\\top} \\;+\\; W.\n$$\nUsing the derivation from first principles described above, argue that $G$ is a valid subgradient of the nuclear norm at $X$ by verifying the necessary orthogonality and norm constraints on $W$. Then, compute the exact value of the squared Frobenius norm $\\|G\\|_{F}^{2}$ as a single real number. Provide the exact value; no rounding is required.", "solution": "The user wants me to critically validate the provided problem statement and, if valid, provide a reasoned solution.\n\n### Step 1: Extract Givens\n- **Matrix Definition**: $X \\in \\mathbb{R}^{3 \\times 2}$ is a rank-$1$ matrix.\n- **Singular Value Decomposition (SVD)**: $X = \\sigma\\, u v^{\\top}$, with $\\sigma > 0$. The vectors $u \\in \\mathbb{R}^{3}$ and $v \\in \\mathbb{R}^{2}$ are unit-norm.\n- **Nuclear Norm Definition**: The nuclear norm $\\|\\cdot\\|_{*}$ is defined as the dual of the spectral (operator) norm $\\|\\cdot\\|$, i.e., $\\|X\\|_{*} = \\sup_{\\|Y\\| \\le 1} \\langle Y, X \\rangle$.\n- **Subdifferential Definition**: For a closed, convex function $f$ given by the support functional $f(x) = \\sup_{y \\in \\mathcal{C}} \\langle y, x \\rangle$, the subdifferential $\\partial f(x)$ is the set of maximizers of this supremum over the polar set $\\mathcal{C}$.\n- **Concrete Data**:\n  - $u = \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{1}{3} \\\\ \\frac{2}{3} \\end{pmatrix}$\n  - $v = \\begin{pmatrix} \\frac{3}{5} \\\\ \\frac{4}{5} \\end{pmatrix}$\n  - $U_{\\perp} = \\begin{pmatrix} \\frac{1}{\\sqrt{5}} & \\frac{4}{3\\sqrt{5}} \\\\ [6pt] -\\frac{2}{\\sqrt{5}} & \\frac{2}{3\\sqrt{5}} \\\\ [6pt] 0 & -\\frac{5}{3\\sqrt{5}} \\end{pmatrix} \\in \\mathbb{R}^{3 \\times 2}$ (orthonormal basis for the orthogonal complement of the span of $u$).\n  - $V_{\\perp} = \\begin{pmatrix} -\\frac{4}{5} \\\\ \\frac{3}{5} \\end{pmatrix} \\in \\mathbb{R}^{2 \\times 1}$ (unit vector spanning the orthogonal complement of the span of $v$).\n  - $M = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} \\in \\mathbb{R}^{2 \\times 1}$.\n  - $W = U_{\\perp}\\, M\\, V_{\\perp}^{\\top} \\in \\mathbb{R}^{3 \\times 2}$.\n  - $G = u v^{\\top} + W$.\n- **Tasks**:\n  1. Derive the structural form of a subgradient of the nuclear norm at a rank-$1$ point from first principles.\n  2. Argue that the given $G$ is a valid subgradient of the nuclear norm at $X$ by verifying orthogonality and norm constraints on $W$.\n  3. Compute the exact value of the squared Frobenius norm $\\|G\\|_{F}^{2}$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is set in the field of convex optimization and matrix analysis, specifically dealing with the nuclear norm and its subdifferential. These are standard, well-established concepts in mathematics and engineering. All definitions are correct and standard.\n2.  **Well-Posed**: The problem is clearly stated. The provided data for vectors and matrices are numerically consistent (e.g., $u$ and $v$ are indeed unit vectors, $U_\\perp$ has orthonormal columns orthogonal to $u$, and $V_\\perp$ is a unit vector orthogonal to $v$). The tasks are specific and lead to a unique numerical answer.\n3.  **Objective**: The language is precise and mathematical, free from any subjective or biased statements.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a standard exercise in convex analysis and matrix theory. I will proceed with the solution.\n\n### Solution\nThe problem asks for a derivation of the subgradient of the nuclear norm at a rank-$1$ matrix, followed by a specific calculation.\n\nFirst, we establish the general structure of a subgradient of the nuclear norm, $\\|\\cdot\\|_{*}$. The nuclear norm of a matrix $X \\in \\mathbb{R}^{m \\times n}$ is the sum of its singular values, $\\|X\\|_{*} = \\sum_i \\sigma_i(X)$. The problem provides the dual definition, $\\|X\\|_{*} = \\sup_{\\|Y\\| \\le 1} \\langle Y, X \\rangle$, where $\\|\\cdot\\|$ is the spectral norm (largest singular value) and $\\langle Y, X \\rangle = \\text{tr}(Y^\\top X)$ is the Frobenius inner product.\n\nThe subdifferential $\\partial \\|X\\|_{*}$ is defined as the set of maximizers of the supremum. That is, a matrix $G$ is a subgradient of the nuclear norm at $X$, denoted $G \\in \\partial \\|X\\|_{*}$, if and only if it satisfies two conditions:\n1.  $G$ is in the set over which the supremum is taken: $\\|G\\| \\le 1$.\n2.  $G$ achieves the maximum value: $\\langle G, X \\rangle = \\|X\\|_{*}$.\n\nLet the Singular Value Decomposition (SVD) of $X$ be $X = \\sum_{i=1}^r \\sigma_i u_i v_i^\\top$, where $r = \\text{rank}(X)$, $\\{\\sigma_i > 0\\}$ are the singular values, and $\\{u_i\\}$, $\\{v_i\\}$ are the corresponding left and right orthonormal singular vectors. The nuclear norm is $\\|X\\|_{*} = \\sum_{i=1}^r \\sigma_i$.\n\nThe condition $\\langle G, X \\rangle = \\|X\\|_{*}$ becomes:\n$$ \\text{tr}(G^\\top X) = \\text{tr}\\left(G^\\top \\sum_{i=1}^r \\sigma_i u_i v_i^\\top\\right) = \\sum_{i=1}^r \\sigma_i \\text{tr}(G^\\top u_i v_i^\\top) = \\sum_{i=1}^r \\sigma_i \\text{tr}(v_i^\\top G^\\top u_i) = \\sum_{i=1}^r \\sigma_i (u_i^\\top G v_i) $$\nWe need this sum to be equal to $\\sum_{i=1}^r \\sigma_i$. Since $\\|G\\| \\le 1$, we have $|u_i^\\top G v_i| \\le \\|u_i\\|_2 \\|G\\| \\|v_i\\|_2 \\le 1$. For the equality $\\sum \\sigma_i (u_i^\\top G v_i) = \\sum \\sigma_i$ to hold with $\\sigma_i > 0$, we must have $u_i^\\top G v_i = 1$ for all $i=1, \\dots, r$.\n\nNow consider the structure of $G$. Let $U = [u_1, \\dots, u_r]$ and $V = [v_1, \\dots, v_r]$. Let $U_\\perp$ and $V_\\perp$ be matrices whose columns form orthonormal bases for the orthogonal complements of the column spaces of $U$ and $V$, respectively. A general form for a subgradient is $G = UV^\\top + W$, where $W$ must satisfy certain conditions. For this structure to hold, we require $W$ to be orthogonal to the singular subspaces of $X$. Specifically, the column space of $W$ must be orthogonal to that of $U$, and the row space of $W$ must be orthogonal to that of $V$. This implies $U^\\top W = 0$ and $W V = 0$. Such a $W$ can be written as $W = U_\\perp Z V_\\perp^\\top$ for some matrix $Z$ of appropriate dimensions.\n\nThe full subgradient is then of the form $G = UV^\\top + U_\\perp Z V_\\perp^\\top$. The norm condition $\\|G\\| \\le 1$ translates to a condition on $Z$. In the orthonormal bases formed by $[U, U_\\perp]$ and $[V, V_\\perp]$, the operator $G$ is block-diagonal, mapping span$(V)$ to span$(U)$ and span$(V_\\perp)$ to span$(U_\\perp)$. Thus, its spectral norm is the maximum of the norms of the blocks: $\\|G\\| = \\max(\\|UV^\\top\\|, \\|U_\\perp Z V_\\perp^\\top\\|) = \\max(1, \\|Z\\|)$. The condition $\\|G\\| \\le 1$ thus requires $\\|Z\\| \\le 1$.\n\nFor the given rank-$1$ matrix $X = \\sigma u v^\\top$, we have $r=1$, $U=u$, and $V=v$. The structural form of a subgradient is:\n$$ G = u v^\\top + W $$\nwhere $W = U_\\perp Z V_\\perp^\\top$ for a matrix $Z$ with $\\|Z\\| \\le 1$. The dimensions are $u \\in \\mathbb{R}^{3 \\times 1}$, $v \\in \\mathbb{R}^{2 \\times 1}$, $U_\\perp \\in \\mathbb{R}^{3 \\times 2}$, and $V_\\perp \\in \\mathbb{R}^{2 \\times 1}$. So $Z$ must be a $2 \\times 1$ matrix (a column vector).\n\nThe problem provides a specific candidate subgradient $G = u v^\\top + W$, where $W = U_\\perp M V_\\perp^\\top$ and $M = \\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix}$. We must argue that this $G$ is a valid subgradient by verifying the orthogonality and norm constraints on $W$.\n\n1.  **Orthogonality Constraints on $W$**: We verify that $u^\\top W = 0$ and $Wv = 0$.\n    The columns of $U_\\perp$ form an orthonormal basis for the subspace orthogonal to $u$. Therefore, by definition, $u^\\top U_\\perp = 0$.\n    $$ u^\\top W = u^\\top (U_\\perp M V_\\perp^\\top) = (u^\\top U_\\perp) M V_\\perp^\\top = 0 \\cdot M V_\\perp^\\top = 0 $$\n    Similarly, $V_\\perp$ is a basis for the subspace orthogonal to $v$, so $v^\\top V_\\perp = 0$, which implies $V_\\perp^\\top v = 0$.\n    $$ Wv = (U_\\perp M V_\\perp^\\top) v = U_\\perp M (V_\\perp^\\top v) = U_\\perp M \\cdot 0 = 0 $$\n    These orthogonality conditions are satisfied.\n\n2.  **Norm Constraint**: The component $W$ is constructed from $M$, i.e., $W = U_\\perp M V_\\perp^\\top$. Following our derivation, we must check if the spectral norm of the core matrix $M$ is less than or equal to $1$. As $M$ is a vector, its spectral norm is its Euclidean norm.\n    $$ \\|M\\| = \\|M\\|_2 = \\sqrt{\\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2} = \\sqrt{\\frac{1}{4} + \\frac{1}{4}} = \\sqrt{\\frac{1}{2}} = \\frac{1}{\\sqrt{2}} $$\n    Since $\\|M\\| = \\frac{1}{\\sqrt{2}} \\le 1$, the norm constraint is satisfied.\n    This shows that the overall spectral norm of $G$ is $\\|G\\| = \\max(\\|uv^\\top\\|, \\|W\\|) = \\max(1, \\|M\\|) = \\max(1, 1/\\sqrt{2}) = 1$, which satisfies $\\|G\\| \\le 1$.\n    Also, $\\langle G, X \\rangle = \\langle uv^\\top + W, \\sigma u v^\\top \\rangle = \\sigma \\langle uv^\\top, uv^\\top \\rangle + \\sigma \\langle W, uv^\\top \\rangle$.\n    We have $\\langle uv^\\top, uv^\\top \\rangle = \\|uv^\\top\\|_F^2 = \\text{tr}(vu^\\top u v^\\top) = \\text{tr}(v v^\\top) = v^\\top v = 1$.\n    The orthogonality $u^\\top W=0$ implies $\\langle W, uv^\\top \\rangle = \\text{tr}(W^\\top u v^\\top) = \\text{tr}(v^\\top W^\\top u) = 0$.\n    Thus, $\\langle G, X \\rangle = \\sigma(1) + \\sigma(0) = \\sigma = \\|X\\|_{*}$.\n    Both conditions for a subgradient are met, so $G$ is a valid subgradient.\n\nFinally, we must compute the squared Frobenius norm $\\|G\\|_F^2$.\n$$ \\|G\\|_F^2 = \\|u v^\\top + W\\|_F^2 = \\langle u v^\\top + W, u v^\\top + W \\rangle = \\|uv^\\top\\|_F^2 + \\|W\\|_F^2 + 2\\langle uv^\\top, W \\rangle $$\nThe cross-term is $\\langle uv^\\top, W \\rangle = \\text{tr}((uv^\\top)^\\top W) = \\text{tr}(vu^\\top W) = 0$, as shown before. Thus, we have an orthogonal sum:\n$$ \\|G\\|_F^2 = \\|u v^\\top\\|_F^2 + \\|W\\|_F^2 $$\nWe calculate each term:\n- $\\|uv^\\top\\|_F^2 = 1$, as shown earlier.\n- $\\|W\\|_F^2 = \\|U_\\perp M V_\\perp^\\top\\|_F^2$. Using the trace definition of the Frobenius norm:\n  $$ \\|W\\|_F^2 = \\text{tr}(W^\\top W) = \\text{tr}\\left( (U_\\perp M V_\\perp^\\top)^\\top (U_\\perp M V_\\perp^\\top) \\right) = \\text{tr}\\left( V_\\perp M^\\top U_\\perp^\\top U_\\perp M V_\\perp^\\top \\right) $$\n  Since the columns of $U_\\perp$ are orthonormal, $U_\\perp^\\top U_\\perp = I_2$, where $I_2$ is the $2 \\times 2$ identity matrix.\n  $$ \\|W\\|_F^2 = \\text{tr}(V_\\perp M^\\top M V_\\perp^\\top) $$\n  Using the cyclic property of the trace, $\\text{tr}(ABC) = \\text{tr}(CAB)$:\n  $$ \\|W\\|_F^2 = \\text{tr}(M^\\top M V_\\perp^\\top V_\\perp) $$\n  Since $V_\\perp$ is a unit vector, $V_\\perp^\\top V_\\perp = \\|V_\\perp\\|_2^2 = 1$. This is a $1 \\times 1$ matrix.\n  $$ \\|W\\|_F^2 = \\text{tr}(M^\\top M) = \\|M\\|_F^2 $$\n  For the vector $M$, the squared Frobenius norm is its squared Euclidean norm:\n  $$ \\|M\\|_F^2 = \\|M\\|_2^2 = \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2 = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2} $$\n  So, $\\|W\\|_F^2 = \\frac{1}{2}$.\n\nCombining the terms, we get the final result:\n$$ \\|G\\|_F^2 = \\|u v^\\top\\|_F^2 + \\|W\\|_F^2 = 1 + \\frac{1}{2} = \\frac{3}{2} $$\nThe specific values of $u, v, U_\\perp, V_\\perp$ are consistent with the theoretical framework but are not needed for the final numerical computation, which depends only on the structure of $G$ and the norm of $M$.", "answer": "$$\n\\boxed{\\frac{3}{2}}\n$$", "id": "3469357"}, {"introduction": "The subdifferential is not just a theoretical curiosity; it is a fundamental tool for deriving and verifying optimization algorithms. This exercise centers on the proximal operator of the nuclear norm, a core component in algorithms for sparse and low-rank recovery. You will first compute the solution via singular value thresholding and then, crucially, use the first-order optimality conditions to prove your solution is correct by explicitly constructing a valid subgradient. [@problem_id:3469368]", "problem": "Let $X \\in \\mathbb{R}^{3 \\times 2}$ be given with the thin singular value decomposition (SVD) $X = U \\Sigma V^{\\top}$, where\n$$\nU = \\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 & -1 \\\\\n1 & \\phantom{-}1 \\\\\n0 & \\phantom{-}0\n\\end{pmatrix},\n\\quad\nV = \\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 & -1 \\\\\n1 & \\phantom{-}1\n\\end{pmatrix},\n\\quad\n\\Sigma = \\operatorname{diag}(5, 1).\n$$\nConsider the proximal mapping of the nuclear norm defined by\n$$\n\\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(X) \\triangleq \\arg\\min_{Z \\in \\mathbb{R}^{3 \\times 2}} \\left\\{ \\frac{1}{2} \\|Z - X\\|_{F}^{2} + \\lambda \\|Z\\|_* \\right\\},\n$$\nwhere $\\|\\cdot\\|_{F}$ denotes the Frobenius norm and $\\|\\cdot\\|_*$ denotes the nuclear norm. Take $\\lambda = \\frac{3}{2}$.\n\nUsing only foundational convex analysis principles and core definitions, compute the explicit $3 \\times 2$ matrix for $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(X)$ and then verify your answer by directly checking the first-order optimality condition\n$$\n0 \\in Z^{\\star} - X + \\lambda \\, \\partial \\|Z^{\\star}\\|_*,\n$$\nwhere $Z^{\\star} = \\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(X)$ and $\\partial \\|Z^{\\star}\\|_*$ is the subdifferential of the nuclear norm at $Z^{\\star}$. Your verification must exhibit a specific subgradient $G \\in \\partial \\|Z^{\\star}\\|_*$ that satisfies $X - Z^{\\star} = \\lambda G$. Provide the final proximal matrix in closed form.", "solution": "The user wants me to solve a problem.\nI will begin by validating the problem statement.\n\n### Step 1: Extract Givens\n-   Matrix $X \\in \\mathbb{R}^{3 \\times 2}$ with thin Singular Value Decomposition (SVD) $X = U \\Sigma V^{\\top}$.\n-   Left singular vectors matrix: $U = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & -1 \\\\ 1 & \\phantom{-}1 \\\\ 0 & \\phantom{-}0 \\end{pmatrix}$.\n-   Right singular vectors matrix: $V = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & -1 \\\\ 1 & \\phantom{-}1 \\end{pmatrix}$.\n-   Singular values matrix: $\\Sigma = \\operatorname{diag}(5, 1)$, which corresponds to $\\Sigma = \\begin{pmatrix} 5 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\n-   Proximal mapping definition: $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(X) \\triangleq \\arg\\min_{Z \\in \\mathbb{R}^{3 \\times 2}} \\left\\{ \\frac{1}{2} \\|Z - X\\|_{F}^{2} + \\lambda \\|Z\\|_* \\right\\}$.\n-   Regularization parameter: $\\lambda = \\frac{3}{2}$.\n-   First-order optimality condition for verification: $0 \\in Z^{\\star} - X + \\lambda \\, \\partial \\|Z^{\\star}\\|_*$, where $Z^{\\star} = \\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(X)$. This is equivalent to showing $X - Z^{\\star} = \\lambda G$ for some subgradient $G \\in \\partial \\|Z^{\\star}\\|_*$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Soundness**: The problem is situated within the well-established field of convex optimization and sparse recovery. The nuclear norm, its proximal operator (singular value thresholding), and its subdifferential are standard concepts in this domain. The provided matrices $U$ and $V$ have orthonormal columns, as required for an SVD:\n    $U^{\\top}U = \\frac{1}{2}\\begin{pmatrix} 1 & 1 & 0 \\\\ -1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & -1 \\\\ 1 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I_2$.\n    $V^{\\top}V = \\frac{1}{2}\\begin{pmatrix} 1 & 1 \\\\ -1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & -1 \\\\ 1 & 1 \\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I_2$.\n    The setup is mathematically and scientifically sound.\n2.  **Well-Posedness**: The objective function for the proximal mapping is the sum of a strictly convex smooth function ($\\frac{1}{2} \\|Z - X\\|_{F}^{2}$) and a convex function ($\\lambda \\|Z\\|_*$). This ensures that a unique minimizer $Z^{\\star}$ exists. The problem is well-posed.\n3.  **Objectivity**: The problem is stated using precise mathematical language with no subjective or ambiguous terms.\n4.  **Completeness**: All necessary information (matrices, parameters, definitions) is provided.\n\nThe problem is valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will now proceed with the solution.\n\nThe solution consists of two parts: first, computing the proximal map $Z^{\\star}$, and second, verifying the result using the first-order optimality condition.\n\n**Part 1: Computation of the Proximal Map**\n\nThe proximal operator of the nuclear norm, $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}$, is the singular value thresholding (SVT) operator. Given the SVD of a matrix $X = U \\Sigma V^{\\top}$, its proximal map $Z^{\\star}$ is computed as:\n$$\nZ^{\\star} = U S_{\\lambda}(\\Sigma) V^{\\top}\n$$\nwhere $S_{\\lambda}$ is the soft-thresholding operator applied to the singular values $\\sigma_i$ of $X$ on the diagonal of $\\Sigma$. The operator is defined as $S_{\\lambda}(\\sigma_i) = \\max(0, \\sigma_i - \\lambda)$.\n\nThe singular values of $X$ are given by the diagonal of $\\Sigma$: $\\sigma_1 = 5$ and $\\sigma_2 = 1$. The parameter $\\lambda$ is given as $\\frac{3}{2}$.\n\nWe apply soft-thresholding to each singular value:\n$$\n\\hat{\\sigma}_1 = S_{3/2}(5) = \\max\\left(0, 5 - \\frac{3}{2}\\right) = \\max\\left(0, \\frac{10 - 3}{2}\\right) = \\frac{7}{2}\n$$\n$$\n\\hat{\\sigma}_2 = S_{3/2}(1) = \\max\\left(0, 1 - \\frac{3}{2}\\right) = \\max\\left(0, -\\frac{1}{2}\\right) = 0\n$$\n\nThe thresholded singular value matrix, which we denote $\\Sigma_{\\lambda}$, is:\n$$\n\\Sigma_{\\lambda} = S_{\\lambda}(\\Sigma) = \\begin{pmatrix} \\frac{7}{2} & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\n\nNow we reconstruct the matrix $Z^{\\star}$ using the SVD components:\n$$\nZ^{\\star} = U \\Sigma_{\\lambda} V^{\\top} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & -1 \\\\ 1 & \\phantom{-}1 \\\\ 0 & \\phantom{-}0 \\end{pmatrix} \\begin{pmatrix} \\frac{7}{2} & 0 \\\\ 0 & 0 \\end{pmatrix} \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & 1 \\\\ -1 & 1 \\end{pmatrix}\n$$\nFirst, we compute the product $U \\Sigma_{\\lambda}$:\n$$\nU \\Sigma_{\\lambda} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & -1 \\\\ 1 & \\phantom{-}1 \\\\ 0 & \\phantom{-}0 \\end{pmatrix} \\begin{pmatrix} \\frac{7}{2} & 0 \\\\ 0 & 0 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\cdot \\frac{7}{2} + (-1) \\cdot 0 & 1 \\cdot 0 + (-1) \\cdot 0 \\\\ 1 \\cdot \\frac{7}{2} + 1 \\cdot 0 & 1 \\cdot 0 + 1 \\cdot 0 \\\\ 0 \\cdot \\frac{7}{2} + 0 \\cdot 0 & 0 \\cdot 0 + 0 \\cdot 0 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} \\frac{7}{2} & 0 \\\\ \\frac{7}{2} & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nNext, we multiply this result by $V^{\\top}$:\n$$\nZ^{\\star} = (U \\Sigma_{\\lambda}) V^{\\top} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} \\frac{7}{2} & 0 \\\\ \\frac{7}{2} & 0 \\\\ 0 & 0 \\end{pmatrix} \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & 1 \\\\ -1 & 1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} \\frac{7}{2} \\cdot 1 + 0 \\cdot (-1) & \\frac{7}{2} \\cdot 1 + 0 \\cdot 1 \\\\ \\frac{7}{2} \\cdot 1 + 0 \\cdot (-1) & \\frac{7}{2} \\cdot 1 + 0 \\cdot 1 \\\\ 0 \\cdot 1 + 0 \\cdot (-1) & 0 \\cdot 1 + 0 \\cdot 1 \\end{pmatrix}\n$$\n$$\nZ^{\\star} = \\frac{1}{2} \\begin{pmatrix} \\frac{7}{2} & \\frac{7}{2} \\\\ \\frac{7}{2} & \\frac{7}{2} \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{4} & \\frac{7}{4} \\\\ \\frac{7}{4} & \\frac{7}{4} \\\\ 0 & 0 \\end{pmatrix}\n$$\nThis is the explicit matrix for $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(X)$.\n\n**Part 2: Verification via First-Order Optimality Condition**\n\nThe first-order necessary and sufficient condition for optimality is $X - Z^{\\star} \\in \\lambda \\, \\partial \\|Z^{\\star}\\|_*$. We must find a subgradient $G \\in \\partial \\|Z^{\\star}\\|_*$ such that $X - Z^{\\star} = \\lambda G$.\n\nFirst, we compute the matrix $X$ from its SVD:\n$$\nX = U \\Sigma V^{\\top} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & -1 \\\\ 1 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 5 & 0 \\\\ 0 & 1 \\end{pmatrix} \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & 1 \\\\ -1 & 1 \\end{pmatrix}\n$$\n$$\nX = \\frac{1}{2} \\begin{pmatrix} 5 & -1 \\\\ 5 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ -1 & 1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 5 \\cdot 1 + (-1) \\cdot (-1) & 5 \\cdot 1 + (-1) \\cdot 1 \\\\ 5 \\cdot 1 + 1 \\cdot (-1) & 5 \\cdot 1 + 1 \\cdot 1 \\\\ 0 & 0 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 6 & 4 \\\\ 4 & 6 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 3 & 2 \\\\ 2 & 3 \\\\ 0 & 0 \\end{pmatrix}\n$$\nNow, compute the difference $X - Z^{\\star}$:\n$$\nX - Z^{\\star} = \\begin{pmatrix} 3 & 2 \\\\ 2 & 3 \\\\ 0 & 0 \\end{pmatrix} - \\begin{pmatrix} \\frac{7}{4} & \\frac{7}{4} \\\\ \\frac{7}{4} & \\frac{7}{4} \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{12-7}{4} & \\frac{8-7}{4} \\\\ \\frac{8-7}{4} & \\frac{12-7}{4} \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{4} & \\frac{1}{4} \\\\ \\frac{1}{4} & \\frac{5}{4} \\\\ 0 & 0 \\end{pmatrix}\n$$\nThe candidate subgradient $G$ is given by $G = \\frac{1}{\\lambda}(X - Z^{\\star})$:\n$$\nG = \\frac{1}{3/2} \\begin{pmatrix} \\frac{5}{4} & \\frac{1}{4} \\\\ \\frac{1}{4} & \\frac{5}{4} \\\\ 0 & 0 \\end{pmatrix} = \\frac{2}{3} \\begin{pmatrix} \\frac{5}{4} & \\frac{1}{4} \\\\ \\frac{1}{4} & \\frac{5}{4} \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{10}{12} & \\frac{2}{12} \\\\ \\frac{2}{12} & \\frac{10}{12} \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{6} & \\frac{1}{6} \\\\ \\frac{1}{6} & \\frac{5}{6} \\\\ 0 & 0 \\end{pmatrix}\n$$\nNow, we must verify that this matrix $G$ is in the subdifferential $\\partial \\|Z^{\\star}\\|_*$.\nLet the SVD of a matrix $A$ be $A = U_A \\Sigma_A V_A^\\top$. Let $r=\\operatorname{rank}(A)$. Let $U_r$ and $V_r$ be the matrices containing the first $r$ columns of $U_A$ and $V_A$ respectively. The subdifferential of the nuclear norm at $A$ is given by:\n$$\n\\partial \\|A\\|_* = \\{ U_r V_r^{\\top} + W \\mid U_r^{\\top}W = 0, WV_r=0, \\|W\\|_{\\text{op}} \\le 1 \\}\n$$\nThe matrix $Z^{\\star}$ has rank $r=1$. Its SVD is $Z^{\\star} = u_1 \\hat{\\sigma}_1 v_1^{\\top}$, where $u_1$ and $v_1$ are the first columns of the original $U$ and $V$ matrices, and $\\hat{\\sigma}_1 = \\frac{7}{2}$.\n$$\nu_1 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad v_1 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nSo, for $Z^\\star$, we have $U_r = u_1$ and $V_r=v_1$. The first term of the subgradient is:\n$$\nU_r V_r^{\\top} = u_1 v_1^{\\top} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & 1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{2} \\\\ 0 & 0 \\end{pmatrix}\n$$\nWe define $W$ such that $G = u_1 v_1^{\\top} + W$:\n$$\nW = G - u_1 v_1^{\\top} = \\begin{pmatrix} \\frac{5}{6} & \\frac{1}{6} \\\\ \\frac{1}{6} & \\frac{5}{6} \\\\ 0 & 0 \\end{pmatrix} - \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{2} \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{5-3}{6} & \\frac{1-3}{6} \\\\ \\frac{1-3}{6} & \\frac{5-3}{6} \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{6} & -\\frac{2}{6} \\\\ -\\frac{2}{6} & \\frac{2}{6} \\\\ 0 & 0 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\\\ 0 & 0 \\end{pmatrix}\n$$\nNow we check the three conditions on $W$:\n1.  $U_r^{\\top}W = u_1^{\\top}W = 0$:\n    $$u_1^{\\top}W = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix} \\frac{1}{3} \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\frac{1}{3\\sqrt{2}} \\begin{pmatrix} 1-1 & -1+1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\end{pmatrix}$$\n    This condition is satisfied.\n2.  $WV_r = Wv_1 = 0$:\n    $$Wv_1 = \\frac{1}{3} \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\\\ 0 & 0 \\end{pmatrix} \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{3\\sqrt{2}} \\begin{pmatrix} 1-1 \\\\ -1+1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\n    This condition is satisfied.\n3.  $\\|W\\|_{\\text{op}} \\le 1$: We compute the operator norm (largest singular value) of $W$. Note that $W$ can be written in terms of the second singular vectors $u_2$ and $v_2$ from the original SVD:\n    $u_2 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} -1 \\\\ 1 \\\\ 0 \\end{pmatrix}$, $v_2 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$.\n    Let's compute $u_2 v_2^{\\top}$:\n    $$\n    u_2 v_2^{\\top} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} -1 \\\\ 1 \\\\ 0 \\end{pmatrix} \\frac{1}{\\sqrt{2}}\\begin{pmatrix} -1 & 1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\\\ 0 & 0 \\end{pmatrix}\n    $$\n    We can see that $W = \\frac{2}{3} (u_2 v_2^{\\top})$. Since $u_2$ and $v_2$ are unit vectors, the matrix $u_2 v_2^{\\top}$ has an operator norm of $1$. Therefore, the operator norm of $W$ is:\n    $$\n    \\|W\\|_{\\text{op}} = \\left\\| \\frac{2}{3} u_2 v_2^{\\top} \\right\\|_{\\text{op}} = \\frac{2}{3} \\|u_2 v_2^{\\top}\\|_{\\text{op}} = \\frac{2}{3}\n    $$\n    Since $\\frac{2}{3} \\le 1$, this condition is also satisfied.\n\nAll three conditions are met, so $G = \\begin{pmatrix} 5/6 & 1/6 \\\\ 1/6 & 5/6 \\\\ 0 & 0 \\end{pmatrix}$ is a valid subgradient in $\\partial \\|Z^{\\star}\\|_*$. The first-order optimality condition $X - Z^{\\star} = \\lambda G$ is fulfilled, which verifies that our computed $Z^{\\star}$ is correct.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{7}{4} & \\frac{7}{4} \\\\\n\\frac{7}{4} & \\frac{7}{4} \\\\\n0 & 0\n\\end{pmatrix}\n}\n$$", "id": "3469368"}, {"introduction": "The success of recovering a low-rank matrix from limited measurements hinges on the geometric relationship between the measurement operator and the nuclear norm. This advanced practice challenges you to construct a scenario where low-rank recovery fails, illustrating a key theoretical principle. By identifying a feasible descent direction, you will explore how the subdifferential at the target matrix can be used to quantify the \"misalignment\" that leads to failure, providing deep insight into the design of robust recovery systems. [@problem_id:3469376]", "problem": "Consider the vector space of real $2 \\times 2$ matrices equipped with the Frobenius inner product $\\langle X, Y \\rangle = \\operatorname{trace}(X^{\\top} Y)$ and the nuclear norm $\\|X\\|_{*}$, which is the sum of singular values of $X$. Let the target matrix be $X^{\\star} = e_{1} e_{1}^{\\top}$, where $e_{1}$ is the first standard basis vector in $\\mathbb{R}^{2}$. Define the direction $H = -2\\, e_{1} e_{1}^{\\top} + e_{2} e_{2}^{\\top}$, and let $\\widehat{H} = H / \\|H\\|_{F}$ be its unit Frobenius-norm normalization.\n\nYour tasks are:\n1. Construct a linear measurement operator $\\mathcal{A} : \\mathbb{R}^{2 \\times 2} \\to \\mathbb{R}^{3}$, specified by linear functionals of the form $\\mathcal{A}(X) = \\big(\\langle B_{1}, X \\rangle, \\langle B_{2}, X \\rangle, \\langle B_{3}, X \\rangle\\big)$ for explicitly given matrices $B_{1}, B_{2}, B_{3}$, such that $\\ker(\\mathcal{A}) = \\operatorname{span}\\{H\\}$ and $\\mathcal{A}(X^{\\star})$ is well-defined.\n2. Using only foundational convex-analytic principles (duality of norms, definition of subdifferential, and first-order optimality), demonstrate rigorously that the convex program $\\min_{X} \\|X\\|_{*}$ subject to $\\mathcal{A}(X) = \\mathcal{A}(X^{\\star})$ fails to recover $X^{\\star}$ because of a descent direction $H \\in \\ker(\\mathcal{A})$ at $X^{\\star}$.\n3. Quantify the misalignment between $\\ker(\\mathcal{A})$ and the descent cone of $\\|\\cdot\\|_{*}$ at $X^{\\star}$ using the subdifferential as follows. Define\n$$\n\\eta(\\mathcal{A}, X^{\\star}) \\triangleq \\sup_{G \\in \\partial \\| \\cdot \\|_{*}(X^{\\star})} \\langle G, \\widehat{H} \\rangle,\n$$\nwhere $\\partial \\| \\cdot \\|_{*}(X^{\\star})$ denotes the subdifferential of the nuclear norm at $X^{\\star}$. Compute the exact value of $\\eta(\\mathcal{A}, X^{\\star})$.\n\nExpress your final answer as a single exact real number. No rounding is required. No units are involved.", "solution": "We work in $\\mathbb{R}^{2 \\times 2}$ with the Frobenius inner product and the nuclear norm. The target is $X^{\\star} = e_{1} e_{1}^{\\top}$, which is the matrix with a $1$ in the $(1,1)$ entry and zeros elsewhere. The direction is $H = -2\\, e_{1} e_{1}^{\\top} + e_{2} e_{2}^{\\top}$, so $H$ is diagonal with entries $-2$ and $1$ along the diagonal.\n\nStep 1: Constructing $\\mathcal{A}$ with $\\ker(\\mathcal{A}) = \\operatorname{span}\\{H\\}$.\nA linear operator $\\mathcal{A}$ specified by $\\mathcal{A}(X) = \\big(\\langle B_{1}, X \\rangle, \\langle B_{2}, X \\rangle, \\langle B_{3}, X \\rangle\\big)$ has kernel equal to the orthogonal complement (with respect to the Frobenius inner product) of the span of $\\{B_{1}, B_{2}, B_{3}\\}$. To enforce $\\ker(\\mathcal{A}) = \\operatorname{span}\\{H\\}$, choose $B_{1}, B_{2}, B_{3}$ to form a basis of $H^{\\perp}$.\n\nSince $H$ is diagonal, any off-diagonal matrix is orthogonal to $H$. Thus, we can take\n$$\nB_{1} = e_{1} e_{2}^{\\top}, \\quad B_{2} = e_{2} e_{1}^{\\top}.\n$$\nFor the third direction in the diagonal subspace, pick $B_{3}$ orthogonal to $H$. A convenient choice is\n$$\nB_{3} = e_{1} e_{1}^{\\top} + 2\\, e_{2} e_{2}^{\\top}.\n$$\nIndeed,\n$$\n\\langle B_{3}, H \\rangle = \\langle e_{1} e_{1}^{\\top} + 2 e_{2} e_{2}^{\\top},\\, -2 e_{1} e_{1}^{\\top} + e_{2} e_{2}^{\\top} \\rangle = -2 + 2 = 0.\n$$\nClearly, $B_{1}, B_{2}, B_{3}$ are linearly independent and orthogonal to $H$, so they span $H^{\\perp}$. Therefore, with this choice,\n$$\n\\mathcal{A}(X) = \\big(\\langle e_{1} e_{2}^{\\top}, X \\rangle,\\ \\langle e_{2} e_{1}^{\\top}, X \\rangle,\\ \\langle e_{1} e_{1}^{\\top} + 2 e_{2} e_{2}^{\\top}, X \\rangle\\big)\n$$\nsatisfies $\\ker(\\mathcal{A}) = \\operatorname{span}\\{H\\}$. We also have\n$$\n\\mathcal{A}(X^{\\star}) = \\big(0,\\ 0,\\ 1\\big).\n$$\n\nStep 2: Demonstrating failure of nuclear norm recovery via a feasible descent direction.\nWe recall a fundamental convex-analytic fact: for a proper convex function $f$, the directional derivative at $x$ in direction $D$ satisfies\n$$\nf'(x; D) = \\sup_{g \\in \\partial f(x)} \\langle g, D \\rangle,\n$$\nwhere $\\partial f(x)$ is the subdifferential. If $f'(x; D) < 0$, then for sufficiently small $t > 0$, $f(x + t D) < f(x)$.\n\nWe will show that the directional derivative of $\\| \\cdot \\|_{*}$ at $X^{\\star}$ along $H$ is strictly negative. To compute this, we use two well-tested facts:\n\n- The nuclear norm is the dual of the spectral norm: $\\|X\\|_{*} = \\sup_{\\|Y\\|_{2} \\leq 1} \\langle Y, X \\rangle$.\n- The subdifferential of the nuclear norm at $X^{\\star}$ with singular value decomposition (SVD) $X^{\\star} = U \\Sigma V^{\\top}$ and rank $r$ is\n$$\n\\partial \\| \\cdot \\|_{*}(X^{\\star}) = \\{ U V^{\\top} + W : \\|W\\|_{2} \\leq 1,\\ U^{\\top} W = 0,\\ W V = 0 \\}.\n$$\n\nFor $X^{\\star} = e_{1} e_{1}^{\\top}$, an SVD is given by $U = V = e_{1}$ and $\\Sigma = [1]$. Thus, $U V^{\\top} = e_{1} e_{1}^{\\top}$. The tangent space at $X^{\\star}$ is\n$$\nT = \\{ e_{1} a^{\\top} + b e_{1}^{\\top} : a, b \\in \\mathbb{R}^{2} \\},\n$$\nwhich in $2 \\times 2$ includes all matrices with arbitrary first row and first column (with double counting at the $(1,1)$ entry). Its orthogonal complement is\n$$\nT^{\\perp} = \\operatorname{span}\\{ e_{2} e_{2}^{\\top} \\}.\n$$\n\nDecompose $H$ as $H = H_{T} + H_{T^{\\perp}}$. Since $H$ is diagonal with entries $-2$ and $1$, we have\n$$\nH_{T} = -2\\, e_{1} e_{1}^{\\top}, \\qquad H_{T^{\\perp}} = e_{2} e_{2}^{\\top}.\n$$\nUsing the form of the subdifferential above, along with orthogonality of $T$ and $T^{\\perp}$, we compute\n$$\n\\sup_{G \\in \\partial \\| \\cdot \\|_{*}(X^{\\star})} \\langle G, H \\rangle\n= \\langle e_{1} e_{1}^{\\top}, H_{T} \\rangle + \\sup_{\\substack{W \\in T^{\\perp} \\\\ \\|W\\|_{2} \\leq 1}} \\langle W, H_{T^{\\perp}} \\rangle.\n$$\nThe first term is $\\langle e_{1} e_{1}^{\\top}, -2 e_{1} e_{1}^{\\top} \\rangle = -2$. The second term is the support function of the spectral norm ball evaluated at $H_{T^{\\perp}}$, which equals the nuclear norm of $H_{T^{\\perp}}$:\n$$\n\\sup_{\\|W\\|_{2} \\leq 1} \\langle W, H_{T^{\\perp}} \\rangle = \\| H_{T^{\\perp}} \\|_{*}.\n$$\nHere $H_{T^{\\perp}} = e_{2} e_{2}^{\\top}$ is rank one with singular value $1$, so $\\| H_{T^{\\perp}} \\|_{*} = 1$. Therefore\n$$\n\\sup_{G \\in \\partial \\| \\cdot \\|_{*}(X^{\\star})} \\langle G, H \\rangle = -2 + 1 = -1.\n$$\nNormalizing $H$ to $\\widehat{H}$ scales the directional derivative by $\\|H\\|_{F}$, so we compute\n$$\n\\|H\\|_{F} = \\sqrt{(-2)^{2} + 1^{2}} = \\sqrt{5}, \\qquad\n\\sup_{G \\in \\partial \\| \\cdot \\|_{*}(X^{\\star})} \\langle G, \\widehat{H} \\rangle = \\frac{-1}{\\sqrt{5}}.\n$$\nSince this quantity is strictly negative, the directional derivative of the nuclear norm at $X^{\\star}$ along $\\widehat{H}$ is negative. Hence there exists $t > 0$ sufficiently small such that\n$$\n\\|X^{\\star} + t \\widehat{H}\\|_{*} < \\|X^{\\star}\\|_{*}.\n$$\nMoreover, because $H \\in \\ker(\\mathcal{A})$, we have $\\mathcal{A}(X^{\\star} + t H) = \\mathcal{A}(X^{\\star})$, showing that the feasible set contains a point with strictly smaller nuclear norm than $X^{\\star}$. Therefore, the program $\\min_{X} \\|X\\|_{*}$ subject to $\\mathcal{A}(X) = \\mathcal{A}(X^{\\star})$ fails to recover $X^{\\star}$.\n\nFor completeness, we can also verify this descent directly: for $0 < t < \\tfrac{1}{2}$,\n$$\nX^{\\star} + t H = (1 - 2 t)\\, e_{1} e_{1}^{\\top} + t\\, e_{2} e_{2}^{\\top} = \\operatorname{diag}(1 - 2 t,\\, t),\n$$\nwhose nuclear norm is $(1 - 2 t) + t = 1 - t < 1 = \\|X^{\\star}\\|_{*}$, while $\\mathcal{A}(X^{\\star} + t H) = \\mathcal{A}(X^{\\star})$.\n\nStep 3: Misalignment quantification.\nBy definition,\n$$\n\\eta(\\mathcal{A}, X^{\\star}) \\triangleq \\sup_{G \\in \\partial \\| \\cdot \\|_{*}(X^{\\star})} \\langle G, \\widehat{H} \\rangle = \\frac{-1}{\\sqrt{5}}.\n$$\nThis is the exact value; no rounding is required.\n\nThus, the requested misalignment measure is $\\eta(\\mathcal{A}, X^{\\star}) = - 1 / \\sqrt{5}$.", "answer": "$$\\boxed{-\\frac{1}{\\sqrt{5}}}$$", "id": "3469376"}]}