{"hands_on_practices": [{"introduction": "This first practice provides a concrete, small-scale example to build intuition for the synthesis model, where a signal $x$ is represented as $x = Dz$ for a sparse coefficient vector $z$. While the solution to this specific problem can be found by simple inspection, the main goal is to rigorously prove its optimality using the machinery of Lagrangian duality. This exercise will guide you through constructing a dual certificate, a fundamental skill for verifying solutions in sparse optimization [@problem_id:3445067].", "problem": "Consider the synthesis and analysis viewpoints in compressed sensing and sparse optimization. In the synthesis model, a signal $x \\in \\mathbb{R}^{n}$ is represented as $x = D z$ where $D \\in \\mathbb{R}^{n \\times p}$ is a synthesis dictionary and $z \\in \\mathbb{R}^{p}$ is a coefficient vector expected to be sparse. Given noiseless measurements $y \\in \\mathbb{R}^{m}$ and a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$, the synthesis Basis Pursuit (BP) problem is to find $z$ with minimal $\\ell_1$-norm such that the consistency constraint $A D z = y$ holds. In the analysis model, an analysis operator $\\Omega \\in \\mathbb{R}^{q \\times n}$ acts on $x$ to produce $\\Omega x$ expected to be sparse; the analysis BP minimizes the $\\ell_1$-norm of $\\Omega x$ under the measurement constraints.\n\nWork in the concrete setting with $n = 2$, $m = 2$, $p = 2$, $A = I \\in \\mathbb{R}^{2 \\times 2}$, $D = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}$, and $y = (1, 1)^{\\top} \\in \\mathbb{R}^{2}$. Starting from the definition of synthesis BP as the constrained minimization of the $\\ell_1$-norm of $z$ subject to the linear consistency constraint, derive the synthesis BP solution $z^{\\star} \\in \\mathbb{R}^{2}$ and the corresponding signal $x^{\\star} = D z^{\\star} \\in \\mathbb{R}^{2}$. Your derivation should be grounded in first principles, including feasibility and optimality certification via Lagrangian duality, and should make explicit the relationship to the analysis viewpoint through the dual feasibility condition. Express the final answer as the pair $(z^{\\star}, x^{\\star})$. No rounding is required, and no physical units are involved.", "solution": "The problem asks for the solution $(z^{\\star}, x^{\\star})$ to a specific instance of the synthesis Basis Pursuit (BP) problem, with a derivation based on first principles of Lagrangian duality and an explicit connection to the analysis viewpoint.\n\nThe synthesis BP problem is formulated as the minimization of the $\\ell_1$-norm of a coefficient vector $z$ subject to a linear consistency constraint:\n$$ \\min_{z \\in \\mathbb{R}^{p}} \\|z\\|_1 \\quad \\text{subject to} \\quad ADz = y $$\n\nFirst, we substitute the specified values for the matrices and vectors:\n- $n = 2$, $m = 2$, $p = 2$\n- Sensing matrix: $A = I \\in \\mathbb{R}^{2 \\times 2}$ (the identity matrix)\n- Synthesis dictionary: $D = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} \\in \\mathbb{R}^{2 \\times 2}$\n- Measurement vector: $y = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\in \\mathbb{R}^{2}$\n\nThe constraint $ADz = y$ simplifies to $Iz = Dz = y$. The primal optimization problem is therefore:\n$$ \\min_{z = (z_1, z_2)^{\\top} \\in \\mathbb{R}^{2}} |z_1| + |z_2| \\quad \\text{subject to} \\quad \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} z_1 \\\\ z_2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $$\n\nThe constraint represents a system of two linear equations in two variables:\n1. $z_1 + z_2 = 1$\n2. $z_2 = 1$\n\nFrom the second equation, we have $z_2 = 1$. Substituting this into the first equation gives $z_1 + 1 = 1$, which implies $z_1 = 0$.\nThus, the feasible set for the optimization problem consists of a single point, $z = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$. Since this is the only feasible point, it must be the solution to the minimization problem. We denote this primal solution as:\n$$ z^{\\star} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} $$\n\nThe corresponding signal $x^{\\star}$ is found by applying the synthesis dictionary $D$:\n$$ x^{\\star} = D z^{\\star} = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $$\n\nTo formally validate this solution and fulfill the problem's requirements, we use the framework of Lagrangian duality. The Lagrangian for the primal problem is:\n$$ \\mathcal{L}(z, \\nu) = \\|z\\|_1 + \\nu^{\\top}(y - Dz) $$\nwhere $\\nu \\in \\mathbb{R}^2$ is the Lagrange multiplier (or dual variable) associated with the equality constraint.\n\nThe Karush-Kuhn-Tucker (KKT) conditions provide necessary and sufficient conditions for optimality for this convex problem. A pair $(z^{\\star}, \\nu^{\\star})$ is optimal if it satisfies:\n1.  **Primal feasibility**: $Dz^{\\star} = y$.\n    As shown above, $z^{\\star} = (0, 1)^{\\top}$ satisfies this condition: $D z^{\\star} = (1, 1)^{\\top} = y$.\n2.  **Stationarity**: The subgradient of the Lagrangian with respect to $z$ at $z^{\\star}$ must contain the zero vector. This implies $0 \\in \\partial \\|z^\\star\\|_1 - D^{\\top}\\nu^{\\star}$, or equivalently, $D^{\\top}\\nu^{\\star} \\in \\partial \\|z^\\star\\|_1$.\n    The subgradient of the $\\ell_1$-norm $\\|z\\|_1 = |z_1| + |z_2|$ at $z^{\\star} = (0, 1)^{\\top}$ is the set of vectors $s=(s_1, s_2)^{\\top}$ where $s_1 \\in [-1, 1]$ and $s_2 = \\text{sign}(z_2^{\\star}) = 1$.\n    So, we must find a dual vector $\\nu^{\\star}$ such that $D^{\\top}\\nu^{\\star} = s$ for some $s$ with $s_1 \\in [-1, 1]$ and $s_2 = 1$.\n    The transpose of $D$ is $D^{\\top} = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix}$. The condition is:\n    $$ \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} \\nu_1^{\\star} \\\\ \\nu_2^{\\star} \\end{bmatrix} = \\begin{bmatrix} s_1 \\\\ 1 \\end{bmatrix} $$\n    This yields the system $\\nu_1^{\\star} = s_1$ and $\\nu_1^{\\star} + \\nu_2^{\\star} = 1$.\n3.  **Dual feasibility**: The dual variable $\\nu^{\\star}$ must be feasible for the dual problem. The dual problem is $\\max_{\\nu} y^{\\top}\\nu$ subject to $\\|D^{\\top}\\nu\\|_{\\infty} \\le 1$. So the condition is $\\|D^{\\top}\\nu^{\\star}\\|_{\\infty} \\le 1$.\n    From the stationarity condition, we have $D^{\\top}\\nu^{\\star} = (s_1, 1)^{\\top}$. Thus, the dual feasibility condition is $\\|(s_1, 1)^{\\top}\\|_{\\infty} = \\max(|s_1|, |1|) \\le 1$. Since we require $s_1 \\in [-1, 1]$, $|s_1| \\le 1$, and therefore $\\max(|s_1|, 1) = 1$. The condition is satisfied for any choice of $s_1 \\in [-1, 1]$.\n\nWe can choose a specific value for $s_1$, for instance $s_1=0$. This gives $\\nu_1^{\\star} = 0$. From $\\nu_1^{\\star} + \\nu_2^{\\star} = 1$, we get $\\nu_2^{\\star} = 1$. So, a valid dual certificate is $\\nu^{\\star} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$.\nWith this choice, all KKT conditions are satisfied, formally proving that $z^{\\star} = (0, 1)^{\\top}$ is the unique optimal solution.\n\nFinally, we address the relationship to the analysis viewpoint. The dual of the synthesis problem is:\n$$ (\\text{D}_{\\text{synth}}): \\quad \\max_{\\nu} y^{\\top}\\nu \\quad \\text{subject to} \\quad \\|D^{\\top}\\nu\\|_{\\infty} \\le 1 $$\nFor an invertible dictionary $D$ and $A=I$, the synthesis problem is equivalent to an analysis problem with the choice of analysis operator $\\Omega = D^{-1}$. The dual of this analysis problem is:\n$$ (\\text{D}_{\\text{anal}}): \\quad \\max_{\\lambda} y^{\\top}\\lambda \\quad \\text{subject to} \\quad \\lambda = \\Omega^{\\top}\\mu \\text{ for some } \\mu \\text{ with } \\|\\mu\\|_{\\infty} \\le 1 $$\nLet's examine the analysis dual constraint. Substituting $\\Omega = D^{-1}$, the constraint on the dual variable $\\lambda$ is that $\\lambda = (D^{-1})^{\\top}\\mu$ for some $\\mu$ with $\\|\\mu\\|_{\\infty} \\le 1$. Rearranging gives $D^{\\top}\\lambda = \\mu$. The condition $\\|\\mu\\|_{\\infty} \\le 1$ thus becomes $\\|D^{\\top}\\lambda\\|_{\\infty} \\le 1$. This is precisely the same feasibility constraint as for the dual variable $\\nu$ in the synthesis problem. The condition on the dual variable $\\nu$ in the synthesis problem is identical to the condition on the dual variable $\\lambda$ in the analysis problem, unifying the two perspectives through their dual formulations.\n\nThe derived synthesis BP solution is $z^{\\star} = (0, 1)^{\\top}$ and the corresponding signal is $x^{\\star} = (1, 1)^{\\top}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} & \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\end{pmatrix}}$$", "id": "3445067"}, {"introduction": "Shifting our focus to the analysis model, where a signal $x$ is assumed to yield a sparse result $\\Omega x$ after transformation, this exercise offers a parallel exploration to the first. Here, you will dissect an analysis Basis Pursuit problem by reformulating it as a linear program (LP), a standard technique for solving $\\ell_1$-minimization problems. By deriving the dual LP from first principles and solving the primal-dual pair, you will gain insight into the structure of the solution and the meaning of the cosupport through the lens of optimality conditions [@problem_id:3445026].", "problem": "Consider the analysis basis pursuit (ABP) problem, defined as minimizing the analysis one-norm subject to linear measurements: given a linear operator $A \\in \\mathbb{R}^{m \\times n}$, an analysis operator $\\Omega \\in \\mathbb{R}^{p \\times n}$, and data $y \\in \\mathbb{R}^{m}$, the ABP problem is\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\|\\Omega x\\|_{1} \\quad \\text{subject to} \\quad A x = y.\n$$\nYou will analyze the concrete instance with $n=3$, \n$$\n\\Omega=\\begin{bmatrix}1 & -1 & 0\\\\ 0 & 1 & -1\\end{bmatrix}, \\quad A=I_{3}, \\quad y=\\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix}.\n$$\nStarting from first principles, do the following:\n1) Introduce an auxiliary variable $t \\in \\mathbb{R}^{2}$ to rewrite the problem as a linear program in standard inequality form using only linear equalities/inequalities and a linear objective. Justify the equivalence of the formulations rigorously from the definition of the one-norm.\n2) Derive the Lagrangian dual of your linear program by constructing the Lagrangian, enforcing finiteness of the infimum over the primal variables, and writing down the dual feasibility conditions and the dual objective. Do not invoke any pre-memorized dual formulas; derive the dual directly from the Lagrangian.\n3) Solve the primal and dual problems explicitly for the given data. Identify a primal optimal solution $x^{\\star}$ and corresponding optimal slack $t^{\\star}$, as well as a dual optimal solution, and verify complementary slackness and strong duality.\n4) Identify the cosupport (the set of indices $i$ such that $(\\Omega x^{\\star})_{i}=0$). Explain how your identification follows from your computations and the Karush–Kuhn–Tucker conditions.\n\nProvide $x^{\\star}$ as your final reported answer in the form of a row vector. No rounding is required. The cosupport should be identified and justified in your solution, but it is not required in the final boxed answer.", "solution": "The problem as stated constitutes a valid, well-posed problem in convex optimization. It is scientifically sound, self-contained, and all terms are formally defined. Although the primal constraint $A=I_3$ uniquely determines the solution $x$, making the minimization trivial, the problem's request to perform a full analysis of the linear programming formulation, its dual, and the associated optimality conditions is a meaningful and non-trivial exercise. We therefore proceed with a complete solution.\n\nThe problem is to solve:\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\|\\Omega x\\|_1 \\quad \\text{subject to} \\quad A x = y\n$$\nwith the specified data:\n$$\n\\Omega=\\begin{bmatrix} 1 & -1 & 0 \\\\ 0 & 1 & -1 \\end{bmatrix}, \\quad A=I_3=\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}, \\quad y=\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n$$\nThe constraint $A x = y$ becomes $I_3 x = y$, which uniquely determines the feasible solution to be $x = y = \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix}^T$. While this immediately gives the primal optimal solution, we will follow the requested four-step analysis.\n\n**1) Linear Programming Formulation**\n\nThe objective function is the $\\ell_1$-norm of the vector $\\Omega x \\in \\mathbb{R}^2$. Let $z = \\Omega x$. The objective is $\\|z\\|_1 = |z_1| + |z_2|$.\nWe introduce an auxiliary variable $t = \\begin{bmatrix} t_1 \\\\ t_2 \\end{bmatrix} \\in \\mathbb{R}^2$. The expression $\\min \\| \\Omega x \\|_1$ is equivalent to $\\min (t_1 + t_2)$ subject to $t_1 \\ge |(\\Omega x)_1|$ and $t_2 \\ge |(\\Omega x)_2|$.\n\nThe justification for this equivalence is as follows. A value $c$ is an upper bound on $|a|$ if and only if $c \\ge a$ and $c \\ge -a$. Thus, the condition $t \\ge |\\Omega x|$ (element-wise) is equivalent to the pair of linear inequalities: $\\Omega x \\le t$ and $-\\Omega x \\le t$.\n\nGiven any feasible $x$ for the original problem, let us choose $t_i = |(\\Omega x)_i|$ for $i=1,2$. This choice of $t$ satisfies the inequalities $t \\ge \\Omega x$ and $t \\ge -\\Omega x$. The objective value for this $(x,t)$ pair in the new formulation is $\\sum_i t_i = \\sum_i |(\\Omega x)_i| = \\|\\Omega x\\|_1$.\nConversely, consider any feasible pair $(x,t)$ for the new formulation. The constraints require $t_i \\ge |(\\Omega x)_i|$. Since the objective is to minimize $\\sum_i t_i$ and the components $t_i$ are not coupled in any other constraint, the minimum will be achieved when each $t_i$ is as small as possible, i.e., when $t_i = |(\\Omega x)_i|$. Therefore, minimizing $\\sum_i t_i$ over the feasible set for $(x,t)$ is equivalent to minimizing $\\|\\Omega x\\|_1$ over the feasible set for $x$.\n\nThe problem is thus transformed into the following linear program (LP):\n$$\n\\min_{x \\in \\mathbb{R}^3, t \\in \\mathbb{R}^2} \\quad t_1 + t_2\n$$\nsubject to:\n$$\n\\begin{align*}\nA x &= y \\\\\n\\Omega x - t &\\le 0 \\\\\n-\\Omega x - t &\\le 0\n\\end{align*}\n$$\nwhere the inequalities are component-wise and $0$ is the zero vector in $\\mathbb{R}^2$.\n\n**2) Derivation of the Lagrangian Dual**\n\nTo derive the dual, we form the Lagrangian by associating dual variables with each constraint. Let $\\nu \\in \\mathbb{R}^3$ be the Lagrange multiplier for the equality constraint $y - Ax = 0$. Let $\\lambda_1, \\lambda_2 \\in \\mathbb{R}^2$ be the Lagrange multipliers for the inequality constraints, with the non-negativity constraints $\\lambda_1 \\ge 0$ and $\\lambda_2 \\ge 0$.\n\nThe Lagrangian $L(x, t, \\nu, \\lambda_1, \\lambda_2)$ is:\n$$\nL = \\mathbf{1}^T t + \\nu^T(y - Ax) + \\lambda_1^T(\\Omega x - t) + \\lambda_2^T(-\\Omega x - t)\n$$\nwhere $\\mathbf{1} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$. We rearrange the terms to group primal variables $x$ and $t$:\n$$\nL = \\nu^T y + x^T(\\Omega^T(\\lambda_1 - \\lambda_2) - A^T\\nu) + t^T(\\mathbf{1} - \\lambda_1 - \\lambda_2)\n$$\nThe Lagrange dual function $g(\\nu, \\lambda_1, \\lambda_2)$ is the infimum of the Lagrangian over the primal variables $x$ and $t$. For the infimum to be finite (i.e., not $-\\infty$), the terms linear in $x$ and $t$ must vanish. This gives us the dual feasibility conditions:\n1. Coefficient of $x^T$: $\\Omega^T(\\lambda_1 - \\lambda_2) - A^T\\nu = 0 \\implies A^T\\nu = \\Omega^T(\\lambda_1 - \\lambda_2)$.\n2. Coefficient of $t^T$: $\\mathbf{1} - \\lambda_1 - \\lambda_2 = 0 \\implies \\lambda_1 + \\lambda_2 = \\mathbf{1}$.\n\nSubject to these conditions, the Lagrangian becomes $L = \\nu^T y$. The dual problem is to maximize this value:\n$$\n\\max_{\\nu, \\lambda_1, \\lambda_2} \\quad \\nu^T y\n$$\nsubject to:\n$$\n\\begin{align*}\nA^T\\nu &= \\Omega^T(\\lambda_1 - \\lambda_2) \\\\\n\\lambda_1 + \\lambda_2 &= \\mathbf{1} \\\\\n\\lambda_1 &\\ge 0 \\\\\n\\lambda_2 &\\ge 0\n\\end{align*}\n$$\nLet us simplify this formulation. Define $\\mu = \\lambda_1 - \\lambda_2$. From $\\lambda_1 + \\lambda_2 = \\mathbf{1}$, we can express $\\lambda_1 = \\frac{1}{2}(\\mathbf{1} + \\mu)$ and $\\lambda_2 = \\frac{1}{2}(\\mathbf{1} - \\mu)$. The non-negativity constraints $\\lambda_1, \\lambda_2 \\ge 0$ become $\\mathbf{1}+\\mu \\ge 0$ and $\\mathbf{1}-\\mu \\ge 0$, which is equivalent to $-1 \\le \\mu_i \\le 1$ for each component $i$, or $\\|\\mu\\|_\\infty \\le 1$.\nThe first dual constraint becomes $A^T\\nu = \\Omega^T\\mu$. Since $A=I_3$, we have $\\nu = \\Omega^T\\mu$. Substituting this into the objective gives the final dual problem in terms of $\\mu$:\n$$\n\\max_{\\mu} \\quad (\\Omega^T \\mu)^T y = \\mu^T (\\Omega y) \\quad \\text{subject to} \\quad \\|\\mu\\|_\\infty \\le 1.\n$$\nThis is the correct dual formulation.\n\n**3) Primal and Dual Solutions**\n\n**Primal Solution:**\nThe constraint $Ax=y$ with $A=I_3$ fixes the primal solution to be $x^\\star = y = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\nThe vector $\\Omega x^\\star$ is:\n$$\n\\Omega x^\\star = \\begin{bmatrix} 1 & -1 & 0 \\\\ 0 & 1 & -1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}.\n$$\nFor the LP formulation, the optimal slack variable $t^\\star$ must satisfy $t^\\star_i = |(\\Omega x^\\star)_i|$.\n$$\nt^\\star = \\begin{bmatrix} |1| \\\\ |0| \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}.\n$$\nThe optimal value of the primal problem is $t^\\star_1 + t^\\star_2 = 1+0=1$.\n\n**Dual Solution:**\nThe dual problem is $\\max_{\\mu} \\mu^T (\\Omega y)$ subject to $\\|\\mu\\|_\\infty \\le 1$. We have $\\Omega y = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\nSo we must solve:\n$$\n\\max_{\\mu_1, \\mu_2} \\quad \\mu_1(1) + \\mu_2(0) \\quad \\text{subject to} \\quad -1 \\le \\mu_1 \\le 1, \\quad -1 \\le \\mu_2 \\le 1.\n$$\nThe objective is simply $\\mu_1$. To maximize it, we choose the largest possible value, $\\mu_1^\\star = 1$. The variable $\\mu_2$ does not affect the objective, so any value in its feasible range $[-1, 1]$ is optimal. A valid choice is $\\mu_2^\\star = 0$.\nAn optimal dual solution is $\\mu^\\star = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\nThe optimal dual value is $1$. This confirms strong duality, as the primal and dual optimal values are equal.\n\nWe can find the corresponding full set of dual variables:\n$\\nu^\\star = \\Omega^T \\mu^\\star = \\begin{bmatrix} 1 & 0 \\\\ -1 & 1 \\\\ 0 & -1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix}$.\n$\\lambda_1^\\star = \\frac{1}{2}(\\mathbf{1}+\\mu^\\star) = \\frac{1}{2} \\left( \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\right) = \\begin{bmatrix} 1 \\\\ 1/2 \\end{bmatrix}$.\n$\\lambda_2^\\star = \\frac{1}{2}(\\mathbf{1}-\\mu^\\star) = \\frac{1}{2} \\left( \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\right) = \\begin{bmatrix} 0 \\\\ 1/2 \\end{bmatrix}$.\n\n**Verification of Complementary Slackness:**\nThe KKT complementary slackness conditions for the LP are $\\lambda_1^T(\\Omega x - t)=0$ and $\\lambda_2^T(-\\Omega x - t)=0$.\nLet's check these conditions at the optimal solutions $(x^\\star, t^\\star)$ and $(\\lambda_1^\\star, \\lambda_2^\\star)$.\nPrimal slacks:\n$\\Omega x^\\star - t^\\star = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n$-\\Omega x^\\star - t^\\star = \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} -2 \\\\ 0 \\end{bmatrix}$.\nDual variables: $\\lambda_1^\\star = \\begin{bmatrix} 1 \\\\ 1/2 \\end{bmatrix}$, $\\lambda_2^\\star = \\begin{bmatrix} 0 \\\\ 1/2 \\end{bmatrix}$.\n\nCondition 1: $(\\lambda_1^\\star)_i (\\Omega x^\\star - t^\\star)_i = 0$.\n- For $i=1$: $1 \\times 0 = 0$.\n- For $i=2$: $(1/2) \\times 0 = 0$.\nCondition 2: $(\\lambda_2^\\star)_i (-\\Omega x^\\star - t^\\star)_i = 0$.\n- For $i=1$: $0 \\times (-2) = 0$.\n- For $i=2$: $(1/2) \\times 0 = 0$.\nAll conditions are satisfied, verifying the optimality of our solutions.\n\n**4) Cosupport and Karush–Kuhn–Tucker (KKT) Conditions**\n\nThe cosupport of a vector $z$ is the set of indices $i$ for which $z_i=0$. For the optimal solution $x^\\star$, we examine the vector $\\Omega x^\\star = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$. The first component is non-zero, and the second component is zero. Thus, the cosupport of $\\Omega x^\\star$ is the set $\\{2\\}$.\n\nThe connection to the KKT conditions for the original non-smooth problem $\\min_x \\|\\Omega x\\|_1$ s.t. $Ax=y$ provides insight. The stationarity condition is $0 \\in \\partial_x L(x, \\nu)$, where $L(x,\\nu) = \\|\\Omega x\\|_1 + \\nu^T(y-Ax)$. This gives:\n$$\n0 \\in \\Omega^T \\partial(\\|\\cdot\\|_1)|_{\\Omega x} - A^T\\nu\n$$\nwhich means there must exist a vector $\\mu \\in \\partial(\\|\\cdot\\|_1)|_{\\Omega x}$ such that $A^T\\nu = \\Omega^T\\mu$.\nThe subgradient of the $\\ell_1$-norm at a point $z$ is the set of vectors $\\mu$ where:\n- $\\mu_i = \\text{sign}(z_i)$ if $z_i \\ne 0$ (i.e., $i$ is in the support).\n- $\\mu_i \\in [-1, 1]$ if $z_i = 0$ (i.e., $i$ is in the cosupport).\n\nAt the optimal solution $x^\\star$, we have $\\Omega x^\\star = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$. The KKT conditions require the existence of an optimal dual variable $\\mu^\\star$ such that:\n- $\\mu^\\star_1 = \\text{sign}(1) = 1$.\n- $\\mu^\\star_2 \\in [-1, 1]$.\n\nOur dual problem solution yielded a set of optimal dual variables $\\mu$ of the form $\\begin{bmatrix} 1 \\\\ \\mu_2 \\end{bmatrix}$ where $\\mu_2 \\in [-1, 1]$. This is perfectly consistent with the KKT conditions. Our specific choice, $\\mu^\\star = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, is one such valid certificate.\n\nThe identification of the cosupport follows from this relationship. The fact that the optimal dual variable $\\mu^\\star$ is not uniquely determined for its second component (any $\\mu_2 \\in [-1,1]$ is valid) implies that the second component of $\\Omega x^\\star$ must lie on the point of non-differentiability of the absolute value function, which is $0$. Conversely, the first component $\\mu_1^\\star=1$ is fixed, corresponding to the fact that $(\\Omega x^\\star)_1 \\ne 0$. More precisely, if we find an optimal dual solution $\\mu^\\star$ where $|\\mu_i^\\star| < 1$, it is necessary that $(\\Omega x^\\star)_i=0$, so $i$ must be in the cosupport. In our case, we found $\\mu^\\star=\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$. Since $|\\mu_2^\\star|=0 < 1$, we can conclude that the index $2$ is in the cosupport of $\\Omega x^\\star$. Since $|\\mu_1^\\star|=1$, the index $1$ is in the support. This matches our direct computation.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 0 & 0\n\\end{pmatrix}\n}\n$$", "id": "3445026"}, {"introduction": "Having established the mechanics of both models, this final practice moves from manual calculations to a computational experiment that addresses a central question: when should we prefer the analysis model over the synthesis model? You will construct a scenario where a highly coherent synthesis dictionary requires many measurements for signal recovery, whereas an analysis model tailored to the signal's structure succeeds with far fewer. This exercise empirically demonstrates the powerful theoretical concept that the geometry of the signal model, not just its sparsity, dictates the sample complexity required for recovery [@problem_id:3445022].", "problem": "Consider a noiseless compressed sensing setup in which an unknown signal $\\mathbf{x} \\in \\mathbb{R}^{n}$ is measured through a linear operator $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, producing $\\mathbf{y} = \\mathbf{A}\\mathbf{x}$. Two convex formulations are used for recovery:\n\n1. Synthesis formulation: Given a dictionary $\\mathbf{D} \\in \\mathbb{R}^{n \\times p}$, recover $\\boldsymbol{\\alpha} \\in \\mathbb{R}^{p}$ via the problem\n$$\n\\min_{\\boldsymbol{\\alpha} \\in \\mathbb{R}^{p}} \\|\\boldsymbol{\\alpha}\\|_{1} \\quad \\text{subject to} \\quad \\mathbf{A}\\mathbf{D}\\boldsymbol{\\alpha} = \\mathbf{y}, \n$$\nand form the estimate $\\hat{\\mathbf{x}}_{\\text{syn}} = \\mathbf{D}\\hat{\\boldsymbol{\\alpha}}$.\n\n2. Analysis formulation: Given an analysis operator $\\boldsymbol{\\Omega} \\in \\mathbb{R}^{r \\times n}$, recover $\\mathbf{x}$ via the problem\n$$\n\\min_{\\mathbf{x} \\in \\mathbb{R}^{n}} \\|\\boldsymbol{\\Omega}\\mathbf{x}\\|_{1} \\quad \\text{subject to} \\quad \\mathbf{A}\\mathbf{x} = \\mathbf{y}.\n$$\n\nThe mutual coherence between two collections of vectors is defined as the maximum absolute inner product between any two unit-norm vectors drawn from the respective collections. For a dictionary $\\mathbf{D}$, the mutual coherence among its columns is\n$$\n\\mu(\\mathbf{D}) = \\max_{i \\neq j} \\frac{|\\mathbf{d}_i^{\\top} \\mathbf{d}_j|}{\\|\\mathbf{d}_i\\|_2 \\|\\mathbf{d}_j\\|_2},\n$$\nwhere $\\mathbf{d}_i$ are columns of $\\mathbf{D}$. For an analysis operator $\\boldsymbol{\\Omega}$ and measurement operator $\\mathbf{A}$, a representative mutual coherence is taken between their unit-norm rows,\n$$\n\\mu(\\mathbf{A}, \\boldsymbol{\\Omega}) = \\max_{i,j} \\frac{|\\mathbf{a}_i^{\\top} \\boldsymbol{\\omega}_j|}{\\|\\mathbf{a}_i\\|_2 \\|\\boldsymbol{\\omega}_j\\|_2},\n$$\nwhere $\\mathbf{a}_i$ are rows of $\\mathbf{A}$ and $\\boldsymbol{\\omega}_j$ are rows of $\\boldsymbol{\\Omega}$.\n\nFrom the perspective of geometric compressed sensing, the number of measurements sufficient for exact recovery in noiseless settings is governed by the Gaussian width of the descent (tangent) cone of the objective at the ground truth, which depends on the structure and coherence of the model. Highly coherent dictionaries produce wider descent cones in the synthesis formulation, while structured cosupports under low mutual coherence with the measurement operator produce narrower descent cones in the analysis formulation.\n\nUsing this principle, construct a self-contained computational experiment with the following specifications:\n\n- Use $n = 32$ and create a highly coherent dictionary $\\mathbf{D} \\in \\mathbb{R}^{32 \\times 64}$ by starting from an orthonormal Discrete Cosine Transform basis (columns of $\\mathbb{R}^{32 \\times 32}$) and appending near-duplicate columns to raise $\\mu(\\mathbf{D})$ above $0.95$. Normalize all columns to unit norm.\n- Use an analysis operator $\\boldsymbol{\\Omega} \\in \\mathbb{R}^{31 \\times 32}$ as the first-order discrete difference: each row has $+1$ at position $i$ and $-1$ at position $i+1$, for $i=1,\\dots,31$. This produces a structured cosupport for piecewise-constant signals.\n- Use measurement operators $\\mathbf{A} \\in \\mathbb{R}^{m \\times 32}$ with rows sampled independently from a standard normal distribution and then normalized to unit norm; for the boundary case $m=n$, use $\\mathbf{A} = \\mathbf{I}_{32}$ to ensure an exact solution exists. These random measurements yield low $\\mu(\\mathbf{A}, \\boldsymbol{\\Omega})$.\n- Define three ground-truth signals $\\mathbf{x}$ for the test suite:\n  1. A piecewise-constant signal with four segments of equal length: $\\mathbf{x} = [0,\\dots,0,\\, 1,\\dots,1,\\, -0.5,\\dots,-0.5,\\, 0.5,\\dots,0.5]$.\n  2. A boundary case with a single jump: the first $16$ entries equal $0$ and the remaining $16$ entries equal $1$.\n  3. An edge case where $\\mathbf{x}$ has independent and identically distributed standard normal entries and then is scaled to unit norm.\n- For each $\\mathbf{x}$, for $m$ ranging over the set $\\{8,12,16,20,24,28,32\\}$, compute $\\mathbf{y} = \\mathbf{A}\\mathbf{x}$ and solve both the synthesis and analysis problems by reducing the $\\ell_{1}$ objectives to linear programs with auxiliary variables:\n  - Synthesis linear program:\n    - Variables: $\\boldsymbol{\\alpha} \\in \\mathbb{R}^{p}$ and $\\mathbf{s} \\in \\mathbb{R}^{p}$.\n    - Objective: minimize $\\sum_{i=1}^{p} s_i$.\n    - Constraints: $\\mathbf{A}\\mathbf{D}\\boldsymbol{\\alpha} = \\mathbf{y}$, $-\\mathbf{s} \\le \\boldsymbol{\\alpha} \\le \\mathbf{s}$, and $\\mathbf{s} \\ge \\mathbf{0}$.\n  - Analysis linear program:\n    - Variables: $\\mathbf{x} \\in \\mathbb{R}^{n}$ and $\\mathbf{t} \\in \\mathbb{R}^{r}$.\n    - Objective: minimize $\\sum_{i=1}^{r} t_i$.\n    - Constraints: $\\mathbf{A}\\mathbf{x} = \\mathbf{y}$, $-\\mathbf{t} \\le \\boldsymbol{\\Omega}\\mathbf{x} \\le \\mathbf{t}$, and $\\mathbf{t} \\ge \\mathbf{0}$.\n- Declare exact recovery if the relative error satisfies $\\|\\hat{\\mathbf{x}} - \\mathbf{x}\\|_{2} / \\|\\mathbf{x}\\|_{2} \\le 10^{-3}$ and the linear program reports success.\n- For each signal in the test suite, determine the minimal number of measurements $m_{\\text{an}}$ for analysis recovery and $m_{\\text{syn}}$ for synthesis recovery over the given set of $m$ values. Compute and report the integer differences $m_{\\text{syn}} - m_{\\text{an}}$ for each case to empirically demonstrate that analysis recovers with fewer measurements when the dictionary is highly coherent and the cosupport is structured with low mutual coherence with the measurements.\n\nYour program should produce a single line of output containing the three integer differences as a comma-separated list enclosed in square brackets (e.g., \"[d1,d2,d3]\").", "solution": "The problem presents a computational experiment to compare the sample complexity of two a priori signal models in compressed sensing: the synthesis model and the analysis model. It is hypothesized that for signals with a specific structure (piecewise-constant), the analysis formulation will achieve exact recovery with fewer measurements than the synthesis formulation, particularly when the synthesis dictionary is highly coherent. This experiment is a demonstration of a known duality and a key concept in the theoretical underpinnings of compressed sensing. The problem is well-posed, scientifically grounded, and provides all necessary specifications for a reproducible computational study.\n\nThe core of the problem lies in the distinction between two ways of formalizing signal sparsity.\n\n$1$. **The Synthesis Model**: This model posits that a signal $\\mathbf{x} \\in \\mathbb{R}^{n}$ can be synthesized as a sparse linear combination of atoms from a dictionary $\\mathbf{D} \\in \\mathbb{R}^{n \\times p}$. That is, $\\mathbf{x} = \\mathbf{D}\\boldsymbol{\\alpha}$, where the coefficient vector $\\boldsymbol{\\alpha} \\in \\mathbb{R}^{p}$ has few non-zero entries (is sparse). Given measurements $\\mathbf{y} = \\mathbf{A}\\mathbf{x} = \\mathbf{A}\\mathbf{D}\\boldsymbol{\\alpha}$, one recovers the sparse coefficients $\\boldsymbol{\\alpha}$ by solving the Basis Pursuit problem, a convex relaxation of the intractable $\\ell_0$ pseudo-norm minimization:\n$$\n\\hat{\\boldsymbol{\\alpha}} = \\arg\\min_{\\boldsymbol{\\alpha} \\in \\mathbb{R}^{p}} \\|\\boldsymbol{\\alpha}\\|_{1} \\quad \\text{subject to} \\quad \\mathbf{A}\\mathbf{D}\\boldsymbol{\\alpha} = \\mathbf{y}.\n$$\nThe signal estimate is then formed as $\\hat{\\mathbf{x}}_{\\text{syn}} = \\mathbf{D}\\hat{\\boldsymbol{\\alpha}}$. The success of this recovery heavily depends on the properties of the composite sensing matrix $\\mathbf{A}\\mathbf{D}$. A critical parameter is the mutual coherence of the dictionary, $\\mu(\\mathbf{D})$. A high $\\mu(\\mathbf{D})$ implies that atoms in the dictionary are nearly linearly dependent, which degrades recovery performance and typically necessitates a larger number of measurements $m$.\n\n$2$. **The Analysis Model**: This model offers a more general perspective. It assumes that the signal $\\mathbf{x}$ is not necessarily sparse itself but becomes sparse after being transformed by an analysis operator $\\boldsymbol{\\Omega} \\in \\mathbb{R}^{r \\times n}$. That is, $\\boldsymbol{\\Omega}\\mathbf{x}$ is sparse. A classic example is a piecewise-constant signal, which is not sparse in the standard basis but its gradient (or finite difference) is. Given measurements $\\mathbf{y} = \\mathbf{A}\\mathbf{x}$, the signal is recovered by solving:\n$$\n\\hat{\\mathbf{x}}_{\\text{an}} = \\arg\\min_{\\mathbf{x} \\in \\mathbb{R}^{n}} \\|\\boldsymbol{\\Omega}\\mathbf{x}\\|_{1} \\quad \\text{subject to} \\quad \\mathbf{A}\\mathbf{x} = \\mathbf{y}.\n$$\nThe success of this approach depends on the properties of $\\mathbf{A}$ and $\\boldsymbol{\\Omega}$, specifically on the geometric relationship between the null space of $\\mathbf{A}$ and the set of signals with non-sparse transforms $\\{\\mathbf{z} : \\|\\boldsymbol{\\Omega}\\mathbf{z}\\|_1 \\text{ is large}\\}$. Low mutual coherence $\\mu(\\mathbf{A}, \\boldsymbol{\\Omega})$ between the measurement and analysis operators is beneficial for recovery.\n\nThe experiment is designed to create a scenario where the synthesis model is disadvantaged and the analysis model is advantaged.\n- A highly coherent dictionary with $\\mu(\\mathbf{D}) > 0.95$ is constructed. This makes it difficult for the synthesis problem to distinguish between similar atoms, thus requiring more information (i.e., more measurements $m$).\n- The test signals are chosen to be piecewise-constant, a class of signals for which the first-order finite difference operator $\\boldsymbol{\\Omega}$ is an effective sparsifying transform.\n- The measurement matrix $\\mathbf{A}$ is constructed with random Gaussian rows, which, with high probability, will have low coherence with the structured rows of $\\boldsymbol{\\Omega}$.\n\nThis setup predicts that for the piecewise-constant signals, the analysis model will successfully recover the signal with a smaller number of measurements ($m_{\\text{an}}$) compared to the synthesis model ($m_{\\text{syn}}$), leading to a positive difference $m_{\\text{syn}} - m_{\\text{an}}$. A random noise signal, which is not sparse in either model, is included as a control case.\n\nThe implementation proceeds as follows:\n\nFirst, we construct the operators and a test suite of signals as specified.\n- The dimension is $n=32$.\n- The analysis operator $\\boldsymbol{\\Omega} \\in \\mathbb{R}^{31 \\times 32}$ is the first-order finite difference matrix.\n- The synthesis dictionary $\\mathbf{D} \\in \\mathbb{R}^{32 \\times 64}$ is constructed from a $32 \\times 32$ Discrete Cosine Transform (DCT) matrix, let's call it $\\mathbf{C}$. The first $32$ columns of $\\mathbf{D}$ are the columns of $\\mathbf{C}$. The next $32$ columns are created to be near-duplicates of the first $32$, ensuring high coherence. Specifically, for each column $\\mathbf{c}_i$ of $\\mathbf{C}$, a new column is formed as $\\mathbf{d}_{32+i} = \\cos(\\theta)\\mathbf{c}_i + \\sin(\\theta)\\mathbf{c}_{i+1}$ (with index wrap-around) for a small angle $\\theta$. Choosing $\\theta = 0.3$ radians gives an inner product of $\\cos(0.3) \\approx 0.955 > 0.95$, satisfying the condition. Since the columns of $\\mathbf{C}$ are orthonormal, the new columns are already unit-norm.\n- The three ground-truth signals $\\mathbf{x}$ are created: two piecewise-constant signals and one unit-norm Gaussian noise signal.\n\nSecond, for each signal, we iterate through the number of measurements $m \\in \\{8, 12, 16, 20, 24, 28, 32\\}$.\n- For each $m$, a measurement matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times 32}$ is generated (or set to $\\mathbf{I}_{32}$ for $m=32$), and the measurements $\\mathbf{y} = \\mathbf{A}\\mathbf{x}$ are computed.\n- The $\\ell_1$-minimization problems for both synthesis and analysis are converted into linear programs (LPs) with auxiliary variables. The synthesis LP involves $2p = 128$ variables, while the analysis LP involves $n+r = 32+31 = 63$ variables.\n- These LPs are solved using a standard numerical solver.\n- After a successful LP solution, the recovered signal $\\hat{\\mathbf{x}}$ is obtained, and the relative recovery error $\\|\\hat{\\mathbf{x}} - \\mathbf{x}\\|_{2} / \\|\\mathbf{x}\\|_{2}$ is calculated.\n- If the error is below the threshold of $10^{-3}$, the recovery is deemed exact for that value of $m$. The minimal $m$ for which this occurs is recorded as $m_{\\text{syn}}$ or $m_{\\text{an}}$.\n\nFinally, the difference $m_{\\text{syn}} - m_{\\text{an}}$ is computed for each of the three test signals. The provided Python code executes this complete computational experiment.", "answer": "```python\nimport numpy as np\nfrom scipy.fftpack import dct\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Solves the compressed sensing problem as described.\n    This function sets up the computational experiment to compare synthesis\n    and analysis recovery models, finds the minimal number of measurements\n    for each, and computes their difference.\n    \"\"\"\n    \n    # --------------------------------------------------------------------------\n    # 1. Define constants and operators\n    # --------------------------------------------------------------------------\n    N = 32\n    P = 64\n    R = N - 1\n    M_VALUES = [8, 12, 16, 20, 24, 28, 32]\n    RECOVERY_TOLERANCE = 1e-3\n    \n    # Analysis operator Omega (first-order finite difference)\n    Omega = np.zeros((R, N))\n    for i in range(R):\n        Omega[i, i] = 1\n        Omega[i, i + 1] = -1\n\n    # Synthesis dictionary D (highly coherent DCT-based dictionary)\n    # Start with an orthonormal DCT-II basis\n    C = dct(np.eye(N), type=2, norm='ortho')\n    \n    # Append near-duplicate columns to increase coherence\n    C_prime = np.zeros_like(C)\n    theta = 0.3  # cos(0.3) is approx 0.955 > 0.95\n    for i in range(N):\n        j = (i + 1) % N\n        C_prime[:, i] = np.cos(theta) * C[:, i] + np.sin(theta) * C[:, j]\n    \n    # All columns are already unit-norm due to orthonormality of C\n    D = np.hstack([C, C_prime])\n\n    # --------------------------------------------------------------------------\n    # 2. Define ground-truth signals\n    # --------------------------------------------------------------------------\n    x1 = np.concatenate([\n        np.zeros(N // 4),\n        np.ones(N // 4),\n        -0.5 * np.ones(N // 4),\n        0.5 * np.ones(N // 4)\n    ])\n    \n    x2 = np.concatenate([\n        np.zeros(N // 2),\n        np.ones(N // 2)\n    ])\n    \n    RNG_sig = np.random.default_rng(seed=42)\n    x3_raw = RNG_sig.standard_normal(N)\n    x3 = x3_raw / np.linalg.norm(x3_raw)\n    \n    test_signals = [x1, x2, x3]\n\n    # --------------------------------------------------------------------------\n    # 3. Main loop for the experiment\n    # --------------------------------------------------------------------------\n    results = []\n    \n    for signal_idx, x_true in enumerate(test_signals):\n        x_norm = np.linalg.norm(x_true)\n        if x_norm == 0: x_norm = 1.0\n        \n        m_syn_min = float('inf')\n        m_an_min = float('inf')\n\n        # Find minimal m for synthesis recovery\n        for m in M_VALUES:\n            # Measurement operator A. Use a fixed seed for reproducibility.\n            if m  N:\n                RNG_m = np.random.default_rng(seed=42 + m)\n                A_raw = RNG_m.standard_normal((m, N))\n                A = A_raw / np.linalg.norm(A_raw, axis=1, keepdims=True)\n            else: # m == N\n                A = np.eye(N)\n                \n            y = A @ x_true\n            \n            # Synthesis LP formulation\n            c_syn = np.concatenate([np.zeros(P), np.ones(P)])\n            AD = A @ D\n            A_eq_syn = np.hstack([AD, np.zeros((m, P))])\n            b_eq_syn = y\n            \n            Id_P = np.eye(P)\n            A_ub_syn = np.vstack([\n                np.hstack([Id_P, -Id_P]),\n                np.hstack([-Id_P, -Id_P])\n            ])\n            b_ub_syn = np.zeros(2 * P)\n            \n            bounds_syn = [(None, None)] * P + [(0, None)] * P\n\n            res_syn = linprog(c_syn, A_ub=A_ub_syn, b_ub=b_ub_syn,\n                              A_eq=A_eq_syn, b_eq=b_eq_syn,\n                              bounds=bounds_syn, method='highs')\n            \n            if res_syn.success:\n                alpha_hat = res_syn.x[:P]\n                x_hat_syn = D @ alpha_hat\n                err = np.linalg.norm(x_hat_syn - x_true) / x_norm\n                if err = RECOVERY_TOLERANCE:\n                    m_syn_min = m\n                    break\n\n        # Find minimal m for analysis recovery\n        for m in M_VALUES:\n            # Measurement operator A. Use the same seed as synthesis for fair comparison.\n            if m  N:\n                RNG_m = np.random.default_rng(seed=42 + m)\n                A_raw = RNG_m.standard_normal((m, N))\n                A = A_raw / np.linalg.norm(A_raw, axis=1, keepdims=True)\n            else: # m == N\n                A = np.eye(N)\n            \n            y = A @ x_true\n            \n            # Analysis LP formulation\n            c_an = np.concatenate([np.zeros(N), np.ones(R)])\n            A_eq_an = np.hstack([A, np.zeros((m, R))])\n            b_eq_an = y\n            \n            Id_R = np.eye(R)\n            A_ub_an = np.vstack([\n                np.hstack([Omega, -Id_R]),\n                np.hstack([-Omega, -Id_R])\n            ])\n            b_ub_an = np.zeros(2 * R)\n\n            bounds_an = [(None, None)] * N + [(0, None)] * R\n\n            res_an = linprog(c_an, A_ub=A_ub_an, b_ub=b_ub_an,\n                             A_eq=A_eq_an, b_eq=b_eq_an,\n                             bounds=bounds_an, method='highs')\n            \n            if res_an.success:\n                x_hat_an = res_an.x[:N]\n                err = np.linalg.norm(x_hat_an - x_true) / x_norm\n                if err = RECOVERY_TOLERANCE:\n                    m_an_min = m\n                    break\n                    \n        results.append(m_syn_min - m_an_min)\n\n    # --------------------------------------------------------------------------\n    # 4. Final output\n    # --------------------------------------------------------------------------\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3445022"}]}