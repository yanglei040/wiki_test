{"hands_on_practices": [{"introduction": "This first practice provides a foundational exercise in the synthesis sparsity model. You will analyze a classic compressed sensing scenario involving partial Fourier measurements and signals that are sparse in the standard basis. By calculating the mutual coherence and deriving the resulting recovery guarantee, you will solidify your understanding of how incoherence between the sensing and sparsity bases directly enables efficient signal reconstruction from incomplete data [@problem_id:3485055].", "problem": "Consider a discrete signal space of dimension $n \\in \\mathbb{N}$. Let $F \\in \\mathbb{C}^{n \\times n}$ be the unitary discrete Fourier transform matrix with entries $F_{k\\ell} = \\frac{1}{\\sqrt{n}} \\exp\\!\\big(-2\\pi i \\frac{k\\ell}{n}\\big)$ for $k,\\ell \\in \\{0,1,\\dots,n-1\\}$, so that each row and each column of $F$ has unit $\\ell_{2}$ norm. Form a sensing matrix $A \\in \\mathbb{C}^{m \\times n}$ by selecting $m$ distinct rows of $F$ uniformly at random, where $1 \\leq m \\leq n$. Consider the synthesis sparsity model with a fixed dictionary $D \\in \\mathbb{C}^{n \\times n}$ equal to the identity matrix, so that any synthesis coefficient vector $x \\in \\mathbb{C}^{n}$ generates a signal $z = D x = x$, and $s$-sparse synthesis signals satisfy $|\\operatorname{supp}(x)| \\leq s$.\n\nDefine the scaled mutual coherence between the sensing matrix $A$ and the dictionary $D$ by\n$$\n\\mu(A,D) := \\sqrt{n} \\, \\max_{i \\in \\{1,\\dots,m\\}} \\max_{j \\in \\{1,\\dots,n\\}} \\left| \\langle a_{i}, d_{j} \\rangle \\right|,\n$$\nwhere $a_{i} \\in \\mathbb{C}^{n}$ denotes the $i$-th row of $A$ viewed as a unit-norm vector in $\\mathbb{C}^{n}$ and $d_{j} \\in \\mathbb{C}^{n}$ denotes the $j$-th column of $D$, also of unit norm. Assume noiseless measurements $y = A z = A D x$.\n\nStarting from well-tested facts in compressed sensing about incoherent sensing with synthesis sparsity, and without invoking any shortcut formulas not logically derived from those facts, analyze $\\mu(A,D)$ for this setting, and then derive the worst-case sufficient recovery guarantee for exact reconstruction of any $s$-sparse synthesis vector $x$ via $\\ell_{1}$ minimization as a function of $m$, $n$, and an absolute constant $C > 0$. Express the largest sparsity level $s_{\\max}(m,n,C)$ that this guarantee can certify as a single closed-form analytic expression. No numerical rounding is required, and no physical units are involved. Your final answer must be a single analytic expression.", "solution": "The problem asks for the derivation of the largest sparsity level $s_{\\max}$ for which exact recovery of an $s$-sparse signal is guaranteed via $\\ell_1$ minimization, given a specific compressed sensing setup. The derivation must start from fundamental principles and a careful analysis of the provided quantities.\n\nFirst, we validate and analyze the quantities defined in the problem statement.\nThe signal space is $\\mathbb{C}^{n}$. The synthesis dictionary is the identity matrix, $D = I_n \\in \\mathbb{C}^{n \\times n}$. This implies that the signals are sparse in the standard (canonical) basis. An $s$-sparse signal $z \\in \\mathbb{C}^n$ can be written as $z = D x = x$, where $x \\in \\mathbb{C}^n$ is a coefficient vector with at most $s$ non-zero entries, i.e., $\\|x\\|_0 = |\\operatorname{supp}(x)| \\leq s$. The columns of the dictionary $D=I_n$ are the canonical basis vectors $\\{e_j\\}_{j=0}^{n-1}$, denoted as $d_j$. Each $d_j$ is a unit-norm vector, $\\|d_j\\|_2 = 1$.\n\nThe sensing process is defined by a matrix $A \\in \\mathbb{C}^{m \\times n}$, which is constructed by selecting $m$ distinct rows, uniformly at random, from the $n \\times n$ unitary discrete Fourier transform (DFT) matrix $F$. The entries of $F$ are given by $F_{k\\ell} = \\frac{1}{\\sqrt{n}} \\exp\\big(-2\\pi i \\frac{k\\ell}{n}\\big)$ for $k,\\ell \\in \\{0, 1, \\dots, n-1\\}$. The rows of $F$ are orthonormal, and thus each row has a unit $\\ell_2$ norm. Consequently, the rows of $A$, which are a subset of the rows of $F$, are also unit-norm vectors in $\\mathbb{C}^n$. Let the set of selected row indices be $\\Omega \\subset \\{0, 1, \\dots, n-1\\}$, with $|\\Omega| = m$.\n\nThe problem defines the scaled mutual coherence between the sensing matrix $A$ and the dictionary $D$ as\n$$\n\\mu(A,D) := \\sqrt{n} \\, \\max_{i \\in \\{1,\\dots,m\\}} \\max_{j \\in \\{1,\\dots,n\\}} \\left| \\langle a_{i}, d_{j} \\rangle \\right|.\n$$\nHere, $a_i$ represents the $i$-th row of $A$ viewed as a vector in $\\mathbb{C}^n$, and $d_j$ is the $j$-th column of $D$. Let's analyze the inner product $\\langle a_i, d_j \\rangle$. Let the $i$-th row of $A$ correspond to the $k$-th row of $F$ for some $k \\in \\Omega$. We represent this row as a vector in $\\mathbb{C}^n$. The standard inner product in $\\mathbb{C}^n$ is $\\langle u, v \\rangle = v^* u$. However, given the context, $\\langle a_i, d_j \\rangle$ is most naturally interpreted as the matrix product of the row vector $a_i$ and the column vector $d_j$, or as the inner product between the column vector representation of the row $a_i$ and the column vector $d_j$. Both interpretations lead to the same magnitude.\n\nLet $a_i$ be the row vector corresponding to the $k$-th row of $F$. Its components are $(a_i)_{\\ell} = F_{k\\ell}$. The vector $d_j$ is the canonical basis vector $e_j$. The inner product $\\langle a_i^T, d_j \\rangle = d_j^* a_i^T = e_j^T a_i^T = (a_i e_j^T)^T$. This interpretation is notationally cumbersome. A more standard approach is to consider the inner product between the vector formed by the conjugate transpose of the row $a_i$ (let's call it $a_i^{\\text{vec}}$) and the column vector $d_j$. Let us assume this standard interpretation: we take the row vector, say $r_k^T$, and treat it as a column vector $r_k$. The inner product is then $\\langle r_k, e_j \\rangle = e_j^* r_k$. This gives the $j$-th component of the vector $r_k$. The $j$-th component of the $k$-th row of $F$ is $F_{kj}$. So, $|\\langle a_i, d_j \\rangle| = |F_{kj}|$.\nThe magnitude of any entry of the DFT matrix $F$ is\n$$\n|F_{k\\ell}| = \\left| \\frac{1}{\\sqrt{n}} \\exp\\left(-2\\pi i \\frac{k\\ell}{n}\\right) \\right| = \\frac{1}{\\sqrt{n}} \\left| \\exp\\left(-2\\pi i \\frac{k\\ell}{n}\\right) \\right| = \\frac{1}{\\sqrt{n}}.\n$$\nThis value is constant for all entries. Therefore, the maximum of these magnitudes is also $\\frac{1}{\\sqrt{n}}$.\n$$\n\\max_{i \\in \\{1,\\dots,m\\}} \\max_{j \\in \\{1,\\dots,n\\}} \\left| \\langle a_{i}, d_{j} \\rangle \\right| = \\frac{1}{\\sqrt{n}}.\n$$\nThis result is independent of the specific choice of $m$ rows. Now we can compute the scaled mutual coherence:\n$$\n\\mu(A,D) = \\sqrt{n} \\cdot \\left( \\frac{1}{\\sqrt{n}} \\right) = 1.\n$$\nThis quantity, which the problem labels $\\mu(A,D)$, is more precisely the coherence between the full Fourier basis $F$ and the standard basis $D=I_n$, and it quantifies their maximal incoherence.\n\nThe next step is to use this result to derive the recovery guarantee. The problem states we should start from \"well-tested facts in compressed sensing\". A cornerstone result of compressed sensing, established by Candès, Romberg, and Tao, provides such a guarantee for the setting of random partial measurements of a signal sparse in an incoherent basis.\nThe theorem states that if a sensing matrix $\\mathcal{A} \\in \\mathbb{C}^{m \\times n}$ is formed by selecting $m$ rows uniformly at random from an $n \\times n$ unitary matrix $U$, then any signal $z$ that is $s$-sparse with respect to an orthonormal basis $\\Psi$ (i.e., $z = \\Psi x$ with $\\|x\\|_0 \\le s$) can be recovered exactly and stably from the measurements $y = \\mathcal{A} z$ via $\\ell_1$ minimization. A sufficient condition on the number of measurements $m$ for this recovery to hold with high probability is:\n$$\nm \\geq C \\cdot (\\mu(U, \\Psi))^2 \\cdot s \\cdot \\ln(n)\n$$\nwhere $C > 0$ is an absolute constant, and $\\mu(U, \\Psi) = \\sqrt{n} \\max_{k,\\ell} |\\langle u_k, \\psi_\\ell \\rangle|$ is the mutual coherence between the rows $u_k$ of $U$ and the columns $\\psi_\\ell$ of $\\Psi$.\n\nIn our specific problem:\n1.  The unitary matrix is the DFT matrix, $U=F$.\n2.  The sensing matrix $A$ is formed by selecting $m$ rows of $F$.\n3.  The signal $z=x$ is sparse in the standard basis, which means the synthesis dictionary is the identity matrix, $\\Psi = D = I_n$.\n4.  The coherence parameter $\\mu(U, \\Psi)$ corresponds exactly to the quantity we have calculated to be $\\mu(A,D)=1$ (if interpreted for the full matrices $F$ and $I_n$).\nSo, we have $\\mu(F, I_n) = 1$.\n\nSubstituting this into the condition from the theorem, we get:\n$$\nm \\geq C \\cdot (1)^2 \\cdot s \\cdot \\ln(n)\n$$\n$$\nm \\geq C s \\ln(n)\n$$\nThis inequality provides a sufficient condition on $m$ for the successful recovery of any $s$-sparse signal. The problem asks for the largest sparsity level $s_{\\max}(m,n,C)$ that this guarantee can certify. We can find this by rearranging the inequality to solve for $s$:\n$$\ns \\leq \\frac{m}{C \\ln(n)}\n$$\nThe guarantee holds for any sparsity $s$ up to this bound. Therefore, the maximum sparsity level is:\n$$\ns_{\\max}(m,n,C) = \\frac{m}{C \\ln(n)}\n$$\nThis is the worst-case sufficient recovery guarantee in the sense that for a randomly chosen $A$ (with $m$ satisfying the condition), any $s$-sparse signal can be recovered. The guarantee is probabilistic with respect to the choice of $A$, but holds for all $s$-sparse signals once $A$ is fixed.\nThe expression is a closed-form analytic function of $m$, $n$, and $C$, as required.", "answer": "$$\\boxed{\\frac{m}{C \\ln(n)}}$$", "id": "3485055"}, {"introduction": "The synthesis and analysis models, while related, are not equivalent, and this exercise provides a crucial, hands-on demonstration of their differences by guiding you through a concrete counterexample. You will solve an identical recovery problem under both frameworks and observe a case where the analysis model succeeds in exact recovery while the synthesis model fails. This practice offers deep insight into the distinct underlying principles and recovery capabilities of each model [@problem_id:3485109].", "problem": "Consider a finite-dimensional linear inverse problem in compressed sensing where the measurement operator is $\\Phi \\in \\mathbb{R}^{m \\times n}$, the synthesis dictionary is $D \\in \\mathbb{R}^{n \\times q}$, and the analysis operator is $\\Omega \\in \\mathbb{R}^{p \\times n}$. The synthesis model posits that a signal $x \\in \\mathbb{R}^{n}$ admits a sparse representation $x = D \\alpha$ for some coefficient vector $\\alpha \\in \\mathbb{R}^{q}$, and reconstruction can be attempted by solving $\\min_{\\alpha} \\|\\alpha\\|_{1}$ subject to $y = \\Phi D \\alpha$, where $y \\in \\mathbb{R}^{m}$ are the measurements. The analysis model posits that the vector $\\Omega x$ is sparse, and reconstruction can be attempted by solving $\\min_{x} \\|\\Omega x\\|_{1}$ subject to $y = \\Phi x$. The cosupport of $\\Omega x$ is defined as the set of indices of the rows of $\\Omega$ for which $(\\Omega x)_{i} = 0$.\n\nConstruct a counterexample where exact recovery holds for the analysis model but fails for the synthesis model due to a mismatch between the cosupport of $\\Omega x$ and the support of $\\alpha$ in $D$, even though the measurements $y = \\Phi x$ are identical. Use the following concrete setting:\n- Let $n = 3$ and $m = 2$.\n- Let $\\Phi \\in \\mathbb{R}^{2 \\times 3}$ be given by\n$$\n\\Phi = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{pmatrix}.\n$$\n- Let the ground-truth signal be $x_{0} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$, so that $y = \\Phi x_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n- Let the analysis operator $\\Omega \\in \\mathbb{R}^{2 \\times 3}$ be\n$$\n\\Omega = \\begin{pmatrix}\n1 & -1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\n- Let the synthesis dictionary $D \\in \\mathbb{R}^{3 \\times 2}$ have columns $v_{1} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$ and $v_{2} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$, namely\n$$\nD = \\begin{pmatrix}\n1 & 0 \\\\\n1 & 0 \\\\\n1 & 1\n\\end{pmatrix}.\n$$\n\nTasks:\n1. Using the analysis model, solve the convex program $\\min_{x \\in \\mathbb{R}^{3}} \\|\\Omega x\\|_{1}$ subject to $y = \\Phi x$, and determine the unique minimizer $x_{A}^{\\star}$.\n2. Using the synthesis model, solve the convex program $\\min_{\\alpha \\in \\mathbb{R}^{2}} \\|\\alpha\\|_{1}$ subject to $y = \\Phi D \\alpha$, and determine the unique minimizer $x_{S}^{\\star} = D \\alpha^{\\star}$.\n3. Compute the squared Euclidean distance $\\|x_{A}^{\\star} - x_{S}^{\\star}\\|_{2}^{2}$ and express your result as an exact real number. No rounding is required. If you introduce any angles, express them in radians.\n\nYour construction must explicitly exhibit that the analysis model achieves exact recovery of $x_{0}$, whereas the synthesis model fails to recover $x_{0}$ under the same measurements $y$, and the mismatch between the cosupport of $\\Omega x$ and the support of $\\alpha$ should be clear from your derivation. The final answer must be the single real number equal to $\\|x_{A}^{\\star} - x_{S}^{\\star}\\|_{2}^{2}$.", "solution": "The objective is to solve two distinct sparse recovery problems, one based on the analysis model and the other on the synthesis model, and to compare their solutions for a given ground-truth signal $x_{0}$ and measurement setup.\n\n**1. Analysis Model Recovery**\n\nThe analysis model seeks to find a signal $x$ that is consistent with the measurements $y = \\Phi x$ and has the sparsest possible analysis representation $\\Omega x$. The optimization problem is:\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\|\\Omega x\\|_{1} \\quad \\text{subject to} \\quad y = \\Phi x\n$$\nWe are given $y = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $\\Phi = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix}$. Let $x = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}$. The constraint $y = \\Phi x$ becomes:\n$$\n\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}\n$$\nThis implies that any feasible solution must have $x_1 = 1$ and $x_2 = 1$. The feasible set is the affine subspace of all vectors of the form $x = \\begin{pmatrix} 1 \\\\ 1 \\\\ z \\end{pmatrix}$ for any $z \\in \\mathbb{R}$.\n\nNow, we evaluate the objective function $\\|\\Omega x\\|_{1}$ for such a vector:\n$$\n\\Omega x = \\begin{pmatrix} 1 & -1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ z \\end{pmatrix} = \\begin{pmatrix} (1)(1) + (-1)(1) + (0)(z) \\\\ (0)(1) + (0)(1) + (1)(z) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ z \\end{pmatrix}\n$$\nThe $\\ell_1$-norm is $\\|\\Omega x\\|_{1} = |0| + |z| = |z|$.\nThe optimization problem reduces to finding $z$ that minimizes $|z|$:\n$$\n\\min_{z \\in \\mathbb{R}} |z|\n$$\nThe minimum value is $0$, which is uniquely achieved at $z=0$.\nTherefore, the unique minimizer of the analysis problem is:\n$$\nx_{A}^{\\star} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nThis is precisely the ground-truth signal $x_{0}$. The analysis model achieves exact recovery. For this solution, the analysis representation is $\\Omega x_{A}^{\\star} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, which is maximally sparse. The cosupport of $\\Omega x_{A}^{\\star}$ is $\\{1, 2\\}$.\n\n**2. Synthesis Model Recovery**\n\nThe synthesis model assumes the signal can be sparsely represented as $x = D\\alpha$ and seeks the sparsest coefficient vector $\\alpha$ consistent with the measurements. The problem is:\n$$\n\\min_{\\alpha \\in \\mathbb{R}^{2}} \\|\\alpha\\|_{1} \\quad \\text{subject to} \\quad y = \\Phi D \\alpha\n$$\nFirst, we compute the effective measurement matrix $\\Phi D$:\n$$\n\\Phi D = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & 0 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 0 \\end{pmatrix}\n$$\nThe constraint $y = \\Phi D \\alpha$ with $y = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $\\alpha = \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\end{pmatrix}$ is:\n$$\n\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\end{pmatrix} = \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_1 \\end{pmatrix}\n$$\nThis implies $\\alpha_1=1$, while $\\alpha_2$ can be any real number. The feasible set for $\\alpha$ consists of all vectors of the form $\\alpha = \\begin{pmatrix} 1 \\\\ w \\end{pmatrix}$ for any $w \\in \\mathbb{R}$.\n\nWe minimize the objective function $\\|\\alpha\\|_{1}$ over this feasible set:\n$$\n\\|\\alpha\\|_{1} = \\left\\| \\begin{pmatrix} 1 \\\\ w \\end{pmatrix} \\right\\|_{1} = |1| + |w| = 1 + |w|\n$$\nThe problem reduces to $\\min_{w \\in \\mathbb{R}} (1 + |w|)$. The minimum value is $1$, which is uniquely achieved when $w=0$.\nThus, the unique optimal coefficient vector is:\n$$\n\\alpha^{\\star} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\nThe recovered signal via the synthesis model is $x_{S}^{\\star} = D \\alpha^{\\star}$:\n$$\nx_{S}^{\\star} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 0 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nThis result, $x_{S}^{\\star} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$, is not equal to the ground-truth signal $x_{0} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$. The synthesis model fails to achieve exact recovery.\n\nThe reason for failure is that the ground-truth signal $x_0$ does not have a sparse representation in the dictionary $D$. We can find the true coefficient vector $\\alpha_0$ such that $x_0 = D\\alpha_0$:\n$$\n\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 0 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\end{pmatrix} \\implies \\alpha_1=1, \\alpha_1+\\alpha_2=0 \\implies \\alpha_2=-1\n$$\nSo, $\\alpha_0 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$. The support of $\\alpha_0$ is $\\{1, 2\\}$. The $\\ell_1$-norm is $\\|\\alpha_0\\|_{1} = |1| + |-1| = 2$. However, the synthesis recovery found a different solution $\\alpha^{\\star} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ with a smaller norm $\\|\\alpha^{\\star}\\|_{1} = 1$. The signal $x_{S}^{\\star}$ corresponding to this sparser representation also satisfies the measurements, since $\\Phi x_{S}^{\\star} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = y$. The $\\ell_1$-minimization naturally prefers the solution corresponding to the coefficient vector with the smaller norm, which in this case is the incorrect one.\n\n**3. Final Calculation**\n\nWe compute the squared Euclidean distance between the two recovered signals, $x_{A}^{\\star}$ and $x_{S}^{\\star}$:\n$$\nx_{A}^{\\star} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad x_{S}^{\\star} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nThe difference vector is:\n$$\nx_{A}^{\\star} - x_{S}^{\\star} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ -1 \\end{pmatrix}\n$$\nThe squared Euclidean distance is:\n$$\n\\|x_{A}^{\\star} - x_{S}^{\\star}\\|_{2}^{2} = \\left\\| \\begin{pmatrix} 0 \\\\ 0 \\\\ -1 \\end{pmatrix} \\right\\|_{2}^{2} = 0^{2} + 0^{2} + (-1)^{2} = 1\n$$\nThe final result is $1$.", "answer": "$$\\boxed{1}$$", "id": "3485109"}, {"introduction": "Moving from theoretical guarantees to practical estimation, this exercise addresses the inherent bias of analysis $\\ell_1$-minimization, a common challenge when dealing with noisy data. You will first identify the source of this bias from the optimality conditions of the analysis LASSO problem. Subsequently, you will derive and apply a principled debiasing technique—a constrained least-squares refitting—to improve the accuracy of an initial estimate, a valuable skill for practical applications [@problem_id:3485107].", "problem": "Consider the linear observation model $y = A x_{0} + w$ with known measurement matrix $A \\in \\mathbb{R}^{m \\times n}$, unknown signal $x_{0} \\in \\mathbb{R}^{n}$, noise $w \\in \\mathbb{R}^{m}$, and a fixed analysis operator $\\Omega \\in \\mathbb{R}^{p \\times n}$. The analysis $\\ell_{1}$ estimator is defined as the solution $\\widehat{x}_{\\lambda} \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\tfrac{1}{2} \\|A x - y\\|_{2}^{2} + \\lambda \\|\\Omega x\\|_{1} \\right\\}$ for a regularization parameter $\\lambda > 0$. Let the estimated cosupport be the index set $\\Lambda \\subset \\{1,\\dots,p\\}$ such that $(\\Omega \\widehat{x}_{\\lambda})_{i} = 0$ for all $i \\in \\Lambda$, and define the cosupport subspace $\\mathcal{S}_{\\Lambda} := \\{ x \\in \\mathbb{R}^{n} : \\Omega_{\\Lambda} x = 0 \\}$, where $\\Omega_{\\Lambda}$ is the submatrix of $\\Omega$ with rows indexed by $\\Lambda$.\n\nStarting from first principles grounded in convex optimality and linear algebra, do the following. First, characterize the stationarity condition for the analysis $\\ell_{1}$ problem using the Karush–Kuhn–Tucker (KKT) conditions, and identify the term that induces bias relative to the unregularized least-squares solution. Next, derive a principled debiasing step that refits the signal by solving a constrained least-squares problem over the estimated cosupport subspace, namely\n$$\nx_{\\mathrm{db}} \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\|A x - y\\|_{2}^{2} \\quad \\text{subject to} \\quad \\Omega_{\\Lambda} x = 0,\n$$\nand obtain a closed-form expression for $x_{\\mathrm{db}}$ under the assumptions that $A$ has full column rank and $\\Omega_{\\Lambda}$ has full row rank.\n\nFinally, evaluate your general expression in the concrete case with $n = 3$, $m = 3$, $A = I_{3}$, $\\Omega \\in \\mathbb{R}^{2 \\times 3}$ given by\n$$\n\\Omega = \\begin{pmatrix}\n1 & -1 & 0 \\\\\n0 & 1 & -1\n\\end{pmatrix},\n$$\nnoiseless data $w = 0$, measurement vector $y = \\begin{pmatrix} 3 \\\\ 1 \\\\ 2 \\end{pmatrix}$, and an estimated cosupport $\\Lambda = \\{1\\}$ (meaning the first analysis coefficient is estimated to be zero). Compute the exact debiased estimate $x_{\\mathrm{db}}$ for this instance. Express your final answer as a single row vector using exact values. No rounding is required.", "solution": "The solution is presented in three parts as requested: characterization of the stationarity condition for the analysis $\\ell_1$ estimator, derivation of the closed-form debiased estimator, and evaluation for a specific instance.\n\n**Part 1: Stationarity Condition and Bias Identification**\n\nThe analysis $\\ell_1$ estimation problem is defined as:\n$$\n\\widehat{x}_{\\lambda} \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ F(x) := \\tfrac{1}{2} \\|A x - y\\|_{2}^{2} + \\lambda \\|\\Omega x\\|_{1} \\right\\}\n$$\nThis is a convex optimization problem, as the objective function $F(x)$ is a sum of a convex quadratic term and a convex (but non-differentiable) $\\ell_1$ term. A vector $\\widehat{x}_{\\lambda}$ is a minimizer of $F(x)$ if and only if the zero vector is an element of the subdifferential of $F$ at $\\widehat{x}_{\\lambda}$. The optimality condition is:\n$$\n0 \\in \\partial F(\\widehat{x}_{\\lambda})\n$$\nThe subdifferential of $F(x)$ is given by the sum of the gradient of the differentiable part and the subdifferential of the non-differentiable part:\n$$\n\\partial F(x) = \\nabla \\left( \\tfrac{1}{2} \\|A x - y\\|_{2}^{2} \\right) + \\lambda \\partial \\left( \\|\\Omega x\\|_{1} \\right)\n$$\nThe gradient of the least-squares term is $A^T(Ax - y)$. The subdifferential of the composite function $\\|\\Omega x\\|_{1}$ is given by the chain rule for subgradients: $\\partial (\\|\\Omega \\cdot\\|_{1})(x) = \\Omega^T \\partial (\\|\\cdot\\|_{1})(\\Omega x)$.\n\nThe subdifferential of the $\\ell_1$-norm of a vector $u \\in \\mathbb{R}^p$, denoted $\\partial \\|u\\|_1$, is the set of vectors $s \\in \\mathbb{R}^p$ such that for each component $i=1,\\dots,p$:\n$$\ns_i = \\begin{cases}\n\\text{sign}(u_i) & \\text{if } u_i \\neq 0 \\\\\n\\in [-1, 1] & \\text{if } u_i = 0\n\\end{cases}\n$$\nwhere $\\text{sign}(\\cdot)$ is the signum function.\n\nThus, the optimality condition for $\\widehat{x}_{\\lambda}$ is that there must exist a vector $s \\in \\partial (\\|\\cdot\\|_{1})(\\Omega \\widehat{x}_{\\lambda})$ such that:\n$$\nA^T(A \\widehat{x}_{\\lambda} - y) + \\lambda \\Omega^T s = 0\n$$\nThis is the Karush–Kuhn–Tucker (KKT) stationarity condition for the analysis $\\ell_1$ problem. Rearranging this equation yields:\n$$\nA^T A \\widehat{x}_{\\lambda} = A^T y - \\lambda \\Omega^T s\n$$\nFor comparison, the unregularized least-squares solution, which we denote by $x_{\\text{LS}}$, is the solution to the normal equations:\n$$\nA^T A x_{\\text{LS}} = A^T y\n$$\nBy comparing the two systems of equations, the term $-\\lambda \\Omega^T s$ is identified as the source of bias in the analysis $\\ell_1$ estimator $\\widehat{x}_{\\lambda}$ relative to the least-squares solution. This term systematically shrinks the analysis coefficients $\\Omega x$, which in turn biases the estimate of $x$ itself.\n\n**Part 2: Derivation of the Debiased Estimator**\n\nThe debiasing step involves solving the following constrained least-squares problem:\n$$\nx_{\\mathrm{db}} \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\|A x - y\\|_{2}^{2} \\quad \\text{subject to} \\quad \\Omega_{\\Lambda} x = 0\n$$\nTo find the solution, we employ the method of Lagrange multipliers. The objective function is $\\frac{1}{2}\\|Ax - y\\|_2^2$ (the factor of $\\frac{1}{2}$ does not change the minimizer but simplifies differentiation). The Lagrangian $\\mathcal{L}(x, \\mu)$ is:\n$$\n\\mathcal{L}(x, \\mu) = \\tfrac{1}{2} \\|A x - y\\|_{2}^{2} + \\mu^T (\\Omega_{\\Lambda} x)\n$$\nwhere $\\mu$ is the vector of Lagrange multipliers. The KKT conditions for optimality are obtained by setting the gradients of the Lagrangian with respect to $x$ and $\\mu$ to zero.\n\n1.  $\\nabla_x \\mathcal{L}(x, \\mu) = A^T(A x - y) + \\Omega_{\\Lambda}^T \\mu = 0$\n2.  $\\nabla_\\mu \\mathcal{L}(x, \\mu) = \\Omega_{\\Lambda} x = 0$\n\nFrom the first equation, we can express $x$ in terms of $\\mu$:\n$$\nA^T A x = A^T y - \\Omega_{\\Lambda}^T \\mu\n$$\nThe problem assumes $A$ has full column rank, which implies $A^T A$ is invertible. Therefore:\n$$\nx = (A^T A)^{-1} (A^T y - \\Omega_{\\Lambda}^T \\mu)\n$$\nSubstituting this expression for $x$ into the second KKT condition (the constraint equation):\n$$\n\\Omega_{\\Lambda} \\left[ (A^T A)^{-1} (A^T y - \\Omega_{\\Lambda}^T \\mu) \\right] = 0\n$$\nExpanding and rearranging to solve for $\\mu$:\n$$\n\\Omega_{\\Lambda} (A^T A)^{-1} A^T y - \\Omega_{\\Lambda} (A^T A)^{-1} \\Omega_{\\Lambda}^T \\mu = 0\n$$\n$$\n\\left( \\Omega_{\\Lambda} (A^T A)^{-1} \\Omega_{\\Lambda}^T \\right) \\mu = \\Omega_{\\Lambda} (A^T A)^{-1} A^T y\n$$\nThe matrix $M = \\Omega_{\\Lambda} (A^T A)^{-1} \\Omega_{\\Lambda}^T$ is invertible. This is because $A^T A$ is positive definite, so its inverse is also positive definite. Since $\\Omega_{\\Lambda}$ is assumed to have full row rank, for any non-zero vector $v$, we have $\\Omega_{\\Lambda}^T v \\neq 0$. Thus, $v^T M v = (\\Omega_{\\Lambda}^T v)^T (A^T A)^{-1} (\\Omega_{\\Lambda}^T v) > 0$, which means $M$ is positive definite and therefore invertible.\n\nWe can now solve for $\\mu$:\n$$\n\\mu = \\left( \\Omega_{\\Lambda} (A^T A)^{-1} \\Omega_{\\Lambda}^T \\right)^{-1} \\Omega_{\\Lambda} (A^T A)^{-1} A^T y\n$$\nFinally, we substitute this expression for $\\mu$ back into the equation for $x$ to obtain the closed-form solution for the debiased estimator $x_{\\mathrm{db}}$:\n$$\nx_{\\mathrm{db}} = (A^T A)^{-1} A^T y - (A^T A)^{-1} \\Omega_{\\Lambda}^T \\left( \\Omega_{\\Lambda} (A^T A)^{-1} \\Omega_{\\Lambda}^T \\right)^{-1} \\Omega_{\\Lambda} (A^T A)^{-1} A^T y\n$$\n\n**Part 3: Evaluation for the Specific Case**\n\nWe are given the following specific values:\n-   $n = 3$, $m = 3$\n-   $A = I_3$ (the $3 \\times 3$ identity matrix)\n-   $y = \\begin{pmatrix} 3 \\\\ 1 \\\\ 2 \\end{pmatrix}$\n-   $\\Omega = \\begin{pmatrix} 1 & -1 & 0 \\\\ 0 & 1 & -1 \\end{pmatrix}$\n-   $\\Lambda = \\{1\\}$\n\nThe assumption that $A$ has full column rank is satisfied, as $I_3$ is invertible.\nThe cosupport $\\Lambda = \\{1\\}$ means we select the first row of $\\Omega$ to form $\\Omega_{\\Lambda}$:\n$$\n\\Omega_{\\Lambda} = \\begin{pmatrix} 1 & -1 & 0 \\end{pmatrix}\n$$\nThis $1 \\times 3$ matrix has rank $1$, so it has full row rank as required.\n\nWith $A = I_3$, the terms in the general expression for $x_{\\mathrm{db}}$ simplify significantly:\n-   $A^T A = I_3^T I_3 = I_3$\n-   $(A^T A)^{-1} = I_3^{-1} = I_3$\n-   $(A^T A)^{-1} A^T y = I_3 y = y$\n\nSubstituting these into the general formula gives:\n$$\nx_{\\mathrm{db}} = y - I_3 \\Omega_{\\Lambda}^T (\\Omega_{\\Lambda} I_3 \\Omega_{\\Lambda}^T)^{-1} \\Omega_{\\Lambda} I_3 y = y - \\Omega_{\\Lambda}^T (\\Omega_{\\Lambda} \\Omega_{\\Lambda}^T)^{-1} \\Omega_{\\Lambda} y\n$$\nThis is the formula for the orthogonal projection of $y$ onto the null space of $\\Omega_{\\Lambda}$, which is the subspace $\\mathcal{S}_{\\Lambda}$.\n\nNow, we compute the components:\n-   $\\Omega_{\\Lambda} = \\begin{pmatrix} 1 & -1 & 0 \\end{pmatrix}$\n-   $\\Omega_{\\Lambda}^T = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}$\n-   $\\Omega_{\\Lambda} \\Omega_{\\Lambda}^T = \\begin{pmatrix} 1 & -1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = (1)(1) + (-1)(-1) + (0)(0) = 2$\n-   $(\\Omega_{\\Lambda} \\Omega_{\\Lambda}^T)^{-1} = (2)^{-1} = \\frac{1}{2}$\n-   $\\Omega_{\\Lambda} y = \\begin{pmatrix} 1 & -1 & 0 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\\\ 2 \\end{pmatrix} = (1)(3) + (-1)(1) + (0)(2) = 2$\n\nNow, we assemble the correction term:\n$$\n\\Omega_{\\Lambda}^T (\\Omega_{\\Lambda} \\Omega_{\\Lambda}^T)^{-1} \\Omega_{\\Lambda} y = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} \\left(\\frac{1}{2}\\right) (2) = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}\n$$\nFinally, we compute $x_{\\mathrm{db}}$:\n$$\nx_{\\mathrm{db}} = y - \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 1 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 - 1 \\\\ 1 - (-1) \\\\ 2 - 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix}\n$$\nThe problem asks for the answer as a single row vector.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2 & 2 & 2\n\\end{pmatrix}\n}\n$$", "id": "3485107"}]}