{"hands_on_practices": [{"introduction": "To build a solid understanding of the co-sparsity model, we begin with an idealized yet fundamental scenario: the oracle estimator. This exercise [@problem_id:3486309] asks you to compute the signal estimate assuming the true cosupport is known beforehand. By working through the constrained least-squares problem, you will master the essential mechanics of the model and establish a theoretical benchmark against which practical algorithms are measured.", "problem": "Consider the co-sparsity analysis model in compressed sensing, where an analysis operator $\\Omega \\in \\mathbb{R}^{p \\times n}$ acts on a signal $x \\in \\mathbb{R}^{n}$, and the co-support $\\Lambda \\subseteq \\{1,2,\\dots,p\\}$ is the set of indices where the analysis coefficients vanish, that is, $\\Omega_{\\Lambda} x = 0$. The oracle estimator is defined as the unique minimizer $x^{\\star}$ of the equality-constrained least-squares problem $\\min_{x \\in \\mathbb{R}^{n}} \\|A x - y\\|_{2}^{2}$ subject to $\\Omega_{\\Lambda} x = 0$, when the true co-support $\\Lambda$ is known.\n\nWork with the following small instance at dimension $n=5$ and measurements $m=3$. Let the analysis operator $\\Omega \\in \\mathbb{R}^{4 \\times 5}$ be the first-difference operator\n$$\n\\Omega \\;=\\;\n\\begin{pmatrix}\n1 & -1 & 0 & 0 & 0 \\\\\n0 & 1 & -1 & 0 & 0 \\\\\n0 & 0 & 1 & -1 & 0 \\\\\n0 & 0 & 0 & 1 & -1\n\\end{pmatrix},\n$$\nand let the co-support be $\\Lambda = \\{1,3,4\\}$, so that the exact linear constraints are\n$$\n\\Omega_{\\Lambda} x = 0 \\quad \\Longleftrightarrow \\quad\n\\begin{cases}\nx_{1} - x_{2} = 0, \\\\\nx_{3} - x_{4} = 0, \\\\\nx_{4} - x_{5} = 0.\n\\end{cases}\n$$\nLet the measurement operator $A \\in \\mathbb{R}^{3 \\times 5}$ and the measurement vector $y \\in \\mathbb{R}^{3}$ be\n$$\nA \\;=\\;\n\\begin{pmatrix}\n1 & 0 & 1 & 0 & 0 \\\\\n0 & 1 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1\n\\end{pmatrix},\n\\qquad\ny \\;=\\;\n\\begin{pmatrix}\n2 \\\\\n2 \\\\\n3\n\\end{pmatrix}.\n$$\n\nTasks:\n1. Starting from first principles, derive the equality-constrained least-squares optimality conditions by forming the Lagrangian and the Karush-Kuhn-Tucker (KKT) system, and explicitly solve the resulting stacked linear system to obtain the oracle estimator $x^{\\star}$.\n2. Verify the uniqueness of the solution by computing the rank of the vertically stacked matrix $\\begin{pmatrix} A \\\\ \\Omega_{\\Lambda} \\end{pmatrix}$ and interpreting it in terms of the intersection of null spaces.\n3. Report the value of the third component $x^{\\star}_{3}$ of the oracle estimator as your final answer.\n\nNo rounding is required. Provide your final answer as a single real number without units.", "solution": "The problem is validated as being scientifically grounded, well-posed, objective, and complete. It is a standard exercise in optimization and linear algebra within the context of compressed sensing. I will proceed with the solution.\n\nThe problem asks for the oracle estimator $x^{\\star} \\in \\mathbb{R}^{n}$, which is the unique solution to the equality-constrained least-squares problem:\n$$ \\min_{x \\in \\mathbb{R}^{n}} \\|A x - y\\|_{2}^{2} \\quad \\text{subject to} \\quad \\Omega_{\\Lambda} x = 0 $$\nwhere $n=5$. The measurement operator $A \\in \\mathbb{R}^{3 \\times 5}$ and vector $y \\in \\mathbb{R}^{3}$ are given by:\n$$ A = \\begin{pmatrix} 1 & 0 & 1 & 0 & 0 \\\\ 0 & 1 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 & 1 \\end{pmatrix}, \\qquad y = \\begin{pmatrix} 2 \\\\ 2 \\\\ 3 \\end{pmatrix} $$\nThe analysis operator is $\\Omega \\in \\mathbb{R}^{4 \\times 5}$ and the co-support is $\\Lambda = \\{1,3,4\\}$. The constraint matrix $\\Omega_{\\Lambda}$ consists of the rows of $\\Omega$ indexed by $\\Lambda$:\n$$ \\Omega_{\\Lambda} = \\begin{pmatrix} 1 & -1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & -1 & 0 \\\\ 0 & 0 & 0 & 1 & -1 \\end{pmatrix} $$\nThe constraint $\\Omega_{\\Lambda} x = 0$ corresponds to the system of linear equations:\n$$ \\begin{cases} x_{1} - x_{2} = 0 \\\\ x_{3} - x_{4} = 0 \\\\ x_{4} - x_{5} = 0 \\end{cases} $$\n\n**1. Derivation of Optimality Conditions and Solution**\n\nTo solve this constrained optimization problem, we form the Lagrangian $\\mathcal{L}(x, \\nu)$, where $\\nu \\in \\mathbb{R}^{3}$ is the vector of Lagrange multipliers. The objective function is $f(x) = \\|A x - y\\|_{2}^{2} = (Ax-y)^T(Ax-y)$.\n$$ \\mathcal{L}(x, \\nu) = \\|A x - y\\|_{2}^{2} + \\nu^T (\\Omega_{\\Lambda} x) $$\nThe Karush-Kuhn-Tucker (KKT) conditions for optimality are obtained by setting the gradients of the Lagrangian with respect to $x$ and $\\nu$ to zero.\n\nThe gradient with respect to $x$ is:\n$$ \\nabla_{x} \\mathcal{L}(x, \\nu) = \\nabla_{x} (x^T A^T A x - 2y^T A x + y^T y) + \\nabla_{x} (\\nu^T \\Omega_{\\Lambda} x) = 2A^T A x - 2A^T y + \\Omega_{\\Lambda}^T \\nu $$\nSetting this to zero gives the stationarity condition:\n$$ 2A^T A x + \\Omega_{\\Lambda}^T \\nu = 2A^T y $$\nThe gradient with respect to $\\nu$ gives the primal feasibility condition, which is simply the original constraint:\n$$ \\nabla_{\\nu} \\mathcal{L}(x, \\nu) = \\Omega_{\\Lambda} x = 0 $$\nThese two conditions form a system of linear equations in $x$ and $\\nu$, known as the KKT system:\n$$ \\begin{pmatrix} 2A^T A & \\Omega_{\\Lambda}^T \\\\ \\Omega_{\\Lambda} & 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ \\nu \\end{pmatrix} = \\begin{pmatrix} 2A^T y \\\\ 0 \\end{pmatrix} $$\nThis constitutes the derivation from first principles as requested.\n\nTo solve for $x^{\\star}$, we can directly use the constraints to simplify the problem. The constraints imply $x_1 = x_2$ and $x_3 = x_4 = x_5$. This means the solution vector $x^{\\star}$ lies in the subspace spanned by the vectors $(1, 1, 0, 0, 0)^T$ and $(0, 0, 1, 1, 1)^T$. Any vector $x$ in this subspace can be parameterized as $x = (c_1, c_1, c_2, c_2, c_2)^T$ for some scalars $c_1, c_2 \\in \\mathbb{R}$.\n\nSubstituting this form of $x$ into the expression $Ax$:\n$$ Ax = \\begin{pmatrix} 1 & 0 & 1 & 0 & 0 \\\\ 0 & 1 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} c_1 \\\\ c_1 \\\\ c_2 \\\\ c_2 \\\\ c_2 \\end{pmatrix} = \\begin{pmatrix} c_1 + c_2 \\\\ c_1 + c_2 \\\\ c_2 \\end{pmatrix} $$\nThe objective function becomes a function of $c_1$ and $c_2$:\n$$ \\|Ax - y\\|_{2}^{2} = \\left\\| \\begin{pmatrix} c_1 + c_2 \\\\ c_1 + c_2 \\\\ c_2 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 2 \\\\ 3 \\end{pmatrix} \\right\\|_{2}^{2} = 2(c_1 + c_2 - 2)^2 + (c_2 - 3)^2 $$\nThis is an unconstrained quadratic minimization problem in $c_1$ and $c_2$. We find the minimum by setting the partial derivatives to zero:\n$$ \\frac{\\partial}{\\partial c_1} [2(c_1 + c_2 - 2)^2 + (c_2 - 3)^2] = 4(c_1 + c_2 - 2) = 0 $$\n$$ \\frac{\\partial}{\\partial c_2} [2(c_1 + c_2 - 2)^2 + (c_2 - 3)^2] = 4(c_1 + c_2 - 2) + 2(c_2 - 3) = 0 $$\nFrom the first equation, we get $c_1 + c_2 - 2 = 0$. Substituting this into the second equation gives:\n$$ 4(0) + 2(c_2 - 3) = 0 \\implies c_2 = 3 $$\nSubstituting $c_2 = 3$ back into $c_1 + c_2 - 2 = 0$ gives $c_1 + 3 - 2 = 0$, which implies $c_1 = -1$.\n\nThus, the optimal coefficients are $c_1 = -1$ and $c_2 = 3$. The oracle estimator is:\n$$ x^{\\star} = \\begin{pmatrix} c_1 \\\\ c_1 \\\\ c_2 \\\\ c_2 \\\\ c_2 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -1 \\\\ 3 \\\\ 3 \\\\ 3 \\end{pmatrix} $$\n\n**2. Verification of Uniqueness**\n\nThe uniqueness of the solution $x^{\\star}$ is guaranteed if the matrix formed by vertically stacking $A$ and $\\Omega_{\\Lambda}$ has full column rank. Let this matrix be $M$:\n$$ M = \\begin{pmatrix} A \\\\ \\Omega_{\\Lambda} \\end{pmatrix} = \\begin{pmatrix}\n1 & 0 & 1 & 0 & 0 \\\\\n0 & 1 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1 \\\\\n1 & -1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & -1 & 0 \\\\\n0 & 0 & 0 & 1 & -1\n\\end{pmatrix} $$\nThis is a $6 \\times 5$ matrix. It has full column rank if its rank is $5$, which is equivalent to its null space being trivial, i.e., $\\text{ker}(M) = \\{0\\}$. Let's solve $Mx=0$ for $x \\in \\mathbb{R}^5$:\n1. $x_1 + x_3 = 0$\n2. $x_2 + x_4 = 0$\n3. $x_5 = 0$\n4. $x_1 - x_2 = 0 \\implies x_1 = x_2$\n5. $x_3 - x_4 = 0 \\implies x_3 = x_4$\n6. $x_4 - x_5 = 0 \\implies x_4 = x_5$\n\nFrom $(3)$, $x_5 = 0$. Using $(6)$, we find $x_4 = 0$. Using $(5)$, we find $x_3 = 0$. Using $(2)$, we have $x_2 + 0 = 0$, so $x_2 = 0$. Finally, using $(4)$, we get $x_1 = 0$.\nThe only solution to $Mx=0$ is $x=(0,0,0,0,0)^T$. Thus, $\\text{ker}(M)=\\{0\\}$ and $\\text{rank}(M) = 5$.\nThis condition is equivalent to the intersection of the null spaces of $A$ and $\\Omega_{\\Lambda}$ being trivial: $\\text{ker}(A) \\cap \\text{ker}(\\Omega_{\\Lambda}) = \\{0\\}$. This ensures that there is at most one vector $x$ satisfying both the measurement model and the co-sparsity constraints, guaranteeing a unique solution to the oracle problem.\n\n**3. Final Answer**\n\nThe oracle estimator is $x^{\\star} = (-1, -1, 3, 3, 3)^T$. The third component is $x_3^{\\star}$.\n$$ x_3^{\\star} = 3 $$", "answer": "$$\\boxed{3}$$", "id": "3486309"}, {"introduction": "While the oracle estimator assumes a single, known cosupport, real signals can exhibit structural ambiguity. This exercise [@problem_id:3486316] presents a scenario where a signal is perfectly annihilated by multiple, distinct sets of analysis vectors. By quantifying this \"cosupport ambiguity,\" you will gain insight into a fundamental challenge for recovery algorithms that must infer a signal's structure from measurements.", "problem": "Consider the co-sparsity (analysis) model in compressed sensing and sparse optimization, where an analysis operator $\\Omega \\in \\mathbb{R}^{p \\times n}$ induces a cosupport $\\Lambda \\subset \\{1,\\dots,p\\}$ via the vanishing constraints $\\Omega_{\\Lambda} x = 0$ for a signal $x \\in \\mathbb{R}^{n}$. The associated analysis subspace is $\\mathcal{U}_{\\Lambda} \\triangleq \\{x \\in \\mathbb{R}^{n} : \\Omega_{\\Lambda} x = 0\\}$. A signal that lies in the intersection $\\mathcal{U}_{\\Lambda_{1}} \\cap \\mathcal{U}_{\\Lambda_{2}}$ for distinct cosupports $\\Lambda_{1} \\neq \\Lambda_{2}$ exhibits an intrinsic ambiguity of cosupport.\n\nWork in $\\mathbb{R}^{4}$ with the analysis operator $\\Omega \\in \\mathbb{R}^{5 \\times 4}$ whose rows are\n$$\nr_{1} = \\begin{bmatrix}1 & 0 & 0 & 0\\end{bmatrix},\\quad\nr_{2} = \\begin{bmatrix}0 & 1 & 0 & 0\\end{bmatrix},\\quad\nr_{3} = \\begin{bmatrix}0 & 1 & 1 & 0\\end{bmatrix},\\quad\nr_{4} = \\begin{bmatrix}1 & 0 & -1 & 0\\end{bmatrix},\\quad\nr_{5} = \\begin{bmatrix}0 & 0 & 0 & 1\\end{bmatrix},\n$$\nso that\n$$\n\\Omega \\triangleq \\begin{bmatrix}\n1 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0\\\\\n0 & 1 & 1 & 0\\\\\n1 & 0 & -1 & 0\\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}.\n$$\nLet $x \\in \\mathbb{R}^{4}$ be the vector $x = \\begin{bmatrix}0 & 0 & 0 & 1\\end{bmatrix}^{\\top}$. Define the two cosupports $\\Lambda_{1} \\triangleq \\{1,2\\}$ and $\\Lambda_{2} \\triangleq \\{3,4\\}$. It is required that you reason from the definitions of the co-sparsity model and linear algebraic subspaces, without invoking any specialized identification theorems.\n\nDefine the size-$\\ell$ ambiguity index for $x$ as\n$$\nA_{\\ell}(x) \\triangleq \\left|\\left\\{\\Lambda \\subset \\{1,\\dots,5\\} : |\\Lambda| = \\ell,\\ \\Omega_{\\Lambda} x = 0\\right\\}\\right|,\n$$\nnamely, the number of distinct cosupports of cardinality $\\ell$ that annihilate $x$.\n\nUsing only fundamental definitions and linear algebra, determine the value of $A_{2}(x)$ for the $\\Omega$ and $x$ given above. In your reasoning, interpret how the existence of two distinct subspaces $\\mathcal{U}_{\\Lambda_{1}}$ and $\\mathcal{U}_{\\Lambda_{2}}$ with nontrivial intersection containing $x$ induces ambiguity for algorithms that explicitly estimate a cosupport by selecting rows of $\\Omega$ whose responses on $x$ vanish. Your final output must be a single integer. No rounding is required and no units are involved.", "solution": "The problem asks for the determination of the size-$2$ ambiguity index, denoted $A_{2}(x)$, for a specific signal $x \\in \\mathbb{R}^{4}$ and analysis operator $\\Omega \\in \\mathbb{R}^{5 \\times 4}$. The analysis operator $\\Omega$ consists of $p=5$ rows, denoted $r_i$ for $i \\in \\{1, 2, 3, 4, 5\\}$, and acts on signals in an $n=4$ dimensional space.\n\nThe size-$\\ell$ ambiguity index $A_{\\ell}(x)$ is defined as the number of distinct cosupports of cardinality $\\ell$ that annihilate the signal $x$. Formally,\n$$\nA_{\\ell}(x) \\triangleq \\left|\\left\\{\\Lambda \\subset \\{1,\\dots,p\\} : |\\Lambda| = \\ell,\\ \\Omega_{\\Lambda} x = 0\\right\\}\\right|\n$$\nFor this problem, we are interested in $\\ell=2$ and $p=5$. Thus, we need to find the value of:\n$$\nA_{2}(x) \\triangleq \\left|\\left\\{\\Lambda \\subset \\{1,\\dots,5\\} : |\\Lambda| = 2,\\ \\Omega_{\\Lambda} x = 0\\right\\}\\right|\n$$\nThe condition $\\Omega_{\\Lambda} x = 0$ for a cosupport $\\Lambda$ is equivalent to the condition that for every row index $i \\in \\Lambda$, the dot product of the $i$-th row of $\\Omega$, $r_i$, with the vector $x$ is zero. That is, $r_i x = 0$ for all $i \\in \\Lambda$.\n\nOur first step is to identify the full set of row indices that annihilate the given signal $x = \\begin{bmatrix}0 & 0 & 0 & 1\\end{bmatrix}^{\\top}$. Let this set be denoted by $S_{x} \\triangleq \\{i \\in \\{1, \\dots, 5\\} : r_i x = 0\\}$. We compute the dot product $r_i x$ for each row of the given matrix $\\Omega$:\n$$\n\\Omega = \\begin{bmatrix}\n1 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0\\\\\n0 & 1 & 1 & 0\\\\\n1 & 0 & -1 & 0\\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nr_1 \\\\ r_2 \\\\ r_3 \\\\ r_4 \\\\ r_5\n\\end{bmatrix}\n$$\n\nThe dot products are:\n$r_1 x = \\begin{bmatrix}1 & 0 & 0 & 0\\end{bmatrix} \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1\\end{bmatrix} = (1)(0) + (0)(0) + (0)(0) + (0)(1) = 0$.\n$r_2 x = \\begin{bmatrix}0 & 1 & 0 & 0\\end{bmatrix} \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1\\end{bmatrix} = (0)(0) + (1)(0) + (0)(0) + (0)(1) = 0$.\n$r_3 x = \\begin{bmatrix}0 & 1 & 1 & 0\\end{bmatrix} \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1\\end{bmatrix} = (0)(0) + (1)(0) + (1)(0) + (0)(1) = 0$.\n$r_4 x = \\begin{bmatrix}1 & 0 & -1 & 0\\end{bmatrix} \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1\\end{bmatrix} = (1)(0) + (0)(0) + (-1)(0) + (0)(1) = 0$.\n$r_5 x = \\begin{bmatrix}0 & 0 & 0 & 1\\end{bmatrix} \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1\\end{bmatrix} = (0)(0) + (0)(0) + (0)(0) + (1)(1) = 1$.\n\nFrom these calculations, we see that the rows that annihilate $x$ are $r_1, r_2, r_3,$ and $r_4$. The fifth row, $r_5$, does not. Therefore, the set of annihilating row indices is $S_{x} = \\{1, 2, 3, 4\\}$.\n\nTo find $A_{2}(x)$, we need to count the number of subsets $\\Lambda \\subset \\{1, \\dots, 5\\}$ of size $|\\Lambda|=2$ such that $\\Omega_{\\Lambda} x = 0$. This condition is satisfied if and only if both indices in $\\Lambda$ are from the set $S_x$. In other words, $\\Lambda$ must be a $2$-element subset of $S_{x}$.\n\nThe set $S_{x}$ is $\\{1, 2, 3, 4\\}$, and its cardinality is $|S_{x}| = 4$. We need to find the number of distinct subsets of $S_{x}$ of size $2$. This is a combinatorial problem solved by the binomial coefficient \"4 choose 2\":\n$$\nA_{2}(x) = \\binom{|S_x|}{2} = \\binom{4}{2} = \\frac{4!}{2!(4-2)!} = \\frac{4!}{2!2!} = \\frac{4 \\times 3}{2 \\times 1} = 6\n$$\nThe six distinct cosupports of size $2$ are: $\\{1,2\\}, \\{1,3\\}, \\{1,4\\}, \\{2,3\\}, \\{2,4\\},$ and $\\{3,4\\}$.\n\nThe problem statement gives two example cosupports, $\\Lambda_1=\\{1,2\\}$ and $\\Lambda_2=\\{3,4\\}$. Our calculation confirms that these are indeed valid cosupports for the signal $x$. The existence of more than one such cosupport (in this case, six) illustrates the concept of cosupport ambiguity. The signal $x$ lies in the intersection of multiple distinct analysis subspaces, e.g., $x \\in \\mathcal{U}_{\\Lambda_1} \\cap \\mathcal{U}_{\\Lambda_2}$. If an algorithm attempts to estimate the cosupport by observing which rows of $\\Omega$ annihilate $x$, it would find that the rows indexed by $\\{1,2,3,4\\}$ all yield a zero response. If the algorithm is designed to find a cosupport of size $2$, it is faced with $6$ equally valid choices, with no further information to distinguish between them. This ambiguity, quantified by $A_2(x)=6$, is a fundamental challenge in the co-sparsity model, indicating that the signal $x$ is not uniquely identifiable by a single cosupport of size $2$.", "answer": "$$\\boxed{6}$$", "id": "3486316"}, {"introduction": "Having explored the ideal case and its theoretical challenges, we now turn to a powerful practical method for signal recovery: regularization. This problem [@problem_id:3486321] focuses on Total Variation (TV) minimization, a widely used technique that promotes co-sparsity by penalizing the $\\ell_1$-norm of the signal's differences. By working through the KKT conditions using subgradient calculus, you will connect the abstract co-sparsity model to a concrete, solvable optimization problem used in numerous real-world applications.", "problem": "Consider the analysis co-sparsity formulation in compressed sensing, where one seeks a signal $x \\in \\mathbb{R}^{n}$ whose analysis coefficients $B x$ are sparse with respect to a fixed linear operator $B$. In one dimension, a common choice is the first-order difference operator, which induces piecewise-constant solutions through Total Variation (TV) regularization. Given a measurement operator $A \\in \\mathbb{R}^{m \\times n}$ and observed data $y \\in \\mathbb{R}^{m}$, the TV-regularized estimator is defined as the unique minimizer of the convex objective\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|B x\\|_{1},\n$$\nwhere $\\lambda > 0$ is a regularization parameter and $B \\in \\mathbb{R}^{(n-1) \\times n}$ is the one-dimensional forward difference matrix.\n\nWork with the specific case $n=5$, $A = I_{5}$, $\\lambda = 2$, and\n$$\ny = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\\\ 1 \\\\ 0 \\end{pmatrix}.\n$$\nLet $B \\in \\mathbb{R}^{4 \\times 5}$ be the 1D difference matrix whose rows are $(-1, 1, 0, 0, 0)$, $(0, -1, 1, 0, 0)$, $(0, 0, -1, 1, 0)$, and $(0, 0, 0, -1, 1)$, so that $(B x)_{i} = x_{i+1} - x_{i}$ for $i=1,2,3,4$.\n\nStarting from first principles of convex optimization and the definition of subgradients, derive the Karush–Kuhn–Tucker (KKT) conditions for this problem, write them explicitly for the given $n=5$ instance, and use them to compute the TV-regularized estimator $\\widehat{x}$. Express the final estimator as an exact row vector with rational entries. No rounding is required, and no units are involved.", "solution": "The user-provided problem is a valid optimization problem grounded in the principles of compressed sensing and convex optimization. All parameters are well-defined, and the problem is well-posed.\n\nThe problem is to find the unique minimizer $\\widehat{x}$ of the objective function:\n$$\nF(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|B x\\|_{1}\n$$\nGiven the parameters $n=5$, $A=I_5$, $\\lambda=2$, and $y = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\\\ 1 \\\\ 0 \\end{pmatrix}$, the objective function becomes:\n$$\nF(x) = \\frac{1}{2}\\|x - y\\|_{2}^{2} + 2 \\|B x\\|_{1}\n$$\nwhere $x \\in \\mathbb{R}^{5}$ and $B \\in \\mathbb{R}^{4 \\times 5}$ is the first-order forward difference matrix. The term $\\frac{1}{2}\\|x - y\\|_{2}^{2}$ is strictly convex and differentiable. The term $2\\|Bx\\|_{1}$ is convex but not differentiable everywhere. The sum $F(x)$ is therefore strictly convex, which guarantees the existence of a unique minimizer $\\widehat{x}$.\n\nThe first-order necessary and sufficient condition for a point $\\widehat{x}$ to be the minimizer of $F(x)$ is that the zero vector must be in the subdifferential of $F(x)$ at $\\widehat{x}$:\n$$\n0 \\in \\partial F(\\widehat{x})\n$$\nThe subdifferential of a sum of two convex functions is the sum of their subdifferentials (by the Moreau-Rockafellar theorem). The first term is differentiable, so its subdifferential is just its gradient.\n$$\n\\partial F(x) = \\nabla \\left(\\frac{1}{2}\\|x - y\\|_{2}^{2}\\right) + \\partial \\left(2\\|B x\\|_{1}\\right)\n$$\nThe gradient of the first term is $\\nabla \\left(\\frac{1}{2}\\|x-y\\|_2^2\\right) = x-y$.\nFor the second term, we use the chain rule for subdifferentials. If $h(x) = g(L(x))$ where $g(z) = 2\\|z\\|_1$ and $L(x) = Bx$, then $\\partial h(x) = L^T \\partial g(L(x))$. Here, $L^T = B^T$.\nThe subdifferential of $g(z) = 2\\|z\\|_1$ is a set of vectors $v \\in \\mathbb{R}^{4}$ such that for each component $v_i$:\n\\begin{itemize}\n    \\item $v_i = 2 \\cdot \\text{sign}(z_i)$ if $z_i \\neq 0$\n    \\item $v_i \\in [-2, 2]$ if $z_i = 0$\n\\end{itemize}\nwhere $z=B\\widehat{x}$. Combining these, the optimality condition $0 \\in \\partial F(\\widehat{x})$ becomes:\n$$\n0 \\in (\\widehat{x} - y) + B^T v\n$$\nwhere $v$ is a vector in $\\mathbb{R}^4$ whose components $v_i$ satisfy the conditions above with $z=B\\widehat{x}$. This can be written as:\n$$\ny - \\widehat{x} = B^T v\n$$\nThese are the Karush–Kuhn–Tucker (KKT) conditions for this problem. Let's write them explicitly for $n=5$. The matrix $B^T$ is:\n$$\nB^T = \\begin{pmatrix} -1 & 0 & 0 & 0 \\\\ 1 & -1 & 0 & 0 \\\\ 0 & 1 & -1 & 0 \\\\ 0 & 0 & 1 & -1 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix}\n$$\nThe equation $y - \\widehat{x} = B^T v$ component-wise is:\n\\begin{align*}\ny_1 - \\widehat{x}_1 &= -v_1 \\\\\ny_2 - \\widehat{x}_2 &= v_1 - v_2 \\\\\ny_3 - \\widehat{x}_3 &= v_2 - v_3 \\\\\ny_4 - \\widehat{x}_4 &= v_3 - v_4 \\\\\ny_5 - \\widehat{x}_5 &= v_4\n\\end{align*}\nA key property of $B^T$ is that the sum of its columns is the zero vector, which implies that $\\mathbf{1}^T B^T = \\mathbf{0}^T$. Therefore, left-multiplying the KKT equation by $\\mathbf{1}^T$:\n$$\n\\mathbf{1}^T (y - \\widehat{x}) = \\mathbf{1}^T (B^T v) = (\\mathbf{1}^T B^T) v = \\mathbf{0}^T v = 0\n$$\nThis gives an important condition on the solution $\\widehat{x}$:\n$$\n\\sum_{i=1}^{5} (y_i - \\widehat{x}_i) = 0 \\implies \\sum_{i=1}^{5} \\widehat{x}_i = \\sum_{i=1}^{5} y_i\n$$\nThe sum of the components of the given data vector $y$ is $\\sum y_i = 1+2+2+1+0 = 6$. Thus, we must have $\\sum \\widehat{x}_i = 6$.\n\nThe nature of Total Variation regularization suggests that the solution $\\widehat{x}$ is piecewise constant. A simple hypothesis is that the solution is fully constant, i.e., $\\widehat{x}_i = c$ for all $i=1, \\dots, 5$.\nIf $\\widehat{x} = c \\cdot \\mathbf{1}$, then $\\sum \\widehat{x}_i = 5c$. From the condition above, $5c = 6$, which gives $c = \\frac{6}{5}$.\nSo, let's test the candidate solution $\\widehat{x} = (\\frac{6}{5}, \\frac{6}{5}, \\frac{6}{5}, \\frac{6}{5}, \\frac{6}{5})^T$.\n\nFor this candidate, all differences are zero: $(B\\widehat{x})_i = \\widehat{x}_{i+1} - \\widehat{x}_i = 0$ for $i=1,2,3,4$.\nAccording to the KKT conditions, this requires the existence of a vector $v \\in \\mathbb{R}^4$ such that $y - \\widehat{x} = B^T v$ and its components satisfy $|v_i| \\le \\lambda=2$ for all $i$.\n\nLet's compute the residual vector $r = y - \\widehat{x}$:\n$$\nr = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 6/5 \\\\ 6/5 \\\\ 6/5 \\\\ 6/5 \\\\ 6/5 \\end{pmatrix} = \\begin{pmatrix} 1 - 6/5 \\\\ 2 - 6/5 \\\\ 2 - 6/5 \\\\ 1 - 6/5 \\\\ 0 - 6/5 \\end{pmatrix} = \\begin{pmatrix} -1/5 \\\\ 4/5 \\\\ 4/5 \\\\ -1/5 \\\\ -6/5 \\end{pmatrix}\n$$\nNow we must solve for $v$ from the system $r = B^T v$:\n\\begin{align*}\n-v_1 &= r_1 = -1/5 \\implies v_1 = 1/5 \\\\\nv_1 - v_2 &= r_2 = 4/5 \\implies 1/5 - v_2 = 4/5 \\implies v_2 = -3/5 \\\\\nv_2 - v_3 &= r_3 = 4/5 \\implies -3/5 - v_3 = 4/5 \\implies v_3 = -7/5 \\\\\nv_3 - v_4 &= r_4 = -1/5 \\implies -7/5 - v_4 = -1/5 \\implies v_4 = -6/5 \\\\\nv_4 &= r_5 = -6/5\n\\end{align*}\nThe system is consistent and yields a unique solution for $v$:\n$$\nv = \\begin{pmatrix} 1/5 \\\\ -3/5 \\\\ -7/5 \\\\ -6/5 \\end{pmatrix}\n$$\nThe final step is to check if this vector $v$ satisfies the subgradient condition, which for this case is $|v_i| \\le 2$ for all $i=1,2,3,4$.\n\\begin{itemize}\n    \\item $|v_1| = |1/5| = 1/5 \\le 2$\n    \\item $|v_2| = |-3/5| = 3/5 \\le 2$\n    \\item $|v_3| = |-7/5| = 7/5 \\le 2$\n    \\item $|v_4| = |-6/5| = 6/5 \\le 2$\n\\end{itemize}\nAll conditions are satisfied. Since we have found a pair $(\\widehat{x}, v)$ that satisfies the KKT conditions for this strictly convex problem, we have found the unique minimizer.\n\nThe TV-regularized estimator is $\\widehat{x} = (\\frac{6}{5}, \\frac{6}{5}, \\frac{6}{5}, \\frac{6}{5}, \\frac{6}{5})^T$. The problem asks for the answer as an exact row vector with rational entries.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{6}{5} & \\frac{6}{5} & \\frac{6}{5} & \\frac{6}{5} & \\frac{6}{5} \\end{pmatrix}}\n$$", "id": "3486321"}]}