{"hands_on_practices": [{"introduction": "The central mechanism behind Sparse Bayesian Learning is \"Automatic Relevance Determination,\" a process where the model itself prunes irrelevant features. This practice delves into the mathematical heart of this mechanism by analyzing how the model evidence changes when a new basis function is considered for inclusion [@problem_id:3433874]. By deriving a decision criterion based on the 'quality' and 'sparsity' factors, you will gain a fundamental understanding of how the RVM automatically achieves sparsity by rewarding only those features that genuinely improve the model's ability to explain the data.", "problem": "Consider the Bayesian linear regression model used in Sparse Bayesian Learning (SBL) and the Relevance Vector Machine (RVM). Let $y \\in \\mathbb{R}^{N}$ be generated by $y = \\Phi w + \\varepsilon$, where $\\Phi \\in \\mathbb{R}^{N \\times M}$ is a fixed design matrix, $w \\in \\mathbb{R}^{M}$ is a weight vector with an Automatic Relevance Determination (ARD) Gaussian prior $p(w \\mid \\alpha) = \\prod_{j=1}^{M} \\mathcal{N}(w_{j} \\mid 0, \\alpha_{j}^{-1})$ with precisions $\\alpha_{j} > 0$, and $\\varepsilon \\sim \\mathcal{N}(0, \\beta^{-1} I_{N})$ with noise precision $\\beta > 0$. The marginal likelihood (evidence) has the Gaussian form $p(y \\mid \\alpha, \\beta) = \\mathcal{N}(y \\mid 0, C)$ with $C = \\beta^{-1} I_{N} + \\Phi A^{-1} \\Phi^{\\top}$ and $A = \\mathrm{diag}(\\alpha_{1}, \\dots, \\alpha_{M})$. Suppose that, for an index $i$, the $i$-th basis (column) is currently excluded, meaning $\\alpha_{i} = \\infty$, and define $C_{-i} = \\beta^{-1} I_{N} + \\sum_{j \\neq i} \\alpha_{j}^{-1} \\phi_{j} \\phi_{j}^{\\top}$, where $\\phi_{j}$ is the $j$-th column of $\\Phi$.\n\nDefine the sparsity factor $s_{i}$ and the quality factor $q_{i}$ by\n$$\ns_{i} \\triangleq \\phi_{i}^{\\top} C_{-i}^{-1} \\phi_{i}, \n\\qquad\nq_{i} \\triangleq \\phi_{i}^{\\top} C_{-i}^{-1} y.\n$$\nYou will analyze the change in the log marginal likelihood (log-evidence) when the previously excluded $i$-th basis is (hypothetically) endowed with a finite precision $\\alpha_{i} \\in (0, \\infty)$ while keeping all other hyperparameters fixed.\n\nTasks:\n1. Starting from the Gaussian evidence $p(y \\mid \\alpha, \\beta) = \\mathcal{N}(y \\mid 0, C)$ and fundamental linear algebra identities for rank-one updates, derive an exact expression for the change in log-evidence\n$$\n\\Delta \\mathcal{L}(\\alpha_{i}) \\triangleq \\ln p(y \\mid \\alpha_{-i}, \\alpha_{i}, \\beta) - \\ln p(y \\mid \\alpha_{-i}, \\alpha_{i} = \\infty, \\beta),\n$$\nexpressed solely in terms of $s_{i}$, $q_{i}$, and $\\alpha_{i}$. Then, by analyzing the stationary point of $\\Delta \\mathcal{L}(\\alpha_{i})$ over $\\alpha_{i} \\in (0, \\infty)$, derive a necessary and sufficient condition on $s_{i}$ and $q_{i}$ under which adding the $i$-th basis with any finite $\\alpha_{i}$ strictly decreases the log-evidence relative to leaving it excluded.\n\n2. Validation via a synthetic test: Let $N = 3$, let the current active set be empty so that $C_{-i} = \\beta^{-1} I_{N}$, and take $\\beta = 1$. Consider a single candidate basis vector $\\phi_{i} = [1, 0, 0]^{\\top}$ and an observed data vector $y = [0.5, 0.2, -0.1]^{\\top}$. Using your formula and setting the trial precision to the self-sparsity value $\\alpha_{i} = s_{i}$, compute the numerical value of $\\Delta \\mathcal{L}(\\alpha_{i})$. Round your answer to four significant figures.\n\nYour final answer should be only the numerical value of $\\Delta \\mathcal{L}(\\alpha_{i})$ for the synthetic test in Task $2$ (unitless, in natural-log units), rounded as specified. Do not include any units or additional text in your final answer.", "solution": "We begin from the Bayesian linear regression evidence with an Automatic Relevance Determination (ARD) prior. The marginal likelihood is Gaussian with covariance\n$$\nC = \\beta^{-1} I_{N} + \\Phi A^{-1} \\Phi^{\\top},\n$$\nwhere $A = \\mathrm{diag}(\\alpha_{1}, \\dots, \\alpha_{M})$. The log marginal likelihood (log-evidence) is\n$$\n\\mathcal{L}(\\alpha, \\beta) = \\ln p(y \\mid \\alpha, \\beta) = -\\frac{1}{2}\\left( N \\ln (2\\pi) + \\ln |C| + y^{\\top} C^{-1} y \\right).\n$$\nWe consider the effect of including, or not, the $i$-th basis function. Let $\\alpha_{i} = \\infty$ denote exclusion, and consider a finite $\\alpha_{i} \\in (0, \\infty)$ for hypothetical inclusion, keeping all other hyperparameters fixed. Define\n$$\nC_{-i} \\triangleq \\beta^{-1} I_{N} + \\sum_{j \\neq i} \\alpha_{j}^{-1} \\phi_{j} \\phi_{j}^{\\top}.\n$$\nThen, when including the $i$-th basis at finite $\\alpha_{i}$, we have a rank-one update\n$$\nC = C_{-i} + \\alpha_{i}^{-1} \\phi_{i} \\phi_{i}^{\\top}.\n$$\nDefine the sparsity and quality factors\n$$\ns_{i} \\triangleq \\phi_{i}^{\\top} C_{-i}^{-1} \\phi_{i}, \n\\qquad\nq_{i} \\triangleq \\phi_{i}^{\\top} C_{-i}^{-1} y.\n$$\n\nTo obtain the change in log-evidence, we use two well-known linear algebra identities for rank-one updates: the matrix determinant lemma and the Sherman–Morrison–Woodbury (SMW) identity. The determinant lemma gives\n$$\n|C| = |C_{-i} + \\alpha_{i}^{-1} \\phi_{i} \\phi_{i}^{\\top}| \n= |C_{-i}| \\left( 1 + \\alpha_{i}^{-1} \\phi_{i}^{\\top} C_{-i}^{-1} \\phi_{i} \\right) \n= |C_{-i}| \\left( 1 + \\frac{s_{i}}{\\alpha_{i}} \\right),\n$$\nso\n$$\n\\ln |C| - \\ln |C_{-i}| = \\ln \\left( 1 + \\frac{s_{i}}{\\alpha_{i}} \\right).\n$$\nFor the inverse, the Sherman–Morrison–Woodbury identity yields\n$$\nC^{-1} = C_{-i}^{-1} - \\frac{C_{-i}^{-1} \\phi_{i} \\phi_{i}^{\\top} C_{-i}^{-1}}{\\alpha_{i} + s_{i}}.\n$$\nTherefore, the quadratic form satisfies\n$$\ny^{\\top} C^{-1} y = y^{\\top} C_{-i}^{-1} y - \\frac{(y^{\\top} C_{-i}^{-1} \\phi_{i})^{2}}{\\alpha_{i} + s_{i}}\n= y^{\\top} C_{-i}^{-1} y - \\frac{q_{i}^{2}}{\\alpha_{i} + s_{i}}.\n$$\nThus the change in log-evidence upon moving from $\\alpha_{i} = \\infty$ (excluded) to finite $\\alpha_{i}$ (included) while holding all else fixed is\n$$\n\\Delta \\mathcal{L}(\\alpha_{i}) \\triangleq \\mathcal{L}(\\alpha_{-i}, \\alpha_{i}, \\beta) - \\mathcal{L}(\\alpha_{-i}, \\alpha_{i} = \\infty, \\beta) \n= -\\frac{1}{2} \\left[ \\ln \\left( 1 + \\frac{s_{i}}{\\alpha_{i}} \\right) - \\frac{q_{i}^{2}}{\\alpha_{i} + s_{i}} \\right].\n$$\nEquivalently,\n$$\n\\Delta \\mathcal{L}(\\alpha_{i}) = \\frac{1}{2} \\left( \\frac{q_{i}^{2}}{\\alpha_{i} + s_{i}} - \\ln \\left( 1 + \\frac{s_{i}}{\\alpha_{i}} \\right) \\right).\n$$\n\nWe now analyze when adding the basis decreases the log-evidence for all finite $\\alpha_{i}$. Consider the stationary point by differentiating with respect to $\\alpha_{i}$:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\alpha_{i}} \\Delta \\mathcal{L}(\\alpha_{i}) \n= \\frac{1}{2} \\left( - \\frac{q_{i}^{2}}{(\\alpha_{i} + s_{i})^{2}} + \\frac{s_{i}}{\\alpha_{i}(\\alpha_{i} + s_{i})} \\right).\n$$\nSetting this derivative to zero and solving for $\\alpha_{i}$ gives\n$$\n- \\frac{q_{i}^{2}}{(\\alpha_{i} + s_{i})^{2}} + \\frac{s_{i}}{\\alpha_{i}(\\alpha_{i} + s_{i})} = 0\n\\quad \\Longleftrightarrow \\quad\n\\frac{s_{i}}{\\alpha_{i}} = \\frac{q_{i}^{2}}{\\alpha_{i} + s_{i}}\n\\quad \\Longleftrightarrow \\quad\n\\alpha_{i}^{\\star} = \\frac{s_{i}^{2}}{q_{i}^{2} - s_{i}}.\n$$\nA finite positive stationary point exists if and only if $q_{i}^{2} > s_{i}$. In that case, $\\alpha_{i}^{\\star} > 0$ yields a local maximum of $\\Delta \\mathcal{L}(\\alpha_{i})$. If $q_{i}^{2} \\leq s_{i}$, there is no positive stationary point, and $\\Delta \\mathcal{L}(\\alpha_{i})$ is strictly increasing in $\\alpha_{i}$ for $\\alpha_{i} \\in (0, \\infty)$ up to the limiting value\n$$\n\\lim_{\\alpha_{i} \\to \\infty} \\Delta \\mathcal{L}(\\alpha_{i}) \n= \\frac{1}{2} \\left( 0 - \\ln(1 + 0) \\right) = 0,\n$$\nimplying that for any finite $\\alpha_{i}$ we have $\\Delta \\mathcal{L}(\\alpha_{i}) < 0$. Therefore, the necessary and sufficient condition under which adding the $i$-th basis strictly decreases the log-evidence for all finite $\\alpha_{i}$ is\n$$\nq_{i}^{2} \\leq s_{i}.\n$$\n\nWe now validate this condition on the synthetic test. Take $N = 3$, $\\beta = 1$, $C_{-i} = \\beta^{-1} I_{N} = I_{N}$, $\\phi_{i} = [1, 0, 0]^{\\top}$, and $y = [0.5, 0.2, -0.1]^{\\top}$. Then\n$$\nC_{-i}^{-1} = I_{N}, \\quad s_{i} = \\phi_{i}^{\\top} C_{-i}^{-1} \\phi_{i} = \\phi_{i}^{\\top} \\phi_{i} = 1,\n$$\nand\n$$\nq_{i} = \\phi_{i}^{\\top} C_{-i}^{-1} y = \\phi_{i}^{\\top} y = 0.5.\n$$\nHence $q_{i}^{2} = 0.25 \\leq s_{i} = 1$, which predicts that any finite inclusion decreases the log-evidence. Following the instruction to take the trial precision $\\alpha_{i} = s_{i}$, we compute\n$$\n\\Delta \\mathcal{L}(\\alpha_{i} = s_{i}) \n= \\frac{1}{2} \\left( \\frac{q_{i}^{2}}{\\alpha_{i} + s_{i}} - \\ln \\left( 1 + \\frac{s_{i}}{\\alpha_{i}} \\right) \\right)\n= \\frac{1}{2} \\left( \\frac{0.25}{1 + 1} - \\ln(1 + 1) \\right)\n= \\frac{1}{2} \\left( \\frac{0.25}{2} - \\ln 2 \\right).\n$$\nThis simplifies to\n$$\n\\Delta \\mathcal{L}(\\alpha_{i} = s_{i}) = 0.0625 - \\frac{1}{2} \\ln 2.\n$$\nNumerically, using $\\ln 2 \\approx 0.6931471805599453$, we obtain\n$$\n\\Delta \\mathcal{L}(\\alpha_{i} = s_{i}) \\approx 0.0625 - 0.34657359027997264 \\approx -0.28407359027997264.\n$$\nRounded to four significant figures, the value is $-0.2841$.", "answer": "$$\\boxed{-0.2841}$$", "id": "3433874"}, {"introduction": "A complete Bayesian model must account not only for the signal but also for the noise. This exercise focuses on deriving the update rule for the noise variance, $\\sigma^2$, through the principle of evidence maximization [@problem_id:3433924]. You will uncover a key result in SBL that elegantly connects the noise estimate to the model's prediction error and its complexity, measured by the 'effective number of parameters.' This provides deep insight into the model's self-regularizing behavior and its ability to infer the appropriate level of data fidelity.", "problem": "Consider the linear Gaussian model used in Sparse Bayesian Learning (SBL) and the Relevance Vector Machine (RVM). Let $y \\in \\mathbb{R}^{n}$ be the measurement vector and $A \\in \\mathbb{R}^{n \\times m}$ be the design matrix with columns $\\{a_{i}\\}_{i=1}^{m}$. The unknown coefficient vector $x \\in \\mathbb{R}^{m}$ has a zero-mean Gaussian prior $x \\sim \\mathcal{N}(0,\\Gamma)$ with diagonal covariance $\\Gamma=\\mathrm{diag}(\\gamma_{1},\\ldots,\\gamma_{m})$. The likelihood is $y \\mid x,\\sigma^{2} \\sim \\mathcal{N}(A x, \\sigma^{2} I_{n})$, where $\\sigma^{2}>0$ is the noise variance. Denote by $\\mu \\in \\mathbb{R}^{m}$ and $\\Sigma \\in \\mathbb{R}^{m \\times m}$ the posterior mean and covariance of $x$ given $y$ under fixed $\\Gamma$ and $\\sigma^{2}$. Let the marginal covariance be $C=\\sigma^{2} I_{n}+A \\Gamma A^{\\top}$, and define the influence terms $s_{i}=a_{i}^{\\top} C^{-1} a_{i}$.\n\nStarting from fundamental definitions of the Gaussian likelihood, Gaussian prior, posterior conditioning identities, and the marginal likelihood under Gaussian integration, derive the fixed-point equation for the noise variance $\\sigma^{2}$ that results from evidence maximization (Type-II maximum likelihood) with $\\Gamma$ held fixed. Express the update entirely in terms of the residual norm $\\|y-A \\mu\\|_{2}^{2}$, the sample size $n$, and a sum of the basis-specific influence terms $\\{s_{i}\\}$ weighted by $\\{\\gamma_{i}\\}$. Establish the degrees-of-freedom interpretation by identifying a suitable “hat” matrix built from $(A,\\Sigma,\\sigma^{2})$ and relating its trace to $\\sum_{i} \\gamma_{i} s_{i}$.\n\nYour final answer must be a single closed-form analytical expression for $\\sigma^{2}$ in terms of $y$, $A$, $\\mu$, $\\{\\gamma_{i}\\}$, and $\\{s_{i}\\}$. No numerical rounding is required, and no units are needed.", "solution": "The problem asks for the derivation of the fixed-point update equation for the noise variance $\\sigma^2$ obtained by maximizing the marginal likelihood (evidence) in a Sparse Bayesian Learning (SBL) framework.\n\nFirst, we establish the probabilistic model as defined in the problem statement.\nThe likelihood of the measurements $y \\in \\mathbb{R}^n$ given the coefficients $x \\in \\mathbb{R}^m$ and noise variance $\\sigma^2$ is Gaussian:\n$$p(y|x, \\sigma^2) = \\mathcal{N}(y | Ax, \\sigma^2 I_n) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y-Ax\\|_2^2\\right)$$\nThe prior on the coefficients $x$ is a zero-mean Gaussian with a diagonal covariance matrix $\\Gamma = \\mathrm{diag}(\\gamma_1, \\ldots, \\gamma_m)$:\n$$p(x|\\Gamma) = \\mathcal{N}(x | 0, \\Gamma) = (2\\pi)^{-m/2} |\\Gamma|^{-1/2} \\exp\\left(-\\frac{1}{2}x^\\top \\Gamma^{-1} x\\right)$$\nThe goal is to find the value of $\\sigma^2$ that maximizes the marginal likelihood, or evidence, $p(y|\\Gamma, \\sigma^2)$, with $\\Gamma$ held fixed. This is known as Type-II Maximum Likelihood or evidence maximization.\n\nThe marginal likelihood is obtained by integrating out the coefficients $x$:\n$$p(y|\\Gamma, \\sigma^2) = \\int p(y|x, \\sigma^2) p(x|\\Gamma) dx$$\nSince both the likelihood and prior are Gaussian, the marginal distribution is also Gaussian. The mean is $E[y] = E[Ax] = A E[x] = 0$. The covariance is $\\mathrm{Cov}(y) = \\mathrm{Cov}(Ax+\\epsilon) = A \\mathrm{Cov}(x) A^\\top + \\mathrm{Cov}(\\epsilon) = A\\Gamma A^\\top + \\sigma^2 I_n$. This is the matrix $C$ defined in the problem.\nThus, $y \\sim \\mathcal{N}(0, C)$, where $C = \\sigma^2 I_n + A\\Gamma A^\\top$.\n\nThe log-marginal likelihood is:\n$$\\mathcal{L}(\\sigma^2, \\Gamma) = \\ln p(y|\\Gamma, \\sigma^2) = -\\frac{1}{2} y^\\top C^{-1} y - \\frac{1}{2} \\ln|C| - \\frac{n}{2}\\ln(2\\pi)$$\nMaximizing this with respect to $\\sigma^2$ can be done by setting its derivative to zero. However, a more structured approach, which yields the same result, is the Expectation-Maximization (EM) algorithm. In the context of SBL, we treat the coefficients $x$ as hidden variables. The M-step for updating $\\sigma^2$ involves maximizing the expectation of the complete-data log-likelihood with respect to the posterior distribution of $x$ computed with the current parameter estimates.\n\nLet the current estimate of the noise variance be $\\sigma^2_{old}$. The posterior distribution of $x$ is $p(x|y, \\Gamma, \\sigma^2_{old}) = \\mathcal{N}(x|\\mu, \\Sigma)$, where $\\mu$ and $\\Sigma$ are the posterior mean and covariance, respectively. The M-step requires maximizing the Q-function:\n$$Q(\\sigma^2) = E_{p(x|y, \\Gamma, \\sigma^2_{old})} [\\ln p(y,x | \\sigma^2, \\Gamma)]$$\n$$p(y,x | \\sigma^2, \\Gamma) = p(y|x, \\sigma^2) p(x|\\Gamma)$$\nSince $p(x|\\Gamma)$ does not depend on $\\sigma^2$, we only need to consider the likelihood term:\n$$Q(\\sigma^2) = E[\\ln p(y|x, \\sigma^2)] + \\text{const} = E\\left[-\\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\|y-Ax\\|_2^2\\right] + \\text{const}$$\n$$Q(\\sigma^2) = -\\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} E\\left[\\|y-Ax\\|_2^2\\right] + \\text{const}$$\nThe expectation is taken with respect to $p(x|y, \\Gamma, \\sigma^2_{old})$. We have $E[x]=\\mu$ and $E[(x-\\mu)(x-\\mu)^\\top] = \\Sigma$.\n$$E\\left[\\|y-Ax\\|_2^2\\right] = E\\left[(y-A\\mu-A(x-\\mu))^\\top(y-A\\mu-A(x-\\mu))\\right]$$\n$$= \\|y-A\\mu\\|_2^2 - 2(y-A\\mu)^\\top A E[x-\\mu] + E\\left[(x-\\mu)^\\top A^\\top A (x-\\mu)\\right]$$\nSince $E[x-\\mu]=0$, the cross-term vanishes. Using the trace identity for the quadratic form:\n$$E\\left[\\mathrm{Tr}((x-\\mu)^\\top A^\\top A (x-\\mu))\\right] = \\mathrm{Tr}\\left(A^\\top A E[(x-\\mu)(x-\\mu)^\\top]\\right) = \\mathrm{Tr}(A^\\top A \\Sigma)$$\nSo, $E\\left[\\|y-Ax\\|_2^2\\right] = \\|y-A\\mu\\|_2^2 + \\mathrm{Tr}(A^\\top A \\Sigma)$.\n\nSubstituting this back into the Q-function:\n$$Q(\\sigma^2) = -\\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\left(\\|y-A\\mu\\|_2^2 + \\mathrm{Tr}(A^\\top A \\Sigma)\\right)$$\nTo maximize $Q(\\sigma^2)$, we differentiate with respect to $\\sigma^2$ and set the result to zero:\n$$\\frac{\\partial Q}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\left(\\|y-A\\mu\\|_2^2 + \\mathrm{Tr}(A^\\top A \\Sigma)\\right) = 0$$\nMultiplying by $2(\\sigma^2)^2$ gives:\n$$-n\\sigma^2 + \\|y-A\\mu\\|_2^2 + \\mathrm{Tr}(A^\\top A \\Sigma) = 0$$\nSolving for $\\sigma^2$ gives the update rule:\n$$\\sigma^2 = \\frac{1}{n}\\left(\\|y-A\\mu\\|_2^2 + \\mathrm{Tr}(A^\\top A \\Sigma)\\right)$$\nThis expression must be related to the influence terms $s_i = a_i^\\top C^{-1} a_i$. To do this, we establish the degrees-of-freedom interpretation. The predicted output is $\\hat{y} = A\\mu$. The posterior mean is given by $\\mu = \\sigma^{-2}\\Sigma A^\\top y$. Thus, we can write $\\hat{y} = A(\\sigma^{-2}\\Sigma A^\\top)y$. We can define a \"hat\" matrix $H = \\sigma^{-2}A\\Sigma A^\\top$. The effective number of parameters, or degrees of freedom, consumed by the model is $\\delta = \\mathrm{Tr}(H)$.\n$$\\delta = \\mathrm{Tr}(\\sigma^{-2}A\\Sigma A^\\top) = \\sigma^{-2}\\mathrm{Tr}(A\\Sigma A^\\top) = \\sigma^{-2}\\mathrm{Tr}(\\Sigma A^\\top A)$$\nThis implies $\\mathrm{Tr}(A^\\top A \\Sigma) = \\sigma^2 \\delta$.\nSubstituting this into our equation for $\\sigma^2$:\n$$n\\sigma^2 = \\|y-A\\mu\\|_2^2 + \\sigma^2 \\delta \\quad\\implies\\quad \\sigma^2 = \\frac{\\|y-A\\mu\\|_2^2}{n-\\delta}$$\nNow we must relate $\\delta$ to the sum $\\sum_i \\gamma_i s_i$.\n$$\\delta = \\mathrm{Tr}(H) = \\mathrm{Tr}(\\sigma^{-2}A\\Sigma A^\\top)$$\nWe use the Woodbury matrix identity relating the marginal covariance inverse $C^{-1}$ to the posterior covariance $\\Sigma$:\n$$C^{-1} = (\\sigma^2 I_n + A\\Gamma A^\\top)^{-1} = \\frac{1}{\\sigma^2}I_n - \\frac{1}{\\sigma^4}A(\\Gamma^{-1}+\\frac{1}{\\sigma^2}A^\\top A)^{-1}A^\\top = \\frac{1}{\\sigma^2}I_n - \\frac{1}{\\sigma^4}A\\Sigma A^\\top$$\nRearranging this identity, we get $A\\Sigma A^\\top = \\sigma^2 I_n - \\sigma^4 C^{-1}$.\nNow we can compute the trace of the hat matrix:\n$$\\delta = \\mathrm{Tr}(H) = \\mathrm{Tr}(\\sigma^{-2}A\\Sigma A^\\top) = \\mathrm{Tr}(\\sigma^{-2}(\\sigma^2 I_n - \\sigma^4 C^{-1}))$$\n$$\\delta = \\mathrm{Tr}(I_n - \\sigma^2 C^{-1}) = \\mathrm{Tr}(I_n) - \\sigma^2 \\mathrm{Tr}(C^{-1}) = n - \\sigma^2 \\mathrm{Tr}(C^{-1})$$\nNext, we evaluate the term $\\sum_i \\gamma_i s_i$:\n$$\\sum_{i=1}^m \\gamma_i s_i = \\sum_{i=1}^m \\gamma_i (a_i^\\top C^{-1} a_i) = \\sum_{i=1}^m \\mathrm{Tr}(\\gamma_i a_i^\\top C^{-1} a_i) = \\sum_{i=1}^m \\mathrm{Tr}(C^{-1} a_i \\gamma_i a_i^\\top)$$\nUsing the linearity of the trace operator:\n$$\\sum_{i=1}^m \\gamma_i s_i = \\mathrm{Tr}\\left(C^{-1} \\sum_{i=1}^m \\gamma_i a_i a_i^\\top\\right)$$\nThe sum in the parenthesis is precisely $A\\Gamma A^\\top$.\n$$\\sum_{i=1}^m \\gamma_i s_i = \\mathrm{Tr}(C^{-1} (A\\Gamma A^\\top))$$\nUsing the definition $C = \\sigma^2 I_n + A\\Gamma A^\\top$, we have $A\\Gamma A^\\top = C - \\sigma^2 I_n$.\n$$\\sum_{i=1}^m \\gamma_i s_i = \\mathrm{Tr}(C^{-1} (C - \\sigma^2 I_n)) = \\mathrm{Tr}(I_n - \\sigma^2 C^{-1}) = n - \\sigma^2 \\mathrm{Tr}(C^{-1})$$\nComparing the two results, we find the identity:\n$$\\delta = n - \\sigma^2 \\mathrm{Tr}(C^{-1}) = \\sum_{i=1}^m \\gamma_i s_i$$\nSo, the effective number of parameters is $\\delta = \\sum_{i=1}^m \\gamma_i s_i$. Substituting this into our expression $\\sigma^2 = \\frac{\\|y-A\\mu\\|_2^2}{n-\\delta}$:\n$$\\sigma^2 = \\frac{\\|y-A\\mu\\|_2^2}{n - \\sum_{i=1}^{m} \\gamma_i s_i}$$\nThis is the desired fixed-point equation for $\\sigma^2$. It is a fixed-point equation because $\\sigma^2$ appears on the right-hand side implicitly, as the influence terms $s_i = a_i^\\top C^{-1} a_i$ depend on $C = \\sigma^2 I_n + A\\Gamma A^\\top$. The expression is entirely in terms of the required quantities.", "answer": "$$\\boxed{\\sigma^2 = \\frac{\\|y - A\\mu\\|_2^2}{n - \\sum_{i=1}^{m} \\gamma_i s_i}}$$", "id": "3433924"}, {"introduction": "Theoretical models are only as good as their practical implementations, especially in high-dimensional settings where computational cost and numerical stability are paramount. This hands-on coding challenge addresses the common scenario where the number of features far exceeds the number of observations ($p \\gg n$) [@problem_id:3433942]. You will use the Woodbury matrix identity and Cholesky factorization to develop an efficient algorithm that avoids prohibitive $p \\times p$ matrix inversions, making the powerful SBL framework feasible for real-world sparse recovery problems.", "problem": "Consider a standard linear Gaussian model used in Sparse Bayesian Learning (SBL) and the Relevance Vector Machine (RVM) for sparse recovery. Let $A \\in \\mathbb{R}^{n \\times p}$ with $n \\ll p$, an unknown coefficient vector $x \\in \\mathbb{R}^{p}$, and observations $y \\in \\mathbb{R}^{n}$ generated by $y = A x + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\beta^{-1} I_{n})$ with noise precision $\\beta > 0$. Assume the prior $x \\sim \\mathcal{N}(0, \\Gamma)$ where $\\Gamma = \\operatorname{diag}(\\tau_{1},\\dots,\\tau_{p}) \\succ 0$. The posterior distribution is Gaussian, $x \\mid y \\sim \\mathcal{N}(\\mu, \\Sigma)$, and the marginal likelihood (evidence) has covariance $C = \\beta^{-1} I_{n} + A \\Gamma A^{\\top}$.\n\nTasks:\n1) Starting only from the fundamental definitions of multivariate Gaussian conditioning and the matrix inversion lemma (Woodbury identity), derive low-rank expressions suitable for the regime $n \\ll p$:\n- Derive $\\Sigma$ in terms of $\\Gamma$, $A$, and $C^{-1}$ without inverting any $p \\times p$ matrix directly.\n- Derive $C^{-1}$ in terms of $\\beta$, $A$, and $\\Sigma$ or $\\Gamma$, avoiding $p \\times p$ inversions when possible.\nProvide expressions that make explicit use of the Woodbury identity and identify the matrix whose Cholesky factorization can be used to avoid explicit inversion.\n\n2) Using only fundamental properties of Cholesky factorization for symmetric positive definite matrices, derive numerically stable formulas to evaluate the evidence log-density\n$$\n\\log p(y \\mid \\beta, \\Gamma) = -\\tfrac{1}{2}\\left( n \\log(2\\pi) + \\log \\lvert C \\rvert + y^{\\top} C^{-1} y \\right),\n$$\nthat do not explicitly form $C^{-1}$. Your derivation should express $\\log \\lvert C \\rvert$ and $y^{\\top} C^{-1} y$ using triangular solves.\n\n3) Design an algorithm that, given $(A, y, \\tau, \\beta)$, computes:\n- $\\mu$ using a formula that requires only solves with $C$,\n- $\\Sigma$ using a low-rank expression based on solves with $C$,\n- $\\log p(y \\mid \\beta, \\Gamma)$ using the Cholesky factorization of $C$,\nand verifies consistency against direct dense computations on small problems by computing the following error metrics:\n- $e_{\\Sigma} = \\max\\limits_{i,j} \\lvert \\Sigma_{\\text{low-rank}}(i,j) - \\Sigma_{\\text{direct}}(i,j) \\rvert$,\n- $e_{C^{-1}} = \\max\\limits_{i,j} \\lvert C_{\\text{low-rank}}^{-1}(i,j) - C_{\\text{direct}}^{-1}(i,j) \\rvert$,\n- $e_{\\log \\text{ev}} = \\big\\lvert \\log p_{\\text{Cholesky}}(y \\mid \\beta, \\Gamma) - \\log p_{\\text{direct}}(y \\mid \\beta, \\Gamma) \\big\\rvert$,\n- $e_{\\mu} = \\lVert \\mu_{\\Gamma A^{\\top} C^{-1} y} - \\mu_{\\beta \\Sigma A^{\\top} y} \\rVert_{2}$.\nHere, $\\Sigma_{\\text{direct}}$ denotes the inverse of $\\Gamma^{-1} + \\beta A^{\\top} A$ computed directly on the $p \\times p$ system for small $p$; $C_{\\text{direct}}^{-1}$ denotes the inverse of $C$ computed directly on the $n \\times n$ system; and the evidence comparison contrasts a Cholesky-based computation with a direct dense approach.\n\nTest Suite to be implemented by your program:\nConstruct four test cases with $n \\ll p$ using deterministic, formula-defined data. For each case, $A$, $y$, $\\tau$, and $\\beta$ must be defined exactly as follows.\n\n- Case $1$: $n = 3$, $p = 8$.\n  - For $i \\in \\{0,1,2\\}$ and $j \\in \\{0,\\dots,7\\}$,\n    $$\n    A_{ij} = \\sin\\big((i+1)(j+1)\\big) + 0.1 \\cos\\big((i+1) + 2(j+1)\\big).\n    $$\n  - For $i \\in \\{0,1,2\\}$,\n    $$\n    y_{i} = \\cos(i+1) + 0.5 \\sin\\big(2(i+1)\\big).\n    $$\n  - For $j \\in \\{0,\\dots,7\\}$,\n    $$\n    \\tau_{j} = 0.5 + 0.1(j+1).\n    $$\n  - $\\beta = 25.0$.\n\n- Case $2$: $n = 3$, $p = 8$.\n  - Let $v = \\begin{bmatrix} 1.0 \\\\ -2.0 \\\\ 3.0 \\end{bmatrix}$, $\\epsilon = 10^{-3}$, and for each $j \\in \\{0,\\dots,7\\}$ define\n    $$\n    w_{j} = \\begin{bmatrix} \\sin(j+1) \\\\ \\cos(j+1) \\\\ \\sin\\big(2(j+1)\\big) \\end{bmatrix}, \\quad A_{:,j} = v + \\epsilon w_{j}.\n    $$\n  - $y = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n  - For $j \\in \\{0,\\dots,7\\}$,\n    $$\n    \\tau_{j} = 10^{-4 + 6 \\cdot \\frac{j}{7}}.\n    $$\n  - $\\beta = 1.0$.\n\n- Case $3$: $n = 4$, $p = 10$.\n  - For $i \\in \\{0,1,2,3\\}$ and $j \\in \\{0,\\dots,9\\}$,\n    $$\n    A_{ij} = \\sin\\big(0.3(i+1) + 0.7(j+1)\\big) + 0.05\\big((i+1) - (j+1)\\big).\n    $$\n  - For $i \\in \\{0,1,2,3\\}$,\n    $$\n    y_{i} = \\sin\\big(1.5(i+1)\\big).\n    $$\n  - For $j \\in \\{0,\\dots,9\\}$,\n    $$\n    \\tau_{j} = 0.2 + 0.05 j.\n    $$\n  - $\\beta = 1000.0$.\n\n- Case $4$: $n = 2$, $p = 6$.\n  - For $i \\in \\{0,1\\}$ and $j \\in \\{0,\\dots,5\\}$,\n    $$\n    A_{ij} = \\cos\\big( (i+1)(j+2) \\big) + 0.2 \\sin\\big( (i+2) + (j+1) \\big).\n    $$\n  - $y = \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix}$.\n  - For $j \\in \\{0,\\dots,5\\}$,\n    $$\n    \\tau_{j} = \\begin{cases}\n    10^{6}, & \\text{if } j+1 \\in \\{1,2\\},\\\\\n    10^{-6}, & \\text{if } j+1 = 3,\\\\\n    1.0, & \\text{otherwise.}\n    \\end{cases}\n    $$\n  - $\\beta = 10.0$.\n\nRequired output of your program:\n- For each case, compute the quadruple $\\big(e_{\\Sigma}, e_{C^{-1}}, e_{\\log \\text{ev}}, e_{\\mu}\\big)$ as defined above.\n- Aggregate all results into a single list in the order of cases $1$ through $4$, concatenating the quadruples in that order.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[r_{1}, r_{2}, \\dots, r_{16}]$ where each $r_{k}$ is a floating-point number.\n\nAll computations are purely mathematical and require no physical units. Angles inside trigonometric functions are in radians by convention. Ensure numerical stability by using Cholesky factorizations and triangular solves wherever applicable and by avoiding explicit matrix inverses when they are not strictly necessary for the small direct-comparison baselines in this test.", "solution": "We begin with the linear Gaussian model used in Sparse Bayesian Learning (SBL) and the Relevance Vector Machine (RVM): $y = A x + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\beta^{-1} I_{n})$, and a Gaussian prior $x \\sim \\mathcal{N}(0, \\Gamma)$ with $\\Gamma = \\operatorname{diag}(\\tau_{1},\\dots,\\tau_{p}) \\succ 0$. The marginal likelihood (evidence) of $y$ is Gaussian, $y \\sim \\mathcal{N}(0, C)$ with $C = \\beta^{-1} I_{n} + A \\Gamma A^{\\top}$, and the posterior $x \\mid y \\sim \\mathcal{N}(\\mu, \\Sigma)$.\n\nDerivation of low-rank expressions using the matrix inversion lemma:\nThe posterior covariance from Gaussian conditioning can be written as $\\Sigma = \\left(\\Gamma^{-1} + \\beta A^{\\top} A\\right)^{-1}$. To avoid inverting the $p \\times p$ matrix directly when $p$ is large, we use the matrix inversion lemma (Woodbury identity). The Woodbury identity states that for conformable matrices with appropriate inverses,\n$$\n\\left(U + B R B^{\\top}\\right)^{-1} = U^{-1} - U^{-1} B \\left( R^{-1} + B^{\\top} U^{-1} B \\right)^{-1} B^{\\top} U^{-1}.\n$$\nChoose $U = \\Gamma^{-1}$, $B = \\sqrt{\\beta} A^{\\top}$, and $R = I_{n}$. Then\n$$\n\\Sigma = \\left(\\Gamma^{-1} + \\beta A^{\\top} A\\right)^{-1} = \\Gamma - \\Gamma A^{\\top} \\left( \\beta^{-1} I_{n} + A \\Gamma A^{\\top} \\right)^{-1} A \\Gamma.\n$$\nDefining $C = \\beta^{-1} I_{n} + A \\Gamma A^{\\top}$ yields the low-rank form\n$$\n\\Sigma = \\Gamma - \\Gamma A^{\\top} C^{-1} A \\Gamma,\n$$\nwhich requires only solving $n \\times n$ systems with $C$.\n\nSimilarly, starting from $C = \\beta^{-1} I_{n} + A \\Gamma A^{\\top}$, applying the Woodbury identity with $U = \\beta^{-1} I_{n}$, $B = A$, $R = \\Gamma$ yields\n$$\nC^{-1} = \\beta I_{n} - \\beta^{2} A \\left( \\Gamma^{-1} + \\beta A^{\\top} A \\right)^{-1} A^{\\top} = \\beta I_{n} - \\beta^{2} A \\Sigma A^{\\top}.\n$$\nThus, $C^{-1}$ can be expressed using $\\Sigma$ without explicitly inverting any $n \\times n$ matrix beyond those needed for the Cholesky solves.\n\nPosterior mean equivalences:\nFrom Gaussian conditioning, the posterior mean satisfies\n$$\n\\mu = \\beta \\Sigma A^{\\top} y.\n$$\nUsing the low-rank expression for $\\Sigma$, we can also write\n$$\n\\mu = \\Gamma A^{\\top} C^{-1} y,\n$$\nwhich uses only solves with $C$ and matrix multiplications with $\\Gamma$.\n\nNumerically stable evaluation of the evidence with Cholesky factorization:\nFor a symmetric positive definite matrix $C$, the Cholesky factorization $C = L L^{\\top}$ with $L$ lower triangular guarantees:\n- The log-determinant can be evaluated as\n$$\n\\log \\lvert C \\rvert = 2 \\sum_{i=1}^{n} \\log L_{ii}.\n$$\n- The quadratic form can be evaluated by solving $L z = y$ and then $L^{\\top} w = z$, giving $w = C^{-1} y$, hence\n$$\ny^{\\top} C^{-1} y = \\lVert z \\rVert_{2}^{2}.\n$$\nCombining these, the log-evidence is\n$$\n\\log p(y \\mid \\beta, \\Gamma) = -\\tfrac{1}{2} \\left( n \\log(2\\pi) + 2 \\sum_{i=1}^{n} \\log L_{ii} + \\lVert z \\rVert_{2}^{2} \\right).\n$$\nThese expressions avoid forming $C^{-1}$ explicitly and are numerically stable.\n\nAlgorithm design for $n \\ll p$:\nGiven $A$, $y$, $\\tau$, and $\\beta$:\n- Form $\\Gamma = \\operatorname{diag}(\\tau)$ and $C = \\beta^{-1} I_{n} + A \\Gamma A^{\\top}$.\n- Compute the Cholesky factorization $C = L L^{\\top}$, adding a minimal diagonal jitter if necessary to ensure numerical positive definiteness.\n- Compute $\\mu$ stably:\n  - Solve $L z = y$ and $L^{\\top} w = z$ so that $w = C^{-1} y$.\n  - Compute $\\mu = \\Gamma A^{\\top} w$.\n  - For verification, also compute $\\mu' = \\beta \\Sigma A^{\\top} y$ once $\\Sigma$ is available (below), and compare $\\lVert \\mu - \\mu' \\rVert_{2}$.\n- Compute $\\Sigma$ in low-rank form:\n  - Form $AG = A \\Gamma$ and solve $L Y = AG$ and $L^{\\top} Z = Y$ so that $Z = C^{-1} A \\Gamma$.\n  - Compute $\\Sigma = \\Gamma - (\\Gamma A^{\\top}) Z$.\n- Compute $C^{-1}$ in low-rank form via $\\Sigma$:\n  - Compute $C^{-1}_{\\text{low-rank}} = \\beta I_{n} - \\beta^{2} A \\Sigma A^{\\top}$.\n- Compute the log-evidence using $L$ as above.\n- For direct baselines on small dimensions:\n  - Compute $\\Sigma_{\\text{direct}}$ by inverting $K = \\Gamma^{-1} + \\beta A^{\\top} A$ via its Cholesky factorization.\n  - Compute $C^{-1}_{\\text{direct}}$ by inverting $C$ via its Cholesky factorization.\n  - Compute the log-evidence directly using either the sign-log-determinant computation and a linear solve, or equivalently via the Cholesky factor again.\n- Report error metrics:\n  - $e_{\\Sigma} = \\max\\limits_{i,j} \\lvert \\Sigma - \\Sigma_{\\text{direct}} \\rvert$,\n  - $e_{C^{-1}} = \\max\\limits_{i,j} \\lvert C^{-1}_{\\text{low-rank}} - C^{-1}_{\\text{direct}} \\rvert$,\n  - $e_{\\log \\text{ev}} = \\lvert \\log p_{\\text{Cholesky}} - \\log p_{\\text{direct}} \\rvert$,\n  - $e_{\\mu} = \\lVert \\mu - \\beta \\Sigma_{\\text{direct}} A^{\\top} y \\rVert_{2}$.\nBecause the Cholesky-based and direct computations are mathematically equivalent, for well-conditioned cases the errors should be near machine precision (on the order of $10^{-12}$ to $10^{-9}$ in double precision), while ill-conditioned test cases may exhibit slightly larger but still small discrepancies due to numerical round-off.\n\nComplexity considerations:\n- Forming $C$ costs $\\mathcal{O}(n^{2} p)$ if done via $A \\Gamma A^{\\top}$. The dominant cost in the low-rank strategy is the Cholesky factorization of $C$ with cost $\\mathcal{O}(n^{3})$, and triangular solves with cost $\\mathcal{O}(n^{2} p)$.\n- No $p \\times p$ matrix inversion is required in the low-rank pathway; the direct $p \\times p$ inversion is used only for the small-scale verification mandated by the test suite.\n\nThe program implements the above algorithm for the four deterministic test cases specified, and outputs the concatenated list of the four error metrics for each case as a single line $[r_{1}, r_{2}, \\dots, r_{16}]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Execution environment: Python 3.12, numpy 1.23.5, scipy 1.11.4\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular\n\ndef chol_with_jitter(M, lower=True, max_tries=5, initial_jitter=0.0):\n    \"\"\"Compute a Cholesky factorization with increasing diagonal jitter if needed.\"\"\"\n    jitter = initial_jitter\n    for _ in range(max_tries):\n        try:\n            L = cholesky(M + jitter * np.eye(M.shape[0]), lower=lower, check_finite=False)\n            return L, jitter\n        except Exception:\n            # Increase jitter geometrically\n            jitter = 1e-12 if jitter == 0.0 else jitter * 10.0\n    # Final attempt, let it raise\n    L = cholesky(M + jitter * np.eye(M.shape[0]), lower=lower, check_finite=False)\n    return L, jitter\n\ndef build_case(case_id):\n    if case_id == 1:\n        n, p = 3, 8\n        A = np.zeros((n, p), dtype=float)\n        for i in range(n):\n            for j in range(p):\n                A[i, j] = np.sin((i+1)*(j+1)) + 0.1*np.cos((i+1) + 2*(j+1))\n        y = np.array([np.cos(i+1) + 0.5*np.sin(2*(i+1)) for i in range(n)], dtype=float)\n        tau = np.array([0.5 + 0.1*(j+1) for j in range(p)], dtype=float)\n        beta = 25.0\n        return A, y, tau, beta\n    elif case_id == 2:\n        n, p = 3, 8\n        v = np.array([1.0, -2.0, 3.0], dtype=float)\n        eps = 1e-3\n        A = np.zeros((n, p), dtype=float)\n        for j in range(p):\n            wj = np.array([np.sin(j+1), np.cos(j+1), np.sin(2*(j+1))], dtype=float)\n            A[:, j] = v + eps * wj\n        y = np.zeros(n, dtype=float)\n        tau = np.array([10.0 ** (-4.0 + 6.0 * (j/7.0)) for j in range(p)], dtype=float)\n        beta = 1.0\n        return A, y, tau, beta\n    elif case_id == 3:\n        n, p = 4, 10\n        A = np.zeros((n, p), dtype=float)\n        for i in range(n):\n            for j in range(p):\n                A[i, j] = np.sin(0.3*(i+1) + 0.7*(j+1)) + 0.05*((i+1) - (j+1))\n        y = np.array([np.sin(1.5*(i+1)) for i in range(n)], dtype=float)\n        tau = np.array([0.2 + 0.05*j for j in range(p)], dtype=float)\n        beta = 1000.0\n        return A, y, tau, beta\n    elif case_id == 4:\n        n, p = 2, 6\n        A = np.zeros((n, p), dtype=float)\n        for i in range(n):\n            for j in range(p):\n                A[i, j] = np.cos((i+1)*(j+2)) + 0.2*np.sin((i+2) + (j+1))\n        y = np.array([1.0, -1.0], dtype=float)\n        tau = np.zeros(p, dtype=float)\n        for j in range(p):\n            if (j+1) in (1, 2):\n                tau[j] = 1e6\n            elif (j+1) == 3:\n                tau[j] = 1e-6\n            else:\n                tau[j] = 1.0\n        beta = 10.0\n        return A, y, tau, beta\n    else:\n        raise ValueError(\"Unknown case id\")\n\ndef stable_evidence_from_cholesky(L, y):\n    # C = L L^T, y^T C^{-1} y = ||L^{-1} y||^2\n    z = solve_triangular(L, y, lower=True, check_finite=False)\n    quad = float(np.dot(z, z))\n    logdet = 2.0 * np.sum(np.log(np.diag(L)))\n    n = L.shape[0]\n    log_ev = -0.5 * (n * np.log(2.0*np.pi) + logdet + quad)\n    return log_ev, quad, logdet\n\ndef invert_via_cholesky(L):\n    # Given C = L L^T, compute C^{-1} without forming C^{-1} explicitly via solving for I\n    n = L.shape[0]\n    I = np.eye(n)\n    # Solve L Z = I -> Z = L^{-1}\n    Z = solve_triangular(L, I, lower=True, check_finite=False)\n    # Then C^{-1} = (L^{-T}) (L^{-1}) = Z^T Z\n    Cinv = Z.T @ Z\n    return Cinv\n\ndef sigma_direct(Gamma, A, beta):\n    # Sigma_direct = (Gamma^{-1} + beta A^T A)^{-1} via Cholesky\n    p = Gamma.shape[0]\n    Ginv = np.diag(1.0 / np.diag(Gamma))\n    K = Ginv + beta * (A.T @ A)\n    Lk, _ = chol_with_jitter(K, lower=True)\n    # Invert K via Cholesky\n    pI = np.eye(p)\n    Z = solve_triangular(Lk, pI, lower=True, check_finite=False)\n    Kinv = Z.T @ Z\n    return Kinv\n\ndef compute_errors_for_case(A, y, tau, beta):\n    n, p = A.shape\n    Gamma = np.diag(tau)\n    # Build C\n    C = (1.0/beta) * np.eye(n) + A @ Gamma @ A.T\n    # Cholesky of C\n    Lc, jitter = chol_with_jitter(C, lower=True)\n    # Low-rank mu: mu = Gamma A^T C^{-1} y\n    z = solve_triangular(Lc, y, lower=True, check_finite=False)\n    w = solve_triangular(Lc.T, z, lower=False, check_finite=False)\n    mu_lowrank = Gamma @ (A.T @ w)\n    # Low-rank Sigma: Sigma = Gamma - Gamma A^T C^{-1} A Gamma\n    AG = A @ Gamma\n    Y = solve_triangular(Lc, AG, lower=True, check_finite=False)\n    Z = solve_triangular(Lc.T, Y, lower=False, check_finite=False)  # Z = C^{-1} A Gamma\n    Sigma_lowrank = Gamma - (Gamma @ A.T) @ Z\n    # Low-rank Cinv via Sigma: C^{-1} = beta I - beta^2 A Sigma A^T\n    Cinv_lowrank = beta * np.eye(n) - (beta**2) * (A @ Sigma_lowrank @ A.T)\n    # Evidence via Cholesky\n    log_ev_chol, quad_chol, logdet_chol = stable_evidence_from_cholesky(Lc, y)\n    # Direct baselines\n    Sigma_dir = sigma_direct(Gamma, A, beta)\n    # Direct Cinv via Cholesky inversion (reuse Lc to be consistent)\n    Cinv_dir = invert_via_cholesky(Lc)\n    # Direct evidence using C (via slogdet and solve)\n    sign, logdet_direct = np.linalg.slogdet(C)\n    if sign <= 0:\n        # Fallback to Cholesky logdet if numerical issue\n        logdet_direct = logdet_chol\n    sol = np.linalg.solve(C, y)\n    quad_direct = float(y @ sol)\n    log_ev_direct = -0.5 * (n * np.log(2.0*np.pi) + logdet_direct + quad_direct)\n    # Posterior mean via beta Sigma A^T y\n    mu_via_sigma = beta * (Sigma_dir @ (A.T @ y))\n    # Errors\n    e_sigma = float(np.max(np.abs(Sigma_lowrank - Sigma_dir)))\n    e_cinv = float(np.max(np.abs(Cinv_lowrank - Cinv_dir)))\n    e_logev = float(abs(log_ev_chol - log_ev_direct))\n    e_mu = float(np.linalg.norm(mu_lowrank - mu_via_sigma))\n    return e_sigma, e_cinv, e_logev, e_mu\n\ndef solve():\n    test_cases = [1, 2, 3, 4]\n    results = []\n    for cid in test_cases:\n        A, y, tau, beta = build_case(cid)\n        e_sigma, e_cinv, e_logev, e_mu = compute_errors_for_case(A, y, tau, beta)\n        results.extend([e_sigma, e_cinv, e_logev, e_mu])\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3433942"}]}