{"hands_on_practices": [{"introduction": "Before building complex algorithms, it is essential to understand how non-convex thresholding operators behave at a fundamental level. This first exercise [@problem_id:3462713] provides a direct numerical application of the Smoothly Clipped Absolute Deviation (SCAD) and Minimax Concave Penalty (MCP) estimators for a single data point. By working through the piecewise formulas, you will gain a concrete intuition for how these penalties shrink coefficients differently, laying a solid foundation for the more advanced topics that follow.", "problem": "Consider a single-coordinate proximal update in penalized least squares for sparse estimation, where the estimator $T(z)$ minimizes the univariate objective $F(b;z) = \\frac{1}{2}(b - z)^{2} + P(|b|)$ with respect to $b \\in \\mathbb{R}$. The penalty $P$ is either the Smoothly Clipped Absolute Deviation (SCAD) penalty or the Minimax Concave Penalty (MCP), each parameterized by a regularization strength $\\lambda > 0$ and a shape parameter. Work in the standard coordinate-descent framework where the optimality condition for a differentiable branch is $b - z + \\partial P(b) = 0$, interpreting $\\partial P(b)$ as the derivative of $P$ with respect to $b$ on $b \\ge 0$ and using symmetry in the sign of $z$.\n\nUse the following fundamental definitions for the penalty derivatives on the nonnegative half-line:\n- For SCAD with parameters $(\\lambda,a)$ where $a > 2$, define for $t \\ge 0$ the derivative $p'_{\\text{SCAD}}(t)$ of $P_{\\text{SCAD}}(t)$ by\n$$\np'_{\\text{SCAD}}(t) = \n\\begin{cases}\n\\lambda  \\text{if } 0 \\le t \\le \\lambda \\\\\n\\frac{a\\lambda - t}{a - 1}  \\text{if } \\lambda  t \\le a\\lambda \\\\\n0  \\text{if } t > a\\lambda\n\\end{cases}\n$$\nand extend it to $b  0$ by odd symmetry of the subgradient in $b$.\n- For MCP with parameters $(\\lambda,\\gamma)$ where $\\gamma > 1$, define for $t \\ge 0$ the derivative $p'_{\\text{MCP}}(t)$ of $P_{\\text{MCP}}(t)$ by\n$$\np'_{\\text{MCP}}(t) = \n\\begin{cases}\n\\lambda\\left(1 - \\frac{t}{\\gamma\\lambda}\\right)  \\text{if } 0 \\le t \\le \\gamma\\lambda \\\\\n0  \\text{if } t > \\gamma\\lambda\n\\end{cases}\n$$\nand extend it to $b  0$ by odd symmetry of the subgradient in $b$.\n\nTake the numerical instance $z = 3.2$, $\\lambda = 1.0$, $a = 3.7$, and $\\gamma = 2.5$. Compute explicitly the SCAD threshold $T_{\\text{SCAD}}(z)$ and the MCP threshold $T_{\\text{MCP}}(z)$, understood as the minimizers of $\\frac{1}{2}(b - z)^{2} + P(|b|)$ under the respective penalties. Then, assuming the true underlying coefficient is zero, compare the two estimators under squared loss by evaluating $\\left(T_{\\text{SCAD}}(z) - 0\\right)^{2}$ and $\\left(T_{\\text{MCP}}(z) - 0\\right)^{2}$. Provide the value of the difference\n$$\n\\Delta = \\left(T_{\\text{MCP}}(z)\\right)^{2} - \\left(T_{\\text{SCAD}}(z)\\right)^{2}.\n$$\n\nYour final answer must be the row matrix containing the three quantities $T_{\\text{SCAD}}(z)$, $T_{\\text{MCP}}(z)$, and $\\Delta$, in that order, written as exact values without rounding.", "solution": "The problem requires the computation of thresholding functions for the Smoothly Clipped Absolute Deviation (SCAD) and Minimax Concave Penalty (MCP) penalties, and the subsequent evaluation of these functions and their difference in squared loss for a specific case.\n\nThe estimator $T(z)$ is defined as the minimizer of the univariate objective function:\n$$F(b;z) = \\frac{1}{2}(b - z)^{2} + P(|b|)$$\nThe minimizer $b$ is the proximal operator of the penalty $P(|\\cdot|)$ evaluated at $z$. Due to the symmetry of the penalty term $P(|b|)$ and the quadratic term $(b-z)^2$, the solution $b = T(z)$ will have the same sign as $z$. Given $z = 3.2 > 0$, we can restrict our search for the minimizer to $b \\ge 0$. For $b \\ge 0$, the objective function becomes $F(b;z) = \\frac{1}{2}(b - z)^{2} + P(b)$ (since $|b|=b$).\n\nThe first-order necessary condition for a minimum at $b > 0$ is that the derivative of $F(b;z)$ with respect to $b$ is zero. This condition is given as $b-z+p'(b) = 0$, which can be rearranged to $z = b + p'(b)$, where $p'(b)$ is the derivative of the penalty function $P(t)$ evaluated at $t=b$. For $b=0$ to be the solution, we must satisfy the subgradient optimality condition $0 \\in \\partial F(0;z)$, which is $z \\in \\partial P(|b|)|_{b=0}$. For both SCAD and MCP, the subgradient at the origin is the interval $[-\\lambda, \\lambda]$. Thus, for $z \\in [0, \\lambda]$, the solution is $b=0$.\n\nWe are given the numerical values $z = 3.2$, $\\lambda = 1.0$, $a = 3.7$ for SCAD, and $\\gamma = 2.5$ for MCP.\n\nFirst, we calculate the SCAD threshold $T_{\\text{SCAD}}(z)$.\nThe derivative of the SCAD penalty for $t \\ge 0$ is given as:\n$$\np'_{\\text{SCAD}}(t) = \n\\begin{cases}\n\\lambda  \\text{if } 0 \\le t \\le \\lambda \\\\\n\\frac{a\\lambda - t}{a - 1}  \\text{if } \\lambda  t \\le a\\lambda \\\\\n0  \\text{if } t > a\\lambda\n\\end{cases}\n$$\nWe analyze the solution $b$ based on the equation $z = b + p'_{\\text{SCAD}}(b)$ by considering which region the observation $z$ falls into.\n1.  For $z \\in (\\lambda, 2\\lambda]$, the solution is the soft-thresholding rule $b = z-\\lambda$. This corresponds to the penalty derivative being $p'_{\\text{SCAD}}(b)=\\lambda$.\n2.  For $z \\in (2\\lambda, a\\lambda]$, the solution is $b = \\frac{z(a-1) - a\\lambda}{a-2}$. This corresponds to the penalty derivative being $p'_{\\text{SCAD}}(b)=\\frac{a\\lambda - b}{a - 1}$.\n3.  For $z > a\\lambda$, the solution is $b=z$. This corresponds to the penalty derivative being $p'_{\\text{SCAD}}(b)=0$.\n\nWith the given values, $\\lambda=1.0$ and $a=3.7$, the critical points for $z$ are $2\\lambda = 2.0$ and $a\\lambda = 3.7$. The given value $z=3.2$ satisfies $2\\lambda  z \\le a\\lambda$ (i.e., $2.0  3.2 \\le 3.7$). Therefore, we use the formula from case 2:\n$$T_{\\text{SCAD}}(z) = b = \\frac{z(a-1) - a\\lambda}{a-2}$$\nSubstituting the numerical values:\n$$T_{\\text{SCAD}}(3.2) = \\frac{3.2(3.7-1) - 3.7(1.0)}{3.7-2} = \\frac{3.2(2.7) - 3.7}{1.7} = \\frac{8.64 - 3.7}{1.7} = \\frac{4.94}{1.7}$$\nTo express this as an exact fraction:\n$$T_{\\text{SCAD}}(3.2) = \\frac{4.94}{1.7} = \\frac{494}{170} = \\frac{247}{85}$$\n\nNext, we calculate the MCP threshold $T_{\\text{MCP}}(z)$.\nThe derivative of the MCP penalty for $t \\ge 0$ is:\n$$\np'_{\\text{MCP}}(t) = \n\\begin{cases}\n\\lambda\\left(1 - \\frac{t}{\\gamma\\lambda}\\right)  \\text{if } 0 \\le t \\le \\gamma\\lambda \\\\\n0  \\text{if } t > \\gamma\\lambda\n\\end{cases}\n$$\nWe analyze the solution $b$ from $z = b + p'_{\\text{MCP}}(b)$ for $z > \\lambda$.\n1.  If $z \\in (\\lambda, \\gamma\\lambda]$, the solution is $b = \\frac{\\gamma}{\\gamma-1}(z-\\lambda)$.\n2.  If $z > \\gamma\\lambda$, the solution is $b=z$.\n\nWith the given values, $\\lambda=1.0$ and $\\gamma=2.5$, the critical point for $z$ is $\\gamma\\lambda = 2.5$. The given value $z=3.2$ satisfies $z > \\gamma\\lambda$ (i.e., $3.2 > 2.5$). Therefore, we use the formula from case 2:\n$$T_{\\text{MCP}}(z) = z$$\nSubstituting the numerical value $z=3.2$:\n$$T_{\\text{MCP}}(3.2) = 3.2 = \\frac{32}{10} = \\frac{16}{5}$$\n\nFinally, we compute the difference in squared estimators, $\\Delta$:\n$$\\Delta = \\left(T_{\\text{MCP}}(z)\\right)^{2} - \\left(T_{\\text{SCAD}}(z)\\right)^{2}$$\nSubstituting the calculated values:\n$$\\Delta = \\left(\\frac{16}{5}\\right)^{2} - \\left(\\frac{247}{85}\\right)^{2}$$\nTo simplify the calculation, we find a common denominator. Since $85 = 5 \\times 17$, we can write $\\frac{16}{5} = \\frac{16 \\times 17}{5 \\times 17} = \\frac{272}{85}$.\n$$\\Delta = \\left(\\frac{272}{85}\\right)^{2} - \\left(\\frac{247}{85}\\right)^{2} = \\frac{272^{2} - 247^{2}}{85^{2}}$$\nUsing the difference of squares formula, $x^2 - y^2 = (x-y)(x+y)$:\n$$\\Delta = \\frac{(272 - 247)(272 + 247)}{85^{2}} = \\frac{(25)(519)}{85^{2}}$$\nSince $85^2 = (5 \\times 17)^2 = 25 \\times 17^2 = 25 \\times 289$:\n$$\\Delta = \\frac{25 \\times 519}{25 \\times 289} = \\frac{519}{289}$$\nThe fraction $\\frac{519}{289}$ is irreducible because $289 = 17^2$ and $519 = 17 \\times 30 + 9$, so $519$ is not divisible by $17$.\n\nThe three requested quantities are $T_{\\text{SCAD}}(z) = \\frac{247}{85}$, $T_{\\text{MCP}}(z) = \\frac{16}{5}$, and $\\Delta = \\frac{519}{289}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{247}{85}  \\frac{16}{5}  \\frac{519}{289}\n\\end{pmatrix}\n}\n$$", "id": "3462713"}, {"introduction": "A key motivation for using non-convex penalties is their ability to produce nearly unbiased estimates for large, important coefficients, overcoming a major drawback of the $\\ell_1$ norm. This desirable property, however, depends on a careful selection of the penalty's parameters. This practice [@problem_id:3462708] guides you through the theoretical derivation of a \"safe region\" for the MCP parameters $(\\lambda, \\gamma)$ that guarantees both exact recovery of true signals and correct suppression of noise, revealing the elegant interplay between the penalty's shape, signal strength, and noise level.", "problem": "Consider the following sparse estimation model. Let $x^{\\star} \\in \\mathbb{R}^{n}$ be $s$-sparse with support $S \\subset \\{1,\\dots,n\\}$ and smallest nonzero magnitude $x_{\\min} := \\min_{i \\in S} |x^{\\star}_{i}|  0$. Let $y = x^{\\star} + w$ be observed with $\\|w\\|_{\\infty} \\le \\sigma$ for a known noise bound $\\sigma \\ge 0$. Consider the estimator\n$$\n\\hat{x} \\in \\arg\\min_{b \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|y - b\\|_{2}^{2} + \\sum_{i=1}^{n} p_{\\lambda,\\gamma}(|b_{i}|) \\right\\},\n$$\nwhere $p_{\\lambda,\\gamma}$ is the Minimax Concave Penalty (MCP), defined for $\\lambda  0$ and $\\gamma  1$ by\n$$\np_{\\lambda,\\gamma}(t) \\;:=\\; \\lambda \\int_{0}^{t} \\left(1 - \\frac{s}{\\gamma \\lambda}\\right)_{+} \\, ds, \\quad t \\ge 0,\n$$\nand $(u)_{+} := \\max\\{u,0\\}$. Throughout, interpret unbiasedness for a coordinate as the absence of regularization-induced shrinkage, namely the condition that the coordinate-wise solution equals the corresponding data value $y_{i}$ (which coincides with $x^{\\star}_{i}$ in the noiseless limit).\n\nStarting from the given definitions and the first-order optimality conditions for the above separable problem, derive a safe region in the $(\\lambda,\\gamma)$-plane that guarantees both of the following simultaneously:\n- for every $i \\in S$ with $|x^{\\star}_{i}| = x_{\\min}$, the unique coordinate-wise minimizer is unbiased in the above sense under the worst-case admissible noise;\n- for every $j \\notin S$, the unique coordinate-wise minimizer is exactly zero under the worst-case admissible noise.\n\nThen, analyze a near-threshold nonzero coefficient by considering a hypothetical entry of magnitude $x_{\\mathrm{nt}} := \\gamma \\lambda + \\varepsilon$ with $\\varepsilon  0$, and determine, under the same worst-case noise model, the necessary and sufficient condition on $\\varepsilon$ for this entry to remain unbiased.\n\nFinally, express in closed form the supremum value of $\\gamma$ (as a function of $x_{\\min}$ and $\\sigma$) for which the derived safe region in $(\\lambda,\\gamma)$ is nonempty. Provide this supremum as your final answer in a single analytic expression.", "solution": "The problem asks for an analysis of a sparse estimator based on the Minimax Concave Penalty (MCP). The process involves deriving conditions for reliable estimation, analyzing a near-threshold scenario, and finding a supremum value for a penalty parameter.\n\nFirst, we must characterize the estimator. The optimization problem is\n$$\n\\hat{x} \\in \\arg\\min_{b \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|y - b\\|_{2}^{2} + \\sum_{i=1}^{n} p_{\\lambda,\\gamma}(|b_{i}|) \\right\\}\n$$\nThis problem is separable, meaning we can solve for each coordinate $\\hat{x}_i$ independently by minimizing\n$$\nL_i(b_i) = \\frac{1}{2} (y_i - b_i)^2 + p_{\\lambda,\\gamma}(|b_i|)\n$$\nThe MCP penalty for $t \\ge 0$ is given by $p_{\\lambda,\\gamma}(t) = \\lambda \\int_{0}^{t} \\left(1 - \\frac{s}{\\gamma \\lambda}\\right)_{+} ds$. Its derivative, which is needed for the first-order optimality conditions, is $p'_{\\lambda,\\gamma}(t) = \\lambda \\left(1 - \\frac{t}{\\gamma \\lambda}\\right)_{+}$ for $t \\ge 0$. Explicitly,\n$$\np'_{\\lambda,\\gamma}(t) = \\begin{cases} \\lambda - \\frac{t}{\\gamma}  \\text{if } 0 \\le t \\le \\gamma\\lambda \\\\ 0  \\text{if } t  \\gamma\\lambda \\end{cases}\n$$\nThe first-order conditions for a minimizer $\\hat{b}_i$ of $L_i(b_i)$ are given by the subgradient inclusion $0 \\in \\hat{b}_i - y_i + \\partial_t [p_{\\lambda,\\gamma}(|t|)]|_{t=\\hat{b}_i}$. Solving these conditions yields the MCP thresholding function, $\\hat{b}_i = T_{\\lambda,\\gamma}(y_i)$, which is\n$$\nT_{\\lambda,\\gamma}(y_i) = \\begin{cases} 0  \\text{if } |y_i| \\le \\lambda \\\\ \\mathrm{sgn}(y_i) \\frac{\\gamma(|y_i|-\\lambda)}{\\gamma-1}  \\text{if } \\lambda  |y_i| \\le \\gamma\\lambda \\\\ y_i  \\text{if } |y_i|  \\gamma\\lambda \\end{cases}\n$$\nThis function characterizes the unique coordinate-wise minimizer for $\\gamma  1$.\n\nNow, we derive the safe region in the $(\\lambda, \\gamma)$-plane. This region must simultaneously satisfy two conditions under the worst-case noise model $\\|w\\|_{\\infty} \\le \\sigma$.\n\n1.  **Unbiasedness on the support set**: For any $i \\in S$ with $|x^{\\star}_i| = x_{\\min}$, the estimator must be unbiased, i.e., $\\hat{x}_i = y_i$. According to the thresholding function, this occurs if and only if $|y_i|  \\gamma\\lambda$. The term \"worst-case admissible noise\" implies that this condition must hold even for the noise realization that makes it hardest to satisfy. The challenge here is to keep $|y_i|$ large. The value of $|y_i| = |x^{\\star}_i + w_i|$ is minimized when the noise $w_i$ counteracts the signal $x^{\\star}_i$. The minimum value is $\\min_{|w_i|\\le\\sigma} |x^{\\star}_i + w_i| = \\max(0, |x^{\\star}_i|-\\sigma)$. For our case, this is $\\max(0, x_{\\min}-\\sigma)$. Therefore, we require $\\max(0, x_{\\min}-\\sigma)  \\gamma\\lambda$. This single inequality implies two conditions: first, $x_{\\min}  \\sigma$, and second, $x_{\\min} - \\sigma  \\gamma\\lambda$. So the first condition for the safe region is:\n    $$\n    \\gamma\\lambda  x_{\\min} - \\sigma\n    $$\n\n2.  **Sparsity off the support set**: For any $j \\notin S$, the estimator must be zero, i.e., $\\hat{x}_j=0$. According to the thresholding function, this occurs if and only if $|y_j| \\le \\lambda$. For such an index $j$, $x^{\\star}_j=0$, so $y_j = w_j$. The \"worst-case admissible noise\" here is the noise that makes $|y_j|$ as large as possible, challenging the condition $|y_j| \\le \\lambda$. This worst case is any $w_j$ with $|w_j|=\\sigma$. Thus, we must have $\\max_{|w_j|\\le\\sigma} |w_j| \\le \\lambda$, which simplifies to:\n    $$\n    \\lambda \\ge \\sigma\n    $$\n\nCombining these two conditions with the parameter constraints $\\lambda  0$ and $\\gamma  1$, the safe region in the $(\\lambda, \\gamma)$-plane is the set of points satisfying\n$$\n\\sigma \\le \\lambda  \\frac{x_{\\min}-\\sigma}{\\gamma} \\quad \\text{and} \\quad \\gamma  1\n$$\nNote that for this region to exist, we must have $x_{\\min}-\\sigma  0$, or $x_{\\min}  \\sigma$.\n\nNext, we analyze the near-threshold coefficient. Consider a hypothetical entry with magnitude $|x^{\\star}_{\\mathrm{nt}}| = \\gamma\\lambda + \\varepsilon$ for $\\varepsilon  0$. For this entry to be unbiased ($\\hat{x}_{\\mathrm{nt}} = y_{\\mathrm{nt}}$), we require $|y_{\\mathrm{nt}}|  \\gamma\\lambda$. This must hold under the worst-case noise, so we need $\\min_{|w|\\le\\sigma} |x^{\\star}_{\\mathrm{nt}} + w|  \\gamma\\lambda$. This minimum is $| |x^{\\star}_{\\mathrm{nt}}| - \\sigma |$. Substituting the value of $|x^{\\star}_{\\mathrm{nt}}|$, the condition becomes\n$$\n|(\\gamma\\lambda + \\varepsilon) - \\sigma|  \\gamma\\lambda\n$$\nAssuming the parameters $(\\lambda, \\gamma)$ are chosen from the safe region, we have $\\lambda \\ge \\sigma$ and $\\gamma  1$. Let's examine the sign of the term inside the absolute value: $\\gamma\\lambda + \\varepsilon - \\sigma$. Since $\\lambda \\ge \\sigma$, we have $\\gamma\\lambda \\ge \\gamma\\sigma$. Thus, $\\gamma\\lambda + \\varepsilon - \\sigma \\ge \\gamma\\sigma + \\varepsilon - \\sigma = (\\gamma-1)\\sigma + \\varepsilon$. As $\\gamma  1$, $\\sigma \\ge 0$, and $\\varepsilon  0$, this term is strictly positive. We can therefore drop the absolute value:\n$$\n\\gamma\\lambda + \\varepsilon - \\sigma  \\gamma\\lambda\n$$\nThis simplifies to $\\varepsilon  \\sigma$. This is the necessary and sufficient condition on $\\varepsilon$ for the entry to remain unbiased.\n\nFinally, we find the supremum value of $\\gamma$ for which the safe region is nonempty. A nonempty safe region requires the existence of a $\\lambda$ such that $\\sigma \\le \\lambda  \\frac{x_{\\min}-\\sigma}{\\gamma}$. This is only possible if the lower bound is strictly less than the upper bound:\n$$\n\\sigma  \\frac{x_{\\min}-\\sigma}{\\gamma}\n$$\nAssuming $\\sigma  0$, we can rearrange this inequality to solve for $\\gamma$. Since $\\gamma  1$ is positive, we multiply by $\\gamma$:\n$$\n\\gamma\\sigma  x_{\\min} - \\sigma\n$$\n$$\n\\gamma\\sigma + \\sigma  x_{\\min}\n$$\n$$\n(\\gamma+1)\\sigma  x_{\\min}\n$$\n$$\n\\gamma + 1  \\frac{x_{\\min}}{\\sigma}\n$$\n$$\n\\gamma  \\frac{x_{\\min}}{\\sigma} - 1\n$$\nThe set of all $\\gamma$ values for which the safe region is nonempty is therefore given by the intersection of the conditions $\\gamma  1$ and $\\gamma  \\frac{x_{\\min}}{\\sigma} - 1$. This defines the interval $\\left(1, \\frac{x_{\\min}}{\\sigma} - 1\\right)$. For this interval to be non-empty, we need $1  \\frac{x_{\\min}}{\\sigma} - 1$, which means $2  \\frac{x_{\\min}}{\\sigma}$, or $x_{\\min}  2\\sigma$, a standard condition in sparse recovery theory.\n\nThe question asks for the supremum of the set of possible $\\gamma$ values. The supremum of the interval $\\left(1, \\frac{x_{\\min}}{\\sigma} - 1\\right)$ is its upper endpoint.\nIf $\\sigma = 0$, the conditions for the safe region become $\\lambda \\ge 0$ and $\\gamma\\lambda  x_{\\min}$. For any $\\gamma  1$, we can choose $\\lambda \\in (0, x_{\\min}/\\gamma)$, so the region is non-empty. In this case, the set of valid $\\gamma$ is $(1, \\infty)$, and the supremum is $\\infty$. The derived expression $\\frac{x_{\\min}}{\\sigma} - 1$ correctly diverges to $\\infty$ as $\\sigma \\to 0^+$. Thus, it serves as the general closed-form answer.", "answer": "$$\n\\boxed{\\frac{x_{\\min}}{\\sigma} - 1}\n$$", "id": "3462708"}, {"introduction": "Theoretical models often rely on idealized assumptions, such as perfectly normalized data, but real-world applications require more robust methods. This practice challenges you to adapt theory to a more realistic setting by deriving a generalized coordinate descent update rule for cases where the columns of the design matrix are not of unit norm. By deriving this renormalized thresholding rule and implementing a complete coordinate descent solver to test its performance [@problem_id:3462665], you will bridge the crucial gap between abstract mathematical concepts and robust, practical code.", "problem": "You are given a linear inverse model in compressed sensing with a design matrix $A \\in \\mathbb{R}^{m \\times n}$, a sparse ground-truth vector $x_0 \\in \\mathbb{R}^n$, and noiseless measurements $y = A x_0$. In practice, columns of $A$ are often normalized, but manufacturing or preprocessing errors can introduce per-column scaling factors so that the actual design matrix used is $A D$, where $D = \\mathrm{diag}(s_1,\\dots,s_n)$ with $s_j  0$. This implies that the column $A_j$ is scaled as $A_j \\gets s_j A_j$, and the per-column squared norm $c_j = \\lVert A_j \\rVert_2^2$ deviates from its target. You will examine the impact of such column normalization errors on the thresholding behavior of non-convex penalties and derive a renormalized thresholding rule that correctly accounts for $c_j$ when performing coordinate descent updates under the Smoothly Clipped Absolute Deviation (SCAD) and the Minimax Concave Penalty (MCP).\n\nStarting from the objective\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; \\frac{1}{2} \\lVert y - A x \\rVert_2^2 + \\sum_{j=1}^n p(x_j;\\lambda,\\theta),\n$$\nwhere $p(\\cdot;\\lambda,\\theta)$ is either the Smoothly Clipped Absolute Deviation (SCAD) penalty or the Minimax Concave Penalty (MCP), both parameterized by a regularization level $\\lambda  0$ and a shape parameter $\\theta$ (for SCAD use $a  2$, for MCP use $\\gamma  1$), derive, from first principles, the coordinate-wise one-dimensional subproblem for the $j$-th coordinate and obtain a renormalized thresholding rule that explicitly depends on the per-column squared norm $c_j = \\lVert A_j \\rVert_2^2$ and the scalar $z_j = A_j^\\top r + c_j x_j$, where $r = y - A x$ is the current residual and $x_j$ is the current iterate for the $j$-th coefficient. Your derivation must start from the coordinate descent perspective on the least-squares term and the subgradient stationarity condition for the non-convex penalty, and it must not invoke any shortcut formulas directly.\n\nThe Smoothly Clipped Absolute Deviation (SCAD) penalty uses parameter $a  2$. The Minimax Concave Penalty (MCP) uses parameter $\\gamma  1$. For each penalty, derive the piecewise thresholding function for the one-dimensional subproblem that maps $(z_j, c_j, \\lambda, \\theta)$ to an updated coefficient $t_j^\\star$. The renormalized rule must reduce to the known thresholding behavior when $c_j = 1$, and it must correctly adjust thresholds and shrinkage for $c_j \\neq 1$.\n\nImplement a proximal coordinate descent solver that:\n- Initializes $x$ to the zero vector.\n- Maintains the residual $r = y - A x$.\n- Iterates cyclically over coordinates $j = 1,\\dots,n$, computing $z_j = A_j^\\top r + c_j x_j$, then updating $x_j \\gets t_j^\\star$ via your derived renormalized thresholding rule for the chosen penalty, and updating the residual accordingly.\n- Stops when the maximum absolute change across coordinates falls below a tolerance or when a maximum number of iterations is reached.\n\nSimulate recovery under random per-column scaling noise as follows:\n- Generate $A_{\\text{base}}$ with independent and identically distributed entries from a standard normal distribution, then normalize each column to unit Euclidean norm.\n- Draw $s_j$ independently and uniformly from $[1 - \\delta, 1 + \\delta]$ for a given $\\delta \\in [0,1)$, and set $A = A_{\\text{base}} \\cdot \\mathrm{diag}(s)$ (column-wise scaling).\n- Choose a support set of size $k$ uniformly at random, and populate $x_0$ on that support with random signs and magnitudes uniformly drawn from $[0.5, 1.0]$. Set $y = A x_0$.\n- Run your solver with either SCAD or MCP penalty, using given hyperparameters $(\\lambda, a)$ for SCAD or $(\\lambda, \\gamma)$ for MCP.\n\nDefine exact support recovery as the event that the set of indices of the $k$ largest absolute entries of the recovered $x$ equals the true support of $x_0$.\n\nYour program must evaluate the following test suite, each test case specified as $(\\text{penalty}, \\delta, \\lambda, \\theta, m, n, k, \\text{seed})$:\n- Case $1$: MCP, $\\delta = 0.0$, $\\lambda = 0.08$, $\\gamma = 3.0$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 1$.\n- Case $2$: MCP, $\\delta = 0.1$, $\\lambda = 0.08$, $\\gamma = 3.0$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 2$.\n- Case $3$: MCP, $\\delta = 0.3$, $\\lambda = 0.08$, $\\gamma = 1.2$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 3$.\n- Case $4$: SCAD, $\\delta = 0.0$, $\\lambda = 0.08$, $a = 3.7$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 4$.\n- Case $5$: SCAD, $\\delta = 0.3$, $\\lambda = 0.08$, $a = 2.1$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 5$.\n- Case $6$: SCAD, $\\delta = 0.2$, $\\lambda = 0.25$, $a = 3.7$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 6$.\n\nFor each case, your program must produce a boolean indicating whether exact support recovery was achieved. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5,r_6]$).", "solution": "The objective is to derive and implement a renormalized coordinate-wise update rule for the SCAD and MCP penalties in a linear regression context where the columns of the design matrix are not necessarily unit norm.\n\n### 1. Coordinate Descent Subproblem\n\nWe start with the objective function:\n$$\nF(x) = \\frac{1}{2} \\lVert y - A x \\rVert_2^2 + \\sum_{j=1}^n p(x_j;\\lambda,\\theta)\n$$\nIn a coordinate descent algorithm, we optimize $F(x)$ with respect to a single coordinate $x_j$ at a time, holding all other coordinates $x_k$ ($k \\neq j$) fixed. Let the current iterate be $x$, and let $t$ be the new value for the $j$-th coordinate. We can express the vector being updated as $x - x_j e_j + t e_j$, where $e_j$ is the $j$-th standard basis vector.\n\nThe least-squares term can be written as:\n$$\n\\frac{1}{2} \\lVert y - A(x - x_j e_j + t e_j) \\rVert_2^2 = \\frac{1}{2} \\lVert (y - A x) + (x_j-t)A_j \\rVert_2^2\n$$\nLet $r = y - Ax$ be the current residual and $A_j$ be the $j$-th column of $A$. Expanding the squared norm, we get:\n$$\n\\frac{1}{2} ( \\lVert r \\rVert_2^2 + 2(x_j-t) A_j^\\top r + (x_j-t)^2 \\lVert A_j \\rVert_2^2 )\n$$\nThe one-dimensional subproblem for updating $x_j$ to $t$ is to minimize:\n$$\n\\min_{t \\in \\mathbb{R}} \\left\\{ \\frac{1}{2}\\lVert A_j \\rVert_2^2 (t - x_j)^2 - (t - x_j)A_j^\\top r + p(t;\\lambda,\\theta) \\right\\}\n$$\nLet $c_j = \\lVert A_j \\rVert_2^2$. We collect terms involving $t$:\n$$\n\\min_{t \\in \\mathbb{R}} \\left\\{ \\frac{c_j}{2} (t^2 - 2tx_j) - t A_j^\\top r + p(t;\\lambda,\\theta) + \\text{const} \\right\\}\n$$\n$$\n\\min_{t \\in \\mathbb{R}} \\left\\{ \\frac{c_j}{2} t^2 - t(c_j x_j + A_j^\\top r) + p(t;\\lambda,\\theta) \\right\\}\n$$\nAs defined in the problem, let $z_j = c_j x_j + A_j^\\top r$. The subproblem becomes:\n$$\n\\min_{t \\in \\mathbb{R}} \\left\\{ \\frac{c_j}{2} t^2 - z_j t + p(t;\\lambda,\\theta) \\right\\}\n$$\nBy completing the square, this is equivalent to minimizing:\n$$\n\\min_{t \\in \\mathbb{R}} \\left\\{ \\frac{c_j}{2} \\left( t - \\frac{z_j}{c_j} \\right)^2 + p(t;\\lambda,\\theta) \\right\\}\n$$\nThis is the proximal operator of the function $\\frac{1}{c_j}p(\\cdot)$ evaluated at $\\frac{z_j}{c_j}$. The first-order necessary condition for a minimum $t^\\star_j$ is given by the subgradient stationarity condition:\n$$\n0 \\in \\frac{\\partial}{\\partial t} \\left[ \\frac{c_j}{2} t^2 - z_j t \\right] + \\partial p(t;\\lambda,\\theta) \\bigg|_{t=t^\\star_j}\n$$\n$$\n0 \\in c_j t^\\star_j - z_j + \\partial p(t^\\star_j;\\lambda,\\theta) \\quad \\implies \\quad z_j - c_j t^\\star_j \\in \\partial p(t^\\star_j;\\lambda,\\theta)\n$$\nWe now solve this for the SCAD and MCP penalties. The penalties are symmetric, so we derive the rules for $z_j  0$ (which implies $t^\\star_j \\ge 0$) and extend by symmetry.\n\n### 2. Renormalized SCAD Thresholding\n\nThe SCAD penalty and its derivative for $t0$ with parameter $a2$ are:\n$$\np(t; \\lambda, a) = \\begin{cases} \\lambda t  \\text{if } 0 \\le t \\le \\lambda \\\\ \\frac{-t^2 + 2a\\lambda t - \\lambda^2}{2(a-1)}  \\text{if } \\lambda  t \\le a\\lambda \\\\ \\frac{(a+1)\\lambda^2}{2}  \\text{if } t  a\\lambda \\end{cases}\n\\quad \\implies \\quad p'(t) = \\begin{cases} \\lambda  \\text{if } 0  t \\le \\lambda \\\\ \\frac{a\\lambda - t}{a-1}  \\text{if } \\lambda  t \\le a\\lambda \\\\ 0  \\text{if } t  a\\lambda \\end{cases}\n$$\nThe subgradient at $t=0$ is $\\partial p(0) = [-\\lambda, \\lambda]$.\n\nWe solve $z_j - c_j t = p'(t)$ for $t0$ and $z_j0$.\n\n1.  If $t^\\star_j = 0$, we need $z_j \\in [-\\lambda, \\lambda]$. For $z_j  0$, this means $0  z_j \\le \\lambda$.\n2.  If $0  t^\\star_j \\le \\lambda$, $p'(t) = \\lambda$. Then $z_j - c_j t = \\lambda \\implies t^\\star_j = (z_j - \\lambda)/c_j$. This is valid if $0  (z_j-\\lambda)/c_j \\le \\lambda$, which requires $\\lambda  z_j \\le (c_j+1)\\lambda$.\n3.  If $t^\\star_j  a\\lambda$, $p'(t) = 0$. Then $z_j - c_j t = 0 \\implies t^\\star_j = z_j/c_j$. This is valid if $z_j/c_j  a\\lambda$, which requires $z_j  ac_j\\lambda$.\n4.  If $\\lambda  t^\\star_j \\le a\\lambda$, $p'(t) = (a\\lambda - t)/(a-1)$. Then $z_j - c_j t = (a\\lambda-t)/(a-1) \\implies t^\\star_j(c_j(a-1)-1) = z_j(a-1) - a\\lambda$.\n    If $c_j(a-1)-1 \\neq 0$, then $t^\\star_j = \\frac{(a-1)z_j - a\\lambda}{c_j(a-1)-1}$. This holds for $z_j$ values between the previous regions, i.e., $(c_j+1)\\lambda  z_j \\le ac_j\\lambda$.\n\nCombining these cases for $|z_j|$, the renormalized SCAD thresholding operator $S_{\\text{SCAD}}(z_j, c_j, \\lambda, a)$ is:\n$$\nt^\\star_j = \\mathrm{sgn}(z_j) \\times \\begin{cases}\n0  \\text{if } |z_j| \\le \\lambda \\\\\n\\frac{|z_j| - \\lambda}{c_j}  \\text{if } \\lambda  |z_j| \\le (c_j+1)\\lambda \\\\\n\\frac{(a-1)|z_j| - a\\lambda}{c_j(a-1)-1}  \\text{if } (c_j+1)\\lambda  |z_j| \\le ac_j\\lambda \\text{ and } c_j(a-1) \\ne 1 \\\\\n\\frac{|z_j|}{c_j}  \\text{if } |z_j|  ac_j\\lambda\n\\end{cases}\n$$\n\n### 3. Renormalized MCP Thresholding\n\nThe MCP penalty and its derivative for $t0$ with parameter $\\gamma1$ are:\n$$\np(t; \\lambda, \\gamma) = \\begin{cases} \\lambda t - \\frac{t^2}{2\\gamma}  \\text{if } 0 \\le t \\le \\gamma\\lambda \\\\ \\frac{\\gamma\\lambda^2}{2}  \\text{if } t  \\gamma\\lambda \\end{cases}\n\\quad \\implies \\quad p'(t) = \\begin{cases} \\lambda - t/\\gamma  \\text{if } 0  t \\le \\gamma\\lambda \\\\ 0  \\text{if } t  \\gamma\\lambda \\end{cases}\n$$\nThe subgradient at $t=0$ is $\\partial p(0) = [-\\lambda, \\lambda]$. We solve $z_j - c_j t = p'(t)$ for $t0, z_j0$.\n\nThe convexity of the subproblem depends on the sign of $c_j - 1/\\gamma$.\n\n**Case 1: $\\gamma c_j  1$ (Convex Subproblem)**\nThe subproblem objective is convex, admitting a unique minimizer.\n1.  If $t^\\star_j = 0$: $z_j \\in [-\\lambda, \\lambda]$. For $z_j0$, this is $0  z_j \\le \\lambda$.\n2.  If $t^\\star_j  \\gamma\\lambda$: $p'(t)=0 \\implies t^\\star_j=z_j/c_j$. Valid for $z_j/c_j  \\gamma\\lambda \\implies z_j  \\gamma c_j \\lambda$.\n3.  If $0  t^\\star_j \\le \\gamma\\lambda$: $z_j - c_j t = \\lambda - t/\\gamma \\implies t(c_j - 1/\\gamma) = z_j-\\lambda \\implies t^\\star_j = \\frac{z_j-\\lambda}{c_j-1/\\gamma} = \\frac{\\gamma(z_j-\\lambda)}{\\gamma c_j-1}$. This is valid if $0  t^\\star_j \\le \\gamma\\lambda$, which requires $\\lambda  z_j \\le \\gamma c_j \\lambda$.\n\n**Case 2: $\\gamma c_j \\le 1$ (Non-Convex Subproblem)**\nThe subproblem objective is non-convex. The global minimizer must be one of the local minima, which are at $t=0$ and in the region $|t|\\gamma\\lambda$, where the minimizer is $t = z_j/c_j$. We find the global minimum by comparing the objective function value at these two points. We choose $t^\\star_j=z_j/c_j$ if $J(z_j/c_j)  J(0)$, which implies $|z_j|  \\lambda\\sqrt{\\gamma c_j}$. This is a hard-thresholding rule.\n\nCombining these cases for $|z_j|$, the operator $S_{\\text{MCP}}(z_j, c_j, \\lambda, \\gamma)$ is:\nIf $\\gamma c_j  1$:\n$$\nt^\\star_j = \\mathrm{sgn}(z_j) \\times \\begin{cases}\n0  \\text{if } |z_j| \\le \\lambda \\\\\n\\frac{\\gamma(|z_j| - \\lambda)}{\\gamma c_j - 1}  \\text{if } \\lambda  |z_j| \\le \\gamma c_j \\lambda \\\\\n\\frac{|z_j|}{c_j}  \\text{if } |z_j|  \\gamma c_j \\lambda\n\\end{cases}\n$$\nIf $\\gamma c_j \\le 1$:\n$$\nt^\\star_j = \\frac{z_j}{c_j} \\mathbb{I}(|z_j|  \\lambda\\sqrt{\\gamma c_j})\n$$\n\n### 4. Proximal Coordinate Descent Algorithm\n\nThe solver implements proximal coordinate descent as follows:\n1.  Initialize $x=0$, residual $r=y$.\n2.  Pre-compute column squared norms $c_j = \\lVert A_j \\rVert_2^2$.\n3.  Iterate until convergence:\n    a. For each coordinate $j=1, \\dots, n$:\n        i.  Store old coefficient $x_j^{\\text{old}} = x_j$.\n        ii. Compute $z_j = A_j^\\top r + c_j x_j^{\\text{old}}$.\n        iii. Compute new coefficient $x_j^{\\text{new}} = S(z_j, c_j, \\lambda, \\theta)$ using the appropriate derived rule (SCAD or MCP).\n        iv. Update residual: $r \\gets r - (x_j^{\\text{new}} - x_j^{\\text{old}}) A_j$.\n        v.  Update coefficient: $x_j \\gets x_j^{\\text{new}}$.\n    b. Check stopping criterion: max coordinate change falls below a tolerance.\n4.  Return the estimated sparse vector $x$.", "answer": "[True,True,True,True,True,True]", "id": "3462665"}]}