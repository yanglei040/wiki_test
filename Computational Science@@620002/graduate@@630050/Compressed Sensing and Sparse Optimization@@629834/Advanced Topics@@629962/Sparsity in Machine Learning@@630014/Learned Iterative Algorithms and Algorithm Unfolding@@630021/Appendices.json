{"hands_on_practices": [{"introduction": "The Alternating Direction Method of Multipliers (ADMM) is a powerful and versatile framework for solving complex optimization problems by splitting them into smaller, more manageable subproblems. In this exercise, we will unfold the ADMM algorithm and replace one of its core components—the proximal step—with a learned shrinkage function. This hands-on calculation will demystify how learned parameters operate within each layer of an unfolded algorithm, providing a concrete foundation for understanding these powerful hybrid models.", "problem": "Consider the standard sparse regression model in compressed sensing, where the latent signal $x \\in \\mathbb{R}^{n}$ is estimated from linear measurements $y \\in \\mathbb{R}^{m}$ via a matrix $A \\in \\mathbb{R}^{m \\times n}$ under an $\\ell_{1}$ sparsity penalty. The canonical formulation introduces a splitting variable $z \\in \\mathbb{R}^{n}$ and enforces the constraint $z = x$, leading to the constrained optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{n},\\, z \\in \\mathbb{R}^{n}} \\;\\; \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|z\\|_{1}\n\\quad \\text{subject to} \\quad z = x,\n$$\nwith penalty weight $\\lambda > 0$. The Alternating Direction Method of Multipliers (ADMM) is applied to the augmented Lagrangian associated with the above constraint using the scaled dual variable $u \\in \\mathbb{R}^{n}$ and penalty parameter $\\rho > 0$. In a learned iterative algorithm (algorithm unfolding), the $z$-subproblem is replaced by a learned shrinkage function applied elementwise, defined for a scalar input $v \\in \\mathbb{R}$ by\n$$\nS_{\\theta}(v) = \\alpha \\cdot \\mathrm{sign}(v) \\cdot \\max\\!\\big(|v| - \\tau,\\, 0\\big),\n$$\nwith learned parameters $\\theta = (\\alpha, \\tau)$, where $\\alpha > 0$ and $\\tau \\geq 0$.\n\nLet $n = m = 2$, and consider the specific instance with\n$$\nA = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}, \\qquad y = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\qquad \\lambda = 1, \\qquad \\rho = 1.\n$$\nInitialize the unrolled ADMM at\n$$\nx^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\qquad z^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\qquad u^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\nPerform two unrolled ADMM layers (iterations). In layer $k \\in \\{1,2\\}$, use the learned shrinkage parameters\n$$\n\\theta_{1} = \\big(\\alpha_{1}, \\tau_{1}\\big) = \\left(\\frac{9}{10}, \\frac{1}{4}\\right), \\qquad\n\\theta_{2} = \\big(\\alpha_{2}, \\tau_{2}\\big) = \\left(\\frac{11}{10}, \\frac{2}{5}\\right),\n$$\nand apply the learned shrinkage as $z^{(k)} = S_{\\theta_{k}}\\!\\big(x^{(k)} + u^{(k-1)}\\big)$, elementwise. The $x$-update and $u$-update are those implied by the ADMM method with the scaled dual variable for the above constrained formulation and penalty parameter $\\rho$.\n\nCompute the value of the squared Euclidean norm $\\|x^{(2)}\\|_{2}^{2}$ after completing these two layers, starting from the specified initial values. Express your final answer as a single real number in exact form. No rounding is required.", "solution": "The user wants to compute the squared Euclidean norm of the state variable $x^{(2)}$ after two iterations of a learned Alternating Direction Method of Multipliers (ADMM) algorithm.\n\nFirst, I will validate the problem statement.\n\n### Step 1: Extract Givens\n- **Optimization Problem**: $\\min_{x, z} \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|z\\|_{1}$ subject to $z = x$.\n- **Dimensions**: $n=2$, $m=2$.\n- **Data**:\n  - $A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$\n  - $y = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n- **Parameters**:\n  - $\\lambda = 1$\n  - $\\rho = 1$\n- **Initial Conditions**:\n  - $x^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n  - $z^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n  - $u^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n- **Algorithm**: Unrolled ADMM with scaled dual variables, for $k \\in \\{1, 2\\}$.\n- **Update Rules**:\n  - The $x$-update and $u$-update are standard ADMM updates.\n  - The $z$-update is a learned shrinkage: $z^{(k)} = S_{\\theta_{k}}(x^{(k)} + u^{(k-1)})$, applied elementwise.\n  - The learned shrinkage function is $S_{\\theta}(v) = \\alpha \\cdot \\mathrm{sign}(v) \\cdot \\max(|v| - \\tau, 0)$.\n- **Learned Parameters**:\n  - Layer 1 ($k=1$): $\\theta_{1} = (\\alpha_{1}, \\tau_{1}) = \\left(\\frac{9}{10}, \\frac{1}{4}\\right)$.\n  - Layer 2 ($k=2$): $\\theta_{2} = (\\alpha_{2}, \\tau_{2}) = \\left(\\frac{11}{10}, \\frac{2}{5}\\right)$.\n- **Objective**: Compute $\\|x^{(2)}\\|_{2}^{2}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the fields of sparse optimization, compressed sensing, and machine learning (specifically, algorithm unfolding). The formulation of ADMM and the learned shrinkage function are standard in this context. The problem is well-posed, with all necessary data, initial conditions, and update rules provided to uniquely determine the result. The language is precise and objective. There are no contradictions or ambiguities. The problem is valid.\n\n### Step 3: Proceed to Solution\n\nThe ADMM updates for the scaled dual variable $u$ are as follows, where $k$ is the iteration index:\n1.  **$x$-update**: $x^{(k)} = \\arg\\min_{x} \\left( \\frac{1}{2}\\|Ax - y\\|_{2}^{2} + \\frac{\\rho}{2}\\|x - z^{(k-1)} + u^{(k-1)}\\|_{2}^{2} \\right)$.\n    The first-order optimality condition yields a linear system:\n    $A^T(Ax - y) + \\rho(x - z^{(k-1)} + u^{(k-1)}) = 0$\n    $(A^T A + \\rho I)x = A^T y + \\rho(z^{(k-1)} - u^{(k-1)})$\n    $x^{(k)} = (A^T A + \\rho I)^{-1} (A^T y + \\rho(z^{(k-1)} - u^{(k-1)}))$\n\n2.  **$z$-update**: The problem specifies a learned update rule:\n    $z^{(k)} = S_{\\theta_k}(x^{(k)} + u^{(k-1)})$\n\n3.  **$u$-update**: $u^{(k)} = u^{(k-1)} + x^{(k)} - z^{(k)}$.\n\nFirst, I pre-compute the constant parts of the $x$-update.\nGiven $A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$ and $\\rho = 1$:\n$A^T A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} = \\begin{pmatrix} 5 & 4 \\\\ 4 & 5 \\end{pmatrix}$.\n$A^T A + \\rho I = \\begin{pmatrix} 5 & 4 \\\\ 4 & 5 \\end{pmatrix} + 1 \\cdot \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 6 & 4 \\\\ 4 & 6 \\end{pmatrix}$.\nThe inverse is:\n$(A^T A + \\rho I)^{-1} = \\frac{1}{6 \\cdot 6 - 4 \\cdot 4} \\begin{pmatrix} 6 & -4 \\\\ -4 & 6 \\end{pmatrix} = \\frac{1}{20} \\begin{pmatrix} 6 & -4 \\\\ -4 & 6 \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 3 & -2 \\\\ -2 & 3 \\end{pmatrix}$.\n\nThe term $A^T y$ is:\n$A^T y = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$.\n\nThe initial conditions are $x^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, $z^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, and $u^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\n**Layer 1 ($k=1$):**\n\n1.  **Compute $x^{(1)}$**:\n    $x^{(1)} = (A^T A + \\rho I)^{-1} (A^T y + \\rho(z^{(0)} - u^{(0)}))$\n    Since $z^{(0)} = u^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, the expression simplifies to:\n    $x^{(1)} = (A^T A + \\rho I)^{-1} A^T y = \\frac{1}{10} \\begin{pmatrix} 3 & -2 \\\\ -2 & 3 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 3(2) - 2(1) \\\\ -2(2) + 3(1) \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 4 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{5} \\\\ -\\frac{1}{10} \\end{pmatrix}$.\n\n2.  **Compute $z^{(1)}$**:\n    The learned update uses $\\theta_1 = (\\alpha_1, \\tau_1) = (\\frac{9}{10}, \\frac{1}{4})$.\n    The input to the shrinkage function is $v^{(1)} = x^{(1)} + u^{(0)} = \\begin{pmatrix} \\frac{2}{5} \\\\ -\\frac{1}{10} \\end{pmatrix}$.\n    We apply $S_{\\theta_1}$ elementwise:\n    For the first component, $v_1 = \\frac{2}{5} = 0.4$. The threshold is $\\tau_1 = \\frac{1}{4} = 0.25$.\n    Since $|v_1| > \\tau_1$, the first component of $z^{(1)}$ is:\n    $z_1^{(1)} = \\alpha_1 \\cdot \\mathrm{sign}(v_1) \\cdot (|v_1| - \\tau_1) = \\frac{9}{10} \\cdot 1 \\cdot \\left(\\frac{2}{5} - \\frac{1}{4}\\right) = \\frac{9}{10} \\cdot \\left(\\frac{8-5}{20}\\right) = \\frac{9}{10} \\cdot \\frac{3}{20} = \\frac{27}{200}$.\n    For the second component, $v_2 = -\\frac{1}{10} = -0.1$.\n    Since $|v_2| = 0.1 < \\tau_1 = 0.25$, the second component of $z^{(1)}$ is:\n    $z_2^{(1)} = \\alpha_1 \\cdot \\mathrm{sign}(v_2) \\cdot \\max(|v_2| - \\tau_1, 0) = \\frac{9}{10} \\cdot (-1) \\cdot 0 = 0$.\n    So, $z^{(1)} = \\begin{pmatrix} \\frac{27}{200} \\\\ 0 \\end{pmatrix}$.\n\n3.  **Compute $u^{(1)}$**:\n    $u^{(1)} = u^{(0)} + x^{(1)} - z^{(1)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} \\frac{2}{5} \\\\ -\\frac{1}{10} \\end{pmatrix} - \\begin{pmatrix} \\frac{27}{200} \\\\ 0 \\end{pmatrix}$.\n    To subtract, we use a common denominator of $200$: $\\frac{2}{5} = \\frac{80}{200}$ and $-\\frac{1}{10} = -\\frac{20}{200}$.\n    $u^{(1)} = \\begin{pmatrix} \\frac{80}{200} - \\frac{27}{200} \\\\ -\\frac{20}{200} - 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{53}{200} \\\\ -\\frac{20}{200} \\end{pmatrix} = \\begin{pmatrix} \\frac{53}{200} \\\\ -\\frac{1}{10} \\end{pmatrix}$.\n\n**Layer 2 ($k=2$):**\n\n1.  **Compute $x^{(2)}$**:\n    $x^{(2)} = (A^T A + \\rho I)^{-1} (A^T y + \\rho(z^{(1)} - u^{(1)}))$.\n    First, we calculate the term $z^{(1)} - u^{(1)}$:\n    $z^{(1)} - u^{(1)} = \\begin{pmatrix} \\frac{27}{200} \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} \\frac{53}{200} \\\\ -\\frac{1}{10} \\end{pmatrix} = \\begin{pmatrix} \\frac{27-53}{200} \\\\ 0 - (-\\frac{1}{10}) \\end{pmatrix} = \\begin{pmatrix} -\\frac{26}{200} \\\\ \\frac{1}{10} \\end{pmatrix} = \\begin{pmatrix} -\\frac{13}{100} \\\\ \\frac{1}{10} \\end{pmatrix}$.\n    Next, we compute the vector to be multiplied by the matrix:\n    $A^T y + \\rho(z^{(1)} - u^{(1)}) = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} + 1 \\cdot \\begin{pmatrix} -\\frac{13}{100} \\\\ \\frac{1}{10} \\end{pmatrix} = \\begin{pmatrix} 2 - \\frac{13}{100} \\\\ 1 + \\frac{1}{10} \\end{pmatrix} = \\begin{pmatrix} \\frac{200-13}{100} \\\\ \\frac{10+1}{10} \\end{pmatrix} = \\begin{pmatrix} \\frac{187}{100} \\\\ \\frac{11}{10} \\end{pmatrix}$.\n    Now, we compute $x^{(2)}$:\n    $x^{(2)} = \\frac{1}{10} \\begin{pmatrix} 3 & -2 \\\\ -2 & 3 \\end{pmatrix} \\begin{pmatrix} \\frac{187}{100} \\\\ \\frac{11}{10} \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 3(\\frac{187}{100}) - 2(\\frac{11}{10}) \\\\ -2(\\frac{187}{100}) + 3(\\frac{11}{10}) \\end{pmatrix}$.\n    Let's work with a common denominator of $100$ inside the vector: $\\frac{11}{10} = \\frac{110}{100}$.\n    $x^{(2)} = \\frac{1}{10} \\begin{pmatrix} \\frac{3(187) - 2(110)}{100} \\\\ \\frac{-2(187) + 3(110)}{100} \\end{pmatrix} = \\frac{1}{1000} \\begin{pmatrix} 561 - 220 \\\\ -374 + 330 \\end{pmatrix} = \\frac{1}{1000} \\begin{pmatrix} 341 \\\\ -44 \\end{pmatrix} = \\begin{pmatrix} \\frac{341}{1000} \\\\ -\\frac{44}{1000} \\end{pmatrix}$.\n\n**Final Calculation:**\n\nThe goal is to compute $\\|x^{(2)}\\|_{2}^{2}$.\n$\\|x^{(2)}\\|_{2}^{2} = \\left(\\frac{341}{1000}\\right)^2 + \\left(-\\frac{44}{1000}\\right)^2 = \\frac{341^2 + (-44)^2}{1000^2}$.\n$341^2 = 116281$.\n$(-44)^2 = 44^2 = 1936$.\n$341^2 + 44^2 = 116281 + 1936 = 118217$.\n$1000^2 = 1000000$.\nTherefore, $\\|x^{(2)}\\|_{2}^{2} = \\frac{118217}{1000000}$.\nThis can also be written as $0.118217$.", "answer": "$$ \\boxed{\\frac{118217}{1000000}} $$", "id": "3456561"}, {"introduction": "Approximate Message Passing (AMP) algorithms represent a class of high-performance solvers for certain statistical estimation problems, distinguished by their use of a special state-evolution analysis. This practice focuses on a key and unique feature of AMP: the Onsager correction term, which acts to decorrelate the iterates from the measurement matrix, improving convergence. By performing a single layer of a Learned AMP (LAMP) algorithm, you will gain tangible experience with the structure of these advanced methods and see how model-based corrections are integrated into a learnable architecture.", "problem": "Consider the linear inverse problem in compressed sensing with measurement model $y = A x_{\\star} + w$, where $A \\in \\mathbb{R}^{m \\times n}$ has independent and identically distributed $\\mathcal{N}(0,1/m)$ entries, $x_{\\star} \\in \\mathbb{R}^{n}$ is $k$-sparse, and $w \\in \\mathbb{R}^{m}$ is additive noise. You will perform one layer of the Learned Approximate Message Passing (LAMP) algorithm (Approximate Message Passing (AMP) with learned parameters) starting from the zero vector. Use the following concrete instance:\n- $m = 3$, $n = 4$.\n- \n$$\nA = \n\\begin{pmatrix}\n0.62 & -0.17 & 0.35 & 0.08 \\\\\n-0.44 & 0.59 & -0.28 & 0.49 \\\\\n0.22 & 0.31 & 0.07 & -0.53\n\\end{pmatrix}.\n$$\n- The true signal is $x_{\\star} = (1.5,\\, 0,\\, 0,\\, -0.8)^{\\top}$ and the noise is $w = (0.05,\\,-0.02,\\,0.03)^{\\top}$, so that $y = A x_{\\star} + w$.\n- Initialize $x^{0} = 0$ and $r^{0} = y$.\n- Use a learned scalar step matrix $B^{0} = \\alpha I$ with $\\alpha = 0.9$.\n- Use a learned threshold scaling $\\lambda = 1.0$ and the residual-based noise estimate $\\sigma_{0} = \\|r^{0}\\|_{2}/\\sqrt{m}$, so the threshold is $\\theta_{0} = \\lambda \\sigma_{0}$.\n- Use elementwise soft-thresholding $\\eta(u;\\theta) = \\mathrm{sign}(u)\\,\\max(|u|-\\theta,\\,0)$ with weak derivative $\\eta'(u;\\theta) = \\mathbf{1}\\{|u| > \\theta\\}$.\n\nA single LAMP layer $t=0 \\to 1$ is defined by\n- The linear step $u^{0} = x^{0} + B^{0} A^{\\top} r^{0}$.\n- The denoising step $x^{1} = \\eta(u^{0}; \\theta_{0})$.\n- The Onsager coefficient $b_{0} = \\frac{1}{m} \\sum_{i=1}^{n} \\eta'(u^{0}_{i}; \\theta_{0})$.\n- The residual update $r^{1} = y - A x^{1} + b_{0} r^{0}$.\n\nCompute the first component of the updated residual $r^{1}$ numerically for the specified instance, including the Onsager term. Round your answer to six significant figures. No units are required.", "solution": "The user wants to compute the first component of the updated residual, $r_1^1$, after one iteration of the Learned Approximate Message Passing (LAMP) algorithm. The update equations for the state variables from iteration $t=0$ to $t=1$ are provided. We will follow these steps sequentially, using the given numerical values.\n\nThe problem specifies the following quantities:\n- Dimensions: $m = 3$, $n = 4$.\n- Sensing matrix: $A = \\begin{pmatrix} 0.62 & -0.17 & 0.35 & 0.08 \\\\ -0.44 & 0.59 & -0.28 & 0.49 \\\\ 0.22 & 0.31 & 0.07 & -0.53 \\end{pmatrix}$.\n- True signal: $x_{\\star} = (1.5, 0, 0, -0.8)^{\\top}$.\n- Additive noise: $w = (0.05, -0.02, 0.03)^{\\top}$.\n- Initial signal estimate: $x^{0} = (0, 0, 0, 0)^{\\top}$.\n- Learned parameters: step size $\\alpha = 0.9$ (so $B^0 = 0.9I$) and threshold scaling $\\lambda = 1.0$.\n\nThe goal is to compute the first component of $r^1$, where the updated residual vector $r^1$ is given by:\n$$r^{1} = y - A x^{1} + b_{0} r^{0}$$\nWe will compute each term in this expression.\n\n**Step 1: Compute the measurement vector $y$ and the initial residual $r^0$.**\nThe measurement model is $y = A x_{\\star} + w$.\nFirst, we compute the product $A x_{\\star}$:\n$$\nA x_{\\star} = \\begin{pmatrix} 0.62 & -0.17 & 0.35 & 0.08 \\\\ -0.44 & 0.59 & -0.28 & 0.49 \\\\ 0.22 & 0.31 & 0.07 & -0.53 \\end{pmatrix} \\begin{pmatrix} 1.5 \\\\ 0 \\\\ 0 \\\\ -0.8 \\end{pmatrix} = \\begin{pmatrix} (0.62)(1.5) + (-0.17)(0) + (0.35)(0) + (0.08)(-0.8) \\\\ (-0.44)(1.5) + (0.59)(0) + (-0.28)(0) + (0.49)(-0.8) \\\\ (0.22)(1.5) + (0.31)(0) + (0.07)(0) + (-0.53)(-0.8) \\end{pmatrix}\n$$\n$$\nA x_{\\star} = \\begin{pmatrix} 0.93 - 0.064 \\\\ -0.66 - 0.392 \\\\ 0.33 + 0.424 \\end{pmatrix} = \\begin{pmatrix} 0.866 \\\\ -1.052 \\\\ 0.754 \\end{pmatrix}\n$$\nNow, we add the noise vector $w$:\n$$\ny = A x_{\\star} + w = \\begin{pmatrix} 0.866 \\\\ -1.052 \\\\ 0.754 \\end{pmatrix} + \\begin{pmatrix} 0.05 \\\\ -0.02 \\\\ 0.03 \\end{pmatrix} = \\begin{pmatrix} 0.916 \\\\ -1.072 \\\\ 0.784 \\end{pmatrix}\n$$\nThe initial residual is $r^{0} = y$, so $r^0 = (0.916, -1.072, 0.784)^{\\top}$.\n\n**Step 2: Compute the threshold $\\theta_0$.**\nThe threshold is defined as $\\theta_{0} = \\lambda \\sigma_{0}$, where $\\sigma_{0} = \\|r^{0}\\|_{2}/\\sqrt{m}$.\nFirst, calculate the squared Euclidean norm of $r^0$:\n$$\n\\|r^{0}\\|_{2}^2 = (0.916)^2 + (-1.072)^2 + (0.784)^2 = 0.839056 + 1.149184 + 0.614656 = 2.602896\n$$\nThe noise estimate $\\sigma_0$ is:\n$$\n\\sigma_{0} = \\frac{\\sqrt{2.602896}}{\\sqrt{3}} = \\sqrt{\\frac{2.602896}{3}} = \\sqrt{0.867632} \\approx 0.93146766\n$$\nWith $\\lambda = 1.0$, the threshold is $\\theta_{0} = \\lambda \\sigma_{0} = \\sigma_{0} \\approx 0.93146766$.\n\n**Step 3: Compute the linear step $u^0$.**\nThe linear step is $u^{0} = x^{0} + B^{0} A^{\\top} r^{0}$. Given $x^0 = 0$ and $B^0 = \\alpha I = 0.9I$, this simplifies to $u^{0} = 0.9 A^{\\top} r^{0}$.\nFirst, we compute the product $A^{\\top} r^{0}$:\n$$\nA^{\\top} r^{0} = \\begin{pmatrix} 0.62 & -0.44 & 0.22 \\\\ -0.17 & 0.59 & 0.31 \\\\ 0.35 & -0.28 & 0.07 \\\\ 0.08 & 0.49 & -0.53 \\end{pmatrix} \\begin{pmatrix} 0.916 \\\\ -1.072 \\\\ 0.784 \\end{pmatrix}\n$$\n$$\nA^{\\top} r^{0} = \\begin{pmatrix} (0.62)(0.916) + (-0.44)(-1.072) + (0.22)(0.784) \\\\ (-0.17)(0.916) + (0.59)(-1.072) + (0.31)(0.784) \\\\ (0.35)(0.916) + (-0.28)(-1.072) + (0.07)(0.784) \\\\ (0.08)(0.916) + (0.49)(-1.072) + (-0.53)(0.784) \\end{pmatrix} = \\begin{pmatrix} 0.56792 + 0.47168 + 0.17248 \\\\ -0.15572 - 0.63248 + 0.24304 \\\\ 0.3206 + 0.30016 + 0.05488 \\\\ 0.07328 - 0.52528 - 0.41552 \\end{pmatrix} = \\begin{pmatrix} 1.21208 \\\\ -0.54516 \\\\ 0.67564 \\\\ -0.86752 \\end{pmatrix}\n$$\nNow, we multiply by $\\alpha=0.9$:\n$$\nu^{0} = 0.9 \\begin{pmatrix} 1.21208 \\\\ -0.54516 \\\\ 0.67564 \\\\ -0.86752 \\end{pmatrix} = \\begin{pmatrix} 1.090872 \\\\ -0.490644 \\\\ 0.608076 \\\\ -0.780768 \\end{pmatrix}\n$$\n\n**Step 4: Compute the denoised signal estimate $x^1$.**\nThe denoising step is $x^{1} = \\eta(u^{0}; \\theta_{0})$, where $\\eta$ is the element-wise soft-thresholding function $\\eta(u;\\theta) = \\mathrm{sign}(u)\\,\\max(|u|-\\theta,\\,0)$. We apply this to each component of $u^0$ with the threshold $\\theta_0 \\approx 0.93146766$.\n- For $u^0_1 = 1.090872$: $|u^0_1| > \\theta_0$. So, $x^1_1 = u^0_1 - \\theta_0 = 1.090872 - 0.93146766 = 0.15940434$.\n- For $u^0_2 = -0.490644$: $|u^0_2| < \\theta_0$. So, $x^1_2 = 0$.\n- For $u^0_3 = 0.608076$: $|u^0_3| < \\theta_0$. So, $x^1_3 = 0$.\n- For $u^0_4 = -0.780768$: $|u^0_4| < \\theta_0$. So, $x^1_4 = 0$.\nThus, the updated signal estimate is $x^1 = (0.15940434, 0, 0, 0)^{\\top}$.\n\n**Step 5: Compute the Onsager coefficient $b_0$.**\nThe Onsager coefficient is $b_{0} = \\frac{1}{m} \\sum_{i=1}^{n} \\eta'(u^{0}_{i}; \\theta_{0})$, where $\\eta'(u;\\theta) = \\mathbf{1}\\{|u| > \\theta\\}$.\nWe check the condition $|u_i^0| > \\theta_0$ for each component:\n- $|u^0_1| = 1.090872 > \\theta_0 \\implies \\eta'(u^0_1; \\theta_0) = 1$.\n- $|u^0_2| = 0.490644 < \\theta_0 \\implies \\eta'(u^0_2; \\theta_0) = 0$.\n- $|u^0_3| = 0.608076 < \\theta_0 \\implies \\eta'(u^0_3; \\theta_0) = 0$.\n- $|u^0_4| = 0.780768 < \\theta_0 \\implies \\eta'(u^0_4; \\theta_0) = 0$.\nThe sum is $\\sum_{i=1}^4 \\eta'(u^0_i; \\theta_0) = 1 + 0 + 0 + 0 = 1$. With $m=3$, the coefficient is:\n$$\nb_0 = \\frac{1}{3} \\times 1 = \\frac{1}{3}\n$$\n\n**Step 6: Compute the first component of the updated residual, $r^1_1$.**\nThe formula for the first component is $r^{1}_1 = y_1 - (A x^{1})_1 + b_{0} r^{0}_1$.\nWe have:\n- $y_1 = 0.916$.\n- $r^0_1 = 0.916$.\n- $b_0 = 1/3$.\nWe need to compute $(A x^{1})_1$. Since $x^1_2 = x^1_3 = x^1_4 = 0$, this simplifies significantly:\n$$\n(A x^{1})_1 = A_{11} x^1_1 + A_{12} x^1_2 + A_{13} x^1_3 + A_{14} x^1_4 = (0.62)(x^1_1) + 0 + 0 + 0\n$$\n$$\n(A x^{1})_1 = 0.62 \\times 0.15940434 \\approx 0.09883069\n$$\nNow, we substitute all values into the equation for $r^1_1$:\n$$\nr^1_1 = 0.916 - 0.09883069 + \\frac{1}{3} (0.916)\n$$\nWe can group the terms with $0.916$:\n$$\nr^1_1 = 0.916 \\left(1 + \\frac{1}{3}\\right) - 0.09883069 = 0.916 \\left(\\frac{4}{3}\\right) - 0.09883069\n$$\n$$\nr^1_1 \\approx 1.22133333 - 0.09883069 = 1.12250264\n$$\nThe problem requires rounding the answer to six significant figures. The value is $1.12250264...$. The first six significant digits are $1$, $1$, $2$, $2$, $5$, $0$. The seventh digit is $2$, so we round down.\n\nThe final numerical value is $1.12250$.", "answer": "$$\n\\boxed{1.12250}\n$$", "id": "3456609"}, {"introduction": "While the forward pass defines how an unfolded algorithm performs inference, its practical success hinges on its trainability, which is governed by the backward pass of gradients. Like other deep architectures, unfolded algorithms can suffer from vanishing or exploding gradients, hindering learning. This exercise moves from implementation to design, guiding you to theoretically analyze this problem by linking gradient decay to the contraction properties of the learned operators, and then to architect a solution using skip connections to ensure stable training.", "problem": "Consider the sparse optimization objective in compressed sensing defined by the function $F(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1}$, where $A \\in \\mathbb{R}^{m \\times n}$ is the sensing matrix, $y \\in \\mathbb{R}^{m}$ is the measurement vector, and $\\lambda > 0$ is a regularization parameter. A classical model-based iterative method to minimize $F$ is the proximal gradient scheme, whose update has the form $x^{k+1} = \\mathrm{S}_{\\alpha \\lambda}\\!\\left(x^{k} - \\alpha A^{\\top}(A x^{k} - y)\\right)$, where $\\alpha > 0$ is a stepsize and $\\mathrm{S}_{\\tau}(\\cdot)$ denotes the elementwise soft-thresholding operator at level $\\tau$. In learned iterative algorithms and algorithm unfolding, one often replaces the hand-crafted linear maps by learned ones and unties parameters across layers while preserving the proximal skeleton, leading to updates of the form\n$$\nx^{k+1} = \\mathrm{S}_{\\lambda}\\!\\left(W x^{k} + B y\\right), \\quad k = 0,1,\\ldots,K-1,\n$$\nwhere $W \\in \\mathbb{R}^{n \\times n}$ and $B \\in \\mathbb{R}^{n \\times m}$ are learned matrices constrained to reflect the model-based structure (e.g., $W \\approx I - \\alpha A^{\\top}A$ and $B \\approx \\alpha A^{\\top}$). Assume we work in a regime where the active support of the soft-thresholding operator is fixed across layers and its local derivative on the active coordinates is the identity. Under this local linearization, the per-layer Jacobian with respect to $x^{k}$ is approximately the matrix $W$ restricted to the active subspace. Let the spectral norm of $W$ on that subspace be a contraction factor $\\rho \\in (0,1)$, that is, $\\|W\\|_{2} = \\rho$.\n\nLet the training loss be $L(x^{K}) = \\frac{1}{2}\\|A x^{K} - y\\|_{2}^{2}$, and consider its gradient backpropagation through the $K$ unfolded layers. Using only core definitions of contraction mappings, spectral norms, and the chain rule, perform the following:\n\n1. Derive an upper bound on the gradient norm decay from layer $K$ to layer $0$ under the stated linearization assumptions, expressed in terms of $K$ and $\\rho$. Your answer must be a single analytical expression for the bound as a function of $K$ and $\\rho$.\n\n2. Propose a skip-connection-based relaxation that preserves the model-based fixed points by forming a convex combination between the current iterate and the model-based proximal update,\n$$\nx^{k+1} = (1 - \\eta)\\, x^{k} + \\eta\\, \\mathrm{S}_{\\lambda}\\!\\left(W x^{k} + B y\\right),\n$$\nwith a scalar relaxation parameter $\\eta > 0$ that is shared across layers. Under the same local linearization assumptions and treating the spectrum of $W$ on the active subspace as lying in $[0,\\rho]$, determine a closed-form choice $\\eta^{\\star}(\\rho)$ that improves gradient flow by maximizing, subject to the forward map remaining a contraction, the worst-case per-layer backpropagation gain at the endpoints of the spectral interval. Express $\\eta^{\\star}(\\rho)$ as a single closed-form analytical expression depending only on $\\rho$.\n\nYour final answer must consist of the pair comprising the gradient decay bound from part 1 and the relaxation parameter from part 2, formatted as a single row matrix. No numerical evaluation is required. If at any point you make an approximation, clearly justify it in your reasoning. The final answer must be expressed without units.", "solution": "The problem is well-posed, scientifically grounded, and internally consistent. It presents a standard but non-trivial question within the domain of learned iterative algorithms for sparse optimization, a subfield of signal processing and machine learning. All necessary assumptions and definitions are provided to derive the requested quantities. We may therefore proceed with the solution.\n\n**Part 1: Gradient Decay Bound**\n\nThe core of the problem lies in analyzing the flow of gradients through the unfolded iterative algorithm. The algorithm is defined by the recurrence relation for $k = 0, 1, \\ldots, K-1$:\n$$\nx^{k+1} = f_k(x^k) = \\mathrm{S}_{\\lambda}\\!\\left(W x^{k} + B y\\right)\n$$\nThe total transformation from the initial state $x^0$ to the final state $x^K$ is the composition of these $K$ functions: $x^K = f_{K-1} \\circ \\cdots \\circ f_0 (x^0)$. The loss function is $L(x^K)$. We are interested in the relationship between the gradient of the loss with respect to the initial state, $\\nabla_{x^0} L = \\frac{\\partial L}{\\partial x^0}$, and the gradient with respect to the final state, $\\nabla_{x^K} L = \\frac{\\partial L}{\\partial x^K}$.\n\nUsing the chain rule, the gradient at layer $k$ can be related to the gradient at layer $k+1$ as follows:\n$$\n\\nabla_{x^k} L = \\left(\\frac{\\partial x^{k+1}}{\\partial x^k}\\right)^{\\top} \\nabla_{x^{k+1}} L\n$$\nLet $J_k = \\frac{\\partial x^{k+1}}{\\partial x^k}$ be the Jacobian matrix of the layer-$k$ transformation. The problem states a key linearization assumption: the active support of the soft-thresholding operator $\\mathrm{S}_{\\lambda}(\\cdot)$ is fixed, and its derivative on this support is the identity. Under this assumption, the Jacobian $J_k$ is locally equivalent to the learned matrix $W$ restricted to the active subspace. The problem further specifies that the spectral norm of this effective operator is a contraction factor $\\rho \\in (0,1)$, that is, $\\|J_k\\|_2 = \\rho$.\n\nApplying the submultiplicative property of matrix norms to the backpropagation equation, we obtain a bound on the norm of the gradient:\n$$\n\\|\\nabla_{x^k} L\\|_2 \\le \\left\\|J_k^{\\top}\\right\\|_2 \\|\\nabla_{x^{k+1}} L\\|_2\n$$\nA fundamental property of the spectral norm is that $\\|A^{\\top}\\|_2 = \\|A\\|_2$. Therefore, $\\|J_k^{\\top}\\|_2 = \\|J_k\\|_2 = \\rho$. This gives us the per-layer gradient norm decay relationship:\n$$\n\\|\\nabla_{x^k} L\\|_2 \\le \\rho \\|\\nabla_{x^{k+1}} L\\|_2\n$$\nTo find the total decay from layer $K$ to layer $0$, we unroll this inequality recursively:\n$$\n\\|\\nabla_{x^{K-1}} L\\|_2 \\le \\rho \\|\\nabla_{x^K} L\\|_2\n$$\n$$\n\\|\\nabla_{x^{K-2}} L\\|_2 \\le \\rho \\|\\nabla_{x^{K-1}} L\\|_2 \\le \\rho (\\rho \\|\\nabla_{x^K} L\\|_2) = \\rho^2 \\|\\nabla_{x^K} L\\|_2\n$$\nContinuing this process for all $K$ layers, we arrive at the relationship between the gradient at layer $0$ and layer $K$:\n$$\n\\|\\nabla_{x^0} L\\|_2 \\le \\rho^K \\|\\nabla_{x^K} L\\|_2\n$$\nThe problem asks for an upper bound on the gradient norm decay, which is the ratio $\\frac{\\|\\nabla_{x^0} L\\|_2}{\\|\\nabla_{x^K} L\\|_2}$. From the inequality above, this ratio is bounded as:\n$$\n\\frac{\\|\\nabla_{x^0} L\\|_2}{\\|\\nabla_{x^K} L\\|_2} \\le \\rho^K\n$$\nThus, the upper bound on the gradient norm decay over $K$ layers is $\\rho^K$.\n\n**Part 2: Optimal Relaxation Parameter**\n\nWe now consider the modified update rule incorporating a skip connection with a relaxation parameter $\\eta > 0$:\n$$\nx^{k+1} = (1 - \\eta)\\, x^{k} + \\eta\\, \\mathrm{S}_{\\lambda}\\!\\left(W x^{k} + B y\\right)\n$$\nTo analyze its properties, we first compute its Jacobian, $J_{\\eta} = \\frac{\\partial x^{k+1}}{\\partial x^k}$, under the same local linearization assumption as in Part 1. The derivative of $\\mathrm{S}_{\\lambda}(\\cdot)$ on the active support is the identity, so we have:\n$$\nJ_{\\eta} = (1 - \\eta)I + \\eta W\n$$\nHere, $I$ and $W$ are operators on the active subspace. The problem states that the spectrum of $W$ on this subspace, let's call it $\\sigma(W)$, lies in the real interval $[0, \\rho]$. This implies that $W$ acts as a symmetric positive semi-definite operator on this subspace. Consequently, $J_{\\eta}$ is also symmetric, and its eigenvalues are given by $\\lambda_i(J_{\\eta}) = (1 - \\eta) + \\eta \\sigma_i(W)$ for each $\\sigma_i(W) \\in \\sigma(W)$. The spectrum of $J_{\\eta}$ is therefore contained in the interval $[1-\\eta, 1-\\eta+\\eta\\rho]$.\n\nThe first constraint is that the forward map remains a contraction, i.e., $\\|J_{\\eta}\\|_2 < 1$. For a symmetric matrix, the spectral norm is the largest absolute eigenvalue. Thus, we require:\n$$\n\\|J_{\\eta}\\|_2 = \\max\\left(|1-\\eta|, |1-\\eta+\\eta\\rho|\\right) < 1\n$$\nGiven $\\eta > 0$ and $\\rho \\in (0,1)$, we have $1-\\rho > 0$, so $1-\\eta+\\eta\\rho = 1-\\eta(1-\\rho) < 1$. The condition $|1-\\eta+\\eta\\rho|<1$ becomes $-1 < 1-\\eta+\\eta\\rho$, which simplifies to $2 > \\eta(1-\\rho)$, or $\\eta < \\frac{2}{1-\\rho}$. The other condition, $|1-\\eta|<1$, implies $-1 < 1-\\eta < 1$, which gives $0 < \\eta < 2$. Since $\\frac{2}{1-\\rho} > 2$, the effective constraint on $\\eta$ is $0 < \\eta < 2$.\n\nThe objective is to maximize the \"worst-case per-layer backpropagation gain at the endpoints of the spectral interval\". The backpropagation operator is $J_{\\eta}^{\\top} = J_{\\eta}$. Its eigenvalues determine the gain for different gradient components. The worst-case gain corresponds to the eigenvalue with the minimum magnitude. The eigenvalues at the endpoints of the spectral interval of $J_{\\eta}$ are $1-\\eta$ and $1-\\eta+\\eta\\rho$. Therefore, we aim to solve the following optimization problem:\n$$\n\\eta^{\\star} = \\arg\\max_{0 < \\eta < 2} \\min\\left(|1-\\eta|, |1-\\eta+\\eta\\rho|\\right)\n$$\nThe maximum of the minimum of two functions is typically found at their intersection, where their values are equal. We set the magnitudes of the two endpoint eigenvalues to be equal:\n$$\n|1-\\eta| = |1-\\eta+\\eta\\rho|\n$$\nThis leads to two possibilities:\n1. $1-\\eta = 1-\\eta+\\eta\\rho \\implies \\eta\\rho=0 \\implies \\eta=0$, which is outside our domain $(0,2)$.\n2. $1-\\eta = -(1-\\eta+\\eta\\rho) \\implies 1-\\eta = -1+\\eta-\\eta\\rho \\implies 2 = 2\\eta-\\eta\\rho = \\eta(2-\\rho)$.\n\nSolving for $\\eta$ in the second case gives:\n$$\n\\eta^{\\star} = \\frac{2}{2-\\rho}\n$$\nWe must verify that this solution lies within the valid domain $\\eta \\in (0,2)$. Since $\\rho \\in (0,1)$, the denominator $2-\\rho$ is in $(1,2)$. Therefore, $\\eta^{\\star} = \\frac{2}{2-\\rho}$ is in the interval $(\\frac{2}{2}, \\frac{2}{1}) = (1, 2)$. This falls within our constraint $0 < \\eta < 2$. At this optimal value, the magnitudes of the endpoint eigenvalues are balanced, which maximizes the minimum gain and thus improves the conditioning of the backpropagation step.\n\nThe final answer consists of the bound from Part 1 and the optimal parameter from Part 2.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\rho^{K} & \\frac{2}{2-\\rho}\n\\end{pmatrix}\n}\n$$", "id": "3456587"}]}