{"hands_on_practices": [{"introduction": "The analysis sparsity model provides a powerful framework for signals that are not sparse themselves but have a sparse representation after being transformed by an analysis operator $\\Omega$. A fundamental example is total variation (TV) denoising, where $\\Omega$ is a finite difference operator, promoting piecewise-constant solutions. This exercise provides hands-on practice in solving a core analysis sparsity problem from first principles, using subdifferential calculus to find and rigorously justify the unique solution for a simple signal [@problem_id:3430855].", "problem": "Consider the analysis sparse model in which an analysis operator $\\Omega$ encourages structured sparsity in $\\Omega \\mathbf{x}$. Let $\\Omega \\in \\mathbb{R}^{(n-1)\\times n}$ be the one-dimensional ($1$D) forward finite difference operator defined by $(\\Omega \\mathbf{x})_{i} = x_{i+1} - x_{i}$ for $i \\in \\{1,\\dots,n-1\\}$, and let the sensing matrix be the identity $A = I \\in \\mathbb{R}^{n \\times n}$. Given a piecewise constant observation $\\mathbf{y} \\in \\mathbb{R}^{n}$ and regularization parameter $\\lambda  0$, consider the convex optimization problem (one-dimensional Total Variation (TV) denoising in analysis form)\n$$\n\\min_{\\mathbf{x} \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|\\mathbf{x} - \\mathbf{y}\\|_{2}^{2} + \\lambda \\|\\Omega \\mathbf{x}\\|_{1}.\n$$\nLet $n = 6$, choose\n$$\n\\mathbf{y} = \\begin{pmatrix} 0  0  5  5  0  0 \\end{pmatrix}^{\\top},\n$$\nand set $\\lambda = 4$. Determine the unique minimizer $\\mathbf{x}^{\\star}$ of the problem above and justify its optimality using a rigorous argument grounded in first principles of convex analysis and linear operators. Express your final answer as a single row vector. No rounding is required.", "solution": "The user wants to solve a one-dimensional Total Variation (TV) denoising problem.\n\n### Step 1: Problem Validation\n\n#### Extract Givens\n- **Objective Function**: Minimize $F(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{x} - \\mathbf{y}\\|_{2}^{2} + \\lambda \\|\\Omega \\mathbf{x}\\|_{1}$ over $\\mathbf{x} \\in \\mathbb{R}^{n}$.\n- **Dimension**: $n = 6$.\n- **Analysis Operator**: $\\Omega \\in \\mathbb{R}^{(n-1)\\times n}$ is the $1$D forward finite difference operator, defined by $(\\Omega \\mathbf{x})_{i} = x_{i+1} - x_{i}$ for $i \\in \\{1, \\dots, n-1\\}$. For $n=6$, this is a $5 \\times 6$ matrix.\n- **Sensing Matrix**: $A = I \\in \\mathbb{R}^{n \\times n}$ (the identity matrix), as implied by the data fidelity term $\\|\\mathbf{x} - \\mathbf{y}\\|_{2}^{2}$.\n- **Observation Vector**: $\\mathbf{y} = \\begin{pmatrix} 0  0  5  5  0  0 \\end{pmatrix}^{\\top} \\in \\mathbb{R}^{6}$.\n- **Regularization Parameter**: $\\lambda = 4$.\n- **Task**: Determine the unique minimizer $\\mathbf{x}^{\\star}$ and justify its optimality.\n\n#### Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is a standard instance of analysis-form sparse regularization, specifically $1$D Total Variation denoising. This is a fundamental and well-established problem in signal processing, statistics, and machine learning. The model is mathematically and scientifically sound.\n2.  **Well-Posed**: The objective function $F(\\mathbf{x})$ is the sum of two convex functions: $f(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{x} - \\mathbf{y}\\|_{2}^{2}$ and $g(\\mathbf{x}) = \\lambda \\|\\Omega \\mathbf{x}\\|_{1}$. The function $f(\\mathbf{x})$ is strictly convex. The sum of a strictly convex function and a convex function is strictly convex. A strictly convex function on $\\mathbb{R}^{n}$ has at most one minimizer. Since $F(\\mathbf{x})$ is also coercive (i.e., $F(\\mathbf{x}) \\to \\infty$ as $\\|\\mathbf{x}\\|_2 \\to \\infty$), a minimizer is guaranteed to exist. Therefore, the problem is well-posed and has a unique solution.\n3.  **Objective**: All terms, parameters, and conditions are defined with mathematical precision. The problem is objective and free of ambiguity.\n4.  **Completeness and Consistency**: The problem provides all necessary information ($n$, $\\mathbf{y}$, $\\lambda$, definitions of all operators and norms) to find a solution. There are no contradictory constraints.\n\n#### Verdict and Action\nThe problem is valid. I will proceed with the solution.\n\n### Step 2: Solution\n\nThe problem is to find the unique minimizer $\\mathbf{x}^{\\star}$ of the convex function:\n$$\nF(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{x} - \\mathbf{y}\\|_{2}^{2} + \\lambda \\|\\Omega \\mathbf{x}\\|_{1}\n$$\nwhere $n=6$, $\\mathbf{y} = \\begin{pmatrix} 0  0  5  5  0  0 \\end{pmatrix}^{\\top}$, and $\\lambda = 4$.\n\nA vector $\\mathbf{x}^{\\star} \\in \\mathbb{R}^{n}$ is the unique minimizer of the strictly convex function $F(\\mathbf{x})$ if and only if it satisfies the first-order optimality condition from convex analysis, which states that the zero vector must be an element of the subdifferential of $F$ at $\\mathbf{x}^{\\star}$:\n$$\n\\mathbf{0} \\in \\partial F(\\mathbf{x}^{\\star})\n$$\nThe objective function is a sum of a differentiable term, $f(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{x} - \\mathbf{y}\\|_{2}^{2}$, and a non-differentiable term, $g(\\mathbf{x}) = \\lambda \\|\\Omega \\mathbf{x}\\|_{1}$.\nUsing the sum rule for subdifferentials, we have:\n$$\n\\partial F(\\mathbf{x}) = \\nabla f(\\mathbf{x}) + \\partial g(\\mathbf{x})\n$$\nThe gradient of $f(\\mathbf{x})$ is $\\nabla f(\\mathbf{x}) = \\mathbf{x} - \\mathbf{y}$.\nThe subdifferential of $g(\\mathbf{x})$ is found using the chain rule for subdifferentials: $\\partial (\\lambda \\|\\Omega \\mathbf{x}\\|_{1}) = \\lambda \\Omega^{\\top} \\partial(\\|\\mathbf{u}\\|_{1})|_{\\mathbf{u}=\\Omega\\mathbf{x}}$.\n\nThe subdifferential of the $\\ell_1$-norm, $\\|\\mathbf{u}\\|_1$, at a point $\\mathbf{u} \\in \\mathbb{R}^{m}$ is the set of vectors $\\mathbf{v} \\in \\mathbb{R}^{m}$ such that for each component $i$:\n$$\nv_{i} = \\begin{cases} \\text{sign}(u_{i})  \\text{if } u_{i} \\neq 0 \\\\ \\in [-1, 1]  \\text{if } u_{i} = 0 \\end{cases}\n$$\nThis can be compactly written as $\\mathbf{v} \\in \\partial(\\|\\mathbf{u}\\|_1)$.\n\nThus, the optimality condition for $\\mathbf{x}^{\\star}$ is that there must exist a vector $\\mathbf{v} \\in \\mathbb{R}^{n-1}$ (here $n-1=5$) such that:\n1.  $\\mathbf{x}^{\\star} - \\mathbf{y} + \\lambda \\Omega^{\\top} \\mathbf{v} = \\mathbf{0}$\n2.  $\\mathbf{v} \\in \\partial(\\|\\Omega\\mathbf{x}^{\\star}\\|_1)$\n\nThe operator $\\Omega \\in \\mathbb{R}^{5 \\times 6}$ and its transpose $\\Omega^{\\top} \\in \\mathbb{R}^{6 \\times 5}$ are:\n$$\n\\Omega = \\begin{pmatrix}\n-1  1  0  0  0  0 \\\\\n0  -1  1  0  0  0 \\\\\n0  0  -1  1  0  0 \\\\\n0  0  0  -1  1  0 \\\\\n0  0  0  0  -1  1\n\\end{pmatrix}, \\quad\n\\Omega^{\\top} = \\begin{pmatrix}\n-1  0  0  0  0 \\\\\n1  -1  0  0  0 \\\\\n0  1  -1  0  0 \\\\\n0  0  1  -1  0 \\\\\n0  0  0  1  -1 \\\\\n0  0  0  0  1\n\\end{pmatrix}\n$$\nAn important property of $\\Omega^{\\top}$ is that the sum of the elements of any column of $\\Omega$ is $0$, which implies that the vector $\\mathbf{1} = \\begin{pmatrix} 1  \\dots  1 \\end{pmatrix}^\\top$ is in the null space of $\\Omega$. Similarly, the sum of components of any vector in the range of $\\Omega^{\\top}$ is zero: $\\mathbf{1}^{\\top}(\\Omega^{\\top}\\mathbf{v}) = (\\Omega \\mathbf{1})^{\\top}\\mathbf{v} = \\mathbf{0}^{\\top}\\mathbf{v} = 0$.\nFrom the optimality condition $\\mathbf{x}^{\\star} - \\mathbf{y} = -\\lambda\\Omega^{\\top}\\mathbf{v}$, we can deduce that $\\mathbf{1}^{\\top}(\\mathbf{x}^{\\star} - \\mathbf{y}) = 0$, which implies $\\sum_{i=1}^{n} x^{\\star}_{i} = \\sum_{i=1}^{n} y_{i}$.\nFor the given $\\mathbf{y}$, $\\sum_{i=1}^{6} y_i = 0+0+5+5+0+0 = 10$.\n\nLet us test the hypothesis that the solution $\\mathbf{x}^{\\star}$ is a constant vector, i.e., $\\mathbf{x}^{\\star} = c \\mathbf{1}$ for some scalar $c$. If this is the case, then from the sum property, $6c = 10$, which gives $c = 10/6 = 5/3$.\nLet's propose the candidate solution:\n$$\n\\mathbf{x}^{\\star} = \\begin{pmatrix} 5/3  5/3  5/3  5/3  5/3  5/3 \\end{pmatrix}^{\\top}\n$$\nFor this $\\mathbf{x}^{\\star}$, we have $\\Omega \\mathbf{x}^{\\star} = \\mathbf{0}$, since all adjacent elements are identical. The condition $\\mathbf{v} \\in \\partial(\\|\\Omega\\mathbf{x}^{\\star}\\|_1)$ simplifies to $\\|\\mathbf{v}\\|_{\\infty} \\le 1$, meaning $|v_i| \\le 1$ for all $i \\in \\{1, \\dots, 5\\}$.\n\nNow, we must check if there exists such a $\\mathbf{v}$ that also satisfies the first optimality condition:\n$\\mathbf{x}^{\\star} - \\mathbf{y} = -\\lambda \\Omega^{\\top} \\mathbf{v}$.\nWith $\\lambda = 4$, this is $\\mathbf{x}^{\\star} - \\mathbf{y} = -4 \\Omega^{\\top} \\mathbf{v}$.\nFirst, we compute the difference vector $\\mathbf{x}^{\\star} - \\mathbf{y}$:\n$$\n\\mathbf{x}^{\\star} - \\mathbf{y} =\n\\begin{pmatrix} 5/3 - 0 \\\\ 5/3 - 0 \\\\ 5/3 - 5 \\\\ 5/3 - 5 \\\\ 5/3 - 0 \\\\ 5/3 - 0 \\end{pmatrix}\n= \\begin{pmatrix} 5/3 \\\\ 5/3 \\\\ -10/3 \\\\ -10/3 \\\\ 5/3 \\\\ 5/3 \\end{pmatrix}\n$$\nNow we solve for $\\mathbf{v}$ from the system of linear equations given by $(\\mathbf{x}^{\\star} - \\mathbf{y})_i = -4 (\\Omega^{\\top}\\mathbf{v})_i$:\n\\begin{itemize}\n    \\item $(\\mathbf{x}^{\\star} - \\mathbf{y})_1 = 5/3 = -4(-v_1) = 4v_1 \\implies v_1 = 5/12$.\n    \\item $(\\mathbf{x}^{\\star} - \\mathbf{y})_2 = 5/3 = -4(v_1 - v_2) = -4(5/12 - v_2) \\implies -5/12 = 5/12 - v_2 \\implies v_2 = 10/12 = 5/6$.\n    \\item $(\\mathbf{x}^{\\star} - \\mathbf{y})_3 = -10/3 = -4(v_2 - v_3) = -4(5/6 - v_3) \\implies 10/12 = 5/6 = 5/6 - v_3 \\implies v_3 = 0$.\n    \\item $(\\mathbf{x}^{\\star} - \\mathbf{y})_4 = -10/3 = -4(v_3 - v_4) = -4(0 - v_4) = 4v_4 \\implies v_4 = -10/12 = -5/6$.\n    \\item $(\\mathbf{x}^{\\star} - \\mathbf{y})_5 = 5/3 = -4(v_4 - v_5) = -4(-5/6 - v_5) \\implies -5/12 = -5/6 - v_5 \\implies v_5 = -5/6 + 5/12 = -5/12$.\n\\end{itemize}\nAs a consistency check for the last component:\n$$\n(\\mathbf{x}^{\\star} - \\mathbf{y})_6 = 5/3. \\quad -4(\\Omega^{\\top}\\mathbf{v})_6 = -4(v_5) = -4(-5/12) = 20/12 = 5/3.\n$$\nThe values match, so the system is consistent. The resulting dual vector is:\n$$\n\\mathbf{v} = \\begin{pmatrix} 5/12  5/6  0  -5/6  -5/12 \\end{pmatrix}^{\\top}\n$$\nThe final step is to verify if this $\\mathbf{v}$ satisfies the subgradient condition $\\|\\mathbf{v}\\|_{\\infty} \\le 1$.\nThe absolute values of the components of $\\mathbf{v}$ are:\n$|v_1| = 5/12 \\approx 0.417$\n$|v_2| = 5/6 \\approx 0.833$\n$|v_3| = 0$\n$|v_4| = 5/6 \\approx 0.833$\n$|v_5| = 5/12 \\approx 0.417$\nThe largest absolute value is $5/6$, which is less than $1$. Thus, the condition $\\|\\mathbf{v}\\|_{\\infty} \\le 1$ is satisfied.\n\nSince we have found a vector $\\mathbf{v}$ that satisfies the necessary and sufficient optimality conditions for our candidate solution $\\mathbf{x}^{\\star} = (5/3)\\mathbf{1}$, and since the objective function is strictly convex, this candidate solution is indeed the unique minimizer.", "answer": "$$\n\\boxed{\n\\begin{pmatrix} \\frac{5}{3}  \\frac{5}{3}  \\frac{5}{3}  \\frac{5}{3}  \\frac{5}{3}  \\frac{5}{3} \\end{pmatrix}\n}\n$$", "id": "3430855"}, {"introduction": "While solving small-scale analysis sparsity problems from first principles is insightful, practical applications require efficient iterative algorithms, many of which are built upon the proximal operator. Unlike in the simpler synthesis case, the proximal operator for the analysis model term $\\lambda \\|\\Omega x\\|_1$ generally lacks a closed-form solution when the operator $\\Omega$ is not orthogonal. This practice challenges you to identify this key computational bottleneck and recognize the standard strategy—variable splitting—used to overcome it, a concept crucial for designing effective large-scale solvers [@problem_id:2897753].", "problem": "Consider the proximal operator associated with an analysis-sparsity regularizer in a signal recovery subproblem. Let $g:\\mathbb{R}^{n}\\to\\mathbb{R}$ be defined by $g(x)=\\lambda \\lVert W x\\rVert_{1}$, where $x\\in \\mathbb{R}^{n}$, $\\lambda0$, and $W\\in \\mathbb{R}^{m\\times n}$ is a fixed linear analysis operator that is not orthogonal. For a given $v\\in \\mathbb{R}^{n}$ and a stepsize $\\gamma0$, the proximal operator of $\\gamma g$ at $v$ is defined by\n$$\n\\mathrm{prox}_{\\gamma g}(v)\\triangleq \\underset{x\\in \\mathbb{R}^{n}}{\\arg\\min}\\;\\frac{1}{2}\\lVert x-v\\rVert_{2}^{2}+\\gamma\\,g(x)=\\underset{x\\in \\mathbb{R}^{n}}{\\arg\\min}\\;\\frac{1}{2}\\lVert x-v\\rVert_{2}^{2}+\\gamma\\lambda \\lVert W x\\rVert_{1}.\n$$\nRecall that the proximal operator of the $\\ell_{1}$-norm is separable across coordinates and yields soft-thresholding when applied directly to a vector argument. However, here the $\\ell_{1}$-norm is composed with a non-orthogonal linear operator $W$.\n\nWhich single option correctly explains why computing $\\mathrm{prox}_{\\gamma g}(v)$ is nontrivial in this setting and proposes a valid variable-splitting strategy to handle it?\n\nA. The composition with a non-orthogonal $W$ destroys separability across coordinates in $x$, so the proximal operator lacks a closed-form soft-thresholding in $x$. A standard remedy is to introduce an auxiliary variable $z=W x$ and enforce $z=W x$ via a constraint, then solve \n$$\n\\min_{x,z}\\;\\frac{1}{2}\\lVert x-v\\rVert_{2}^{2}+\\gamma\\lambda \\lVert z\\rVert_{1}\\quad \\text{subject to}\\quad z=W x,\n$$\nfor example with Alternating Direction Method of Multipliers (ADMM), where the $z$-update is soft-thresholding and the $x$-update is a least-squares step.\n\nB. Since the $\\ell_{1}$-norm is proximable, the chain rule for proximal operators implies $\\mathrm{prox}_{\\gamma g}(v)=v-\\gamma\\lambda\\, W^{\\top}\\,\\mathrm{soft}(W v,1)$ for any full-column-rank $W$, so non-orthogonality of $W$ does not matter.\n\nC. Reparameterizing with $x=W^{\\top} z$ reduces the problem to $\\min_{z}\\;\\frac{1}{2}\\lVert W^{\\top} z-v\\rVert_{2}^{2}+\\gamma\\lambda \\lVert z\\rVert_{1}$, whose solution is given by component-wise soft-thresholding of $W v$ followed by multiplication by $W^{\\top}$, even when $W$ is not orthogonal.\n\nD. One can instead form a saddle-point problem and apply a Primal-Dual Hybrid Gradient method, so the proximal of $\\gamma g$ is always obtained by projecting onto an $\\ell_{\\infty}$-norm ball in the dual; therefore no variable splitting is needed and the proximal is closed-form regardless of $W$.", "solution": "The problem asks for a correct explanation of the difficulty in computing the proximal operator of $g(x) = \\lambda \\lVert Wx \\rVert_1$ for a non-orthogonal matrix $W$, and a valid strategy to address it.\n\nFirst, let us perform the validation of the problem statement.\n\n### Step 1: Extract Givens\n- The function to be regularized is $g(x) = \\lambda \\lVert Wx \\rVert_1$, where $x \\in \\mathbb{R}^n$, $\\lambda  0$.\n- The analysis operator $W \\in \\mathbb{R}^{m \\times n}$ is a fixed linear operator that is **not orthogonal**.\n- The proximal operator of $\\gamma g$ at a point $v \\in \\mathbb{R}^n$ with a stepsize $\\gamma  0$ is defined as:\n$$ \\mathrm{prox}_{\\gamma g}(v) \\triangleq \\underset{x\\in \\mathbb{R}^{n}}{\\arg\\min}\\;\\frac{1}{2}\\lVert x-v\\rVert_{2}^{2}+\\gamma\\,g(x) = \\underset{x\\in \\mathbb{R}^{n}}{\\arg\\min}\\;\\frac{1}{2}\\lVert x-v\\rVert_{2}^{2}+\\gamma\\lambda \\lVert W x\\rVert_{1} $$\n- The problem states that the proximal operator of the $\\ell_1$-norm is the soft-thresholding operator when applied directly to a vector.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically and mathematically sound.\n- **Scientific Grounding**: The problem setup is a cornerstone of modern optimization in signal processing, machine learning, and inverse problems. Analysis-sparsity regularization using the $\\ell_1$-norm composed with a linear operator $W$ is a standard technique. Examples include Total Variation ($W$ is a finite difference operator) and analysis dictionary learning models.\n- **Well-Posed**: The objective function in the definition of the proximal operator, $F(x) = \\frac{1}{2}\\lVert x-v\\rVert_{2}^{2}+\\gamma\\lambda \\lVert W x\\rVert_{1}$, is the sum of a strongly convex function, $\\frac{1}{2}\\lVert x-v\\rVert_{2}^{2}$, and a convex function, $\\gamma\\lambda \\lVert W x\\rVert_{1}$. The sum is therefore strongly convex, which guarantees the existence of a unique minimizer. Thus, the problem of finding $\\mathrm{prox}_{\\gamma g}(v)$ is well-posed.\n- **Objectivity and Clarity**: The terms used (proximal operator, $\\ell_1$-norm, analysis operator, non-orthogonal) are standard and precisely defined in the field of convex optimization. The core difficulty of the problem hinges on the non-orthogonality of $W$, which is explicitly stated.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will now proceed with the solution derivation and analysis of the provided options.\n\nThe core of the problem is to compute $x^* = \\mathrm{prox}_{\\gamma g}(v)$, which is the unique solution to the optimization problem:\n$$ x^* = \\underset{x\\in \\mathbb{R}^{n}}{\\arg\\min} \\; F(x) = \\underset{x\\in \\mathbb{R}^{n}}{\\arg\\min} \\left( \\frac{1}{2}\\lVert x-v\\rVert_{2}^{2}+\\gamma\\lambda \\lVert W x\\rVert_{1} \\right) $$\nThe term $\\lVert W x\\rVert_{1}$ is defined as $\\sum_{i=1}^m |(Wx)_i|$, where $(Wx)_i = \\sum_{j=1}^n W_{ij}x_j$. Because the components $(Wx)_i$ are linear combinations of the elements $x_j$, the term $\\lVert W x\\rVert_{1}$ couples the variables $x_1, \\dots, x_n$. This coupling prevents the problem from being separable with respect to the coordinates of $x$. If $W$ were the identity matrix (or any diagonal matrix), the problem would be separable, and the solution would be given by applying the component-wise soft-thresholding operator directly to $v$. Because $W$ is a general non-orthogonal linear operator, a closed-form solution in terms of simple functions like soft-thresholding applied to $x$ or $v$ does not exist. The computation is therefore nontrivial and requires an iterative algorithm.\n\nNow, we evaluate each option.\n\n**A. The composition with a non-orthogonal $W$ destroys separability across coordinates in $x$, so the proximal operator lacks a closed-form soft-thresholding in $x$. A standard remedy is to introduce an auxiliary variable $z=W x$ and enforce $z=W x$ via a constraint, then solve**\n$$\n\\min_{x,z}\\;\\frac{1}{2}\\lVert x-v\\rVert_{2}^{2}+\\gamma\\lambda \\lVert z\\rVert_{1}\\quad \\text{subject to}\\quad z=W x,\n$$\n**for example with Alternating Direction Method of Multipliers (ADMM), where the $z$-update is soft-thresholding and the $x$-update is a least-squares step.**\n\nThis option correctly identifies the root of the problem: the composition with $W$ destroys separability. The proposed remedy, known as variable splitting, is a standard and powerful technique. By introducing $z=Wx$, the original problem is reformulated into an equivalent constrained problem:\n$$ \\underset{x,z}{\\min}\\;\\frac{1}{2}\\lVert x-v\\rVert_{2}^{2}+\\gamma\\lambda \\lVert z\\rVert_{1}\\quad \\text{s.t.}\\quad z=W x. $$\nThis structure is perfectly suited for ADMM. The augmented Lagrangian is $L_\\rho(x,z,u) = \\frac{1}{2}\\lVert x-v\\rVert_{2}^{2}+\\gamma\\lambda \\lVert z\\rVert_{1} + u^\\top(Wx-z) + \\frac{\\rho}{2}\\lVert Wx-z \\rVert_2^2$.\nThe ADMM algorithm alternates between minimizing $L_\\rho$ with respect to $x$ and $z$:\n1.  **$x$-update**: $\\underset{x}{\\min} \\; \\frac{1}{2}\\lVert x-v\\rVert_{2}^{2} + u^\\top Wx + \\frac{\\rho}{2}\\lVert Wx-z \\rVert_2^2$. This is a quadratic optimization problem in $x$, which is a least-squares problem. The solution is found by solving a linear system $(I+\\rho W^\\top W)x = v - W^\\top u + \\rho W^\\top z$.\n2.  **$z$-update**: $\\underset{z}{\\min} \\; \\gamma\\lambda \\lVert z\\rVert_{1} - u^\\top z + \\frac{\\rho}{2}\\lVert Wx-z \\rVert_2^2$. This is equivalent to computing $\\underset{z}{\\min} \\; \\frac{1}{2} \\left\\lVert z - (Wx + u/\\rho) \\right\\rVert_2^2 + \\frac{\\gamma\\lambda}{\\rho}\\lVert z \\rVert_1$. The solution is precisely the proximal operator of the $\\ell_1$-norm, which is soft-thresholding: $z = \\mathrm{soft}(Wx+u/\\rho, \\gamma\\lambda/\\rho)$.\nThe description in this option is entirely accurate.\n**Verdict: Correct.**\n\n**B. Since the $\\ell_{1}$-norm is proximable, the chain rule for proximal operators implies $\\mathrm{prox}_{\\gamma g}(v)=v-\\gamma\\lambda\\, W^{\\top}\\,\\mathrm{soft}(W v,1)$ for any full-column-rank $W$, so non-orthogonality of $W$ does not matter.**\n\nThis statement is incorrect. There is no such general \"chain rule\" for proximal operators. A specific formula exists for composition with an **orthogonal** operator ($W^\\top W = I$), which states $\\mathrm{prox}_{h \\circ W}(v) = W^\\top \\mathrm{prox}_h(Wv)$. For our problem, this would yield $\\mathrm{prox}_{\\gamma g}(v) = W^\\top \\mathrm{soft}(Wv, \\gamma\\lambda)$. The formula provided in the option, $v-\\gamma\\lambda\\, W^{\\top}\\,\\mathrm{soft}(W v,1)$, is fictitious and incorrect. The argument to soft-thresholding should be $\\gamma\\lambda$, not $1$. Most importantly, the problem specifies that $W$ is **not** orthogonal, which makes such simple closed-form expressions invalid. The non-orthogonality of $W$ is precisely the reason the problem is difficult.\n**Verdict: Incorrect.**\n\n**C. Reparameterizing with $x=W^{\\top} z$ reduces the problem to $\\min_{z}\\;\\frac{1}{2}\\lVert W^{\\top} z-v\\rVert_{2}^{2}+\\gamma\\lambda \\lVert z\\rVert_{1}$, whose solution is given by component-wise soft-thresholding of $W v$ followed by multiplication by $W^{\\top}$, even when $W$ is not orthogonal.**\n\nThis option contains several errors.\n1.  The reparameterization $x=W^\\top z$ is a severe and generally invalid restriction on the solution space. The optimal solution $x^*$ does not necessarily lie in the range of $W^\\top$.\n2.  Even if we were to make this substitution, the term $\\lVert Wx \\rVert_1$ becomes $\\lVert W W^\\top z \\rVert_1$, which does not simplify to $\\lVert z \\rVert_1$ unless $W W^\\top=I$. Since $W$ is not orthogonal, this simplification is invalid.\n3.  The problem $\\min_{z}\\;\\frac{1}{2}\\lVert W^{\\top} z-v\\rVert_{2}^{2}+\\gamma\\lambda \\lVert z\\rVert_{1}$ is a standard LASSO problem. Its solution is **not** obtained by simple soft-thresholding of $Wv$. The term $\\lVert W^\\top z - v \\rVert_2^2$ couples the variables in $z$ through the matrix $W^\\top$, requiring an iterative solver like ISTA or FISTA.\n**Verdict: Incorrect.**\n\n**D. One can instead form a saddle-point problem and apply a Primal-Dual Hybrid Gradient method, so the proximal of $\\gamma g$ is always obtained by projecting onto an $\\ell_{\\infty}$-norm ball in the dual; therefore no variable splitting is needed and the proximal is closed-form regardless of $W$.**\n\nThis option correctly identifies that a primal-dual algorithm is a valid method to solve the problem. The dual of the $\\ell_1$-norm regularizer is indeed an indicator function of an $\\ell_\\infty$-norm ball, so the dual update step in such an algorithm involves a projection onto this ball. However, the option draws incorrect conclusions from this fact.\n1.  A primal-dual algorithm is an **iterative** procedure. It provides a sequence of iterates that converge to the solution $x^*$. It does not provide a **closed-form solution** for the proximal operator. The claim that \"the proximal is closed-form regardless of $W$\" is false.\n2.  The statement that \"no variable splitting is needed\" is misleading. Primal-dual methods are themselves a form of splitting, operating on primal and dual variables in a saddle-point formulation, which is an alternative to the primal variable splitting described in option A.\nThe key error is the claim of a closed-form solution. An iterative method is not a closed-form solution.\n**Verdict: Incorrect.**\n\nIn summary, only option A provides a correct analysis of the problem and proposes a valid, standard computational strategy.", "answer": "$$\\boxed{A}$$", "id": "2897753"}, {"introduction": "The true power of the analysis model is realized when the operator $\\Omega$ is not fixed beforehand but is learned directly from data, which gives rise to a non-convex joint optimization problem over both the signals $X$ and the operator $\\Omega$. A common and effective strategy for this task is alternating minimization, which iterates between estimating the signals and updating the operator, often requiring manifold optimization techniques to handle constraints on $\\Omega$. This capstone exercise guides you through deriving and implementing a complete algorithm for analysis operator learning, turning abstract theory into working code by combining signal updates using methods like ADMM [@problem_id:2897753] with operator updates on the oblique manifold [@problem_id:3430809].", "problem": "You are given the task of formulating and implementing an alternating minimization algorithm for learning an analysis operator in the setting of compressed sensing and sparse optimization with latent variables. For integers $n$, $m$, $p$, and $K$, let $\\{(A_k, y_k)\\}_{k=1}^K$ be a collection of linear measurement operators $A_k \\in \\mathbb{R}^{m \\times n}$ and observations $y_k \\in \\mathbb{R}^{m}$ generated from unknown latent signals $x_k^\\star \\in \\mathbb{R}^n$ via $y_k \\approx A_k x_k^\\star$. The goal is to jointly estimate $X \\triangleq [x_1,\\dots,x_K] \\in \\mathbb{R}^{n \\times K}$ and an analysis operator $\\Omega \\in \\mathbb{R}^{p \\times n}$ with unit-norm rows (the oblique manifold constraint) by minimizing the following objective:\n$$\nF(X,\\Omega) \\triangleq \\sum_{k=1}^{K} \\left( \\frac{1}{2} \\left\\| y_k - A_k x_k \\right\\|_2^2 + \\lambda \\left\\| \\Omega x_k \\right\\|_1 \\right),\n$$\nsubject to the constraint that each row of $\\Omega$ has Euclidean norm equal to $1$ (that is, if $\\omega_i^\\top$ denotes the $i$-th row of $\\Omega$, then $\\|\\omega_i\\|_2 = 1$ for all $i \\in \\{1,\\dots,p\\}$). Here $\\lambda  0$ is a regularization parameter. The latent variables are the signal estimates $x_k$.\n\nYou must derive and implement an alternating minimization scheme that iterates the following two blocks:\n- X-update: With $\\Omega$ fixed, compute $x_k$ for each $k \\in \\{1,\\dots,K\\}$ by solving the analysis Lasso subproblem \n$$\n\\min_{x \\in \\mathbb{R}^n} \\; \\frac{1}{2} \\left\\| y_k - A_k x \\right\\|_2^2 + \\lambda \\left\\| \\Omega x \\right\\|_1.\n$$\n- $\\Omega$-update: With $X$ fixed, take a descent step on the oblique manifold to reduce $F(X,\\Omega)$ while maintaining unit-norm rows of $\\Omega$. Use a first-order method that relies on a subgradient of $F$ with respect to $\\Omega$ and a projection onto the tangent space of the oblique manifold, followed by a row-wise renormalization retraction. Implement a backtracking strategy to ensure that the step provides a decrease in the objective for fixed $X$.\n\nYour derivation must start from core definitions in convex optimization and Riemannian constrained optimization. In particular, use the definitions of subgradients for the $\\ell_1$-norm, the concept of the oblique manifold (row-wise unit spheres), the Euclidean gradient or subgradient with respect to $\\Omega$, the tangent space at a point on the oblique manifold, and the row-wise orthogonal projection of a matrix onto that tangent space. Do not assume any specialized shortcut formulas; reason from first principles.\n\nAnalyze and state sufficient conditions under which the overall objective $F$ is guaranteed to decrease across outer iterations of the alternating method. Your analysis should be based on the exact minimization in the $X$-update and a sufficiently small step size in the $\\Omega$-update that yields a strict or nonincreasing decrease when $X$ is fixed. Discuss how nonsmoothness due to the $\\ell_1$-term affects the subgradient-based descent and how backtracking ensures descent away from nondifferentiable kinks.\n\nImplement the algorithm in a single, complete, runnable program. The program must:\n- Construct synthetic test instances by generating $A_k$ and $y_k$ from latent $x_k^\\star$ with small additive noise, with all sizes and parameters given explicitly below.\n- For solving each analysis Lasso subproblem in the $X$-update, use a splitting method based on a variable $z_k \\in \\mathbb{R}^p$ with the linear constraint $z_k = \\Omega x_k$ to separate the data fidelity and sparsity terms, and then minimize via a first-order primal-dual splitting or an alternating direction scheme that derives from the method of multipliers, using only linear algebra and the soft-thresholding operator. Your derivation must define the necessary optimality conditions and updates from first principles, without assuming any black-box solver.\n- For the $\\Omega$-update, compute a valid subgradient of the nonsmooth term with respect to $\\Omega$, project it onto the tangent space of the oblique manifold at the current $\\Omega$, and perform a backtracking line search with a row-wise renormalization retraction to ensure a decrease in $F$ for fixed $X$.\n\nTest Suite:\nYou must implement and run the algorithm on the following three test cases. All matrices and vectors must be real-valued. There are no physical units in this problem. Angles are not used. All scalar quantities in the output must be dimensionless.\n\n- Test case $1$ (happy path): $n=20$, $m=16$, $p=30$, $K=3$, $\\lambda=0.1$, outer iterations $T=6$. Use a single matrix $A \\in \\mathbb{R}^{m \\times n}$ for all $k \\in \\{1,\\dots,K\\}$, with independent and identically distributed Gaussian entries with zero mean and variance chosen so that columns have unit expected norm after normalization. Generate $x_k^\\star$ with exactly $3$ nonzero entries at random positions with standard normal magnitudes, and $y_k = A x_k^\\star + \\varepsilon_k$ where $\\varepsilon_k$ has independent and identically distributed Gaussian entries with zero mean and standard deviation $0.01$.\n- Test case $2$ (boundary case $\\lambda=0$): $n=15$, $m=12$, $p=25$, $K=2$, $\\lambda=0.0$, $T=4$. Same generation protocol for $A$ and $y_k$, except $x_k^\\star$ has exactly $2$ nonzero entries and noise standard deviation $0.01$.\n- Test case $3$ (square analysis operator): $n=12$, $m=12$, $p=12$, $K=4$, $\\lambda=0.5$, $T=5$. Same generation protocol for $A$ and $y_k$, except $x_k^\\star$ has exactly $4$ nonzero entries and noise standard deviation $0.01$.\n\nInitialization:\n- Initialize $\\Omega$ in each test by drawing a random Gaussian matrix in $\\mathbb{R}^{p \\times n}$ and renormalizing each row to have unit norm.\n- Initialize $x_k$ to the zero vector.\n\nAlgorithmic details:\n- Use a fixed penalty parameter $\\rho = 1.0$ in the splitting method for the $X$-update.\n- Use a maximum of $100$ inner iterations for the splitting method, and terminate early if the primal and dual residual norms both fall below $10^{-6}$.\n- For the $\\Omega$-update, use a backtracking line search with initial step size $\\eta_0 = 1.0$, contraction factor $\\beta = 0.5$, and Armijo parameter $c = 10^{-4}$, limiting to at most $30$ backtracking steps per outer iteration. Use the row-wise renormalization retraction to stay on the oblique manifold.\n\nOutput requirement:\nFor each of the three test cases, record the sequence of objective values after each outer iteration, where one outer iteration consists of one full $X$-update followed by one $\\Omega$-update. For each test case, return a boolean indicating whether this sequence is monotonically nonincreasing up to numerical tolerance $10^{-8}$. Your program should produce a single line of output containing these three boolean results as a comma-separated list enclosed in square brackets, for example, \"[True,False,True]\".", "solution": "The problem requires the formulation and implementation of an alternating minimization algorithm to solve for an analysis operator $\\Omega$ and a set of signals $X$ from compressed measurements. The validity of the problem statement is first assessed.\n\n### Step 1: Extract Givens\n- **Objective Function**: $F(X,\\Omega) \\triangleq \\sum_{k=1}^{K} \\left( \\frac{1}{2} \\left\\| y_k - A_k x_k \\right\\|_2^2 + \\lambda \\left\\| \\Omega x_k \\right\\|_1 \\right)$.\n- **Variables**: Signal matrix $X = [x_1,\\dots,x_K] \\in \\mathbb{R}^{n \\times K}$ and analysis operator $\\Omega \\in \\mathbb{R}^{p \\times n}$.\n- **Data**: A set of measurement operators $\\{A_k\\}_{k=1}^K$ where $A_k \\in \\mathbb{R}^{m \\times n}$ and observations $\\{y_k\\}_{k=1}^K$ where $y_k \\in \\mathbb{R}^{m}$.\n- **Constraint**: The rows of $\\Omega$, denoted $\\omega_i^\\top$, must satisfy $\\|\\omega_i\\|_2 = 1$ for all $i \\in \\{1,\\dots,p\\}$. This constrains $\\Omega$ to the oblique manifold $\\mathcal{O}^{p,n}$.\n- **Algorithm**: Alternating minimization between $X$ and $\\Omega$.\n  - **X-update**: Fix $\\Omega$, solve for each $x_k$ via $\\min_{x \\in \\mathbb{R}^n} \\; \\frac{1}{2} \\left\\| y_k - A_k x \\right\\|_2^2 + \\lambda \\left\\| \\Omega x \\right\\|_1$ using a splitting method based on variable substitution and ADMM.\n  - **$\\Omega$-update**: Fix $X$, perform a descent step for $\\Omega$ on the oblique manifold using a projected subgradient method with a backtracking line search and row-wise renormalization retraction.\n- **Test Cases**: Three specific instances are defined with parameters $(n, m, p, K, \\lambda, T)$, data generation procedures, and noise levels.\n  - Case 1: $(20, 16, 30, 3, 0.1, 6)$, sparsity $3$.\n  - Case 2: $(15, 12, 25, 2, 0.0, 4)$, sparsity $2$.\n  - Case 3: $(12, 12, 12, 4, 0.5, 5)$, sparsity $4$.\n- **Initialization**: $\\Omega$ is a random Gaussian matrix with rows normalized to unit length. $X$ is the zero matrix.\n- **Algorithmic Parameters**: ADMM penalty $\\rho=1.0$, ADMM max iterations $100$, ADMM tolerance $10^{-6}$. Backtracking parameters: initial step $\\eta_0=1.0$, contraction factor $\\beta=0.5$, Armijo parameter $c=10^{-4}$, max backtracking steps $30$.\n- **Output**: A list of three booleans, indicating for each test case whether the sequence of objective function values is monotonically nonincreasing (within tolerance $10^{-8}$).\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed based on the predefined criteria.\n- **Scientifically Grounded**: The problem is well-grounded in the established fields of sparse optimization, compressed sensing, and numerical optimization. The objective function is a standard analysis-formulation sparse recovery model. The proposed algorithmic components—alternating minimization, ADMM, and projected subgradient descent on manifolds—are all standard and theoretically sound techniques.\n- **Well-Posed**: The problem is well-posed for its stated computational goal. Each subproblem is well-defined. The X-update subproblem is convex and has a unique solution. The $\\Omega$-update involves a descent step on a non-convex manifold, which is a standard procedure in manifold optimization. The problem asks to implement the algorithm and verify a property (monotonicity), not to prove global convergence, which is appropriate for this non-convex setting.\n- **Objective**: The problem is stated in precise, objective mathematical language. All terms like \"analysis operator,\" \"oblique manifold,\" \"subgradient,\" and \"retraction\" have formal definitions in the relevant literature.\n- **Completeness and Consistency**: The problem is self-contained. All necessary parameters, algorithmic details, initial conditions, and data generation protocols for the test cases are explicitly provided. There are no apparent contradictions in the setup.\n- **Realism and Feasibility**: The problem is a standard computational task in signal processing and machine learning research. The specified dimensions and parameters are well within the range of modern computational feasibility.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a well-defined, scientifically grounded, and computationally feasible task. We proceed with the derivation and solution.\n\n### Algorithm Derivation\n\nThe algorithm performs alternating minimization on the objective function $F(X, \\Omega)$. Let $(X^{(t)}, \\Omega^{(t)})$ be the estimates at outer iteration $t$. An iteration consists of two steps:\n1. $X^{(t+1)} = \\arg\\min_X F(X, \\Omega^{(t)})$.\n2. $\\Omega^{(t+1)}$ is found by performing a descent step from $\\Omega^{(t)}$ to reduce $F(X^{(t+1)}, \\Omega)$.\n\nThe overall objective function value is guaranteed to be nonincreasing.\n$$ F(X^{(t+1)}, \\Omega^{(t+1)}) \\le F(X^{(t+1)}, \\Omega^{(t)}) \\le F(X^{(t)}, \\Omega^{(t)}) $$\nThe first inequality, $F(X^{(t+1)}, \\Omega^{(t+1)}) \\le F(X^{(t+1)}, \\Omega^{(t)})$, is ensured by the $\\Omega$-update being a descent method. The second inequality, $F(X^{(t+1)}, \\Omega^{(t)}) \\le F(X^{(t)}, \\Omega^{(t)})$, holds because the $X$-update solves the minimization problem for $X$ to optimality (or near-optimality) while $\\Omega$ is fixed.\n\n#### X-Update: Solving the Analysis Lasso Subproblem\nFor a fixed $\\Omega$, the objective function is separable with respect to each column $x_k$ of $X$. For each $k \\in \\{1, \\dots, K\\}$, we must solve the analysis Lasso problem:\n$$ \\min_{x_k \\in \\mathbb{R}^n} \\; \\frac{1}{2} \\left\\| y_k - A_k x_k \\right\\|_2^2 + \\lambda \\left\\| \\Omega x_k \\right\\|_1 $$\nWe solve this using the Alternating Direction Method of Multipliers (ADMM). We introduce an auxiliary variable $z_k \\in \\mathbb{R}^p$ and reformulate the problem as:\n$$ \\min_{x_k, z_k} \\; \\frac{1}{2} \\left\\| y_k - A_k x_k \\right\\|_2^2 + \\lambda \\left\\| z_k \\right\\|_1 \\quad \\text{subject to} \\quad \\Omega x_k - z_k = 0 $$\nThe augmented Lagrangian (using the scaled dual variable formulation) is:\n$$ L_\\rho(x_k, z_k, u_k) = \\frac{1}{2} \\|y_k - A_k x_k\\|_2^2 + \\lambda \\|z_k\\|_1 + \\frac{\\rho}{2} \\|\\Omega x_k - z_k + u_k\\|_2^2 - \\frac{\\rho}{2} \\|u_k\\|_2^2 $$\nHere, $u_k$ is the scaled dual variable and $\\rho  0$ is a penalty parameter. ADMM proceeds by iteratively minimizing $L_\\rho$ with respect to $x_k$ and $z_k$, followed by an update of the dual variable $u_k$.\n\n1.  **$x_k$-update**: We minimize $L_\\rho$ with respect to $x_k$:\n    $$ x_k^{(j+1)} = \\arg\\min_{x_k} \\frac{1}{2} \\|y_k - A_k x_k\\|_2^2 + \\frac{\\rho}{2} \\|\\Omega x_k - z_k^{(j)} + u_k^{(j)}\\|_2^2 $$\n    This is a quadratic problem. Setting the gradient to zero yields a linear system:\n    $$ A_k^\\top (A_k x_k - y_k) + \\rho \\Omega^\\top (\\Omega x_k - z_k^{(j)} + u_k^{(j)}) = 0 $$\n    $$ (A_k^\\top A_k + \\rho \\Omega^\\top \\Omega) x_k = A_k^\\top y_k + \\rho \\Omega^\\top (z_k^{(j)} - u_k^{(j)}) $$\n    The solution is $x_k^{(j+1)} = (A_k^\\top A_k + \\rho \\Omega^\\top \\Omega)^{-1} \\left( A_k^\\top y_k + \\rho \\Omega^\\top (z_k^{(j)} - u_k^{(j)}) \\right)$. The matrix to be inverted is positive definite if $A_k$ has full column rank or if $\\rho  0$ and $\\Omega$ has full row rank.\n\n2.  **$z_k$-update**: We minimize $L_\\rho$ with respect to $z_k$:\n    $$ z_k^{(j+1)} = \\arg\\min_{z_k} \\lambda \\|z_k\\|_1 + \\frac{\\rho}{2} \\|\\Omega x_k^{(j+1)} - z_k + u_k^{(j)}\\|_2^2 $$\n    This is equivalent to solving a proximal problem:\n    $$ z_k^{(j+1)} = \\text{prox}_{\\frac{\\lambda}{\\rho}\\|\\cdot\\|_1} (\\Omega x_k^{(j+1)} + u_k^{(j)}) $$\n    The solution is given by the soft-thresholding operator, $S_{\\tau}(v)_i = \\text{sign}(v_i) \\max(|v_i|-\\tau, 0)$:\n    $$ z_k^{(j+1)} = S_{\\lambda/\\rho}(\\Omega x_k^{(j+1)} + u_k^{(j)}) $$\n\n3.  **$u_k$-update**: The dual variable is updated via gradient ascent:\n    $$ u_k^{(j+1)} = u_k^{(j)} + \\Omega x_k^{(j+1)} - z_k^{(j+1)} $$\n\nThese three steps are repeated until convergence, which is assessed by monitoring the primal residual $\\|\\Omega x_k^{(j+1)} - z_k^{(j+1)}\\|_2$ and dual residual $\\|\\rho \\Omega^\\top (z_k^{(j+1)} - z_k^{(j)})\\|_2$.\n\n#### $\\Omega$-Update: Projected Subgradient Descent on the Oblique Manifold\nFor a fixed $X$, the $\\Omega$-subproblem is:\n$$ \\min_{\\Omega \\in \\mathcal{O}^{p,n}} G(\\Omega) \\triangleq \\lambda \\sum_{k=1}^{K} \\|\\Omega x_k\\|_1 = \\lambda \\|\\Omega X\\|_1 $$\nwhere $\\mathcal{O}^{p,n} = \\{\\Omega \\in \\mathbb{R}^{p \\times n} : \\text{diag}(\\Omega\\Omega^\\top) = I_p\\}$ is the oblique manifold. We use a projected subgradient descent method.\n\n1.  **Euclidean Subgradient**: The function $G(\\Omega)$ is convex but nonsmooth. The subdifferential of the absolute value function $|v|$ is $\\text{sign}(v)$ for $v \\neq 0$ and the interval $[-1, 1]$ for $v=0$. We can select an element from the subdifferential to define a subgradient. A valid Euclidean subgradient of $G(\\Omega)$ is:\n    $$ \\mathcal{G}_\\Omega = \\frac{\\partial G}{\\partial \\Omega} = \\lambda \\cdot \\text{sign}(\\Omega X) X^\\top $$\n    where $\\text{sign}(0)$ is defined to be $0$.\n\n2.  **Projection onto Tangent Space**: A step in the direction of the negative subgradient $-\\mathcal{G}_\\Omega$ may violate the manifold constraint. We first project this direction onto the tangent space of the manifold at the current point $\\Omega$. The tangent space at $\\Omega \\in \\mathcal{O}^{p,n}$ is $T_\\Omega \\mathcal{O}^{p,n} = \\{Z \\in \\mathbb{R}^{p \\times n} : \\text{diag}(\\Omega Z^\\top) = 0 \\}$, which means that for each row $i$, the row vector $z_i^\\top$ of $Z$ must be orthogonal to the corresponding row vector $\\omega_i^\\top$ of $\\Omega$.\n    The projection of a matrix $H \\in \\mathbb{R}^{p \\times n}$ onto $T_\\Omega\\mathcal{O}^{p,n}$ is performed row-wise. For each row $h_i^\\top$ of $H$, its projection onto the tangent space at row $\\omega_i^\\top$ of $\\Omega$ is:\n    $$ \\text{proj}_{T_{\\omega_i^\\top}}(h_i^\\top) = h_i^\\top - (\\omega_i^\\top h_i) \\omega_i^\\top $$\n    Applying this to the subgradient $\\mathcal{G}_\\Omega$, the projected subgradient $P_{T_\\Omega}(\\mathcal{G}_\\Omega)$ has rows:\n    $$ (\\text{proj}(\\mathcal{G}_\\Omega))_i^\\top = (\\mathcal{G}_\\Omega)_i^\\top - ((\\mathcal{G}_\\Omega)_i^\\top \\omega_i) \\omega_i^\\top $$\n    The descent direction is $D = -P_{T_\\Omega}(\\mathcal{G}_\\Omega)$.\n\n3.  **Retraction and Line Search**: We take a step in the direction $D$ and then map the result back to the manifold. The update candidate is $\\Omega' = \\Omega + \\eta D$, where $\\eta  0$ is the step size. The row-wise renormalization retraction maps $\\Omega'$ back to $\\mathcal{O}^{p,n}$:\n    $$ \\Omega_\\text{new} = \\text{Retr}_\\Omega(\\eta D) \\quad \\text{with} \\quad (\\Omega_\\text{new})_i^\\top = \\frac{(\\Omega + \\eta D)_i^\\top}{\\|(\\Omega + \\eta D)_i^\\top\\|_2} $$\n    The step size $\\eta$ is determined by a backtracking line search to satisfy the Armijo condition, ensuring sufficient decrease in the objective. We seek the largest $\\eta = \\eta_0 \\beta^j$ for integers $j \\ge 0$ such that:\n    $$ G(\\Omega_\\text{new}) \\le G(\\Omega) + c \\eta \\langle \\mathcal{G}_\\Omega, D \\rangle_F $$\n    where $\\langle \\cdot, \\cdot \\rangle_F$ is the Frobenius inner product. Since $D = -P_{T_\\Omega}(\\mathcal{G}_\\Omega)$, and $P_{T_\\Omega}$ is an orthogonal projector, $\\langle \\mathcal{G}_\\Omega, D \\rangle_F = -\\langle \\mathcal{G}_\\Omega, P_{T_\\Omega}(\\mathcal{G}_\\Omega) \\rangle_F = -\\|P_{T_\\Omega}(\\mathcal{G}_\\Omega)\\|_F^2 = -\\|D\\|_F^2$. The condition becomes:\n    $$ G(\\Omega_\\text{new}) \\le G(\\Omega) - c \\eta \\|D\\|_F^2 $$\n    This guarantees a decrease in the objective as long as $D \\neq 0$. If $D=0$, we are at a stationary point of the subproblem.", "answer": "```python\nimport numpy as np\n\ndef soft_threshold(v, tau):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(v) * np.maximum(np.abs(v) - tau, 0)\n\ndef objective_function(X, Omega, A, Y, lambda_):\n    \"\"\"Computes the objective function F(X, Omega).\"\"\"\n    K = X.shape[1]\n    data_fit_term = 0.0\n    for k in range(K):\n        data_fit_term += 0.5 * np.linalg.norm(Y[:, k] - A @ X[:, k])**2\n    \n    sparsity_term = lambda_ * np.sum(np.abs(Omega @ X))\n    return data_fit_term + sparsity_term\n\ndef update_x(y_k, A, Omega, lambda_, rho, x_init, max_iter, tol):\n    \"\"\"\n    Solves the analysis LASSO subproblem for one x_k using ADMM.\n    min_x 0.5 * ||y_k - A*x_k||^2 + lambda * ||Omega*x_k||_1\n    \"\"\"\n    n, p = Omega.shape[1], Omega.shape[0]\n    \n    if lambda_ == 0.0:\n        # For lambda = 0, the problem is least squares.\n        # A.T A might be singular if m  n.\n        # The ADMM formulation with rho  0 regularizes the problem.\n        # Therefore, we use the general ADMM solver even for lambda=0.\n        pass\n\n    # Precompute matrix inverse for x-update\n    try:\n        L_inv = np.linalg.inv(A.T @ A + rho * Omega.T @ Omega)\n    except np.linalg.LinAlgError:\n         # Fallback to pseudo-inverse if matrix is singular\n        L_inv = np.linalg.pinv(A.T @ A + rho * Omega.T @ Omega)\n\n    x = x_init.copy()\n    z = np.zeros(p)\n    u = np.zeros(p)\n    \n    At_yk = A.T @ y_k\n    rho_Omegat = rho * Omega.T\n\n    for _ in range(max_iter):\n        # x-update\n        x = L_inv @ (At_yk + rho_Omegat @ (z - u))\n        \n        # z-update\n        z_old = z\n        Omega_x_u = Omega @ x + u\n        z = soft_threshold(Omega_x_u, lambda_ / rho)\n        \n        # u-update\n        u = u + Omega @ x - z\n        \n        # Check for convergence\n        primal_resid = np.linalg.norm(Omega @ x - z)\n        dual_resid = np.linalg.norm(rho * Omega.T @ (z - z_old))\n        \n        if primal_resid  tol and dual_resid  tol:\n            break\n            \n    return x\n\ndef update_omega(X, Omega, lambda_, eta0, beta, c, max_backtrack_iter):\n    \"\"\"\n    Performs a projected subgradient descent step on Omega with backtracking.\n    \"\"\"\n    p, n = Omega.shape\n    \n    if lambda_ == 0.0:\n        # Objective doesn't depend on Omega, so no update needed.\n        return Omega\n\n    # The part of the objective that depends on Omega\n    def G(O):\n        return lambda_ * np.sum(np.abs(O @ X))\n\n    # Compute Euclidean subgradient\n    S = np.sign(Omega @ X)\n    G_grad = lambda_ * S @ X.T\n\n    # Project subgradient onto tangent space of the oblique manifold\n    row_prods = np.sum(Omega * G_grad, axis=1, keepdims=True)\n    P_G = G_grad - row_prods * Omega\n    \n    D = -P_G  # Descent direction\n    \n    # Backtracking line search\n    eta = eta0\n    current_obj_G = G(Omega)\n    slope = np.sum(G_grad * D) # equivalent to -np.linalg.norm(D, 'fro')**2\n\n    for _ in range(max_backtrack_iter):\n        Omega_new_unnorm = Omega + eta * D\n        \n        # Retraction: row-wise normalization\n        norms = np.linalg.norm(Omega_new_unnorm, axis=1, keepdims=True)\n        # Avoid division by zero for zero rows, though unlikely\n        norms[norms == 0] = 1.0\n        Omega_new = Omega_new_unnorm / norms\n        \n        # We need the full objective F(X, Omega) for the check\n        # But Omega-update only sees G(Omega). The Armijo check should be on G\n        new_obj_G = G(Omega_new)\n        \n        # Armijo condition for the G part of the objective\n        if new_obj_G = current_obj_G + c * eta * slope:\n            return Omega_new\n        \n        eta *= beta\n        \n    # If backtracking fails, return original Omega\n    return Omega\n\ndef run_case(n, m, p, K, lambda_, T, sparsity, seed):\n    \"\"\"\n    Runs one full test case for the alternating minimization algorithm.\n    \"\"\"\n    np.random.seed(seed)\n\n    # --- Data Generation ---\n    A = np.random.randn(m, n)\n    A /= np.sqrt(m) # Normalize to have columns with expected norm approx 1\n\n    X_star = np.zeros((n, K))\n    for k in range(K):\n        indices = np.random.choice(n, sparsity, replace=False)\n        X_star[indices, k] = np.random.randn(sparsity)\n\n    noise_std = 0.01\n    Y = A @ X_star + noise_std * np.random.randn(m, K)\n\n    # --- Initialization ---\n    Omega = np.random.randn(p, n)\n    Omega /= np.linalg.norm(Omega, axis=1, keepdims=True)\n    X = np.zeros((n, K))\n\n    # --- Algorithm Parameters ---\n    rho = 1.0\n    admm_max_iter = 100\n    admm_tol = 1e-6\n    eta0, beta, c = 1.0, 0.5, 1e-4\n    backtrack_max_iter = 30\n    \n    objective_values = []\n    \n    # Initial objective value\n    obj_val = objective_function(X, Omega, A, Y, lambda_)\n    objective_values.append(obj_val)\n\n    # --- Alternating Minimization Loop ---\n    for _ in range(T):\n        # X-update\n        X_new = np.zeros_like(X)\n        for k in range(K):\n            X_new[:, k] = update_x(Y[:, k], A, Omega, lambda_, rho, X[:, k],\n                                   admm_max_iter, admm_tol)\n        X = X_new\n        \n        # Get objective after X-update\n        # obj_after_x = objective_function(X, Omega, A, Y, lambda_)\n        \n        # Omega-update\n        Omega_new = update_omega(X, Omega, lambda_, eta0, beta, c, backtrack_max_iter)\n        Omega = Omega_new\n\n        # Record objective value\n        obj_val = objective_function(X, Omega, A, Y, lambda_)\n        objective_values.append(obj_val)\n        \n    # --- Check for Monotonicity ---\n    is_monotonic = True\n    monotonicity_tol = 1e-8\n    for i in range(len(objective_values) - 1):\n        if objective_values[i+1] > objective_values[i] + monotonicity_tol:\n            is_monotonic = False\n            break\n            \n    return is_monotonic\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite.\n    \"\"\"\n    test_cases = [\n        # (n, m, p, K, lambda, T, sparsity, seed)\n        (20, 16, 30, 3, 0.1, 6, 3, 42),\n        (15, 12, 25, 2, 0.0, 4, 2, 43),\n        (12, 12, 12, 4, 0.5, 5, 4, 44),\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_case(*params)\n        results.append(result)\n\n    print(f\"[{','.join(map(lambda x: str(x), results))}]\")\n\nsolve()\n\n```", "id": "3430809"}]}