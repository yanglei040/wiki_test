{"hands_on_practices": [{"introduction": "Moving from theory to practice, a primary challenge in non-uniform sampling (NUS) is to decide how many data points are sufficient for a reliable reconstruction. This exercise provides a quantitative foundation for this decision, directly applying compressed sensing theory to calculate the minimum required sampling density for a given signal sparsity. By working through this problem, you will learn to connect abstract theoretical guarantees to concrete experimental design parameters and understand how to devise simulations to verify them [@problem_id:3715729].", "problem": "In a two-dimensional Nuclear Magnetic Resonance (NMR) experiment for spectrometric identification of organic compounds, the indirect time dimension is non-uniformly sampled. Consider the indirect dimension discretized on a grid of $N$ uniformly spaced evolution points, with only $M$ points acquired according to a sampling schedule that selects rows uniformly at random from the $N \\times N$ unitary Discrete Fourier Transform (DFT) operator. The resulting forward operator may thus be modeled as an $M \\times N$ random partial Fourier matrix. The spectrum along the indirect dimension is assumed $K$-sparse in the canonical basis, with $K$ isolated resonances on the $N$-point grid. The measured data are corrupted by additive complex Gaussian noise that is independent and identically distributed across acquired points.\n\nYou will design the minimum $M$ that ensures reliable recovery of the $K$-sparse spectrum using convex sparse recovery from non-uniform sampling, based on first-principles sparse recovery guarantees, and then specify a simulation protocol to test the design.\n\nAssume the following experimentally realistic parameters for a heteronuclear two-dimensional experiment targeting small-molecule organic structures:\n- Indirect dimension grid size $N = 4096$.\n- True number of isolated resonances $K = 12$.\n- Target failure probability $\\delta = 0.01$, meaning the recovery guarantee should hold with probability at least $1 - \\delta$ over the random choice of the sampling schedule.\n- Minimal resonance amplitude $A_{\\min} = 1$ (arbitrary consistent unit), and per-acquired-point Signal-to-Noise Ratio (SNR) $SNR_{\\mathrm{dB}} = 18$, defined as $SNR_{\\mathrm{dB}} = 20 \\log_{10}\\!\\left(\\frac{A_{\\min}}{\\sigma_n}\\right)$, where $\\sigma_n$ is the standard deviation of the complex Gaussian noise on each acquired point.\n\nUse as your foundational base the following well-tested facts:\n1. Random partial Fourier ensembles satisfy the Restricted Isometry Property (RIP) with high probability once the number of acquired rows $M$ scales linearly in the sparsity $K$ and logarithmically in the ambient dimension $N$ and the inverse failure probability, with an absolute constant prefactor.\n2. For stable recovery of $K$-sparse vectors using convex $\\ell_1$-minimization in the presence of bounded noise, the RIP constant bound ensures that the support and amplitudes are robustly reconstructed; the noise level influences the reconstruction error but does not alter the linear-in-$K$, logarithmic-in-$N$ sample scaling of the guarantee.\n\nTake the absolute constant in the sufficient condition to be $C = 5$ for the purpose of this design. Beginning from these bases, derive an explicit lower bound for $M$ in terms of $K$, $N$, $\\delta$, and $C$. Then insert the given numerical values and determine the smallest integer $M$ that satisfies your bound. State any intermediate mathematical steps you use to pass from the general RIP-based guarantee to your bound on $M$.\n\nFinally, specify, without executing, a scientifically sound simulation protocol to empirically verify the recovery rate: define how to generate $K$-sparse spectra on the $N$-point grid with random supports and complex amplitudes with minimum magnitude $A_{\\min}$, how to draw non-uniform sampling schedules of size $M$, how to add noise consistent with the defined $SNR_{\\mathrm{dB}}$, and how to reconstruct via Basis Pursuit Denoising (BPDN). Your final answer must be the single integer value of $M$ obtained from your bound. No unit conversion is required. Provide the smallest integer $M$; no rounding to significant figures is needed beyond taking the smallest integer meeting the bound.", "solution": "The primary task is to determine the minimum number of acquired points, $M$, for the non-uniformly sampled indirect dimension of a two-dimensional NMR experiment. This design is based on theoretical guarantees for sparse signal recovery.\n\n**1. Derivation of the Minimum Number of Samples, $M$**\n\nThe problem provides a foundational principle for the design: for a random partial Fourier measurement ensemble, the number of measurements $M$ required to ensure the Restricted Isometry Property (RIP) holds with a probability of at least $1 - \\delta$ scales linearly with the sparsity $K$ and logarithmically with the ambient dimension $N$ and the inverse failure probability $\\delta^{-1}$. This is a standard result in compressed sensing theory.\n\nUsing the provided absolute constant $C$, this sufficient condition can be expressed mathematically as:\n$$\nM \\ge C \\cdot K \\cdot \\ln\\left(\\frac{N}{\\delta}\\right)\n$$\nThe satisfaction of the RIP, in turn, guarantees that a $K$-sparse signal (in our case, the NMR spectrum) can be robustly and stably reconstructed from the $M$ noisy measurements by solving a convex optimization problem, such as $\\ell_1$-norm minimization. The problem specifies Basis Pursuit Denoising (BPDN), a common $\\ell_1$-minimization algorithm for noisy data. The parameters related to signal strength ($A_{\\min}$) and noise level ($SNR_{\\mathrm{dB}}$, $\\sigma_n$) affect the accuracy of the reconstructed amplitudes but do not alter the scaling law for $M$ that ensures the identifiability of the sparse support.\n\nWe are given the following explicit parameters:\n-   Ambient dimension (grid size): $N = 4096$\n-   Sparsity (number of resonances): $K = 12$\n-   Target failure probability: $\\delta = 0.01$\n-   Scaling constant: $C = 5$\n\nSubstituting these values into the inequality yields the lower bound for $M$:\n$$\nM \\ge 5 \\cdot 12 \\cdot \\ln\\left(\\frac{4096}{0.01}\\right)\n$$\nWe simplify the expression:\n$$\nM \\ge 60 \\cdot \\ln(409600)\n$$\nNow, we evaluate the numerical value of the expression on the right-hand side. The natural logarithm of $409600$ is:\n$$\n\\ln(409600) \\approx 12.9231805\n$$\nSubstituting this value back into the inequality for $M$:\n$$\nM \\ge 60 \\cdot 12.9231805\n$$\n$$\nM \\ge 775.39083\n$$\nSince $M$ represents the number of acquired data points, it must be an integer. To satisfy the inequality, we must take the smallest integer greater than or equal to this value, which is the ceiling of the result.\n$$\nM = \\lceil 775.39083 \\rceil = 776\n$$\nThus, the minimum number of samples required to ensure reliable recovery with at least $99\\%$ probability, according to the specified theoretical model, is $776$.\n\n**2. Simulation Protocol for Empirical Verification**\n\nTo test this design, a Monte Carlo simulation protocol is specified. This protocol will empirically measure the recovery success rate for the calculated value of $M=776$.\n\n**Simulation Objective:** To empirically estimate the probability of successful sparse recovery for the derived sampling parameter $M$ under the specified experimental conditions.\n\n**Protocol:**\n\n1.  **Initialization:**\n    -   Set the problem parameters: grid size $N=4096$, sparsity $K=12$, number of measurements $M=776$, minimum amplitude $A_{\\min}=1$, target failure probability $\\delta=0.01$.\n    -   Set the number of Monte Carlo trials to a statistically significant number, e.g., $L=1000$.\n    -   Initialize a success counter to $0$.\n\n2.  **Monte Carlo Loop:** For each trial $i = 1, \\dots, L$:\n    a.  **Generate Ground Truth Spectrum:**\n        -   Create a true spectrum vector $x_0 \\in \\mathbb{C}^N$, initially all zeros.\n        -   Randomly select a support set $\\mathcal{S}$ of $K$ distinct indices from $\\{1, ..., N\\}$ without replacement.\n        -   For each index $j \\in \\mathcal{S}$, generate a complex amplitude $x_{0,j} = A_j e^{i\\phi_j}$, where the magnitude $A_j$ is drawn from a uniform distribution $U[A_{\\min}, 2A_{\\min}]$ (i.e., $U[1, 2]$) and the phase $\\phi_j$ is drawn from a uniform distribution $U[0, 2\\pi)$. This ensures the minimal amplitude condition is met.\n\n    b.  **Simulate Non-Uniform Sampling and Noise:**\n        -   Define the measurement operator $\\Phi$ as an $M \\times N$ matrix. This is constructed by selecting $M=776$ rows uniformly at random without replacement from the $N \\times N$ unitary Discrete Fourier Transform (DFT) matrix.\n        -   Calculate the noise standard deviation, $\\sigma_n$. Given $SNR_{\\mathrm{dB}} = 18$ and the definition $SNR_{\\mathrm{dB}} = 20 \\log_{10}(A_{\\min}/\\sigma_n)$, we solve for $\\sigma_n$:\n            $$\n            18 = 20 \\log_{10}\\left(\\frac{1}{\\sigma_n}\\right) \\implies \\sigma_n = 10^{-18/20} = 10^{-0.9} \\approx 0.1259\n            $$\n        -   Generate an $M$-dimensional complex noise vector $w$, where each component is an independent draw from the complex normal distribution $\\mathcal{CN}(0, \\sigma_n^2)$.\n        -   The final measurement vector $y \\in \\mathbb{C}^M$ is computed as $y = \\Phi x_0 + w$.\n\n    c.  **Perform Sparse Recovery:**\n        -   Reconstruct the spectrum $\\hat{x}$ by solving the Basis Pursuit Denoising (BPDN) problem:\n            $$\n            \\hat{x} = \\arg\\min_{z \\in \\mathbb{C}^N} \\frac{1}{2} \\| y - \\Phi z \\|_2^2 + \\lambda \\|z\\|_1\n            $$\n        -   The regularization parameter $\\lambda$ should be set to a theoretically motivated value. A standard choice that balances the data fidelity and sparsity terms is $\\lambda = \\sigma_n \\sqrt{2 \\ln(N)}$, which corresponds to the universal threshold for wavelet denoising and is related to the Dantzig selector.\n\n    d.  **Assess Recovery Success:**\n        -   A trial is deemed successful if the support of the true signal is correctly identified.\n        -   Determine the estimated support $\\hat{\\mathcal{S}}$ by finding the indices of the $K$ largest-magnitude coefficients in the reconstructed vector $\\hat{x}$.\n        -   If $\\hat{\\mathcal{S}}$ is identical to the true support $\\mathcal{S}$, increment the success counter.\n\n3.  **Evaluate Performance:**\n    -   After completing all $L$ trials, calculate the empirical success probability (recovery rate) as $P_{\\text{success}} = (\\text{Number of successful trials}) / L$.\n    -   Compare this rate to the target reliability: the design is considered empirically validated if $P_{\\text{success}} \\ge 1 - \\delta = 0.99$.", "answer": "$$\\boxed{776}$$", "id": "3715729"}, {"introduction": "A key motivation for using non-uniform sampling (NUS) is to save experimental time, but how does this impact sensitivity? This practice explores the fundamental trade-off between acquisition speed and the final signal-to-noise ratio ($SNR$) by comparing NUS with traditional uniform sampling under conditions of equal total measurement time. You will derive the relationship between the two, revealing the inherent $SNR$ cost associated with non-linear reconstruction and providing a deeper insight into the performance characteristics of NUS-based methods [@problem_id:3715677].", "problem": "Consider a two-dimensional Nuclear Magnetic Resonance (NMR) experiment used for spectrometric identification of organic compounds, where the indirect evolution dimension is discretely sampled at $N$ uniformly spaced evolution times. Let the indirect-dimension time-domain signal be modeled as a sum of $K$ isolated complex exponentials (peaks) with additive independent, zero-mean, circular Gaussian noise of per-scan variance $\\sigma^2$. Assume one peak of interest is isolated and exactly on-grid in frequency, with indirect-dimension amplitude denoted by $A$ in the fully sampled, properly phase-corrected spectrum (i.e., the peak amplitude that would be obtained under ideal uniform sampling and perfect Fourier reconstruction without regularization).\n\nTwo acquisition strategies are compared at equal total instrument time:\n\n- Uniform sampling: Acquire all $N$ evolution times with one scan per time point.\n- Non-Uniform Sampling (NUS): Randomly select $M$ out of $N$ evolution times to acquire. To maintain equal total instrument time, allocate $S = N/M$ scans to each acquired evolution time.\n\nAssume the following realistic and standard conditions:\n\n1. The noise in each acquired time-domain data point is independent and Gaussian with per-scan variance $\\sigma^2$, so the variance per acquired point scales inversely with the number of scans $S$, i.e., $\\sigma^2/S$.\n2. The indirect-dimension peak amplitude estimator under uniform sampling can be taken as the matched-filter (least-squares) estimator aligned to the peak’s frequency, and therefore its estimator variance scales inversely with the number of independent samples accumulated.\n3. For the NUS case, assume the reconstruction is performed by convex optimization with $\\ell_1$ (ell-one) regularization, specifically Basis Pursuit Denoising, under a measurement operator that satisfies the Restricted Isometry Property (RIP) of order $2K$ with restricted isometry constant $\\delta_{2K} \\in (0,1)$. Under this condition, the reconstruction is stable to additive noise and exhibits a finite noise amplification relative to the matched-filter variance; represent this amplification by a dimensionless factor $\\kappa(\\delta_{2K}) > 1$ that depends only on $\\delta_{2K}$ and the sparsity order (you may leave $\\kappa$ symbolic as a function of $\\delta_{2K}$).\n\nStarting from the definitions above and first principles of noise accumulation and estimator variance for linear models, derive the expected signal-to-noise ratio of the reconstructed peak amplitude under NUS with $\\ell_1$ regularization in terms of $A$, $N$, $\\sigma$, and $\\kappa(\\delta_{2K})$. Then, compare it to the expected signal-to-noise ratio under uniform sampling at equal total instrument time. Express your final answer as a single closed-form analytic expression for the ratio of the NUS $\\ell_1$-regularized expected signal-to-noise ratio to the uniform-sampling expected signal-to-noise ratio. No numerical evaluation is required. The ratio is unitless and no rounding is necessary. State any assumptions you use explicitly in your derivation.", "solution": "The objective is to derive the ratio of the expected signal-to-noise ratio (SNR) for the Non-Uniform Sampling (NUS) case to that of the Uniform Sampling (US) case, $\\frac{\\text{SNR}_{\\text{NUS}}}{\\text{SNR}_{\\text{US}}}$. The SNR is defined as the ratio of the expected signal amplitude to the standard deviation of the amplitude estimator. To facilitate the derivation in a way that is consistent with the provided assumptions, we will work with the time-domain amplitude of the signal.\n\nLet the complex time-domain signal for the single peak of interest be $s(t_j) = A_{TD} \\exp(i\\omega_j t)$, where $A_{TD}$ is the real-valued time-domain amplitude at time $t=0$. The problem defines $A$ as the amplitude in the fully sampled spectrum. For a standard Discrete Fourier Transform (DFT) without normalization, the spectral amplitude $A$ is the coherent sum of the time-domain signal over all $N$ points: $A = N A_{TD}$. An estimator for $A$, denoted $\\hat{A}$, will be related to an estimator for $A_{TD}$, denoted $\\hat{A}_{TD}$, by $\\hat{A} = N \\hat{A}_{TD}$. The SNR for $A$ is then:\n$$\n\\text{SNR}_A = \\frac{E[\\hat{A}]}{\\sqrt{\\text{Var}(\\hat{A})}} = \\frac{N A_{TD}}{\\sqrt{N^2 \\text{Var}(\\hat{A}_{TD})}} = \\frac{N A_{TD}}{N \\sqrt{\\text{Var}(\\hat{A}_{TD})}} = \\frac{A_{TD}}{\\sqrt{\\text{Var}(\\hat{A}_{TD})}} = \\text{SNR}_{A_{TD}}\n$$\nThus, the ratio of SNRs is independent of whether we consider the time-domain or frequency-domain amplitude, as long as they are related by a simple scaling factor. We will proceed by calculating the SNR for the estimator of $A_{TD}$.\n\n**1. Signal-to-Noise Ratio for Uniform Sampling (US)**\n\nUnder the US strategy, we acquire $N$ data points, $y_j = s_j + n_j$, for $j \\in \\{0, 1, ..., N-1\\}$. Each point is acquired with one scan, so the complex noise $n_j$ at each point has variance $\\text{Var}(n_j) = E[|n_j|^2] = \\sigma^2$.\n\nThe problem states that the amplitude is estimated using a matched filter, which is equivalent to a least-squares estimator. The least-squares estimator for the time-domain amplitude $A_{TD}$ is obtained by projecting the data onto the known signal waveform (normalized):\n$$\n\\hat{A}_{TD,US} = \\frac{\\sum_{j=0}^{N-1} y_j \\exp(-i\\omega_j t)}{\\sum_{j=0}^{N-1} |\\exp(i\\omega_j t)|^2} = \\frac{1}{N} \\sum_{j=0}^{N-1} y_j \\exp(-i\\omega_j t)\n$$\nThe expected value of this estimator is $E[\\hat{A}_{TD,US}] = A_{TD}$, as the noise is zero-mean. The variance of the estimator is:\n$$\n\\text{Var}(\\hat{A}_{TD,US}) = \\text{Var}\\left(\\frac{1}{N} \\sum_{j=0}^{N-1} n_j \\exp(-i\\omega_j t)\\right)\n$$\nSince the noise terms $n_j$ are independent, the variance of the sum is the sum of the variances:\n$$\n\\text{Var}(\\hat{A}_{TD,US}) = \\frac{1}{N^2} \\sum_{j=0}^{N-1} \\text{Var}(n_j \\exp(-i\\omega_j t)) = \\frac{1}{N^2} \\sum_{j=0}^{N-1} E[|n_j|^2] = \\frac{1}{N^2} \\sum_{j=0}^{N-1} \\sigma^2 = \\frac{N\\sigma^2}{N^2} = \\frac{\\sigma^2}{N}\n$$\nThis result is consistent with Assumption 2, as the variance is inversely proportional to the number of samples, $N$. The SNR for the US case is:\n$$\n\\text{SNR}_{\\text{US}} = \\frac{E[\\hat{A}_{TD,US}]}{\\sqrt{\\text{Var}(\\hat{A}_{TD,US})}} = \\frac{A_{TD}}{\\sqrt{\\sigma^2/N}} = \\frac{A_{TD}\\sqrt{N}}{\\sigma}\n$$\n\n**2. Signal-to-Noise Ratio for Non-Uniform Sampling (NUS)**\n\nUnder the NUS strategy, we acquire $M$ data points. To maintain equal total time, each of these $M$ points is the average of $S = N/M$ scans. According to Assumption 1, the noise variance at each acquired point $y_m$ (for $m$ in the set of sampled indices $\\mathcal{I}$) is:\n$$\n\\sigma^2_{NUS,pt} = \\frac{\\sigma^2}{S} = \\frac{\\sigma^2}{N/M} = \\frac{M\\sigma^2}{N}\n$$\nThe problem specifies that the variance of the $\\ell_1$-reconstructed peak is a factor $\\kappa(\\delta_{2K})$ larger than the \"matched-filter variance\". We must first determine this reference variance. The matched-filter or least-squares estimator is the ideal linear estimator, and its variance represents the best possible performance for an unbiased linear estimator given the data, which corresponds to the Cramér-Rao Lower Bound (CRLB) for this problem. Let's calculate the variance of the LS estimator for $A_{TD}$ based on the $M$ NUS data points.\n$$\n\\hat{A}_{TD,LS-NUS} = \\frac{\\sum_{m \\in \\mathcal{I}} y_m \\exp(-i\\omega_m t)}{\\sum_{m \\in \\mathcal{I}} |\\exp(i\\omega_m t)|^2} = \\frac{1}{M} \\sum_{m \\in \\mathcal{I}} y_m \\exp(-i\\omega_m t)\n$$\nThe variance of this ideal estimator for the NUS data is:\n$$\n\\text{Var}(\\hat{A}_{TD,LS-NUS}) = \\frac{1}{M^2} \\sum_{m \\in \\mathcal{I}} \\text{Var}(n_m) = \\frac{1}{M^2} (M \\cdot \\sigma^2_{NUS,pt}) = \\frac{1}{M} \\left(\\frac{M\\sigma^2}{N}\\right) = \\frac{\\sigma^2}{N}\n$$\nThis is the \"matched-filter variance\" to which the $\\ell_1$ reconstruction is compared. It is crucial to note that this ideal variance, $\\sigma^2_{MF} = \\sigma^2/N$, is identical to the variance of the US estimator. This is a manifestation of the principle that for an ideal estimator, the ultimate precision is determined by the total acquired information, which is equivalent in both scenarios due to the equal total experiment time.\n\nAccording to Assumption 3, the variance of the actual amplitude estimator obtained from $\\ell_1$ reconstruction, $\\hat{A}_{TD,NUS}$, is amplified by the factor $\\kappa(\\delta_{2K})$:\n$$\n\\text{Var}(\\hat{A}_{TD,NUS}) = \\kappa(\\delta_{2K}) \\cdot \\sigma^2_{MF} = \\kappa(\\delta_{2K}) \\frac{\\sigma^2}{N}\n$$\nThe SNR for the NUS case is therefore:\n$$\n\\text{SNR}_{\\text{NUS}} = \\frac{E[\\hat{A}_{TD,NUS}]}{\\sqrt{\\text{Var}(\\hat{A}_{TD,NUS})}} = \\frac{A_{TD}}{\\sqrt{\\kappa(\\delta_{2K}) \\sigma^2/N}} = \\frac{A_{TD}\\sqrt{N}}{\\sigma\\sqrt{\\kappa(\\delta_{2K})}}\n$$\n\n**3. Ratio of SNRs**\n\nFinally, we compute the ratio of the NUS SNR to the US SNR:\n$$\n\\frac{\\text{SNR}_{\\text{NUS}}}{\\text{SNR}_{\\text{US}}} = \\frac{\\frac{A_{TD}\\sqrt{N}}{\\sigma\\sqrt{\\kappa(\\delta_{2K})}}}{\\frac{A_{TD}\\sqrt{N}}{\\sigma}} = \\frac{1}{\\sqrt{\\kappa(\\delta_{2K})}}\n$$\nThis result shows that, for a single on-grid peak, the SNR performance of NUS with $\\ell_1$ reconstruction is reduced compared to ideal uniform sampling by a factor related to the noise amplification of the reconstruction algorithm. The benefit of NUS lies in its ability to reconstruct spectra with many peaks from far fewer than $N$ samples, enabling dramatic time savings, a consideration not captured by this specific problem focused on equal-time SNR comparison for a single peak.", "answer": "$$\n\\boxed{\\frac{1}{\\sqrt{\\kappa(\\delta_{2K})}}}\n$$", "id": "3715677"}, {"introduction": "Proper data handling is paramount for successful reconstruction, and methods that work for traditional Fourier transform NMR are not always suitable for non-uniform sampling (NUS). This exercise tackles a critical processing pitfall: the premature application of apodization (window) functions to NUS data before non-linear reconstruction. Analyzing this scenario will reveal why this traditional practice is counterproductive for compressed sensing and instill a crucial best practice for the modern processing pipeline [@problem_id:3715757].", "problem": "A two-dimensional nuclear magnetic resonance (NMR) experiment for spectrometric identification of organic compounds acquires data non uniformly sampled in the indirect time dimension. Let the indirect dimension be denoted by time $t_1$ and the direct dimension by time $t_2$. The true time-domain signal is $x(t_1,t_2)$, and the acquisition uses a binary sampling mask $m(t_1)$ that selects a subset of evolution times in $t_1$. The measured data are modeled as\n$$y(t_1,t_2) = m(t_1)\\,x(t_1,t_2) + n(t_1,t_2),$$\nwhere $n(t_1,t_2)$ is additive noise that is stationary and white in $t_1$ with variance $\\sigma^2$ per acquired point. A practitioner considers multiplying each acquired point by a deterministic window function $w(t_1)$, for example, a cosine window $w(t_1) = \\cos\\!\\left(\\frac{\\pi t_1}{2 T}\\right)$ with $T$ being the maximum evolution time, prior to any reconstruction (e.g., compressed sensing or maximum entropy). This yields modified data\n$$y'(t_1,t_2) = m(t_1)\\,w(t_1)\\,x(t_1,t_2) + w(t_1)\\,n(t_1,t_2).$$\nAssume the goal of reconstruction is to recover the unwindowed $x(t_1,t_2)$ (or its Fourier transform along $t_1$), and that the reconstruction approach exploits sparsity of resonances in the frequency domain along $t_1$.\n\nBased on first principles of sampling and reconstruction, select all statements that are correct about applying such apodization prior to reconstruction in non uniform sampling (NUS) multidimensional NMR:\n\nA. Applying a cosine window $w(t_1)$ to the acquired $t_1$ points before reconstruction universally improves recovery in compressed sensing by increasing mutual incoherence between the sampling basis and the sparsifying basis.\n\nB. Multiplying $x(t_1,t_2)$ by $w(t_1)$ in the time domain prior to reconstruction corresponds to convolving the $t_1$-frequency spectrum with the Fourier transform $W(\\omega_1)$ of $w(t_1)$, which broadens resonances and reduces sparsity; this generally degrades compressed sensing or maximum entropy reconstructions unless $w(t_1)$ is explicitly included in the forward model and the target of reconstruction remains the unwindowed $x(t_1,t_2)$.\n\nC. Because NUS already randomizes truncation artefacts, apodization prior to reconstruction is redundant but harmless; it only rescales peak amplitudes and does not alter resolution or the noise statistics relevant to reconstruction.\n\nD. If the window $w(t_1)$ is incorporated into the forward operator used by the reconstruction algorithm, pre-reconstruction apodization becomes exactly equivalent to applying the same window after reconstruction in the time domain, with no impact on noise weighting or artefact structure.\n\nE. Traditional windows such as cosine are often counterproductive for NUS in organic structure elucidation: they overemphasize early $t_1$ points, broaden the effective point-spread function (reducing resolution and sparsity), and misweight the noise across acquired points; best practice is to avoid pre-apodization and, if desired, use mild post-reconstruction line-shaping or regularization to control artefacts.", "solution": "The core of non-linear reconstruction methods like compressed sensing (CS) or maximum entropy (MaxEnt) is to solve an optimization problem. This problem typically seeks a solution $\\hat{x}$ that is sparse in a transform domain (here, the frequency domain $\\omega_1$, related to $t_1$ by a Fourier Transform $\\mathcal{F}$) and consistent with the measured data. A common formulation is:\n$$ \\min_{\\hat{x}} \\lambda \\|\\mathcal{F}_{t_1}(\\hat{x})\\|_1 \\quad \\text{subject to} \\quad \\|m(t_1)\\hat{x}(t_1,t_2) - y(t_1,t_2)\\|_2^2 \\leq \\epsilon $$\nThe term $\\|m(t_1)\\hat{x}(t_1,t_2) - y(t_1,t_2)\\|_2^2$ is the data fidelity term, which measures the squared error between the proposed solution and the acquired data points. The use of the L2-norm is statistically optimal when the noise $n(t_1,t_2)$ is independent and identically distributed (i.i.d.) Gaussian noise, which is consistent with the problem's description of stationary, white noise with constant variance $\\sigma^2$.\n\nLet's analyze the effects of applying the window function $w(t_1)$ to the data *before* reconstruction, resulting in $y'(t_1, t_2)$.\n\n1.  **Effect on the Signal Sparsity:** The practitioner applies the window to the true signal, creating an effective signal $x_w(t_1, t_2) = w(t_1)x(t_1, t_2)$. The Fourier transform of $x_w$ along $t_1$ is, by the convolution theorem, the convolution of the individual Fourier transforms:\n    $$ \\mathcal{F}_{t_1}\\{x_w\\}(\\omega_1, t_2) = (\\mathcal{F}_{t_1}\\{w\\} * \\mathcal{F}_{t_1}\\{x\\})(\\omega_1, t_2) = W(\\omega_1) * X(\\omega_1, t_2) $$\n    The spectrum of the original signal, $X(\\omega_1, t_2)$, is assumed to be sparse, meaning it consists of sharp resonances (ideally, delta functions or narrow Lorentzians). The Fourier transform of the window, $W(\\omega_1)$, is its lineshape function (e.g., for a cosine window, it is a sum of two shifted sinc-like functions). Convolving the sharp, sparse spectrum $X(\\omega_1, t_2)$ with $W(\\omega_1)$ broadens every resonance. A broader resonance is less sparse, as it requires more non-zero coefficients to be represented in the frequency domain. This fundamentally undermines the core assumption of sparsity upon which the reconstruction relies, generally leading to a degraded result (e.g., loss of resolution).\n\n2.  **Effect on the Noise Statistics:** The original noise $n(t_1, t_2)$ is stationary with variance $\\sigma^2$ at each acquired point. The new noise term in the modified data is $n'(t_1, t_2) = w(t_1)n(t_1, t_2)$. The variance of this new noise is no longer constant:\n    $$ \\text{Var}[n'(t_1, t_2)] = \\text{Var}[w(t_1)n(t_1, t_2)] = w(t_1)^2 \\text{Var}[n(t_1, t_2)] = w(t_1)^2 \\sigma^2 $$\n    Since $w(t_1)$ is not constant (e.g., $w(t_1)$ for a cosine window decreases from $1$ at $t_1=0$ to $0$ at $t_1=T$), the noise becomes heteroscedastic (non-stationary). The variance is highest for early $t_1$ points and decreases for later points. If a standard reconstruction algorithm using an unweighted L2-norm for data fidelity is applied to this modified data $y'$, it implicitly (and incorrectly) assumes the noise is still stationary. This misweighting of the noise is statistically suboptimal. The algorithm will place too much emphasis on fitting the early, high-amplitude but noisy data points, and too little on the later, low-amplitude but less noisy points.\n\nBased on these two fundamental effects—reduction of sparsity and introduction of heteroscedastic noise—pre-reconstruction apodization is generally detrimental to modern NUS reconstruction methods.\n\n**Evaluation of Options**\n\n**A. Applying a cosine window $w(t_1)$ to the acquired $t_1$ points before reconstruction universally improves recovery in compressed sensing by increasing mutual incoherence between the sampling basis and the sparsifying basis.**\nThis statement is incorrect. Mutual incoherence is a property related to the sampling operator $m(t_1)$ and the sparsifying basis (Fourier basis). Applying a window `w(t_1)` amounts to weighting the rows of the sensing matrix. There is no principle suggesting this would *universally improve* incoherence. Moreover, the primary and guaranteed effect of windowing is the reduction of signal sparsity, which is highly detrimental to compressed sensing performance. The claim is unsubstantiated and contradicts the known principles of CS.\n**Verdict:** **Incorrect**.\n\n**B. Multiplying $x(t_1,t_2)$ by $w(t_1)$ in the time domain prior to reconstruction corresponds to convolving the $t_1$-frequency spectrum with the Fourier transform $W(\\omega_1)$ of $w(t_1)$, which broadens resonances and reduces sparsity; this generally degrades compressed sensing or maximum entropy reconstructions unless $w(t_1)$ is explicitly included in the forward model and the target of reconstruction remains the unwindowed $x(t_1,t_2)$.**\nThis statement accurately describes the consequences. The first clause correctly states the convolution theorem. The second clause correctly deduces that this convolution broadens resonances and thus reduces sparsity, which is the main assumption of CS and MaxEnt, leading to performance degradation. The final clause correctly identifies the only theoretically sound (though complex and often unnecessary) way to proceed with windowed data: modify the forward model to solve for the original, unwindowed signal. This demonstrates a complete understanding of the issue.\n**Verdict:** **Correct**.\n\n**C. Because NUS already randomizes truncation artefacts, apodization prior to reconstruction is redundant but harmless; it only rescales peak amplitudes and does not alter resolution or the noise statistics relevant to reconstruction.**\nThis statement is factually incorrect on multiple accounts. While apodization might be considered redundant in its traditional role of suppressing truncation sidelobes (which NUS handles differently), it is far from harmless. It directly alters resolution by broadening lineshapes (convolution in the frequency domain). It most certainly alters the noise statistics by making the noise variance dependent on $t_1$ ($w(t_1)^2 \\sigma^2$), a critical change for any statistically-aware algorithm. It does much more than simply rescaling peak amplitudes.\n**Verdict:** **Incorrect**.\n\n**D. If the window $w(t_1)$ is incorporated into the forward operator used by the reconstruction algorithm, pre-reconstruction apodization becomes exactly equivalent to applying the same window after reconstruction in the time domain, with no impact on noise weighting or artefact structure.**\nThis statement is incorrect. Applying a process *before* a non-linear operation (CS reconstruction) is not equivalent to applying it *after*. Let $\\mathcal{R}$ be the non-linear reconstruction operator. The statement claims $\\mathcal{R}(w \\cdot y) = w \\cdot \\mathcal{R}(y)$. This is not true. In the pre-apodization case, the reconstruction minimizes a cost function where the noise is heteroscedastic ($w(t_1)n(t_1,t_2)$). In the post-apodization case, the reconstruction operates on the original data with stationary noise $n(t_1,t_2)$, and the window is applied to the final result. The noise weighting is fundamentally different, and the resulting artefact structures and signal estimates will not be identical. The claim of \"exactly equivalent\" is false.\n**Verdict:** **Incorrect**.\n\n**E. Traditional windows such as cosine are often counterproductive for NUS in organic structure elucidation: they overemphasize early $t_1$ points, broaden the effective point-spread function (reducing resolution and sparsity), and misweight the noise across acquired points; best practice is to avoid pre-apodization and, if desired, use mild post-reconstruction line-shaping or regularization to control artefacts.**\nThis statement provides an accurate and comprehensive summary of the practical implications.\n- \"counterproductive\": Correct, for the reasons derived above.\n- \"overemphasize early $t_1$ points\": Correct. An unweighted L2-norm fidelity term applied to windowed data will be dominated by the larger-magnitude values at early $t_1$, which also have the highest noise variance.\n- \"broaden the effective point-spread function (reducing resolution and sparsity)\": Correct. This is a direct consequence of the convolution described in option B.\n- \"misweight the noise across acquired points\": Correct. The noise becomes non-stationary ($Var = w(t_1)^2 \\sigma^2$), but a standard algorithm will treat it as stationary, which constitutes misweighting.\n- \"best practice is to avoid pre-apodization...\": This is the logical consequence and the accepted wisdom in the field of NUS-NMR.\n- \"use mild post-reconstruction line-shaping...\": This is the correct procedure. Any desired line-shaping for cosmetic purposes should be done by linear filtering *after* the non-linear reconstruction is complete, so as not to interfere with the reconstruction process itself.\n**Verdict:** **Correct**.", "answer": "$$\\boxed{\\text{B, E}}$$", "id": "3715757"}]}