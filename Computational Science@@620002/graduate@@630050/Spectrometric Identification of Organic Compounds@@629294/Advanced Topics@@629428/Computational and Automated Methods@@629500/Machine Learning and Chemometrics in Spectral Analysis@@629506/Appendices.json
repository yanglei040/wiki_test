{"hands_on_practices": [{"introduction": "The most sophisticated chemometric models are still reliant on the quality of the input data. A crucial skill for any analyst is understanding how real-world imperfections, such as instrumental error, affect model outputs. This first practice provides a foundational exercise in error propagation, asking you to derive how a small, systematic deviation in a spectrophotometer's path length impacts concentration estimates from a classical Ordinary Least Squares (OLS) model. By working through this derivation [@problem_id:3711398], you will gain fundamental insight into the sensitivity of linear models and the importance of instrument calibration.", "problem": "Consider a multicomponent mixture of $p$ organic analytes measured by Ultraviolet-Visible (UV-Vis) absorbance spectroscopy under the Beer–Lambert law. Let $m$ wavelengths $\\{\\lambda_{i}\\}_{i=1}^{m}$ be used, and define the $m \\times p$ molar absorptivity matrix $\\mathbf{E}$ with entries $E_{i,j}=\\varepsilon_{j}(\\lambda_{i})$. For a nominal cuvette path length $l_{0}$, the ideal absorbance vector is $\\mathbf{y} \\in \\mathbb{R}^{m}$ with\n$$\n\\mathbf{y} = l_{0}\\,\\mathbf{E}\\,\\mathbf{c},\n$$\nwhere $\\mathbf{c} \\in \\mathbb{R}^{p}$ is the vector of analyte concentrations. In practice, there is an unknown, small, wavelength-independent relative path length deviation $\\delta$ (that is, the effective path length is $l_{0}(1+\\delta)$), and there may also be additive spectrally structured noise $\\boldsymbol{\\epsilon} \\in \\mathbb{R}^{m}$. The observed absorbance vector is therefore modeled as\n$$\n\\mathbf{y}_{\\text{obs}} = \\mathbf{y}\\,(1+\\delta) + \\boldsymbol{\\epsilon}.\n$$\nA chemometric concentration estimate $\\widehat{\\mathbf{c}}$ is obtained by ordinary least squares using the nominal design matrix $\\mathbf{X}=l_{0}\\,\\mathbf{E}$:\n$$\n\\widehat{\\mathbf{c}}=\\arg\\min_{\\mathbf{c}\\in\\mathbb{R}^{p}} \\left\\| \\mathbf{y}_{\\text{obs}} - \\mathbf{X}\\,\\mathbf{c} \\right\\|_{2}^{2}.\n$$\nAssume that the calibration basis is exact in the sense that $\\operatorname{col}(\\mathbf{X})$ contains the ideal absorbance vector $\\mathbf{y}$, and neglect additive noise by taking $\\boldsymbol{\\epsilon}=\\mathbf{0}$. Under these conditions and to first order in small $|\\delta| \\ll 1$, derive the signed relative error in the concentration estimate that is common to all analytes, defined componentwise by\n$$\nr \\equiv \\frac{\\widehat{c}_{j} - c_{j}}{c_{j}} \\quad \\text{for any } j \\in \\{1,\\dots,p\\}.\n$$\nProvide your final result as a single closed-form symbolic expression in terms of $\\delta$ only. Do not introduce any additional symbols. If any approximation is made, it must be justified from first principles. No rounding is required.", "solution": "The problem requires the derivation of the signed relative error in concentration estimates obtained via Ordinary Least Squares (OLS) when there is a systematic, wavelength-independent error in the spectrophotometer's cuvette path length.\n\nFirst, we establish the OLS estimator for the concentration vector $\\mathbf{c}$. The OLS estimate, denoted $\\widehat{\\mathbf{c}}$, is the vector that minimizes the sum of squared residuals, which is the squared Euclidean norm of the residual vector $\\mathbf{y}_{\\text{obs}} - \\mathbf{X}\\,\\mathbf{c}$.\n$$\n\\widehat{\\mathbf{c}} = \\arg\\min_{\\mathbf{c}\\in\\mathbb{R}^{p}} \\left\\| \\mathbf{y}_{\\text{obs}} - \\mathbf{X}\\,\\mathbf{c} \\right\\|_{2}^{2}\n$$\nThe solution to this minimization problem is found via the normal equations, which are obtained by setting the gradient of the objective function with respect to $\\mathbf{c}$ to zero. The resulting expression for $\\widehat{\\mathbf{c}}$ is:\n$$\n\\widehat{\\mathbf{c}} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}_{\\text{obs}}\n$$\nThis solution is unique provided that the matrix $\\mathbf{X}^{\\top}\\mathbf{X}$ is invertible, which is equivalent to the columns of $\\mathbf{X}$ being linearly independent. Since $\\mathbf{X} = l_{0}\\mathbf{E}$, this requires the columns of the molar absorptivity matrix $\\mathbf{E}$ to be linearly independent. This is a standard prerequisite for uniquely identifying the concentrations of the $p$ analytes from their mixed spectrum, and we assume it holds.\n\nThe problem states that the observed absorbance vector, $\\mathbf{y}_{\\text{obs}}$, is related to the ideal absorbance vector, $\\mathbf{y}$, by the model:\n$$\n\\mathbf{y}_{\\text{obs}} = \\mathbf{y}\\,(1+\\delta) + \\boldsymbol{\\epsilon}\n$$\nWe are instructed to neglect additive noise, which means we set $\\boldsymbol{\\epsilon} = \\mathbf{0}$. Therefore, the observed absorbance is:\n$$\n\\mathbf{y}_{\\text{obs}} = \\mathbf{y}\\,(1+\\delta)\n$$\nThe ideal absorbance vector $\\mathbf{y}$ is defined by the Beer-Lambert law as $\\mathbf{y} = l_{0}\\mathbf{E}\\mathbf{c}$. The nominal design matrix is given as $\\mathbf{X} = l_{0}\\mathbf{E}$. Substituting this into the equation for $\\mathbf{y}$, we can write the ideal absorbance vector in terms of the design matrix and the true concentration vector $\\mathbf{c}$:\n$$\n\\mathbf{y} = \\mathbf{X}\\mathbf{c}\n$$\nNow, we can express $\\mathbf{y}_{\\text{obs}}$ in terms of $\\mathbf{X}$, $\\mathbf{c}$, and $\\delta$:\n$$\n\\mathbf{y}_{\\text{obs}} = (\\mathbf{X}\\mathbf{c})(1+\\delta)\n$$\nSubstitute this expression for $\\mathbf{y}_{\\text{obs}}$ into the OLS estimator equation for $\\widehat{\\mathbf{c}}$:\n$$\n\\widehat{\\mathbf{c}} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top} [(\\mathbf{X}\\mathbf{c})(1+\\delta)]\n$$\nThe term $(1+\\delta)$ is a scalar and can be moved outside the matrix multiplication:\n$$\n\\widehat{\\mathbf{c}} = (1+\\delta) (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}(\\mathbf{X}\\mathbf{c})\n$$\nWe can regroup the matrices:\n$$\n\\widehat{\\mathbf{c}} = (1+\\delta) [(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}(\\mathbf{X}^{\\top}\\mathbf{X})] \\mathbf{c}\n$$\nThe product of a matrix and its inverse is the identity matrix, $\\mathbf{I}$:\n$$\n(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}(\\mathbf{X}^{\\top}\\mathbf{X}) = \\mathbf{I}\n$$\nThus, the expression for $\\widehat{\\mathbf{c}}$ simplifies to:\n$$\n\\widehat{\\mathbf{c}} = (1+\\delta) \\mathbf{I} \\mathbf{c} = (1+\\delta)\\mathbf{c}\n$$\nThis vector equation shows that the estimated concentration vector $\\widehat{\\mathbf{c}}$ is equal to the true concentration vector $\\mathbf{c}$ scaled by a factor of $(1+\\delta)$.\n\nWe are asked to find the signed relative error, $r$, which is defined component-wise for any analyte $j$ as:\n$$\nr \\equiv \\frac{\\widehat{c}_{j} - c_{j}}{c_{j}}\n$$\nFrom the vector equation $\\widehat{\\mathbf{c}} = (1+\\delta)\\mathbf{c}$, we can write the corresponding component-wise relation for each analyte $j \\in \\{1, \\dots, p\\}$:\n$$\n\\widehat{c}_{j} = (1+\\delta)c_{j}\n$$\nSubstituting this into the definition of $r$, assuming $c_{j} \\ne 0$:\n$$\nr = \\frac{(1+\\delta)c_{j} - c_{j}}{c_{j}}\n$$\nFactor out $c_{j}$ from the numerator:\n$$\nr = \\frac{c_{j}((1+\\delta) - 1)}{c_{j}}\n$$\nCanceling the non-zero term $c_{j}$ from the numerator and denominator gives:\n$$\nr = (1+\\delta) - 1 = \\delta\n$$\nThis result is independent of the analyte index $j$, confirming that the relative error is common to all analytes. This result is exact under the given assumptions ($\\boldsymbol{\\epsilon}=\\mathbf{0}$ and invertible $\\mathbf{X}^{\\top}\\mathbf{X}$). The problem asks for the result to be derived to first order in small $|\\delta| \\ll 1$. Since our exact result for the error, $r=\\delta$, is already a linear function of $\\delta$, it is identical to its first-order Taylor series expansion around $\\delta=0$. Therefore, no approximation is necessary, and the first-order result is simply $\\delta$.", "answer": "$$\n\\boxed{\\delta}\n$$", "id": "3711398"}, {"introduction": "Raw spectral data is rarely perfect; it is often corrupted by baseline drift, random noise, and other artifacts. Preprocessing is therefore a critical first step in any analysis, but choosing the right method and parameters can be challenging. This practice moves beyond simply applying a filter to a core task in modern chemometrics: the data-driven optimization of a preprocessing workflow. You will design and implement an algorithm to select the optimal parameters for a Savitzky-Golay filter by maximizing a custom objective function that balances class separability against noise-induced artifacts [@problem_id:3711432], a powerful approach for developing robust analytical methods.", "problem": "You are to design and implement a data-driven algorithm that selects parameters for the Savitzky–Golay (SG) filter that maximize class separability of functional groups in simulated infrared absorbance spectra while simultaneously controlling false derivative artifacts. The approach must be grounded in well-established chemometric principles and signal processing definitions, starting from the Beer–Lambert law and the local polynomial approximation underpinning the SG filter. Your solution must produce a single numerical result per test case and aggregate all results in the specified output format.\n\nBegin from the Beer–Lambert law: for a wavenumber axis point denoted by $\\nu$, the absorbance $A(\\nu)$ of a multi-component organic sample is modeled as\n$$\nA(\\nu) = \\sum_{i=1}^{K} \\varepsilon_i(\\nu)\\,c_i\\,L + \\beta(\\nu) + \\eta(\\nu),\n$$\nwhere $\\varepsilon_i(\\nu)$ is the molar absorptivity of component $i$, $c_i$ is its concentration, $L$ is the optical path length, $\\beta(\\nu)$ is a slowly varying baseline contribution, and $\\eta(\\nu)$ is additive stochastic noise. In the absence of exact tabulated $\\varepsilon_i(\\nu)$, model each functional group signature by a sum of Gaussian bands\n$$\n\\varepsilon_i(\\nu) \\propto \\sum_{j=1}^{M_i} I_{ij}\\,\\exp\\left(-\\frac{(\\nu-\\nu_{ij})^2}{2\\,\\sigma_{ij}^2}\\right),\n$$\nwith center positions $\\nu_{ij}$, widths $\\sigma_{ij}$, and intensities $I_{ij}$ chosen to emulate known infrared signatures (for example, alcohol and alkane groups). The Savitzky–Golay (SG) filter performs local polynomial regression of degree $p$ over an odd window of length $m$ to estimate smoothed values and derivatives; applying the first derivative emphasizes slope changes associated with absorbance band edges, which often enhances class separability. However, setting $(m,p)$ too aggressively can amplify high-frequency noise, introducing false derivative artifacts.\n\nTo quantify class separability after SG differentiation, use the classical Fisher trace criterion from Linear Discriminant Analysis (LDA). Let $G$ be the number of classes (here $G=2$), $n_g$ be the sample count of class $g$, $\\boldsymbol{\\mu}_g$ be the mean feature vector of class $g$ after SG first derivative transformation, and $\\boldsymbol{\\mu}$ be the global mean. Define the between-class scatter trace and the within-class scatter trace as\n$$\n\\mathrm{tr}(S_B) = \\sum_{g=1}^{G} n_g\\,\\lVert \\boldsymbol{\\mu}_g - \\boldsymbol{\\mu} \\rVert_2^2,\\qquad\n\\mathrm{tr}(S_W) = \\sum_{g=1}^{G} \\sum_{i=1}^{n_g} \\lVert \\mathbf{x}_i^{(g)} - \\boldsymbol{\\mu}_g \\rVert_2^2,\n$$\nwhere $\\mathbf{x}_i^{(g)}$ denotes the $i$-th sample of class $g$ (the full derivative spectrum vector). The separability score is then\n$$\nJ = \\frac{\\mathrm{tr}(S_B)}{\\mathrm{tr}(S_W)}.\n$$\nTo quantify false derivative artifacts, use the Discrete Fourier Transform (DFT) energy ratio in the second derivative domain. Let $\\widehat{\\mathbf{y}}^{(2)}$ be the real-valued spectrum of the SG second derivative for a single sample, and let $\\mathcal{F}\\{\\widehat{\\mathbf{y}}^{(2)}\\}$ denote its real-valued, one-sided DFT (e.g., using a real-input fast Fourier transform). By Parseval’s theorem, total energy can be measured in the frequency domain. For a cutoff proportion $\\alpha \\in (0,1)$ of the Nyquist frequency, define the artifact ratio for a single sample as\n$$\nr = \\frac{\\sum_{f \\geq f_c(\\alpha)} \\lvert Y(f) \\rvert^2}{\\sum_{f \\geq 0} \\lvert Y(f) \\rvert^2},\n$$\nwhere $Y(f)$ are the DFT coefficients and $f_c(\\alpha)$ is the index corresponding to the fraction $\\alpha$. The overall artifact metric $A$ is the average of $r$ across all samples. Combine separability and artifact penalty into a single scalar objective\n$$\nO(m,p;\\lambda,\\alpha) = J(m,p) - \\lambda\\,A(m,p;\\alpha),\n$$\nwhere $\\lambda \\ge 0$ controls the penalty strength. The parameter selection problem is to choose $(m,p)$ that maximizes $O$ under the constraints that $m$ is odd and $m > p$.\n\nAlgorithm design requirements:\n- Simulate absorbance spectra for two functional groups using Gaussian bands, additive baseline, and stochastic noise consistent with the Beer–Lambert model. Use a fixed random seed per test case to ensure reproducibility.\n- For each candidate pair $(m,p)$ in a provided grid, compute SG first and second derivatives, evaluate $J$, evaluate $A$, and compute $O$. Select the pair with the largest $O$. Break ties by preferring smaller $A$, and then by smaller $m$.\n- Do not estimate or use any shortcut closed-form for optimal $(m,p)$; the only acceptable method is explicit computation following the definitions above.\n\nUnits: Since the output is purely numerical parameter selection, no physical unit is required in the final answer. Angles are not involved. All numeric outputs must be plain integers.\n\nTest suite:\n- Case $1$ (happy path): two classes with moderately separated bands and moderate noise.\n  - Axis length $N=1024$, wavenumber axis $\\nu \\in [650, 4000]$ sampled uniformly with $N$ points.\n  - Per-class sample count $n_g=40$.\n  - Baseline slope $b=10^{-4}$, noise standard deviation $\\sigma=2\\times 10^{-3}$.\n  - Artifact cutoff proportion $\\alpha=0.5$, penalty weight $\\lambda=0.1$.\n  - Candidate windows $m \\in \\{5,7,9,11\\}$, candidate polynomial orders $p \\in \\{2,3,4\\}$.\n- Case $2$ (boundary condition: high noise): same axis, stronger baseline and noise.\n  - $N=1024$, $n_g=40$, $b=2\\times 10^{-4}$, $\\sigma=1\\times 10^{-2}$.\n  - $\\alpha=0.4$, $\\lambda=1.0$.\n  - $m \\in \\{7,9,11,13\\}$, $p \\in \\{2,3,4\\}$.\n- Case $3$ (edge case: high resolution, narrow bands): tighter, narrower absorption features for one class and low noise.\n  - $N=2048$, $n_g=30$, $b=0$, $\\sigma=1\\times 10^{-3}$.\n  - $\\alpha=0.6$, $\\lambda=0.05$.\n  - $m \\in \\{5,7\\}$, $p \\in \\{2,3\\}$.\n  - One class should have at least two narrow Gaussian bands (full width approximately $20$–$30$ wavenumber units) to test resolution.\n\nFunctional group modeling:\n- Class $0$ (alcohol-like): broad band near $\\nu\\approx 3300$ with width $\\sigma\\approx 120$ and intensity $I\\approx 1.0$, and additional bands near $\\nu\\approx 1710$ with $\\sigma\\approx 25$, $I\\approx 0.5$, and near $\\nu\\approx 1050$ with $\\sigma\\approx 30$, $I\\approx 0.7$.\n- Class $1$ (alkane-like): bands near $\\nu\\approx 2960$ with $\\sigma\\approx 40$, $I\\approx 1.0$, near $\\nu\\approx 2870$ with $\\sigma\\approx 40$, $I\\approx 0.8$, and near $\\nu\\approx 1460$ with $\\sigma\\approx 30$, $I\\approx 0.6$. For Case $3$, narrow two bands at $\\nu\\approx 2960$ and $\\nu\\approx 1460$ with $\\sigma\\approx 20$ and $I$ similar to above.\n\nImplementation constraints:\n- Use a fixed seed per case: seed $=123+\\text{case\\_index}$ where case index is $0$ for Case $1$, $1$ for Case $2$, and $2$ for Case $3$.\n- For each sample and each class, draw a multiplicative intensity factor $c$ uniformly from $[0.8, 1.2]$ per band and sum all bands, then add baseline $b\\,(\\nu-\\nu_{\\min})$, then add independent Gaussian noise with standard deviation $\\sigma$.\n- Apply the SG filter with the specified $(m,p)$ to compute the first derivative ($\\text{deriv}=1$) and second derivative ($\\text{deriv}=2$) of the spectra.\n\nFinal output specification:\n- For each test case, output the selected SG parameter pair as a two-element list $[m,p]$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each pair formatted as $[m,p]$. For example: $[[7,3],[11,3],[5,2]]$.", "solution": "The problem statement has been analyzed and is determined to be **valid**. It is scientifically sound, well-posed, and contains all necessary information and definitions to construct a unique and verifiable solution. The methodology is grounded in established principles of chemometrics and signal processing.\n\nThe task is to design a data-driven algorithm for selecting optimal parameters for a Savitzky–Golay (SG) filter. The optimization goal is to maximize the separability of spectral classes, representing different chemical functional groups, while simultaneously penalizing derivative artifacts caused by noise amplification. This is formulated as a grid-search optimization problem over the SG filter's window size $m$ and polynomial order $p$.\n\nThe algorithm proceeds in three main stages for each test case: data simulation, feature extraction and metric evaluation for each parameter pair, and finally, selection of the optimal pair.\n\n**1. Spectroscopic Data Simulation**\n\nThe foundation of the simulation is the Beer–Lambert law, which models the absorbance $A$ at a specific wavenumber $\\nu$ as a sum of contributions from different chemical components, a baseline drift, and noise:\n$$\nA(\\nu) = \\sum_{i=1}^{K} \\varepsilon_i(\\nu)\\,c_i\\,L + \\beta(\\nu) + \\eta(\\nu)\n$$\nHere, $\\varepsilon_i(\\nu)$ is the molar absorptivity of component $i$ at concentration $c_i$, $L$ is the optical path length (which we can set to $1$ without loss of generality by absorbing it into $\\varepsilon_i$), $\\beta(\\nu)$ is a background baseline, and $\\eta(\\nu)$ is stochastic noise.\n\nFollowing the problem specification, we model the characteristic signature $\\varepsilon_i(\\nu)$ of each functional group (class) as a sum of Gaussian bands:\n$$\n\\varepsilon_i(\\nu) \\propto \\sum_{j=1}^{M_i} I_{ij}\\,\\exp\\left(-\\frac{(\\nu-\\nu_{ij})^2}{2\\,\\sigma_{ij}^2}\\right)\n$$\nwhere $I_{ij}$, $\\nu_{ij}$, and $\\sigma_{ij}$ are the intensity, center position, and width of the $j$-th band for the $i$-th class, respectively. The parameters for two classes, representing alcohol-like and alkane-like groups, are provided.\n\nFor each simulated sample, sample-to-sample variability, analogous to concentration fluctuations, is introduced by multiplying each Gaussian band's intensity $I_{ij}$ by a random factor drawn from a uniform distribution $U(0.8, 1.2)$. The baseline is modeled as a linear function $\\beta(\\nu) = b\\,(\\nu - \\nu_{\\min})$, and the noise $\\eta(\\nu)$ is drawn from a Gaussian distribution with mean $0$ and a specified standard deviation $\\sigma$. The process is repeated to generate $n_g$ sample spectra for each of the $G=2$ classes on a discrete wavenumber axis $\\nu$ of length $N$. A fixed random seed is used for each test case to ensure reproducibility.\n\n**2. Feature Extraction and Objective Function Evaluation**\n\nFor each candidate pair of SG parameters $(m, p)$, where $m$ is the window size and $p$ is the polynomial order ($m>p$, $m$ is odd), we evaluate a composite objective function $O(m,p)$. This function is designed to balance class separability with artifact control.\n\nFirst, the raw absorbance spectra are transformed using the SG filter to compute their first and second derivatives. These derivatives serve as the features for evaluation.\n\n**2.1. Class Separability Metric ($J$)**\n\nThe first derivative of a spectrum enhances sharp features like band edges, which can improve class discrimination. To quantify this, we use the Fisher trace criterion, $J$, derived from Linear Discriminant Analysis (LDA). It is the ratio of the between-class scatter to the within-class scatter:\n$$\nJ(m,p) = \\frac{\\mathrm{tr}(S_B)}{\\mathrm{tr}(S_W)}\n$$\nLet $\\mathbf{x}_i^{(g)}$ denote the first-derivative spectrum (a vector of values at each wavenumber) of the $i$-th sample of class $g$. The mean spectrum for class $g$ is $\\boldsymbol{\\mu}_g$, and the global mean spectrum is $\\boldsymbol{\\mu}$. The traces of the between-class scatter matrix $S_B$ and within-class scatter matrix $S_W$ are computed as:\n$$\n\\mathrm{tr}(S_B) = \\sum_{g=1}^{G} n_g\\,\\lVert \\boldsymbol{\\mu}_g - \\boldsymbol{\\mu} \\rVert_2^2\n$$\n$$\n\\mathrm{tr}(S_W) = \\sum_{g=1}^{G} \\sum_{i=1}^{n_g} \\lVert \\mathbf{x}_i^{(g)} - \\boldsymbol{\\mu}_g \\rVert_2^2\n$$\nA higher value of $J$ indicates greater separation between the classes relative to their internal variation.\n\n**2.2. Artifact Penalty Metric ($A$)**\n\nWhile differentiation can enhance features, it also amplifies high-frequency noise. Overly aggressive filtering (e.g., small $m$, high $p$) can create spurious peaks, or \"false derivative artifacts,\" in the derivative spectra. To quantify this, we analyze the energy distribution in the frequency domain of the second derivative spectrum, which is even more sensitive to noise.\n\nFor each second derivative spectrum $\\widehat{\\mathbf{y}}^{(2)}$, its one-sided, real-valued Discrete Fourier Transform (DFT), denoted $\\mathcal{F}\\{\\widehat{\\mathbf{y}}^{(2)}\\}$, is computed. Let $Y(f)$ be the DFT coefficients. The artifact ratio $r$ for this single sample is the ratio of energy in the high-frequency region to the total energy:\n$$\nr = \\frac{\\sum_{f \\geq f_c(\\alpha)} \\lvert Y(f) \\rvert^2}{\\sum_{f \\geq 0} \\lvert Y(f) \\rvert^2}\n$$\nThe cutoff frequency index $f_c(\\alpha)$ is determined by a specified proportion $\\alpha$ of the Nyquist frequency range. The overall artifact metric, $A(m,p;\\alpha)$, is the average of $r$ across all samples from all classes. A higher value of $A$ signifies a greater proportion of high-frequency content, which is attributed to noise-induced artifacts.\n\n**2.3. Combined Objective Function ($O$)**\n\nThe two metrics are combined into a single scalar objective function $O$ to be maximized:\n$$\nO(m,p;\\lambda,\\alpha) = J(m,p) - \\lambda\\,A(m,p;\\alpha)\n$$\nThe hyperparameter $\\lambda \\ge 0$ is a penalty weight that controls the trade-off: a larger $\\lambda$ places more importance on suppressing artifacts.\n\n**3. Optimization and Parameter Selection**\n\nThe algorithm performs a grid search over the provided sets of candidate pairs $(m, p)$. For each valid pair, it simulates the full dataset of spectra, applies the SG filter to compute first and second derivatives, and calculates the objective function $O(m,p)$.\n\nAfter evaluating all pairs in the grid, the pair $(m,p)$ that yields the maximum value of $O$ is selected as the optimal one. The problem specifies a clear tie-breaking rule: if multiple pairs result in the same maximal $O$, the one with the smaller artifact metric $A$ is chosen. If a tie still persists, the one with the smaller window size $m$ is selected. This procedure guarantees a unique optimal parameter pair from the candidate set for each test case. The final output is the list of these optimal pairs for the sequence of test cases.", "answer": "```python\nimport numpy as np\nfrom scipy.signal import savgol_filter\nfrom scipy.fft import rfft\n\ndef generate_spectra(case_params, class_bands):\n    \"\"\"Generates a dataset of spectra for all classes.\"\"\"\n    N = case_params['N']\n    nu_min, nu_max = 650, 4000\n    wavenumber_axis = np.linspace(nu_min, nu_max, N)\n    \n    rng = np.random.default_rng(seed=case_params['seed'])\n    \n    all_spectra = []\n    for class_id in range(len(class_bands)):\n        class_spectra = []\n        bands = class_bands[class_id]\n        \n        for _ in range(case_params['ng']):\n            spectrum = np.zeros(N)\n            # Add Gaussian bands with random intensity\n            for band_params in bands:\n                nu_ij, sigma_ij, I_ij = band_params\n                c = rng.uniform(0.8, 1.2)\n                intensity = c * I_ij\n                gaussian = intensity * np.exp(-((wavenumber_axis - nu_ij)**2) / (2 * sigma_ij**2))\n                spectrum += gaussian\n            \n            # Add baseline\n            if case_params['b'] > 0:\n                spectrum += case_params['b'] * (wavenumber_axis - nu_min)\n            \n            # Add noise\n            noise = rng.normal(0, case_params['sigma'], N)\n            spectrum += noise\n            class_spectra.append(spectrum)\n        all_spectra.append(np.array(class_spectra))\n        \n    return all_spectra\n\ndef calculate_j_metric(class_data):\n    \"\"\"Calculates the Fisher trace criterion J.\"\"\"\n    n_classes = len(class_data)\n    if n_classes == 0:\n        return 0.0\n    \n    n_samples_per_class = [len(d) for d in class_data]\n    total_samples = sum(n_samples_per_class)\n    \n    if total_samples == 0:\n        return 0.0\n\n    all_data = np.vstack(class_data)\n    global_mean = np.mean(all_data, axis=0)\n    \n    class_means = [np.mean(d, axis=0) for d in class_data]\n    \n    # Between-class scatter trace\n    tr_S_B = 0.0\n    for g in range(n_classes):\n        tr_S_B += n_samples_per_class[g] * np.sum((class_means[g] - global_mean)**2)\n        \n    # Within-class scatter trace\n    tr_S_W = 0.0\n    for g in range(n_classes):\n        tr_S_W += np.sum((class_data[g] - class_means[g])**2)\n        \n    if tr_S_W == 0:\n        return np.inf if tr_S_B > 0 else 0.0\n        \n    return tr_S_B / tr_S_W\n\ndef calculate_a_metric(deriv2_spectra_all, alpha):\n    \"\"\"Calculates the artifact metric A.\"\"\"\n    r_values = []\n    \n    for spectrum in deriv2_spectra_all:\n        dft_coeffs = rfft(spectrum)\n        dft_energy = np.abs(dft_coeffs)**2\n        \n        total_energy = np.sum(dft_energy)\n        if total_energy == 0:\n            r_values.append(0.0)\n            continue\n            \n        n_fft = len(dft_coeffs)\n        cutoff_idx = int(alpha * (n_fft - 1))\n        \n        high_freq_energy = np.sum(dft_energy[cutoff_idx:])\n        \n        r_values.append(high_freq_energy / total_energy)\n        \n    return np.mean(r_values) if r_values else 0.0\n\ndef solve_case(case_params, class_bands):\n    \"\"\"Solves a single test case.\"\"\"\n    all_raw_spectra = generate_spectra(case_params, class_bands)\n    \n    # Combine all spectra for easier processing\n    all_spectra_flat = np.vstack(all_raw_spectra)\n    \n    results = []\n    for m in case_params['m_grid']:\n        for p in case_params['p_grid']:\n            if not (m > p and m % 2 != 0):\n                continue\n            \n            # 1. Calculate J metric using 1st derivatives\n            deriv1_spectra_all = savgol_filter(all_spectra_flat, window_length=m, polyorder=p, deriv=1, axis=1)\n            deriv1_by_class = np.split(deriv1_spectra_all, np.cumsum([len(c) for c in all_raw_spectra])[:-1])\n            J = calculate_j_metric(deriv1_by_class)\n            \n            # 2. Calculate A metric using 2nd derivatives\n            deriv2_spectra_all = savgol_filter(all_spectra_flat, window_length=m, polyorder=p, deriv=2, axis=1)\n            A = calculate_a_metric(deriv2_spectra_all, case_params['alpha'])\n\n            # 3. Calculate objective function O\n            O = J - case_params['lambda'] * A\n            \n            results.append({'O': O, 'A': A, 'm': m, 'p': p})\n\n    # Find the best parameters according to the tie-breaking rules\n    # 1. Maximize O -> Sort by -O\n    # 2. Minimize A -> Sort by A\n    # 3. Minimize m -> Sort by m\n    best_result = sorted(results, key=lambda x: (-x['O'], x['A'], x['m']))[0]\n    \n    return [best_result['m'], best_result['p']]\n\ndef solve():\n    \"\"\"Main function to define test cases and run the solver.\"\"\"\n    \n    # Class 0: alcohol-like bands\n    bands_c0 = [\n        (3300, 120, 1.0),\n        (1710, 25, 0.5),\n        (1050, 30, 0.7)\n    ]\n    # Class 1: alkane-like bands (standard)\n    bands_c1_std = [\n        (2960, 40, 1.0),\n        (2870, 40, 0.8),\n        (1460, 30, 0.6)\n    ]\n    # Class 1: alkane-like bands (Case 3 modification)\n    bands_c1_case3 = [\n        (2960, 20, 1.0), # Narrowed\n        (2870, 40, 0.8),\n        (1460, 20, 0.6)  # Narrowed\n    ]\n\n    test_cases = [\n        {\n            'N': 1024, 'ng': 40, 'b': 1e-4, 'sigma': 2e-3, \n            'alpha': 0.5, 'lambda': 0.1,\n            'm_grid': [5, 7, 9, 11], 'p_grid': [2, 3, 4],\n            'bands': [bands_c0, bands_c1_std]\n        },\n        {\n            'N': 1024, 'ng': 40, 'b': 2e-4, 'sigma': 1e-2, \n            'alpha': 0.4, 'lambda': 1.0,\n            'm_grid': [7, 9, 11, 13], 'p_grid': [2, 3, 4],\n            'bands': [bands_c0, bands_c1_std]\n        },\n        {\n            'N': 2048, 'ng': 30, 'b': 0.0, 'sigma': 1e-3, \n            'alpha': 0.6, 'lambda': 0.05,\n            'm_grid': [5, 7], 'p_grid': [2, 3],\n            'bands': [bands_c0, bands_c1_case3]\n        }\n    ]\n\n    final_results = []\n    for i, case in enumerate(test_cases):\n        case['seed'] = 123 + i\n        best_params = solve_case(case, case['bands'])\n        final_results.append(best_params)\n    \n    # Format the final output string without spaces\n    formatted_results = [f\"[{m},{p}]\" for m, p in final_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3711432"}, {"introduction": "Building effective preprocessing steps and models is only part of the battle; assembling them into a valid, trustworthy, and reproducible scientific workflow is the ultimate goal. This final practice zooms out to address the architectural principles of modern chemometric analysis, focusing on how to prevent the pervasive issue of data leakage during model validation. You will evaluate different pipeline construction strategies to understand how to correctly encapsulate every data-dependent step, from baseline correction to the final classifier, ensuring your performance metrics are unbiased and your model is ready for robust deployment [@problem_id:3711419].", "problem": "A laboratory is building a reproducible chemometric pipeline for spectrometric identification of organic compounds from Near Infrared (NIR) spectra. The raw data are represented as a matrix $X_{\\mathrm{raw}} \\in \\mathbb{R}^{n \\times m}$ of $n$ samples and $m$ wavelengths, with class labels $y \\in \\{1, \\dots, C\\}$ indicating compound identity. Each sample’s spectrum is a vector $x \\in \\mathbb{R}^{m}$. The goal is to design a pipeline object that encapsulates all steps from raw spectra to predictions, including preprocessing, feature extraction, and modeling, and to justify why serialization of the fitted pipeline prevents data leakage and ensures reproducibility.\n\nThe chemometric pre-processing stages available include:\n- Baseline removal using Asymmetric Least Squares (ALS), which solves for a baseline $\\beta \\in \\mathbb{R}^{m}$ by minimizing a weighted penalized least squares objective with smoothing parameter $\\lambda$ and asymmetry parameter $p$, yielding a corrected spectrum $x' = x - \\beta$.\n- Standard Normal Variate (SNV), applied per-spectrum to produce $S(x) = \\dfrac{x - \\bar{x}\\mathbf{1}}{s_x}$, where $\\bar{x}$ is the mean of the entries of $x$, $s_x$ is their standard deviation, and $\\mathbf{1} \\in \\mathbb{R}^{m}$ is the vector of ones.\n- Savitzky–Golay (SG) derivative filtering, $D_{\\mathrm{SG}}(x)$, parameterized by a window length and polynomial order.\n\nFeature extraction choices include Principal Component Analysis (PCA) with $k$ components and Partial Least Squares (PLS) with $k$ latent variables. Classifiers include Linear Discriminant Analysis (LDA) and Support Vector Machine (SVM). The team will use Cross-Validation (CV) for model selection, potentially nested CV for hyperparameter tuning, and will consider replicate measurements that must not leak across folds.\n\nFrom a first-principles statistical learning perspective, let a pipeline be the composition $f = M_{\\theta_M} \\circ E_{\\theta_E} \\circ D_{\\theta_D} \\circ S_{\\theta_S} \\circ B_{\\theta_B}$, where $B_{\\theta_B}$, $S_{\\theta_S}$, $D_{\\theta_D}$, and $E_{\\theta_E}$ are preprocessing and feature extraction mappings with parameters $\\theta_B$, $\\theta_S$, $\\theta_D$, and $\\theta_E$ learned on training data only, and $M_{\\theta_M}$ is the classifier parameterized by $\\theta_M$. The expected generalization risk is $R(f) = \\mathbb{E}_{(x,y)\\sim \\mathcal{D}}[\\ell(f(x), y)]$ for a loss $\\ell$, and unbiased estimation by CV relies on maintaining independence between training and validation folds so that no parameters are influenced by validation or test data. Serialization refers to persisting the fitted pipeline $(\\hat{\\theta}_B, \\hat{\\theta}_S, \\hat{\\theta}_D, \\hat{\\theta}_E, \\hat{\\theta}_M)$ to storage and later loading it to apply $f$ without any refitting.\n\nConsider the following options that propose different pipeline designs and CV strategies. Select all options that correctly specify a reproducible, leakage-safe pipeline from raw spectra to predictions and correctly justify how serialization avoids data leakage in deployment.\n\nA. Fit $B_{\\theta_B}$, $S_{\\theta_S}$, and $E_{\\theta_E}$ (PCA with $k$ components) once using all of $X_{\\mathrm{raw}}$; then perform CV only on $M_{\\theta_M}$ (SVM) using the precomputed transformed data. After training, serialize only $M_{\\theta_M}$; on new spectra $x_{\\mathrm{new}}$ recompute $B_{\\theta_B}$, $S_{\\theta_S}$, and $E_{\\theta_E}$ on the full available data including $x_{\\mathrm{new}}$ before predicting.\n\nB. Construct a single pipeline object $f = M_{\\theta_M} \\circ E_{\\theta_E} \\circ D_{\\theta_D} \\circ S_{\\theta_S} \\circ B_{\\theta_B}$, where each step is an estimator with separate $\\mathrm{fit}$ and $\\mathrm{transform}$ semantics. Use nested CV: the inner loop tunes hyperparameters (including SG window length/order, PCA or PLS dimension $k$, and the classifier’s regularization/kernel), and the outer loop estimates $R(f)$. In every CV fold, fit all steps using only the training subset to produce $(\\hat{\\theta}_B, \\hat{\\theta}_S, \\hat{\\theta}_D, \\hat{\\theta}_E, \\hat{\\theta}_M)$, then evaluate on the validation subset using the pipeline’s $\\mathrm{predict}$ without refitting. Fix pseudo-random seeds in stochastic steps to ensure reproducibility. Serialize the entire fitted pipeline (all $\\hat{\\theta}$) so that at deployment $f(x_{\\mathrm{new}})$ uses the fixed transforms and model; no parameters are recomputed on $x_{\\mathrm{new}}$, thereby preventing leakage and preserving the exact training state.\n\nC. Place PCA inside a pipeline but compute global centering and scaling across samples as $x \\mapsto \\dfrac{x - \\mu}{\\sigma}$ using $\\mu$ and $\\sigma$ estimated from all of $X_{\\mathrm{raw}}$ before CV. Use Stratified CV for the classifier. Serialize the entire pipeline; argue that because serialization fixes $\\mu$ and $\\sigma$, it prevents leakage.\n\nD. Build a pipeline with the order $E_{\\theta_E}$ (PCA) $\\rightarrow S_{\\theta_S}$ (SNV) $\\rightarrow D_{\\theta_D}$ (SG derivative) $\\rightarrow B_{\\theta_B}$ (baseline), followed by $M_{\\theta_M}$ (SVM). Tune PCA components using all of $X_{\\mathrm{raw}}$ to stabilize the directions. During deployment, recompute $S_{\\theta_S}$ and $D_{\\theta_D}$ parameters using the incoming batch to adapt to instrument drift, but keep the serialized PCA and SVM fixed; claim that this adaptation reduces leakage because it uses unlabeled test data only.\n\nE. Use Grouped CV (GroupKFold) to ensure replicate spectra from the same physical sample are grouped so they never split across folds. Define $f = M_{\\theta_M} \\circ E_{\\theta_E} \\circ D_{\\theta_D} \\circ S_{\\theta_S} \\circ B_{\\theta_B}$ with $B_{\\theta_B}$ (ALS baseline), $S_{\\theta_S}$ (per-spectrum SNV), $D_{\\theta_D}$ (SG derivative), and $E_{\\theta_E}$ (PLS with $k$ latent variables). Perform nested CV where the inner loop tunes $\\lambda$, $p$ for ALS, SG window length and order, PLS dimension $k$, and classifier regularization. In every fold, fit all steps only on the training subset and evaluate on held-out groups. Serialize the entire fitted pipeline together with the CV configuration (random states and group assignments) so that at deployment $f(x_{\\mathrm{new}})$ uses fixed learned parameters $(\\hat{\\theta}_B, \\hat{\\theta}_D, \\hat{\\theta}_E, \\hat{\\theta}_M)$ and per-spectrum $S(x_{\\mathrm{new}})$, with no refitting on $x_{\\mathrm{new}}$, thereby preventing leakage and enabling exact reproducibility of the selection process.\n\nWhich option(s) are correct?", "solution": "The problem statement is critically evaluated for validity before proceeding to a solution.\n\n### Step 1: Extract Givens\n- **Data Matrix**: $X_{\\mathrm{raw}} \\in \\mathbb{R}^{n \\times m}$ ($n$ samples, $m$ wavelengths).\n- **Labels**: $y \\in \\{1, \\dots, C\\}$ (compound identity).\n- **Sample Vector**: $x \\in \\mathbb{R}^{m}$.\n- **Objective**: Design a reproducible chemometric pipeline for spectrometric identification. Justify how serialization prevents data leakage and ensures reproducibility.\n- **Preprocessing Operations**:\n    - Asymmetric Least Squares (ALS) baseline removal: $x' = x - \\beta$, parameters $\\lambda, p$.\n    - Standard Normal Variate (SNV): $S(x) = \\dfrac{x - \\bar{x}\\mathbf{1}}{s_x}$.\n    - Savitzky–Golay (SG) derivative filtering: $D_{\\mathrm{SG}}(x)$, parameters window length, polynomial order.\n- **Feature Extraction Operations**:\n    - Principal Component Analysis (PCA) with $k$ components.\n    - Partial Least Squares (PLS) with $k$ latent variables.\n- **Classifiers**: Linear Discriminant Analysis (LDA), Support Vector Machine (SVM).\n- **Validation**: Cross-Validation (CV), potentially nested. Mention of replicate measurements that must not leak across folds.\n- **Formal Pipeline Definition**: Composition of mappings $f = M_{\\theta_M} \\circ E_{\\theta_E} \\circ D_{\\theta_D} \\circ S_{\\theta_S} \\circ B_{\\theta_B}$. Parameters $\\theta$ are to be learned on training data.\n- **Risk Estimation**: $R(f) = \\mathbb{E}_{(x,y)\\sim \\mathcal{D}}[\\ell(f(x), y)]$. Unbiased estimation via CV requires independence between training and validation folds.\n- **Serialization**: Persisting the fitted pipeline state $(\\hat{\\theta}_B, \\hat{\\theta}_S, \\hat{\\theta}_D, \\hat{\\theta}_E, \\hat{\\theta}_M)$ for later application without refitting.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is grounded in the established principles of analytical chemistry (spectroscopy), chemometrics, and statistical learning theory. All methods listed (NIR, ALS, SNV, SG, PCA, PLS, SVM, CV) are standard and correctly described. The core concepts of data leakage, reproducibility, and model persistence (serialization) are central to modern applied machine learning.\n- **Well-Posed**: The problem is well-posed. It asks for the evaluation of several proposed methodologies against the fundamental principles of statistical model validation, which are provided within the prompt itself. The goal is clear, and a unique set of correct options can be determined by applying these principles.\n- **Objective**: The problem is stated objectively using precise mathematical and technical terminology. It is free of ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. It is scientifically sound, well-posed, and objective. I will proceed to derive the solution by analyzing each option.\n\n### Solution Derivation\n\nThe fundamental principle for validating a machine learning pipeline and estimating its generalization performance is the prevention of **data leakage**. Data leakage occurs when information from outside the training dataset is used to create the model. In the context of cross-validation (CV), this means that any and all data-driven parameter learning—including preprocessing, feature extraction, and model fitting—must be performed *exclusively* on the training portion of the data for each CV fold. The validation portion must be treated as unseen data, transformed using the parameters learned from the training fold, and then used for evaluation. A reproducible pipeline is one that, when executed with the same data, code, and random seeds, yields the identical result. Serialization of a *fully fitted* pipeline ensures that the exact model validated via CV is the one deployed for future predictions, preventing leakage from new data and ensuring consistency.\n\nWe now evaluate each option against these principles.\n\n**A. Fit $B_{\\theta_B}$, $S_{\\theta_S}$, and $E_{\\theta_E}$ (PCA with $k$ components) once using all of $X_{\\mathrm{raw}}$; then perform CV only on $M_{\\theta_M}$ (SVM) using the precomputed transformed data. After training, serialize only $M_{\\theta_M}$; on new spectra $x_{\\mathrm{new}}$ recompute $B_{\\theta_B}$, $S_{\\theta_S}$, and $E_{\\theta_E}$ on the full available data including $x_{\\mathrm{new}}$ before predicting.**\n\nThis proposal contains multiple critical flaws.\n1.  **Data Leakage during Training/Validation**: Fitting the PCA transformation $E_{\\theta_E}$ on the entire dataset $X_{\\mathrm{raw}}$ before performing cross-validation is a canonical example of data leakage. The principal components (the basis vectors for the new feature space) are determined by the variance structure of the *entire* dataset, including the samples that will be used for validation in each CV fold. This makes the validation sets \"seen\" by the model construction process, leading to an optimistically biased estimate of the generalization risk $R(f)$. The same logic applies to any other transformation (like $B_{\\theta_B}$) whose parameters are learned from the data distribution.\n2.  **Incorrect Serialization**: Serializing only the classifier $M_{\\theta_M}$ is insufficient. The complete function $f$ includes the preprocessing and feature extraction steps. To make a valid prediction, the new data must be transformed in the exact same way as the training data, using the parameters $(\\hat{\\theta}_B, \\hat{\\theta}_S, \\hat{\\theta}_E)$ learned during training.\n3.  **Data Leakage during Deployment**: Recomputing transformations on a new set of data including $x_{\\mathrm{new}}$ means that the function applied to $x_{\\mathrm{new}}$ is different from the function that was validated. The prediction becomes dependent on other data points being predicted simultaneously, which violates the principle of applying a fixed, validated model.\n\n**Verdict on A**: **Incorrect**.\n\n**B. Construct a single pipeline object $f = M_{\\theta_M} \\circ E_{\\theta_E} \\circ D_{\\theta_D} \\circ S_{\\theta_S} \\circ B_{\\theta_B}$, where each step is an estimator with separate $\\mathrm{fit}$ and $\\mathrm{transform}$ semantics. Use nested CV: the inner loop tunes hyperparameters (including SG window length/order, PCA or PLS dimension $k$, and the classifier’s regularization/kernel), and the outer loop estimates $R(f)$. In every CV fold, fit all steps using only the training subset to produce $(\\hat{\\theta}_B, \\hat{\\theta}_S, \\hat{\\theta}_D, \\hat{\\theta}_E, \\hat{\\theta}_M)$, then evaluate on the validation subset using the pipeline’s $\\mathrm{predict}$ without refitting. Fix pseudo-random seeds in stochastic steps to ensure reproducibility. Serialize the entire fitted pipeline (all $\\hat{\\theta}$) so that at deployment $f(x_{\\mathrm{new}})$ uses the fixed transforms and model; no parameters are recomputed on $x_{\\mathrm{new}}$, thereby preventing leakage and preserving the exact training state.**\n\nThis option correctly describes the \"gold standard\" for building and validating a machine learning model.\n1.  **Pipeline Integrity**: Encapsulating all steps within a single pipeline object that respects the `fit`/`transform` contract ensures that all data-dependent steps are correctly handled within the CV loop.\n2.  **Correct CV Procedure**: The use of nested CV for hyperparameter tuning and performance estimation is rigorous. Crucially, it states that \"In every CV fold, fit all steps using only the training subset\". This correctly isolates the validation set and prevents data leakage, leading to an unbiased estimate of $R(f)$.\n3.  **Reproducibility**: Fixing random seeds is the correct action to ensure computational reproducibility.\n4.  **Correct Deployment Strategy**: Serializing the *entire* fitted pipeline—including all preprocessing and feature extraction parameters learned from the final training run on all data—is the correct approach. Applying this fixed pipeline to new data $x_{\\mathrm{new}}$ without any refitting ensures that the deployed model is exactly the one that was validated and prevents any leakage from $x_{\\mathrm{new}}$ into the model parameters. The statement \"no parameters are recomputed on $x_{\\mathrm{new}}$\" correctly refers to parameters learned from the training population (e.g., PCA basis). Per-sample operations like SNV are correctly handled by the pipeline's transform logic.\n\n**Verdict on B**: **Correct**.\n\n**C. Place PCA inside a pipeline but compute global centering and scaling across samples as $x \\mapsto \\dfrac{x - \\mu}{\\sigma}$ using $\\mu$ and $\\sigma$ estimated from all of $X_{\\mathrm{raw}}$ before CV. Use Stratified CV for the classifier. Serialize the entire pipeline; argue that because serialization fixes $\\mu$ and $\\sigma$, it prevents leakage.**\n\nThis option is flawed.\n1.  **Data Leakage during Preprocessing**: The initial step, \"compute global centering and scaling... using $\\mu$ and $\\sigma$ estimated from all of $X_{\\mathrm{raw}}$ before CV\", introduces data leakage. The mean $\\mu$ and standard deviation $\\sigma$ are parameters learned from the data. By computing them on the entire dataset, information from the validation folds is leaked into the training process of every fold.\n2.  **Flawed Justification**: The argument that \"serialization fixes $\\mu$ and $\\sigma$, it prevents leakage\" is a non-sequitur. Serialization prevents leakage at *deployment* time, but it does not retroactively fix the leakage that occurred during the *validation* phase. The performance estimate obtained from this flawed CV procedure is invalid, and thus we have no reliable estimate of the serialized model's true performance.\n\n**Verdict on C**: **Incorrect**.\n\n**D. Build a pipeline with the order $E_{\\theta_E}$ (PCA) $\\rightarrow S_{\\theta_S}$ (SNV) $\\rightarrow D_{\\theta_D}$ (SG derivative) $\\rightarrow B_{\\theta_B}$ (baseline), followed by $M_{\\theta_M}$ (SVM). Tune PCA components using all of $X_{\\mathrm{raw}}$ to stabilize the directions. During deployment, recompute $S_{\\theta_S}$ and $D_{\\theta_D}$ parameters using the incoming batch to adapt to instrument drift, but keep the serialized PCA and SVM fixed; claim that this adaptation reduces leakage because it uses unlabeled test data only.**\n\nThis option is incorrect for several reasons.\n1.  **Questionable Pipeline Order**: The proposed order of operations ($PCA \\rightarrow SNV \\rightarrow ...$) is highly suspect from a chemometric standpoint. PCA is sensitive to baseline shifts and scaling artifacts, which are precisely what baseline correction and SNV are designed to mitigate. The standard, effective order is typically the reverse: correct artifacts first, then extract features.\n2.  **Data Leakage during Training**: \"Tune PCA components using all of $X_{\\mathrm{raw}}$\" is again a clear violation of the no-leakage principle, identical to the flaw in options A and C.\n3.  **Flawed Deployment Strategy and Justification**: Recomputing parameters on an \"incoming batch\" at deployment is invalid. It makes the model's output for a given sample dependent on other samples in the batch. The claim that this \"reduces leakage because it uses unlabeled test data only\" is nonsensical. Using test data (labeled or unlabeled) to fit *any* part of the transformation or model is a form of information leakage (specifically, transductive inference), and it invalidates the performance guarantees from the original CV procedure.\n\n**Verdict on D**: **Incorrect**.\n\n**E. Use Grouped CV (GroupKFold) to ensure replicate spectra from the same physical sample are grouped so they never split across folds. Define $f = M_{\\theta_M} \\circ E_{\\theta_E} \\circ D_{\\theta_D} \\circ S_{\\theta_S} \\circ B_{\\theta_B}$ with $B_{\\theta_B}$ (ALS baseline), $S_{\\theta_S}$ (per-spectrum SNV), $D_{\\theta_D}$ (SG derivative), and $E_{\\theta_E}$ (PLS with $k$ latent variables). Perform nested CV where the inner loop tunes $\\lambda$, $p$ for ALS, SG window length and order, PLS dimension $k$, and classifier regularization. In every fold, fit all steps only on the training subset and evaluate on held-out groups. Serialize the entire fitted pipeline together with the CV configuration (random states and group assignments) so that at deployment $f(x_{\\mathrm{new}})$ uses fixed learned parameters $(\\hat{\\theta}_B, \\hat{\\theta}_D, \\hat{\\theta}_E, \\hat{\\theta}_M)$ and per-spectrum $S(x_{\\mathrm{new}})$, with no refitting on $x_{\\mathrm{new}}$, thereby preventing leakage and enabling exact reproducibility of the selection process.**\n\nThis option describes a highly rigorous and correct methodology, building upon the principles in option B and adding further specificity.\n1.  **Correct Handling of Dependent Data**: The problem statement explicitly mentions \"replicate measurements\". Using Grouped CV (e.g., `GroupKFold`) is the correct statistical procedure to handle this non-i.i.d. data structure, preventing artificially inflated performance estimates that would arise from having highly similar replicate spectra in both training and validation sets.\n2.  **Pipeline Integrity and Correct CV**: Like option B, it correctly proposes a full pipeline, nested CV, and fitting all parameters (including those for ALS, SG, and PLS) strictly within the training fold.\n3.  **Correct and Precise Deployment Strategy**: It correctly identifies that the entire fitted pipeline must be serialized. It astutely distinguishes between population-learned parameters $(\\hat{\\theta}_B, \\hat{\\theta}_D, \\hat{\\theta}_E, \\hat{\\theta}_M)$, which are fixed, and the application of a per-spectrum transform like SNV, $S(x_{\\mathrm{new}})$, which is correctly applied to the new sample using only that sample's data. This level of detail is superior.\n4.  **Full Reproducibility**: Mentioning the serialization of the CV configuration (seeds, group assignments) addresses the goal of making the entire *model selection process* reproducible, which is an advanced and commendable practice.\n\n**Verdict on E**: **Correct**.\n\nBoth options B and E describe valid, leakage-free, and reproducible pipelines. Option E provides a more complete solution by explicitly and correctly addressing the issue of replicate samples mentioned in the problem description.", "answer": "$$\\boxed{BE}$$", "id": "3711419"}]}