{"hands_on_practices": [{"introduction": "这项练习是计算物理中的一个经典入门问题：使用迭代法求解拉普拉斯方程。你将亲手实现 Jacobi、Gauss-Seidel 和 SOR 方法，并直接比较它们在解决一个具体的边值问题时的收敛速度。通过这个实践，你将直观地感受到不同迭代策略在效率上的巨大差异，为选择合适的求解器打下坚实的实践基础。[@problem_id:2406769]", "problem": "考虑方形域 $\\Omega = (0,1)\\times(0,1)$ 上的二维拉普拉斯方程 $\\nabla^2 u = 0$，其带有狄利克雷边界条件。设边界数据为 $u(x,0) = \\sin(\\pi x)$，$u(x,1) = 0$，$u(0,y)=0$ 以及 $u(1,y)=0$，其中角度以弧度为单位。使用均匀网格对该域进行离散化，每个空间方向有 $N$ 个内部点（因此网格间距为 $h = \\frac{1}{N+1}$，包括边界在内的总网格点数为 $(N+2)\\times(N+2)$）。使用拉普拉斯算子的标准二阶中心有限差分近似，以获得关于内部未知数的线性系统。从连续方程和中心差分推导出的离散拉普拉斯算子定义出发，实现三种定常迭代法来求解离散方程：雅可比法、高斯-赛德尔法以及松弛参数 $\\omega$ 满足 $0 < \\omega < 2$ 的逐次超松弛 (SOR) 法。对于每种方法，使用以下基本依据和定义：\n\n- 内部网格点 $(i,j)$ 处的离散拉普拉斯方程由 $\\nabla^2 u = 0$ 的中心差分近似得到：\n$$\\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2} + \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^2} = 0,$$\n该方程在代数上等价于差分格式方程\n$$-u_{i+1,j} - u_{i-1,j} - u_{i,j+1} - u_{i,j-1} + 4 u_{i,j} = 0.$$\n- 由此，定义内部的离散残差为\n$$r_{i,j} = 4u_{i,j} - \\left(u_{i+1,j}+u_{i-1,j}+u_{i,j+1}+u_{i,j-1}\\right),$$\n及其无穷范数为 $\\|r\\|_{\\infty} = \\max_{i,j} |r_{i,j}|$.\n- 使用停止准则 $\\|r^{(k)}\\|_{\\infty} \\le \\text{tol}\\cdot \\|r^{(0)}\\|_{\\infty}$，其中 $\\text{tol}$ 是给定的容差，$k$ 是迭代指数。初始猜测值 $u^{(0)}$ 在所有内部点上为零，并在边界点上取固定的边界值。\n\n你的程序必须：\n- 实现雅可比迭代，该迭代仅使用上一次迭代的值来更新所有内部值。\n- 实现高斯-赛德尔迭代，该迭代方式使用最新的可用邻居值（你可以使用红黑着色排序来实现）。\n- 使用红黑着色排序和松弛参数 $\\omega$ 实现 SOR 迭代，通过以下公式\n$$u^{(k+1)}_{i,j} = u^{(k)}_{i,j} + \\omega\\left(\\frac{1}{4}\\left(u^{(*)}_{i+1,j}+u^{(*)}_{i-1,j}+u^{(*)}_{i,j+1}+u^{(*)}_{i,j-1}\\right) - u^{(k)}_{i,j}\\right),$$\n其中 $u^{(*)}$ 表示在每种颜色上与高斯-赛德尔排序一致的最新值。取 $\\omega = 1$ 可恢复为高斯-赛德尔法。\n\n你的任务是通过报告每种方法在各测试用例中满足停止准则所需的迭代次数，来定量地比较这三种方法的收敛速度。对所有方法使用相同的离散化、边界条件和停止准则，并将迭代次数报告为整数。\n\n测试套件。在以下参数集上运行你的程序，其中每个测试用例是一个三元组 $(N, \\text{tol}, \\omega)$:\n- 测试 1：$(20, 10^{-5}, 1.5)$。\n- 测试 2：$(20, 10^{-5}, 1.0)$。\n- 测试 3：$(10, 10^{-8}, 1.8)$。\n- 测试 4：$(40, 10^{-4}, 1.9)$。\n\n对于每个测试用例，你的程序必须生成一个列表 $[n_J, n_{GS}, n_{SOR}]$，其中分别包含雅可比法、高斯-赛德尔法和 SOR 法满足停止准则所需的迭代次数。将所有测试的结果汇总为单行，形式为这些列表的逗号分隔列表，不含空格，并用方括号括起来。例如，你的输出格式必须与以下示例完全一致\n$[[n_J^{(1)},n_{GS}^{(1)},n_{SOR}^{(1)}],[n_J^{(2)},n_{GS}^{(2)},n_{SOR}^{(2)}],[n_J^{(3)},n_{GS}^{(3)},n_{SOR}^{(3)}],[n_J^{(4)},n_{GS}^{(4)},n_{SOR}^{(4)}]]$,\n作为单行打印。不涉及物理单位。正弦函数中的角度必须以弧度解释。", "solution": "所述问题是椭圆型偏微分方程数值解领域一个标准的、适定的练习。该问题在科学上是合理的，内容自洽，且算法上已明确指定。不存在矛盾、歧义或事实错误。因此，我们直接进行求解。\n\n问题要求在单位方形域 $\\Omega = (0,1)\\times(0,1)$ 上求解二维拉普拉斯方程 $\\nabla^2 u = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = 0$。位势 $u(x,y)$ 满足狄利克雷边界条件：$u(x,0) = \\sin(\\pi x)$，并且在其他三条边界上 $u=0$。\n\n第一步是将连续问题离散化。该域被一个含 $(N+2) \\times (N+2)$ 个点的均匀网格所覆盖，其中 $N$ 是每个方向上内部点的数量。网格间距为 $h = \\frac{1}{N+1}$。对于下标 $i,j \\in \\{0, 1, \\dots, N+1\\}$，一个网格点表示为 $(x_i, y_j) = (ih, jh)$。该点的位势值为 $u_{i,j} \\approx u(x_i, y_j)$。\n\n拉普拉斯算子 $\\nabla^2$ 在每个内部网格点 $(i,j)$ 处使用二阶中心差分公式进行近似：\n$$ \\nabla^2 u \\bigg|_{(x_i, y_j)} \\approx \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2} + \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^2} $$\n将此近似值设为 $0$，得到内部点（$1 \\le i,j \\le N$）的离散拉普拉斯方程：\n$$ u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} - 4u_{i,j} = 0 $$\n这个五点差分格式方程可以重排，将 $u_{i,j}$ 表示为其四个邻点的平均值：\n$$ u_{i,j} = \\frac{1}{4} \\left( u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} \\right) $$\n这组关于 $N^2$ 个内部未知数值的 $N^2$ 个线性方程构成一个形如 $A\\mathbf{u} = \\mathbf{b}$ 的大型稀疏线性系统，其中 $\\mathbf{u}$ 是未知数 $u_{i,j}$ 的向量，矩阵 $A$ 代表离散拉普拉斯算子。右端向量 $\\mathbf{b}$ 包含了固定的边界值。此类系统非常适合用迭代法求解。\n\n我们的任务是实现三种经典的定常迭代法：Jacobi法、Gauss-Seidel法和逐次超松弛 (SOR) 法。这些方法从一个初始猜测值 $u^{(0)}$ 开始，生成一个收敛到真实解的近似序列 $u^{(k)}$。\n\nJacobi法是最简单的迭代格式。对于每个点 $(i,j)$，新值 $u_{i,j}^{(k+1)}$ 仅使用上一次迭代 $u^{(k)}$ 的值来计算。其更新规则是平均公式的直接应用：\n$$ u_{i,j}^{(k+1)} = \\frac{1}{4} \\left( u_{i+1,j}^{(k)} + u_{i-1,j}^{(k)} + u_{i,j+1}^{(k)} + u_{i,j-1}^{(k)} \\right) $$\n这个更新可以对所有内部点同时（或以任何顺序）执行，因为在同一次迭代中，每个点的计算都与其他点相互独立。在向量化实现中，需要一个第 $k$ 次迭代的完整网格副本来计算第 $k+1$ 次迭代的网格。\n\nGauss-Seidel法通过在当前迭代中使用最新计算出的值，对Jacobi法进行了改进。其更新规则是：\n$$ u_{i,j}^{(k+1)} = \\frac{1}{4} \\left( u_{i+1,j}^{(*)} + u_{i-1,j}^{(*)} + u_{i,j+1}^{(*)} + u_{i,j-1}^{(*)} \\right) $$\n其中 $u^{(*)}$ 表示可用的最新值。例如，在字典序（逐行逐列）中，计算 $u_{i,j}^{(k+1)}$ 会使用当前迭代 $k+1$ 中的 $u_{i-1,j}^{(k+1)}$ 和 $u_{i,j-1}^{(k+1)}$，以及上一次迭代 $k$ 中的 $u_{i+1,j}^{(k)}$ 和 $u_{i,j+1}^{(k)}$。这种对更新顺序的依赖性使并行化变得复杂。红黑着色排序方案规避了这个问题。网格点被像棋盘一样着色。首先更新所有“红”点，使用其“黑”邻点的值（来自上一次迭代）。然后，更新所有“黑”点，使用其“红”邻点新计算出的值。这两个阶段（红点更新，黑点更新）中的每一个都可以被完全向量化。\n\n逐次超松弛 (SOR) 法是Gauss-Seidel法的一种外推，旨在加速收敛。它计算Gauss-Seidel更新量，然后通过一个松弛参数 $\\omega$ 控制，将解朝该方向进一步推进。其更新公式为：\n$$ u_{i,j}^{(k+1)} = u_{i,j}^{(k)} + \\omega \\left( u_{i,j}^{\\text{GS}} - u_{i,j}^{(k)} \\right) = (1-\\omega)u_{i,j}^{(k)} + \\omega u_{i,j}^{\\text{GS}} $$\n其中 $u_{i,j}^{\\text{GS}}$ 是Gauss-Seidel法在该点计算出的值。与Gauss-Seidel法一样，SOR法也使用红黑着色排序来高效地利用最新值。当 $\\omega=1$ 时，SOR法精确地简化为Gauss-Seidel法。对于拉普拉斯型问题，选择范围在 $1 < \\omega < 2$ 内的最优 $\\omega$（超松弛）通常会使收敛速度显著加快。\n\n停止准则基于离散残差的无穷范数，其定义为 $\\|r^{(k)}\\|_{\\infty} = \\max_{i,j} |r_{i,j}^{(k)}|$，其中 $r_{i,j}^{(k)} = 4u_{i,j}^{(k)} - (u_{i+1,j}^{(k)} + u_{i-1,j}^{(k)} + u_{i,j+1}^{(k)} + u_{i,j-1}^{(k)})$。当 $\\|r^{(k)}\\|_{\\infty} \\le \\text{tol} \\cdot \\|r^{(0)}\\|_{\\infty}$ 时，迭代停止。这里 $\\text{tol}$ 是给定的容差，$\\|r^{(0)}\\|_{\\infty}$ 是初始猜测值（内部 $u^{(0)}=0$）的残差范数。这个相对准则确保了在不同问题设置之间进行公平比较。\n\n实现包含三个主要函数。一个函数用于设置 $(N+2) \\times (N+2)$ 网格，将内部初始化为 0 并设置边界条件。第二个函数实现Jacobi迭代。第三个函数实现带红黑着色排序的 SOR 迭代，通过设置 $\\omega=1$ 也可用于Gauss-Seidel法。一个辅助函数在每一步计算残差范数。主程序遍历测试用例，调用相应的求解器函数，并记录收敛所需的迭代次数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef setup_initial_state(N):\n    \"\"\"\n    Initializes the grid with boundary conditions and zero interior.\n\n    Args:\n        N (int): Number of interior points in each direction.\n\n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: The (N+2)x(N+2) grid `u`.\n            - float: The grid spacing `h`.\n    \"\"\"\n    h = 1.0 / (N + 1)\n    u = np.zeros((N + 2, N + 2))\n    \n    # Set boundary condition u(x,0) = sin(pi*x)\n    # The j=0 row corresponds to y=0.\n    x_coords = np.linspace(0, 1, N + 2)\n    u[0, :] = np.sin(np.pi * x_coords)\n    \n    # Other boundaries u(x,1)=0, u(0,y)=0, u(1,y)=0 are already zero.\n    return u, h\n\ndef calculate_residual_norm(u, N):\n    \"\"\"\n    Calculates the infinity norm of the residual on the interior grid.\n\n    Args:\n        u (np.ndarray): The full (N+2)x(N+2) grid.\n        N (int): Number of interior grid points.\n\n    Returns:\n        float: The infinity norm of the residual.\n    \"\"\"\n    interior = u[1:N + 1, 1:N + 1]\n    neighbors_sum = (u[1:N + 1, 2:N + 2] + u[1:N + 1, 0:N] +\n                     u[2:N + 2, 1:N + 1] + u[0:N, 1:N + 1])\n    residual = 4 * interior - neighbors_sum\n    return np.max(np.abs(residual))\n\ndef solve_jacobi(N, tol):\n    \"\"\"\n    Solves the Laplace equation using the Jacobi method.\n\n    Args:\n        N (int): Number of interior grid points.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    u, h = setup_initial_state(N)\n    \n    r0_norm = calculate_residual_norm(u, N)\n    if r0_norm == 0:\n        return 0\n    \n    threshold = tol * r0_norm\n    \n    k = 0\n    while True:\n        k += 1\n        \n        u_old = u.copy()\n        \n        neighbors_sum = (u_old[1:N + 1, 2:N + 2] + u_old[1:N + 1, 0:N] +\n                         u_old[2:N + 2, 1:N + 1] + u_old[0:N, 1:N + 1])\n        u[1:N + 1, 1:N + 1] = 0.25 * neighbors_sum\n        \n        r_norm = calculate_residual_norm(u, N)\n        if r_norm = threshold:\n            return k\n\ndef solve_sor(N, tol, omega):\n    \"\"\"\n    Solves the Laplace equation using SOR with red-black ordering.\n    Recovers Gauss-Seidel for omega=1.0.\n\n    Args:\n        N (int): Number of interior grid points.\n        tol (float): Convergence tolerance.\n        omega (float): Relaxation parameter.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    u, h = setup_initial_state(N)\n    \n    r0_norm = calculate_residual_norm(u, N)\n    if r0_norm == 0:\n        return 0\n        \n    threshold = tol * r0_norm\n    \n    # Create red-black masks for the interior (N x N) grid.\n    # (j, i) indices for the interior part start from 0.\n    # Grid point (j_grid, i_grid) where j_grid, i_grid in [1,N]\n    # corresponds to mask point (j_grid-1, i_grid-1).\n    # Color depends on (j_grid + i_grid). (j_grid-1) + (i_grid-1) has same parity.\n    I, J = np.meshgrid(np.arange(N), np.arange(N))\n    red_mask = (I + J) % 2 == 0\n    black_mask = ~red_mask\n    \n    k = 0\n    while True:\n        k += 1\n        \n        # Keep a copy of the interior from the start of the iteration\n        # for the (1-omega) term.\n        u_old_interior = u[1:N + 1, 1:N + 1].copy()\n\n        # Update red points. Neighbors are black, use values from start of iteration.\n        neighbors_sum = (u[1:N + 1, 2:N + 2] + u[1:N + 1, 0:N] +\n                         u[2:N + 2, 1:N + 1] + u[0:N, 1:N + 1])\n        gs_update = 0.25 * neighbors_sum\n        u[1:N + 1, 1:N + 1][red_mask] = (1 - omega) * u_old_interior[red_mask] + \\\n                                      omega * gs_update[red_mask]\n\n        # Update black points. Neighbors are red, use newly updated values.\n        neighbors_sum = (u[1:N + 1, 2:N + 2] + u[1:N + 1, 0:N] +\n                         u[2:N + 2, 1:N + 1] + u[0:N, 1:N + 1])\n        gs_update = 0.25 * neighbors_sum\n        u[1:N + 1, 1:N + 1][black_mask] = (1 - omega) * u_old_interior[black_mask] + \\\n                                        omega * gs_update[black_mask]\n        \n        r_norm = calculate_residual_norm(u, N)\n        if r_norm = threshold:\n            return k\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (20, 1e-5, 1.5),\n        (20, 1e-5, 1.0),\n        (10, 1e-8, 1.8),\n        (40, 1e-4, 1.9),\n    ]\n\n    results = []\n    for N, tol, omega_sor in test_cases:\n        # Calculate iterations for Jacobi\n        n_J = solve_jacobi(N, tol)\n        \n        # Calculate iterations for Gauss-Seidel (SOR with omega=1.0)\n        n_GS = solve_sor(N, tol, 1.0)\n        \n        # Calculate iterations for SOR with the specified omega\n        n_SOR = solve_sor(N, tol, omega_sor)\n        \n        results.append([n_J, n_GS, n_SOR])\n\n    # Format the output string as specified: [[r1,r2,r3],[...],...]\n    formatted_results = [f'[{\",\".join(map(str, r))}]' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2406769"}, {"introduction": "在了解了不同方法的收敛速度差异后，本练习将深入探究其背后的理论根源。你将通过分析两个具有不同对角占优程度的假设矩阵，来研究矩阵性质如何影响迭代收敛性。这个实践旨在揭示对角占优性、迭代矩阵的谱半径以及实际收敛速度之间的定量关系，从而加深对迭代法收敛理论的理解。[@problem_id:2406932]", "problem": "您必须编写一个完整、可运行的程序，用于评估指定线性系统的线性定常迭代法。考虑大小为 $n \\times n$ 的实对称方阵，其右端向量为 $\\mathbf{b} \\in \\mathbb{R}^n$。对于每个系统，从零向量 $\\mathbf{x}^{(0)} = \\mathbf{0}$ 开始，迭代至残差的无穷范数 $\\|\\mathbf{r}^{(k)}\\|_{\\infty} = \\|\\mathbf{b} - A \\mathbf{x}^{(k)}\\|_{\\infty}$ 小于或等于容差 $\\tau$，或直至达到最大迭代次数 $k_{\\max}$。使用 Jacobi 方法和 Gauss–Seidel 方法的标准定义。对于每个指定的矩阵，报告 Jacobi 方法和 Gauss–Seidel 方法满足残差容差所需的最小迭代次数，以及 Jacobi 迭代矩阵的谱半径。所有计算均为纯数值计算，不涉及任何物理单位。\n\n使用以下构成测试套件的参数：\n\n- 维度 $n = 6$。\n- 右端项 $\\mathbf{b} = [1,2,3,4,5,6]^T$。\n- 初始猜测值 $\\mathbf{x}^{(0)} = \\mathbf{0}$。\n- 残差容差 $\\tau = 10^{-8}$。\n- 最大迭代次数 $k_{\\max} = 20000$。\n\n定义三个矩阵 $A \\in \\mathbb{R}^{6 \\times 6}$ 如下：\n\n1) 强对角占优的稠密矩阵，具有恒定的非对角元素：\n- 参数 $\\alpha_{\\mathrm{s}} = 20.0$, $\\gamma = -1.0$。\n- 元素：\n  - $A_{ii} = \\alpha_{\\mathrm{s}}$ 对于所有 $i \\in \\{1,\\dots,6\\}$。\n  - $A_{ij} = \\gamma$ 对于所有 $i \\neq j$。\n\n2) 临界对角占优的稠密矩阵，具有恒定的非对角元素：\n- 参数 $\\alpha_{\\mathrm{w}} = 5.1$, $\\gamma = -1.0$。\n- 元素：\n  - $A_{ii} = \\alpha_{\\mathrm{w}}$ 对于所有 $i \\in \\{1,\\dots,6\\}$。\n  - $A_{ij} = \\gamma$ 对于所有 $i \\neq j$。\n\n3) 边缘情况，对称三对角矩阵 (一维离散拉普拉斯形式)：\n- 元素：\n  - $A_{ii} = 2.0$ 对于所有 $i \\in \\{1,\\dots,6\\}$。\n  - $A_{i,i+1} = A_{i+1,i} = -1.0$ 对于所有 $i \\in \\{1,\\dots,5\\}$。\n  - 所有其他非对角元素均为 $0.0$。\n\n对于这三个矩阵中的每一个，执行以下操作：\n- 使用 Jacobi 方法，确定最小迭代次数 $k_{\\mathrm{J}}$，使得 $\\|\\mathbf{b} - A \\mathbf{x}^{(k_{\\mathrm{J}})}\\|_{\\infty} \\le \\tau$；如果在 $k_{\\max}$ 次迭代内未达到，则返回 $k_{\\max}$。\n- 使用 Gauss–Seidel 方法，确定最小迭代次数 $k_{\\mathrm{GS}}$，使得 $\\|\\mathbf{b} - A \\mathbf{x}^{(k_{\\mathrm{GS}})}\\|_{\\infty} \\le \\tau$；如果在 $k_{\\max}$ 次迭代内未达到，则返回 $k_{\\max}$。\n- 对于 Jacobi 方法，计算其迭代矩阵的谱半径 $\\rho_{\\mathrm{J}}$。报告保留六位小数的 $\\rho_{\\mathrm{J}}$。\n\n您的程序应生成单行输出，按以下顺序包含九个结果：\n- 对于强对角占优矩阵：$k_{\\mathrm{J}}$、$k_{\\mathrm{GS}}$、$\\rho_{\\mathrm{J}}$ (保留六位小数)。\n- 对于临界对角占优矩阵：$k_{\\mathrm{J}}$、$k_{\\mathrm{GS}}$、$\\rho_{\\mathrm{J}}$ (保留六位小数)。\n- 对于三对角边缘情况矩阵：$k_{\\mathrm{J}}$、$k_{\\mathrm{GS}}$、$\\rho_{\\mathrm{J}}$ (保留六位小数)。\n\n最终输出格式必须是单行、用方括号括起来的逗号分隔列表，例如 $[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_9]$，其中 $k_{\\mathrm{J}}$ 和 $k_{\\mathrm{GS}}$ 是整数，每个 $\\rho_{\\mathrm{J}}$ 是保留六位小数的浮点数。", "solution": "该问题已经过验证，并被确定为有效。它在数值线性代数领域具有科学依据，所有必要参数均已定义，问题提法适定且表述客观。该问题要求实现和评估两种基本的线性定常迭代方法——Jacobi 法和 Gauss–Seidel 法，用以求解线性方程组 $A\\mathbf{x} = \\mathbf{b}$。\n\n这些方法基于将矩阵 $A$ 分解为其构成部分。一个方阵 $A$ 可以分解为 $A = D + L + U$，其中 $D$ 是包含 $A$ 的对角元素的对角矩阵，$L$ 是严格下三角矩阵，$U$ 是严格上三角矩阵。因此，系统 $A\\mathbf{x} = \\mathbf{b}$可以写作 $(D+L+U)\\mathbf{x} = \\mathbf{b}$。\n\n**Jacobi 方法**\n\nJacobi 方法将系统重排为 $D\\mathbf{x} = \\mathbf{b} - (L+U)\\mathbf{x}$。这导出了以下迭代格式：\n$$ D\\mathbf{x}^{(k+1)} = \\mathbf{b} - (L+U)\\mathbf{x}^{(k)} $$\n假设 $D$ 是可逆的（即没有零对角元素，本问题中所有矩阵都满足此条件），我们得到迭代公式：\n$$ \\mathbf{x}^{(k+1)} = D^{-1}(\\mathbf{b} - (L+U)\\mathbf{x}^{(k)}) $$\n这可以对向量 $\\mathbf{x}^{(k+1)}$ 的每个分量 $i$ 按分量计算：\n$$ x_i^{(k+1)} = \\frac{1}{A_{ii}} \\left( b_i - \\sum_{j=1, j \\neq i}^{n} A_{ij} x_j^{(k)} \\right) $$\nJacobi 方法的一个关键特性是，每个分量 $x_i^{(k+1)}$ 的计算仅依赖于前一次迭代的向量 $\\mathbf{x}^{(k)}$ 的分量。这使得新向量分量的计算可以并行进行。\n\n**Gauss–Seidel 方法**\n\nGauss–Seidel 方法旨在通过使用可用的最新信息来提高收敛速度。它将系统重排为 $(D+L)\\mathbf{x} = \\mathbf{b} - U\\mathbf{x}$，得到迭代格式：\n$$ (D+L)\\mathbf{x}^{(k+1)} = \\mathbf{b} - U\\mathbf{x}^{(k)} $$\n由此得出迭代公式：\n$$ \\mathbf{x}^{(k+1)} = (D+L)^{-1}(\\mathbf{b} - U\\mathbf{x}^{(k)}) $$\n在实践中，这通过前向替换来实现。其分量形式的公式为：\n$$ x_i^{(k+1)} = \\frac{1}{A_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} A_{ij} x_j^{(k+1)} - \\sum_{j=i+1}^{n} A_{ij} x_j^{(k)} \\right) $$\n请注意，在计算 $x_i^{(k+1)}$ 时，我们使用了当前迭代 $k+1$ 中已经计算出的分量 $x_j^{(k+1)}$ (对于 $j  i$)，以及上一次迭代 $k$ 的旧分量 $x_j^{(k)}$ (对于 $j  i$)。这种顺序依赖性意味着各分量必须按顺序更新。\n\n**收敛性与谱半径**\n\n任何线性定常迭代都可以写成 $\\mathbf{x}^{(k+1)} = T \\mathbf{x}^{(k)} + \\mathbf{c}$ 的形式，其中 $T$ 是迭代矩阵。对于任何初始猜测值 $\\mathbf{x}^{(0)}$，该方法收敛的充分必要条件是迭代矩阵的谱半径 $\\rho(T)$ 严格小于1。谱半径定义为 $T$ 的特征值的最大绝对值，即 $\\rho(T) = \\max_i |\\lambda_i(T)|$。\n\n对于 Jacobi 方法，其迭代矩阵 $T_J$ 由下式给出：\n$$ T_J = -D^{-1}(L+U) = I - D^{-1}A $$\n谱半径 $\\rho(T_J)$ 决定了 Jacobi 方法的收敛性。谱半径越小，意味着渐近收敛速度越快。本问题要求计算该值。\n\n**实现策略**\n\n解决方案将使用 Python 中的 `numpy` 库来实现。\n1.  **矩阵构建**：将三个指定的矩阵——$A_1$ (强对角占优)、$A_2$ (临界对角占优) 和 $A_3$ (三对角)——构建为 `numpy` 数组。定义问题参数 $n=6$、$\\mathbf{b}=[1,2,3,4,5,6]^T$、$\\mathbf{x}^{(0)}=\\mathbf{0}$、$\\tau=10^{-8}$ 和 $k_{\\max}=20000$。\n2.  **迭代求解器**：实现 Jacobi 方法和 Gauss–Seidel 方法的函数。每个函数将接收一个矩阵 $A$、向量 $\\mathbf{b}$、初始猜测值 $\\mathbf{x}^{(0)}$、容差 $\\tau$ 和最大迭代次数 $k_{\\max}$ 作为输入。循环将从 $k=1$ 到 $k_{\\max}$ 运行，在每一步更新解向量 $\\mathbf{x}$。每次更新后，将检查残差的无穷范数 $\\|\\mathbf{r}^{(k)}\\|_{\\infty} = \\|\\mathbf{b} - A\\mathbf{x}^{(k)}\\|_{\\infty}$ 是否满足容差 $\\tau$。如果条件满足，则返回当前迭代次数 $k$。如果循环在未收敛的情况下完成，则返回 $k_{\\max}$。还会对 $k=0$ 的情况进行初始检查。\n3.  **谱半径计算**：一个函数将计算 Jacobi 迭代矩阵 $T_J = I - D^{-1}A$。将使用 `numpy.linalg.eigvals` 找到 $T_J$ 的特征值，谱半径将是其绝对值的最大值。\n4.  **执行与输出**：程序的主体部分将遍历三个测试用例（矩阵）。对于每个用例，它将调用求解器函数以获取迭代次数 $k_J$ 和 $k_{GS}$，并调用谱半径函数获取 $\\rho_J$。结果将被收集并按照问题陈述中指定的格式格式化为单个字符串。", "answer": "```python\nimport numpy as np\n\ndef jacobi(A: np.ndarray, b: np.ndarray, x0: np.ndarray, tol: float, k_max: int) -> int:\n    \"\"\"\n    Solves the system Ax=b using the Jacobi method.\n\n    Args:\n        A: The n x n coefficient matrix.\n        b: The n x 1 right-hand side vector.\n        x0: The initial guess vector.\n        tol: The residual tolerance.\n        k_max: The maximum number of iterations.\n\n    Returns:\n        The number of iterations required for convergence.\n    \"\"\"\n    n = A.shape[0]\n    x = x0.copy()\n\n    # Check for k=0\n    residual_norm = np.linalg.norm(b - A @ x, np.inf)\n    if residual_norm = tol:\n        return 0\n\n    D = np.diag(A)\n    R = A - np.diag(D)  # R = L + U\n\n    for k in range(1, k_max + 1):\n        x_new = (b - R @ x) / D\n        x = x_new\n        residual_norm = np.linalg.norm(b - A @ x, np.inf)\n        if residual_norm = tol:\n            return k\n    \n    return k_max\n\ndef gauss_seidel(A: np.ndarray, b: np.ndarray, x0: np.ndarray, tol: float, k_max: int) -> int:\n    \"\"\"\n    Solves the system Ax=b using the Gauss-Seidel method.\n\n    Args:\n        A: The n x n coefficient matrix.\n        b: The n x 1 right-hand side vector.\n        x0: The initial guess vector.\n        tol: The residual tolerance.\n        k_max: The maximum number of iterations.\n\n    Returns:\n        The number of iterations required for convergence.\n    \"\"\"\n    n = A.shape[0]\n    x = x0.copy()\n\n    # Check for k=0\n    residual_norm = np.linalg.norm(b - A @ x, np.inf)\n    if residual_norm = tol:\n        return 0\n\n    for k in range(1, k_max + 1):\n        x_old = x.copy()\n        for i in range(n):\n            sum1 = np.dot(A[i, :i], x[:i])\n            sum2 = np.dot(A[i, i + 1:], x_old[i + 1:])\n            x[i] = (b[i] - sum1 - sum2) / A[i, i]\n        \n        residual_norm = np.linalg.norm(b - A @ x, np.inf)\n        if residual_norm = tol:\n            return k\n            \n    return k_max\n\ndef get_spectral_radius_J(A: np.ndarray) -> float:\n    \"\"\"\n    Computes the spectral radius of the Jacobi iteration matrix.\n\n    Args:\n        A: The n x n coefficient matrix.\n\n    Returns:\n        The spectral radius of the Jacobi matrix T_J.\n    \"\"\"\n    D = np.diag(np.diag(A))\n    D_inv = np.linalg.inv(D)\n    T_J = np.eye(A.shape[0]) - D_inv @ A\n    eigenvalues = np.linalg.eigvals(T_J)\n    spectral_radius = np.max(np.abs(eigenvalues))\n    return spectral_radius\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Global parameters\n    n = 6\n    b = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n    x0 = np.zeros(n)\n    tol = 1e-8\n    k_max = 20000\n\n    # Define the three matrices\n    test_cases = []\n\n    # Case 1: Strongly diagonally dominant\n    alpha_s = 20.0\n    gamma1 = -1.0\n    A1 = np.full((n, n), gamma1)\n    np.fill_diagonal(A1, alpha_s)\n    test_cases.append(A1)\n\n    # Case 2: Just-barely diagonally dominant\n    alpha_w = 5.1\n    gamma2 = -1.0\n    A2 = np.full((n, n), gamma2)\n    np.fill_diagonal(A2, alpha_w)\n    test_cases.append(A2)\n\n    # Case 3: Tridiagonal edge case\n    A3 = 2.0 * np.eye(n) - np.eye(n, k=1) - np.eye(n, k=-1)\n    test_cases.append(A3)\n\n    results = []\n    for A in test_cases:\n        k_J = jacobi(A, b, x0, tol, k_max)\n        k_GS = gauss_seidel(A, b, x0, tol, k_max)\n        rho_J = get_spectral_radius_J(A)\n\n        results.append(k_J)\n        results.append(k_GS)\n        results.append(round(rho_J, 6))\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2406932"}, {"introduction": "标准的 Gauss-Seidel 方法因其数据依赖性而难以并行化，这在现代计算中是一个主要瓶颈。本练习将引导你实现一种更高级的技巧——红黑着色排序法——来解决这个问题。通过将计算网格点分为“红点”和“黑点”，你可以打破顺序更新的限制，从而为并行计算打开大门，这是提升大规模科学计算效率的关键一步。[@problem_id:2442121]", "problem": "实现一个完整、可运行的程序，该程序使用红黑Gauss-Seidel定常迭代法，构建并求解由二阶椭圆偏微分方程的有限差分-离散化所产生的线性系统。从以下基本事实出发：在带有Dirichlet边界条件的矩形均匀网格上，泊松型算子会产生一个稀疏线性系统 $A \\mathbf{u} = \\mathbf{b}$，其中 $A$ 来自中心差分，并且定常迭代法仅基于当前和最近的值通过局部松弛来更新近似解。\n\n您必须完成以下任务。\n\n- 考虑在单位正方形 $\\Omega = (0,1)\\times(0,1)$ 上的模型问题族，该区域在均匀网格上离散化，每个坐标方向有 $N$ 个内部点，网格间距为 $h = 1/(N+1)$。在边界 $\\partial \\Omega$ 上施加齐次Dirichlet边界条件 $u=0$。\n- 使用中心差分在均匀网格上离散化各向异性算子 $- \\left( \\alpha \\, \\partial_{xx} + \\beta \\, \\partial_{yy} \\right) u = f$。对于各向同性情况，这会导出一个标准 $5$ 点模板形式的稀疏线性系统 $A \\mathbf{u} = \\mathbf{b}$；对于 $\\alpha \\neq \\beta$ 的情况，则为其自然的各向异性推广。\n- 实现一个红黑Gauss-Seidel方法。红黑着色根据内部网格点 $(i,j)$ 的坐标和 $i+j$ 的奇偶性来定义：一种颜色对应 $(i+j)$ 为偶数，另一种对应 $(i+j)$ 为奇数。一次完整的扫描包括：\n  - 同时更新所有红点（它们仅依赖于黑点邻居）。\n  - 同时更新所有黑点（它们仅依赖于红点邻居）。\n- 使用每个内部网格点上由离散方程所隐含的标准定常松弛更新，不引入任何超松弛参数。\n- 将网格上的离散残差定义为 $r = \\mathbf{b} - A \\mathbf{u}$，并在内部节点上使用欧几里得范数（离散 $2$-范数），即 $\\lVert r \\rVert_2 = \\left( \\sum_{i=1}^{N} \\sum_{j=1}^{N} r_{ij}^2 \\right)^{1/2}$。不要用 $h$ 进行缩放。\n- 所有迭代都从零初始猜测开始。\n\n您的程序必须执行以下测试套件，并按规定汇总结果。此问题不涉及任何物理单位。\n\n测试套件：\n\n1) 常数右端项的收敛性（各向同性）\n- 参数: $N = 64$, $\\alpha = 1$, $\\beta = 1$。\n- 右端项: 在内部点上 $f(x,y) \\equiv 1$。\n- 迭代次数: $K = 50$ 次红黑扫描。\n- 此案例的输出: $K$ 次扫描后的残差缩减因子，即浮点数 $q_1 = \\lVert r^{K} \\rVert_2 / \\lVert r^{0} \\rVert_2$。\n\n2) 人造解的精度（各向同性）\n- 参数: $N = 64$, $\\alpha = 1$, $\\beta = 1$。\n- 精确解: $u^\\star(x,y) = \\sin(\\pi x) \\sin(\\pi y)$。\n- 右端项: 选择与 $- \\Delta u^\\star = f$ 在内部点上一致的 $f$。\n- 迭代次数: $K = 200$ 次红黑扫描。\n- 此案例的输出: 相对离散 $2$-范数误差 $q_2 = \\lVert u^{K} - u^\\star \\rVert_2 / \\lVert u^\\star \\rVert_2$，以十进制小数表示。\n\n3) 各向异性情况下的收敛性（中等各向异性）\n- 参数: $N = 64$, $\\alpha = 1$, $\\beta = 10$。\n- 右端项: 在内部点上 $f(x,y) \\equiv 1$。\n- 迭代次数: $K = 200$ 次红黑扫描。\n- 此案例的输出: 残差缩减因子 $q_3 = \\lVert r^{K} \\rVert_2 / \\lVert r^{0} \\rVert_2$。\n\n4) 最小网格边界情况（单个内部节点）\n- 参数: $N = 1$, $\\alpha = 1$, $\\beta = 1$。\n- 右端项: 在单个内部点上 $f(x,y) \\equiv 1$。\n- 迭代次数: $K = 1$ 次红黑扫描。\n- 此案例的输出: 计算出的内部值 $q_4 = u^{K}_{1,1}$，为浮点数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为 $[q_1,q_2,q_3,q_4]$。例如，一个有效的输出格式是 $[0.12345,0.001234,0.5,0.0625]$（这只是格式示例，并非正确值）。\n\n程序必须是自包含的，不得读取任何输入，也不得访问任何外部资源。问题不涉及角度；无需指定角度单位。百分比必须以十进制小数表示，不能使用百分号。通过遵循有限差分-离散化方法和基于前述基本事实的红黑Gauss-Seidel方法定义，确保科学真实性。不要在问题陈述中包含任何关于更新步骤的提示或公式；所有推导都属于解答部分。", "solution": "所呈现的问题是计算物理学中一个适定且有科学依据的任务，要求实现一个用于各向异性泊松方程的红黑Gauss-Seidel迭代求解器。问题陈述完整、一致，且所有参数都得到了明确定义。因此，该问题被认为是有效的。我们着手进行推导和求解。\n\n控制偏微分方程(PDE)是一个在单位正方形域 $\\Omega = (0,1) \\times (0,1)$ 上的各向异性二阶椭圆方程：\n$$\n- \\left( \\alpha \\frac{\\partial^2 u}{\\partial x^2} + \\beta \\frac{\\partial^2 u}{\\partial y^2} \\right) = f(x,y)\n$$\n带有齐次Dirichlet边界条件，$u=0$ on $\\partial\\Omega$。常数 $\\alpha  0$ 和 $\\beta  0$ 代表各向异性扩散系数。\n\n我们在一个均匀网格上对此问题进行离散化，该网格在每个坐标方向上有 $N$ 个内部点。网格间距为 $h = 1/(N+1)$。网格点为 $(x_i, y_j) = (ih, jh)$，其中 $i, j \\in \\{0, 1, \\dots, N+1\\}$。在这些点上的离散解记为 $u_{i,j}$。内部点对应于索引 $i, j \\in \\{1, \\dots, N\\}$。边界条件意味着如果 $i \\in \\{0, N+1\\}$ 或 $j \\in \\{0, N+1\\}$，则 $u_{i,j} = 0$。\n\n二阶偏导数使用二阶中心差分公式进行近似：\n$$\n\\frac{\\partial^2 u}{\\partial x^2}\\bigg|_{(x_i, y_j)} \\approx \\frac{u_{i-1,j} - 2u_{i,j} + u_{i+1,j}}{h^2}\n$$\n$$\n\\frac{\\partial^2 u}{\\partial y^2}\\bigg|_{(x_i, y_j)} \\approx \\frac{u_{i,j-1} - 2u_{i,j} + u_{i,j+1}}{h^2}\n$$\n将这些近似代入每个内部点 $(i,j)$ 的PDE中，得到一个线性方程组：\n$$\n- \\left( \\alpha \\frac{u_{i-1,j} - 2u_{i,j} + u_{i+1,j}}{h^2} + \\beta \\frac{u_{i,j-1} - 2u_{i,j} + u_{i,j+1}}{h^2} \\right) = f_{i,j}\n$$\n其中 $f_{i,j} = f(x_i, y_j)$。整理各项，我们得到5点模板方程：\n$$\n2(\\alpha + \\beta)u_{i,j} - \\alpha(u_{i-1,j} + u_{i+1,j}) - \\beta(u_{i,j-1} + u_{i,j+1}) = h^2 f_{i,j}\n$$\n这个关于 $N^2$ 个内部未知数 $u_{i,j}$ 的 $N^2$ 个方程组构成了线性系统 $A \\mathbf{u} = \\mathbf{b}$。根据问题陈述，我们定义矩阵 $A$ 和向量 $\\mathbf{b}$，使得节点 $(i,j)$ 处的方程为 $(A\\mathbf{u})_{i,j} = b_{i,j}$。我们采用一个标准的选择：\n$$\n(A\\mathbf{u})_{i,j} = \\frac{1}{h^2} \\left( 2(\\alpha + \\beta)u_{i,j} - \\alpha(u_{i-1,j} + u_{i+1,j}) - \\beta(u_{i,j-1} + u_{i,j+1}) \\right)\n$$\n$$\nb_{i,j} = f_{i,j}\n$$\n\nGauss-Seidel方法是求解 $A \\mathbf{u} = \\mathbf{b}$ 的一种迭代技术。$u_{i,j}$ 的更新是通过在模板方程中将其分离出来，并使用其邻居的最新值得到的。红黑Gauss-Seidel方法通过将内部网格点划分为两个集合来组织此过程：$i+j$ 为偶数的“红”点和$i+j$ 为奇数的“黑”点。5点模板的一个关键特性是，红点的所有邻居都是黑点，黑点的所有邻居都是红点。这种依赖结构允许在单次扫描内进行两阶段、可并行的更新。\n\n设 $u^k$ 是 $k$ 次扫描后的解向量。从 $k$ 到 $k+1$ 的一次扫描按如下方式进行：\n首先，我们同时更新所有红点。对于每个红点 $(i,j)$，新值 $u_{i,j}^{k+1/2}$ 使用其黑点邻居在上一轮迭代 $k$ 中的值来计算：\n$$\nu_{i,j}^{k+1/2} = \\frac{1}{2(\\alpha+\\beta)} \\left( h^2 f_{i,j} + \\alpha(u_{i-1,j}^k + u_{i+1,j}^k) + \\beta(u_{i,j-1}^k + u_{i,j+1}^k) \\right) \\quad \\text{对于 } i+j \\text{ 为偶数}\n$$\n其次，我们同时更新所有黑点。对于每个黑点 $(i,j)$，新值 $u_{i,j}^{k+1}$ 使用其红点邻居在中间步骤 $k+1/2$ 中新更新的值来计算：\n$$\nu_{i,j}^{k+1} = \\frac{1}{2(\\alpha+\\beta)} \\left( h^2 f_{i,j} + \\alpha(u_{i-1,j}^{k+1/2} + u_{i+1,j}^{k+1/2}) + \\beta(u_{i,j-1}^{k+1/2} + u_{i,j+1}^{k+1/2}) \\right) \\quad \\text{对于 } i+j \\text{ 为奇数}\n$$\n迭代从初始猜测 $u^0$ 开始，该初始猜测指定为零向量。\n\n离散残差定义为 $r = \\mathbf{b} - A\\mathbf{u}$。在网格点 $(i,j)$ 处，这是：\n$$\nr_{i,j} = f_{i,j} - \\frac{1}{h^2} \\left[ 2(\\alpha+\\beta)u_{i,j} - \\alpha(u_{i-1,j} + u_{i+1,j}) - \\beta(u_{i,j-1} + u_{i,j+1}) \\right]\n$$\n问题要求计算在内部网格上定义的向量的范数。网格函数 $v_{i,j}$ 的离散 $2$-范数是 $\\lVert v \\rVert_2 = \\left( \\sum_{i=1}^{N} \\sum_{j=1}^{N} v_{i,j}^2 \\right)^{1/2}$。\n\n测试案例要求进行特定的计算：\n1.  **案例 1 和 3**：残差缩减因子是 $q = \\lVert r^K \\rVert_2 / \\lVert r^0 \\rVert_2$。对于初始猜测 $u^0 = \\mathbf{0}$，初始残差为 $r^0 = \\mathbf{b} - A\\mathbf{0} = \\mathbf{b}$，对应于网格函数 $f_{i,j}$。因此，$\\lVert r^0 \\rVert_2 = \\lVert f \\rVert_2$。\n2.  **案例 2**：人造解是 $u^\\star(x,y) = \\sin(\\pi x) \\sin(\\pi y)$。对应的右端项是 $f = -(\\alpha \\partial_{xx} + \\beta \\partial_{yy})u^\\star$。对于各向同性情况（$\\alpha=\\beta=1$），$f = -\\Delta u^\\star = 2\\pi^2\\sin(\\pi x)\\sin(\\pi y) = 2\\pi^2 u^\\star$。相对误差是 $q_2 = \\lVert u^K - u^\\star \\rVert_2 / \\lVert u^\\star \\rVert_2$，其中 $u^\\star$ 在内部网格点上求值。\n3.  **案例 4**：对于 $N=1$，只有一个内部点 $(1,1)$，其中 $i+j=2$ 为偶数。这是一个红点。红黑扫描更新这一个点，而黑点更新步骤是无操作的。精确解是 $u_{1,1} = h^2 f_{1,1} / (2(\\alpha+\\beta))$。迭代从 $u^0_{1,1}=0$ 开始。迭代的第一步（也是唯一一步）计算出精确解。对于给定的参数 $N=1, \\alpha=1, \\beta=1, f=1$，我们有 $h=1/2$，所以 $u^1_{1,1} = (1/2)^2 \\cdot 1 / (2(1+1)) = (1/4)/4 = 1/16 = 0.0625$。\n\n实现将使用Python语言和NumPy库来完成，以进行高效的数组操作。解 $u$ 将由一个 $(N+2) \\times (N+2)$ 的数组表示，以容纳零值边界条件，从而通过数组切片简化邻居访问。红点和黑点的更新将对相应的网格点进行向量化操作。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to execute the test suite for the red-black Gauss-Seidel solver.\n    \"\"\"\n\n    def run_red_black_gs(N, alpha, beta, f_grid, K):\n        \"\"\"\n        Executes K sweeps of the red-black Gauss-Seidel method.\n\n        Args:\n            N (int): Number of interior grid points per dimension.\n            alpha (float): Coefficient for the d_xx term.\n            beta (float): Coefficient for the d_yy term.\n            f_grid (np.ndarray): The right-hand side function f on the interior grid.\n            K (int): The number of sweeps to perform.\n\n        Returns:\n            np.ndarray: The solution u on the full (N+2)x(N+2) grid after K sweeps.\n        \"\"\"\n        h = 1.0 / (N + 1)\n        h2 = h * h\n        \n        # Initialize solution grid u with homogeneous Dirichlet boundary conditions.\n        u = np.zeros((N + 2, N + 2))\n\n        # Create red/black masks for the interior of the grid.\n        # Interior points (i,j) are 1-indexed, map to array indices [i-1, j-1].\n        # Parity is based on (i+j).\n        I, J = np.ogrid[0:N, 0:N]\n        red_mask_interior = ((I + 1) + (J + 1)) % 2 == 0\n        black_mask_interior = ~red_mask_interior\n        \n        # Pre-calculate components of the update formula.\n        h2_f = h2 * f_grid\n        denom = 1.0 / (2.0 * (alpha + beta))\n\n        for _ in range(K):\n            # Update red points\n            # These depend only on black neighbors from the previous full step.\n            neighbors_sum_r = (\n                alpha * (u[0:N, 1:N+1] + u[2:N+2, 1:N+1]) +\n                beta * (u[1:N+1, 0:N] + u[1:N+1, 2:N+2])\n            )\n            u_updated_r = (h2_f + neighbors_sum_r) * denom\n            u[1:N+1, 1:N+1][red_mask_interior] = u_updated_r[red_mask_interior]\n\n            # Update black points\n            # These depend on the newly updated red neighbors.\n            neighbors_sum_b = (\n                alpha * (u[0:N, 1:N+1] + u[2:N+2, 1:N+1]) +\n                beta * (u[1:N+1, 0:N] + u[1:N+1, 2:N+2])\n            )\n            u_updated_b = (h2_f + neighbors_sum_b) * denom\n            u[1:N+1, 1:N+1][black_mask_interior] = u_updated_b[black_mask_interior]\n            \n        return u\n\n    def calculate_residual_norm(u, N, alpha, beta, f_grid):\n        \"\"\"\n        Calculates the 2-norm of the residual r = b - Au.\n        Here b corresponds to f_grid.\n        \"\"\"\n        h = 1.0 / (N + 1)\n        h2 = h * h\n        \n        # Apply the discrete operator A to the interior of u\n        Au_interior = (\n            2.0 * (alpha + beta) * u[1:N+1, 1:N+1] -\n            alpha * (u[0:N, 1:N+1] + u[2:N+2, 1:N+1]) -\n            beta * (u[1:N+1, 0:N] + u[1:N+1, 2:N+2])\n        ) / h2\n        \n        residual_interior = f_grid - Au_interior\n        return np.linalg.norm(residual_interior)\n\n    results = []\n\n    # Test Case 1: Convergence on a constant right-hand side (isotropic)\n    N1, alpha1, beta1, K1 = 64, 1.0, 1.0, 50\n    f1_grid = np.ones((N1, N1))\n    r0_norm_1 = np.linalg.norm(f1_grid)\n    u_K1 = run_red_black_gs(N1, alpha1, beta1, f1_grid, K1)\n    rK_norm_1 = calculate_residual_norm(u_K1, N1, alpha1, beta1, f1_grid)\n    q1 = rK_norm_1 / r0_norm_1\n    results.append(q1)\n\n    # Test Case 2: Manufactured-solution accuracy (isotropic)\n    N2, alpha2, beta2, K2 = 64, 1.0, 1.0, 200\n    h2 = 1.0 / (N2 + 1)\n    # Grid for interior points\n    x2_int = np.linspace(h2, 1.0 - h2, N2)\n    y2_int = np.linspace(h2, 1.0 - h2, N2)\n    xx2_int, yy2_int = np.meshgrid(x2_int, y2_int)\n    \n    u_exact_interior = np.sin(np.pi * xx2_int) * np.sin(np.pi * yy2_int)\n    f2_grid = 2.0 * np.pi**2 * u_exact_interior\n    \n    u_K2 = run_red_black_gs(N2, alpha2, beta2, f2_grid, K2)\n    u_K2_interior = u_K2[1:N2+1, 1:N2+1]\n    \n    error_norm_2 = np.linalg.norm(u_K2_interior - u_exact_interior)\n    u_exact_norm_2 = np.linalg.norm(u_exact_interior)\n    q2 = error_norm_2 / u_exact_norm_2\n    results.append(q2)\n\n    # Test Case 3: Convergence for anisotropy (moderate anisotropy)\n    N3, alpha3, beta3, K3 = 64, 1.0, 10.0, 200\n    f3_grid = np.ones((N3, N3))\n    r0_norm_3 = np.linalg.norm(f3_grid)\n    u_K3 = run_red_black_gs(N3, alpha3, beta3, f3_grid, K3)\n    rK_norm_3 = calculate_residual_norm(u_K3, N3, alpha3, beta3, f3_grid)\n    q3 = rK_norm_3 / r0_norm_3\n    results.append(q3)\n\n    # Test Case 4: Minimal grid edge case (single interior node)\n    N4, alpha4, beta4, K4 = 1, 1.0, 1.0, 1\n    f4_grid = np.ones((N4, N4))\n    u_K4 = run_red_black_gs(N4, alpha4, beta4, f4_grid, K4)\n    q4 = u_K4[1, 1]\n    results.append(q4)\n\n    # Print results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2442121"}]}