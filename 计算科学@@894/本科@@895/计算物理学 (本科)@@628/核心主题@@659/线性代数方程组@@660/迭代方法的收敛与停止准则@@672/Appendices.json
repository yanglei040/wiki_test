{"hands_on_practices": [{"introduction": "在迭代算法中，“何时停止”是一个核心问题。一个理想的停止准则应能可靠地表明我们已足够接近真实解。本练习将探讨两种最常见的停止准ぞ则：残差大小 $|p(x_k)|$ 和步长大小 $|x_k - x_{k-1}|$。通过分析牛顿法在求解一个特殊多项式根时的表现，你将亲手揭示这些看似直观的准则在何种情况下会失效，并理解为何仅依赖单一准则可能导致算法提前终止，从而错过真正的解 [@problem_id:2199023]。", "problem": "一位数值分析课程的学生接到的任务是使用 Newton's method 求多项式 $p(x) = x^3 - 3x + 2$ 的一个根。Newton's method 的迭代公式为 $x_{k+1} = x_k - \\frac{p(x_k)}{p'(x_k)}$，其中 $p'(x)$ 是 $p(x)$ 的导数。该学生从初始猜测值 $x_0 = 1.2$ 开始。\n\n为了决定何时停止迭代，该学生考虑了两个常见的停止准则，两者都使用 $\\epsilon = 0.05$ 的容差：\n\n*   **准则A**：残差容差，当 $|p(x_k)| < \\epsilon$ 时满足。\n*   **准则B**：步长容差，当 $k \\ge 1$ 时，若 $|x_k - x_{k-1}| < \\epsilon$ 则满足。\n\n执行 Newton's method 的前两次迭代，求出 $x_1$ 和 $x_2$。你应该在中间计算中使用足够的精度，以避免舍入误差影响你的结论。根据你对 $x_1$ 和 $x_2$ 的计算结果，判断下列哪个陈述对情况的分析最为准确。\n\nA) 准则A和准则B在第一次迭代后（$k=1$时）都满足，表明收敛快速且稳健。\n\nB) 经过两次迭代后（$k=2$时），准则A和准则B都未满足，表明该方法收敛非常慢或发散。\n\nC) 准则A在第一次迭代后（$k=1$时）满足，但准则B不满足。这种现象表明，单独使用准则A可能导致算法过早停止。\n\nD) 准则B在第一次迭代后（$k=1$时）满足，但准则A不满足。这种现象表明，单独使用准则B可能导致算法过早停止。\n\nE) 两个准则都在第二次迭代时（$k=2$时）首次满足。", "solution": "给定 $p(x) = x^{3} - 3x + 2$ 以及 Newton 迭代\n$$\nx_{k+1} = x_{k} - \\frac{p(x_{k})}{p'(x_{k})},\n$$\n其中 $x_{0} = 1.2$，容差 $\\epsilon = 0.05$。首先计算导数：\n$$\np'(x) = 3x^{2} - 3.\n$$\n\n第一次迭代（$k=0$ 到 $k=1$）。在 $x_{0} = 1.2$ 处计算 $p$ 和 $p'$ 的值：\n$$\np(1.2) = 1.2^{3} - 3\\cdot 1.2 + 2 = 1.728 - 3.6 + 2 = 0.128,\n$$\n$$\np'(1.2) = 3\\cdot (1.2)^{2} - 3 = 3\\cdot 1.44 - 3 = 4.32 - 3 = 1.32.\n$$\n因此\n$$\nx_{1} = 1.2 - \\frac{0.128}{1.32} = \\frac{6}{5} - \\frac{16}{165} = \\frac{182}{165}.\n$$\n通过计算 $|p(x_{1})|$ 来检验 $k=1$ 时的准则A。使用精确算术，\n$$\np\\!\\left(\\frac{182}{165}\\right) = \\left(\\frac{182}{165}\\right)^{3} - 3\\left(\\frac{182}{165}\\right) + 2\n= \\frac{6,028,568}{4,492,125} - \\frac{182}{55} + 2\n= \\frac{147,968}{4,492,125}.\n$$\n与 $\\epsilon = \\frac{1}{20}$ 比较：因为 $147,968 \\cdot 20 = 2,959,360 < 4,492,125$，我们有\n$$\n|p(x_{1})| = \\frac{147,968}{4,492,125} < \\frac{1}{20} = \\epsilon,\n$$\n所以准则A在 $k=1$ 时满足。\n\n检验 $k=1$ 时的准则B：\n$$\n|x_{1} - x_{0}| = \\left|\\frac{182}{165} - \\frac{6}{5}\\right| = \\frac{16}{165}.\n$$\n与 $\\epsilon = \\frac{1}{20}$ 比较：因为 $16\\cdot 20 = 320 > 165$，我们有\n$$\n|x_{1} - x_{0}| = \\frac{16}{165} > \\frac{1}{20} = \\epsilon,\n$$\n所以准则B在 $k=1$ 时不满足。\n\n第二次迭代（$k=1$ 到 $k=2$）。我们需要 $p'(x_{1})$ 和 $p(x_{1})$：\n$$\np'(x_{1}) = 3\\left(\\frac{182}{165}\\right)^{2} - 3 = \\frac{5899}{9075}, \\quad p(x_{1}) = \\frac{147,968}{4,492,125}.\n$$\n因此\n$$\nx_{2} = x_{1} - \\frac{p(x_{1})}{p'(x_{1})}\n= \\frac{182}{165} - \\left(\\frac{147,968}{4,492,125}\\right)\\left(\\frac{9075}{5899}\\right)\n= \\frac{182}{165} - \\frac{147,968}{2,920,005}\n= \\frac{3,072,886}{2,920,005}.\n$$\n使用 Newton 步长的大小来检验 $k=2$ 时的准则B：\n$$\n|x_{2} - x_{1}| = \\frac{147,968}{2,920,005}.\n$$\n与 $\\epsilon = \\frac{1}{20}$ 比较：因为 $147,968 \\cdot 20 = 2,959,360 > 2,920,005$，所以\n$$\n|x_{2} - x_{1}| = \\frac{147,968}{2,920,005} > \\frac{1}{20} = \\epsilon,\n$$\n所以准则B在 $k=2$ 时仍然不满足。同时，由于 $x_{2}$ 比 $x_{1}$ 更接近位于 $x=1$ 的二重根，且 $p(x) = (x-1)^{2}(x+2)$，我们有 $|p(x_{2})| = |x_{2}-1|^{2}\\,|x_{2}+2| < |p(x_{1})| < \\epsilon$，所以准则A仍然满足。\n\n因此，在第一次迭代后，准则A满足而准则B不满足，这与陈述C相符。此外，即使在两次迭代后，准则B仍未满足，这进一步说明了在重根附近仅依赖准则A可能会导致方法过早停止。", "answer": "$$\\boxed{C}$$", "id": "2199023"}, {"introduction": "当迭代的对象从标量变为矢量时，例如在求解特征向量时，停止准则的设计需要更加精细。由于特征向量是一个方向，其数值表示可能在迭代过程中发生符号翻转，简单的比较方法可能会失效。本练习引导你为幂迭代法设计一个更稳健的停止准则，该准则基于连续两次迭代向量之间的夹角 $\\theta_k$ [@problem_id:2427048]。通过这种方式，你将学会如何创建一个不受向量具体数值影响、仅关注其方向收敛性的可靠判据。", "problem": "您接到的任务是为迭代特征向量近似设计并验证一个鲁棒的停止准则。该准则依赖于连续单位范数迭代向量之间的夹角，而不仅仅是估计特征值的变化。设 $A \\in \\mathbb{R}^{n \\times n}$ 为一个实矩阵，$\\|\\cdot\\|$ 表示欧氏范数。对于一个非零初始向量 $v_0 \\in \\mathbb{R}^n$，根据每个测试用例中指定的方法，通过以下更新规则之一来定义归一化的迭代序列 $\\{u_k\\}$：\n\n- 标准迭代：\n$$\nu_{k+1} \\;=\\; \\frac{A u_k}{\\|A u_k\\|}.\n$$\n\n- 带实位移 $\\sigma$ 的位移-反转迭代：\n$$\n\\text{求解 } (A - \\sigma I) w = u_k \\text{ 得到 } w, \\quad u_{k+1} \\;=\\; \\frac{w}{\\|w\\|}.\n$$\n\n在每次迭代中，连续单位向量 $u_k$ 和 $u_{k+1}$ 之间的锐主角定义为\n$$\n\\theta_k \\;=\\; \\arccos\\!\\big(\\,|u_{k+1}^\\top u_k|\\,\\big),\n$$\n角度以弧度为单位。鲁棒的停止准则规定如下：当 $\\theta_k \\le \\tau_\\theta$ 连续成立 $s$ 次迭代时，宣布收敛，其中 $\\tau_\\theta > 0$ 和 $s \\in \\mathbb{N}$ 是给定的。如果在 $N_{\\max}$ 次迭代内未达到收敛，则宣布该测试不收敛。\n\n您的程序必须为下述每个测试用例计算满足基于角度的停止准则所需的迭代次数。如果在位移-反转迭代中，线性系统对于指定的位移是奇异的或数值上不可解的，或者在迭代上限内未满足停止准则，则为该测试用例返回整数 $-1$。\n\n所有角度都必须以弧度计算。所有内积和范数都必须是欧氏的。最终输出必须是一行，包含整个测试套件的结果，形式为用方括号括起来的逗号分隔的整数列表。\n\n测试套件（所有矩阵和向量都明确给出）：\n\n- 案例 $1$（标准迭代，良态对称矩阵，理想情况）：\n  - $A^{(1)} = \\begin{bmatrix} 4 & 1 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 0 & 2 \\end{bmatrix}$,\n    $v_0^{(1)} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$,\n    $\\tau_\\theta^{(1)} = 10^{-8}$,\n    $s^{(1)} = 2$,\n    $N_{\\max}^{(1)} = 1000$。\n- 案例 $2$（标准迭代，因主特征值几乎相等导致的慢收敛，边缘情况）：\n  - $A^{(2)} = \\begin{bmatrix} 1 & 10^{-4} & 0 \\\\ 10^{-4} & 0.999 & 0 \\\\ 0 & 0 & 0.5 \\end{bmatrix}$,\n    $v_0^{(2)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0.1 \\end{bmatrix}$,\n    $\\tau_\\theta^{(2)} = 10^{-10}$,\n    $s^{(2)} = 3$,\n    $N_{\\max}^{(2)} = 50000$。\n- 案例 $3$（标准迭代，带负号的主导特征值，通过锐角实现的符号翻转鲁棒性）：\n  - $A^{(3)} = \\begin{bmatrix} -5 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 2 \\end{bmatrix}$,\n    $v_0^{(3)} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$,\n    $\\tau_\\theta^{(3)} = 10^{-8}$,\n    $s^{(3)} = 2$,\n    $N_{\\max}^{(3)} = 1000$。\n- 案例 $4$（位移-反转迭代，以 $\\sigma = 0$ 为目标寻找最小特征值）：\n  - $A^{(4)} = \\begin{bmatrix} 10 & 2 & 0 \\\\ 2 & 5 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix}$,\n    $v_0^{(4)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n    $\\sigma^{(4)} = 0$,\n    $\\tau_\\theta^{(4)} = 10^{-8}$,\n    $s^{(4)} = 2$,\n    $N_{\\max}^{(4)} = 1000$。\n- 案例 $5$（位移-反转迭代，$\\sigma$ 接近一个已知特征值以引发极快收敛，边界情况）：\n  - $A^{(5)} = \\begin{bmatrix} 7 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$,\n    $v_0^{(5)} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$,\n    $\\sigma^{(5)} = 3.001$,\n    $\\tau_\\theta^{(5)} = 10^{-12}$,\n    $s^{(5)} = 1$,\n    $N_{\\max}^{(5)} = 100$。\n\n要求的最终输出格式：您的程序应产生一行输出，其中按顺序包含案例 1 到 5 的迭代次数，格式为方括号括起来的逗号分隔列表（例如，$\\texttt{[n1,n2,n3,n4,n5]}$），其中每个 $n_j$ 是一个整数。", "solution": "问题陈述经评估是有效的。它科学地基于数值线性代数的既定原则，特别是用于特征值问题的幂法和反幂法。该问题是适定的，为每个测试用例提供了所有必要的矩阵、向量和参数。目标定义清晰，术语精确且无歧义。\n\n这个问题的核心在于实现两种相关的迭代算法来寻找矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 的特征向量，并应用一个基于连续迭代向量之间夹角的鲁棒停止准则。\n\n第一个算法是标准幂迭代，由递推关系定义\n$$\nu_{k+1} \\;=\\; \\frac{A u_k}{\\|A u_k\\|}\n$$\n其中 $u_k$ 是第 $k$ 次迭代时的归一化向量，$\\|\\cdot\\|$ 是欧氏范数。对于大多数起始向量 $v_0$（从中可以导出 $u_0 = v_0 / \\|v_0\\|$），该序列收敛到对应于模最大特征值（主导特征值）$\\lambda_1$ 的特征向量。收敛速度由比率 $|\\lambda_2 / \\lambda_1|$ 决定，其中 $\\lambda_2$ 是模第二大的特征值。如果这个比率接近 1，收敛会很慢。\n\n第二个算法是位移-反转迭代法。对于给定的位移 $\\sigma \\in \\mathbb{R}$，其更新规则是\n$$\n\\text{求解 } (A - \\sigma I) w = u_k \\text{ 得到 } w, \\quad u_{k+1} \\;=\\; \\frac{w}{\\|w\\|}.\n$$\n这在数学上等同于对矩阵 $(A - \\sigma I)^{-1}$ 应用幂迭代。$(A - \\sigma I)^{-1}$ 的特征值为 $1/(\\lambda_i - \\sigma)$，其中 $\\lambda_i$ 是 $A$ 的特征值。对此逆矩阵进行幂迭代将收敛到其主导特征值对应的特征向量。$(A - \\sigma I)^{-1}$ 的这个主导特征值对应于使 $|\\lambda_j - \\sigma|$ 最小化的 $A$ 的特征值 $\\lambda_j$。因此，位移-反转是一种强大的方法，可以找到与所选位移 $\\sigma$ 最接近的 $A$ 的特征值所对应的特征向量。通过设置 $\\sigma=0$，该方法（此时称为简单反迭代法）找到模最小的特征值所对应的特征向量。如果选择的 $\\sigma$ 非常接近一个特征值，收敛会异常迅速。如果 $\\sigma$ 恰好是一个特征值，该方法会失败，因为矩阵 $(A - \\sigma I)$ 会变为奇异矩阵，线性系统不可解。\n\n停止准则基于连续归一化迭代向量之间的锐主角 $\\theta_k = \\arccos(|u_{k+1}^\\top u_k|)$。一个特征向量是一个方向，仅在非零标量倍数的意义上是确定的。迭代方法可能产生方向上收敛但符号翻转的估计，即 $u_{k+1} \\approx -u_k$。在这种情况下，点积 $u_{k+1}^\\top u_k$ 趋近于 $-1$。通过取绝对值 $|u_{k+1}^\\top u_k|$，我们确保 $\\arccos$ 的参数趋近于 $1$，从而使角度 $\\theta_k$ 正确地趋近于 $0$，表明特征向量方向的收敛。该准则要求 $\\theta_k \\le \\tau_\\theta$ 这个条件连续满足 $s$ 次迭代，以防止因偶然出现的小角度而过早终止。\n\n解决每个案例的算法如下：\n$1$. 通过归一化起始向量来初始化迭代：$u_0 = v_0 / \\|v_0\\|$。\n$2$. 初始化连续成功计数器 `consecutive_successes` 为 $0$。\n$3$. 从 $k = 1, 2, \\dots, N_{\\max}$ 开始，至多循环 $N_{\\max}$ 次迭代。\n$4$. 在每次迭代 $k$ 中，根据前一个迭代向量 $u_{k-1}$，使用标准或位移-反转公式计算下一个迭代向量 $u_k$。对于后者，这涉及求解一个线性系统，如果矩阵是奇异的，则可能失败。这种失败会导致终止并返回 $-1$。\n$5$. 计算角度 $\\theta_{k-1} = \\arccos(\\text{clip}(|u_k^\\top u_{k-1}|, -1.0, 1.0))$。裁剪（clip）是一种数值保护措施，以防止浮点误差可能导致点积的模略大于 $1$。\n$6$. 检查是否 $\\theta_{k-1} \\le \\tau_\\theta$。如果为真，则将 `consecutive_successes` 加一。如果为假，则将其重置为 $0$。\n$7$. 如果 `consecutive_successes` 达到 $s$，则准则被满足。过程终止，当前迭代次数 $k$ 即为结果。\n$8$. 如果循环在未满足准则的情况下完成，意味着已超过 $N_{\\max}$。过程终止，结果为 $-1$。\n\n这种程序化设计直接实现了指定的迭代方法和停止准则的数学原理，并按规定处理了潜在的数值问题和失败模式。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_case(A, v0, method, params):\n    \"\"\"\n    Computes the number of iterations for an eigenvector approximation to converge.\n\n    Args:\n        A (np.ndarray): The matrix.\n        v0 (np.ndarray): The initial vector.\n        method (str): 'standard' or 'shift-and-invert'.\n        params (dict): A dictionary of parameters:\n            - tau_theta (float): Angle tolerance.\n            - s (int): Number of consecutive successful checks required.\n            - N_max (int): Maximum number of iterations.\n            - sigma (float, optional): Shift for shift-and-invert.\n\n    Returns:\n        int: The number of iterations, or -1 for failure/non-convergence.\n    \"\"\"\n    tau_theta = params['tau_theta']\n    s = params['s']\n    N_max = params['N_max']\n    sigma = params.get('sigma')\n\n    # 1. Normalize the initial vector.\n    norm_v0 = np.linalg.norm(v0)\n    if np.isclose(norm_v0, 0):\n        return -1 # Should not happen with given problems.\n    u_prev = v0 / norm_v0\n\n    consecutive_successes = 0\n\n    # 2. Main iteration loop.\n    for k in range(1, N_max + 1):\n        # 3. Compute the next iterate u_k.\n        try:\n            if method == 'standard':\n                v_next = A @ u_prev\n                norm_v = np.linalg.norm(v_next)\n                if np.isclose(norm_v, 0): # Iteration breaks down\n                    return -1\n                u_k = v_next / norm_v\n            elif method == 'shift-and-invert':\n                M = A - sigma * np.identity(A.shape[0])\n                w = np.linalg.solve(M, u_prev)\n                norm_w = np.linalg.norm(w)\n                if np.isclose(norm_w, 0):\n                    return -1\n                u_k = w / norm_w\n            else:\n                # Should not be reached\n                return -1\n        except np.linalg.LinAlgError:\n            # Singular matrix in shift-and-invert\n            return -1\n\n        # 4. Compute the angle theta.\n        dot_product = np.dot(u_k, u_prev)\n        # Use absolute value for the acute angle.\n        # Clip to handle floating-point inaccuracies where |dot_product| > 1.\n        angle = np.arccos(np.clip(np.abs(dot_product), -1.0, 1.0))\n        \n        # 5. Check the stopping criterion.\n        if angle <= tau_theta:\n            consecutive_successes += 1\n        else:\n            consecutive_successes = 0\n            \n        # 6. Check for convergence.\n        if consecutive_successes >= s:\n            return k # Return current iteration number on convergence.\n\n        # Prepare for the next iteration.\n        u_prev = u_k\n        \n    # 7. N_max reached without convergence.\n    return -1\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            'A': np.array([[4, 1, 0], [1, 3, 0], [0, 0, 2]], dtype=float),\n            'v0': np.array([1, 1, 1], dtype=float),\n            'method': 'standard',\n            'params': {'tau_theta': 1e-8, 's': 2, 'N_max': 1000}\n        },\n        # Case 2\n        {\n            'A': np.array([[1, 1e-4, 0], [1e-4, 0.999, 0], [0, 0, 0.5]], dtype=float),\n            'v0': np.array([1, 0, 0.1], dtype=float),\n            'method': 'standard',\n            'params': {'tau_theta': 1e-10, 's': 3, 'N_max': 50000}\n        },\n        # Case 3\n        {\n            'A': np.array([[-5, 0, 0], [0, 3, 0], [0, 0, 2]], dtype=float),\n            'v0': np.array([1, 1, 1], dtype=float),\n            'method': 'standard',\n            'params': {'tau_theta': 1e-8, 's': 2, 'N_max': 1000}\n        },\n        # Case 4\n        {\n            'A': np.array([[10, 2, 0], [2, 5, 1], [0, 1, 1]], dtype=float),\n            'v0': np.array([1, 0, 0], dtype=float),\n            'method': 'shift-and-invert',\n            'params': {'tau_theta': 1e-8, 's': 2, 'N_max': 1000, 'sigma': 0}\n        },\n        # Case 5\n        {\n            'A': np.array([[7, 0, 0], [0, 3, 0], [0, 0, 1]], dtype=float),\n            'v0': np.array([1, 1, 1], dtype=float),\n            'method': 'shift-and-invert',\n            'params': {'tau_theta': 1e-12, 's': 1, 'N_max': 100, 'sigma': 3.001}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_case(case['A'], case['v0'], case['method'], case['params'])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2427048"}, {"introduction": "在处理现实世界的带噪数据时，迭代算法的收敛目标变得更加复杂——我们的目标往往不是无限逼近一个理论上的“完美”解，而是在信号恢复和噪声放大之间找到最佳平衡点。本练习将你带入图像去模糊的真实场景，使用 Richardson-Lucy 算法，并实现一个基于“噪声残差”的智能停止准则 [@problem_id:2382781]。你将通过监控残差的高频分量，学习如何判断迭代何时从“恢复信号”转向“放大噪声”，并在关键时刻停止，以获得最佳的图像复原效果。", "problem": "考虑一个大小为 $n \\times n$ 的方形网格上的离散二维成像模型，像素索引为 $(i,j)$，其中 $i \\in \\{0,1,\\dots,n-1\\}$ 且 $j \\in \\{0,1,\\dots,n-1\\}$。令 $x^{(k)} \\in \\mathbb{R}_{\\ge 0}^{n \\times n}$ 表示在第 $k$ 次迭代时未知场景的非负估计值，$h \\in \\mathbb{R}_{\\ge 0}^{m \\times m}$ 为一个非负、归一化的点扩散函数，而 $y_{\\mathrm{obs}} \\in \\mathbb{R}_{\\ge 0}^{n \\times n}$ 为观测到的模糊图像。前向图像形成过程通过带有反射边界条件的离散卷积来建模。因此，对于任何 $z \\in \\mathbb{R}^{n \\times n}$，操作 $h * z$ 表示 $h$ 和 $z$ 在反射边界处理下的离散卷积，其输出大小与输入相同，为 $n \\times n$。迭代估计序列 $\\{x^{(k)}\\}_{k \\ge 0}$ 由以下递推关系定义：\n$$\nx^{(k+1)} \\;=\\; x^{(k)} \\,\\odot\\, \\Big( h^{\\top} * \\big( \\tfrac{y_{\\mathrm{obs}}}{h * x^{(k)} + \\varepsilon} \\big) \\Big),\n$$\n其中 $\\odot$ 表示逐元素乘法，除法也是逐元素的，$h^{\\top}$ 是沿两个轴翻转的点扩散函数，$\\varepsilon > 0$ 是一个固定的微小标量，用以防止除以零。初始迭代值为 $x^{(0)} = \\max(y_{\\mathrm{obs}}, 0)$，逐元素取值。\n\n将第 $k$ 次迭代的数据拟合残差定义为\n$$\nr_{\\mathrm{data}}^{(k)} \\;=\\; y_{\\mathrm{obs}} - (h * x^{(k)}).\n$$\n令 $S_{\\sigma}$ 表示一个标准差为 $\\sigma$（单位为像素）的高斯平滑算子，它通过与一个可分离、归一化的离散高斯核进行卷积来实现，并采用反射边界条件。标量噪声残差则为\n$$\n\\rho_k \\;=\\; \\big\\| r_{\\mathrm{data}}^{(k)} - S_{\\sigma}\\!\\big(r_{\\mathrm{data}}^{(k)}\\big) \\big\\|_2,\n$$\n其中 $\\|\\cdot\\|_2$ 是对所有像素计算的欧几里得范数。停止索引 $k_{\\mathrm{stop}}$ 定义为满足 $\\rho_k > \\rho_{k-1}$ 的最小整数 $k \\in \\{1,2,\\dots,K_{\\max}\\}$。如果在 $K_{\\max}$ 之前不存在这样的 $k$，则定义 $k_{\\mathrm{stop}} = K_{\\max}$。\n\n对所有测试用例使用以下通用参数：网格大小 $n = 25$，点扩散函数大小 $m = 9$，点扩散函数标准差 $\\sigma_h = 1.2$，以及除法保护值 $\\varepsilon = 10^{-12}$。点扩散函数 $h$ 是一个 $m \\times m$ 的离散高斯核，其元素为\n$$\nh[a,b] \\propto \\exp\\!\\Big(-\\frac{a^2 + b^2}{2\\sigma_h^2}\\Big)\n$$\n其中 $a,b \\in \\{-\\tfrac{m-1}{2},\\dots,\\tfrac{m-1}{2}\\}$，并进行归一化以使 $\\sum_{a,b} h[a,b] = 1$。\n\n按如下方式构建三个测试用例。\n\n- 测试用例 1 (无噪声单源)：令真实场景 $x_{\\mathrm{true}}$ 为位于中心像素 $(i_0,j_0) = (\\tfrac{n-1}{2},\\tfrac{n-1}{2})$ 的单个单位脉冲，即 $x_{\\mathrm{true}}[i_0,j_0] = 1$，其他位置为 $x_{\\mathrm{true}}[i,j] = 0$。令观测值为无噪声的 $y_{\\mathrm{obs}} = h * x_{\\mathrm{true}}$。设置平滑参数 $\\sigma = 1.0$，最大迭代次数 $K_{\\max} = 50$。\n\n- 测试用例 2 (带泊松噪声的单源)：使用与测试用例 1 相同的 $x_{\\mathrm{true}}$，并形成无噪声的均值图像 $y = h * x_{\\mathrm{true}}$。通过对所有像素抽取独立样本 $Y[i,j] \\sim \\mathrm{Poisson}(M \\, y[i,j])$，生成一个已知计数缩放因子 $M = 5000$ 的泊松观测值，并定义 $y_{\\mathrm{obs}}[i,j] = Y[i,j]/M$。固定随机数生成器种子以确保确定性。设置 $\\sigma = 1.0$ 和 $K_{\\max} = 50$。\n\n- 测试用例 3 (带泊松噪声的双源)：令 $x_{\\mathrm{true}}$ 包含位于像素坐标 $(i_1,j_1) = (\\tfrac{n-1}{2}-3,\\tfrac{n-1}{2})$ 和 $(i_2,j_2) = (\\tfrac{n-1}{2}+3,\\tfrac{n-1}{2})$ 的两个单位脉冲。定义 $y = h * x_{\\mathrm{true}}$，并通过 $Y[i,j] \\sim \\mathrm{Poisson}(M \\, y[i,j])$ 生成一个缩放因子为 $M = 2000$ 的泊松观测值，$y_{\\mathrm{obs}} = Y/M$，使用与测试用例 2 相同的固定随机数生成器种子。设置 $\\sigma = 0.8$ 和 $K_{\\max} = 80$。\n\n您的程序必须为每个测试用例根据上述定义计算 $k_{\\mathrm{stop}}$。要求的最终输出格式为单行文本，其中包含三个按顺序排列的停止索引，以逗号分隔，并用方括号括起，例如 $[k_1,k_2,k_3]$，其中每个 $k_t$ 都是整数。\n\n请严格按照此单行格式提供结果。此问题不要求物理单位。所有角度均与此问题无关，不得使用。所有答案均为整数。测试用例 2 和 3 中的随机性必须由固定的种子控制，以使结果是确定性的。", "solution": "所提出的问题是图像复原领域中一个有效的计算物理学练习。它具有科学依据，问题定义明确，并且所有参数和程序都得到了客观和完整的规定。该任务涉及实现 Richardson-Lucy (RL) 迭代反卷积算法，并应用一个基于数据残差统计特性的特定停止准则。\n\n问题的核心是该迭代公式：\n$$\nx^{(k+1)} \\;=\\; x^{(k)} \\,\\odot\\, \\Big( h^{\\top} * \\big( \\tfrac{y_{\\mathrm{obs}}}{h * x^{(k)} + \\varepsilon} \\big) \\Big)\n$$\n这就是 Richardson-Lucy 算法，一种广泛用于解决成像领域逆问题的方法，尤其适用于数据受泊松噪声影响的情况。其公式可以从泊松分布数据的最大似然估计框架中推导出来。物理成像过程被建模为 $y_{\\mathrm{obs}} \\approx h * x_{\\mathrm{true}}$，其中一个未知的真实场景 $x_{\\mathrm{true}}$ 通过卷积（$*$）操作被点扩散函数（PSF）$h$ 模糊化。目标是在给定观测到的可能带噪的图像 $y_{\\mathrm{obs}}$ 和已知的 PSF $h$ 的情况下，恢复出对 $x_{\\mathrm{true}}$ 的估计。\n\nRL 迭代的各个组成部分具有清晰的物理和数学解释：\n1. 项 $h * x^{(k)}$ 代表了对当前估计值 $x^{(k)}$ 的预测“模糊”。它是当前解估计在数据空间上的前向投影。\n2. 比率 $\\tfrac{y_{\\mathrm{obs}}}{h * x^{(k)} + \\varepsilon}$ 是一个乘法校正因子，对每个像素逐元素计算。它将实际观测到的图像 $y_{\\mathrm{obs}}$ 与预测图像进行比较。在预测值过低的地方，该因子大于1；在预测值过高的地方，该因子小于1。微小常数 $\\varepsilon > 0$ 是一种标准的正则化技术，通过防止除以零来确保数值稳定性。\n3. 操作 $h^{\\top} * (\\dots)$ 是将此校正因子从数据空间（图像空间）反向投影回解空间（场景空间）。它将空间变化的校正信息分配回估计值 $x^{(k)}$ 上。问题将 $h^{\\top}$ 定义为沿两个轴翻转的PSF。然而，给定的 PSF，$h[a,b] \\propto \\exp(-\\frac{a^2 + b^2}{2\\sigma_h^2})$，是一个中心化的离散高斯函数，它对两个轴都是对称的。因此，翻转它没有效果，即 $h^{\\top} = h$。这使得反向投影简化为 $h * (\\dots)$。\n4. 逐元素乘法 $\\odot$ 更新当前估计值。这种乘法更新的一个关键特性是它能确保解的非负性，即如果 $x^{(0)} \\ge 0$，则对于所有 $k > 0$ 都有 $x^{(k)} \\ge 0$，这是对光强度的一个至关重要的物理约束。\n\n迭代反卷积的一个关键方面是确定何时停止。虽然 RL 算法会收敛到最大似然解，但这个解通常会极度放大 $y_{\\mathrm{obs}}$ 中存在的任何噪声。迭代次数过多会导致重建结果充满噪声和/或尖峰。因此，问题定义了一个停止准则，通过提前终止过程来对解进行正则化。\n\n停止准则基于监测数据拟合残差 $r_{\\mathrm{data}}^{(k)} = y_{\\mathrm{obs}} - (h * x^{(k)})$ 的属性。随着迭代 $k$ 的进行，估计值 $x^{(k)}$ 应该会改善，预测图像 $h * x^{(k)}$ 应能更好地逼近 $y_{\\mathrm{obs}}$。因此，理想情况下，残差 $r_{\\mathrm{data}}^{(k)}$ 应接近观测值中的噪声分量。\n\n标量噪声残差 $\\rho_k$ 定义为：\n$$\n\\rho_k \\;=\\; \\big\\| r_{\\mathrm{data}}^{(k)} - S_{\\sigma}\\!\\big(r_{\\mathrm{data}}^{(k)}\\big) \\big\\|_2\n$$\n在这里，$S_{\\sigma}$ 是一个高斯平滑算子。表达式 $r_{\\mathrm{data}}^{(k)} - S_{\\sigma}(r_{\\mathrm{data}}^{(k)})$ 充当了残差上的一个高通滤波器，分离出其高频分量或“粗糙度”。欧几里得范数 $\\rho_k$ 量化了这种粗糙度的总量。\n\n停止规则 $k_{\\mathrm{stop}} = \\min \\{k \\in \\{1,\\dots,K_{\\max}\\} \\mid \\rho_k > \\rho_{k-1}\\}$ 的基本原理如下：\n- 在初始迭代阶段，算法主要恢复真实场景 $x_{\\mathrm{true}}$ 的大尺度结构。残差 $r_{\\mathrm{data}}^{(k)}$ 包含对应于信号中尚未重建部分的显著、平滑的结构化分量。随着这些结构化分量的拟合，粗糙度度量 $\\rho_k$ 预计会减小。\n- 经过数次迭代后，估计值 $x^{(k)}$ 已经捕获了大部分信号。残差 $r_{\\mathrm{data}}^{(k)}$ 变得越来越由随机测量噪声主导。\n- 如果迭代继续进行，算法会开始“拟合噪声”。这个噪声放大过程会将细粒度的高频伪影引入估计值 $x^{(k)}$ 中，这反过来又会使残差 $r_{\\mathrm{data}}^{(k)}$ 本身变得更粗糙。\n- 条件 $\\rho_k > \\rho_{k-1}$ 检测到这一新阶段的开始。它标志着残差的粗糙度开始增加的那个点，表明噪声放大已经开始主导信号恢复过程。在这一点 $k_{\\mathrm{stop}}$ 停止，是一种启发式方法，用以获得在信号保真度和噪声抑制之间取得合理权衡的解。如果粗糙度一直不增加，则意味着对于给定的最大迭代次数 $K_{\\max}$，算法仍处于信号恢复阶段，因此过程在 $K_{\\max}$ 处停止。这在无噪声的情况下是预期的。\n\n数值实现首先生成通用的 PSF $h$，一个 $m \\times m = 9 \\times 9$ 且 $\\sigma_h = 1.2$ 的离散高斯核。对于每个测试用例，构建特定的真实场景 $x_{\\mathrm{true}}$，并通过无噪声卷积或添加模拟的泊松噪声来生成观测图像 $y_{\\mathrm{obs}}$。初始估计按规定设置为 $x^{(0)} = y_{\\mathrm{obs}}$。用于 $S_{\\sigma}$ 的平滑核也是一个大小为 $9 \\times 9$、标准差为指定 $\\sigma$ 的离散高斯核。然后主循环迭代计算 $x^{(k+1)}$ 和 $\\rho_{k+1}$，并存储 $\\rho$ 值的历史记录，以便在每一步检查停止条件。卷积操作按要求使用反射边界条件执行。固定的随机数生成器种子确保了带噪测试用例的可复现性。计算出的停止索引对于第一个案例是 $k_{\\mathrm{stop}}=50$，第二个案例是 $k_{\\mathrm{stop}}=6$，第三个案例是 $k_{\\mathrm{stop}}=14$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.ndimage import convolve\n\ndef make_gaussian_kernel(size, sigma):\n    \"\"\"\n    Creates a square, normalized 2D Gaussian kernel.\n    \"\"\"\n    coords = np.arange(-(size - 1) / 2.0, (size - 1) / 2.0 + 1)\n    x, y = np.meshgrid(coords, coords)\n    kernel = np.exp(-(x**2 + y**2) / (2 * sigma**2))\n    return kernel / np.sum(kernel)\n\ndef run_simulation(n, y_obs, K_max, sigma_s, h, psf_size, epsilon):\n    \"\"\"\n    Runs the Richardson-Lucy iteration and determines the stopping index.\n    \"\"\"\n    # The PSF h is symmetric, so the flipped PSF h^T is equal to h.\n    # The smoothing kernel for the stopping criterion is also symmetric.\n    # The problem does not specify the size of the smoothing kernel; a size\n    # equal to the PSF size is a reasonable choice.\n    s_kernel = make_gaussian_kernel(psf_size, sigma_s)\n\n    # Initial estimate: x_0 = max(y_obs, 0). Since y_obs >= 0, x_0 = y_obs.\n    x_k = y_obs.copy()\n\n    # Calculate rho_0\n    r_data_k = y_obs - convolve(x_k, h, mode='reflect')\n    s_r_k = convolve(r_data_k, s_kernel, mode='reflect')\n    rho_k = np.linalg.norm(r_data_k - s_r_k)\n    rho_history = [rho_k]\n\n    k_stop = K_max\n    for k in range(K_max):  # loop variable k from 0 to K_max - 1\n        # This loop computes x_{k+1}, where the iteration number is k+1.\n        \n        # Predicted image from current estimate x_k\n        y_pred = convolve(x_k, h, mode='reflect')\n        \n        # Correction factor\n        ratio = y_obs / (y_pred + epsilon)\n        \n        # Back-projection of the correction\n        correction = convolve(ratio, h, mode='reflect')\n        \n        # Multiplicative update\n        x_k_plus_1 = x_k * correction\n        \n        # Calculate rho_{k+1}\n        r_data_k_plus_1 = y_obs - convolve(x_k_plus_1, h, mode='reflect')\n        s_r_k_plus_1 = convolve(r_data_k_plus_1, s_kernel, mode='reflect')\n        rho_k_plus_1 = np.linalg.norm(r_data_k_plus_1 - s_r_k_plus_1)\n\n        # The stopping index is the smallest k in {1, ..., K_max}\n        # My loop computes iteration 'k+1'. Check if rho_{k+1} > rho_k\n        if rho_k_plus_1 > rho_history[-1]:\n            k_stop = k + 1\n            break\n        \n        rho_history.append(rho_k_plus_1)\n        x_k = x_k_plus_1\n\n    return k_stop\n\ndef solve():\n    # Define the common parameters from the problem statement.\n    n = 25\n    m = 9\n    sigma_h = 1.2\n    epsilon = 1e-12\n\n    # Generate the common Point Spread Function (PSF)\n    h = make_gaussian_kernel(m, sigma_h)\n    \n    # Initialize a random number generator with a fixed seed for reproducibility\n    rng = np.random.default_rng(seed=42)\n    \n    results = []\n\n    # --- Test Case 1: Noiseless single source ---\n    K_max_1 = 50\n    sigma_s_1 = 1.0\n    x_true_1 = np.zeros((n, n), dtype=np.float64)\n    center = (n - 1) // 2\n    x_true_1[center, center] = 1.0\n    y_obs_1 = convolve(x_true_1, h, mode='reflect')\n    \n    k_stop_1 = run_simulation(n, y_obs_1, K_max_1, sigma_s_1, h, m, epsilon)\n    results.append(k_stop_1)\n\n    # --- Test Case 2: Single source with Poisson noise ---\n    K_max_2 = 50\n    sigma_s_2 = 1.0\n    M_2 = 5000.0\n    x_true_2 = np.zeros((n, n), dtype=np.float64)\n    x_true_2[center, center] = 1.0\n    y_mean_2 = convolve(x_true_2, h, mode='reflect')\n    y_obs_2 = rng.poisson(M_2 * y_mean_2) / M_2\n\n    k_stop_2 = run_simulation(n, y_obs_2, K_max_2, sigma_s_2, h, m, epsilon)\n    results.append(k_stop_2)\n    \n    # --- Test Case 3: Two sources with Poisson noise ---\n    K_max_3 = 80\n    sigma_s_3 = 0.8\n    M_3 = 2000.0\n    x_true_3 = np.zeros((n, n), dtype=np.float64)\n    x_true_3[center - 3, center] = 1.0\n    x_true_3[center + 3, center] = 1.0\n    y_mean_3 = convolve(x_true_3, h, mode='reflect')\n    y_obs_3 = rng.poisson(M_3 * y_mean_3) / M_3\n    \n    k_stop_3 = run_simulation(n, y_obs_3, K_max_3, sigma_s_3, h, m, epsilon)\n    results.append(k_stop_3)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2382781"}]}