{"hands_on_practices": [{"introduction": "理论知识需要通过实践来巩固。我们的第一个练习将从一个直观的一维热传导问题入手。通过为一个简化的物理模型计算单步迭代，你将亲身体验逐次超松弛（SOR）方法的基本计算流程，并理解其如何应用于实际的物理系统中。[@problem_id:1369790]", "problem": "正在分析一个薄绝缘杆的简化稳态热分布模型。该杆被离散化为五个等距点，$P_0, P_1, P_2, P_3, P_4$。两个端点的温度保持恒定，分别为 $T_0 = 25.0^\\circ\\text{C}$ 和 $T_4 = 100.0^\\circ\\text{C}$。三个内部点（记为 $T_1, T_2,$ 和 $T_3$）的温度是未知的。\n\n在稳态下，每个内部点的温度是其两个直接相邻点温度的算术平均值。这个物理原理导出了一个关于未知温度 $T_1, T_2,$ 和 $T_3$ 的线性方程组。\n\n为了求解该方程组，采用了逐次超松弛（SOR）迭代法。松弛参数选择为 $\\omega = 1.15$。迭代从所有未知温度的初始猜测值为零开始，即 $T_1^{(0)} = T_2^{(0)} = T_3^{(0)} = 0$。\n\n计算第一次完整迭代后第一个内部点 $T_1$ 的温度值，记为 $T_1^{(1)}$。答案以摄氏度表示，并四舍五入到三位有效数字。", "solution": "一维网格上的稳态条件要求，对于每个内部节点，满足以下平均关系\n$$\nT_{i}=\\frac{T_{i-1}+T_{i+1}}{2}.\n$$\n对于这三个未知数，这会产生以下线性系统\n$$\n\\begin{aligned}\n2T_{1}-T_{2}&=T_{0},\\\\\n-T_{1}+2T_{2}-T_{3}&=0,\\\\\n-T_{2}+2T_{3}&=T_{4}.\n\\end{aligned}\n$$\n对于系统 $A\\mathbf{T}=\\mathbf{b}$，使用SOR迭代法，\n$$\nT_{i}^{(k+1)}=(1-\\omega)T_{i}^{(k)}+\\frac{\\omega}{a_{ii}}\\left(b_{i}-\\sum_{j<i}a_{ij}T_{j}^{(k+1)}-\\sum_{j>i}a_{ij}T_{j}^{(k)}\\right),\n$$\n$T_{1}$ 的更新公式（其中 $a_{11}=2$, $a_{12}=-1$, $b_{1}=T_{0}$）为\n$$\nT_{1}^{(1)}=(1-\\omega)T_{1}^{(0)}+\\frac{\\omega}{2}\\left(T_{0}-(-1)T_{2}^{(0)}\\right).\n$$\n使用初始猜测值 $T_{1}^{(0)}=T_{2}^{(0)}=0$，\n$$\nT_{1}^{(1)}=\\frac{\\omega}{2}T_{0}.\n$$\n代入 $\\omega=1.15$ 和 $T_{0}=25.0$ 得\n$$\nT_{1}^{(1)}=\\frac{1.15}{2}\\times 25.0=14.375.\n$$\n四舍五入到三位有效数字，结果为 $14.4$ 摄氏度。", "answer": "$$\\boxed{14.4}$$", "id": "1369790"}, {"introduction": "计算方法不仅仅是遵循公式，更重要的是理解其背后的原理和适用条件。这个练习是一个思想实验，旨在揭示SOR方法在特定条件下的惊人效率。通过分析一个具有特殊结构（下三角矩阵）的线性系统，你将深入理解迭代方法的收敛特性，并发现它与直接法之间的深刻联系。[@problem_id:2207402]", "problem": "考虑求解一个 $n \\times n$ 线性方程组 $A\\mathbf{x} = \\mathbf{b}$ 的问题，其中矩阵 $A$ 已知是非奇异的下三角矩阵。我们希望使用逐次超松弛（SOR）方法来求解该方程组，这是一种迭代技术。\n\nSOR 迭代的分量形式公式由下式给出：\n$$x_i^{(k+1)} = (1-\\omega)x_i^{(k)} + \\frac{\\omega}{a_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} a_{ij}x_j^{(k+1)} - \\sum_{j=i+1}^{n} a_{ij}x_j^{(k)} \\right)$$\n对 $i = 1, 2, \\dots, n$。此处，$\\mathbf{x}^{(k)}$ 是第 $k$ 次迭代时的解向量，$a_{ij}$ 是矩阵 $A$ 的元素，$b_i$ 是向量 $\\mathbf{b}$ 的分量，而 $\\omega$ 是松弛因子，它是一个在区间 $(0, 2)$ 内的实数。\n\n对于一个通用矩阵 $A$，当 $k \\to \\infty$ 时，SOR 方法会收敛到真解 $\\mathbf{x}^*$。然而，对于矩阵结构和参数 $\\omega$ 的某些特定组合，迭代有可能在有限步内得到*精确*解。\n\n假设您可以为这种特定类型的矩阵选择最有效的松弛因子 $\\omega$ 值，那么对于任意的初始猜测 $\\mathbf{x}^{(0)}$（假定 $\\mathbf{x}^{(0)} \\neq \\mathbf{x}^*$），要保证 $\\mathbf{x}^{(k)} = \\mathbf{x}^*$ 所需的最小迭代次数是多少？", "solution": "我们从给定的 SOR 分量形式迭代公式开始：\n$$\nx_{i}^{(k+1)}=(1-\\omega)x_{i}^{(k)}+\\frac{\\omega}{a_{ii}}\\left(b_{i}-\\sum_{j=1}^{i-1}a_{ij}x_{j}^{(k+1)}-\\sum_{j=i+1}^{n}a_{ij}x_{j}^{(k)}\\right),\\quad i=1,2,\\dots,n.\n$$\n由于 $A$ 是下三角矩阵，我们有对所有 $j>i$ 都有 $a_{ij}=0$。因此第二个求和项消失，迭代简化为\n$$\nx_{i}^{(k+1)}=(1-\\omega)x_{i}^{(k)}+\\frac{\\omega}{a_{ii}}\\left(b_{i}-\\sum_{j=1}^{i-1}a_{ij}x_{j}^{(k+1)}\\right).\n$$\n因为 $A$ 是非奇异的下三角矩阵，所以每个 $a_{ii}\\neq 0$，因此除以 $a_{ii}$ 的运算是良定义的。\n\n选择松弛因子 $\\omega=1$。那么迭代变为\n$$\nx_{i}^{(k+1)}=\\frac{1}{a_{ii}}\\left(b_{i}-\\sum_{j=1}^{i-1}a_{ij}x_{j}^{(k+1)}\\right),\\quad i=1,2,\\dots,n.\n$$\n这正是求解 $A\\mathbf{x}=\\mathbf{b}$ 的前向代换方程：精确解 $\\mathbf{x}^{*}$ 对每个 $i$ 满足：\n$$\na_{ii}x_{i}^{*}=b_{i}-\\sum_{j=1}^{i-1}a_{ij}x_{j}^{*}.\n$$\n我们通过对 $i$ 进行数学归纳来证明，经过一次扫描（一次迭代）后，我们得到对所有 $i$ 都有 $x_{i}^{(k+1)}=x_{i}^{*}$，这与初始猜测 $\\mathbf{x}^{(0)}$ 无关。\n\n基础情况 $i=1$：由于求和项为空，\n$$\nx_{1}^{(k+1)}=\\frac{b_{1}}{a_{11}}=x_{1}^{*}.\n$$\n归纳步骤：假设对 $j=1,\\dots,i-1$ 有 $x_{j}^{(k+1)}=x_{j}^{*}$。那么\n$$\nx_{i}^{(k+1)}=\\frac{1}{a_{ii}}\\left(b_{i}-\\sum_{j=1}^{i-1}a_{ij}x_{j}^{(k+1)}\\right)\n=\\frac{1}{a_{ii}}\\left(b_{i}-\\sum_{j=1}^{i-1}a_{ij}x_{j}^{*}\\right)=x_{i}^{*}.\n$$\n因此，通过数学归纳法，在 $\\omega=1$ 的情况下经过一次迭代后，我们得到 $\\mathbf{x}^{(k+1)}=\\mathbf{x}^{*}$，这与初始猜测 $\\mathbf{x}^{(0)}$ 无关。\n\n最后，由于初始猜测是任意的，且可能满足 $\\mathbf{x}^{(0)}\\neq\\mathbf{x}^{*}$，零次迭代不能保证 $\\mathbf{x}^{(0)}=\\mathbf{x}^{*}$。因此，对于任意初始猜测（当 $\\mathbf{x}^{(0)}\\neq\\mathbf{x}^{*}$ 时），保证 $\\mathbf{x}^{(k)}=\\mathbf{x}^{*}$ 的最小迭代次数是一次。", "answer": "$$\\boxed{1}$$", "id": "2207402"}, {"introduction": "现在，让我们将所学知识应用于一个更复杂、更真实的二维问题中。这个实践任务要求你通过编程来求解物理学中经典的拉普拉斯方程 $\\nabla^2 u = 0$。你将亲手实现并比较雅可比法、高斯-赛德尔法以及SOR方法的收敛速度，从而直观地量化SOR方法在加速收敛方面的强大优势。[@problem_id:2406769]", "problem": "考虑在方形域 $\\Omega = (0,1)\\times(0,1)$ 上的二维拉普拉斯方程 $\\nabla^2 u = 0$，其边界条件为狄利克雷（Dirichlet）边界条件。设边界数据为 $u(x,0) = \\sin(\\pi x)$、$u(x,1) = 0$、$u(0,y)=0$ 和 $u(1,y)=0$，其中角度以弧度为单位。使用均匀网格对该域进行离散化，每个空间方向有 $N$ 个内部点（因此网格间距为 $h = \\frac{1}{N+1}$，包括边界在内共有 $(N+2)\\times(N+2)$ 个总网格点）。使用标准的拉普拉斯算子二阶中心有限差分近似，为内部未知数获得一个线性系统。从连续方程和中心差分导出的离散拉普拉斯算子定义出发，实现三种定常迭代法来求解离散方程：雅可比（Jacobi）法、高斯–赛德尔（Gauss–Seidel）法和逐次超松弛（SOR）法（松弛参数 $\\omega$ 满足 $0 < \\omega < 2$）。对每种方法，使用以下基本依据和定义：\n\n- 内部网格点 $(i,j)$ 处的离散拉普拉斯方程由 $\\nabla^2 u = 0$ 的中心差分近似得到：\n$$\\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2} + \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^2} = 0,$$\n这在代数上等价于差分格式方程\n$$-u_{i+1,j} - u_{i-1,j} - u_{i,j+1} - u_{i,j-1} + 4 u_{i,j} = 0.$$\n- 由此，将内部的离散残差定义为\n$$r_{i,j} = 4u_{i,j} - \\left(u_{i+1,j}+u_{i-1,j}+u_{i,j+1}+u_{i,j-1}\\right),$$\n其无穷范数定义为 $\\|r\\|_{\\infty} = \\max_{i,j} |r_{i,j}|$.\n- 使用停止准则 $\\|r^{(k)}\\|_{\\infty} \\le \\text{tol}\\cdot \\|r^{(0)}\\|_{\\infty}$，其中 $\\text{tol}$ 是给定的容差， $k$ 是迭代指数，初始猜测 $u^{(0)}$ 在所有内部点上为零，并在边界点上取固定的边界值。\n\n您的程序必须：\n- 实现雅可比迭代，该迭代仅使用前一次迭代的值来更新所有内部值。\n- 实现高斯–赛德尔迭代，其方式是使用最新的可用邻近值（您可以使用红黑排序来实现此目的）。\n- 使用红黑排序和松弛参数 $\\omega$ 实现SOR迭代，通过以下公式进行\n$$u^{(k+1)}_{i,j} = u^{(k)}_{i,j} + \\omega\\left(\\frac{1}{4}\\left(u^{(*)}_{i+1,j}+u^{(*)}_{i-1,j}+u^{(*)}_{i,j+1}+u^{(*)}_{i,j-1}\\right) - u^{(k)}_{i,j}\\right),$$\n其中 $u^{(*)}$ 表示与每种颜色上的高斯–赛德尔排序一致的最新值。取 $\\omega = 1$ 可恢复为高斯–赛德尔法。\n\n您的任务是通过报告每种方法在每个测试用例中满足停止准则所需的迭代次数，来定量比较这三种方法的收敛速率。对所有方法使用相同的离散化、边界条件和停止判据，并将迭代次数报告为整数。\n\n测试套件。在以下参数集上运行您的程序，其中每个测试用例是一个三元组 $(N, \\text{tol}, \\omega)$:\n- 测试 1：$(20, 10^{-5}, 1.5)$。\n- 测试 2：$(20, 10^{-5}, 1.0)$。\n- 测试 3：$(10, 10^{-8}, 1.8)$。\n- 测试 4：$(40, 10^{-4}, 1.9)$。\n\n对于每个测试用例，您的程序必须生成一个列表 $[n_J, n_{GS}, n_{SOR}]$，其中包含雅可比法、高斯–赛德尔法和SOR法各自达到停止准则所需的迭代次数。将所有测试的结果聚合为单行，形式为一个由这些列表组成的、用逗号分隔的列表，没有空格，并用方括号括起来。例如，您的输出必须严格如下所示\n$[[n_J^{(1)},n_{GS}^{(1)},n_{SOR}^{(1)}],[n_J^{(2)},n_{GS}^{(2)},n_{SOR}^{(3)}],[n_J^{(3)},n_{GS}^{(3)},n_{SOR}^{(3)}],[n_J^{(4)},n_{GS}^{(4)},n_{SOR}^{(4)}]]$,\n并作为单行打印。不涉及物理单位。正弦函数中的角度必须以弧度解释。", "solution": "所述问题是一个标准的、适定的关于椭圆型偏微分方程数值解法的练习。它在科学上是合理的、自洽的，并且算法上是明确的。不存在矛盾、歧义或事实错误。因此，我们直接进行求解。\n\n该问题要求在单位方形域 $\\Omega = (0,1)\\times(0,1)$ 上求解二维拉普拉斯方程 $\\nabla^2 u = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = 0$。位势 $u(x,y)$ 受狄利克雷（Dirichlet）边界条件约束：$u(x,0) = \\sin(\\pi x)$，并且在其他三条边界上 $u=0$。\n\n第一步是对连续问题进行离散化。该域被一个含 $(N+2) \\times (N+2)$ 个点的均匀网格所覆盖，其中 $N$ 是每个方向上内部点的数量。网格间距为 $h = \\frac{1}{N+1}$。一个网格点由 $(x_i, y_j) = (ih, jh)$ 表示，其中索引 $i,j \\in \\{0, 1, \\dots, N+1\\}$。该点处的位势值为 $u_{i,j} \\approx u(x_i, y_j)$。\n\n在每个内部网格点 $(i,j)$ 处，拉普拉斯算子 $\\nabla^2$ 使用二阶中心差分公式进行近似：\n$$ \\nabla^2 u \\bigg|_{(x_i, y_j)} \\approx \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2} + \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^2} $$\n将此近似值设为 $0$，得到内部点（$1 \\le i,j \\le N$）的离散拉普拉斯方程：\n$$ u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} - 4u_{i,j} = 0 $$\n这个五点差分格式方程可以重排，将 $u_{i,j}$ 表示为其四个邻近点的平均值：\n$$ u_{i,j} = \\frac{1}{4} \\left( u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} \\right) $$\n这组关于 $N^2$ 个内部未知数值的 $N^2$ 个线性方程构成一个大型稀疏线性系统，形式为 $A\\mathbf{u} = \\mathbf{b}$，其中 $\\mathbf{u}$ 是未知数 $u_{i,j}$ 的向量，矩阵 $A$ 代表离散拉普拉斯算子。右端向量 $\\mathbf{b}$ 包含了固定的边界值。此类系统非常适合用迭代法求解。\n\n我们的任务是实现三种经典的定常迭代法：雅可比（Jacobi）法、高斯–赛德尔（Gauss-Seidel）法和逐次超松弛（SOR）法。这些方法从一个初始猜测 $u^{(0)}$ 开始，生成一系列收敛到真实解的近似值 $u^{(k)}$。\n\n雅可比法是最简单的迭代格式。对于每个点 $(i,j)$，新值 $u_{i,j}^{(k+1)}$ 仅使用前一次迭代 $u^{(k)}$ 的值来计算。更新规则是平均值公式的直接应用：\n$$ u_{i,j}^{(k+1)} = \\frac{1}{4} \\left( u_{i+1,j}^{(k)} + u_{i-1,j}^{(k)} + u_{i,j+1}^{(k)} + u_{i,j-1}^{(k)} \\right) $$\n这个更新可以对所有内部点同时（或以任何顺序）执行，因为在同一次迭代中，每个点的计算都独立于其他点。在向量化实现中，需要一个迭代 $k$ 的完整网格副本来计算迭代 $k+1$ 的网格。\n\n高斯–赛德尔法通过在当前迭代中使用最新计算出的值来改进雅可比法。更新规则是：\n$$ u_{i,j}^{(k+1)} = \\frac{1}{4} \\left( u_{i+1,j}^{(*)} + u_{i-1,j}^{(*)} + u_{i,j+1}^{(*)} + u_{i,j-1}^{(*)} \\right) $$\n其中 $u^{(*)}$ 表示可用的最新值。例如，在字典序（逐行逐列）中，计算 $u_{i,j}^{(k+1)}$ 会使用当前迭代 $k+1$ 的 $u_{i-1,j}^{(k+1)}$ 和 $u_{i,j-1}^{(k+1)}$，以及前一次迭代 $k$ 的 $u_{i+1,j}^{(k)}$ 和 $u_{i,j+1}^{(k)}$。这种对更新顺序的依赖使得并行化变得复杂。红黑排序方案规避了这个问题。网格点被像棋盘一样着色。所有“红”点首先被更新，使用其“黑”邻居（来自前一次迭代）的值。然后，所有“黑”点被更新，使用其“红”邻居新计算出的值。这两个阶段（红点更新，黑点更新）中的每一个都可以被完全向量化。\n\n逐次超松弛（SOR）法是高斯–赛德尔法的一种外推，旨在加速收敛。它计算高斯–赛德尔更新，然后由一个松弛参数 $\\omega$ 控制，将解在该方向上进一步推进。更新公式为：\n$$ u_{i,j}^{(k+1)} = u_{i,j}^{(k)} + \\omega \\left( u_{i,j}^{\\text{GS}} - u_{i,j}^{(k)} \\right) = (1-\\omega)u_{i,j}^{(k)} + \\omega u_{i,j}^{\\text{GS}} $$\n其中 $u_{i,j}^{\\text{GS}}$ 是在该点上通过高斯–赛德尔步骤计算出的值。与高斯–赛德尔法一样，SOR法使用红黑排序来实现，以有效地使用最新值。当 $\\omega=1$ 时，SOR法精确地简化为高斯–赛德尔法。对于拉普拉斯类问题，选择一个在 $1 < \\omega < 2$ 范围内的最优 $\\omega$ （超松弛）通常会显著加快收敛速度。\n\n停止判据基于离散残差的无穷范数，定义为 $\\|r^{(k)}\\|_{\\infty} = \\max_{i,j} |r_{i,j}^{(k)}|$，其中 $r_{i,j}^{(k)} = 4u_{i,j}^{(k)} - (u_{i+1,j}^{(k)} + u_{i-1,j}^{(k)} + u_{i,j+1}^{(k)} + u_{i,j-1}^{(k)})$。当 $\\|r^{(k)}\\|_{\\infty} \\le \\text{tol} \\cdot \\|r^{(0)}\\|_{\\infty}$ 时，迭代停止，其中 $\\text{tol}$ 是给定的容差，$\\|r^{(0)}\\|_{\\infty}$ 是初始猜测（内部 $u^{(0)}=0$）的残差范数。这个相对判据确保了不同问题设置之间的公平比较。\n\n该实现由三个主要函数组成。一个函数设置 $(N+2) \\times (N+2)$ 网格，将内部初始化为 $0$ 并设置边界条件。第二个函数实现雅可比迭代。第三个函数实现带红黑排序的SOR迭代，通过设置 $\\omega=1$ 也可用于高斯–赛德尔法。一个辅助函数在每一步计算残差范数。主程序遍历测试用例，调用相应的求解函数，并记录收敛所需的迭代次数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef setup_initial_state(N):\n    \"\"\"\n    Initializes the grid with boundary conditions and zero interior.\n\n    Args:\n        N (int): Number of interior points in each direction.\n\n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: The (N+2)x(N+2) grid `u`.\n            - float: The grid spacing `h`.\n    \"\"\"\n    h = 1.0 / (N + 1)\n    u = np.zeros((N + 2, N + 2))\n    \n    # Set boundary condition u(x,0) = sin(pi*x)\n    # The j=0 row corresponds to y=0.\n    x_coords = np.linspace(0, 1, N + 2)\n    u[0, :] = np.sin(np.pi * x_coords)\n    \n    # Other boundaries u(x,1)=0, u(0,y)=0, u(1,y)=0 are already zero.\n    return u, h\n\ndef calculate_residual_norm(u, N):\n    \"\"\"\n    Calculates the infinity norm of the residual on the interior grid.\n\n    Args:\n        u (np.ndarray): The full (N+2)x(N+2) grid.\n        N (int): Number of interior grid points.\n\n    Returns:\n        float: The infinity norm of the residual.\n    \"\"\"\n    interior = u[1:N + 1, 1:N + 1]\n    neighbors_sum = (u[1:N + 1, 2:N + 2] + u[1:N + 1, 0:N] +\n                     u[2:N + 2, 1:N + 1] + u[0:N, 1:N + 1])\n    residual = 4 * interior - neighbors_sum\n    return np.max(np.abs(residual))\n\ndef solve_jacobi(N, tol):\n    \"\"\"\n    Solves the Laplace equation using the Jacobi method.\n\n    Args:\n        N (int): Number of interior grid points.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    u, h = setup_initial_state(N)\n    \n    r0_norm = calculate_residual_norm(u, N)\n    if r0_norm == 0:\n        return 0\n    \n    threshold = tol * r0_norm\n    \n    k = 0\n    while True:\n        k += 1\n        \n        u_old = u.copy()\n        \n        neighbors_sum = (u_old[1:N + 1, 2:N + 2] + u_old[1:N + 1, 0:N] +\n                         u_old[2:N + 2, 1:N + 1] + u_old[0:N, 1:N + 1])\n        u[1:N + 1, 1:N + 1] = 0.25 * neighbors_sum\n        \n        r_norm = calculate_residual_norm(u, N)\n        if r_norm <= threshold:\n            return k\n\ndef solve_sor(N, tol, omega):\n    \"\"\"\n    Solves the Laplace equation using SOR with red-black ordering.\n    Recovers Gauss-Seidel for omega=1.0.\n\n    Args:\n        N (int): Number of interior grid points.\n        tol (float): Convergence tolerance.\n        omega (float): Relaxation parameter.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    u, h = setup_initial_state(N)\n    \n    r0_norm = calculate_residual_norm(u, N)\n    if r0_norm == 0:\n        return 0\n        \n    threshold = tol * r0_norm\n    \n    # Create red-black masks for the interior (N x N) grid.\n    # (j, i) indices for the interior part start from 0.\n    # Grid point (j_grid, i_grid) where j_grid, i_grid in [1,N]\n    # corresponds to mask point (j_grid-1, i_grid-1).\n    # Color depends on (j_grid + i_grid). (j_grid-1) + (i_grid-1) has same parity.\n    I, J = np.meshgrid(np.arange(N), np.arange(N))\n    red_mask = (I + J) % 2 == 0\n    black_mask = ~red_mask\n    \n    k = 0\n    while True:\n        k += 1\n        \n        # Keep a copy of the interior from the start of the iteration\n        # for the (1-omega) term.\n        u_old_interior = u[1:N + 1, 1:N + 1].copy()\n\n        # Update red points. Neighbors are black, use values from start of iteration.\n        neighbors_sum = (u[1:N + 1, 2:N + 2] + u[1:N + 1, 0:N] +\n                         u[2:N + 2, 1:N + 1] + u[0:N, 1:N + 1])\n        gs_update = 0.25 * neighbors_sum\n        u[1:N + 1, 1:N + 1][red_mask] = (1 - omega) * u_old_interior[red_mask] + \\\n                                      omega * gs_update[red_mask]\n\n        # Update black points. Neighbors are red, use newly updated values.\n        neighbors_sum = (u[1:N + 1, 2:N + 2] + u[1:N + 1, 0:N] +\n                         u[2:N + 2, 1:N + 1] + u[0:N, 1:N + 1])\n        gs_update = 0.25 * neighbors_sum\n        u[1:N + 1, 1:N + 1][black_mask] = (1 - omega) * u_old_interior[black_mask] + \\\n                                        omega * gs_update[black_mask]\n        \n        r_norm = calculate_residual_norm(u, N)\n        if r_norm <= threshold:\n            return k\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (20, 1e-5, 1.5),\n        (20, 1e-5, 1.0),\n        (10, 1e-8, 1.8),\n        (40, 1e-4, 1.9),\n    ]\n\n    results = []\n    for N, tol, omega_sor in test_cases:\n        # Calculate iterations for Jacobi\n        n_J = solve_jacobi(N, tol)\n        \n        # Calculate iterations for Gauss-Seidel (SOR with omega=1.0)\n        n_GS = solve_sor(N, tol, 1.0)\n        \n        # Calculate iterations for SOR with the specified omega\n        n_SOR = solve_sor(N, tol, omega_sor)\n        \n        results.append([n_J, n_GS, n_SOR])\n\n    # Format the output string as specified: [[r1,r2,r3],[...],...]\n    formatted_results = [f'[{\",\".join(map(str, r))}]' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2406769"}]}