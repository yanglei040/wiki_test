{"hands_on_practices": [{"introduction": "这第一个实践是基础性的。我们将构建一个通用的龙格-库塔方法求解器，它可以通过布彻表进行配置。通过将此求解器应用于一个具有已知人造解的问题，并测试从简单的欧拉法到经典的四阶龙格-库塔法等多种标准方法，您将获得数值验证理论精度阶数的实践经验，这是计算科学中的一项核心技能。[@problem_id:2376768]", "problem": "编写一个完整的程序，该程序给定几个以 Butcher 表形式表示的显式 Runge–Kutta (RK) 方法，通过对一个常微分方程 (ODE) 初值问题的构造解进行测试，来数值验证每个方法的精度阶数。构造解定义如下：设精确解为 $y(t)=\\exp(\\sin t)$，其中 $t \\in [0,1]$，角度以弧度为单位。该 ODE 为 $y'(t)=f(t,y(t))$，其中 $f(t,y)=\\cos(t)\\,y$，初始条件为 $y(0)=1$，最终时间为 $T=1$。使用最终时间 $t=T$ 时的全局误差来量化精度。\n\n使用以下显式 Runge–Kutta 方法，每个方法均由其 Butcher 表 $(A,b,c)$ 指定：\n- 方法 1（显式欧拉法，期望阶数 1）：$s=1$, $A=\\begin{bmatrix}0\\end{bmatrix}$, $b=\\begin{bmatrix}1\\end{bmatrix}$, $c=\\begin{bmatrix}0\\end{bmatrix}$。\n- 方法 2（显式中点法，期望阶数 2）：$s=2$, $A=\\begin{bmatrix}0 & 0\\\\ \\tfrac{1}{2} & 0\\end{bmatrix}$, $b=\\begin{bmatrix}0 & 1\\end{bmatrix}$, $c=\\begin{bmatrix}0 & \\tfrac{1}{2}\\end{bmatrix}$。\n- 方法 3（Kutta 三阶方法，期望阶数 3）：$s=3$, $A=\\begin{bmatrix}0 & 0 & 0\\\\ \\tfrac{1}{2} & 0 & 0\\\\ -1 & 2 & 0\\end{bmatrix}$, $b=\\begin{bmatrix}\\tfrac{1}{6} & \\tfrac{2}{3} & \\tfrac{1}{6}\\end{bmatrix}$, $c=\\begin{bmatrix}0 & \\tfrac{1}{2} & 1\\end{bmatrix}$。\n- 方法 4（经典 RK4，期望阶数 4）：$s=4$, $A=\\begin{bmatrix}0 & 0 & 0 & 0\\\\ \\tfrac{1}{2} & 0 & 0 & 0\\\\ 0 & \\tfrac{1}{2} & 0 & 0\\\\ 0 & 0 & 1 & 0\\end{bmatrix}$, $b=\\begin{bmatrix}\\tfrac{1}{6} & \\tfrac{1}{3} & \\tfrac{1}{3} & \\tfrac{1}{6}\\end{bmatrix}$, $c=\\begin{bmatrix}0 & \\tfrac{1}{2} & \\tfrac{1}{2} & 1\\end{bmatrix}$。\n\n对每种方法，在 $[0,1]$ 上执行时间积分，使用大小为 $h=1/N$ 的均匀时间步长，其中 $N$ 来自测试集 $\\{10,20,40,80,160,320\\}$。对于每个 $N$，计算在 $t=1$ 时的数值近似值 $y_N$，计算全局误差 $E(h)=\\lvert y_N - y(1)\\rvert$，然后通过对测试集中所有 $N$ 值的数据点 $(\\log h,\\log E(h))$ 进行线性拟合，将所得直线的最小二乘斜率作为观测阶数 $p$ 的估计值。使用自然对数计算 $\\log$。\n\n您的程序必须在单行中输出一个用方括号括起来的逗号分隔列表，其中包含方法 1 到方法 4 的四个估计阶数 $(p_1,p_2,p_3,p_4)$，每个阶数都四舍五入到两位小数。不应打印任何其他文本。\n\n测试集和答案规范：\n- 测试集包含上述四种方法，每种方法均使用 $N \\in \\{10,20,40,80,160,320\\}$ 进行测试。\n- 最终答案是四个浮点数 $p_1, p_2, p_3, p_4$，每个数分别是相应方法的观测阶数估计值。\n- 最终输出格式必须严格为 $\\texttt{[p1,p2,p3,p4]}$ 的单行形式，其中每个 $p_k$ 都四舍五入到两位小数并以十进制数形式打印。", "solution": "问题陈述已经过严格分析。它具有科学依据，是适定的，并包含了获得唯一且有意义解所需的所有信息。指定的常微分方程、其构造的解析解、通过 Butcher 表定义的 Runge-Kutta 方法，以及用于数值验证精度阶数的程序，全都是标准的、正确的和自洽的。该问题是有效的。我们现在将构建解决方案。\n\n基本任务是求解以下形式的初值问题 (IVP)：\n$$ y'(t) = f(t, y(t)), \\quad y(t_0) = y_0 $$\n其中 $t \\in [t_0, T]$。问题给出了具体函数 $f(t, y) = \\cos(t) y$、初始条件 $y(0) = 1$ 和时间区间 $[0, 1]$。给出的精确解为 $y(t) = \\exp(\\sin t)$，这可以通过求导来轻松验证：$y'(t) = \\exp(\\sin t) \\cdot \\cos(t) = y(t)\\cos(t)$，并检查初始条件：$y(0) = \\exp(\\sin 0) = \\exp(0) = 1$。\n\n一个 $s$ 级的显式 Runge-Kutta (RK) 方法通过以步长 $h$ 向前推进时间来近似求解。从时间 $t_n$ 的解 $y_n$ 计算出时间 $t_{n+1} = t_n + h$ 的解 $y_{n+1}$。该方法由一组排列在 Butcher 表中的系数定义：\n$$\n\\begin{array}{c|c}\nc & A \\\\\n\\hline\n  & b^T\n\\end{array} \\quad \\text{其中 } c \\in \\mathbb{R}^s, b \\in \\mathbb{R}^s, A \\in \\mathbb{R}^{s \\times s}\n$$\n对于显式方法，矩阵 $A$ 是严格下三角矩阵，即当 $j \\ge i$ 时 $a_{ij} = 0$。计算分级进行。首先，计算 $s$ 个级导数 $k_i$，$i=1, 2, \\dots, s$：\n$$ k_i = f\\left(t_n + c_i h, y_n + h \\sum_{j=1}^{i-1} a_{ij} k_j\\right) $$\n然后，使用这些级导数的加权平均来推进解：\n$$ y_{n+1} = y_n + h \\sum_{i=1}^{s} b_i k_i $$\n\n数值方法的精度由其收敛阶 $p$ 来表征。对于一个 $p$ 阶方法，在固定的最终时间 $T$ 处的全局误差（表示为 $E(h)$）预计会随着步长 $h$ 的减小而遵循以下关系式：\n$$ E(h) = |y_N - y(T)| \\approx C h^p $$\n其中 $y_N$ 是在 $T=Nh$ 时的数值解，$C$ 是一个取决于方法和问题但与 $h$ 无关的常数。\n\n为了数值验证阶数 $p$，我们可以通过对两边取自然对数来转换这个关系：\n$$ \\ln(E(h)) \\approx \\ln(C) + p \\ln(h) $$\n这个方程的形式为 $Y = mX + B$，其中 $Y = \\ln(E(h))$，$X = \\ln(h)$，斜率为 $m = p$，截距为 $B = \\ln(C)$。这种线性关系意味着 $\\ln(E(h))$ 关于 $\\ln(h)$ 的图将近似于一条直线，其斜率即为方法的阶数 $p$。\n\n指定的程序如下：\n1. 对给定的四种 RK 方法中的每一种，都必须在区间 $[0, 1]$ 上进行一系列数值积分。\n2. 积分将使用一系列递减的步长 $h = 1/N$，其中 $N \\in \\{10, 20, 40, 80, 160, 320\\}$。\n3. 对于每个特定的步长 $h$ 的积分，计算最终时间的数值近似值 $y_N$。\n4. 计算全局误差 $E(h) = |y_N - y(1)|$，其中精确值为 $y(1) = \\exp(\\sin 1)$。\n5. 计算完所有步长的误差后，收集数据对 $(\\ln(h), \\ln(E(h)))$。\n6. 对这些数据点进行线性最小二乘回归。所得最佳拟合线的斜率提供了精度阶数 $p$ 的实验估计值。对于一组数据点 $(x_i, y_i)$，斜率 $p$ 由以下公式给出：\n$$ p = \\frac{\\sum_{i=1}^{M} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{M} (x_i - \\bar{x})^2} $$\n其中 $x_i = \\ln(h_i)$，$y_i = \\ln(E(h_i))$，$\\bar{x}$ 和 $\\bar{y}$ 是平均值，$M=6$ 是测试集中步长的数量。\n\n这个过程将为所提供的四个 Butcher 表中的每一个实现，从而产生四个估计的精度阶数 $(p_1, p_2, p_3, p_4)$，预计它们将分别接近其理论值 $1, 2, 3$ 和 $4$。最终结果将是这四个值，四舍五入到两位小数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Numerically verifies the order of accuracy of several explicit Runge-Kutta methods.\n    \"\"\"\n    # Define the Ordinary Differential Equation and its analytical solution\n    f = lambda t, y: np.cos(t) * y\n    t_start = 0.0\n    y_start = 1.0\n    t_end = 1.0\n    \n    # Pre-calculate the exact solution at the final time for error computation\n    y_exact_final = np.exp(np.sin(t_end))\n\n    # Define the Butcher tableaus for the four RK methods\n    methods = [\n        {\n            # Method 1: Explicit Euler (Order 1)\n            'A': np.array([[0.0]]),\n            'b': np.array([1.0]),\n            'c': np.array([0.0])\n        },\n        {\n            # Method 2: Explicit Midpoint (Order 2)\n            'A': np.array([[0.0, 0.0], [0.5, 0.0]]),\n            'b': np.array([0.0, 1.0]),\n            'c': np.array([0.0, 0.5])\n        },\n        {\n            # Method 3: Kutta's third-order method (Order 3)\n            'A': np.array([[0.0, 0.0, 0.0], [0.5, 0.0, 0.0], [-1.0, 2.0, 0.0]]),\n            'b': np.array([1.0/6.0, 2.0/3.0, 1.0/6.0]),\n            'c': np.array([0.0, 0.5, 1.0])\n        },\n        {\n            # Method 4: Classical RK4 (Order 4)\n            'A': np.array([\n                [0.0, 0.0, 0.0, 0.0],\n                [0.5, 0.0, 0.0, 0.0],\n                [0.0, 0.5, 0.0, 0.0],\n                [0.0, 0.0, 1.0, 0.0]\n            ]),\n            'b': np.array([1.0/6.0, 1.0/3.0, 1.0/3.0, 1.0/6.0]),\n            'c': np.array([0.0, 0.5, 0.5, 1.0])\n        }\n    ]\n\n    # Test suite of step counts\n    N_values = [10, 20, 40, 80, 160, 320]\n    h_values = np.array([1.0 / N for N in N_values])\n\n    estimated_orders = []\n\n    for method in methods:\n        A, b, c = method['A'], method['b'], method['c']\n        s = len(b)  # Number of stages\n        errors = []\n\n        for N in N_values:\n            h = (t_end - t_start) / N\n            y_current = y_start\n            \n            # Time integration loop\n            for n in range(N):\n                t_n = t_start + n * h\n                k_stages = np.zeros(s)\n                \n                # Calculate stage derivatives k_i\n                for i in range(s):\n                    stage_sum = 0.0\n                    for j in range(i):\n                        stage_sum += A[i, j] * k_stages[j]\n                    \n                    y_stage_input = y_current + h * stage_sum\n                    t_stage_input = t_n + c[i] * h\n                    k_stages[i] = f(t_stage_input, y_stage_input)\n                \n                # Update solution\n                y_current += h * np.dot(b, k_stages)\n            \n            # Store the global error at t=T\n            errors.append(np.abs(y_current - y_exact_final))\n\n        # Use natural logarithm for the log-log plot\n        log_h = np.log(h_values)\n        log_E = np.log(np.array(errors))\n        \n        # Perform linear regression (polynomial fit of degree 1)\n        # The slope of the line is the estimated order of accuracy\n        # np.polyfit returns [slope, intercept]\n        slope = np.polyfit(log_h, log_E, 1)[0]\n        estimated_orders.append(slope)\n        \n    # Format the output as specified: [p1,p2,p3,p4]\n    formatted_results = [f'{p:.2f}' for p in estimated_orders]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2376768"}, {"introduction": "尽管高阶方法承诺更快的收敛速度，但它们的实际应用是在截断误差和舍入误差之间寻求平衡。本练习将超越理论阶数，探讨这一关键的权衡。通过使用不同的浮点运算精度，实现并比较欧拉法和四阶龙格-库塔法，您将亲眼观察到在极小的步长下，舍入误差如何占据主导地位，从而揭示了总误差最小时的“最优”步长。[@problem_id:2447459]", "problem": "您将为常微分方程 $y^{\\prime}(t)=-y(t)$（初始条件为 $y(0)=1$，区间为 $t\\in[0,1]$）所定义的初值问题，实现并分析两种单步数值积分方法。其精确解为 $y(t)=e^{-t}$。您的任务是通过控制步长和浮点格式，比较截断误差的阶，并揭示截断误差与舍入误差之间的相互作用。请从基本定义出发：局部截断误差是当方法应用于精确解时，单步所产生的误差；全局误差是数值解与精确解在最终时刻的偏差。仅使用显式欧拉法和经典四阶龙格-库塔法的定义；不要假设任何预先推导出的误差阶公式。您应通过泰勒展开和单步法的一致性来推断误差阶的特征，然后通过计算来验证它们。\n\n实现以下组件：\n- 一个使用显式欧拉法推进单步的函数，以及另一个使用经典四阶龙格-库塔法推进单步的函数。每个积分器必须在指定的浮点格式下运行：binary$64$（双精度）和 binary$32$（单精度）。\n- 一个驱动程序，对于给定的步长 $h$（使得 $1/h$ 为整数），从 $t=0$ 开始应用所选方法到 $t=1$，初始值为 $y(0)=1$，并返回在 $t=1$ 时的绝对全局误差，即 $\\lvert y_{N}-e^{-1}\\rvert$，其中 $N=1/h$。\n- 一个函数，通过对给定步长列表的 $\\log(\\text{error})$ 与 $\\log(h)$ 进行最小二乘法直线拟合，来估计观测到的精度阶。\n\n设计您的实现，使得数值更新、常数和中间量都在所要求的浮点格式下进行计算。在计算误差时，使用 binary$64$ 格式的精确解 $y(1)=e^{-1}$ 作为参考值。\n\n测试套件：\n请按此确切顺序，使用指定的步长和格式，计算以下六个结果：\n1. 使用显式欧拉法，步长 $h=0.1$，在 binary$64$ 格式下，$t=1$ 时的绝对全局误差。\n2. 使用经典四阶龙格-库塔法，步长 $h=0.1$，在 binary$64$ 格式下，$t=1$ 时的绝对全局误差。\n3. 使用显式欧拉法，在 binary$64$ 格式下，根据步长 $h\\in\\{0.2,0.1,0.05,0.025\\}$ 估计的观测精度阶。\n4. 使用经典四阶龙格-库塔法，在 binary$64$ 格式下，根据步长 $h\\in\\{0.2,0.1,0.05,0.025\\}$ 估计的观测精度阶。\n5. 使用经典四阶龙格-库塔法，在 binary$32$ 格式下，计算步长 $h\\in\\{2^{-3},2^{-5},2^{-7},2^{-9},2^{-11},2^{-13}\\}$ 在 $t=1$ 时的绝对全局误差。返回列表中该误差达到最小值时的从零开始的索引 $k^{\\ast}$。此索引应反映截断误差与舍入误差相平衡时的步长。\n6. 使用经典四阶龙格-库塔法，在 binary$64$ 格式下，计算步长 $h\\in\\{2^{-6},2^{-7},2^{-8},2^{-9}\\}$ 在 $t=1$ 时的绝对全局误差。返回一个布尔值，指示这些误差是否随着 $h$ 的减小而严格递减。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个逗号分隔的列表，列表被方括号括起，内含六个结果，顺序与上述完全一致。该列表必须包含前两个误差的两个浮点数、两个观测阶的两个浮点数、一个索引的整数，以及一个单调性检查的布尔值，总共六个条目在一个列表中。", "solution": "所给出的问题是常微分方程数值分析中的一个标准练习。它在科学上是合理的、适定的，并包含了完整求解所需的所有信息。我们将着手进行分析和实现。\n\n所考虑的初值问题 (IVP) 由以下常微分方程 (ODE) 给出：\n$$\ny^{\\prime}(t) = -y(t), \\quad t \\in [0, 1]\n$$\n初始条件为 $y(0) = 1$。该 IVP 的解析解为 $y(t) = e^{-t}$。我们的任务是使用两种不同的单步法对该问题进行数值求解，并分析它们的误差特性。\n\n对于 IVP $y^{\\prime}(t) = f(t, y(t))$，一个通用的单步法在离散时间点 $t_n = t_0 + nh$（其中 $h$ 为步长）上近似求解。数值解 $y_n \\approx y(t_n)$ 通过以下形式的公式从一步推进到下一步：\n$$\ny_{n+1} = y_n + h \\Phi(t_n, y_n, h)\n$$\n其中 $\\Phi$ 是定义该方法的增量函数。\n\n第一种方法是显式欧拉法。它由 $y(t_{n+1})$ 在 $t_n$ 处的一阶泰勒展开推导而来：\n$$\ny(t_{n+1}) = y(t_n) + h y^{\\prime}(t_n) + \\mathcal{O}(h^2)\n$$\n通过代入 $y^{\\prime}(t_n) = f(t_n, y(t_n))$ 并截去 $\\mathcal{O}(h^2)$及更高阶的项，我们得到该方法：\n$$\ny_{n+1} = y_n + h f(t_n, y_n)\n$$\n对于给定的问题，$f(t, y) = -y$，所以显式欧拉法的更新规则是：\n$$\ny_{n+1} = y_n - h y_n = (1 - h) y_n\n$$\n\n第二种方法是经典四阶龙格-库塔法 (RK4)。它由以下方程组定义：\n$$\ny_{n+1} = y_n + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)\n$$\n其中各阶段 $k_i$ 计算如下：\n$$\n\\begin{aligned}\nk_1 &= f(t_n, y_n) \\\\\nk_2 &= f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_1\\right) \\\\\nk_3 &= f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_2\\right) \\\\\nk_4 &= f(t_n + h, y_n + h k_3)\n\\end{aligned}\n$$\n对于我们的问题，其中 $f(t, y) = -y$，这些阶段变为：\n$$\n\\begin{aligned}\nk_1 &= -y_n \\\\\nk_2 &= -\\left(y_n + \\frac{h}{2}(-y_n)\\right) = -y_n\\left(1 - \\frac{h}{2}\\right) \\\\\nk_3 &= -\\left(y_n + \\frac{h}{2}k_2\\right) = -y_n\\left(1 - \\frac{h}{2} + \\frac{h^2}{4}\\right) \\\\\nk_4 &= -(y_n + hk_3) = -y_n\\left(1 - h + \\frac{h^2}{2} - \\frac{h^3}{4}\\right)\n\\end{aligned}\n$$\n将这些代入 RK4 公式并化简得到：\n$$\ny_{n+1} = y_n\\left(1 - h + \\frac{h^2}{2} - \\frac{h^3}{6} + \\frac{h^4}{24}\\right)\n$$\n此表达式恰好是 $y_n e^{-h}$ 泰勒级数展开的前五项，因为 $e^{-h} = 1 - h + \\frac{h^2}{2!} - \\frac{h^3}{3!} + \\frac{h^4}{4!} - \\frac{h^5}{5!} + \\dots$。\n\n数值方法的精度由其阶来表征。局部截断误差 (LTE) 是在单步中引入的误差，假设该方法从精确解 $y(t_n)$ 开始。对于一个 $p$ 阶方法，LTE 为 $\\mathcal{O}(h^{p+1})$。全局误差，即在积分区间末端的累积误差，则为 $\\mathcal{O}(h^p)$。\n\n对于显式欧拉法，其 LTE 为：\n$$\nLTE_{n+1} = y(t_{n+1}) - \\left[y(t_n) + h f(t_n, y(t_n))\\right] = \\left[y(t_n) + h y^{\\prime}(t_n) + \\frac{h^2}{2}y^{\\prime\\prime}(\\xi)\\right] - \\left[y(t_n) + h y^{\\prime}(t_n)\\right] = \\frac{h^2}{2}y^{\\prime\\prime}(\\xi)\n$$\n对于某个 $\\xi \\in (t_n, t_{n+1})$。LTE 为 $\\mathcal{O}(h^2)$，这意味着欧拉法是一阶精度的，因此其全局误差行为如 $\\mathcal{O}(h^1)$。\n\n对于 RK4 方法，针对我们特定问题的更新规则与 $e^{-h}$ 的泰勒级数在 $h^4$ 项之前都相匹配。因此，单步误差由级数中的下一项主导：\n$$\nLTE_{n+1} = y(t_n)e^{-h} - y(t_n)\\left(1 - h + \\frac{h^2}{2} - \\frac{h^3}{6} + \\frac{h^4}{24}\\right) = y(t_n)\\left(-\\frac{h^5}{120} + \\mathcal{O}(h^6)\\right)\n$$\nLTE 为 $\\mathcal{O}(h^5)$，所以 RK4 方法是四阶精度的，其全局误差行为如 $\\mathcal{O}(h^4)$。\n\n为了通过计算验证阶数 $p$，我们假设最终时刻的全局误差 $E$ 遵循 $E \\approx C h^p$，其中 $C$ 为某个常数。对两边取对数得到：\n$$\n\\log(E) \\approx \\log(C) + p \\log(h)\n$$\n这表明 $\\log(E)$ 和 $\\log(h)$ 之间存在线性关系，其斜率即为精度阶 $p$。我们可以通过计算一系列步长对应的误差，并对对数-对数数据进行线性最小二乘拟合来估计 $p$。\n\n最后，我们必须考虑有限精度算术的影响。总的数值误差是截断误差（源于方法的近似）和舍入误差（源于浮点表示）之和。截断误差随着 $h$ 的减小而减小（例如，$\\propto h^p$）。然而，舍入误差往往会随着步数 $N = T/h$ 的增加而累积。累积的舍入误差可以建模为与 $\\epsilon/h$ 成正比，其中 $\\epsilon$ 是浮点格式的机器精度。总误差近似为：\n$$\nE_{total} \\approx C h^p + \\frac{D\\epsilon}{h}\n$$\n此函数对于某个最优步长 $h^*$ 存在一个最小值。当 $h > h^*$ 时，截断误差占主导地位，误差随 $h$ 减小而减小。当 $h < h^*$ 时，舍入误差占主导地位，误差随 $h$ 减小而增大。这种效应在单精度（binary$32$，$\\epsilon \\approx 10^{-8}$）中比在双精度（binary$64$，$\\epsilon \\approx 10^{-16}$）中更明显，因为较大的 $\\epsilon$ 会导致较大的最优 $h^*$。这正是测试用例旨在研究的内容。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef euler_step(y, h, dtype):\n    \"\"\"\n    Performs one step of the explicit Euler method for y' = -y.\n    All calculations are performed in the specified dtype.\n    \n    Args:\n        y: Current value of the solution (of type dtype).\n        h: Step size (Python float).\n        dtype: The numpy floating-point type (np.float32 or np.float64).\n        \n    Returns:\n        The next value of the solution (of type dtype).\n    \"\"\"\n    h_typed = dtype(h)\n    one = dtype(1.0)\n    # The update is y_{n+1} = y_n * (1 - h)\n    return y * (one - h_typed)\n\ndef rk4_step(y, h, dtype):\n    \"\"\"\n    Performs one step of the classical 4th-order Runge-Kutta method for y' = -y.\n    All calculations are performed in the specified dtype.\n    \n    Args:\n        y: Current value of the solution (of type dtype).\n        h: Step size (Python float).\n        dtype: The numpy floating-point type (np.float32 or np.float64).\n        \n    Returns:\n        The next value of the solution (of type dtype).\n    \"\"\"\n    h_typed = dtype(h)\n    \n    # Define constants in the target precision\n    c_half = dtype(0.5)\n    c_two = dtype(2.0)\n    c_six = dtype(6.0)\n\n    # ODE is y' = f(y) = -y\n    k1 = -y\n    k2 = -(y + c_half * h_typed * k1)\n    k3 = -(y + c_half * h_typed * k2)\n    k4 = -(y + h_typed * k3)\n    \n    return y + (h_typed / c_six) * (k1 + c_two * k2 + c_two * k3 + k4)\n\ndef solve_ode(step_func, h, dtype):\n    \"\"\"\n    Solves the ODE y'=-y from t=0 to t=1 with y(0)=1.\n    \n    Args:\n        step_func: The function for a single integration step (e.g., euler_step).\n        h: The step size.\n        dtype: The numpy floating-point type for computation.\n        \n    Returns:\n        The absolute global error at t=1.\n    \"\"\"\n    # The problem statement guarantees 1/h is an integer.\n    # Use round() to be robust against floating point inaccuracies, e.g., 1.0/0.1\n    num_steps = int(round(1.0 / h))\n    \n    # Initial condition y(0)=1, cast to the specified precision\n    y = dtype(1.0)\n\n    for _ in range(num_steps):\n        y = step_func(y, h, dtype)\n\n    # Reference solution y(1)=e^-1 in double precision (binary64)\n    y_exact_64 = np.exp(np.float64(-1.0))\n    \n    # Error is computed against the high-precision reference.\n    # The subtraction will promote the result to float64, which is desired.\n    return np.abs(y - y_exact_64)\n\ndef estimate_order(step_func, h_list, dtype):\n    \"\"\"\n    Estimates the order of accuracy of a method by log-log linear regression.\n    \n    Args:\n        step_func: The single-step integration function.\n        h_list: A list of step sizes to use.\n        dtype: The numpy floating-point type.\n        \n    Returns:\n        The estimated order of accuracy (slope of the log-log plot).\n    \"\"\"\n    h_array = np.array(h_list, dtype=np.float64)\n    errors = np.array([solve_ode(step_func, h, dtype) for h in h_list], dtype=np.float64)\n\n    log_h = np.log(h_array)\n    log_err = np.log(errors)\n\n    # Perform linear least squares to find the slope p\n    # of the line log(error) = log(C) + p * log(h)\n    # Using the standard formula for the slope of a simple linear regression.\n    n = len(log_h)\n    sum_xy = np.sum(log_h * log_err)\n    sum_x = np.sum(log_h)\n    sum_y = np.sum(log_err)\n    sum_x2 = np.sum(log_h**2)\n    \n    slope_p = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x**2)\n    return slope_p\n\ndef solve():\n    \"\"\"\n    Main function to compute the six required results for the test suite.\n    \"\"\"\n    # Test 1: Global error for Euler, h=0.1, binary64\n    error1 = solve_ode(euler_step, 0.1, np.float64)\n\n    # Test 2: Global error for RK4, h=0.1, binary64\n    error2 = solve_ode(rk4_step, 0.1, np.float64)\n\n    # Test 3: Observed order for Euler, binary64\n    h_order_list = [0.2, 0.1, 0.05, 0.025]\n    order3 = estimate_order(euler_step, h_order_list, np.float64)\n\n    # Test 4: Observed order for RK4, binary64\n    order4 = estimate_order(rk4_step, h_order_list, np.float64)\n    \n    # Test 5: Index of minimum error for RK4, binary32 (truncation vs. round-off)\n    h_balance_list = [2.0**-k for k in [3, 5, 7, 9, 11, 13]]\n    errors_balance = [solve_ode(rk4_step, h, np.float32) for h in h_balance_list]\n    index5 = np.argmin(errors_balance)\n    \n    # Test 6: Monotonicity check for RK4, binary64\n    h_mono_list = [2.0**-k for k in [6, 7, 8, 9]]\n    errors_mono = [solve_ode(rk4_step, h, np.float64) for h in h_mono_list]\n    # Check if errors are strictly decreasing: e[i] > e[i+1] for all i\n    bool6 = all(np.diff(errors_mono)  0)\n\n    results = [error1, error2, order3, order4, index5, bool6]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2447459"}, {"introduction": "保证方法精度阶数的收敛性定理依赖于对解的光滑性的某些假设。本实践旨在探究当这些假设被违背时会发生什么。您将把一个高阶龙格-库塔方法应用于一个其解具有有限时间奇异性的常微分方程，并研究当积分区间趋近该奇点时，观测到的精度阶数是如何退化的。[@problem_id:2422962] 这个练习揭示了一个重要的事实：误差公式 $E \\approx C h^p$ 中的“常数”$C$ 可能极大地依赖于解的导数，从而影响实际的收敛行为。", "problem": "考虑一个常微分方程 (ODE) 的初值问题：求一个函数 $y(t)$，使得 $y'(t) = y(t)^2$ 且 $y(0) = 1$。其精确解为 $y(t) = \\dfrac{1}{1 - t}$（对于 $t  1$），该解在 $t = 1$ 处有一个有限时间奇点。\n\n设 $y(t)$ 的数值近似解由经典的显式四阶龙格-库塔方法（常简写为 RK4）计算得出。该方法以均匀时间步长 $h$ 应用于区间 $[0,T]$，从 $y(0)=1$ 开始，在时间 $T$ 产生最终数值 $Y_h(T)$。当精确解及其导数保持有界时，此方法的理论全局精度阶为 $4$。\n\n定义在最终时间 $T \\in (0,1)$ 的观测精度阶如下。对于三个步长 $h_i = T/N_i$（其中 $N_i \\in \\{N_0, 2N_0, 4N_0\\}$），计算相应的全局绝对误差\n$$\nE(h_i;T) = \\left|Y_{h_i}(T) - y(T)\\right|.\n$$\n那么，观测阶 $p_{\\mathrm{obs}}(T)$ 是点集 $\\left(\\log h_i, \\log E(h_i;T)\\right)$（其中 $i=1,2,3$）在最小二乘意义下的最佳仿射拟合的斜率。\n\n你的任务是编写一个完整的程序，对下面指定的每个测试用例，根据上述定义计算 $p_{\\mathrm{obs}}(T)$。必须使用经典的显式四阶龙格-库塔方法计算 $Y_{h}(T)$，并且必须使用精确解 $y(T) = \\dfrac{1}{1-T}$ 来评估 $E(h;T)$。\n\n测试套件：\n- 用例 1（理想情况）：$T = 0.5$, $N_0 = 16$。\n- 用例 2（更接近奇点）：$T = 0.9$, $N_0 = 64$。\n- 用例 3（接近奇异）：$T = 0.99$, $N_0 = 512$。\n- 用例 4（非常接近奇异）：$T = 0.999$, $N_0 = 4096$。\n\n所有计算都是无量纲的。不涉及角度。无需百分比。\n\n最终输出格式：\n你的程序应该生成单行输出，其中包含上述测试用例的四个观测阶 $p_{\\mathrm{obs}}(T)$，顺序与列表一致，四舍五入到三位小数，并以逗号分隔列表的形式用方括号括起来。例如，一个有效的输出看起来像这样：$[4.000,3.987,3.452,2.718]$（这只是一个示例；你的程序必须计算实际值）。", "solution": "我们分析初值问题 $y'(t) = y(t)^2$ 及 $y(0) = 1$。精确解可通过分离变量法得到：$\\dfrac{dy}{dt} = y^2$ 意味着 $\\dfrac{dy}{y^2} = dt$，积分得到 $-1/y = t + C$。利用初始条件 $y(0)=1$ 可得 $C = -1$。因此，对于 $t  1$，解为 $y(t) = \\dfrac{1}{1 - t}$，并且在 $t=1$ 处存在一个有限时间爆破（奇点）。\n\n对于数值近似，我们使用经典的显式四阶龙格-库塔方法（RK4）。对于给定的步长 $h$ 和当前状态 $(t_n, y_n)$，定义阶段斜率 $k_1, k_2, k_3, k_4$ 如下：\n$$\nk_1 = f(t_n, y_n), \\quad\nk_2 = f\\!\\left(t_n + \\tfrac{h}{2}, y_n + \\tfrac{h}{2} k_1\\right), \\quad\nk_3 = f\\!\\left(t_n + \\tfrac{h}{2}, y_n + \\tfrac{h}{2} k_2\\right), \\quad\nk_4 = f\\!\\left(t_n + h, y_n + h k_3\\right),\n$$\n其中 $f(t,y) = y^2$。更新公式为：\n$$\ny_{n+1} = y_n + \\frac{h}{6}\\left(k_1 + 2k_2 + 2k_3 + k_4 \\right), \\quad t_{n+1} = t_n + h.\n$$\n从 $t_0 = 0$ 和 $y_0 = 1$ 开始，推进 $N = T/h$ 个均匀步长，得到 $Y_h(T) = y_N$。\n\n为了量化在固定最终时间 $T$ 的观测精度阶，我们依赖于定义 $E(h;T) = \\left|Y_h(T) - y(T)\\right|$，其中 $y(T) = \\dfrac{1}{1 - T}$。对于一个理论阶为 $p_{\\mathrm{th}}$ 的方法，当解足够光滑时，其渐近行为为 $E(h;T) \\approx C(T)\\, h^{p_{\\mathrm{th}}}$（当 $h \\to 0$ 时），其中常数 $C(T)$ 取决于精确解在 $[0,T]$ 上导数的界。经典的 RK4 方法具有 $p_{\\mathrm{th}} = 4$。然而，在奇点附近，$y(t)$ 的导数会迅速增长：通过直接微分或归纳法可以发现\n$$\ny^{(m)}(t) = \\frac{m!}{\\left(1 - t\\right)^{m+1}},\n$$\n因此，对于接近 1 的固定 $T$ 值，局部截断误差中的系数会随着 $(1-T)^{-1}$ 的幂次增加。RK4 的局部截断误差为 $\\mathcal{O}(h^5)$，其系数涉及 $y^{(5)}(t)$ 及更低阶的导数；因此，全局误差中的常数 $C(T)$ 的行为类似于 $(1-T)^{-5}$ 的一个正数倍（不计动力学系统带来的乘法因子）。因此，当 $T \\to 1^-$ 时，即使是中等小的 $h$ 也可能尚未进入渐近区域，在该区域的对数-对数图上可以观察到清晰的斜率 4。这种现象表现为，在使用一组固定的步长进行测量时，数值计算出的精度阶会发生劣化。\n\n为了直接根据定义计算观测阶，我们取三个步长 $h_i = T/N_i$（其中 $N_i \\in \\{N_0, 2N_0, 4N_0\\}$），计算相应的误差 $E(h_i;T)$，然后将 $p_{\\mathrm{obs}}(T)$ 定义为点集 $(\\log h_i, \\log E(h_i;T))$ 的最小二乘仿射拟合的斜率。具体来说，令 $x_i = \\log h_i$ 和 $y_i = \\log E(h_i;T)$（对于 $i=1,2,3$），则斜率 $p_{\\mathrm{obs}}(T)$ 为\n$$\np_{\\mathrm{obs}}(T) = \\frac{\\sum_{i=1}^3 (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^3 (x_i - \\bar{x})^2}, \\quad\n\\bar{x} = \\frac{1}{3}\\sum_{i=1}^3 x_i, \\quad \\bar{y} = \\frac{1}{3}\\sum_{i=1}^3 y_i.\n$$\n该斜率的计算与所用对数的底无关。\n\n测试套件指定了四对 $(T, N_0)$：$(0.5, 16)$、$(0.9, 64)$、$(0.99, 512)$ 和 $(0.999, 4096)$。对于每个 $T$，三个网格分别使用 $N \\in \\{N_0, 2N_0, 4N_0\\}$ 个步数，对应的步长为 $h = T/N$。这些选择确保了对 $[0,T]$ 的均匀划分，同时探测了逐渐增高的分辨率。通过 RK4 方法进行的数值积分得到 $Y_h(T)$，而精确解 $y(T) = 1/(1 - T)$ 为误差计算提供了真值。\n\n我们预期会有以下定性行为：\n- 对于 $T = 0.5$，解及其导数在 $[0, 0.5]$ 上的幅值不大，因此 $C(T)$ 是适度的，观测阶 $p_{\\mathrm{obs}}(0.5)$ 应该接近 4。\n- 随着 $T$ 增加到 $0.9$、$0.99$ 和 $0.999$，高阶导数在 $[0,T]$ 上的幅值变得非常大，这增大了 $C(T)$，并使得这组有限的步长对于渐近区域的代表性降低。因此，测得的斜率 $p_{\\mathrm{obs}}(T)$ 通常会降低到 4 以下，其中在 $T = 0.999$ 附近降低最为显著。\n\n程序实现了以下步骤：对每个网格使用 RK4 方法进行数值推进，使用精确解评估绝对误差，为 $(\\log h, \\log E)$ 计算最小二乘斜率，并以指定的单行列表格式输出四个观测阶（四舍五入到三位小数）。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rk4_final_value(f, y0, T, N):\n    \"\"\"\n    Advance the ODE y' = f(t,y) from t=0 to t=T using N uniform RK4 steps.\n    Returns the final value y_N at t = T.\n    \"\"\"\n    h = T / N\n    t = 0.0\n    y = y0\n    for _ in range(N):\n        k1 = f(t, y)\n        k2 = f(t + 0.5 * h, y + 0.5 * h * k1)\n        k3 = f(t + 0.5 * h, y + 0.5 * h * k2)\n        k4 = f(t + h, y + h * k3)\n        y = y + (h / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\n        t += h\n    return y\n\ndef observed_order_at_T(T, N0):\n    \"\"\"\n    Compute the observed order p_obs(T) using three meshes with N in {N0, 2N0, 4N0}.\n    \"\"\"\n    # Define the ODE\n    def f(t, y):\n        return y * y\n\n    exact = 1.0 / (1.0 - T)\n\n    Ns = [N0, 2 * N0, 4 * N0]\n    hs = []\n    errs = []\n    for N in Ns:\n        h = T / N\n        yN = rk4_final_value(f, 1.0, T, N)\n        err = abs(yN - exact)\n        hs.append(h)\n        errs.append(err)\n\n    hs = np.array(hs, dtype=float)\n    errs = np.array(errs, dtype=float)\n\n    # Guard against log(0) by adding a tiny floor (does not affect realistic errors)\n    tiny = 1e-300\n    x = np.log(hs)\n    y = np.log(errs + tiny)\n\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    denom = np.sum((x - x_mean) ** 2)\n    # In the degenerate case (should not happen for distinct hs), fall back to 0\n    if denom == 0.0:\n        return 0.0\n    slope = np.sum((x - x_mean) * (y - y_mean)) / denom\n    # The slope of log(error) vs log(h) is the observed order\n    return slope\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (T, N0)\n    test_cases = [\n        (0.5, 16),     # happy path\n        (0.9, 64),     # closer to singularity\n        (0.99, 512),   # near-singular\n        (0.999, 4096), # very near-singular\n    ]\n\n    results = []\n    for T, N0 in test_cases:\n        p_obs = observed_order_at_T(T, N0)\n        # Round to three decimal places for output formatting\n        results.append(f\"{p_obs:.3f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2422962"}]}