{"hands_on_practices": [{"introduction": "本节的第一个实践是基础性的。我们将为一个简单的测试问题实现并比较一阶欧拉方法和四阶龙格-库塔方法，这将使我们能够直接观察到一个方法的精度阶数如何影响其全局截断误差，并通过计算验证理论预测。我们还将探讨在极小步长下，浮点精度（舍入误差）如何与截断误差相互作用[@problem_id:2447459]。", "problem": "您将为常微分方程 $y^{\\prime}(t)=-y(t)$（初始条件为 $y(0)=1$，区间为 $t\\in[0,1]$）所定义的初值问题，实现并分析两种单步数值积分方法。其精确解为 $y(t)=e^{-t}$。您的任务是通过控制步长和浮点数格式，比较截断误差的阶，并揭示截断误差与舍入误差之间的相互作用。请从基本定义出发：局部截断误差是当方法应用于精确解时，单步所产生的误差；全局误差是在最终时刻，数值解与精确解之间的偏差。仅使用显式欧拉法和经典四阶龙格-库塔法的定义；不要假定任何预先推导出的误差阶公式。您应当从泰勒展开和单步法的一致性出发来确定误差的阶，然后通过计算进行验证。\n\n实现以下组件：\n- 一个使用显式欧拉法前进一步的函数，以及另一个使用经典四阶龙格-库塔法前进一步的函数。每个积分器必须在指定的浮点数格式下运行：binary64（双精度）和 binary32（单精度）。\n- 一个驱动程序，对于给定的步长 $h$（其中 $1/h$ 为整数），从 $t=0$ 和 $y(0)=1$ 开始，应用所选方法直到 $t=1$，并返回在 $t=1$ 时的绝对全局误差，即 $\\lvert y_{N}-e^{-1}\\rvert$，其中 $N=1/h$。\n- 一个函数，通过对给定步长列表的 $\\log(\\text{error})$ 与 $\\log(h)$ 的关系使用最小二乘法进行直线拟合，来估计观测到的精度阶。\n\n设计您的实现时，应确保数值更新、常量和中间量都在要求的浮点数格式下进行计算。在计算误差时，使用 binary64 格式的精确解 $y(1)=e^{-1}$ 作为参考值。\n\n测试套件：\n请按此确切顺序，使用指定的步长和格式，计算以下六个结果：\n1. 使用显式欧拉法，步长 $h=0.1$，在 binary64 格式下计算 $t=1$ 时的绝对全局误差。\n2. 使用经典四阶龙格-库塔法，步长 $h=0.1$，在 binary64 格式下计算 $t=1$ 时的绝对全局误差。\n3. 对于显式欧拉法，在 binary64 格式下，根据步长 $h\\in\\{0.2,0.1,0.05,0.025\\}$ 估计其观测到的精度阶。\n4. 对于经典四阶龙格-库塔法，在 binary64 格式下，根据步长 $h\\in\\{0.2,0.1,0.05,0.025\\}$ 估计其观测到的精度阶。\n5. 使用经典四阶龙格-库塔法，在 binary32 格式下，计算步长 $h\\in\\{2^{-3},2^{-5},2^{-7},2^{-9},2^{-11},2^{-13}\\}$ 在 $t=1$ 时的绝对全局误差。返回列表中误差达到最小值处的从零开始的索引 $k^{\\ast}$。该索引应反映截断误差与舍入误差相平衡时的步长。\n6. 使用经典四阶龙格-库塔法，在 binary64 格式下，计算步长 $h\\in\\{2^{-6},2^{-7},2^{-8},2^{-9}\\}$ 在 $t=1$ 时的绝对全局误差。返回一个布尔值，指示这些误差是否随着 $h$ 的减小而严格递减。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表内含六个结果，并严格遵循上述顺序。该列表必须包含前两个误差（两个浮点数）、两个观测到的阶（两个浮点数）、一个索引（一个整数）和一个用于单调性检查的布尔值，总共一个列表中的六个条目。", "solution": "所给出的问题是常微分方程数值分析中的一个标准练习。它在科学上是合理的、适定的，并包含了完整求解所需的全部信息。我们将着手进行分析和实现。\n\n我们所考虑的初值问题(IVP)由以下常微分方程(ODE)给出：\n$$\ny^{\\prime}(t) = -y(t), \\quad t \\in [0, 1]\n$$\n初始条件为 $y(0) = 1$。该初值问题的解析解为 $y(t) = e^{-t}$。我们的任务是使用两种不同的单步法对此问题进行数值求解，并分析它们的误差特性。\n\n对于初值问题 $y^{\\prime}(t) = f(t, y(t))$，一个通用的单步法在离散时间点 $t_n = t_0 + nh$（其中 $h$ 是步长）上逼近其解。数值解 $y_n \\approx y(t_n)$ 通过以下形式的公式从一步前进到下一步：\n$$\ny_{n+1} = y_n + h \\Phi(t_n, y_n, h)\n$$\n其中 $\\Phi$ 是定义该方法的增量函数。\n\n第一种方法是显式欧拉法。它源自于 $y(t_{n+1})$ 在 $t_n$ 点的一阶泰勒展开：\n$$\ny(t_{n+1}) = y(t_n) + h y^{\\prime}(t_n) + \\mathcal{O}(h^2)\n$$\n通过代入 $y^{\\prime}(t_n) = f(t_n, y(t_n))$ 并截断 $\\mathcal{O}(h^2)$ 及更高阶的项，我们得到该方法：\n$$\ny_{n+1} = y_n + h f(t_n, y_n)\n$$\n对于给定的问题，$f(t, y) = -y$，因此显式欧拉法的更新规则是：\n$$\ny_{n+1} = y_n - h y_n = (1 - h) y_n\n$$\n\n第二种方法是经典四阶龙格-库塔法(RK4)。它由以下方程组定义：\n$$\ny_{n+1} = y_n + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)\n$$\n其中各阶段值 $k_i$ 计算如下：\n$$\n\\begin{aligned}\nk_1 &= f(t_n, y_n) \\\\\nk_2 &= f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_1\\right) \\\\\nk_3 &= f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_2\\right) \\\\\nk_4 &= f(t_n + h, y_n + h k_3)\n\\end{aligned}\n$$\n对于我们的问题，其中 $f(t, y) = -y$，这些阶段值变为：\n$$\n\\begin{aligned}\nk_1 &= -y_n \\\\\nk_2 &= -\\left(y_n + \\frac{h}{2}(-y_n)\\right) = -y_n\\left(1 - \\frac{h}{2}\\right) \\\\\nk_3 &= -\\left(y_n + \\frac{h}{2}k_2\\right) = -y_n\\left(1 - \\frac{h}{2} + \\frac{h^2}{4}\\right) \\\\\nk_4 &= -(y_n + hk_3) = -y_n\\left(1 - h + \\frac{h^2}{2} - \\frac{h^3}{4}\\right)\n\\end{aligned}\n$$\n将这些代入RK4公式并化简，得到：\n$$\ny_{n+1} = y_n\\left(1 - h + \\frac{h^2}{2} - \\frac{h^3}{6} + \\frac{h^4}{24}\\right)\n$$\n这个表达式恰好是 $y_n e^{-h}$ 的泰勒级数展开的前五项，因为 $e^{-h} = 1 - h + \\frac{h^2}{2!} - \\frac{h^3}{3!} + \\frac{h^4}{4!} - \\frac{h^5}{5!} + \\dots$。\n\n数值方法的精度由其阶来表征。局部截断误差(LTE)是在假定方法从精确解 $y(t_n)$ 开始时，单步引入的误差。对于一个 $p$ 阶方法，其局部截断误差为 $\\mathcal{O}(h^{p+1})$。而全局误差是积分区间末端的累积误差，为 $\\mathcal{O}(h^p)$。\n\n对于显式欧拉法，其局部截断误差为：\n$$\nLTE_{n+1} = y(t_{n+1}) - \\left[y(t_n) + h f(t_n, y(t_n))\\right] = \\left[y(t_n) + h y^{\\prime}(t_n) + \\frac{h^2}{2}y^{\\prime\\prime}(\\xi)\\right] - \\left[y(t_n) + h y^{\\prime}(t_n)\\right] = \\frac{h^2}{2}y^{\\prime\\prime}(\\xi)\n$$\n对于某个 $\\xi \\in (t_n, t_{n+1})$。局部截断误差为 $\\mathcal{O}(h^2)$，这意味着欧拉法是一阶精度的，因此其全局误差表现为 $\\mathcal{O}(h^1)$。\n\n对于RK4方法，其在我们特定问题上的更新规则与 $e^{-h}$ 的泰勒级数匹配到 $h^4$ 项。因此，单步误差由级数中的下一项主导：\n$$\nLTE_{n+1} = y(t_n)e^{-h} - y(t_n)\\left(1 - h + \\frac{h^2}{2} - \\frac{h^3}{6} + \\frac{h^4}{24}\\right) = y(t_n)\\left(-\\frac{h^5}{120} + \\mathcal{O}(h^6)\\right)\n$$\n局部截断误差为 $\\mathcal{O}(h^5)$，因此RK4方法是四阶精度的，其全局误差表现为 $\\mathcal{O}(h^4)$。\n\n为了通过计算来验证阶数 $p$，我们假设最终时刻的全局误差 $E$ 遵循 $E \\approx C h^p$ 的关系，其中 $C$ 为某个常数。对两边取对数，得到：\n$$\n\\log(E) \\approx \\log(C) + p \\log(h)\n$$\n这表明在 $\\log(E)$ 和 $\\log(h)$ 之间存在线性关系，其斜率即为精度阶 $p$。我们可以通过计算一系列步长下的误差，并对这些对数-对数数据进行线性最小二乘拟合来估计 $p$。\n\n最后，我们必须考虑有限精度算术的影响。总数值误差是截断误差（源于方法的近似）和舍入误差（源于浮点数表示）之和。截断误差随着 $h$ 的减小而减小（例如，$\\propto h^p$）。然而，舍入误差倾向于随着步数 $N = T/h$ 的增加而累积。累积的舍入误差可以建模为与 $\\epsilon/h$ 成正比，其中 $\\epsilon$ 是浮点数格式的机器ε。总误差近似为：\n$$\nE_{total} \\approx C h^p + \\frac{D\\epsilon}{h}\n$$\n该函数在某个最佳步长 $h^*$ 处有最小值。当 $h > h^*$ 时，截断误差占主导地位，误差随 $h$ 的减小而减小。当 $h < h^*$ 时，舍入误差占主导地位，误差随 $h$ 的减小而增大。这种效应在单精度（binary32，$\\epsilon \\approx 10^{-8}$）中比在双精度（binary64，$\\epsilon \\approx 10^{-16}$）中更为显著，因为较大的 $\\epsilon$ 会导致较大的最佳步长 $h^*$。这正是测试用例旨在研究的内容。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef euler_step(y, h, dtype):\n    \"\"\"\n    Performs one step of the explicit Euler method for y' = -y.\n    All calculations are performed in the specified dtype.\n    \n    Args:\n        y: Current value of the solution (of type dtype).\n        h: Step size (Python float).\n        dtype: The numpy floating-point type (np.float32 or np.float64).\n        \n    Returns:\n        The next value of the solution (of type dtype).\n    \"\"\"\n    h_typed = dtype(h)\n    one = dtype(1.0)\n    # The update is y_{n+1} = y_n * (1 - h)\n    return y * (one - h_typed)\n\ndef rk4_step(y, h, dtype):\n    \"\"\"\n    Performs one step of the classical 4th-order Runge-Kutta method for y' = -y.\n    All calculations are performed in the specified dtype.\n    \n    Args:\n        y: Current value of the solution (of type dtype).\n        h: Step size (Python float).\n        dtype: The numpy floating-point type (np.float32 or np.float64).\n        \n    Returns:\n        The next value of the solution (of type dtype).\n    \"\"\"\n    h_typed = dtype(h)\n    \n    # Define constants in the target precision\n    c_half = dtype(0.5)\n    c_two = dtype(2.0)\n    c_six = dtype(6.0)\n\n    # ODE is y' = f(y) = -y\n    k1 = -y\n    k2 = -(y + c_half * h_typed * k1)\n    k3 = -(y + c_half * h_typed * k2)\n    k4 = -(y + h_typed * k3)\n    \n    return y + (h_typed / c_six) * (k1 + c_two * k2 + c_two * k3 + k4)\n\ndef solve_ode(step_func, h, dtype):\n    \"\"\"\n    Solves the ODE y'=-y from t=0 to t=1 with y(0)=1.\n    \n    Args:\n        step_func: The function for a single integration step (e.g., euler_step).\n        h: The step size.\n        dtype: The numpy floating-point type for computation.\n        \n    Returns:\n        The absolute global error at t=1.\n    \"\"\"\n    # The problem statement guarantees 1/h is an integer.\n    # Use round() to be robust against floating point inaccuracies, e.g., 1.0/0.1\n    num_steps = int(round(1.0 / h))\n    \n    # Initial condition y(0)=1, cast to the specified precision\n    y = dtype(1.0)\n\n    for _ in range(num_steps):\n        y = step_func(y, h, dtype)\n\n    # Reference solution y(1)=e^-1 in double precision (binary64)\n    y_exact_64 = np.exp(np.float64(-1.0))\n    \n    # Error is computed against the high-precision reference.\n    # The subtraction will promote the result to float64, which is desired.\n    return np.abs(y - y_exact_64)\n\ndef estimate_order(step_func, h_list, dtype):\n    \"\"\"\n    Estimates the order of accuracy of a method by log-log linear regression.\n    \n    Args:\n        step_func: The single-step integration function.\n        h_list: A list of step sizes to use.\n        dtype: The numpy floating-point type.\n        \n    Returns:\n        The estimated order of accuracy (slope of the log-log plot).\n    \"\"\"\n    h_array = np.array(h_list, dtype=np.float64)\n    errors = np.array([solve_ode(step_func, h, dtype) for h in h_list], dtype=np.float64)\n\n    log_h = np.log(h_array)\n    log_err = np.log(errors)\n\n    # Perform linear least squares to find the slope p\n    # of the line log(error) = log(C) + p * log(h)\n    # Using the standard formula for the slope of a simple linear regression.\n    n = len(log_h)\n    sum_xy = np.sum(log_h * log_err)\n    sum_x = np.sum(log_h)\n    sum_y = np.sum(log_err)\n    sum_x2 = np.sum(log_h**2)\n    \n    slope_p = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x**2)\n    return slope_p\n\ndef solve():\n    \"\"\"\n    Main function to compute the six required results for the test suite.\n    \"\"\"\n    # Test 1: Global error for Euler, h=0.1, binary64\n    error1 = solve_ode(euler_step, 0.1, np.float64)\n\n    # Test 2: Global error for RK4, h=0.1, binary64\n    error2 = solve_ode(rk4_step, 0.1, np.float64)\n\n    # Test 3: Observed order for Euler, binary64\n    h_order_list = [0.2, 0.1, 0.05, 0.025]\n    order3 = estimate_order(euler_step, h_order_list, np.float64)\n\n    # Test 4: Observed order for RK4, binary64\n    order4 = estimate_order(rk4_step, h_order_list, np.float64)\n    \n    # Test 5: Index of minimum error for RK4, binary32 (truncation vs. round-off)\n    h_balance_list = [2.0**-k for k in [3, 5, 7, 9, 11, 13]]\n    errors_balance = [solve_ode(rk4_step, h, np.float32) for h in h_balance_list]\n    index5 = np.argmin(errors_balance)\n    \n    # Test 6: Monotonicity check for RK4, binary64\n    h_mono_list = [2.0**-k for k in [6, 7, 8, 9]]\n    errors_mono = [solve_ode(rk4_step, h, np.float64) for h in h_mono_list]\n    # Check if errors are strictly decreasing: e[i] > e[i+1] for all i\n    bool6 = all(np.diff(errors_mono) < 0)\n\n    results = [error1, error2, order3, order4, index5, bool6]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2447459"}, {"introduction": "在我们理解了误差结构的基础上，本实践引入了一种强大的技术，称为理查森外推法（Richardson extrapolation）。我们将看到，如何结合一个简单方法（欧拉法）在不同步长下计算出的结果，来系统地消除主导误差项，而无需从头设计一个更复杂的高阶方法。这个练习展示了一个巧妙且广泛适用的提升数值精度的原理[@problem_id:2409197]。", "problem": "实现一个完整的程序，该程序针对一组形如 $\\dfrac{dy}{dt} = f(t,y)$ 且 $y(0) = y_0$ 的初值问题，执行以下操作：从局部截断误差和全局截断误差的定义出发，推导出一个 Richardson 外推组合，该组合通过结合使用步长 $h$ 和 $h/2$ 计算出的两个 Euler 解，来消除显式 Euler 方法的全局误差主项。该组合必须通过假设全局误差有关于 $h$ 的幂次的渐近展开，从第一性原理推导得出，并且它必须产生一个全局截断误差为 $O(h^2)$ 阶的新近似值。\n\n您的推导必须基于以下基本事实：\n- 显式 Euler 方法由递推关系 $y_{n+1} = y_n + h f(t_n, y_n)$ 定义，其中 $t_n = t_0 + n h$。\n- 局部截断误差是将精确解代入单步方法中得到的单步缺陷，对于光滑的 $f$，其阶数为 $O(h^2)$。\n- 在固定的最终时间 $T$，全局截断误差是经过 $N = T/h$ 步后，精确解与数值解之间累积的差异；对于显式 Euler 方法，其阶数为 $O(h)$。\n\n您的程序必须实现：\n1. 一个函数，用以计算在最终时间 $T$ 处，使用均匀步长 $h$ 的显式 Euler 解 $y_h(T)$。\n2. 一个 Richardson 外推组合，它仅使用步长为 $h$ 和 $h/2$ 计算的两个 Euler 解，产生一个改进的近似值 $y_{\\mathrm{RE}}(T)$，其全局截断误差为 $O(h^2)$ 阶。\n3. 一个经验性验证例程，该例程也计算步长为 $h/4$ 的解，以便您能够估计观测到的收敛阶：\n   - 对于显式 Euler 方法，估计 $p_{\\mathrm{E}} \\approx \\log_2\\!\\left(\\dfrac{|y_h(T) - y(T)|}{|y_{h/2}(T) - y(T)|}\\right)$。\n   - 对于 Richardson 外推近似，首先构造 $y_{\\mathrm{RE}}(h) = \\mathrm{RE}(y_h, y_{h/2})$ 和 $y_{\\mathrm{RE}}(h/2) = \\mathrm{RE}(y_{h/2}, y_{h/4})$，然后估计 $p_{\\mathrm{RE}} \\approx \\log_2\\!\\left(\\dfrac{|y_{\\mathrm{RE}}(h) - y(T)|}{|y_{\\mathrm{RE}}(h/2) - y(T)|}\\right)$。\n\n角度必须以弧度为单位进行解释。\n\n测试套件：\n对于下面的每种情况，计算 $y_h(T)$、$y_{h/2}(T)$、$y_{h/4}(T)$，构造 $y_{\\mathrm{RE}}(h)$ 和 $y_{\\mathrm{RE}}(h/2)$，然后计算关于精确解 $y(T)$ 的绝对误差。\n- 情况 A (指数衰减)：$\\dfrac{dy}{dt} = -3 y$， $y(0) = 1$， $T = 1$， 基础步长 $h = 0.2$。精确解：$y(t) = e^{-3 t}$。\n- 情况 B (线性非齐次)：$\\dfrac{dy}{dt} = t + y$， $y(0) = 0$， $T = 1$， 基础步长 $h = 0.2$。精确解：$y(t) = e^{t} - (t + 1)$。\n- 情况 C (变系数增长)：$\\dfrac{dy}{dt} = \\sin(t)\\, y$， $y(0) = 1$， $T = 1$， 基础步长 $h = 0.2$。精确解：$y(t) = \\exp\\!\\big(1 - \\cos(t)\\big)$。\n\n对于每种情况，您的程序必须生成一个包含五个实数的列表：\n- 使用步长 $h$ 的 Euler 方法的绝对全局误差，即 $E_h = |y_h(T) - y(T)|$。\n- 使用步长 $h/2$ 的 Euler 方法的绝对全局误差，即 $E_{h/2} = |y_{h/2}(T) - y(T)|$。\n- 由 $h$ 和 $h/2$ 构建的 Richardson 外推近似的绝对全局误差，即 $E_{\\mathrm{RE}}(h) = |y_{\\mathrm{RE}}(h) - y(T)|$。\n- 上面定义的观测阶 $p_{\\mathrm{E}}$。\n- 上面定义的观测阶 $p_{\\mathrm{RE}}$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含三种情况的结果，按情况 A、情况 B、情况 C 的顺序排列，作为一个逗号分隔的三个列表的列表，并包含在一对单独的方括号中，任何地方都没有空格。每个实数必须以科学记数法打印，并保留十位有效数字。例如，该行必须看起来像：\n[[E_h_A,E_h2_A,E_RE_A,pE_A,pRE_A],[E_h_B,E_h2_B,E_RE_B,pE_B,pRE_B],[E_h_C,E_h2_C,E_RE_C,pE_C,pRE_C]]\n确保所有三角函数参数都以弧度为单位。由于此问题中所有量都是无量纲的，因此不需要单位。", "solution": "对问题陈述进行验证。\n\n逐字提取给定条件：\n1.  初值问题：$\\dfrac{dy}{dt} = f(t,y)$ 且 $y(0) = y_0$。\n2.  显式 Euler 方法递推关系：$y_{n+1} = y_n + h f(t_n, y_n)$，其中 $t_n = t_0 + n h$。\n3.  显式 Euler 方法的局部截断误差阶数为 $\\mathcal{O}(h^2)$。\n4.  显式 Euler 方法在固定时间 $T$ 的全局截断误差阶数为 $\\mathcal{O}(h)$。\n5.  任务：为两个步长为 $h$ 和 $h/2$ 的 Euler 解推导 Richardson 外推组合，以消除全局误差主项，假设全局误差存在渐近展开。得到的近似值的全局误差阶数必须为 $\\mathcal{O}(h^2)$。\n6.  任务：使用步长 $h$、$h/2$ 和 $h/4$ 实现经验性验证。\n7.  Euler 方法的观测阶：$p_{\\mathrm{E}} \\approx \\log_2\\!\\left(\\dfrac{|y_h(T) - y(T)|}{|y_{h/2}(T) - y(T)|}\\right)$。\n8.  Richardson 外推的观测阶：$p_{\\mathrm{RE}} \\approx \\log_2\\!\\left(\\dfrac{|y_{\\mathrm{RE}}(h) - y(T)|}{|y_{\\mathrm{RE}}(h/2) - y(T)|}\\right)$，其中 $y_{\\mathrm{RE}}(h) = \\mathrm{RE}(y_h, y_{h/2})$ 且 $y_{\\mathrm{RE}}(h/2) = \\mathrm{RE}(y_{h/2}, y_{h/4})$。\n9.  测试用例 A: $\\dfrac{dy}{dt} = -3 y$, $y(0) = 1$, $T = 1$, $h = 0.2$。精确解：$y(t) = e^{-3 t}$。\n10. 测试用例 B: $\\dfrac{dy}{dt} = t + y$, $y(0) = 0$, $T = 1$, $h = 0.2$。精确解：$y(t) = e^{t} - (t + 1)$。\n11. 测试用例 C: $\\dfrac{dy}{dt} = \\sin(t)\\, y$, $y(0) = 1$, $T = 1$, $h = 0.2$。精确解：$y(t) = \\exp\\!\\big(1 - \\cos(t)\\big)$。\n12. 每个用例所需的输出：一个包含五个数字的列表：$|y_h(T) - y(T)|$、$|y_{h/2}(T) - y(T)|$、$|y_{\\mathrm{RE}}(h) - y(T)|$、$p_{\\mathrm{E}}$ 和 $p_{\\mathrm{RE}}$。\n\n使用提取的给定条件进行验证：\n该问题具有科学依据。它涉及 Richardson 外推，这是数值分析中一种用于提高数值方法准确性的标准和基本技术。前提条件——显式 Euler 方法的定义及其局部和全局截断误差的阶数——是常微分方程数值解研究中的标准结果。这是一个适定问题；它要求对定义明确、具有唯一解析解的测试用例进行特定的推导和实现，从而确保可以获得并验证唯一且有意义的结果。语言客观而精确。该问题不违反任何无效性标准。这是一个可形式化的、与计算物理和数值方法直接相关的问题。\n\n结论：问题有效。将提供解决方案。\n\nRichardson 外推公式的推导必须按规定从第一性原理构造。该方法的基础是数值方案的全局误差存在渐近展开。对于全局误差为 $\\mathcal{O}(h)$ 阶的显式 Euler 方法，该展开式具有以下形式：\n$$\ny_h(T) = y(T) + C_1 h + C_2 h^2 + C_3 h^3 + \\dots\n$$\n此处，$y(T)$ 是最终时间 $T$ 的精确解，$y_h(T)$ 是用步长 $h$ 获得的数值近似值，系数 $C_k$ 与 $h$ 无关，但依赖于函数 $f(t,y)$ 及其在各点的导数。误差主项是 $C_1 h$。\n\n我们的目标是通过组合两个独立的计算来消除这个主项。让我们用步长 $h$ 和步长 $h/2$ 分别进行数值积分。相应数值解的渐近展开式为：\n$$\n(1) \\quad y_h(T) = y(T) + C_1 h + C_2 h^2 + \\mathcal{O}(h^3)\n$$\n$$\n(2) \\quad y_{h/2}(T) = y(T) + C_1 \\frac{h}{2} + C_2 \\left(\\frac{h}{2}\\right)^2 + \\mathcal{O}(h^3) = y(T) + \\frac{1}{2} C_1 h + \\frac{1}{4} C_2 h^2 + \\mathcal{O}(h^3)\n$$\n我们寻求 $y_h(T)$ 和 $y_{h/2}(T)$ 的一个线性组合，以消去与 $C_1 h$ 成比例的项。设外推近似为 $y_{\\mathrm{RE}}(T)$。为消去 $C_1 h$，我们可以将方程 $(2)$ 乘以 $2$ 并减去方程 $(1)$：\n$$\n2 y_{h/2}(T) - y_h(T) = \\left(2y(T) + C_1 h + \\frac{1}{2} C_2 h^2 + \\dots\\right) - \\left(y(T) + C_1 h + C_2 h^2 + \\dots\\right)\n$$\n$$\n2 y_{h/2}(T) - y_h(T) = (2-1)y(T) + (1-1)C_1 h + \\left(\\frac{1}{2}-1\\right)C_2 h^2 + \\mathcal{O}(h^3)\n$$\n$$\n2 y_{h/2}(T) - y_h(T) = y(T) - \\frac{1}{2} C_2 h^2 + \\mathcal{O}(h^3)\n$$\n由此，我们将 Richardson 外推解定义为 $y_{\\mathrm{RE}}(T)$：\n$$\ny_{\\mathrm{RE}}(T) = 2 y_{h/2}(T) - y_h(T)\n$$\n这个新近似的全局截断误差是差值 $y_{\\mathrm{RE}}(T) - y(T)$。根据我们的推导：\n$$\ny_{\\mathrm{RE}}(T) - y(T) = -\\frac{1}{2} C_2 h^2 + \\mathcal{O}(h^3)\n$$\n这证实了全局截断误差现在是 $\\mathcal{O}(h^2)$ 阶，符合要求。原方法的误差主项已成功消除。\n\n计算实现将包括三个主要部分。首先，一个名为 `explicit_euler` 的函数将使用恒定步长 $h$ 求解给定的常微分方程，从初始时间 $t_0$ 到最终时间 $T$。该函数将实现递推关系 $y_{k+1} = y_k + h f(t_k, y_k)$，迭代 $N = \\text{round}((T-t_0)/h)$ 次。\n\n其次，一个主例程将为每个测试用例执行逻辑。对于一个基础步长 $h$，它将使用步长 $h$、$h/2$ 和 $h/4$ 调用 `explicit_euler` 函数三次，以获得数值解 $y_h(T)$、$y_{h/2}(T)$ 和 $y_{h/4}(T)$。\n\n第三，该例程将使用这些数值结果进行经验性分析。它将计算 Richardson 外推近似值：\n$$\ny_{\\mathrm{RE}}(h) = 2 y_{h/2}(T) - y_h(T)\n$$\n$$\ny_{\\mathrm{RE}}(h/2) = 2 y_{h/4}(T) - y_{h/2}(T)\n$$\n然后计算相对于已知精确解 $y(T)$ 的绝对全局误差。最后，使用指定的对数公式计算观测收敛阶 $p_{\\mathrm{E}}$ 和 $p_{\\mathrm{RE}}$。对于 Euler 方法，这些经验阶数应约为 $1$，对于 Richardson 外推方法，应约为 $2$，从而验证理论上的精度提升。所有测试用例的最终结果将根据指定格式进行格式化和打印。三角函数输入将以弧度为单位进行处理，这是科学计算中的标准做法。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements Richardson extrapolation for the explicit Euler method to solve\n    several initial value problems and empirically verifies the order of convergence.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"f\": lambda t, y: -3.0 * y,\n            \"y0\": 1.0,\n            \"T\": 1.0,\n            \"h\": 0.2,\n            \"exact_y\": lambda t: np.exp(-3.0 * t),\n        },\n        {\n            \"f\": lambda t, y: t + y,\n            \"y0\": 0.0,\n            \"T\": 1.0,\n            \"h\": 0.2,\n            \"exact_y\": lambda t: np.exp(t) - (t + 1.0),\n        },\n        {\n            \"f\": lambda t, y: np.sin(t) * y,\n            \"y0\": 1.0,\n            \"T\": 1.0,\n            \"h\": 0.2,\n            \"exact_y\": lambda t: np.exp(1.0 - np.cos(t)),\n        },\n    ]\n\n    def explicit_euler(f_ode, y_start, t_start, t_end, step_size):\n        \"\"\"\n        Computes the solution of an ODE using the explicit Euler method.\n        \"\"\"\n        t = t_start\n        y = y_start\n        # Use round() to avoid floating-point inaccuracies in step counting\n        num_steps = int(round((t_end - t_start) / step_size))\n        \n        for _ in range(num_steps):\n            y = y + step_size * f_ode(t, y)\n            t = t + step_size\n        return y\n\n    all_results = []\n    for case in test_cases:\n        f = case[\"f\"]\n        y0 = case[\"y0\"]\n        T = case[\"T\"]\n        h_base = case[\"h\"]\n        exact_y_func = case[\"exact_y\"]\n\n        # Define the three step sizes\n        h = h_base\n        h_half = h_base / 2.0\n        h_quarter = h_base / 4.0\n\n        # Compute Euler solutions for each step size\n        y_h = explicit_euler(f, y0, 0.0, T, h)\n        y_h_half = explicit_euler(f, y0, 0.0, T, h_half)\n        y_h_quarter = explicit_euler(f, y0, 0.0, T, h_quarter)\n\n        # Compute the exact solution at the final time T\n        y_exact = exact_y_func(T)\n\n        # Compute Richardson-extrapolated approximations\n        y_re_h = 2.0 * y_h_half - y_h\n        y_re_h_half = 2.0 * y_h_quarter - y_h_half\n\n        # Compute absolute errors\n        E_h = np.abs(y_h - y_exact)\n        E_h_half = np.abs(y_h_half - y_exact)\n        E_re_h = np.abs(y_re_h - y_exact)\n        E_re_h_half = np.abs(y_re_h_half - y_exact)\n\n        # Compute observed orders of convergence\n        # Handle cases where error is zero to avoid division by zero or log(0)\n        p_E = np.log2(E_h / E_h_half) if E_h_half > 0 else 0.0\n        p_RE = np.log2(E_re_h / E_re_h_half) if E_re_h_half > 0 else 0.0\n\n        results = [E_h, E_h_half, E_re_h, p_E, p_RE]\n        all_results.append(results)\n\n    # Format the final output string as specified\n    formatted_cases = []\n    for case_res in all_results:\n        formatted_nums = [f\"{num:.10e}\" for num in case_res]\n        formatted_cases.append(f\"[{','.join(formatted_nums)}]\")\n    \n    final_output = f\"[{','.join(formatted_cases)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2409197"}, {"introduction": "我们的最后一个实践探讨了常微分方程求解中的一个关键挑战，即“刚性”（stiffness）问题。我们将研究这样一种情况：对于像前向欧拉法这样的显式方法，获得稳定解所需的步长 $h$ 远小于仅从局部精度考虑所建议的步长。这个练习将揭示局部截断误差与数值稳定性之间至关重要的相互作用，并说明为何这一因素是为特定问题选择正确求解器时的决定性考量[@problem_id:2409172]。", "problem": "考虑标量线性刚性常微分方程 (ODE)：$y'(t)=-\\lambda y(t)$，其中 $\\lambda>0$ 且初始条件为 $y(0)=1$。其精确解为 $y(t)=e^{-\\lambda t}$。设数值近似解 $y_n$ 是在均匀网格 $t_n = n h$ 上，使用显式前向欧拉单步格式 $y_{n+1}=y_n+h f(t_n,y_n)$ 生成的，其中时间步长 $h>0$ 为常数，且 $f(t,y)=-\\lambda y$。经过 $N$ 步后，最终时间为 $T=N h$。定义初始点的单步局部截断误差 (LTE) 为 $\\mathrm{LTE}_1=\\left|y(h)-\\left(y(0)+h f(0,y(0))\\right)\\right|$，并定义在时间 $T$ 的全局截断误差 (GTE) 为 $\\mathrm{GTE}(T)=\\left|y_N-y(T)\\right|$。此外，定义比率 $R=\\mathrm{GTE}(T)/\\mathrm{LTE}_1$，以及此问题上显式方法的线性稳定性指标 $S$ 为条件 $\\left|1-\\lambda h\\right|<1$ 的布尔值。\n\n您的任务是编写一个完整的程序，对以下测试集中的每个参数三元组 $(\\lambda,T,h)$，使用 $y(0)=1$ 和上述定义来计算量组 $\\left(\\mathrm{GTE}(T),\\mathrm{LTE}_1,R,S\\right)$。所有计算都是无量纲的。不涉及角度。请勿对输出进行四舍五入；以原始浮点数和布尔值的形式生成它们。\n\n测试集（请按此确切顺序提供结果）：\n- 案例 1：$(\\lambda,T,h)=\\left(200,1.0,0.002\\right)$。\n- 案例 2：$(\\lambda,T,h)=\\left(200,1.0,0.01\\right)$。\n- 案例 3：$(\\lambda,T,h)=\\left(200,0.96,0.012\\right)$。\n- 案例 4：$(\\lambda,T,h)=\\left(500,0.468,0.0039\\right)$。\n- 案例 5：$(\\lambda,T,h)=\\left(20,1.0,0.2\\right)$。\n\n您的程序应产生单行输出，其中包含一个以方括号括起来的逗号分隔列表。列表中的每个案例结果本身也是一个列表，形式为 $[\\mathrm{GTE}(T),\\mathrm{LTE}_{1,1},R_1,S_1]$。例如，总体输出格式必须为 $[[\\mathrm{GTE}_1,\\mathrm{LTE}_{1,1},R_1,S_1],[\\mathrm{GTE}_2,\\mathrm{LTE}_{1,2},R_2,S_2],\\dots]$。", "solution": "所提出的问题是常微分方程 (ODE) 数值方法基础分析中的一个标准、适定性良好的练习。它在科学上是合理的，基于计算物理和数值分析的基本原理，并为获得唯一、明确的解提供了所有必要的定义和数据。我们未发现任何瑕疵；因此，我们着手进行推导。\n\n问题的核心是标量线性常微分方程：\n$$\ny'(t) = -\\lambda y(t)\n$$\n其初始条件为 $y(0)=1$ 且参数 $\\lambda > 0$。其精确解给定为：\n$$\ny(t) = e^{-\\lambda t}\n$$\n我们需要分析应用于此方程的显式前向欧拉格式。该格式定义为：\n$$\ny_{n+1} = y_n + h f(t_n, y_n)\n$$\n其中 $h$ 是常数时间步长，且 $f(t,y) = -\\lambda y$。将函数 $f$ 代入格式，可得到在时间 $t_n = n h$ 的数值解 $y_n$ 的迭代关系：\n$$\ny_{n+1} = y_n + h(-\\lambda y_n) = (1 - \\lambda h) y_n\n$$\n这是一个一阶线性递推关系。给定初始条件 $y_0 = y(0) = 1$，我们可以推导出在时间 $T=Nh$ 的数值解 $y_N$ 的闭式表达式：\n$$\ny_N = (1 - \\lambda h)^N y_0 = (1 - \\lambda h)^N\n$$\n由于 $N=T/h$，我们得到在最终时间 $T$ 的数值近似解：\n$$\ny_N = \\left(1 - \\lambda h\\right)^{T/h}\n$$\n基于此，我们现在可以为每个参数集 $(\\lambda, T, h)$ 推导问题所需的四个量的表达式。\n\n1.  全局截断误差，$\\mathrm{GTE}(T)$:\n    GTE 被定义为在最终时间 $T$ 时，数值解与精确解之间的绝对差。\n    $$\n    \\mathrm{GTE}(T) = \\left|y_N - y(T)\\right|\n    $$\n    代入 $y_N$ 和 $y(T)$ 的表达式：\n    $$\n    \\mathrm{GTE}(T) = \\left| \\left(1 - \\lambda h\\right)^{T/h} - e^{-\\lambda T} \\right|\n    $$\n\n2.  局部截断误差，$\\mathrm{LTE}_1$:\n    初始点的单步 LTE 被定义为第一步所产生的误差，假设该步骤从精确解 $y(0)$ 开始。\n    $$\n    \\mathrm{LTE}_1 = \\left|y(h) - \\left(y(0)+h f(0,y(0))\\right)\\right|\n    $$\n    我们代入已知量：$y(h) = e^{-\\lambda h}$，$y(0)=1$，以及 $f(0, y(0)) = f(0,1) = -\\lambda$。\n    $$\n    \\mathrm{LTE}_1 = \\left|e^{-\\lambda h} - \\left(1 + h(-\\lambda)\\right)\\right| = \\left|e^{-\\lambda h} - (1 - \\lambda h)\\right|\n    $$\n    从 $e^{-\\lambda h}$ 的泰勒级数展开式 $e^{-\\lambda h} = 1 - \\lambda h + \\frac{(\\lambda h)^2}{2!} - O((\\lambda h)^3)$ 可以明显看出，$\\mathrm{LTE}_1$ 的阶数为 $O(h^2)$。由于一个 $p$ 阶方法的局部误差为 $O(h^{p+1})$，这证实了前向欧拉法是一阶的 ($p=1$)。\n\n3.  比率，$R$:\n    比率 $R$ 定义为 $R = \\mathrm{GTE}(T)/\\mathrm{LTE}_1$。\n    $$\n    R = \\frac{\\left| \\left(1 - \\lambda h\\right)^{T/h} - e^{-\\lambda T} \\right|}{\\left|e^{-\\lambda h} - (1 - \\lambda h)\\right|}\n    $$\n    这个比率比较了最终全局误差的大小与初始局部误差的大小。\n\n4.  线性稳定性指标，$S$:\n    对于此测试问题，前向欧拉法的稳定性取决于放大因子 $g(\\lambda h) = 1 - \\lambda h$。为使数值解保持有界，我们要求 $|g(\\lambda h)| \\leq 1$。问题为指标 $S$ 指定了一个严格不等式。\n    $$\n    S = \\text{boolean value of } \\left|1 - \\lambda h\\right| < 1\n    $$\n    此不等式等价于 $-1 < 1 - \\lambda h < 1$。\n    不等式的右侧，$1 - \\lambda h < 1$，意味着 $-\\lambda h < 0$，因为 $\\lambda > 0$ 和 $h > 0$，所以此式恒成立。\n    不等式的左侧，$-1 < 1 - \\lambda h$，意味着 $\\lambda h < 2$。\n    因此，稳定性条件简化为 $0 < \\lambda h < 2$。若 $\\lambda h < 2$，指标 $S$ 为 `True`，否则为 `False`。\n\n这四个公式将被直接实现，以计算每个测试案例所需的元组。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes GTE, LTE, their ratio, and a stability indicator for the\n    forward Euler method on a stiff ODE for several test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (200.0, 1.0, 0.002),   # Case 1\n        (200.0, 1.0, 0.01),    # Case 2\n        (200.0, 0.96, 0.012),   # Case 3\n        (500.0, 0.468, 0.0039), # Case 4\n        (20.0, 1.0, 0.2),      # Case 5\n    ]\n\n    results = []\n    for case in test_cases:\n        lambda_, T, h = case\n\n        # Number of steps N = T/h. Ensure it is an integer value.\n        # Problem statement guarantees this, but round for robustness.\n        N = int(round(T / h))\n\n        # Calculate lambda * h, a key dimensionless parameter\n        lambda_h = lambda_ * h\n\n        # 1. Global Truncation Error (GTE) at time T\n        # GTE(T) = |y_N - y(T)|\n        # y_N = (1 - lambda*h)^N\n        # y(T) = exp(-lambda*T)\n        y_num_N = (1.0 - lambda_h)**N\n        y_exact_T = np.exp(-lambda_ * T)\n        GTE = np.abs(y_num_N - y_exact_T)\n\n        # 2. Local Truncation Error (LTE) at the first step\n        # LTE_1 = |y(h) - (y(0) + h*f(0,y(0)))|\n        # y(h) = exp(-lambda*h)\n        # y(0) + h*f(0,y(0)) = 1 - lambda*h\n        y_exact_h = np.exp(-lambda_h)\n        y_num_h = 1.0 - lambda_h\n        LTE1 = np.abs(y_exact_h - y_num_h)\n        \n        # 3. Ratio R = GTE(T) / LTE_1\n        # The problem setup ensures LTE1 is never zero.\n        if LTE1 == 0.0:\n            # Handle potential division by zero, though not expected here.\n            R = np.inf if GTE != 0.0 else 0.0\n        else:\n            R = GTE / LTE1\n\n        # 4. Linear Stability Indicator S\n        # S is True if |1 - lambda*h| < 1, which simplifies to lambda*h < 2.\n        S = lambda_h < 2.0\n        \n        # Store the computed tuple\n        result_tuple_str = f\"[{GTE},{LTE1},{R},{S}]\"\n        results.append(result_tuple_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2409172"}]}