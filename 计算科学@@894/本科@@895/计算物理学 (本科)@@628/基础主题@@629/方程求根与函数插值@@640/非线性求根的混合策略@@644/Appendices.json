{"hands_on_practices": [{"introduction": "理论学习之后，最好的巩固方式就是实践。本节的第一个练习将理论与一个经典问题相结合：求解特殊函数（切比雪夫多项式）的根。这个练习的重点不仅仅是调用一个现成的求解器，而是要求我们首先利用函数的数学特性来制定一个稳健的策略。通过分析切比雪夫多项式根与极值点交错的特性，我们可以为每一个根系统性地确定一个“安全”的括号区间，这体现了“分而治之”的思想，是混合策略在更高层次上的应用。[@problem_id:2402261]", "problem": "给定第一类 Chebyshev 多项式，它对任意整数阶 $n \\ge 0$ 由关系式 $T_n(\\cos \\theta) = \\cos(n \\theta)$ 定义，此式对所有实数 $\\theta$ 成立。对于任意给定的 $n \\ge 1$，多项式 $T_n(x)$ 在开区间 $(-1,1)$ 内恰好有 $n$ 个单根。这 $n$ 个根的精确位置由集合 $\\left\\{\\cos\\left(\\dfrac{(2k-1)\\pi}{2n}\\right) : k = 1,2,\\dots,n \\right\\}$ 给出。角度必须以弧度为单位进行解释。\n\n编写一个完整的、可运行的程序，对每个指定的测试值 $n$，通过对每个根数值求解标量非线性方程 $T_n(x)=0$ 来计算 $T_n(x)$ 在区间 $[-1,1]$ 内的所有 $n$ 个根，然后将数值计算出的根与上述解析集进行比较。您的数值求解器所产生的根，其在 $x$ 上的绝对精度目标必须最多为 $10^{-10}$。在进行数值求解时，您不能假设预先知道根的位置；相反，应直接根据 $T_n(x)$ 的定义性质来确定它们。计算出根集后，将其按升序排序，并计算数值根与解析值 $\\cos\\left(\\dfrac{(2k-1)\\pi}{2n}\\right)$（其中 $k=1,\\dots,n$）之间的最大绝对偏差。\n\n使用以下阶数的测试集：\n- $n = 1$\n- $n = 17$\n- $n = 50$\n- $n = 100$\n- $n = 200$\n\n对于测试集中的每个 $n$，您的程序必须输出一个实数，该实数等于计算出的根与解析集之间（在 $x$ 上）的最大绝对差，并由您编程环境的默认格式进行四舍五入。最终输出必须是单行，其中包含一个由方括号括起来的、由逗号分隔的这五个数字的列表。例如，输出必须具有 \"[r1,r2,r3,r4,r5]\" 的形式，其中每个 rj 是对应测试用例的实值最大绝对偏差。所有角度必须以弧度为单位进行解释。本问题不涉及任何物理单位，所有数值答案均为无单位的纯数。", "solution": "所提出的问题是数值分析中的一个标准练习，特别是在特殊函数求根领域。该问题是有效的、自洽的，并且在科学上是合理的。它要求对第一类 Chebyshev 多项式 $T_n(x)$ 在几个不同阶数 $n$ 下的根进行数值确定，并随后与已知的解析解进行比较，以量化数值误差。其中，求解器不能用解析根位置进行初始化的约束是一项关键且恰当的指令，它强制要求基于多项式本身的基本性质来设计解决方案。\n\n我们的方法将是系统性的，并基于第一性原理。\n\n首先，我们必须有一种方法来计算任意整数 $n \\ge 0$ 和实数自变量 $x \\in [-1, 1]$ 下的 $T_n(x)$。虽然定义 $T_n(x) = \\cos(n \\arccos(x))$ 在数学上是精确的，但在数值求根的上下文中使用它是有问题的，因为它涉及到我们试图表征的函数本身。一个更实用的工具是 Chebyshev 多项式所满足的三项递推关系：\n$$\nT_0(x) = 1 \\\\\nT_1(x) = x \\\\\nT_{k+1}(x) = 2xT_k(x) - T_{k-1}(x) \\quad \\text{for } k \\ge 1\n$$\n这种递推关系提供了一种计算上稳定且高效的方法，可以通过简单的迭代过程来计算任意 $n$ 下的 $T_n(x)$。这将是我们函数求值的核心。\n\n其次，为了满足在不预先知道根位置的情况下找到根的约束，我们必须设计一种策略，将 $n$ 个根中的每一个分离到唯一的界定区间中。我们利用的基本性质是根与极值点的交错特性。众所周知，$T_n(x)$ 在区间 $[-1, 1]$ 上的极值点出现在以下 $n+1$ 个点上：\n$$\nx_k^{\\text{ext}} = \\cos\\left(\\frac{k\\pi}{n}\\right) \\quad \\text{for } k = 0, 1, \\dots, n\n$$\n在这些点上，多项式取值为：\n$$\nT_n(x_k^{\\text{ext}}) = T_n\\left(\\cos\\left(\\frac{k\\pi}{n}\\right)\\right) = \\cos\\left(n \\cdot \\frac{k\\pi}{n}\\right) = \\cos(k\\pi) = (-1)^k\n$$\n由于在任意两个连续的极值点上，函数值在 $+1$ 和 $-1$ 之间交替，因此 $T_n(x_{k}^{\\text{ext}}) \\cdot T_n(x_{k+1}^{\\text{ext}}) = -1$。根据介值定理，在由连续极值点定义的 $n$ 个开区间中的每一个，即 $(\\cos(\\frac{(k+1)\\pi}{n}), \\cos(\\frac{k\\pi}{n}))$（其中 $k=0, 1, \\dots, n-1$），内部都必然存在至少一个根。由于 $T_n(x)$ 是一个 $n$ 次多项式，它最多有 $n$ 个实根。因此，我们已经确定了 $n$ 个不相交的区间，每个区间都恰好包含一个根。这些区间可作为数值求根算法的严格界定区间。\n\n第三，对于每个根的界定区间 $[a, b]$，我们需要一个稳健的数值求解器。虽然二分法（bisection method）保证收敛，但其线性收敛速度较慢。一个更优越的选择是 Brent 方法，它结合了二分法的保证收敛性与割线法（secant method）和逆二次插值（inverse quadratic interpolation）的更快收敛速度。我们将采用 Brent 方法的标准实现，例如 `scipy.optimize.brentq`，来找到每个根，其容差要远小于所要求的 $10^{-10}$ 的绝对精度。对于根的位置 $x$，选择 $10^{-12}$ 的绝对容差是合适的。\n\n最后，验证步骤涉及将 $n$ 个数值计算出的根组成的集合 $\\{x_k^{\\text{num}}\\}$ 与 $n$ 个解析根组成的集合 $\\{x_k^{\\text{ana}}\\}$ 进行比较。$T_n(x)$ 根的解析公式由下式给出：\n$$\nx_k^{\\text{ana}} = \\cos\\left(\\frac{(2k-1)\\pi}{2n}\\right) \\quad \\text{for } k = 1, 2, \\dots, n\n$$\n为了确保正确的比较，数值根集和解析根集都将按升序排序。误差的度量将是排序后列表中对应根之间的最大绝对偏差：\n$$\n\\Delta_{\\text{max}} = \\max_{k=1,\\dots,n} |x_k^{\\text{num}} - x_k^{\\text{ana}}|\n$$\n对于指定的测试集 $n \\in \\{1, 17, 50, 100, 200\\}$ 中的每个 $n$ 值，都将执行此过程。每个 $n$ 得到的 $\\Delta_{\\text{max}}$ 值将构成最终输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Computes the maximum absolute discrepancy between numerically found roots and\n    analytic roots of Chebyshev polynomials T_n(x) for a suite of n values.\n    \"\"\"\n\n    def calculate_T_n(n, x):\n        \"\"\"\n        Computes the Chebyshev polynomial T_n(x) using the three-term recurrence relation.\n        This is numerically stable and efficient.\n\n        Args:\n            n (int): The order of the polynomial.\n            x (float): The point at which to evaluate the polynomial.\n\n        Returns:\n            float: The value of T_n(x).\n        \"\"\"\n        if n == 0:\n            return 1.0\n        if n == 1:\n            return x\n\n        # Iterative calculation using the recurrence relation\n        # T_{k+1}(x) = 2xT_k(x) - T_{k-1}(x)\n        T_k_minus_2 = 1.0\n        T_k_minus_1 = x\n        for _ in range(2, n + 1):\n            T_k = 2.0 * x * T_k_minus_1 - T_k_minus_2\n            T_k_minus_2 = T_k_minus_1\n            T_k_minus_1 = T_k\n        return T_k_minus_1\n\n    # Define the test cases from the problem statement.\n    test_cases = [1, 17, 50, 100, 200]\n    \n    results = []\n    \n    # Absolute accuracy target for the root finding algorithm in x.\n    # Set to a value stricter than the problem's requirement of 1e-10.\n    solver_tolerance = 1e-12\n\n    for n in test_cases:\n        # Define the function whose roots we seek for the current n.\n        # Lambda function captures the current value of n.\n        T_n_func = lambda x: calculate_T_n(n, x)\n\n        numerical_roots = []\n\n        # Find the locations of the extrema of T_n(x), which serve to bracket the roots.\n        # Extrema are at cos(k*pi/n) for k = 0, 1, ..., n.\n        # These points are naturally sorted in descending order from 1 to -1.\n        extrema_points = np.cos(np.arange(n + 1) * np.pi / n)\n\n        # Iterate through the n intervals defined by consecutive extrema.\n        for k in range(n):\n            # The bracket for a single root is [extrema_points[k+1], extrema_points[k]].\n            a = extrema_points[k + 1]\n            b = extrema_points[k]\n            \n            # Use Brent's method to find the root within the bracket [a, b].\n            # brentq is robust and fast, ideal for this task.\n            root = brentq(T_n_func, a, b, xtol=solver_tolerance)\n            numerical_roots.append(root)\n\n        # Convert to a numpy array for vectorized operations.\n        numerical_roots = np.array(numerical_roots)\n        \n        # Sort the numerically found roots in ascending order for consistent comparison.\n        numerical_roots.sort()\n\n        # Compute the analytical roots for comparison.\n        # The formula is cos((2k-1)*pi/(2n)) for k = 1, 2, ..., n.\n        k_vals = np.arange(1, n + 1)\n        analytical_roots = np.cos((2 * k_vals - 1) * np.pi / (2 * n))\n        \n        # The analytic formula produces roots in descending order, so we sort them ascending.\n        analytical_roots.sort()\n        \n        # Compute the maximum absolute difference between the numerical and analytical roots.\n        max_discrepancy = np.max(np.abs(numerical_roots - analytical_roots))\n        results.append(max_discrepancy)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2402261"}, {"introduction": "在掌握了如何为根寻找安全区间后，下一步自然是构建我们自己的混合求解器。这个练习将引导我们实现一个典型的“保护型”混合算法，它将牛顿法或哈雷法等快速开放方法的收敛速度与二分法等封闭方法的收敛保证相结合。更重要的是，本练习引入了计算成本效益分析，通过比较不同阶方法在特定成本模型下的总开销，我们将学会如何判断一个更复杂、更高阶的方法在实践中是否真正“值得”。[@problem_id:2402194]", "problem": "您的任务是设计并评估一种全局收敛的混合求根算法，其中开放部分采用三阶方法。核心设定是，对于一个光滑的非线性标量函数 $f(x)$，在给定一个保证有根的区间 $[a,b]$（满足 $f(a)f(b) < 0$）内，寻找其单根 $x^\\star$。您的设计必须基于以下广为接受的原则：(i) 中值定理，该定理保证如果 $f(a)f(b) < 0$，则在 $[a,b]$ 中至少存在一个根；(ii) 光滑函数的泰勒展开，它为根附近的局部高阶开放方法步骤提供了理论依据。三角函数的角度单位必须是弧度。\n\n您的任务是：\n\n- 从泰勒展开推导，并且不使用任何预先提供的迭代公式，得出一个开放步更新方法，该方法在标准光滑性和非退化条件下对单根能达到三阶局部收敛。此开放步只能使用在当前迭代点计算的 $f(x)$, $f'(x)$ 和 $f''(x)$。\n- 实现一个稳健的混合方法，该方法：\n  - 在每次迭代 $k$ 中维持一个含根区间 $[a_k,b_k]$，满足 $f(a_k)f(b_k) \\le 0$。\n  - 使用您的三阶更新方法，从当前点 $x_k$ 提出一个开放步。\n  - 仅当开放步的结果落在 $(a_k,b_k)$ 区间内，并且严格减小残差的绝对值时，才接受该开放步。否则，必须回退到二分法步骤。\n  - 使用被接受的新点的函数值符号来更新含根区间。\n  - 当绝对残差低于预设容差或区间宽度足够小时终止。\n- 作为比较，还需实现一个基线混合方法，其中开放步是经典的牛顿更新（二阶局部收敛），并采用与上述相同的保护措施和回退机制。\n- 将计算成本建模为求值次数的加权和，其中一次 $f(x)$ 的求值成本为 $c_0$，一次 $f'(x)$ 的成本为 $c_1$，一次 $f''(x)$ 的成本为 $c_2$。计算您的算法执行的每一次求值，并计算每种方法的总加权成本。\n\n使用绝对残差容差 $\\lvert f(x) \\rvert \\le \\varepsilon$（其中 $\\varepsilon = 10^{-12}$）和区间宽度容差 $\\lvert b-a \\rvert \\le \\varepsilon$，并强制最多迭代 $100$ 次。角度必须是弧度。最终答案不涉及物理单位。\n\n测试套件规范。对以下每个测试用例，解析地实现 $f(x)$、$f'(x)$ 和 $f''(x)$。每个用例都包括含根区间 $[a,b]$ 和求值成本 $(c_0,c_1,c_2)$：\n\n- 情况 1：$f(x) = \\cos(x) - x$，$[a,b] = [0,1]$，$(c_0,c_1,c_2) = (1.0,1.0,1.0)$。\n- 情况 2：$f(x) = \\cos(x) - x$，$[a,b] = [0,1]$，$(c_0,c_1,c_2) = (1.0,1.0,10.0)$。\n- 情况 3：$f(x) = x^3 - 2x - 5$，$[a,b] = [2,3]$，$(c_0,c_1,c_2) = (1.0,1.0,1.0)$。\n- 情况 4：$f(x) = \\mathrm{e}^x - 3$，$[a,b] = [0,2]$，$(c_0,c_1,c_2) = (1.0,1.0,5.0)$。\n- 情况 5：$f(x) = \\tanh(x) - 0.5$，$[a,b] = [0,2]$，$(c_0,c_1,c_2) = (1.0,2.0,4.0)$。\n- 情况 6：$f(x) = x^5 - x - 1$，$[a,b] = [1,2]$，$(c_0,c_1,c_2) = (1.0,1.0,3.0)$。\n\n对于每种情况，从区间中点 $x_0 = (a+b)/2$ 开始，运行两种混合求解器（您推导的三阶求解器和基于牛顿法的基线求解器），应用相同的接受标准和回退机制，并计算函数和导数求值的总加权成本。\n\n最终输出要求。您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表。每个条目必须是一个布尔值，表示在该用例指定的成本下，采用三阶开放步的混合方法的总加权成本是否严格低于采用牛顿开放步的混合方法。例如，一个有效的输出可能形如“[True,False,True,True,False,True]”。", "solution": "所给问题是数值分析中一个定义明确的练习，涉及混合求根算法的设计与比较评估。其前提在科学上是合理的，目标清晰且可量化，并且所有必要的数据和约束条件都已提供。因此，该问题被认为是有效的，并将给出完整的解法。\n\n任务是构建一个全局收敛的求根算法，它结合了稳健的区间法（二分法）和快速的三阶收敛开放法。这种混合策略旨在利用开放法的快速局部收敛性，同时保留区间法的保证收敛性。\n\n首先，需要基于泰勒级数展开推导三阶开放步更新，并且只使用函数 $f(x)$ 及其一阶和二阶导数 $f'(x)$ 和 $f''(x)$。设 $x_k$ 是单根 $x^\\star$（其中 $f(x^\\star) = 0$）的当前近似值。$f(x)$ 在 $x_k$ 附近的泰勒级数展开为：\n$$ f(x^\\star) = f(x_k) + (x^\\star - x_k)f'(x_k) + \\frac{(x^\\star-x_k)^2}{2!}f''(x_k) + O\\left((x^\\star-x_k)^3\\right) $$\n令 $f(x^\\star) = 0$，并设 $h = x_{k+1} - x_k$ 为到下一个迭代点的步长，我们使用 $x_{k+1}$ 作为 $x^\\star$ 的改进近似。因此，我们有近似式 $x^\\star - x_k \\approx h$。将此代入截断的级数，得到关于步长 $h$ 的二次方程：\n$$ 0 \\approx f(x_k) + h f'(x_k) + \\frac{h^2}{2} f''(x_k) $$\n为了避免解二次方程，我们可以通过对二次项中的一个因子 $h$ 使用一个低阶近似来线性化这个表达式。构成牛顿法基础的一阶泰勒近似提供了估计 $h \\approx -f(x_k)/f'(x_k)$。将此代入上述方程得到：\n$$ 0 \\approx f(x_k) + h f'(x_k) + \\frac{h}{2} \\left( -\\frac{f(x_k)}{f'(x_k)} \\right) f''(x_k) $$\n对此方程求解 $h$ 得到更新步长：\n$$ h \\left( f'(x_k) - \\frac{f(x_k) f''(x_k)}{2 f'(x_k)} \\right) \\approx -f(x_k) $$\n$$ h = -\\frac{f(x_k)}{f'(x_k) - \\frac{f(x_k) f''(x_k)}{2 f'(x_k)}} $$\n则下一个迭代点为 $x_{k+1} = x_k + h$。该公式被称为 Halley 方法，在标准光滑条件下，它对单根表现出三次（三阶）局部收敛性。\n\n作为比较，需要一个使用经典的二阶牛顿法的基线混合方法。牛顿步是从一阶泰勒展开推导出来的：\n$$ 0 \\approx f(x_k) + (x^\\star - x_k)f'(x_k) $$\n这直接得出了步长 $h = x_{k+1} - x_k \\approx -f(x_k)/f'(x_k)$ 和更新规则：\n$$ x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)} $$\n\n集成了这些开放步的混合算法设计如下。它从一个含根区间 $[a_0, b_0]$（其中 $f(a_0)f(b_0) < 0$）和一个初始猜测 $x_0 = (a_0+b_0)/2$ 开始。在每次迭代 $k$，给定当前区间 $[a_k, b_k]$ 和迭代点 $x_k$：\n1. 从 $x_k$ 提出一个到新点 $x_{open}$ 的开放步，使用 Halley 方法或 Newton 方法计算。这个步骤本身需要对 $f(x_k)$、$f'(x_k)$ 进行求值，对于 Halley 方法，还需要对 $f''(x_k)$ 进行求值。\n2. 提出的步骤需要通过两个保护性条件的验证：\n   a. 区间条件：点 $x_{open}$ 必须严格位于当前含根区间内部，即 $x_{open} \\in (a_k, b_k)$。\n   b. 进展条件：新点的函数绝对值必须严格小于当前点的函数绝对值，即 $|f(x_{open})| < |f(x_k)|$。这需要对 $f$ 在 $x_{open}$ 处进行一次额外的求值。\n3. 如果两个保护性条件都通过，则接受该开放步，并将下一个迭代点设置为 $x_{k+1} = x_{open}$。\n4. 如果开放步被拒绝（由于未能通过保护性条件，或由于数值问题如除以零），算法将回退到二分法步骤。下一个迭代点被设置为区间的中点 $x_{k+1} = (a_k + b_k) / 2$。\n5. 更新含根区间。根据 $f(x_{k+1})$ 的符号，新区间 $[a_{k+1}, b_{k+1}]$ 变为 $[a_k, x_{k+1}]$ 或 $[x_{k+1}, b_k]$，以保持性质 $f(a_{k+1})f(b_{k+1}) \\le 0$。\n6. 当区间宽度 $|b_k - a_k|$ 或函数绝对残差 $|f(x_k)|$ 小于或等于容差 $\\varepsilon = 10^{-12}$，或在达到最大迭代次数 $100$ 次后，过程停止。\n\n计算成本通过函数和导数求值次数的加权和来评估。如果一个算法完成时对 $f(x)$ 进行了 $N_0$ 次求值，对 $f'(x)$ 进行了 $N_1$ 次求值，对 $f''(x)$ 进行了 $N_2$ 次求值，每次求值的成本分别为 $c_0, c_1, c_2$，则总成本 $C$ 计算如下：\n$$ C = N_0 c_0 + N_1 c_1 + N_2 c_2 $$\n这种成本模型使得二阶和三阶混合方法之间可以进行公平比较。问题在于，三阶方法可能减少的迭代次数是否足以抵消在每次尝试开放步时计算二阶导数 $f''(x)$ 的额外成本。", "answer": "```python\nimport numpy as np\n\nclass FuncWithCounter:\n    \"\"\"A wrapper class for a function and its derivatives to count evaluations.\"\"\"\n    def __init__(self, f_def, df_def, d2f_def, costs):\n        self._f = f_def\n        self._df = df_def\n        self._d2f = d2f_def\n        self.costs = costs\n        self.f_evals = 0\n        self.df_evals = 0\n        self.d2f_evals = 0\n\n    def f(self, x):\n        self.f_evals += 1\n        return self._f(x)\n\n    def df(self, x):\n        self.df_evals += 1\n        return self._df(x)\n\n    def d2f(self, x):\n        self.d2f_evals += 1\n        return self._d2f(x)\n\n    def reset(self):\n        \"\"\"Resets all evaluation counters to zero.\"\"\"\n        self.f_evals = 0\n        self.df_evals = 0\n        self.d2f_evals = 0\n\n    def total_cost(self):\n        \"\"\"Computes the total weighted cost of all evaluations.\"\"\"\n        c0, c1, c2 = self.costs\n        return self.f_evals * c0 + self.df_evals * c1 + self.d2f_evals * c2\n\ndef hybrid_solver(f_obj, a_start, b_start, tol, max_iter, open_step_type):\n    \"\"\"\n    Globally convergent hybrid root-finding algorithm.\n\n    Args:\n        f_obj (FuncWithCounter): The function object with evaluation counters.\n        a_start (float): The start of the bracketing interval.\n        b_start (float): The end of the bracketing interval.\n        tol (float): The tolerance for termination.\n        max_iter (int): The maximum number of iterations.\n        open_step_type (str): The type of open step ('newton' or 'halley').\n\n    Returns:\n        float: The total computational cost.\n    \"\"\"\n    a, b = float(a_start), float(b_start)\n    f_obj.reset()\n\n    f_a = f_obj.f(a)\n    f_b = f_obj.f(b)\n\n    if np.sign(f_a) == np.sign(f_b):\n        return float('inf')\n\n    x_cur = (a + b) / 2.0\n    f_cur = f_obj.f(x_cur)\n\n    for _ in range(max_iter):\n        if abs(f_cur) <= tol or (b - a) <= tol:\n            break\n\n        # Propose an open step from the current point\n        open_step_valid = False\n        try:\n            df_cur = f_obj.df(x_cur)\n            if abs(df_cur) < 1e-15: # Avoid division by zero\n                raise ValueError(\"Derivative is too small.\")\n            \n            if open_step_type == 'newton':\n                x_open = x_cur - f_cur / df_cur\n                open_step_valid = True\n            elif open_step_type == 'halley':\n                d2f_cur = f_obj.d2f(x_cur)\n                denom = df_cur - (f_cur * d2f_cur) / (2.0 * df_cur)\n                if abs(denom) < 1e-15: # Avoid division by zero\n                    raise ValueError(\"Halley denominator is too small.\")\n                x_open = x_cur - f_cur / denom\n                open_step_valid = True\n        except (ValueError, ZeroDivisionError):\n            open_step_valid = False\n\n        accepted = False\n        if open_step_valid and (a < x_open < b):\n            f_open = f_obj.f(x_open)\n            if abs(f_open) < abs(f_cur):\n                x_next = x_open\n                f_next = f_open\n                accepted = True\n\n        if not accepted:\n            x_next = (a + b) / 2.0\n            if x_next == x_cur: # Interval is at machine precision limit\n                f_next = f_cur\n            else:\n                f_next = f_obj.f(x_next)\n\n        # Update current point and function value\n        x_cur = x_next\n        f_cur = f_next\n        \n        # Update bracket\n        if np.sign(f_cur) == np.sign(f_a):\n            a = x_cur\n            f_a = f_cur\n        else:\n            b = x_cur\n            f_b = f_cur\n\n    return f_obj.total_cost()\n\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        {'func': lambda x: np.cos(x) - x, 'df': lambda x: -np.sin(x) - 1.0, 'd2f': lambda x: -np.cos(x),\n         'interval': (0.0, 1.0), 'costs': (1.0, 1.0, 1.0)},\n        {'func': lambda x: np.cos(x) - x, 'df': lambda x: -np.sin(x) - 1.0, 'd2f': lambda x: -np.cos(x),\n         'interval': (0.0, 1.0), 'costs': (1.0, 1.0, 10.0)},\n        {'func': lambda x: x**3 - 2.0*x - 5.0, 'df': lambda x: 3.0*x**2 - 2.0, 'd2f': lambda x: 6.0*x,\n         'interval': (2.0, 3.0), 'costs': (1.0, 1.0, 1.0)},\n        {'func': lambda x: np.exp(x) - 3.0, 'df': lambda x: np.exp(x), 'd2f': lambda x: np.exp(x),\n         'interval': (0.0, 2.0), 'costs': (1.0, 1.0, 5.0)},\n        {'func': lambda x: np.tanh(x) - 0.5, 'df': lambda x: 1.0 / (np.cosh(x)**2), 'd2f': lambda x: -2.0 * np.tanh(x) / (np.cosh(x)**2),\n         'interval': (0.0, 2.0), 'costs': (1.0, 2.0, 4.0)},\n        {'func': lambda x: x**5 - x - 1.0, 'df': lambda x: 5.0*x**4 - 1.0, 'd2f': lambda x: 20.0*x**3,\n         'interval': (1.0, 2.0), 'costs': (1.0, 1.0, 3.0)}\n    ]\n\n    results = []\n    TOL = 1e-12\n    MAX_ITER = 100\n\n    for case in test_cases:\n        a, b = case['interval']\n        f_obj = FuncWithCounter(case['func'], case['df'], case['d2f'], case['costs'])\n\n        cost_newton = hybrid_solver(f_obj, a, b, TOL, MAX_ITER, 'newton')\n        cost_halley = hybrid_solver(f_obj, a, b, TOL, MAX_ITER, 'halley')\n        \n        results.append(cost_halley < cost_newton)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2402194"}, {"introduction": "最后，我们将把从单变量方程中学到的技能扩展到更普遍也更具挑战性的非线性方程组。在多维空间中，简单的“括号”概念不再适用，因此我们需要探索一种不同但同样强大的混合策略。这个练习将指导我们结合使用两种方法：首先利用稳健的全局优化方法（梯度下降法）进行几步迭代，将初始猜测值引入一个“好”的区域，然后切换到具有快速局部收敛性的多维牛顿法，从而高效地找到方程组的解。[@problem_id:2402214]", "problem": "请构建一个确定性程序，为下述每个非线性方程组计算一个满足严格残差容差的近似根向量。设方程组写作 $ \\mathbf{f}(\\mathbf{x}) = \\mathbf{0} $，其中 $ \\mathbf{f}: \\mathbb{R}^n \\to \\mathbb{R}^n $ 且 $ \\mathbf{x} \\in \\mathbb{R}^n $。一个有效的近似根 $ \\widehat{\\mathbf{x}} $ 必须满足 $ \\lVert \\mathbf{f}(\\widehat{\\mathbf{x}}) \\rVert_2 \\leq 10^{-10} $，并且必须以四舍五入到八位小数的实数形式报告。这些系统不涉及任何物理单位。如果三角函数内部出现角度，单位均为弧度。您的程序必须使用所提供的函数的精确数学定义，并且必须返回一个单一的、无任何用户交互的确定性输出。\n\n测试套件。对于每个测试用例，都提供了方程组 $ \\mathbf{f}(\\mathbf{x}) = \\mathbf{0} $ 和初始猜测值 $ \\mathbf{x}_0 $。对于每个用例，请从指定的 $ \\mathbf{x}_0 $ 开始，计算一个满足残差容差的近似根 $ \\widehat{\\mathbf{x}} $。如果您的程序在合理的迭代限制内无法找到这样的近似根，它仍然必须产生一个结果；但是，正确性的评估标准要求必须满足残差容差。\n\n- 测试用例 $1$（两个变量）：\n  系统：\n  $$\n  \\begin{cases}\n  f_1(x,y) = e^{x} + y - 1.5 = 0, \\\\\n  f_2(x,y) = x^2 + y - 0.5 = 0,\n  \\end{cases}\n  $$\n  初始猜测值为 $ \\mathbf{x}_0 = (-0.7, 1.0) $。\n\n- 测试用例 $2$（两个变量，存在多个解）：\n  系统：\n  $$\n  \\begin{cases}\n  f_1(x,y) = \\sin(x) - y = 0, \\\\\n  f_2(x,y) = x^2 + y^2 - 0.5 = 0,\n  \\end{cases}\n  $$\n  初始猜测值为 $ \\mathbf{x}_0 = (0.8, 0.6) $。\n\n- 测试用例 $3$（三个变量，两个孤立解）：\n  系统：\n  $$\n  \\begin{cases}\n  f_1(x,y,z) = x + y + z - 1 = 0, \\\\\n  f_2(x,y,z) = x^2 + y^2 + z^2 - 1 = 0, \\\\\n  f_3(x,y,z) = x - y = 0,\n  \\end{cases}\n  $$\n  初始猜测值为 $ \\mathbf{x}_0 = (0.2, 0.2, 0.8) $。\n\n- 测试用例 $4$（两个变量，小尺度耦合）：\n  系统：\n  $$\n  \\begin{cases}\n  f_1(x,y) = x\\,y - 10^{-6} = 0, \\\\\n  f_2(x,y) = x - y = 0,\n  \\end{cases}\n  $$\n  初始猜测值为 $ \\mathbf{x}_0 = (1.0, 0.001) $。\n\n验证。对于每个测试用例，您的程序必须返回一个向量 $ \\widehat{\\mathbf{x}} $，使得 $ \\lVert \\mathbf{f}(\\widehat{\\mathbf{x}}) \\rVert_2 \\leq 10^{-10} $。应使用欧几里得范数 $ \\lVert \\cdot \\rVert_2 $。最终输出中，每个分量必须四舍五入到八位小数。\n\n最终输出格式。您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。每个测试用例的结果必须是其分量的子列表，顺序与上述变量呈现的顺序相同。例如，格式必须是：\n$ [ [x_1,y_1], [x_2,y_2], [x_3,y_3,z_3], [x_4,y_4] ] $\n其中每个 $ x_i, y_i, z_i $ 都四舍五入到八位小数。不应打印任何其他文本。", "solution": "我们需要为 $ \\mathbb{R}^n $ 中的非线性方程组 $ \\mathbf{f}(\\mathbf{x}) = \\mathbf{0} $ 计算高精度的近似根，并从给定的初始猜测值开始。一种利用基本原理的稳健方法建立在两个核心思想之上：源自一阶泰勒展开的牛顿步的局部二次收敛性，以及通过最小化一个价值函数来实现全局化，以确保从较远的初始猜测值也能取得进展。\n\n原理。\n\n1. 方程组的根。一个根 $ \\mathbf{x}^\\star $ 满足 $ \\mathbf{f}(\\mathbf{x}^\\star) = \\mathbf{0} $，其中 $ \\mathbf{f}: \\mathbb{R}^n \\to \\mathbb{R}^n $ 是连续可微的。雅可比矩阵为 $ \\mathbf{J}(\\mathbf{x}) = \\left[ \\partial f_i / \\partial x_j \\right]_{i,j} $。\n\n2. 牛顿方向。牛顿步由一阶泰勒展开得到：\n$$\n\\mathbf{f}(\\mathbf{x} + \\mathbf{s}) \\approx \\mathbf{f}(\\mathbf{x}) + \\mathbf{J}(\\mathbf{x}) \\mathbf{s} .\n$$\n将右侧设为 $ \\mathbf{0} $，得到线性系统：\n$$\n\\mathbf{J}(\\mathbf{x}) \\mathbf{s}_N = - \\mathbf{f}(\\mathbf{x}) ,\n$$\n其解 $ \\mathbf{s}_N $ 即为牛顿方向。如果 $ \\mathbf{J}(\\mathbf{x}^\\star) $ 非奇异且 $ \\mathbf{x} $ 足够接近 $ \\mathbf{x}^\\star $，则迭代 $ \\mathbf{x} \\leftarrow \\mathbf{x} + \\mathbf{s}_N $ 会二次收敛。\n\n3. 价值函数及其梯度。为了从较远的初始猜测值实现全局收敛，一种标准的工具是最小二乘价值函数：\n$$\n\\phi(\\mathbf{x}) = \\tfrac{1}{2} \\lVert \\mathbf{f}(\\mathbf{x}) \\rVert_2^2 .\n$$\n根据链式法则，$ \\phi $ 的梯度为：\n$$\n\\nabla \\phi(\\mathbf{x}) = \\mathbf{J}(\\mathbf{x})^\\top \\mathbf{f}(\\mathbf{x}) .\n$$\n$ \\phi $ 的最速下降方向是 $ -\\nabla \\phi(\\mathbf{x}) $。\n\n4. 混合全局化。两个互补的性质指导着一个稳健的求解器：\n- 远离根时，减小 $ \\phi(\\mathbf{x}) $ 能可靠地改善当前估计的质量。在 $ \\phi $ 上进行几步最速下降可以把 $ \\mathbf{x} $ 移动到一个二阶信息更有用的区域。\n- 在根附近且雅可比矩阵条件良好时，牛顿方向提供快速的（通常是二次的）局部收敛。使用 Armijo 条件沿牛顿方向进行线搜索，可以确保 $ \\phi $ 的充分下降，从而保证全局稳定性。\n\n算法结构。\n\n- 初始化。从给定的 $ \\mathbf{x}_0 $ 开始。定义最大迭代次数 $ N_{\\max} $、一个残差容差 $ \\varepsilon $（此处残差范数目标为 $ \\varepsilon = 10^{-10} $；为安全起见，我们使用一个更严格的内部容差），以及一个基于梯度范数的价值函数下降停止阈值。\n\n- 预处理阶段。对 $ \\phi(\\mathbf{x}) $ 执行少量固定的最速下降步：\n  - 计算 $ \\mathbf{g} = \\nabla \\phi(\\mathbf{x}) = \\mathbf{J}(\\mathbf{x})^\\top \\mathbf{f}(\\mathbf{x}) $。\n  - 使用回溯线搜索找到 $ \\alpha > 0 $，使得\n    $$\n    \\phi(\\mathbf{x} - \\alpha \\mathbf{g}) \\le \\phi(\\mathbf{x}) - c_1 \\alpha \\langle \\mathbf{g}, \\mathbf{g} \\rangle ,\n    $$\n    其中 $ c_1 \\in (0,1) $ 是一个小常数（例如 $ c_1 = 10^{-4} $）。更新 $ \\mathbf{x} \\leftarrow \\mathbf{x} - \\alpha \\mathbf{g} $。这会减小 $ \\phi $ 并提供一个更好的初始估计。\n\n- 主循环（带回退机制的阻尼牛顿法）。对于 $ k = 0, 1, \\dots $，直到收敛：\n  - 计算 $ \\mathbf{f}(\\mathbf{x}) $ 和 $ \\mathbf{J}(\\mathbf{x}) $。\n  - 如果 $ \\lVert \\mathbf{f}(\\mathbf{x}) \\rVert_2 \\le \\varepsilon $，则停止。\n  - 通过求解 $ \\mathbf{J}(\\mathbf{x}) \\mathbf{s}_N = -\\mathbf{f}(\\mathbf{x}) $ 来计算牛顿方向 $ \\mathbf{s}_N $（在条件良好时使用直接求解法，否则使用最小二乘解）。\n  - 计算 $ \\mathbf{g} = \\mathbf{J}(\\mathbf{x})^\\top \\mathbf{f}(\\mathbf{x}) $。如果 $ \\langle \\mathbf{g}, \\mathbf{s}_N \\rangle \\ge 0 $（即牛顿步不是 $ \\phi $ 的下降方向），则将其替换为最速下降方向 $ \\mathbf{s} = -\\mathbf{g} $；否则设置 $ \\mathbf{s} = \\mathbf{s}_N $。\n  - 沿 $ \\mathbf{s} $ 对 $ \\phi $ 执行 Armijo 回溯以找到 $ \\alpha > 0 $，使得\n    $$\n    \\phi(\\mathbf{x} + \\alpha \\mathbf{s}) \\le \\phi(\\mathbf{x}) + c_1 \\alpha \\langle \\mathbf{g}, \\mathbf{s} \\rangle .\n    $$\n    如果对牛顿方向的回溯失败（例如步长下溢），则用最速下降方向重试。\n  - 更新 $ \\mathbf{x} \\leftarrow \\mathbf{x} + \\alpha \\mathbf{s} $。\n\n这种混合策略遵循了基本原理：它利用雅可比线性化来推导牛顿步，并利用残差平方的梯度来定义下降方向和严格的充分下降线搜索。这确保了从给定的初始猜测值开始的全局收敛，同时在根附近恢复了快速的局部收敛。\n\n测试套件的雅可比矩阵。\n\n- 测试用例 $1$：\n  $$\n  \\mathbf{f}(x,y) =\n  \\begin{bmatrix}\n  e^x + y - 1.5 \\\\\n  x^2 + y - 0.5\n  \\end{bmatrix}, \\quad\n  \\mathbf{J}(x,y) =\n  \\begin{bmatrix}\n  e^x & 1 \\\\\n  2x & 1\n  \\end{bmatrix}.\n  $$\n\n- 测试用例 $2$：\n  $$\n  \\mathbf{f}(x,y) =\n  \\begin{bmatrix}\n  \\sin x - y \\\\\n  x^2 + y^2 - 0.5\n  \\end{bmatrix}, \\quad\n  \\mathbf{J}(x,y) =\n  \\begin{bmatrix}\n  \\cos x & -1 \\\\\n  2x & 2y\n  \\end{bmatrix}.\n  $$\n\n- 测试用例 $3$：\n  $$\n  \\mathbf{f}(x,y,z) =\n  \\begin{bmatrix}\n  x + y + z - 1 \\\\\n  x^2 + y^2 + z^2 - 1 \\\\\n  x - y\n  \\end{bmatrix}, \\quad\n  \\mathbf{J}(x,y,z) =\n  \\begin{bmatrix}\n  1 & 1 & 1 \\\\\n  2x & 2y & 2z \\\\\n  1 & -1 & 0\n  \\end{bmatrix}.\n  $$\n\n- 测试用例 $4$：\n  $$\n  \\mathbf{f}(x,y) =\n  \\begin{bmatrix}\n  x y - 10^{-6} \\\\\n  x - y\n  \\end{bmatrix}, \\quad\n  \\mathbf{J}(x,y) =\n  \\begin{bmatrix}\n  y & x \\\\\n  1 & -1\n  \\end{bmatrix}.\n  $$\n\n收敛性与预期结果。\n\n- 测试用例 $1$ 有一个精确解 $ (x,y) = (0, 0.5) $，它满足两个方程。混合方法从给定的初始猜测值迅速收敛到 $ (0, 0.5) $。\n- 测试用例 $2$ 有多个解，由 $ y = \\sin x $ 和圆 $ x^2 + y^2 = 0.5 $ 的交点定义。从给定的 $ y $ 为正的初始猜测值出发，附近的一个解满足 $ x \\approx 0.5109 $ 和 $ y \\approx \\sin(0.5109) \\approx 0.4890 $。求解器会收敛到该解。\n- 测试用例 $3$ 有两个满足 $ x = y $ 的孤立解：$ (x,y,z) = (0,0,1) $ 或 $ (x,y,z) = (2/3, 2/3, -1/3) $。从靠近 $ (0,0,1) $ 的给定初始猜测值出发，求解器会收敛到 $ (0,0,1) $。\n- 测试用例 $4$ 强制 $ x = y $ 和 $ x y = 10^{-6} $，得到精确解 $ (x,y) = (10^{-3}, 10^{-3}) $。尽管雅可比矩阵存在小尺度和轻微病态条件，求解器仍会收敛到此解。\n\n最终的程序实现了所描述的混合策略：对价值函数 $ \\phi(\\mathbf{x}) = \\tfrac{1}{2} \\lVert \\mathbf{f}(\\mathbf{x}) \\rVert_2^2 $ 进行简短的最速下降预处理，然后是带有 Armijo 线搜索的阻尼牛顿法，并且在牛顿步未能成为下降方向或线搜索失败时，回退到最速下降方向。它计算每个测试用例的解以满足所需的残差容差，并打印一行包含按要求格式化和四舍五入的结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef armijo_backtracking(phi, x, d, gTd, phi_x, alpha0=1.0, c1=1e-4, tau=0.5, min_alpha=1e-12, max_backtracks=50):\n    alpha = alpha0\n    for _ in range(max_backtracks):\n        x_new = x + alpha * d\n        phi_new = phi(x_new)\n        if phi_new <= phi_x + c1 * alpha * gTd:\n            return alpha, phi_new\n        alpha *= tau\n        if alpha < min_alpha:\n            break\n    return 0.0, phi_x\n\ndef hybrid_solver(f, J, x0, max_iter=200, tol_res=1e-12, pre_gd_steps=5):\n    x = np.array(x0, dtype=float)\n    # Define merit function and its gradient via J^T f\n    def phi(xv):\n        fv = f(xv)\n        return 0.5 * float(np.dot(fv, fv))\n    def grad_phi(xv):\n        fv = f(xv)\n        Jv = J(xv)\n        return Jv.T @ fv\n\n    # Preconditioning: a few steepest-descent steps on phi\n    for _ in range(pre_gd_steps):\n        fv = f(x)\n        if np.linalg.norm(fv) <= tol_res:\n            return x\n        g = grad_phi(x)\n        gTd = -float(np.dot(g, g))\n        if gTd >= 0:\n            break\n        phi_x = phi(x)\n        d = -g\n        alpha, _ = armijo_backtracking(phi, x, d, gTd, phi_x, alpha0=1.0)\n        if alpha == 0.0:\n            break\n        x = x + alpha * d\n\n    # Main loop: damped Newton with fallback to steepest descent\n    for _ in range(max_iter):\n        fv = f(x)\n        resn = np.linalg.norm(fv)\n        if resn <= tol_res:\n            return x\n        Jx = J(x)\n        # Compute Newton direction: solve J s = -f\n        try:\n            sN = np.linalg.solve(Jx, -fv)\n        except np.linalg.LinAlgError:\n            # Fallback to least squares\n            sN, *_ = np.linalg.lstsq(Jx, -fv, rcond=None)\n        # Gradient of merit phi\n        g = Jx.T @ fv\n        phi_x = 0.5 * float(np.dot(fv, fv))\n        # If Newton direction is not a descent direction for phi, use steepest descent\n        if float(np.dot(g, sN)) >= 0:\n            d = -g\n        else:\n            d = sN\n        gTd = float(np.dot(g, d))\n        # Perform Armijo backtracking; if fails with Newton, try steepest descent\n        alpha, phi_new = armijo_backtracking(phi, x, d, gTd, phi_x, alpha0=1.0)\n        if alpha == 0.0:\n            # Fallback to gradient direction if not already using it\n            d = -g\n            gTd = float(np.dot(g, d))\n            alpha, phi_new = armijo_backtracking(phi, x, d, gTd, phi_x, alpha0=1.0)\n            if alpha == 0.0:\n                # If still failing, take a tiny step to avoid stalling\n                alpha = 1e-8\n                phi_new = phi(x + alpha * d)\n        x = x + alpha * d\n        # Optional: check for tiny step and small gradient as a secondary stop\n        if np.linalg.norm(alpha * d) <= 1e-14 and resn <= tol_res*10:\n            return x\n    return x  # Return best effort\n\n# Define test systems and Jacobians\n\ndef f1(v):\n    x, y = v\n    return np.array([np.exp(x) + y - 1.5,\n                     x*x + y - 0.5], dtype=float)\n\ndef J1(v):\n    x, y = v\n    return np.array([[np.exp(x), 1.0],\n                     [2.0*x,     1.0]], dtype=float)\n\ndef f2(v):\n    x, y = v\n    return np.array([np.sin(x) - y,\n                     x*x + y*y - 0.5], dtype=float)\n\ndef J2(v):\n    x, y = v\n    return np.array([[np.cos(x), -1.0],\n                     [2.0*x,     2.0*y]], dtype=float)\n\ndef f3(v):\n    x, y, z = v\n    return np.array([x + y + z - 1.0,\n                     x*x + y*y + z*z - 1.0,\n                     x - y], dtype=float)\n\ndef J3(v):\n    x, y, z = v\n    return np.array([[1.0, 1.0, 1.0],\n                     [2.0*x, 2.0*y, 2.0*z],\n                     [1.0, -1.0, 0.0]], dtype=float)\n\ndef f4(v):\n    x, y = v\n    return np.array([x*y - 1e-6,\n                     x - y], dtype=float)\n\ndef J4(v):\n    x, y = v\n    return np.array([[y, x],\n                     [1.0, -1.0]], dtype=float)\n\ndef format_vector(vec):\n    return \"[\" + \",\".join(f\"{float(val):.8f}\" for val in vec) + \"]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (f1, J1, np.array([-0.7, 1.0])),\n        (f2, J2, np.array([0.8, 0.6])),\n        (f3, J3, np.array([0.2, 0.2, 0.8])),\n        (f4, J4, np.array([1.0, 0.001])),\n    ]\n\n    results = []\n    for f, J, x0 in test_cases:\n        root = hybrid_solver(f, J, x0, max_iter=200, tol_res=1e-12, pre_gd_steps=5)\n        # Round for output only; internal accuracy meets the tolerance\n        results.append(format_vector(root))\n\n    # Final print statement in the exact required format.\n    print(\"[\" + \",\".join(results) + \"]\")\n\nsolve()\n```", "id": "2402214"}]}