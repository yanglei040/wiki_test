{"hands_on_practices": [{"introduction": "第一个练习将感知器训练的概念置于一个经典的物理问题中：确定谐振子的能量。你将训练一个简单的线性感知器，从位置和速度数据中学习能量函数。本实践超越了基本的梯度下降，引入了一个“惯性”或动量项 ($\\beta (\\mathbf{w}_{t} - \\mathbf{w}_{t-1})$)，让你能够亲手研究这种在机器学习和物理模拟中都至关重要的优化技术，如何显著加速模型的收敛过程。[@problem_id:2425757]", "problem": "要求您实现并分析一个单神经元感知机，该感知机使用基于动量的优化器在确定性物理数据上进行训练。该感知机使用线性（单位）激活函数。训练目标是通过在具有物理意义的特征上拟合一个线性模型，从测量的位置和速度中恢复一维谐振子的能量。\n\n数据与模型规格：\n\n- 考虑一个质量为 $m$、弹簧常数为 $k$ 的一维谐振子，其能量由下式给出\n$$\nE(x,v) = \\tfrac{1}{2} m v^2 + \\tfrac{1}{2} k x^2 \\, .\n$$\n- 使用固定的物理参数 $m = 1\\,\\mathrm{kg}$ 和 $k = 4\\,\\mathrm{N/m}$。\n- 从笛卡尔网格构建一个确定性的训练集\n$$\n\\mathcal{X} = \\{-1.0\\,\\mathrm{m}, -0.5\\,\\mathrm{m}, 0.0\\,\\mathrm{m}, 0.5\\,\\mathrm{m}, 1.0\\,\\mathrm{m}\\},\\quad\n\\mathcal{V} = \\{-1.0\\,\\mathrm{m/s}, 0.0\\,\\mathrm{m/s}, 1.0\\,\\mathrm{m/s}\\} \\, ,\n$$\n形成所有数对 $(x,v) \\in \\mathcal{X} \\times \\mathcal{V}$。对每个数对，计算其目标能量 $E(x,v)$（单位为焦耳）。\n- 使用特征映射\n$$\n\\phi(x,v) = \\begin{bmatrix} x^2 \\\\ v^2 \\\\ 1 \\end{bmatrix},\n$$\n以及一个使用单位激活函数的感知机进行预测\n$$\n\\hat{E}(x,v; \\mathbf{w}) = \\mathbf{w}^\\top \\phi(x,v) \\, ,\n$$\n其中 $\\mathbf{w} \\in \\mathbb{R}^3$ 是可训练的权重向量。除了 $\\phi$ 中的常数特征外，模型没有独立的偏置项。\n\n成本函数与优化：\n\n- 使用均方误差成本函数，并带一个常规的二分之一因子：\n$$\nC(\\mathbf{w}) = \\frac{1}{2N}\\sum_{i=1}^{N}\\left(\\hat{E}_i - E_i\\right)^2 \\, ,\n$$\n其中 $N$ 是样本总数，$\\hat{E}_i = \\mathbf{w}^\\top \\phi(x_i,v_i)$，且 $E_i = E(x_i,v_i)$。\n- 通过动量（惯性）更新法则训练 $\\mathbf{w}$\n$$\n\\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\eta \\nabla C(\\mathbf{w}_{t}) + \\beta \\left(\\mathbf{w}_{t} - \\mathbf{w}_{t-1}\\right),\n$$\n其中学习率 $\\eta > 0$，动量系数 $\\beta \\in [0,1)$。\n- 使用 $\\mathbf{w}_0 = \\mathbf{0}$ 和 $\\mathbf{w}_{-1} = \\mathbf{w}_0$ 进行初始化，以使初始惯性项为零。\n\n停止规则与数值参数：\n\n- 使用终止准则 $C(\\mathbf{w}_t) \\le \\varepsilon$，容差 $\\varepsilon = 10^{-6}\\,\\mathrm{J}^2$（焦耳的平方）。如果在最多 $T_{\\max} = 20000$ 次迭代内未满足此条件，则终止并将该次运行视为未收敛。\n\n测试套件：\n\n评估在以下参数集 $(\\eta,\\beta)$ 下满足停止规则所需的迭代次数：\n\n- 情况 A: $(\\eta,\\beta) = (0.05, 0.00)$。\n- 情况 B: $(\\eta,\\beta) = (0.05, 0.50)$。\n- 情况 C: $(\\eta,\\beta) = (0.05, 0.90)$。\n- 情况 D: $(\\eta,\\beta) = (0.05, 0.99)$。\n\n答案规格：\n\n- 对每种情况，生成一个整数：使得 $C(\\mathbf{w}_t) \\le \\varepsilon$ 成立的最小迭代索引 $t \\in \\{0,1,2,\\dots\\}$。如果算法在 $T_{\\max}$ 次迭代内未能满足容差，则对该情况输出整数 $-1$。\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔结果列表，顺序为 A、B、C、D。\n\n物理与数值单位：\n\n- 目标能量 $E(x,v)$ 必须以焦耳计算。成本 $C(\\mathbf{w})$ 必须解释为焦耳的平方。最终报告的迭代次数是无单位的整数。", "solution": "所述问题已经过验证，并被认定是有效的。它具有科学依据，在数学上是适定的，并且所有参数都已明确指定。它描述了一个以神经网络和计算物理学语言表述的标准线性回归问题，该问题存在唯一的优化解。因此，我们将进行完整的推导和实现。\n\n目标是训练一个单神经元感知机，以学习一维谐振子的能量 $E$。能量由以下函数给出：\n$$\nE(x,v) = \\frac{1}{2} k x^2 + \\frac{1}{2} m v^2\n$$\n其中 $x$ 是位置，$v$ 是速度，$m$ 是质量，$k$ 是弹簧常数。给定的物理参数为 $m = 1\\,\\mathrm{kg}$ 和 $k = 4\\,\\mathrm{N/m}$。\n\n感知机模型使用线性激活函数，并将能量预测为特征的加权和：\n$$\n\\hat{E}(x,v; \\mathbf{w}) = \\mathbf{w}^\\top \\phi(x,v)\n$$\n特征向量 $\\phi(x,v)$ 定义为：\n$$\n\\phi(x,v) = \\begin{bmatrix} x^2 \\\\ v^2 \\\\ 1 \\end{bmatrix}\n$$\n通过这种特征选择，模型的预测为 $\\hat{E} = w_1 x^2 + w_2 v^2 + w_3$。该模型对于真实的能量函数具有精确的表达能力。通过将真实能量 $E = (\\frac{k}{2}) x^2 + (\\frac{m}{2}) v^2 + 0$ 与模型进行比较，我们可以看到当权重向量 $\\mathbf{w} = [w_1, w_2, w_3]^\\top$ 等于真实参数向量 $\\mathbf{w}^*$ 时，可以实现完美拟合：\n$$\n\\mathbf{w}^* = \\begin{bmatrix} k/2 \\\\ m/2 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 4/2 \\\\ 1/2 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2.0 \\\\ 0.5 \\\\ 0.0 \\end{bmatrix}\n$$\n训练过程旨在找到这个最优权重向量 $\\mathbf{w}^*$。\n\n训练数据包含 $N=15$ 个样本，由位置 $\\mathcal{X} = \\{-1.0, -0.5, 0.0, 0.5, 1.0\\}\\,\\mathrm{m}$ 和速度 $\\mathcal{V} = \\{-1.0, 0.0, 1.0\\}\\,\\mathrm{m/s}$ 的笛卡尔积生成。我们可以构建一个大小为 $N \\times 3$ 的设计矩阵 $\\Phi$ 和一个大小为 $N \\times 1$ 的目标向量 $\\mathbf{E}$。$\\Phi$ 的第 $i$ 行是特征向量 $\\phi(x_i, v_i)^\\top$，$\\mathbf{E}$ 的第 $i$ 个元素是真实能量 $E(x_i, v_i)$。\n\n需要最小化的成本函数是均方误差（MSE）：\n$$\nC(\\mathbf{w}) = \\frac{1}{2N}\\sum_{i=1}^{N}\\left(\\mathbf{w}^\\top \\phi_i - E_i\\right)^2 = \\frac{1}{2N} (\\Phi \\mathbf{w} - \\mathbf{E})^\\top (\\Phi \\mathbf{w} - \\mathbf{E})\n$$\n这是关于 $\\mathbf{w}$ 的凸函数，因此基于梯度的优化方法保证能收敛到全局最小值，该最小值对应于 $\\mathbf{w}^*$。为了进行优化更新，需要计算成本函数关于权重 $\\mathbf{w}$ 的梯度。其计算如下：\n$$\n\\nabla_{\\mathbf{w}} C(\\mathbf{w}) = \\frac{1}{2N} \\nabla_{\\mathbf{w}} \\left( (\\Phi \\mathbf{w})^\\top (\\Phi \\mathbf{w}) - 2 (\\Phi \\mathbf{w})^\\top \\mathbf{E} + \\mathbf{E}^\\top \\mathbf{E} \\right)\n$$\n$$\n\\nabla_{\\mathbf{w}} C(\\mathbf{w}) = \\frac{1}{2N} \\left( 2 \\Phi^\\top \\Phi \\mathbf{w} - 2 \\Phi^\\top \\mathbf{E} \\right) = \\frac{1}{N} \\Phi^\\top (\\Phi \\mathbf{w} - \\mathbf{E})\n$$\n优化使用动量更新法则进行，这是梯度下降法的一种变体，旨在加速收敛，尤其是在成本曲面的峡谷区域。在每次迭代 $t$ 中，权重根据以下方式更新：\n$$\n\\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\eta \\nabla C(\\mathbf{w}_{t}) + \\beta (\\mathbf{w}_{t} - \\mathbf{w}_{t-1})\n$$\n这里，$\\eta$ 是学习率，$\\beta$ 是动量系数。初始条件为 $\\mathbf{w}_0 = \\mathbf{0}$ 和 $\\mathbf{w}_{-1} = \\mathbf{w}_0 = \\mathbf{0}$，以确保第一次迭代的动量项为零。\n\n对于每个给定的参数集 $(\\eta, \\beta)$，算法按以下步骤进行：\n1. 初始化权重 $\\mathbf{w}_0 = [0,0,0]^\\top$ 和 $\\mathbf{w}_{-1} = [0,0,0]^\\top$。\n2. 计算初始成本 $C(\\mathbf{w}_0)$。如果 $C(\\mathbf{w}_0) \\le \\varepsilon = 10^{-6}$，则过程以 0 次迭代终止。\n3. 对于从 1 到 $T_{\\max} = 20000$ 的每次迭代 $t$：\n    a. 使用当前权重 $\\mathbf{w}_{t-1}$ 计算梯度 $\\nabla C(\\mathbf{w}_{t-1})$。\n    b. 使用 $\\mathbf{w}_{t-1}$ 和 $\\mathbf{w}_{t-2}$ 的动量更新法则计算下一个权重向量 $\\mathbf{w}_{t}$。注意实现的索引偏移：我们使用 $\\mathbf{w}_{\\text{current}}$ 和 $\\mathbf{w}_{\\text{previous}}$ 来计算 $\\mathbf{w}_{\\text{next}}$。\n    c. 用新计算的权重计算成本 $C(\\mathbf{w}_t)$。\n    d. 如果 $C(\\mathbf{w}_t) \\le \\varepsilon$，则算法已收敛。迭代次数记录为 $t$，当前情况的处理过程终止。\n4. 如果循环完成而成本未能降到 $\\varepsilon$ 以下，则该次运行被视为未收敛，结果记录为-1。\n\n对四个指定的 $(\\eta, \\beta)$ 对分别执行此过程，以确定收敛所需的迭代次数。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes a single-neuron perceptron trained on deterministic physics data\n    using a momentum-based optimizer to recover the energy of a harmonic oscillator.\n    \"\"\"\n    #\n    # --- Problem Constants and Setup ---\n    #\n    \n    # Physical parameters\n    M_KG = 1.0  # Mass in kg\n    K_N_PER_M = 4.0  # Spring constant in N/m\n    \n    # Numerical parameters\n    TOLERANCE_EPSILON = 1e-6  # Convergence tolerance in J^2\n    MAX_ITERATIONS = 20000    # Maximum number of iterations\n\n    #\n    # --- Data Generation ---\n    #\n\n    # Define the Cartesian grid for position and velocity\n    x_vals = np.array([-1.0, -0.5, 0.0, 0.5, 1.0])\n    v_vals = np.array([-1.0, 0.0, 1.0])\n\n    # Create all pairs (x, v) from the grid\n    positions, velocities = np.meshgrid(x_vals, v_vals)\n    positions_flat = positions.flatten()\n    velocities_flat = velocities.flatten()\n\n    num_samples = len(positions_flat)\n\n    # Construct the feature matrix (design matrix) Phi\n    # Features are [x^2, v^2, 1]\n    phi_matrix = np.vstack([\n        positions_flat**2,\n        velocities_flat**2,\n        np.ones(num_samples)\n    ]).T\n\n    # Construct the target energy vector E\n    # E = 0.5 * k * x^2 + 0.5 * m * v^2\n    energy_vector = (0.5 * K_N_PER_M * positions_flat**2 + 0.5 * M_KG * velocities_flat**2).reshape(-1, 1)\n\n    #\n    # --- Optimization ---\n    #\n\n    # Test suite parameters (eta, beta)\n    test_cases = [\n        (0.05, 0.00),  # Case A\n        (0.05, 0.50),  # Case B\n        (0.05, 0.90),  # Case C\n        (0.05, 0.99),  # Case D\n    ]\n\n    results = []\n\n    def calculate_cost(w, phi, E, n):\n        \"\"\"Calculates the mean squared error cost.\"\"\"\n        error = phi @ w - E\n        return (0.5 / n) * np.sum(error**2)\n\n    def calculate_gradient(w, phi, E, n):\n        \"\"\"Calculates the gradient of the cost function.\"\"\"\n        error = phi @ w - E\n        return (1.0 / n) * phi.T @ error\n\n    # Iterate through each test case\n    for eta, beta in test_cases:\n        # Initialize weights for the current run\n        # w_current corresponds to w_t, w_previous corresponds to w_{t-1}\n        w_current = np.zeros((3, 1))\n        w_previous = np.zeros((3, 1))\n\n        # Check cost at initial state (t=0)\n        cost = calculate_cost(w_current, phi_matrix, energy_vector, num_samples)\n        if cost <= TOLERANCE_EPSILON:\n            results.append(0)\n            continue\n            \n        converged = False\n        # Loop for iterations t = 1, 2, ..., T_max\n        for t in range(1, MAX_ITERATIONS + 1):\n            # Calculate gradient at the current weights (w_{t-1} in the problem notation)\n            grad = calculate_gradient(w_current, phi_matrix, energy_vector, num_samples)\n            \n            # Calculate the next weight vector (w_t in problem notation)\n            w_next = w_current - eta * grad + beta * (w_current - w_previous)\n            \n            # Update weights for the next iteration\n            w_previous = w_current\n            w_current = w_next\n            \n            # Check for convergence with the new weights\n            cost = calculate_cost(w_current, phi_matrix, energy_vector, num_samples)\n            if cost <= TOLERANCE_EPSILON:\n                results.append(t)\n                converged = True\n                break\n        \n        # If the loop finished without converging\n        if not converged:\n            results.append(-1)\n            \n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2425757"}, {"introduction": "在探索了如何训练模型之后，我们现在将学习过程本身视为一个具有自身动力学的物理系统。本练习与固态物理学的基石概念——磁滞现象——进行了强有力的类比。你将让一个感知器经受一个由外部参数 $a$ 控制的周期性变化的数据集，并观察到其权重向量 $w_1$ 的演化路径在参数增加和减少时并不重合，从而揭示出一种记忆效应，并形成一个清晰的滞后回环。[@problem_id:2425812]", "problem": "考虑一个带有硬阈值激活函数的感知器，它在一个数据集族上进行在线训练。该数据集族随一个标量控制参数（类似于外部磁场）周期性变化。该感知器有一个权重向量 $\\mathbf{w}\\in \\mathbb{R}^2$ 和一个标量偏置 $b\\in \\mathbb{R}$。对于一个输入 $\\mathbf{x}\\in \\mathbb{R}^2$，感知器的输出为 $\\hat{y}=\\operatorname{sign}(\\mathbf{w}\\cdot \\mathbf{x}+b)$，其中如果 $z\\ge 0$，则 $\\operatorname{sign}(z)=+1$；否则 $\\operatorname{sign}(z)=-1$。学习规则是经典的 Rosenblatt 感知器更新规则：对于一个被错误分类的样本 $(\\mathbf{x},y)$（其中 $y\\in\\{-1,+1\\}$ 且满足 $y(\\mathbf{w}\\cdot \\mathbf{x}+b)\\le 0$），通过 $\\mathbf{w}\\leftarrow \\mathbf{w}+\\eta y \\mathbf{x}$ 和 $b\\leftarrow b+\\eta y$ 进行更新，其中 $\\eta>0$ 是学习率。如果 $y(\\mathbf{w}\\cdot \\mathbf{x}+b)>0$，则不执行更新。\n\n在 $\\mathbb{R}^2$ 的单位圆上定义一个包含 $N$ 个点的固定、确定性训练集：对于 $k\\in\\{0,1,\\dots,N-1\\}$，$\\mathbf{x}_k=\\big(\\cos\\theta_k,\\sin\\theta_k\\big)$，其中 $\\theta_k=\\frac{2\\pi k}{N}$。对于给定的标量偏移量 $a\\in\\mathbb{R}$，标签由 $y_k(a)=\\operatorname{sign}(x_{k,1}+a)$ 定义，其中 $x_{k,1}$ 是 $\\mathbf{x}_k$ 的第一个分量。参数 $a$ 将以均匀的步长从 $-A$ 周期性地变化到 $+A$，然后再回到 $-A$，其中 $A\\ge 0$。\n\n初始化 $\\mathbf{w}=\\mathbf{0}$ 和 $b=0$。对于从 $a_0=-A$ 到 $a_M=+A$ 的正向扫描中的每个 $a$ 值（步长均匀，为 $\\Delta a=\\frac{2A}{M}$，因此对于 $i\\in\\{0,1,\\dots,M\\}$，有 $a_i=-A+i\\Delta a$），使用当前的标签 $y_k(a_i)$，对 $N$ 个样本按固定顺序 $k=0,1,\\dots,N-1$ 执行 $E$ 轮在线训练。在 $a_i$ 处完成这 $E$ 轮训练后，记录数据对 $(a_i,w_1)$，其中 $w_1$ 是 $\\mathbf{w}$ 的第一个分量。然后，对 $i=M-1,M-2,\\dots,0$ 使用相同的过程执行反向扫描，从当前的 $(\\mathbf{w},b)$ 继续训练，并在每个步骤完成 $E$ 轮训练后再次记录 $(a_i,w_1)$。这会在 $(a,w_1)$ 平面中产生一个闭合回路。\n\n将回路面积 $\\mathcal{A}$ 定义为由记录的点序列 $(a,w_1)$沿闭合路径所围成的多边形面积，使用鞋带公式计算。您的程序必须为以下每个测试用例计算 $\\mathcal{A}$，并将面积作为浮点数输出。\n\n测试套件：\n- 用例 1 (非平凡周期性变化): $N=32$, $\\eta=0.2$, $M=60$, $E=1$, $A=0.9$.\n- 用例 2 (无学习): $N=32$, $\\eta=0.0$, $M=60$, $E=1$, $A=0.9$.\n- 用例 3 (无变化): $N=32$, $\\eta=0.2$, $M=60$, $E=1$, $A=0.0$.\n\n要求：\n- 严格按照上述描述实现感知器和训练协议，并设置 $\\operatorname{sign}(0)=+1$。\n- 对于每一轮训练和每一个 $a$ 值，都使用固定的样本顺序 $k=0,1,\\dots,N-1$。\n- 将鞋带公式应用于正向扫描和反向扫描（按此顺序）得到的有序点序列，通过返回到初始点来闭合多边形，从而计算回路面积 $\\mathcal{A}$。\n- 答案是纯数字，不带物理单位，也不带角度。\n- 最终输出格式：您的程序应生成单行输出，其中包含以逗号分隔并用方括号括起来的结果，例如 $[\\mathcal{A}_1,\\mathcal{A}_2,\\mathcal{A}_3]$。", "solution": "对所提出的问题陈述进行了严格验证。\n\n### 步骤 1：提取给定信息\n\n- **感知器定义**：\n  - 权重向量：$\\mathbf{w} \\in \\mathbb{R}^2$\n  - 偏置：$b \\in \\mathbb{R}$\n  - 输入：$\\mathbf{x} \\in \\mathbb{R}^2$\n  - 输出函数：$\\hat{y} = \\operatorname{sign}(\\mathbf{w} \\cdot \\mathbf{x} + b)$\n  - 激活函数：若 $z \\ge 0$，则 $\\operatorname{sign}(z) = +1$；若 $z < 0$，则 $\\operatorname{sign}(z) = -1$。\n\n- **学习规则**：\n  - 更新条件：对于样本 $(\\mathbf{x}, y)$，如果 $y(\\mathbf{w} \\cdot \\mathbf{x} + b) \\le 0$，则进行更新。\n  - 权重更新：$\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta y \\mathbf{x}$\n  - 偏置更新：$b \\leftarrow b + \\eta y$\n  - 学习率：$\\eta > 0$\n\n- **数据集**：\n  - 样本数量：$N$\n  - 样本向量：$\\mathbf{x}_k = (\\cos\\theta_k, \\sin\\theta_k)$，对于 $k \\in \\{0, 1, \\dots, N-1\\}$。\n  - 样本角度：$\\theta_k = \\frac{2\\pi k}{N}$。\n  - 标签生成：$y_k(a) = \\operatorname{sign}(x_{k,1} + a)$，其中 $x_{k,1}$ 是 $\\mathbf{x}_k$ 的第一个分量。\n\n- **训练协议**：\n  - 初始化：$\\mathbf{w} = \\mathbf{0}$， $b = 0$。\n  - 控制参数：$a$ 从 $-A$ 到 $+A$ 再返回，周期性变化。\n  - 正向扫描：$a_i = -A + i \\Delta a$，对于 $i \\in \\{0, 1, \\dots, M\\}$，其中 $\\Delta a = \\frac{2A}{M}$。\n  - 反向扫描：$a_i = -A + i \\Delta a$，对于 $i \\in \\{M-1, M-2, \\dots, 0\\}$。\n  - 每步的轮数：对于每个 $a_i$ 值进行 $E$ 轮训练。\n  - 样本顺序：固定顺序 $k=0, 1, \\dots, N-1$。\n  - 测量：在每个 $a_i$ 处完成 $E$ 轮训练后，记录 $(a_i, w_1)$，其中 $w_1$ 是 $\\mathbf{w}$ 的第一个分量。\n\n- **分析**：\n  - 回路面积 $\\mathcal{A}$：使用鞋带公式对整个周期记录的有序点 $(a, w_1)$ 序列计算。\n\n- **测试用例**：\n  - 用例 1：$N=32$, $\\eta=0.2$, $M=60$, $E=1$, $A=0.9$。\n  - 用例 2：$N=32$, $\\eta=0.0$, $M=60$, $E=1$, $A=0.9$。\n  - 用例 3：$N=32$, $\\eta=0.2$, $M=60$, $E=1$, $A=0.0$。\n\n### 步骤 2：使用提取的信息进行验证\n\n根据既定标准对问题进行分析。\n\n- **科学性**：该问题是计算物理和机器学习领域的一个练习。它使用标准的 Rosenblatt 感知器，这是神经网络中的一个基本模型。问题的框架——一个由外部周期性变化的参数驱动的学习系统——是研究复杂系统中磁滞和记忆效应的常用且有效的范式。它不含伪科学。\n- **良态性 (Well-Posed)**：该问题以数学精度进行了规定。所有参数、初始条件、更新规则以及数据生成和分析的程序都得到了明确定义。算法是确定性的，确保每个测试用例都有唯一的解。\n- **客观性**：问题以客观、技术性的语言陈述，没有主观或含糊的术语。\n\n### 步骤 3：结论与行动\n\n该问题是**有效的**。它是一个定义明确的计算任务，基于机器学习的既定原则及其在物理现象建模中的应用。将提供一个解决方案。\n\n### 解法\n\n任务是模拟一个受周期性变化训练数据集影响的单层感知器，并在驱动参数 ($a$) 和系统响应 ($w_1$) 的参数空间中量化所产生的磁滞回线。解决方案包括直接实现指定的模拟协议，然后进行几何面积计算。\n\n**1. 系统与数据准备**\n\n首先，我们定义系统的静态组件。训练数据输入 $\\{\\mathbf{x}_k\\}$ 是单位圆上的固定点。对于给定的 $N$，每个样本 $k$ 的向量为 $\\mathbf{x}_k = [\\cos(\\frac{2\\pi k}{N}), \\sin(\\frac{2\\pi k}{N})]^T$。这些应预先计算好。\n\n标签 $\\{y_k\\}$ 取决于控制参数 $a$。对于周期中的每个 $a$ 值，根据 $y_k(a) = \\operatorname{sign}(x_{k,1} + a)$ 生成相应的标签。必须严格遵守规则 $\\operatorname{sign}(z)=+1$ (对于 $z \\ge 0$)。\n\n**2. 驱动系统的模拟**\n\n问题的核心是时间演化模拟。“时间”在此上下文中是指控制参数 $a$ 在周期中的演进。系统状态由权重向量 $\\mathbf{w}$ 和偏置 $b$ 描述。\n\n模拟过程如下：\n- 初始化 $\\mathbf{w} = [0, 0]^T$ 和 $b=0$。\n- 为整个周期创建 $a$ 值的有序列表。正向扫描包含从 $a_0 = -A$ 到 $a_M = A$ 的 $M+1$ 个值。反向扫描包含从 $a_{M-1}$ 到 $a_0$ 的 $M$ 个值。$a$ 的总路径将有 $(M+1) + M = 2M+1$ 个步骤。\n- 一个列表 `path_points` 将用于存储记录的数据对 $(a, w_1)$。\n\n然后我们遍历 $a$ 值的有序列表。在每个步骤 $a$：\n- 首先，根据当前 $a$ 值重新计算整个标签集 $\\{y_k(a)\\}$。\n- 然后，执行 $E$ 轮训练。对于每一轮，按固定顺序 $k=0, 1, \\dots, N-1$ 遍历所有数据样本 $(\\mathbf{x}_k, y_k(a))$。\n- 对每个样本，检查感知器的分类。感知器的内部状态为 $z_k = \\mathbf{w} \\cdot \\mathbf{x}_k + b$。如果 $y_k(a) \\cdot z_k \\le 0$，则发生错分。\n- 如果检测到错分，则使用 Rosenblatt 规则更新权重和偏置：\n  $$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta y_k(a) \\mathbf{x}_k $$\n  $$ b \\leftarrow b + \\eta y_k(a) $$\n- 完成当前 $a$ 值的所有 $E$ 轮训练后，我们将数据对 $(a, w_1)$ 附加到 `path_points` 列表中来记录状态。$w_1$ 是当前权重向量 $\\mathbf{w}$ 的第一个分量。\n\n对周期中每个 $a$ 值重复此过程，从一个 $a$ 值到下一个 $a$ 值继续更新 $\\mathbf{w}$ 和 $b$。\n\n**3. 磁滞回线面积计算**\n\n在模拟完成整个周期后，`path_points` 列表将包含 $(a, w_1)$ 平面中一个多边形的 $2M+1$ 个顶点。设这些顶点为 $(x_j, y_j)$，其中 $j=0, \\dots, 2M$，$x_j$ 是 $a$ 值，$y_j$ 是 $w_1$ 值。\n\n该多边形的面积 $\\mathcal{A}$ 使用鞋带公式计算：\n$$ \\mathcal{A} = \\frac{1}{2} \\left| \\sum_{j=0}^{2M} (x_j y_{j+1} - x_{j+1} y_j) \\right| $$\n其中索引是循环的，即 $(x_{2M+1}, y_{2M+1}) = (x_0, y_0)$。此公式可使用向量运算高效实现。具体来说，如果 $\\mathbf{x} = [x_0, \\dots, x_{2M}]$ 且 $\\mathbf{y} = [y_0, \\dots, y_{2M}]$，则总和可计算为 $\\mathbf{x}^T \\mathbf{y}' - \\mathbf{y}^T \\mathbf{x}'$，其中 $\\mathbf{x}'$ 和 $\\mathbf{y}'$ 是 $\\mathbf{x}$ 和 $\\mathbf{y}$ 的循环移位版本。\n\n**4. 测试用例分析**\n\n- **用例 1 ($A=0.9, \\eta=0.2$):** 这代表一般情况。随着 $a$ 的变化，目标分类边界 $x_1 = -a$ 扫过数据点，导致标签翻转。非零学习率 $\\eta$ 使感知器能够适应。感知器的决策边界（由 $\\mathbf{w} \\cdot \\mathbf{x} + b=0$ 定义）试图跟随移动的目标边界。由于学习率有限且一次只处理一个样本，这种适应存在延迟或滞后。这种滞后导致 $a$ 在正向扫描期间 $w_1$ 的路径与反向扫描期间的路径不同，从而形成一个面积 $\\mathcal{A} > 0$ 的非零磁滞回线。\n\n- **用例 2 ($A=0.9, \\eta=0.0$):** 这里，学习率为零。初始化为 0 的权重和偏置将永远不会更新。因此，$w_1$ 在整个模拟过程中保持为 $0$。记录的路径只是在 $a$ 轴上遍历区间 $[-0.9, 0.9]$ 然后返回。这是一个退化多边形（一条线段），其面积为 $\\mathcal{A} = 0$。\n\n- **用例 3 ($A=0.0, \\eta=0.2$):** 在这种情况下，驱动参数的振幅为零，意味着在整个周期中 $a=0$。标签 $y_k = \\operatorname{sign}(x_{k,1})$ 是固定的。感知器在这个相同的静态数据集上重复训练 $(2M+1) \\times E$ 轮。虽然权重 $w_1$ 会从其初始值 0 发生变化，但参数 $a$ 不变。所有记录的点都将是 $(0, w_{1,j})$ 的形式。最终路径是 $w_1$ 轴上的一条垂直线段。线段是退化多边形，其面积为 $\\mathcal{A}=0$。\n\n对这些控制用例的分析为实现T的正确性提供了可靠的检验。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the perceptron hysteresis problem for a given suite of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (nontrivial cyclic variation)\n        {'N': 32, 'eta': 0.2, 'M': 60, 'E': 1, 'A': 0.9},\n        # Case 2 (no learning boundary)\n        {'N': 32, 'eta': 0.0, 'M': 60, 'E': 1, 'A': 0.9},\n        # Case 3 (no variation boundary)\n        {'N': 32, 'eta': 0.2, 'M': 60, 'E': 1, 'A': 0.0},\n    ]\n\n    def run_simulation(N, eta, M, E, A):\n        \"\"\"\n        Runs the full simulation for one set of parameters.\n\n        Args:\n            N (int): Number of points in the dataset.\n            eta (float): Learning rate.\n            M (int): Number of steps in half a cycle.\n            E (int): Number of epochs per step.\n            A (float): Amplitude of the control parameter 'a'.\n\n        Returns:\n            list: A list of (a, w1) tuples representing the path in the parameter space.\n        \"\"\"\n        # Initialize weights and bias\n        w = np.zeros(2)\n        b = 0.0\n\n        # Generate the fixed training set inputs on the unit circle\n        theta_k = (2.0 * np.pi / N) * np.arange(N)\n        x_k = np.stack((np.cos(theta_k), np.sin(theta_k)), axis=1)\n\n        # Define the sign function as specified: sign(z) = +1 if z >= 0, -1 otherwise\n        def sign_func(z):\n            return np.where(z >= 0, 1.0, -1.0)\n\n        # Generate the sequence of 'a' values for the full cycle\n        if M == 0: # Handle edge case where A > 0 but M = 0\n             delta_a = 0.0\n             a_forward = np.array([-A]) if A > 0 else np.array([0.0])\n             a_backward = np.array([])\n        else:\n             delta_a = 2.0 * A / M\n             a_forward = -A + np.arange(M + 1) * delta_a\n             a_backward = -A + np.arange(M - 1, -1, -1) * delta_a\n        \n        a_cycle = np.concatenate((a_forward, a_backward))\n        \n        path_points = []\n        \n        # Main simulation loop\n        for a_val in a_cycle:\n            # Generate labels for the current 'a' value\n            y_k = sign_func(x_k[:, 0] + a_val)\n            \n            # Perform E epochs of training\n            for _ in range(E):\n                for i in range(N):\n                    # Calculate activation\n                    activation = np.dot(w, x_k[i]) + b\n                    \n                    # Check for misclassification and update\n                    if y_k[i] * activation <= 0:\n                        w += eta * y_k[i] * x_k[i]\n                        b += eta * y_k[i]\n            \n            # Record the point (a, w1)\n            path_points.append((a_val, w[0]))\n            \n        return path_points\n\n    def shoelace_area(points):\n        \"\"\"\n        Calculates the area of a polygon using the shoelace formula.\n        \n        Args:\n            points (list): A list of (x, y) tuples for the polygon vertices.\n\n        Returns:\n            float: The area of the polygon.\n        \"\"\"\n        if len(points) < 3:\n            return 0.0\n\n        # Convert list of tuples to a NumPy array for vectorized operations\n        poly = np.array(points)\n        x = poly[:, 0]\n        y = poly[:, 1]\n        \n        # Shoelace formula implementation using NumPy for efficiency\n        # Area = 0.5 * |(x0*y1 + x1*y2 + ...) - (y0*x1 + y1*x2 + ...)|\n        area = 0.5 * np.abs(np.dot(x, np.roll(y, -1)) - np.dot(y, np.roll(x, -1)))\n        \n        return area\n\n    results = []\n    for case in test_cases:\n        path = run_simulation(**case)\n        area = shoelace_area(path)\n        results.append(area)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```", "id": "2425812"}, {"introduction": "在我们的最后一个实践中，我们将超越静态感知器模型，探索一个更具生物物理真实性的动态模型。本练习将介绍泄漏整合发放（Leaky Integrate-and-Fire, LIF）神经元，这是计算神经科学中的一个基石模型，其输出不再是单个数值，而是随时间发出的一系列离散脉冲。通过模拟该神经元对时变输入的响应，你将深入理解物理系统如何通过时间动态进行计算，这是一个对理解神经网络和其它复杂物理系统都至关重要的概念。[@problem_id:2425782]", "problem": "实现一个单神经元感知机，其激活机制是受生物启发的漏积分放电 (Leaky Integrate-and-Fire, LIF) 模型。该神经元接收一个时间序列的输入，并产生一串输出脉冲。时间是离散的，所有物理参数都以指定的单位给出。该模型定义如下。\n\n一个 LIF 感知机具有膜电位 $V$，它在离散时间步 $t \\in \\{0,1,\\dots,T-1\\}$ 中以固定的步长 $\\Delta t$（单位为 $\\mathrm{ms}$）进行更新。在时间步 $t$ 的突触输入电流为\n$$\nI(t) = \\sum_{i=1}^{d} w_i\\,x_i(t) + b\n$$\n其中 $d$ 是输入数量，$w_i$ 是权重（单位为 $\\mathrm{nA}$/单位输入），$x_i(t)$ 是输入振幅（无量纲），$b$ 是偏置电流（单位为 $\\mathrm{nA}$）。膜电位 $V$（单位为 $\\mathrm{mV}$）通过对 LIF 常微分方程进行前向欧拉积分来更新：\n$$\nV \\leftarrow V + \\frac{\\Delta t}{C}\\left(-g_L\\,(V - E_L) + I(t)\\right)\n$$\n其中 $C$ 是膜电容（单位为 $\\mathrm{nF}$），$g_L$ 是漏电导（单位为 $\\mu \\mathrm{S}$），$E_L$ 是漏泄反转电位（单位为 $\\mathrm{mV}$）。当且仅当神经元不处于不应期状态，并且更新后的膜电位满足以下条件时，才会发出一个脉冲：\n$$\nV \\ge V_{\\mathrm{th}}\n$$\n其中 $V_{\\mathrm{th}}$ 是阈值（单位为 $\\mathrm{mV}$）。在时间步 $t$ 产生一个脉冲后，记录一个在时间 $t\\,\\Delta t$（单位为 $\\mathrm{ms}$）的脉冲，然后重置 $V \\leftarrow V_{\\mathrm{reset}}$（单位为 $\\mathrm{mV}$），并在接下来的 $\\tau_{\\mathrm{ref}}$ 个步长内进入绝对不应期状态。在此期间，不进行积分，且 $V$ 被钳制在 $V_{\\mathrm{reset}}$。初始条件为 $V(0) = V_{\\mathrm{reset}}$，并且在 $t=0$ 时神经元不处于不应期。在以下所有测试用例中，取 $E_L = 0\\,\\mathrm{mV}$。\n\n您的任务是编写一个程序，针对以下每个测试用例，计算在指定时间范围内产生的脉冲时间列表（每个时间单位为 $\\mathrm{ms}$）。将每个脉冲时间表示为毫秒单位的整数。您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素是对应测试用例的脉冲时间列表（例如 [[1,3],[],[2]]）。\n\n测试套件：\n\n- 测试用例 1（恒定驱动下的规律性脉冲发放，带不应期）：\n  - 参数：$\\Delta t = 1\\,\\mathrm{ms}$, $T = 50$, $d=1$, $w_1 = 1.0\\,\\mathrm{nA}$, $b = 0.0\\,\\mathrm{nA}$, $C=1.0\\,\\mathrm{nF}$, $g_L=0.05\\,\\mu\\mathrm{S}$, $E_L=0.0\\,\\mathrm{mV}$, $V_{\\mathrm{th}}=1.0\\,\\mathrm{mV}$, $V_{\\mathrm{reset}}=0.0\\,\\mathrm{mV}$, $\\tau_{\\mathrm{ref}}=3$ 步。\n  - 输入：对于所有 $t \\in \\{0,1,\\dots,49\\}$，$x_1(t) = 0.4$。\n\n- 测试用例 2（阈下恒定驱动，无脉冲）：\n  - 参数：$\\Delta t = 1\\,\\mathrm{ms}$, $T = 50$, $d=1$, $w_1 = 1.0\\,\\mathrm{nA}$, $b = 0.0\\,\\mathrm{nA}$, $C=1.0\\,\\mathrm{nF}$, $g_L=0.05\\,\\mu\\mathrm{S}$, $E_L=0.0\\,\\mathrm{mV}$, $V_{\\mathrm{th}}=1.0\\,\\mathrm{mV}$, $V_{\\mathrm{reset}}=0.0\\,\\mathrm{mV}$, $\\tau_{\\mathrm{ref}}=3$ 步。\n  - 输入：对于所有 $t \\in \\{0,1,\\dots,49\\}$，$x_1(t) = 0.02$。\n\n- 测试用例 3（两次更新精确过阈，无不应期）：\n  - 参数：$\\Delta t = 1\\,\\mathrm{ms}$, $T = 10$, $d=1$, $w_1 = 1.0\\,\\mathrm{nA}$, $b = 0.0\\,\\mathrm{nA}$, $C=1.0\\,\\mathrm{nF}$, $g_L=0.05\\,\\mu\\mathrm{S}$, $E_L=0.0\\,\\mathrm{mV}$, $V_{\\mathrm{th}}=1.0\\,\\mathrm{mV}$, $V_{\\mathrm{reset}}=0.0\\,\\mathrm{mV}$, $\\tau_{\\mathrm{ref}}=0$ 步。\n  - 输入：对于所有 $t \\in \\{0,1,\\dots,9\\}$，$x_1(t) = I^\\star$，其中 $I^\\star = \\dfrac{0.05}{1 - (1-0.05)^2} \\approx 0.5128205128205128$。\n\n- 测试用例 4（兴奋性驱动，带有抑制性暂停）：\n  - 参数：$\\Delta t = 1\\,\\mathrm{ms}$, $T = 25$, $d=2$, $w_1 = 1.0\\,\\mathrm{nA}$, $w_2 = -1.0\\,\\mathrm{nA}$, $b = 0.0\\,\\mathrm{nA}$, $C=1.0\\,\\mathrm{nF}$, $g_L=0.05\\,\\mu\\mathrm{S}$, $E_L=0.0\\,\\mathrm{mV}$, $V_{\\mathrm{th}}=1.0\\,\\mathrm{mV}$, $V_{\\mathrm{reset}}=0.0\\,\\mathrm{mV}$, $\\tau_{\\mathrm{ref}}=1$ 步。\n  - 输入：\n    - 当 $t \\in \\{0,1,\\dots,19\\}$ 时，$x_1(t) = 1.0$，否则 $x_1(t)=0.0$。\n    - 当 $t \\in \\{10,11,12,13,14\\}$ 时，$x_2(t) = 1.0$，否则 $x_2(t)=0.0$。\n\n- 测试用例 5（高漏电抑制脉冲发放）：\n  - 参数：$\\Delta t = 1\\,\\mathrm{ms}$, $T = 30$, $d=1$, $w_1 = 1.0\\,\\mathrm{nA}$, $b = 0.0\\,\\mathrm{nA}$, $C=1.0\\,\\mathrm{nF}$, $g_L=0.5\\,\\mu\\mathrm{S}$, $E_L=0.0\\,\\mathrm{mV}$, $V_{\\mathrm{th}}=1.0\\,\\mathrm{mV}$, $V_{\\mathrm{reset}}=0.0\\,\\mathrm{mV}$, $\\tau_{\\mathrm{ref}}=2$ 步。\n  - 输入：对于所有 $t \\in \\{0,1,\\dots,29\\}$，$x_1(t) = 0.4$。\n\n最终输出格式：您的程序应生成一行包含 Python 风格的列表的列表，其中第 $k$ 个内部列表包含第 $k$ 个测试用例的脉冲时间（单位为 $\\mathrm{ms}$，为整数）。例如 [[2,8],[],[1,3,5]]。", "solution": "问题陈述已经过严格验证，并被确定为有效。它具有科学依据，问题设定良好，没有矛盾或含糊之处。所描述的模型是一个标准的漏积分放电 (Leaky Integrate-and-Fire, LIF) 神经元，这是计算神经科学中的一个基本构造。所有参数、单位、初始条件和动力学规则都以足够的精度指定，从而可以得到唯一且可验证的解。因此，我们将进行形式化的推导和模拟。\n\n单神经元感知机的状态由其膜电位 $V$ 定义，它在大小为 $\\Delta t$ 的离散时间步 $t \\in \\{0, 1, \\dots, T-1\\}$ 上演化。该演化过程遵循电容器上的电荷平衡原理，其中包含漏电流和外部突触输入电流 $I(t)$。\n\n在时间步 $t$ 的输入电流是 $d$ 个外部输入 $x_i(t)$ 的加权和，再加上一个偏置电流 $b$：\n$$\nI(t) = \\sum_{i=1}^{d} w_i\\,x_i(t) + b\n$$\n膜电位 $V$ 根据 LIF 微分方程的前向欧拉离散化进行更新：\n$$\n\\frac{dV}{dt} = \\frac{1}{C}\\left(-g_L\\,(V - E_L) + I(t)\\right)\n$$\n这得到了离散更新规则：\n$$\nV(t+\\Delta t) \\approx V(t) + \\frac{\\Delta t}{C}\\left(-g_L\\,(V(t) - E_L) + I(t)\\right)\n$$\n鉴于问题陈述中新值由箭头表示，我们维持使用 $V_{t}$ 表示在时间步 $t$ 开始时的电位， $V_{t+1}$ 表示在时间步 $t$ 内更新后的电位。\n$$\nV_{t+1} \\leftarrow V_{t} + \\frac{\\Delta t}{C}\\left(-g_L\\,(V_{t} - E_L) + I(t)\\right)\n$$\n所有参数（$C$、$g_L$、$E_L$、$\\Delta t$）和变量（$V$、$I$）都以一组一致的生物物理单位（$\\mathrm{nF}$、$\\mu\\mathrm{S}$、$\\mathrm{mV}$、$\\mathrm{ms}$、$\\mathrm{nA}$）给出，确保了量纲的正确性而无需进行单位转换，因为 $\\frac{\\mathrm{ms}}{\\mathrm{nF}} \\cdot \\mu\\mathrm{S}$ 是无量纲的，而 $\\frac{\\mathrm{ms}}{\\mathrm{nF}} \\cdot \\mathrm{nA} = \\mathrm{mV}$。在指定值 $E_L=0\\,\\mathrm{mV}$ 的情况下，更新规则简化为：\n$$\nV_{t+1} \\leftarrow V_{t} \\left(1 - \\frac{\\Delta t \\cdot g_L}{C}\\right) + \\frac{\\Delta t}{C} I(t)\n$$\n\n神经元的激活机制由一个脉冲发放规则定义。如果在时间步 $t$ 更新后的电位 $V_{t+1}$ 达到或超过阈值 $V_{\\mathrm{th}}$，并且神经元不处于不应期状态，则触发一个脉冲。\n形式上，如果 $V_{t+1} \\ge V_{\\mathrm{th}}$：\n1.  在时间 $t \\cdot \\Delta t$ 记录一个脉冲。\n2.  膜电位被重置为 $V_{\\mathrm{reset}}$。\n3.  神经元在接下来的 $\\tau_{\\mathrm{ref}}$ 个时间步内进入绝对不应期。在此期间（从时间步 $t+1$ 到 $t+\\tau_{\\mathrm{ref}}$），电位被钳制在 $V_{\\mathrm{reset}}$，并且不发生积分或进一步的脉冲发放。\n\n每个测试用例的模拟算法如下：\n1.  初始化膜电位 $V_0 = V_{\\mathrm{reset}}$，一个不应期计数器 `ref_counter` 为 $0$，以及一个用于存储脉冲时间的空列表。\n2.  根据测试用例的规范构建输入信号矩阵 $x_i(t)$。\n3.  对每个时间步 $t$ 从 $0$ 到 $T-1$ 进行迭代：\n    a. 检查不应期状态。如果 `ref_counter` &gt; 0，则将 $V_t$ 钳制在 $V_{\\mathrm{reset}}$，将 `ref_counter` 减一，并进入下一个时间步。\n    b. 如果不处于不应期（`ref_counter` = 0），则计算输入电流 $I(t) = \\sum_{i} w_i x_i(t) + b$。\n    c. 基于当前电位 $V_t$，使用前向欧拉规则更新膜电位 $V_{t+1}$。\n    d. 检查是否有脉冲：如果 $V_{t+1} \\ge V_{\\mathrm{th}}$，则记录脉冲时间 $t \\cdot \\Delta t$，将电位重置为 $V_{t+1} = V_{\\mathrm{reset}}$，并设置 `ref_counter` $= \\tau_{\\mathrm{ref}}$。\n    e. 如果没有脉冲发生，则下一个时间步的电位就是 $V_{t+1}$。\n4.  循环完成后，收集到的脉冲时间列表即为该测试用例的结果。\n\n此过程是确定性的，并将为所提供的五个测试用例中的每一个实现。最终输出是所得脉冲时间列表的集合。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Leaky Integrate-and-Fire neuron simulation problem for a suite of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1 (regular spiking)\n        {\n            \"params\": {\"dt\": 1.0, \"T\": 50, \"d\": 1, \"w\": np.array([1.0]), \"b\": 0.0,\n                       \"C\": 1.0, \"gL\": 0.05, \"EL\": 0.0, \"V_th\": 1.0, \"V_reset\": 0.0,\n                       \"tau_ref\": 3},\n            \"input_gen\": lambda T, d: np.full((d, T), 0.4)\n        },\n        # Test case 2 (subthreshold)\n        {\n            \"params\": {\"dt\": 1.0, \"T\": 50, \"d\": 1, \"w\": np.array([1.0]), \"b\": 0.0,\n                       \"C\": 1.0, \"gL\": 0.05, \"EL\": 0.0, \"V_th\": 1.0, \"V_reset\": 0.0,\n                       \"tau_ref\": 3},\n            \"input_gen\": lambda T, d: np.full((d, T), 0.02)\n        },\n        # Test case 3 (exact threshold crossing)\n        {\n            \"params\": {\"dt\": 1.0, \"T\": 10, \"d\": 1, \"w\": np.array([1.0]), \"b\": 0.0,\n                       \"C\": 1.0, \"gL\": 0.05, \"EL\": 0.0, \"V_th\": 1.0, \"V_reset\": 0.0,\n                       \"tau_ref\": 0},\n            \"input_gen\": lambda T, d: np.full((d, T), 0.05 / (1 - (1 - 0.05)**2))\n        },\n        # Test case 4 (inhibitory pause)\n        {\n            \"params\": {\"dt\": 1.0, \"T\": 25, \"d\": 2, \"w\": np.array([1.0, -1.0]), \"b\": 0.0,\n                       \"C\": 1.0, \"gL\": 0.05, \"EL\": 0.0, \"V_th\": 1.0, \"V_reset\": 0.0,\n                       \"tau_ref\": 1},\n            \"input_gen\": lambda T, d: (\n                x := np.zeros((d, T)),\n                x.__setitem__((0, slice(0, 20)), 1.0),\n                x.__setitem__((1, slice(10, 15)), 1.0),\n                x\n            )[-1]\n        },\n        # Test case 5 (high leak)\n        {\n            \"params\": {\"dt\": 1.0, \"T\": 30, \"d\": 1, \"w\": np.array([1.0]), \"b\": 0.0,\n                       \"C\": 1.0, \"gL\": 0.5, \"EL\": 0.0, \"V_th\": 1.0, \"V_reset\": 0.0,\n                       \"tau_ref\": 2},\n            \"input_gen\": lambda T, d: np.full((d, T), 0.4)\n        }\n    ]\n\n    def simulate_lif(params, input_gen):\n        \"\"\"Simulates a single LIF neuron for one test case.\"\"\"\n        dt, T, d, w, b, C, gL, EL, V_th, V_reset, tau_ref = params.values()\n        \n        inputs = input_gen(T, d)\n        \n        v = V_reset\n        refractory_counter = 0\n        spike_times = []\n\n        for t in range(T):\n            if refractory_counter > 0:\n                v = V_reset\n                refractory_counter -= 1\n                continue\n\n            # Calculate input current I(t)\n            current_input = inputs[:, t]\n            i_t = np.dot(w, current_input) + b\n            \n            # Update membrane potential V using forward Euler\n            # Note: The problem statement uses V on the right side, so it means V at the start of the step\n            v_old = v\n            v = v_old + (dt / C) * (-gL * (v_old - EL) + i_t)\n\n            # Check for spike\n            if v >= V_th:\n                spike_times.append(int(t * dt))\n                v = V_reset\n                refractory_counter = tau_ref\n\n        return spike_times\n\n    results = []\n    for case in test_cases:\n        result = simulate_lif(case[\"params\"], case[\"input_gen\"])\n        results.append(result)\n    \n    # Format the output as a compact string representation of a list of lists.\n    string_lists = [f\"[{','.join(map(str, s))}]\" for s in results]\n    final_output = f\"[{','.join(string_lists)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2425782"}]}