{"hands_on_practices": [{"introduction": "在实验数据分析中，一个常见的任务是搜寻已知的特定信号特征。例如，在粒子物理学中，这些特征可能对应于特定的粒子衰变模式。这个练习将这样一个物理事件抽象成一个图论问题，要求你编写一个精确的模式匹配算法来识别模拟数据中一个特定的费曼图拓扑结构 [@problem_id:2425431]。通过这个实践，你将锻炼如何将一个物理模式转化为一套具体的计算规则，这是计算科学家的一项核心技能。", "problem": "给定一个简化的、图论的表述，用于在模拟的散射事件中识别一个特定的费曼图拓扑。每个模拟事件都表示为一个有限、简单、无向、节点标记的图。该图由一个对称邻接矩阵 $\\mathbf{A} \\in \\{0,1\\}^{n \\times n}$（对角线为零）和一个长度为 $n$ 的标签向量 $\\mathbf{L}$ 指定，其中 $\\mathbf{L}[i]$ 是节点 $i$ 的标签。标签从有限集合 $\\{\\mathrm{Z}, \\mathrm{e^+}, \\mathrm{e^-}, \\mu^+, \\mu^-, \\gamma\\}$ 中选取；在程序表示中，这些是ASCII字符串 $\\texttt{'Z'}$、$\\texttt{'e+'}$、$\\texttt{'e-'}$、$\\texttt{'mu+'}$、$\\texttt{'mu-'}$ 和 $\\texttt{'gamma'}$。\n\n目标拓扑是一个特定的导出子图，它抽象了一个$\\mathrm{s}$-道共振：一个包含5个顶点的星形图（与 $K_{1,4}$ 同构），其中心节点标记为 $\\mathrm{Z}$，并精确连接到四个叶节点，这四个叶节点的标签分别为 $\\mathrm{e^+}$、$\\mathrm{e^-}$、$\\mu^+$ 和 $\\mu^-$，每个标签恰好出现一次。在此导出子图中，叶节点之间没有边。形式上，一次出现是指在节点集 $\\{c, \\ell_1, \\ell_2, \\ell_3, \\ell_4\\}$ 上的一个导出子图，使得：\n- $\\mathbf{L}[c] = \\mathrm{Z}$，\n- 在整个图中，$\\deg(c) = 4$，且其邻居集恰好是 $\\{\\ell_1, \\ell_2, \\ell_3, \\ell_4\\}$，\n- $\\{\\mathbf{L}[\\ell_1], \\mathbf{L}[\\ell_2], \\mathbf{L}[\\ell_3], \\mathbf{L}[\\ell_4]\\}$ 作为一个多重集等于 $\\{\\mathrm{e^+}, \\mathrm{e^-}, \\mu^+, \\mu^-\\}$，\n- 对于 $\\{1,2,3,4\\}$ 中所有不同的 $i,j$，$\\mathbf{A}[\\ell_i,\\ell_j] = 0$。\n\n你的任务是编写一个完整的程序来：\n- 对于每个事件，计算实现这种导出子图的不同中心节点 $c$ 的数量。每个符合条件的中心节点只计数一次，不考虑其邻居的顺序。\n\n你可以依赖的基本原理：\n- 一个简单无向图由一个对称的、对角线为零的 $\\{0,1\\}$ 邻接矩阵表示。\n- 节点 $i$ 的度 $\\deg(i)$ 等于 $\\sum_{j=1}^{n} \\mathbf{A}[i,j]$。\n- 节点集 $S$ 上的导出子图精确地包含了原图中存在于 $S$ 中节点之间的所有边。\n\n测试套件。使用以下6个事件，每个事件由其邻接矩阵 $\\mathbf{A}^{(k)}$ 和标签向量 $\\mathbf{L}^{(k)}$ 指定。\n\n事件 $1$ ($n=5$)。一个有效的以Z为中心的星形图。\n$$\n\\mathbf{A}^{(1)}=\n\\begin{bmatrix}\n0 & 1 & 1 & 1 & 1\\\\\n1 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0\n\\end{bmatrix},\\quad\n\\mathbf{L}^{(1)}=(\\mathrm{Z}, \\mathrm{e^+}, \\mathrm{e^-}, \\mu^+, \\mu^-).\n$$\n\n事件 $2$ ($n=10$)。两个不相交的有效星形图。\n$$\n\\mathbf{A}^{(2)}=\n\\begin{bmatrix}\n0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1\\\\\n0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\n\\end{bmatrix},\\quad\n\\mathbf{L}^{(2)}=(\\mathrm{Z}, \\mathrm{e^+}, \\mathrm{e^-}, \\mu^+, \\mu^-,\n\\mathrm{Z}, \\mathrm{e^+}, \\mathrm{e^-}, \\mu^+, \\mu^-).\n$$\n\n事件 $3$ ($n=5$)。一个以Z为中心的星形图，但叶节点多重集不正确（重复的 $\\mathrm{e^+}$ 和缺失的 $\\mu^+$），不应被计数。\n$$\n\\mathbf{A}^{(3)}=\n\\begin{bmatrix}\n0 & 1 & 1 & 1 & 1\\\\\n1 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0\n\\end{bmatrix},\\quad\n\\mathbf{L}^{(3)}=(\\mathrm{Z}, \\mathrm{e^+}, \\mathrm{e^-}, \\mathrm{e^+}, \\mu^-).\n$$\n\n事件 $4$ ($n=6$)。一个度为5的Z节点，连接到四个正确的叶节点和一个额外的 $\\gamma$ 邻居；违反了度约束，不应被计数。\n$$\n\\mathbf{A}^{(4)}=\n\\begin{bmatrix}\n0 & 1 & 1 & 1 & 1 & 1\\\\\n1 & 0 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0 & 0\n\\end{bmatrix},\\quad\n\\mathbf{L}^{(4)}=(\\mathrm{Z}, \\mathrm{e^+}, \\mathrm{e^-}, \\mu^+, \\mu^-, \\gamma).\n$$\n\n事件 $5$ ($n=5$)。一个以Z为中心的星形图，但叶节点之间存在额外的边（破坏了导出星形图的条件），不应被计数。\n$$\n\\mathbf{A}^{(5)}=\n\\begin{bmatrix}\n0 & 1 & 1 & 1 & 1\\\\\n1 & 0 & 0 & 1 & 0\\\\\n1 & 0 & 0 & 0 & 0\\\\\n1 & 1 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0\n\\end{bmatrix},\\quad\n\\mathbf{L}^{(5)}=(\\mathrm{Z}, \\mathrm{e^+}, \\mathrm{e^-}, \\mu^+, \\mu^-).\n$$\n\n事件 $6$ ($n=5$)。一个以γ为中心的星形图，拥有正确的叶节点多重集；违反了中心标签约束，不应被计数。\n$$\n\\mathbf{A}^{(6)}=\n\\begin{bmatrix}\n0 & 1 & 1 & 1 & 1\\\\\n1 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0\n\\end{bmatrix},\\quad\n\\mathbf{L}^{(6)}=(\\gamma, \\mathrm{e^+}, \\mathrm{e^-}, \\mu^+, \\mu^-).\n$$\n\n你的程序必须实现由导出子图定义所蕴含的识别规则，并为每个事件 $k \\in \\{1,2,3,4,5,6\\}$ 计算实现目标拓扑的不同中心的整数数量 $c^{(k)}$。最终输出必须是一行，包含所有六个计数，以逗号分隔并用方括号括起来，例如，格式应如 $\\texttt{[result1,result2,result3,result4,result5,result6]}$。\n\n此问题不需要物理单位或角度单位。所有答案均为整数。", "solution": "该问题是有效的。它是一个定义明确的图上计算模式识别任务，其基础是实验物理数据分析中使用的传统抽象模型。所给出的条件精确、完整且逻辑一致。\n\n任务是为6个给定的基于图的事件中的每一个，计算可作为特定导出子图拓扑中心的节点数量。该拓扑模拟了一个 s-道共振，其中一个 $\\texttt{'Z'}$ 玻色子衰变为四个特定的轻子。设给定事件由一个对称邻接矩阵 $\\mathbf{A}$ 和一个用于具有 $n$ 个顶点的图的节点标签向量 $\\mathbf{L}$ 表示。算法必须识别并计算满足一组严格标准的独立节点 $c$ 的数量。\n\n解决方案的核心是一种算法，该算法遍历给定图的每个节点 $i$（其中 $i \\in \\{0, 1, \\dots, n-1\\}$），并将每个节点视为一个潜在的中心 $c$。对于每个候选节点 $i$，会执行一系列验证检查。如果任何检查失败，该节点将被取消资格，算法继续处理下一个候选节点。只有通过所有检查的节点，计数器才会增加。\n\n候选中心节点 $i$ 的验证步骤如下：\n\n$1$。 **中心标签验证**：第一个条件涉及中心粒子的身份。候选节点 $\\mathbf{L}[i]$ 的标签必须是 $\\texttt{'Z'}$。如果 $\\mathbf{L}[i] \\neq \\texttt{'Z'}$，节点 $i$ 不是一个有效的中心，无需对该节点进行进一步检查。\n\n$2$。 **度验证**：问题指定了一个恰好有四个叶节点的星形图拓扑，这意味着中心节点的度必须恰好为 $4$。节点 $i$ 的度计算为 $\\deg(i) = \\sum_{j=0}^{n-1} \\mathbf{A}[i,j]$。如果 $\\deg(i) \\neq 4$，则该节点不符合条件。\n\n$3$。 **邻居识别**：如果节点 $i$ 的度为 $4$，则识别其四个邻居。设邻居索引集为 $N(i) = \\{\\ell_1, \\ell_2, \\ell_3, \\ell_4\\}$，这些是满足 $\\mathbf{A}[i,j] = 1$ 的索引 $j$。\n\n$4$。 **叶节点标签多重集验证**：四个邻居必须对应于特定的衰变产物。邻居的标签多重集 $\\{\\mathbf{L}[\\ell_1], \\mathbf{L}[\\ell_2], \\mathbf{L}[\\ell_3], \\mathbf{L}[\\ell_4]\\}$ 必须等于目标多重集 $\\{\\texttt{'e+'}, \\texttt{'e-'}, \\texttt{'mu+'}, \\texttt{'mu-'} \\}$。一个高效的检查方法是对收集到的邻居标签列表进行排序，并将其与一个预先排序的目标标签列表进行比较。如果它们不完全相同，则不满足该条件。\n\n$5$。 **导出子图验证**：指定的拓扑是一个*导出*子图，这意味着叶节点之间不能有任何边相连。这是一个关键约束。对于所有不同的邻居对 $\\ell_j, \\ell_k \\in N(i)$，邻接矩阵中的条目必须为零：$\\mathbf{A}[\\ell_j, \\ell_k] = 0$。此检查需要对所有 $\\binom{4}{2} = 6$ 对邻居进行。如果发现任何边（即，对于任何一对 $\\ell_j, \\ell_k$，有 $\\mathbf{A}[\\ell_j, \\ell_k] = 1$），则节点 $i$ 不符合条件。\n\n当且仅当一个节点 $i$ 成功通过所有这五个连续的检查时，它才被计为一个有效的中心。一个事件的总计数是在其对应图中找到的所有此类有效中心的总和。然后，对问题说明中指定的所有 $6$ 个测试事件重复整个过程。\n\n例如，在事件1中，对节点0进行评估。其标签为 $\\texttt{'Z'}$（通过），其度为 $4$（通过），其邻居是节点 $\\{1, 2, 3, 4\\}$，它们的标签 $\\{\\texttt{'e+'}, \\texttt{'e-'}, \\texttt{'mu+'}, \\texttt{'mu-'}\\}$ 构成了正确的多重集（通过）。最后，$\\mathbf{A}$ 中对应于这些邻居的子矩阵是一个零矩阵，因此它们之间不存在边（通过）。因此，节点0是一个有效的中心，事件1的计数为 $1$。相反，事件4未通过度检查，事件3未通过叶节点标签检查，事件5未通过导出子图检查，因此这些事件的计数都为 $0$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the graph pattern recognition problem by counting valid Z-boson\n    decay topologies in a series of simulated events.\n    \"\"\"\n    \n    # Define the 6 test cases from the problem statement.\n    test_cases = [\n        # Event 1: n=5, a single valid Z-centered star.\n        (\n            np.array([\n                [0, 1, 1, 1, 1],\n                [1, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0]\n            ]),\n            ['Z', 'e+', 'e-', 'mu+', 'mu-']\n        ),\n        # Event 2: n=10, two disjoint valid stars.\n        (\n            np.array([\n                [0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n                [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n            ]),\n            ['Z', 'e+', 'e-', 'mu+', 'mu-', 'Z', 'e+', 'e-', 'mu+', 'mu-']\n        ),\n        # Event 3: n=5, incorrect leaf multiset.\n        (\n            np.array([\n                [0, 1, 1, 1, 1],\n                [1, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0]\n            ]),\n            ['Z', 'e+', 'e-', 'e+', 'mu-']\n        ),\n        # Event 4: n=6, center degree is 5.\n        (\n            np.array([\n                [0, 1, 1, 1, 1, 1],\n                [1, 0, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0, 0]\n            ]),\n            ['Z', 'e+', 'e-', 'mu+', 'mu-', 'gamma']\n        ),\n        # Event 5: n=5, extra edge between leaves.\n        (\n            np.array([\n                [0, 1, 1, 1, 1],\n                [1, 0, 0, 1, 0],\n                [1, 0, 0, 0, 0],\n                [1, 1, 0, 0, 0],\n                [1, 0, 0, 0, 0]\n            ]),\n            ['Z', 'e+', 'e-', 'mu+', 'mu-']\n        ),\n        # Event 6: n=5, incorrect center label.\n        (\n            np.array([\n                [0, 1, 1, 1, 1],\n                [1, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0]\n            ]),\n            ['gamma', 'e+', 'e-', 'mu+', 'mu-']\n        )\n    ]\n\n    results = []\n    target_leaf_labels = sorted(['e+', 'e-', 'mu+', 'mu-'])\n\n    for A, L in test_cases:\n        num_nodes = A.shape[0]\n        valid_centers_count = 0\n\n        for i in range(num_nodes):\n            # Let node i be the potential center c.\n            \n            # 1. Check center label: Must be 'Z'.\n            if L[i] != 'Z':\n                continue\n\n            # 2. Check center degree: Must be 4.\n            degree = np.sum(A[i, :])\n            if degree != 4:\n                continue\n\n            # 3. Identify neighbors and check their labels.\n            neighbors = np.where(A[i, :] == 1)[0]\n            \n            leaf_labels = sorted([L[j] for j in neighbors])\n            if leaf_labels != target_leaf_labels:\n                continue\n            \n            # 4. Check induced subgraph condition: No edges between leaves.\n            has_leaf_edge = False\n            for j1_idx in range(4):\n                for j2_idx in range(j1_idx + 1, 4):\n                    n1 = neighbors[j1_idx]\n                    n2 = neighbors[j2_idx]\n                    if A[n1, n2] == 1:\n                        has_leaf_edge = True\n                        break\n                if has_leaf_edge:\n                    break\n            \n            if has_leaf_edge:\n                continue\n\n            # If all checks pass, this is a valid center.\n            valid_centers_count += 1\n            \n        results.append(valid_centers_count)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2425431"}, {"introduction": "模式识别不仅限于寻找固定的结构，更多时候需要处理统计模式。真实世界的信号常常伴随着噪声，而模式通常表现为统计特性的变化，而非一成不变的形状。这个练习将带领你进入贝叶斯变点检测领域，这是一种强大的技术，用于识别系统行为发生根本性转变的时刻 [@problem_id:2425429]。你将从第一性原理出发，实现一个贝叶斯模型，不仅推断变化是否发生，还能确定其最可能发生的时间，从而掌握一种分析时间序列数据的稳健方法。", "problem": "您将处理一项模式识别任务，该任务使用合成的实验时间序列数据，这些数据模拟了一个物理传感器在发生退化事件前稳定运行时测量的标量值。数据由一个分段常数均值过程生成，并受到加性高斯噪声的干扰；这是一个常用且经过充分检验的模型，其合理性可由中心极限定理证明。您的目标是从第一性原理出发，执行贝叶斯变点检测，以推断是否存在变点，如果存在，则推断其最可能发生的位置。\n\n假设存在以下生成模型。对于时间索引 $t = 1, 2, \\dots, N$，观测值 $y_t$ 的分布如下：\n- 对于 $t \\le \\tau$, $y_t \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$\n- 对于 $t > \\tau$, $y_t \\sim \\mathcal{N}(\\mu_2, \\sigma^2)$\n\n其中 $\\sigma^2$ 已知，$\\mu_1$ 和 $\\mu_2$ 是未知均值，$\\tau \\in \\{1, 2, \\dots, N-1\\}$ 是一个未知变点。在每个分段内，测量值是独立同分布的。使用以下贝叶斯先验假设：$\\mu_1 \\sim \\mathcal{N}(m_0, s_0^2)$ 和 $\\mu_2 \\sim \\mathcal{N}(m_0, s_0^2)$ 相互独立，且 $\\tau$ 在 $\\{1, \\dots, N-1\\}$ 上服从均匀分布。为评估是否存在变化，考虑两个模型：$M_1$（如上所述，有一个变点和两个均值）和 $M_0$（没有变点，所有 $t$ 共享一个均值），其先验概率为 $p(M_1) = p(M_0) = 1/2$。在 $M_0$ 模型下，所有 $t$ 的模型为 $y_t \\sim \\mathcal{N}(\\mu, \\sigma^2)$，其中 $\\mu \\sim \\mathcal{N}(m_0, s_0^2)$。\n\n仅从 Bayes 定理、给定参数下观测值的独立性以及高斯分布的性质出发，推导所需的表达式，以实现以下目标：\n- 在已知方差 $\\sigma^2$ 的高斯先验下，通过解析地对未知的段均值进行积分，计算一个数据段的边缘似然。\n- 计算整个序列在 $M_0$ 模型（单个数据段）下以及在 $M_1$ 模型下对每个候选变点 $\\tau$ 的边缘似然。\n- 计算直至一个乘法常数的后验分布 $p(\\tau \\mid \\mathbf{y}, M_1)$、最大后验估计 $\\hat{\\tau}$，并通过结合模型证据和模型先验概率来计算后验概率 $p(M_1 \\mid \\mathbf{y})$。\n\n算法约束：\n- 当对候选 $\\tau$ 值求和时，通过在对数域中进行计算，确保对于中等大小的 $N$ 值的计算是数值稳定的。\n- 为确保效率，您的实现应使用累积和来计算每个数据段的充分统计量（例如样本均值和残差平方和），使得对任何候选 $\\tau$ 的计算都能在常数时间内完成。\n- 每个测试用例的结果必须是一个双元素列表 $[\\hat{\\tau}, p(M_1 \\mid \\mathbf{y})]$，其中 $\\hat{\\tau}$ 是集合 $\\{1, \\dots, N-1\\}$ 中的一个整数，而 $p(M_1 \\mid \\mathbf{y})$ 是一个四舍五入到六位小数的浮点数。\n\n数据模拟协议：\n- 对于每个测试用例，按如下方式生成一个长度为 $N$ 的序列。对于 $t \\le \\tau_{\\text{true}}$，从 $y_t \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$ 中抽样；对于 $t > \\tau_{\\text{true}}$，从 $y_t \\sim \\mathcal{N}(\\mu_2, \\sigma^2)$ 中抽样。如果一个测试用例指定 $\\mu_2 = \\mu_1$，那么即使提供了 $\\tau_{\\text{true}}$，数据也遵循无变化模型；在这种情况下，正确的推断应反映出变化证据较弱。为确保可复现性，请为每个测试用例使用下面指定的固定随机种子。\n- 所有测试用例均使用相同的高斯先验：$m_0 = 0$ 和 $s_0 = 1$。\n\n测试套件：\n- 案例 A（明显变化，内部）：$N = 200$, $\\tau_{\\text{true}} = 120$, $\\mu_1 = 0$, $\\mu_2 = 0.8$, $\\sigma = 0.3$, 种子 $= 2021$。\n- 案例 B（早期变化）：$N = 200$, $\\tau_{\\text{true}} = 5$, $\\mu_1 = 0.1$, $\\mu_2 = 0.6$, $\\sigma = 0.4$, 种子 $= 2022$。\n- 案例 C（晚期变化）：$N = 200$, $\\tau_{\\text{true}} = 195$, $\\mu_1 = 0.2$, $\\mu_2 = -0.5$, $\\sigma = 0.35$, 种子 $= 2023$。\n- 案例 D（无变化）：$N = 200$, $\\tau_{\\text{true}} = 100$, $\\mu_1 = 0$, $\\mu_2 = 0$, $\\sigma = 0.5$, 种子 $= 2024$。\n\n最终输出规范：\n- 对于每个测试用例，计算在 $M_1$ 模型下的最大后验估计 $\\hat{\\tau}$，以及通过将模型证据 $p(\\mathbf{y} \\mid M_1)$ 和 $p(\\mathbf{y} \\mid M_0)$ 与模型先验概率 $p(M_1) = p(M_0) = 1/2$ 相结合得到的后验概率 $p(M_1 \\mid \\mathbf{y})$。\n- 您的程序应生成单行输出，其中包含四个测试用例的结果，格式为一个由方括号括起来的逗号分隔列表，其中每个元素本身就是一个双元素列表 $[\\hat{\\tau}, p(M_1 \\mid \\mathbf{y})]$，并且概率值需四舍五入到六位小数。例如，一个语法上有效的输出形式为 `[[a,b],[c,d],[e,f],[g,h]]`，逗号后没有空格。\n\n不涉及角度单位。输出不需要物理单位。所有数值答案必须以纯数字形式提供，不带单位符号。", "solution": "所提出的问题是贝叶斯模型选择和参数估计中的一个标准练习。它在科学上植根于概率论和贝叶斯推断的原理，提法明确，提供了所有必要信息，并且其表述是客观的。此问题有效。我们继续进行推导和求解。\n\n该任务是推断时间序列中变点的存在和位置。我们有两个竞争模型：模型 $M_0$ 假定没有变点，所有数据点共享一个均值；模型 $M_1$ 假定存在一个变点 $\\tau$，将数据分为具有不同均值的两个段。我们的目标是计算在模型 $M_1$ 下变点的最大后验 (MAP) 估计 $\\hat{\\tau}$，并在给定观测数据 $\\mathbf{y} = (y_1, \\dots, y_N)$ 的情况下，计算模型 $M_1$ 的后验概率 $p(M_1 \\mid \\mathbf{y})$。\n\n我们分析的基础是 Bayes 定理。为了比较模型 $M_0$ 和 $M_1$，我们必须计算它们各自的模型证据，即数据在每个模型下的边缘似然，$p(\\mathbf{y} \\mid M_0)$ 和 $p(\\mathbf{y} \\mid M_1)$。\n\n首先，我们为一个数据段 $\\mathbf{x} = (x_1, \\dots, x_k)$ 推导其边缘似然。该数据段由一个高斯分布生成，其方差已知，而其具有高斯先验的均值未知。\n设数据由 $x_t \\sim \\mathcal{N}(\\mu, \\sigma^2)$ (对于 $t=1, \\dots, k$) 生成。方差 $\\sigma^2$ 已知。均值的先验为 $\\mu \\sim \\mathcal{N}(m_0, s_0^2)$。\n该数据段的边缘似然（或称证据）是通过对未知均值 $\\mu$ 进行积分得到的：\n$$p(\\mathbf{x} \\mid \\sigma^2, m_0, s_0^2) = \\int p(\\mathbf{x} \\mid \\mu, \\sigma^2) p(\\mu \\mid m_0, s_0^2) d\\mu$$\n给定 $\\mu$ 时数据的似然函数为：\n$$p(\\mathbf{x} \\mid \\mu, \\sigma^2) = \\prod_{t=1}^k \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_t - \\mu)^2}{2\\sigma^2}\\right) = (2\\pi\\sigma^2)^{-k/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{t=1}^k (x_t - \\mu)^2\\right)$$\n$\\mu$ 的先验分布为：\n$$p(\\mu \\mid m_0, s_0^2) = \\frac{1}{\\sqrt{2\\pi s_0^2}} \\exp\\left(-\\frac{(\\mu - m_0)^2}{2s_0^2}\\right)$$\n被积函数中的乘积为：\n$$p(\\mathbf{x} \\mid \\mu, \\sigma^2) p(\\mu) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{t=1}^k (x_t - \\mu)^2 - \\frac{1}{2s_0^2} (\\mu - m_0)^2\\right)$$\n指数部分是关于 $\\mu$ 的一个二次函数。通过对 $\\mu$ 进行配方，我们可以确定后验分布 $p(\\mu \\mid \\mathbf{x})$ 的参数，该分布也是一个高斯分布 $\\mathcal{N}(m_k, s_k^2)$。后验精度是先验精度和数据精度之和：$1/s_k^2 = 1/s_0^2 + k/\\sigma^2$。后验均值是先验均值和样本均值的加权平均：$m_k = s_k^2 (m_0/s_0^2 + k\\bar{x}/\\sigma^2)$，其中 $\\bar{x} = \\frac{1}{k}\\sum_{t=1}^k x_t$。\n一个未归一化的高斯分布的积分结果是其归一化常数。边缘似然可以通过恒等式 $p(\\mathbf{x}) = p(\\mathbf{x} \\mid \\mu)p(\\mu)/p(\\mu \\mid \\mathbf{x})$ 求得。更直接的计算可得出以下对数边缘似然的表达式：\n$$ \\log p(\\mathbf{x}) = -\\frac{k}{2}\\log(2\\pi\\sigma^2) + \\frac{1}{2}\\log(s_k^2) - \\frac{1}{2}\\log(s_0^2) - \\frac{1}{2\\sigma^2}\\sum_{t=1}^k x_t^2 - \\frac{m_0^2}{2s_0^2} + \\frac{m_k^2}{2s_k^2} $$\n其中\n$$ s_k^2 = \\left(\\frac{1}{s_0^2} + \\frac{k}{\\sigma^2}\\right)^{-1} $$\n$$ m_k = s_k^2 \\left(\\frac{m_0}{s_0^2} + \\frac{k\\bar{x}}{\\sigma^2}\\right) $$\n该表达式使得对任何数据段的证据进行数值稳定的计算成为可能。\n\n现在，我们将此结果应用于我们的两个模型。\n\n模型 $M_0$（无变化）：整个数据序列 $\\mathbf{y} = (y_1, \\dots, y_N)$ 被视为一个单独的数据段。$M_0$ 的证据就是完整序列的边缘似然：\n$$ \\log p(\\mathbf{y} \\mid M_0) = \\log p(\\mathbf{y}_{1:N}) $$\n该值使用上述公式并代入 $k=N$ 进行计算。\n\n模型 $M_1$（一个变点）：对于一个给定的变点 $\\tau \\in \\{1, \\dots, N-1\\}$，数据被分割成两个独立的段：$\\mathbf{y}_{1:\\tau}$（长度为 $\\tau$）和 $\\mathbf{y}_{\\tau+1:N}$（长度为 $N-\\tau$）。由于均值 $\\mu_1$ 和 $\\mu_2$ 的先验是独立的，对于给定 $\\tau$ 的条件证据是这两个段证据的乘积：\n$$ p(\\mathbf{y} \\mid \\tau, M_1) = p(\\mathbf{y}_{1:\\tau}) p(\\mathbf{y}_{\\tau+1:N}) $$\n在对数域中：\n$$ \\log p(\\mathbf{y} \\mid \\tau, M_1) = \\log p(\\mathbf{y}_{1:\\tau}) + \\log p(\\mathbf{y}_{\\tau+1:N}) $$\n模型 $M_1$ 的完整证据是通过对所有可能的变点 $\\tau$ 进行边缘化得到的：\n$$ p(\\mathbf{y} \\mid M_1) = \\sum_{\\tau=1}^{N-1} p(\\mathbf{y} \\mid \\tau, M_1) p(\\tau \\mid M_1) $$\n考虑到变点上的均匀先验 $p(\\tau \\mid M_1) = 1/(N-1)$，上式变为：\n$$ p(\\mathbf{y} \\mid M_1) = \\frac{1}{N-1} \\sum_{\\tau=1}^{N-1} p(\\mathbf{y} \\mid \\tau, M_1) $$\n为避免数值下溢，这个指数和使用 log-sum-exp 技巧进行计算。设 $L_\\tau = \\log p(\\mathbf{y} \\mid \\tau, M_1)$。那么\n$$ \\log p(\\mathbf{y} \\mid M_1) = -\\log(N-1) + \\text{logsumexp}_{\\tau} (L_\\tau) $$\n其中 $\\text{logsumexp}(L) = L_{\\max} + \\log(\\sum e^{L_\\tau - L_{\\max}})$。\n\n计算出模型证据后，我们就可以求得所需的后验量。\n\n在模型 $M_1$ 下，变点的后验分布由 Bayes 定理给出：\n$$ p(\\tau \\mid \\mathbf{y}, M_1) = \\frac{p(\\mathbf{y} \\mid \\tau, M_1)p(\\tau \\mid M_1)}{p(\\mathbf{y} \\mid M_1)} $$\n由于先验 $p(\\tau \\mid M_1)$ 和分母 $p(\\mathbf{y} \\mid M_1)$ 相对于 $\\tau$ 都是常数，因此后验分布与似然成正比：\n$$ p(\\tau \\mid \\mathbf{y}, M_1) \\propto p(\\mathbf{y} \\mid \\tau, M_1) $$\nMAP 估计 $\\hat{\\tau}$ 是使该后验概率最大化的 $\\tau$ 值。这等价于最大化对数似然 $\\log p(\\mathbf{y} \\mid \\tau, M_1)$：\n$$ \\hat{\\tau} = \\arg\\max_{\\tau \\in \\{1, \\dots, N-1\\}} \\log p(\\mathbf{y} \\mid \\tau, M_1) $$\n\n最后，通过将其证据与总证据进行比较，来计算模型 $M_1$ 的后验概率：\n$$ p(M_1 \\mid \\mathbf{y}) = \\frac{p(\\mathbf{y} \\mid M_1) p(M_1)}{p(\\mathbf{y} \\mid M_0) p(M_0) + p(\\mathbf{y} \\mid M_1) p(M_1)} $$\n在先验概率相等 $p(M_0) = p(M_1) = 1/2$ 的情况下，上式简化为：\n$$ p(M_1 \\mid \\mathbf{y}) = \\frac{p(\\mathbf{y} \\mid M_1)}{p(\\mathbf{y} \\mid M_0) + p(\\mathbf{y} \\mid M_1)} = \\frac{1}{1 + \\frac{p(\\mathbf{y} \\mid M_0)}{p(\\mathbf{y} \\mid M_1)}} $$\n这可以使用先前推导的对数证据 $\\log p(\\mathbf{y} \\mid M_0)$ 和 $\\log p(\\mathbf{y} \\mid M_1)$ 来计算，如下所示：\n$$ p(M_1 \\mid \\mathbf{y}) = \\frac{1}{1 + \\exp(\\log p(\\mathbf{y} \\mid M_0) - \\log p(\\mathbf{y} \\mid M_1))} $$\n\n为了高效实现，每个数据段的充分统计量——和 $\\sum x_t$ 和平方和 $\\sum x_t^2$——都可以通过使用数据序列 $\\mathbf{y}$ 的预计算累积和在 $O(1)$ 时间内计算得出。这将寻找 $\\hat{\\tau}$ 和模型证据的总复杂度降低到 $O(N)$。", "answer": "```python\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef log_marginal_likelihood_segment(sum_y, sum_y2, k, m0, s0, sigma):\n    \"\"\"\n    Computes the log marginal likelihood for a single segment of data.\n    \"\"\"\n    if k == 0:\n        return 0.0\n\n    s02 = s0**2\n    sigma2 = sigma**2\n    mean_y = sum_y / k\n\n    # Posterior variance of the mean mu\n    sk2_inv = 1.0 / s02 + k / sigma2\n    sk2 = 1.0 / sk2_inv\n\n    # Posterior mean of mu\n    mk = sk2 * (m0 / s02 + k * mean_y / sigma2)\n\n    # Log marginal likelihood expression from derivation\n    log_p = (-k / 2.0 * np.log(2.0 * np.pi * sigma2) +\n             0.5 * np.log(sk2) -\n             0.5 * np.log(s02) -\n             sum_y2 / (2.0 * sigma2) -\n             m0**2 / (2.0 * s02) +\n             mk**2 / (2.0 * sk2))\n\n    return log_p\n\ndef analyze_sequence(y, m0, s0, sigma):\n    \"\"\"\n    Performs Bayesian change-point analysis on a time series.\n    \"\"\"\n    N = len(y)\n    \n    # Pre-compute cumulative sums for efficient segment statistic calculation\n    # cum_y[j] = sum(y_i) for i from 0 to j-1\n    cum_y = np.concatenate(([0.0], np.cumsum(y)))\n    cum_y2 = np.concatenate(([0.0], np.cumsum(y**2)))\n\n    # --- Evidence for M0 (no change-point) ---\n    sum_y_total = cum_y[N] - cum_y[0]\n    sum_y2_total = cum_y2[N] - cum_y2[0]\n    log_evidence_m0 = log_marginal_likelihood_segment(sum_y_total, sum_y2_total, N, m0, s0, sigma)\n\n    # --- Likelihoods for M1 (one change-point) for each tau ---\n    log_p_tau = np.zeros(N - 1)\n    \n    # tau is the change-point index from 1 to N-1\n    # a change at tau means y[:tau] is segment 1, y[tau:] is segment 2\n    for tau in range(1, N):\n        # Segment 1: y[0...tau-1] (length tau)\n        k1 = tau\n        sum_y1 = cum_y[tau] - cum_y[0]\n        sum_y2_1 = cum_y2[tau] - cum_y2[0]\n        log_p1 = log_marginal_likelihood_segment(sum_y1, sum_y2_1, k1, m0, s0, sigma)\n\n        # Segment 2: y[tau...N-1] (length N-tau)\n        k2 = N - tau\n        sum_y2 = cum_y[N] - cum_y[tau]\n        sum_y2_2 = cum_y2[N] - cum_y2[tau]\n        log_p2 = log_marginal_likelihood_segment(sum_y2, sum_y2_2, k2, m0, s0, sigma)\n\n        log_p_tau[tau - 1] = log_p1 + log_p2\n\n    # --- MAP estimate for tau under M1 ---\n    # np.argmax returns 0-based index. tau is 1-based.\n    hat_tau = np.argmax(log_p_tau) + 1\n\n    # --- Evidence for M1 ---\n    # Marginalize over tau using uniform prior p(tau)=1/(N-1)\n    # log p(y|M1) = log( sum_tau p(y|tau,M1) * p(tau|M1) )\n    #             = log( sum_tau exp(log_p_tau) * 1/(N-1) )\n    #             = logsumexp(log_p_tau) - log(N-1)\n    log_evidence_m1 = logsumexp(log_p_tau) - np.log(N - 1)\n\n    # --- Posterior probability of M1 ---\n    # p(M1|y) = 1 / (1 + p(y|M0)/p(y|M1))\n    # p(y|M0)/p(y|M1) = exp(log_evidence_m0 - log_evidence_m1)\n    log_bayes_factor_01 = log_evidence_m0 - log_evidence_m1\n    p_m1_posterior = 1.0 / (1.0 + np.exp(log_bayes_factor_01))\n\n    return [int(hat_tau), round(p_m1_posterior, 6)]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        {'N': 200, 'tau_true': 120, 'mu1': 0.0, 'mu2': 0.8, 'sigma': 0.3, 'seed': 2021},\n        {'N': 200, 'tau_true': 5, 'mu1': 0.1, 'mu2': 0.6, 'sigma': 0.4, 'seed': 2022},\n        {'N': 200, 'tau_true': 195, 'mu1': 0.2, 'mu2': -0.5, 'sigma': 0.35, 'seed': 2023},\n        {'N': 200, 'tau_true': 100, 'mu1': 0.0, 'mu2': 0.0, 'sigma': 0.5, 'seed': 2024},\n    ]\n\n    # Shared prior parameters for all cases\n    m0 = 0.0\n    s0 = 1.0\n\n    results = []\n    for case in test_cases:\n        # Generate synthetic data\n        rng = np.random.default_rng(case['seed'])\n        y = np.zeros(case['N'])\n        seg1_len = case['tau_true']\n        seg2_len = case['N'] - case['tau_true']\n        \n        y[:seg1_len] = rng.normal(loc=case['mu1'], scale=case['sigma'], size=seg1_len)\n        if seg2_len > 0:\n            y[seg1_len:] = rng.normal(loc=case['mu2'], scale=case['sigma'], size=seg2_len)\n\n        # Perform analysis\n        result = analyze_sequence(y, m0, s0, case['sigma'])\n        results.append(result)\n\n    # Format output as specified\n    # e.g., [[120,0.999999],[5,0.987654],...]\n    output_str = '['\n    for i, res in enumerate(results):\n        output_str += f'[{res[0]},{res[1]}]'\n        if i < len(results) - 1:\n            output_str += ','\n    output_str += ']'\n    \n    print(output_str)\n\nsolve()\n\n```", "id": "2425429"}, {"introduction": "最后，我们将探索如何利用模式识别来发现物理系统中的宏观涌现现象。这个练习以森林火灾为模型，来研究统计物理学中的一个关键概念——逾渗理论 [@problem_id:2425393]。通过蒙特卡洛方法模拟“森林”，并利用图像分析技术来识别贯穿整个区域的“火势”，你将亲手确定系统发生相变时的临界植被密度。这种结合模拟与分析的方法是研究复杂系统的基础。", "problem": "您将通过将每个模拟的格网视为二值图像，并使用连通分量标记来评估火势是否能跨越系统，从而为森林火灾渗流模型中的临界植被密度实现一个可复现的蒙特卡洛（MC）估计器。森林是一个 $N \\times N$ 的方格格网。每个格点以概率 $p$ 独立地被植被覆盖，或以概率 $1-p$ 为空。火势被假定为通过最近邻接触（四邻域 von Neumann 连通性）在相邻的植被覆盖点之间确定性地蔓延。火灾能否从森林的一侧蔓延到另一侧的问题，等同于被植被覆盖的格点的二值图像中，是否存在一个同时接触到图像左边界和右边界的连通分量。对于固定的格网尺寸 $N$，临界密度 $p_c$ 被定义为左右贯穿概率约等于 $1/2$ 时的 $p$ 值。\n\n基本原理：\n- 每个格点的占据状态是一个独立的伯努利随机变量，其成功概率为 $p$。\n- 在二值图像中，一个连通分量是一个最大的格点集合，其中任意一点都可以通过一条由最近邻（四邻域）格点组成的路径到达集合中的另一点。\n- 贯穿事件定义为：至少存在一个连通分量，它同时与格网的最左列和最右列相交。\n- 对于给定的 $p$ 和有限的 $N$，贯穿概率是一个指示随机变量的期望值。如果采样的格网发生贯穿，该变量为 $1$，否则为 $0$。MC 估计器是该指示变量在多次独立试验中的样本均值，根据大数定律，该估计器会收敛到真实概率。\n\n需要实现的任务：\n1. 对于给定的 $N$、$p$ 和整数种子 $s$，通过对 $[0,1)$ 上的独立同分布均匀随机数进行阈值处理，生成 $T$ 个独立的二值图像。将小于 $p$ 的值视为植被覆盖点（$1$），否则视为空地（$0$）。每个图像都是一个 $N \\times N$ 的数组。\n2. 对每个图像，使用四邻域连通性下的连通分量标记算法来标记植被覆盖点的簇。当且仅当存在一个已标记的连通分量同时接触到左边界 $(x=0)$ 和右边界 $(x=N-1)$ 时，声明该图像为贯穿。计算 MC 估计值 $\\hat{\\pi}_N(p)$，即贯穿图像所占的比例。\n3. 对于固定的 $N$，在一个区间 $[p_{\\mathrm{lo}}, p_{\\mathrm{hi}}]$ 内使用二分法估计 $p_c(N)$，该区间满足 $\\hat{\\pi}_N(p_{\\mathrm{lo}}) < 1/2$ 和 $\\hat{\\pi}_N(p_{\\mathrm{hi}}) \\ge 1/2$。在二分法的每一步，使用相同的种子 $s$ 和试验次数 $T$ 计算 $\\hat{\\pi}_N(p)$，并根据 $\\hat{\\pi}_N(p) \\ge 1/2$ 是否成立来更新区间。当区间宽度小于指定的容差 $\\varepsilon$ 时终止，并报告区间中点作为 $p_c(N)$ 的估计值。\n4. 为确保二分法过程中 MC 估计值在 $p$ 上的可复现性和单调性，对于给定的 $N$，在所有不同 $p$ 值的贯穿概率计算中都使用相同的基础种子 $s$，这样每次试验都使用相同的均匀随机数，只有阈值发生变化。\n\n不涉及角度单位。不涉及物理单位。所有概率必须以小数形式报告，而非百分比。\n\n测试套件和要求的输出：\n- 测试用例1：对于 $N=32$，使用 $T=64$，$p_{\\mathrm{lo}}=0.3$，$p_{\\mathrm{hi}}=0.8$，$\\varepsilon=10^{-3}$ 和种子 $s=12345$ 估计 $p_c$。输出一个四舍五入到4位小数的浮点数。\n- 测试用例2：对于 $N=64$，使用 $T=64$，$p_{\\mathrm{lo}}=0.3$，$p_{\\mathrm{hi}}=0.8$，$\\varepsilon=10^{-3}$ 和种子 $s=24680$ 估计 $p_c$。输出一个四舍五入到4位小数的浮点数。\n- 测试用例3：对于 $N=64$ 和 $p=0.2$，使用 $T=200$ 和种子 $s=98765$ 计算贯穿概率 $\\hat{\\pi}_N(p)$。输出一个四舍五入到4位小数的浮点数。\n- 测试用例4：对于 $N=64$ 和 $p=0.8$，使用 $T=200$ 和种子 $s=54321$ 计算贯穿概率 $\\hat{\\pi}_N(p)$。输出一个四舍五入到4位小数的浮点数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按测试用例1到4的顺序排列结果，每个结果都四舍五入到4位小数。例如，一个有效的输出格式是 `[0.5925,0.5929,0.0125,0.9975]`。", "solution": "所提出的问题是计算统计物理学中一个明确定义的练习，特别关注渗流现象。该问题是有效的，并具备了进行严谨求解所必需的所有要素。我们的任务是数值估计一个有限 $N \\times N$ 离散格网上的临界植被密度 $p_c$，这是一个基础模型，可用于模拟多孔介质中的流体流动、流行病传播以及本案例中的森林火灾等现象。该估计将使用蒙特卡洛方法，并结合连通分量分析和二分搜索算法来执行。\n\n问题的核心在于估计贯穿概率 $\\pi_N(p)$，即一个尺寸为 $N \\times N$、植被密度为 $p$ 的随机生成森林中，存在一条连接左边界和右边界的连续植被路径的概率。临界密度 $p_c(N)$ 定义为该概率约等于 $1/2$ 时的密度。\n\n让我们将此过程形式化。\n\n1.  **格网生成和蒙特卡洛方法**\n\n森林表示为一个 $N \\times N$ 的网格 $\\mathcal{L}$。网格上的每个格点 $(i, j)$（其中 $i, j \\in \\{0, 1, \\dots, N-1\\}$）要么被植被覆盖，要么是空的。格点的状态由一个伯努利随机变量 $\\sigma_{i,j}$ 决定，其成功（植被）概率为 $p$。因此，$P(\\sigma_{i,j}=1) = p$ 且 $P(\\sigma_{i,j}=0) = 1-p$。所有格点的状态都是相互独立的。\n\n为了估计贯穿概率 $\\pi_N(p)$，我们采用蒙特卡洛模拟。我们生成 $T$ 个独立的格网实例，称之为试验。对于每次试验 $k \\in \\{1, \\dots, T\\}$，我们生成一个 $N \\times N$ 的占据数矩阵。我们为每次试验定义一个指示变量 $I_k(p)$：\n$$\nI_k(p) = \\begin{cases} 1 & \\text{若试验 } k \\text{ 产生一个贯穿构型} \\\\ 0 & \\text{否则} \\end{cases}\n$$\n贯穿概率的蒙特卡洛估计器 $\\hat{\\pi}_N(p)$ 是该指示变量在 $T$ 次试验中的样本平均值：\n$$\n\\hat{\\pi}_N(p) = \\frac{1}{T} \\sum_{k=1}^T I_k(p)\n$$\n根据大数定律，当试验次数 $T \\to \\infty$ 时，该估计器收敛到真实的贯穿概率 $\\pi_N(p)$。\n\n2.  **贯穿簇的检测**\n\n为确定给定的格网构型是否贯穿，我们必须识别出植被覆盖点的连通簇。一个簇是一组植被覆盖的格点，其中簇内任意一个格点都可以通过一条由相邻植被格点组成的路径到达簇内任何其他格点。邻接关系由四邻域（von Neumann）规则定义：一个格点 $(i, j)$ 与其位于 $(i\\pm1, j)$ 和 $(i, j\\pm1)$ 的邻居相邻，前提是这些邻居在格网边界之内。\n\n此任务在算法上等同于二值图像中的连通分量标记（CCL）。我们将使用一个成熟的 CCL 算法，它为每个不同的簇分配一个唯一的整数标签。该算法处理由植被覆盖点（值为 $1$）和背景（值为 $0$）组成的二值格网。标记后，我们得到一个新的 $N \\times N$ 整数矩阵，其中所有属于同一簇的格点共享相同的正整数标签，空地格点则被标记为 $0$。\n\n如果至少存在一个簇同时出现在最左列（$j=0$）和最右列（$j=N-1$）中，就发生了贯穿事件。我们可以通过以下方式进行验证：\n-   提取第 $0$ 列中存在的唯一非零标签集合，记作 $\\mathcal{C}_{\\text{left}}$。\n-   提取第 $N-1$ 列中存在的唯一非零标签集合，记作 $\\mathcal{C}_{\\text{right}}$。\n-   当且仅当这两个集合的交集非空时，格网发生贯穿：$\\mathcal{C}_{\\text{left}} \\cap \\mathcal{C}_{\\text{right}} \\neq \\emptyset$。\n\n3.  **用于二分搜索的可复现性与单调性**\n\n临界密度 $p_c(N)$ 由条件 $\\pi_N(p_c(N)) \\approx 1/2$ 定义。为了找到 $p_c$，我们必须求解方程 $\\hat{\\pi}_N(p) - 1/2 = 0$ 以得到 $p$。由于 $\\hat{\\pi}_N(p)$ 是一个阶梯函数（因为它是指示变量的和），只要该函数是单调的，二分搜索就是一种合适的方法。\n\n如果在每次针对不同 $p$ 值的评估中使用独立的随机数，标准的蒙特卡洛估计器 $\\hat{\\pi}_N(p)$ 在有限的 $T$ 下不保证是单调的。为了强制实现单调性，我们使用同一组底层随机数。使用固定的种子 $s$，一次性生成一个大小为 $T \\times N \\times N$ 的大型随机数数组 $U_{k,i,j} \\sim \\mathcal{U}[0,1)$。对于任何给定的概率阈值 $p$，通过设置 $\\sigma_{k,i,j} = 1$（如果 $U_{k,i,j} < p$）和 $\\sigma_{k,i,j} = 0$（否则）来生成格网构型。\n\n通过这种构造，如果我们考虑两个概率 $p_1$ 和 $p_2$ 且 $p_1 < p_2$，则对应于 $p_1$ 的植被点集合是对应于 $p_2$ 的植被点集合的子集。因此，如果一个簇在 $p_1$ 时贯穿，那么它在 $p_2$ 时也必定贯穿。这保证了我们的估计器 $\\hat{\\pi}_N(p)$ 是 $p$ 的一个非递减函数，这是二分算法可靠工作的一个必要条件。\n\n4.  **$p_c(N)$ 的二分算法**\n\n二分法过程如下：\n-   选择一个初始区间 $[p_{\\mathrm{lo}}, p_{\\mathrm{hi}}]$，使得 $\\hat{\\pi}_N(p_{\\mathrm{lo}}) < 1/2$ 且 $\\hat{\\pi}_N(p_{\\mathrm{hi}}) \\ge 1/2$。\n-   算法迭代地优化此区间：\n    1.  计算中点 $p_{\\mathrm{mid}} = (p_{\\mathrm{lo}} + p_{\\mathrm{hi}})/2$。\n    2.  使用固定的随机数集评估贯穿概率 $\\hat{\\pi}_N(p_{\\mathrm{mid}})$。\n    3.  如果 $\\hat{\\pi}_N(p_{\\mathrm{mid}}) < 1/2$，则临界点必定位于区间的上半部分。我们更新下界：$p_{\\mathrm{lo}} \\leftarrow p_{\\mathrm{mid}}$。\n    4.  否则，如果 $\\hat{\\pi}_N(p_{\\mathrm{mid}}) \\ge 1/2$，则临界点位于下半部分。我们更新上界：$p_{\\mathrm{hi}} \\leftarrow p_{\\mathrm{mid}}$。\n-   重复此过程，直到区间的宽度 $p_{\\mathrm{hi}} - p_{\\mathrm{lo}}$ 小于指定的容差 $\\varepsilon$。\n-   $p_c(N)$ 的最终估计值是最终区间的中点 $(p_{\\mathrm{lo}} + p_{\\mathrm{hi}})/2$。\n\n这一完整的方法论允许对指定有限系统的临界渗流阈值进行可复现且科学可靠的估计。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.ndimage import label\n\ndef _check_span(lattice: np.ndarray) -> bool:\n    \"\"\"\n    Checks for a left-right spanning cluster in a single binary lattice.\n    A spanning cluster is a connected component of 1s that touches both\n    the first and last columns of the lattice.\n    \"\"\"\n    if not np.any(lattice):\n        return False\n        \n    # Define 4-neighbor (von Neumann) connectivity\n    structure = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]])\n    \n    # Perform connected-component labeling\n    labeled_lattice, num_labels = label(lattice, structure=structure)\n    \n    # If there are no vegetated clusters, it cannot span\n    if num_labels == 0:\n        return False\n\n    # Get unique labels on left and right boundaries\n    # The `_` is to handle the case where a boundary is all 0s\n    left_labels = np.unique(labeled_lattice[:, 0])\n    right_labels = np.unique(labeled_lattice[:, -1])\n\n    # Remove background label (0), which corresponds to empty sites\n    left_labels = left_labels[left_labels != 0]\n    right_labels = right_labels[right_labels != 0]\n    \n    # If either boundary has no clusters, no spanning is possible\n    if left_labels.size == 0 or right_labels.size == 0:\n        return False\n\n    # Check for any common labels between the left and right boundaries\n    return np.intersect1d(left_labels, right_labels, assume_unique=True).size > 0\n\ndef compute_spanning_prob(N: int, p: float, T: int, seed: int) -> float:\n    \"\"\"\n    Computes spanning probability via Monte Carlo simulation for given parameters.\n    Each trial uses an independent set of random numbers.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    spanning_count = 0\n    for _ in range(T):\n        # Generate one lattice configuration\n        lattice = rng.random((N, N)) < p\n        if _check_span(lattice):\n            spanning_count += 1\n    return spanning_count / T\n\ndef estimate_pc(N: int, T: int, p_lo: float, p_hi: float, epsilon: float, seed: int) -> float:\n    \"\"\"\n    Estimates the critical probability p_c using a bisection search.\n    Crucially, it uses the same set of base random numbers for all evaluations\n    of the spanning probability to ensure monotonicity of the estimator with p.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    # Generate one large block of random numbers to be used for all p values\n    base_random_numbers = rng.random((T, N, N))\n    \n    # Internal helper function that computes spanning probability for a given p\n    # using the pre-generated random numbers.\n    def _get_pi_monotonic(p_val: float) -> float:\n        # Create T lattices by thresholding the base random numbers\n        lattices = base_random_numbers < p_val\n        spanning_count = 0\n        for i in range(T):\n            if _check_span(lattices[i, :, :]):\n                spanning_count += 1\n        return spanning_count / T\n\n    # Bisection search loop\n    current_p_lo, current_p_hi = p_lo, p_hi\n    while (current_p_hi - current_p_lo) >= epsilon:\n        p_mid = (current_p_lo + current_p_hi) / 2.0\n        pi_mid = _get_pi_monotonic(p_mid)\n        \n        if pi_mid < 0.5:\n            current_p_lo = p_mid\n        else:\n            current_p_hi = p_mid\n            \n    return (current_p_lo + current_p_hi) / 2.0\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'type': 'pc', 'params': {'N': 32, 'T': 64, 'p_lo': 0.3, 'p_hi': 0.8, 'epsilon': 1e-3, 'seed': 12345}},\n        {'type': 'pc', 'params': {'N': 64, 'T': 64, 'p_lo': 0.3, 'p_hi': 0.8, 'epsilon': 1e-3, 'seed': 24680}},\n        {'type': 'pi', 'params': {'N': 64, 'p': 0.2, 'T': 200, 'seed': 98765}},\n        {'type': 'pi', 'params': {'N': 64, 'p': 0.8, 'T': 200, 'seed': 54321}},\n    ]\n\n    results = []\n    for case in test_cases:\n        if case['type'] == 'pc':\n            result = estimate_pc(**case['params'])\n        elif case['type'] == 'pi':\n            result = compute_spanning_prob(**case['params'])\n        results.append(result)\n\n    # Format results to 4 decimal places for the final output\n    formatted_results = [f\"{res:.4f}\" for res in results]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the solution\nsolve()\n```", "id": "2425393"}]}