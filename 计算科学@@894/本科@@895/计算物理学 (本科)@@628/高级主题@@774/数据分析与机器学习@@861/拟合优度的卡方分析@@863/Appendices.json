{"hands_on_practices": [{"introduction": "理论通过实践才得以彰显其生命力。在本章节中，我们将通过三个循序渐进的动手实践，将卡方分析的核心概念应用于实际问题。我们将从一个基础的物理模型拟合开始，然后进入更复杂的非线性模型比较，最后探索一个在信号搜索中常见的统计陷阱。这些练习旨在加深你对卡方检验的理解，并锻炼你解决实际科学数据问题的能力。\n\n我们首先从一个经典的应用开始：用实验数据检验一条著名的物理定律。本练习以斯特藩-玻尔兹曼定律为例，你的任务是从给定的温度和辐射功率数据中确定一个未知参数（有效辐射面积 $A$），并使用 $\\chi^2$ 统计量来评估该模型对数据的描述程度。通过这个实践，你将掌握加权最小二乘法拟合的核心思想，并学会如何解读约化卡方值 $\\chi^2_{\\nu}$ 以判断拟合的好坏。[@problem_id:2379497]", "problem": "测量一个有效辐射面积 $A$ 未知的黑体在几个绝对温度 $T$ 下的总辐射功率 $P$。其物理模型为斯蒂芬-玻尔兹曼定律 $P = A\\,\\sigma\\,T^{4}$，其中斯蒂芬-玻尔兹曼常数为 $\\sigma = 5.670\\,374\\,419\\times 10^{-8}\\ \\mathrm{W\\,m^{-2}\\,K^{-4}}$。假设所有温度都是精确已知的，且每次功率测量值 $P_i$ 都有一个独立的、已知的标准不确定度 $u_i$。\n\n对于每个数据集，将 $A$ 视为唯一的未知模型参数。使用所提供的三元组 $(T_i,P_i,u_i)$，确定使加权残差平方和（卡方值）最小的 $\\hat{A}$ 值，\n$$\n\\chi^{2}(A) = \\sum_{i=1}^{N}\\frac{\\left[P_i - A\\,\\sigma\\,T_i^{4}\\right]^2}{u_i^{2}},\n$$\n然后计算：\n- 最小卡方值 $\\chi^{2}_{\\min}$，\n- 约化卡方值 $\\chi^{2}_{\\nu} = \\chi^{2}_{\\min}/\\nu$，其中自由度为 $\\nu = N - 1$，\n- 拟合优度 p值 $p = \\Pr\\!\\left(\\chi^{2}_{\\nu} \\ge \\chi^{2}_{\\min}\\right)$，该值由自由度为 $\\nu$ 的卡方分布计算得出，\n- 一个布尔决策值 $\\mathrm{accept}$，当 $p \\ge 0.05$ 时为 $\\mathrm{True}$，否则为 $\\mathrm{False}$。\n\n所有温度 $T$ 的单位必须是开尔文，所有功率 $P$ 和不确定度 $u$ 的单位必须是瓦特，面积估算值 $\\hat{A}$ 的单位必须是平方米。统计显著性水平为 $0.05$。报告 $\\hat{A}$（单位为平方米）、无量纲的 $\\chi^{2}_{\\nu}$ 和小数形式的 $p$。\n\n使用以下包含3个数据集的测试套件：\n\n- 数据集1（噪声与模型一致）：\n  - $T$（单位：开尔文）：$[300, 400, 500, 600, 700, 800]$。\n  - $P$（单位：瓦特）：$[6.92, 21.50, 53.60, 109.40, 204.90, 347.20]$。\n  - $u$（单位：瓦特）：$[0.30, 0.50, 0.70, 1.00, 1.20, 1.50]$。\n\n- 数据集2（相对于模型存在系统性偏移）：\n  - $T$（单位：开尔文）：$[300, 400, 500, 600, 700, 800]$。\n  - $P$（单位：瓦特）：$[11.90, 26.80, 58.20, 115.40, 208.80, 353.00]$。\n  - $u$（单位：瓦特）：$[0.50, 0.50, 0.50, 0.70, 0.80, 1.00]$。\n\n- 数据集3（接近完美的数据，不确定度小）：\n  - $T$（单位：开尔文）：$[250, 500, 750]$。\n  - $P$（单位：瓦特）：$[2.21499, 35.43984, 179.41419]$。\n  - $u$（单位：瓦特）：$[0.10, 0.10, 0.10]$。\n\n您的程序必须按顺序处理所有数据集，并为每个数据集生成一个形式为 $[\\hat{A}, \\chi^{2}_{\\nu}, p, \\mathrm{accept}]$ 的结果列表。最终的输出必须是单行文本，它是一个包含了所有单个数据集结果列表的列表，元素之间用逗号分隔，并用方括号括起来。例如，输出格式应类似于 $[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3]]$，其中每个 $a_k$ 是一个单位为平方米的浮点数，每个 $b_k$ 是一个浮点数，每个 $c_k$ 是一个浮点数，每个 $d_k$ 是一个布尔值。", "solution": "该问题要求针对斯蒂芬-玻尔兹曼定律 $P = A\\,\\sigma\\,T^{4}$ 和几个实验数据集，执行卡方拟合优度分析。对于每个由 $N$ 个测量值 $(T_i, P_i, u_i)$ 组成的数据集，我们必须找到有效辐射面积的最佳拟合值 $\\hat{A}$，并评估拟合的质量。该模型对于唯一的未知参数 $A$ 是线性的。这是一个经典的加权线性最小二乘回归问题。\n\n需要最小化的量是卡方统计量，其定义为加权残差平方和：\n$$\n\\chi^{2}(A) = \\sum_{i=1}^{N}\\frac{\\left[P_i - P_{\\text{model}}(T_i; A)\\right]^2}{u_i^{2}}\n$$\n在这里，$P_i$ 是在温度 $T_i$ 下测得的功率，$u_i$ 是测量值 $P_i$ 的标准不确定度，$P_{\\text{model}}(T_i; A) = A\\,\\sigma\\,T_i^{4}$ 是模型预测的功率。每个数据点的权重被隐式地定义为 $w_i = 1/u_i^2$。\n\n为了找到使 $\\chi^{2}(A)$ 最小化的 $A$ 值（记为 $\\hat{A}$），我们必须求解方程 $\\frac{d\\chi^2}{dA} = 0$。其导数为：\n$$\n\\frac{d\\chi^{2}}{dA} = \\frac{d}{dA} \\sum_{i=1}^{N}\\frac{\\left[P_i - A\\,\\sigma\\,T_i^{4}\\right]^2}{u_i^{2}} = \\sum_{i=1}^{N} \\frac{2\\left[P_i - A\\,\\sigma\\,T_i^{4}\\right](-\\sigma\\,T_i^{4})}{u_i^{2}}\n$$\n将导数设为零并求解 $\\hat{A}$：\n$$\n\\sum_{i=1}^{N} \\frac{\\left[P_i - \\hat{A}\\,\\sigma\\,T_i^{4}\\right](\\sigma\\,T_i^{4})}{u_i^{2}} = 0\n$$\n$$\n\\sum_{i=1}^{N} \\frac{P_i\\,\\sigma\\,T_i^{4}}{u_i^{2}} - \\sum_{i=1}^{N} \\frac{\\hat{A}\\,(\\sigma\\,T_i^{4})^2}{u_i^{2}} = 0\n$$\n$$\n\\hat{A} \\sum_{i=1}^{N} \\frac{(\\sigma\\,T_i^{4})^2}{u_i^{2}} = \\sum_{i=1}^{N} \\frac{P_i\\,\\sigma\\,T_i^{4}}{u_i^{2}}\n$$\n由此可得最佳拟合参数 $\\hat{A}$ 的解析表达式：\n$$\n\\hat{A} = \\frac{\\sum_{i=1}^{N} (P_i \\sigma T_i^4 / u_i^2)}{\\sum_{i=1}^{N} (\\sigma T_i^4 / u_i)^2}\n$$\n\n一旦确定了 $\\hat{A}$，我们就可以将其代入原始表达式中，计算出最小卡方值 $\\chi^{2}_{\\min}$：\n$$\n\\chi^{2}_{\\min} = \\chi^{2}(\\hat{A}) = \\sum_{i=1}^{N}\\frac{\\left[P_i - \\hat{A}\\,\\sigma\\,T_i^{4}\\right]^2}{u_i^{2}}\n$$\n\n自由度 $\\nu$ 的数量等于数据点数 $N$ 减去拟合参数的数量 $M$。在本例中，$M=1$（即参数 $A$），所以 $\\nu = N - 1$。\n\n约化卡方值 $\\chi^{2}_{\\nu}$ 提供了一种拟合优度的度量，它通过自由度进行了归一化：\n$$\n\\chi^{2}_{\\nu} = \\frac{\\chi^{2}_{\\min}}{\\nu}\n$$\n对于一个良好拟合（即模型正确且不确定度 $u_i$ 被准确估计），我们期望 $\\chi^{2}_{\\nu} \\approx 1$。若 $\\chi^{2}_{\\nu} \\gg 1$，则表示拟合效果差或不确定度被低估；而 $\\chi^{2}_{\\nu} \\ll 1$ 则表示拟合过好，可能是不确定度被高估所致。\n\n为了使拟合优度检验形式化，我们计算p值。p值是在原假设（即模型是正确的）为真的前提下，获得一个至少与观测到的 $\\chi^{2}_{\\min}$ 一样大的卡方统计量的概率。该概率由自由度为 $\\nu$ 的卡方分布的生存函数（1 - 累积分布函数）计算得出：\n$$\np = \\Pr(\\chi^2_{\\text{dist}} \\ge \\chi^{2}_{\\min} \\mid \\nu) = \\int_{\\chi^{2}_{\\min}}^{\\infty} f(x; \\nu) dx\n$$\n其中 $f(x; \\nu)$ 是自由度为 $\\nu$ 的 $\\chi^2$ 分布的概率密度函数。\n\n通过将p值与预定义的显著性水平 $\\alpha = 0.05$ 进行比较来做出最终决策。\n- 如果 $p \\ge 0.05$，观测到的与模型的偏差在统计上不显著。我们接受该模型作为对数据的合理解释。布尔值 `accept` 为 $\\mathrm{True}$。\n- 如果 $p < 0.05$，该偏差在统计上是显著的，这意味着这种差异不太可能仅仅由随机机会造成。我们拒绝该模型。布尔值 `accept` 为 $\\mathrm{False}$。\n\n该流程将应用于所提供的三个数据集中的每一个。对于每个数据集，我们将计算元组 $(\\hat{A}, \\chi^{2}_{\\nu}, p, \\mathrm{accept})$。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Performs chi-squared analysis for the Stefan-Boltzmann law on given datasets.\n    \"\"\"\n    # Stefan-Boltzmann constant in W m^-2 K^-4\n    SIGMA = 5.670374419e-8\n\n    # Define the datasets as provided in the problem statement.\n    test_cases = [\n        {\n            \"T\": np.array([300, 400, 500, 600, 700, 800]),  # kelvins\n            \"P\": np.array([6.92, 21.50, 53.60, 109.40, 204.90, 347.20]),  # watts\n            \"u\": np.array([0.30, 0.50, 0.70, 1.00, 1.20, 1.50]),  # watts\n        },\n        {\n            \"T\": np.array([300, 400, 500, 600, 700, 800]),\n            \"P\": np.array([11.90, 26.80, 58.20, 115.40, 208.80, 353.00]),\n            \"u\": np.array([0.50, 0.50, 0.50, 0.70, 0.80, 1.00]),\n        },\n        {\n            \"T\": np.array([250, 500, 750]),\n            \"P\": np.array([2.21499, 35.43984, 179.41419]),\n            \"u\": np.array([0.10, 0.10, 0.10]),\n        },\n    ]\n\n    results = []\n    significance_level = 0.05\n\n    for case in test_cases:\n        T, P, u = case[\"T\"], case[\"P\"], case[\"u\"]\n\n        # Number of data points\n        N = len(T)\n        \n        # Degrees of freedom (N data points - 1 fitted parameter)\n        nu = N - 1\n\n        # Model value for P with A=1, this is sigma * T^4\n        model_base = SIGMA * T**4\n\n        # Calculate the best-fit parameter A_hat using the derived analytical formula\n        # A_hat = sum(P_i * sigma * T_i^4 / u_i^2) / sum((sigma * T_i^4)^2 / u_i^2)\n        numerator = np.sum(P * model_base / u**2)\n        denominator = np.sum(model_base**2 / u**2)\n        A_hat = numerator / denominator\n\n        # Calculate the minimized chi-squared value\n        residuals = P - A_hat * model_base\n        chi2_min = np.sum((residuals / u)**2)\n\n        # Calculate the reduced chi-squared\n        chi2_nu = chi2_min / nu if nu > 0 else 0.0\n\n        # Calculate the p-value (goodness-of-fit)\n        # It's the probability of getting a chi2 value >= chi2_min\n        p_value = chi2.sf(chi2_min, nu)\n\n        # Make the decision based on the significance level\n        accept = p_value >= significance_level\n        \n        # Store results for this dataset\n        results.append([A_hat, chi2_nu, p_value, accept])\n\n    # Format the final output string according to the problem specification\n    # to avoid spaces inside the lists.\n    list_of_strings = []\n    for res in results:\n        # str(True) -> 'True', str(False) -> 'False'\n        # which is the correct boolean representation in this context.\n        list_of_strings.append(f\"[{res[0]},{res[1]},{res[2]},{res[3]}]\")\n    \n    final_output = f\"[{','.join(list_of_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2379497"}, {"introduction": "在真实的科学研究中，我们面对的数据往往没有一个唯一、明确的理论模型。本练习将引导你从检验单一假设，转向更贴近实际的模型选择任务。你将面对一组实验数据，并为其拟合两种不同的非线性模型——指数衰减模型和幂律模型。通过比较两种模型下的最优拟合约化卡方值，你将学习如何判断哪个模型能更好地描述数据，这在探索未知物理规律时至关重要。这个实践将让你接触到非线性拟合中的数值优化方法，并掌握一个实用的模型比较准则。[@problem_id:2379487]", "problem": "给定三个独立的数据集，每个数据集由三元组 $\\{(x_i,y_i,\\sigma_i)\\}_{i=1}^{N}$ 组成，其中 $x_i$ 是无量纲的正横坐标，$y_i$ 是无量纲的正纵坐标，$\\sigma_i$ 是每个 $y_i$ 对应的已知的、无量纲的正标准差，代表独立的高斯测量噪声。对于每个数据集，考虑两个用于描述严格为正、单调递减信号的竞争参数模型：指数衰减模型 $y(x)=A\\,e^{-\\lambda x}$（参数 $A>0$ 和 $\\lambda\\ge 0$），以及幂律模型 $y(x)=C\\,x^{-\\alpha}$（参数 $C>0$ 和 $\\alpha>0$）。对每个模型和每个数据集，将卡方统计量定义为\n$$\n\\chi^2(A,\\lambda)=\\sum_{i=1}^{N}\\left(\\frac{y_i-Ae^{-\\lambda x_i}}{\\sigma_i}\\right)^2,\\qquad\n\\chi^2(C,\\alpha)=\\sum_{i=1}^{N}\\left(\\frac{y_i-Cx_i^{-\\alpha}}{\\sigma_i}\\right)^2,\n$$\n并将最佳拟合参数定义为在指定参数域上使相应 $\\chi^2$ 最小化的参数。令 $p=2$ 为每个模型中的自由参数数量，令 $\\nu=N-p$ 为自由度，并为每个模型定义约化卡方为 $\\chi^2_\\nu=\\chi^2/\\nu$（当 $\\nu>0$ 时）。对于 $\\nu\\le 0$ 的每个数据集，模型比较结果视为无定论。对于 $\\nu>0$ 的每个数据集，使用以下规则确定优选模型：如果 $\\lvert \\chi^2_{\\nu,\\mathrm{pow}}-\\chi^2_{\\nu,\\mathrm{exp}}\\rvert \\le \\varepsilon$，则宣布结果为无定论；否则，优选具有较小约化卡方的模型。使用容差 $\\varepsilon=10^{-2}$。您的程序必须处理以下数据集测试套件，并得出所需的决策。\n\n测试套件（无量纲）：\n- 数据集 $\\mathrm{D1}$（指数生成，异方差）：\n  - 横坐标：$x_i=i$ 对于 $i\\in\\{1,2,3,4,5,6,7,8\\}$，因此 $N=8$。\n  - 纵坐标：$y_i = A\\,e^{-\\lambda x_i}$，其中 $A=2.5$ 和 $\\lambda=0.5$。\n  - 标准差：$\\sigma_i = 0.05 + 0.01\\,y_i$。\n- 数据集 $\\mathrm{D2}$（幂律生成，同方差）：\n  - 横坐标：$x_i=i$ 对于 $i\\in\\{1,2,3,4,5,6,7,8\\}$，因此 $N=8$。\n  - 纵坐标：$y_i = C\\,x_i^{-\\alpha}$，其中 $C=1.8$ 和 $\\alpha=1.4$。\n  - 标准差：对于所有 $i$，$\\sigma_i = 0.06$。\n- 数据集 $\\mathrm{D3}$（退化边界情况）：\n  - 横坐标：$x_1=1.0$, $x_2=3.0$，因此 $N=2$。\n  - 纵坐标：$y_1=2.0$, $y_2=0.4$。\n  - 标准差：$\\sigma_1=0.05$, $\\sigma_2=0.05$。\n\n您的任务是，对每个数据集，计算使每个模型的卡方最小化的最佳拟合参数，计算约化卡方值 $\\chi^2_{\\nu,\\mathrm{exp}}$ 和 $\\chi^2_{\\nu,\\mathrm{pow}}$（当有定义时），然后使用容差为 $\\varepsilon=10^{-2}$ 的决策规则，为每个数据集输出一个整数代码：如果指数模型更优，输出 $0$；如果幂律模型更优，输出 $1$；如果比较结果为无定论（因为至少一个模型的 $\\nu\\le 0$ 或因为约化卡方值的差异最多为 $\\varepsilon$），则输出 $-1$。所有量均为无单位的。您的程序应生成单行输出，其中包含分别为 $\\mathrm{D1}$、$\\mathrm{D2}$ 和 $\\mathrm{D3}$ 的三个整数决策代码，形式为方括号内以逗号分隔的列表（例如，$[0,1,-1]$）。", "solution": "问题陈述经过严格验证，确认有效。这是一个计算物理学领域中的适定问题，特别是在数据分析和模型比较方面。该问题具有科学依据、客观性，并为其解决提供了一套完整且一致的数据和规则。不存在科学或逻辑上的缺陷。\n\n核心任务是为三个不同的数据集比较两个竞争模型——指数衰减模型 $y(x)=A\\,e^{-\\lambda x}$ 和幂律模型 $y(x)=C\\,x^{-\\alpha}$——的拟合优度。比较度量是约化卡方统计量 $\\chi^2_{\\nu}$，其定义为 $\\chi^2_{\\nu} = \\chi^2 / \\nu$，其中 $\\nu = N - p$ 代表自由度。这里，$N$ 是数据点的数量，$p=2$ 是每个模型中的自由参数数量。\n\n该分析的基础是针对每个模型和数据集最小化卡方函数。指数模型的卡方统计量由下式给出\n$$\n\\chi^2(A,\\lambda)=\\sum_{i=1}^{N}\\left(\\frac{y_i-Ae^{-\\lambda x_i}}{\\sigma_i}\\right)^2\n$$\n幂律模型的卡方统计量由下式给出\n$$\n\\chi^2(C,\\alpha)=\\sum_{i=1}^{N}\\left(\\frac{y_i-Cx_i^{-\\alpha}}{\\sigma_i}\\right)^2.\n$$\n最佳拟合参数 $(A, \\lambda)$ 和 $(C, \\alpha)$ 是在约束条件 $A>0$、$\\lambda\\ge 0$ 以及 $C>0$、$\\alpha>0$ 下使各自 $\\chi^2$ 值最小化的参数。\n\n由于两个模型在其参数上都是非线性的，找到 $\\chi^2$ 函数的最小值需要进行数值优化。一种拟牛顿法，特别是 L-BFGS-B 算法，适合此任务，因为它可以处理参数的边界约束。此类迭代优化算法的成功在很大程度上取决于为参数提供一个合理的初始猜测值。生成此类初始猜测的有效策略是线性化模型。\n\n对于指数模型，取自然对数可得 $\\ln(y) = \\ln(A) - \\lambda x$。这是 $\\ln(y)$ 和 $x$ 之间的线性关系。对 $\\ln(y_i)$ 与 $x_i$ 进行加权线性最小二乘拟合可以提供 $\\ln(A)$（截距）和 $-\\lambda$（斜率）的初始估计。权重必须考虑不确定性的传播，其中 $\\ln(y_i)$ 的方差近似为 $(\\sigma_i/y_i)^2$。\n\n同样，对于幂律模型，对数变换 $\\ln(y) = \\ln(C) - \\alpha \\ln(x)$ 建立了 $\\ln(y)$ 和 $\\ln(x)$ 之间的线性关系。对 $\\ln(y_i)$ 与 $\\ln(x_i)$ 进行加权线性最小二乘拟合可为 $\\ln(C)$ 和 $-\\alpha$ 提供初始估计，使用相同的不确定性传播来计算权重。\n\n每个数据集的处理流程如下：\n1.  确定数据点数 $N$，并计算自由度 $\\nu = N - p = N - 2$。\n2.  如果 $\\nu \\le 0$，问题陈述规定比较结果为无定论。这适用于数据集 $\\mathrm{D3}$，其中 $N=2$，因此 $\\nu=0$。\n3.  如果 $\\nu > 0$，则对两个模型继续进行拟合程序。\n    a. 对于每个模型，使用线性化加权最小二乘法生成初始参数猜测。\n    b. 使用 L-BFGS-B 算法，从初始猜测开始并遵守参数边界，寻找使非线性 $\\chi^2$ 函数最小化的参数。\n    c. 记录最小化的卡方值 $\\chi^2_{\\mathrm{min}}$。\n    d. 计算约化卡方 $\\chi^2_\\nu = \\chi^2_{\\mathrm{min}} / \\nu$。\n4.  应用决策规则：如果 $\\lvert \\chi^2_{\\nu,\\mathrm{pow}}-\\chi^2_{\\nu,\\mathrm{exp}}\\rvert \\le \\varepsilon$，其中容差为 $\\varepsilon=10^{-2}$，则结果为无定论。否则，优选具有较小 $\\chi^2_\\nu$ 值的模型。\n\n基于此严谨流程，分配整数代码：指数模型为 $0$，幂律模型为 $1$，无定论结果为 $-1$。这些数据集是合成的，并且生成时没有加入随机噪声，这意味着对于数据集 $\\mathrm{D1}$（指数）和数据集 $\\mathrm{D2}$（幂律），正确的模型应产生一个数值上接近于零的 $\\chi^2$ 值，从而确保一个决定性的比较结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the model comparison problem for three datasets by performing\n    chi-squared minimization and applying a decision rule.\n    \"\"\"\n    \n    # Define the decision rule tolerance\n    EPSILON = 1e-2\n    # Number of free parameters in each model\n    P_PARAMS = 2\n\n    # --- Dataset Definitions ---\n    # Dataset D1 (exponential-generated, heteroscedastic)\n    x1 = np.arange(1, 9, dtype=float)\n    A_true_d1, lambda_true_d1 = 2.5, 0.5\n    y1 = A_true_d1 * np.exp(-lambda_true_d1 * x1)\n    sigma1 = 0.05 + 0.01 * y1\n    N1 = len(x1)\n    dataset1 = (x1, y1, sigma1, N1)\n\n    # Dataset D2 (power-law-generated, homoscedastic)\n    x2 = np.arange(1, 9, dtype=float)\n    C_true_d2, alpha_true_d2 = 1.8, 1.4\n    y2 = C_true_d2 * x2**(-alpha_true_d2)\n    sigma2 = np.full_like(x2, 0.06)\n    N2 = len(x2)\n    dataset2 = (x2, y2, sigma2, N2)\n\n    # Dataset D3 (degenerate edge case)\n    x3 = np.array([1.0, 3.0])\n    y3 = np.array([2.0, 0.4])\n    sigma3 = np.array([0.05, 0.05])\n    N3 = len(x3)\n    dataset3 = (x3, y3, sigma3, N3)\n\n    test_cases = [dataset1, dataset2, dataset3]\n\n    # --- Fitting and Analysis Functions ---\n    def get_initial_guess(x, y, sigma, model_type):\n        \"\"\"\n        Calculates initial parameter guesses by linearizing the model\n        and performing a weighted linear least-squares fit.\n        \"\"\"\n        # Filter out non-positive y values for log transformation\n        valid_indices = y > 0\n        if not np.any(valid_indices):\n            return [1.0, 1.0] # Default guess if no valid data\n        x_f, y_f, sigma_f = x[valid_indices], y[valid_indices], sigma[valid_indices]\n\n        weights = (y_f / sigma_f)**2\n\n        if model_type == 'exp':\n            # Linear model: log(y) = log(A) - lambda * x\n            fit_x = x_f\n            fit_y = np.log(y_f)\n            m, b = np.polyfit(fit_x, fit_y, 1, w=weights)\n            A0, lambda0 = np.exp(b), -m\n            return [A0, lambda0]\n        elif model_type == 'pow':\n            # Linear model: log(y) = log(C) - alpha * log(x)\n            fit_x = np.log(x_f)\n            fit_y = np.log(y_f)\n            m, b = np.polyfit(fit_x, fit_y, 1, w=weights)\n            C0, alpha0 = np.exp(b), -m\n            return [C0, alpha0]\n        return [1.0, 1.0]\n\n    def fit_model_and_get_chi2(x, y, sigma, model_type):\n        \"\"\"\n        Performs chi-squared minimization for a given model and dataset.\n        Returns the minimized chi-squared value.\n        \"\"\"\n        if model_type == 'exp':\n            model_func = lambda p, x_d: p[0] * np.exp(-p[1] * x_d)\n            bounds = [(1e-9, None), (0.0, None)]  # A > 0, lambda >= 0\n        elif model_type == 'pow':\n            model_func = lambda p, x_d: p[0] * x_d**(-p[1])\n            bounds = [(1e-9, None), (1e-9, None)]  # C > 0, alpha > 0\n        else:\n            raise ValueError(\"Unknown model type\")\n\n        chi2_func = lambda p: np.sum(((y - model_func(p, x)) / sigma)**2)\n        \n        initial_guess = get_initial_guess(x, y, sigma, model_type)\n        # Ensure initial guess is within bounds\n        initial_guess[0] = max(bounds[0][0], initial_guess[0]) if bounds[0][1] is None else min(bounds[0][1], max(bounds[0][0], initial_guess[0]))\n        initial_guess[1] = max(bounds[1][0], initial_guess[1]) if bounds[1][1] is None else min(bounds[1][1], max(bounds[1][0], initial_guess[1]))\n\n        result = minimize(chi2_func, initial_guess, method='L-BFGS-B', bounds=bounds)\n        return result.fun\n\n    # --- Main Loop for Processing Test Cases ---\n    results = []\n    for x_data, y_data, sigma_data, N in test_cases:\n        nu = N - P_PARAMS\n\n        if nu <= 0:\n            results.append(-1)\n            continue\n\n        chi2_min_exp = fit_model_and_get_chi2(x_data, y_data, sigma_data, 'exp')\n        chi2_min_pow = fit_model_and_get_chi2(x_data, y_data, sigma_data, 'pow')\n\n        chi2_nu_exp = chi2_min_exp / nu\n        chi2_nu_pow = chi2_min_pow / nu\n\n        if abs(chi2_nu_pow - chi2_nu_exp) <= EPSILON:\n            results.append(-1)\n        elif chi2_nu_exp < chi2_nu_pow:\n            results.append(0)  # Exponential model preferred\n        else:\n            results.append(1)  # Power-law model preferred\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2379487"}, {"introduction": "最后的这个练习将探讨一个在数据分析中微妙但至关重要的统计陷阱，即“别处张望效应”（Look-Elsewhere Effect）。当你在一个庞大的数据集中寻找一个位置不确定的信号时，你偶然发现一个看起来像信号的随机波动的概率会大大增加。在本练习中，你将通过蒙特卡洛模拟来量化这一效应，亲眼观察当你搜索范围扩大时，最佳拟合的 $\\chi^2$ 统计量分布会如何偏移。这对于在科学研究中避免做出错误的“发现”是一堂至关重要的课程。[@problem_id:2379498]", "problem": "您将研究在多个可能位置搜索局域信号对最佳拟合卡方值分布的统计影响。考虑一个由$N$个采样点组成的一维数组，其位置为 $x_j$ (其中 $j \\in \\{0,1,\\dots,N-1\\}$)，且 $x_j = j/(N-1)$。对于每次模拟试验，生成独立的纯噪声数据 $y_j$，其中 $y_j \\sim \\mathcal{N}(0,\\sigma^2)$ 且 $\\sigma = 1$。对于一个固定的候选中心 $\\mu$ 和一个固定的宽度 $s > 0$，定义模板 $g_j(\\mu) = \\exp\\!\\big(-\\tfrac{(x_j - \\mu)^2}{2 s^2}\\big)$。该试验数据的信号加噪声模型为 $y_j \\approx A\\, g_j(\\mu)$，其中 $A$ 是一个未知振幅。对于每个候选 $\\mu$，拟合优度由卡方值量化：\n$$\n\\chi^2(\\mu) = \\sum_{j=0}^{N-1} \\frac{\\big(y_j - A\\, g_j(\\mu)\\big)^2}{\\sigma^2},\n$$\n并且在该 $\\mu$ 处的最佳拟合卡方值是 $\\chi^2(\\mu)$ 关于 $A$ 的最小值。当在 $M$ 个候选位置 $\\{\\mu_i\\}_{i=0}^{M-1}$ 的有限集上搜索时，该试验在整个搜索中的最佳拟合卡方值为\n$$\n\\chi^2_{\\min} = \\min_{0 \\le i \\le M-1} \\left[ \\min_{A \\in \\mathbb{R}} \\chi^2(\\mu_i) \\right].\n$$\n所有试验都必须是独立的，并且所有候选中心都必须确定性地选择为 $[0,1]$ 区间内的$M$个等间距点，即 $\\mu_i = i/(M-1)$ (其中 $i \\in \\{0,1,\\dots,M-1\\}$)；当 $M=1$ 时，取 $\\mu_0=0$。噪声方差 $\\sigma^2$ 必须视为已知且等于 $1$。\n\n对于下方的每个测试用例，运行 $T$ 次如上所述构建的独立试验。在每次试验中，计算 $\\chi^2_{\\min}$。在所有 $T$ 次试验中，计算以下三个量：\n- $\\chi^2_{\\min}$ 的经验均值。\n- $\\chi^2_{\\min}$ 的经验标准差。\n- 在 $\\chi^2_{\\min}$ 小于或等于自由度为 $N-1$ 的卡方分布的第5百分位数的试验中所占的经验比例（自由度是独立信息片段的数量，缩写为DoF）。此比例必须表示为小数，而不是百分比。\n\n使用单一伪随机数生成器，在程序开始时用种子 $123456789$ 初始化一次，并且不要在测试用例之间重新设定种子。模板宽度 $s$ 在每个测试用例中是固定的，具体如下所述。所有算术运算都是无量纲的；不涉及物理单位。\n\n测试套件 (每个元组为 $(N,M,s,T)$):\n- 用例 1: $(64, 1, 0.08, 5000)$。\n- 用例 2: $(64, 16, 0.08, 5000)$。\n- 用例 3: $(64, 64, \\tfrac{1}{2\\,(64-1)}, 5000)$。\n\n您的程序必须生成单行输出，其中按顺序包含用例 1、2 和 3 的结果，格式为一个由三元素列表组成的逗号分隔列表，其中每个内部列表中的三个数字分别如上定义的经验均值、经验标准差和经验比例。每个数字必须四舍五入到恰好六位小数。要求的最终输出格式为\n\"[ [mean1,std1,frac1], [mean2,std2,frac2], [mean3,std3,frac3] ]\"\n除了所示空格外没有其他空格，并且所有数字都以小数形式呈现；例如，\n\"[[12.345678,9.876543,0.042000],[...],[...]]\"。", "solution": "核心对象是针对单个候选位置 $\\mu$ 的卡方函数，\n$$\n\\chi^2(\\mu;A) \\equiv \\sum_{j=0}^{N-1} \\frac{\\big(y_j - A\\, g_j(\\mu)\\big)^2}{\\sigma^2},\n$$\n其中噪声方差 $\\sigma^2 = 1$ 已知，数据为试验数据 $y_j$，模板 $g_j(\\mu) = \\exp\\!\\big(-\\tfrac{(x_j - \\mu)^2}{2 s^2}\\big)$ 定义在网格 $x_j = j/(N-1)$ 上。在固定 $\\mu$ 时的最佳拟合振幅 $A$ 能使 $\\chi^2(\\mu;A)$ 最小化。对 $A$ 求导并令其为零，得到正规方程\n$$\n\\frac{\\partial \\chi^2(\\mu;A)}{\\partial A} = -2 \\sum_{j=0}^{N-1} \\frac{g_j(\\mu)}{\\sigma^2} \\left(y_j - A\\, g_j(\\mu)\\right) = 0,\n$$\n解得\n$$\n\\widehat{A}(\\mu) = \\frac{\\sum_{j=0}^{N-1} g_j(\\mu)\\, y_j}{\\sum_{j=0}^{N-1} \\big(g_j(\\mu)\\big)^2}.\n$$\n将 $\\widehat{A}(\\mu)$ 代回到 $\\chi^2(\\mu;A)$ 中，该 $\\mu$ 对应的最小化卡方值可以写作闭合形式\n$$\n\\chi^2_{\\min}(\\mu) = \\sum_{j=0}^{N-1} \\frac{y_j^2}{\\sigma^2} - \\frac{\\left(\\sum_{j=0}^{N-1} g_j(\\mu)\\, y_j\\right)^2}{\\sigma^2 \\sum_{j=0}^{N-1} \\big(g_j(\\mu)\\big)^2}.\n$$\n当 $\\sigma^2=1$ 时，上式简化为\n$$\n\\chi^2_{\\min}(\\mu) = \\sum_{j=0}^{N-1} y_j^2 - \\frac{\\left(\\sum_{j=0}^{N-1} g_j(\\mu)\\, y_j\\right)^2}{\\sum_{j=0}^{N-1} \\big(g_j(\\mu)\\big)^2}.\n$$\n这个表达式有一个清晰的几何解释：令 $\\mathbf{y}\\in\\mathbb{R}^N$ 为数据向量，$\\mathbf{g}(\\mu)\\in\\mathbb{R}^N$ 为模板向量。最小二乘拟合移除了 $\\mathbf{y}$ 中平行于 $\\mathbf{g}(\\mu)$ 的分量，剩下的是正交残差的平方范数。即：\n$$\n\\chi^2_{\\min}(\\mu) = \\|\\mathbf{y}\\|_2^2 - \\frac{\\langle \\mathbf{g}(\\mu), \\mathbf{y}\\rangle^2}{\\|\\mathbf{g}(\\mu)\\|_2^2}.\n$$\n在纯噪声零假设下，即 $\\mathbf{y}\\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})$ 且 $\\mu$ 固定，最小化的卡方值 $\\chi^2_{\\min}(\\mu)$ 服从自由度 (DoF) 为 $N-1$ 的卡方分布。这是因为拟合了一个线性参数，因此从 $N$ 维高斯分布中移除了一个维度（即 $\\mathbf{g}(\\mu)$ 的张成空间），剩下 $N-1$ 个独立的高斯分量贡献于平方和。具体来说，其均值为 $N-1$，方差为 $2(N-1)$。\n\n当我们在多个候选位置 $\\{\\mu_i\\}_{i=0}^{M-1}$ 上进行搜索并取集合 $\\{\\chi^2_{\\min}(\\mu_i)\\}$ 中的最小值时，就会出现“别处张望效应”。因为我们是从几个相关的随机变量中选择最小的一个，所以 $\\chi^2_{\\min} \\equiv \\min_i \\chi^2_{\\min}(\\mu_i)$ 的分布与单个位置的情况相比会向更小的值偏移。因此，$\\chi^2_{\\min}$ 低于某个固定分位数（例如，$\\chi^2_{N-1}$ 分布的第5百分位数）的概率会随着 $M$ 的增大而增大。\n\n从算法上讲，对于每个具有指定 $(N,M,s,T)$、固定网格 $x_j = j/(N-1)$ 和候选中心 $\\mu_i = i/(M-1)$（如果 $M=1$，则 $\\mu_0=0$）的测试用例，可通过以下步骤计算所需的统计量：\n1. 预计算$M\\times N$的模板矩阵 $G$，其元素为 $G_{i j} = g_j(\\mu_i)$，以及向量 $S\\in\\mathbb{R}^M$，其元素为 $S_i = \\sum_{j=0}^{N-1} G_{i j}^2$。\n2. 对于 $T$ 次试验中的每一次，抽取 $\\mathbf{y}\\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$，计算 $q = \\|\\mathbf{y}\\|_2^2$，计算 $M$ 个内积 $\\mathbf{h} = G\\,\\mathbf{y}$，并通过 $\\chi^2_{\\min}(\\mu_i) = q - h_i^2/S_i$ 获得 $M$ 个最小化的卡方值。\n3. 记录该次试验的 $\\chi^2_{\\min} = \\min_i \\chi^2_{\\min}(\\mu_i)$。\n4. 在 $T$ 次试验之后，计算经验均值 $\\overline{\\chi^2_{\\min}}$、经验标准差，以及满足 $\\chi^2_{\\min} \\le q_{0.05}$ 的试验所占的经验比例，其中 $q_{0.05}$ 是 $\\chi^2_{N-1}$ 分布的第5百分位数。第5百分位数 $q_{0.05}$ 由累积分布函数 (CDF) 定义：$F_{\\chi^2_{N-1}}(q_{0.05}) = 0.05$。\n\n对于用例1，参数为 $(N,M,s,T)=(64,1,0.08,5000)$，在纯噪声设置下，$\\chi^2_{\\min}$ 的分布与 $\\chi^2_{63}$ 完全匹配，因此理论均值为 $64-1=63$，理论标准差为 $\\sqrt{2(64-1)} = \\sqrt{126}$。考虑到 $T=5000$，模拟得出的经验估计值将接近这些理论值。对于用例2和用例3，其中 $M$ 分别为 $16$ 和 $64$，经验均值将严格小于 $63$，而低于 $\\chi^2_{63}$ 分布第5百分位数的经验比例将大于 $0.05$，这说明了“别处张望效应”。程序完全按照规定计算这些量，使用一个以种子 $123456789$ 初始化一次的伪随机数生成器，并以要求的单行格式为每个用例输出三个所要求的值，四舍五入到六位小数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef simulate_case(N, M, s, T, rng):\n    # Grid of sample positions x_j in [0,1]\n    x = np.linspace(0.0, 1.0, N)\n    # Candidate centers mu_i, equally spaced in [0,1]; if M==1, mu=[0.0]\n    if M == 1:\n        mu = np.array([0.0])\n    else:\n        mu = np.linspace(0.0, 1.0, M)\n    # Build template matrix G of shape (M, N)\n    # G[i, j] = exp( - (x_j - mu_i)^2 / (2 s^2) )\n    # Handle very small s robustly: s > 0 by construction in test suite.\n    G = np.exp(-0.5 * ((x[None, :] - mu[:, None]) / s) ** 2)\n    # Precompute S_i = sum_j G[i,j]^2\n    S = np.sum(G * G, axis=1)  # shape (M,)\n    # Draw all trials at once: Y shape (T, N), y_j ~ N(0,1)\n    Y = rng.normal(loc=0.0, scale=1.0, size=(T, N))\n    # q_t = sum_j Y[t,j]^2\n    q = np.einsum('ij,ij->i', Y, Y)  # shape (T,)\n    # Compute H = G @ Y^T, shape (M, T)\n    H = G @ Y.T\n    # Compute chi2 for each (i,t): chi2(i,t) = q_t - H[i,t]^2 / S[i]\n    # Broadcast S over trials\n    chi2_all = q[None, :] - (H * H) / S[:, None]\n    # Minimum across candidate locations for each trial\n    chi2_min = np.min(chi2_all, axis=0)  # shape (T,)\n    # Empirical statistics\n    mean_val = float(np.mean(chi2_min))\n    std_val = float(np.std(chi2_min, ddof=0))\n    # Threshold at 5th percentile of chi2 with df = N - 1\n    df = N - 1\n    thresh = chi2.ppf(0.05, df=df)\n    frac_low = float(np.mean(chi2_min <= thresh))\n    return mean_val, std_val, frac_low\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple: (N, M, s, T)\n    test_cases = [\n        (64, 1, 0.08, 5000),\n        (64, 16, 0.08, 5000),\n        (64, 64, 1.0 / (2.0 * (64 - 1)), 5000),\n    ]\n\n    # Initialize a single RNG with fixed seed and do not reseed between cases\n    rng = np.random.default_rng(123456789)\n\n    results = []\n    for N, M, s, T in test_cases:\n        mean_val, std_val, frac_low = simulate_case(N, M, s, T, rng)\n        results.append((mean_val, std_val, frac_low))\n\n    # Build the exact output string: list of lists with numbers rounded to 6 decimals, no extra spaces\n    formatted_cases = []\n    for mean_val, std_val, frac_low in results:\n        formatted = f\"[{mean_val:.6f},{std_val:.6f},{frac_low:.6f}]\"\n        formatted_cases.append(formatted)\n    output = \"[\" + \",\".join(formatted_cases) + \"]\"\n\n    # Final print statement in the exact required format.\n    print(output)\n\nsolve()\n```", "id": "2379498"}]}