{"hands_on_practices": [{"introduction": "理论与实践相结合是掌握任何科学工具的关键。在本节中，我们将从一个核心概念——归纳偏置（inductive bias）——开始。这个练习将引导你比较两种不同的神经网络结构——多层感知机（MLP）和卷积神经网络（CNN）——在学习一个具有平移不变性偏微分方程（PDE）解算子时的表现。通过这个直接的对比，你将亲身体会到，将问题的已知对称性（如平移不变性）融入网络架构，是如何极大地提升模型泛化能力的 [@problem_id:2417315]。", "problem": "考虑一个定义在周期性域上的一维、平移不变的椭圆偏微分方程 (PDE)，其形式为 $-u''(x) + a\\,u(x) = f(x)$，在 $[0,1)$ 上具有周期性边界条件。在具有 $N$ 个点和间距 $h = 1/N$ 的均匀网格上对该问题进行离散化。标准的中心二阶差分法会为离散解向量 $u \\in \\mathbb{R}^N$ 和驱动向量 $f \\in \\mathbb{R}^N$ 导出一个循环线性系统。该离散算子可通过离散傅里叶变换进行对角化：每个由整数 $k \\in \\{0,1,\\dots,N-1\\}$ 索引的模式的特征值为 $\\lambda_k = a + \\dfrac{4}{h^2}\\sin^2\\!\\left(\\dfrac{\\pi k}{N}\\right)$。因此，通过将 $f$ 的傅里叶系数除以 $\\lambda_k$ 并变换回物理空间，可以获得精确的离散解。\n\n在本问题中，您将从第一性原理出发，为解算子 $T: f \\mapsto u$ 实现两种神经网络代理模型，并仅用单个输入-输出样本进行训练，然后比较它们的性能，以展示归纳偏置对平移不变算子的影响：\n\n- 一个没有隐藏层的全连接多层感知器 (MLP) (即单个线性映射)，由一个密集矩阵 $W \\in \\mathbb{R}^{N \\times N}$ 表示，其作用方式为 $u_{\\text{MLP}} = W f$。\n\n- 一个一维循环卷积神经网络 (CNN)，带有一个线性卷积层，其核的长度为奇数 $K$，由一个核向量 $w \\in \\mathbb{R}^{K}$ 表示，通过循环卷积作用 $u_{\\text{CNN}}[i] = \\sum_{s=-m}^{m} w[s+m]\\; f[(i - s) \\bmod N]$，其中 $m = (K-1)/2$。\n\n两种模型都将通过最小化模型输出与上述傅里叶对角化方法产生的精确离散解之间的均方误差损失来进行训练。使用具有固定学习率和固定迭代次数的简单梯度下降法。将所有可训练参数初始化为零。任何三角表达式中的角度都必须以弧度为单位。\n\n您的实现必须遵循以下固定的、科学上一致的参数选择和数据集定义：\n\n- 网格大小：$N = 64$ 且间距 $h = 1/N$。\n\n- PDE 参数：$a = 1$。\n\n- CNN 核大小：$K = 31$ (因此 $m = 15$)。\n\n- 训练集：单个训练对 $(f^{\\text{train}}, u^{\\text{train}})$，其中 $f^{\\text{train}}$ 是索引为 $0$ 的单位脉冲 (即 $f^{\\text{train}}[0] = 1$ 且对于所有 $i \\neq 0$，$f^{\\text{train}}[i] = 0$)，而 $u^{\\text{train}}$ 是通过傅里叶对角化计算出的该驱动力的精确离散解。\n\n- 优化细节：对 MLP 使用恒定步长的梯度下降法，迭代次数 $T_{\\text{MLP}} = 2000$，学习率 $\\eta_{\\text{MLP}} = 0.2$；对 CNN 使用迭代次数 $T_{\\text{CNN}} = 2000$，学习率 $\\eta_{\\text{CNN}} = 0.05$。对于损失为 $L = \\dfrac{1}{N} \\|W f - u\\|_2^2$ 的 MLP，通过 $W \\leftarrow W - \\eta_{\\text{MLP}} \\dfrac{2}{N} (W f - u) f^\\top$ 进行更新。对于损失为 $L = \\dfrac{1}{N} \\|w \\circledast f - u\\|_2^2$ 的 CNN (其中 $\\circledast$ 表示如上定义的循环卷积)，通过 $w[s+m] \\leftarrow w[s+m] - \\eta_{\\text{CNN}} \\dfrac{2}{N} \\sum_{i=0}^{N-1} \\big( (w \\circledast f)[i] - u[i] \\big)\\, f[(i - s) \\bmod N]$ (对于所有 $s \\in \\{-m,\\dots,m\\}$) 进行更新。\n\n设计一个测试套件，使用四个固定的驱动力 $f^{(j)}$ 及其精确的离散解 $u^{(j)}$ 来评估平移泛化能力和频率响应：\n\n- 情况 1：$f^{(1)}$ 是索引为 $0$ 的单位脉冲。\n\n- 情况 2：$f^{(2)}$ 是索引为 $16$ 的单位脉冲。\n\n- 情况 3：$f^{(3)}[i] = \\cos\\!\\left(2 \\pi \\cdot 4 \\cdot \\dfrac{i}{N}\\right)$ 对于所有 $i \\in \\{0,\\dots,N-1\\}$ (角度以弧度为单位)。\n\n- 情况 4：$f^{(4)}[i] = 1$ 对于所有 $i \\in \\{0,\\dots,N-1\\}$。\n\n对于每种情况，分别计算 MLP 和 CNN 每个模型相对于精确解的相对 $\\ell_2$ 误差，定义为 $\\varepsilon_{\\text{model}} = \\dfrac{\\|u_{\\text{model}} - u^{(j)}\\|_2}{\\|u^{(j)}\\|_2}$。\n\n您的程序必须实现以上所有内容，使用指定的超参数，并生成单行输出，其中包含按以下顺序排列的八个浮点数结果：$[\\varepsilon_{\\text{MLP}}^{(1)}, \\varepsilon_{\\text{CNN}}^{(1)}, \\varepsilon_{\\text{MLP}}^{(2)}, \\varepsilon_{\\text{CNN}}^{(2)}, \\varepsilon_{\\text{MLP}}^{(3)}, \\varepsilon_{\\text{CNN}}^{(3)}, \\varepsilon_{\\text{MLP}}^{(4)}, \\varepsilon_{\\text{CNN}}^{(4)}]$。该行必须以方括号括起来的逗号分隔列表形式打印，不含任何额外文本。\n\n在所选的 PDE 无量纲化方案下，所有物理量均为无量纲。确保您的代码是一个完整的、可运行的程序，不需要任何输入，并且只使用允许的库。最终答案必须以指定的单行格式表示为浮点数。", "solution": "所提出的问题是有效的。它在数值分析和计算物理学中具有科学依据，具体涉及偏微分方程的数值解以及机器学习方法作为代理模型的现代应用。该问题是适定的 (well-posed)，所有参数、方法和目标都得到了清晰明确的定义。它构成了一个标准的、尽管是简化的科学机器学习练习，旨在展示神经网络架构中归纳偏置的概念。因此，我将继续提供一个完整的解决方案。\n\n该问题要求比较两种神经网络模型——多层感知器 (MLP) 和卷积神经网络 (CNN)——学习一维椭圆偏微分方程解算子的能力。分析将分四个阶段进行：首先，建立 PDE 的精确数值解作为基准真相；其次，构建和训练 MLP 模型；第三，构建和训练 CNN 模型；最后，在一系列测试用例上评估两个已训练的模型，以评估它们的泛化能力。\n\n**1. 控制方程及其精确离散解**\n\n该物理系统由定义在域 $x \\in [0,1)$ 上的一维二阶椭圆偏微分方程描述，具有周期性边界条件：\n$$ -u''(x) + a\\,u(x) = f(x) $$\n这里，$u(x)$ 是解场，$f(x)$ 是源项或驱动项，$a$ 是一个正常数，给定为 $a=1$。在具有 $N=64$ 个点 $x_i = i h$（其中 $i=0, \\dots, N-1$）和间距 $h=1/N$ 的均匀网格上离散化该方程，我们使用标准的二阶精确中心有限差分近似来处理二阶导数：\n$$ -u''(x_i) \\approx \\frac{-u_{i-1} + 2u_i - u_{i+1}}{h^2} $$\n其中 $u_i = u(x_i)$。将此应用于 PDE 会得到一个关于离散解向量 $\\mathbf{u} \\in \\mathbb{R}^N$ 的线性方程组。由于周期性边界条件，得到的系统矩阵是一个循环矩阵。循环矩阵的一个基本性质是它们可以被离散傅里叶变换 (DFT) 对角化。这意味着如果我们将方程转换到傅里叶域，微分算子就变成了一个简单的乘法。\n\n令 $\\hat{\\mathbf{u}}$ 和 $\\hat{\\mathbf{f}}$ 分别为 $\\mathbf{u}$ 和 $\\mathbf{f}$ 的 DFT。在傅里叶域中的线性系统是：\n$$ \\lambda_k \\hat{u}_k = \\hat{f}_k $$\n对于每个傅里叶模式 $k \\in \\{0, 1, \\dots, N-1\\}$。离散算子的特征值 $\\lambda_k$ 由问题陈述给出：\n$$ \\lambda_k = a + \\frac{4}{h^2}\\sin^2\\left(\\frac{\\pi k}{N}\\right) $$\n当 $a=1$ 且 $N=64$ 时，所有特征值 $\\lambda_k$ 均为严格正数，这保证了算子是可逆的，并且对于任何驱动 $\\mathbf{f}$ 都存在唯一解。精确的离散解 $\\mathbf{u}$ 可以通过在傅里叶域中求解 $\\hat{u}_k = \\hat{f}_k / \\lambda_k$，然后应用逆 DFT 返回到物理域来找到：\n$$ \\mathbf{u} = \\text{IDFT}\\left(\\frac{\\text{DFT}(\\mathbf{f})}{\\mathbf{\\lambda}}\\right) $$\n该方法为我们的神经网络代理模型提供了用于训练和测试的基准真相数据。\n\n**2. 多层感知器 (MLP) 模型**\n\n第一个模型是一个没有隐藏层的全连接网络，这等同于一个由密集矩阵 $W \\in \\mathbb{R}^{N \\times N}$ 表示的单一线性变换。该模型的预测是：\n$$ \\mathbf{u}_{\\text{MLP}} = W \\mathbf{f} $$\n这个模型是完全通用的，并且对解算子不作任何先验结构假设。它有 $N^2 = 64^2 = 4096$ 个可训练参数。\n\n该模型在单个数据对 $(\\mathbf{f}^{\\text{train}}, \\mathbf{u}^{\\text{train}})$ 上进行训练，其中 $\\mathbf{f}^{\\text{train}}$ 是在索引 0 处的单位脉冲（即 $\\mathbf{f}^{\\text{train}} = \\mathbf{e}_0$，第一个标准基向量）。训练通过梯度下降来最小化均方误差损失 $L = \\frac{1}{N} \\|\\mathbf{u}_{\\text{MLP}} - \\mathbf{u}^{\\text{train}}\\|_2^2$。损失函数关于权重矩阵 $W$ 的梯度是 $\\nabla_W L = \\frac{2}{N} (\\mathbf{u}_{\\text{MLP}} - \\mathbf{u}^{\\text{train}}) (\\mathbf{f}^{\\text{train}})^\\top$。给定 $\\mathbf{f}^{\\text{train}} = \\mathbf{e}_0$，外积 $(\\mathbf{u}_{\\text{MLP}} - \\mathbf{u}^{\\text{train}}) \\mathbf{e}_0^\\top$ 是一个矩阵，其第一列是误差向量 $(\\mathbf{u}_{\\text{MLP}} - \\mathbf{u}^{\\text{train}})$，而所有其他列均为零。\n\n关键在于，由于权重初始化为 $W=0$ 且训练输入始终是 $\\mathbf{e}_0$，梯度仅在 $W$ 的第一列上非零。因此，在训练过程中，只有 $W$ 的第一列会被更新。MLP 实际上学会了将输入向量的第一个分量 $f_0$ 映射到一个输出向量，而忽略所有其他输入分量 $f_i$ ($i>0$) 。这种架构缺乏平移不变性的*归纳偏置*，而这是底层物理系统的一个关键属性。因此，我们预测它将无法泛化到被平移或具有不同频率内容的输入。\n\n**3. 卷积神经网络 (CNN) 模型**\n\n第二个模型是一个一维循环卷积层。其预测由输入 $\\mathbf{f}$ 与一个可学习的核 $\\mathbf{w} \\in \\mathbb{R}^K$ 的循环卷积给出：\n$$ \\mathbf{u}_{\\text{CNN}} = \\mathbf{w} \\circledast \\mathbf{f} $$\n核大小为 $K=31$。此操作定义为：\n$$ u_{\\text{CNN}}[i] = \\sum_{s=-m}^{m} w[s+m]\\; f[(i - s) \\bmod N] $$\n其中 $m = (K-1)/2 = 15$。CNN 模型只有 $K=31$ 个可训练参数。这种架构明确地内置了平移不变性的假设，因为卷积是一种对平移具有等变性的操作。对于给定的物理算子，这是一个正确的归纳偏置，因为该算子是线性平移不变 (LTI) 的。一个 LTI 系统对任意输入的响应是输入与系统脉冲响应（格林函数）的卷积。\n\nCNN 在相同的脉冲-响应对 $(\\mathbf{f}^{\\text{train}}, \\mathbf{u}^{\\text{train}})$ 上进行训练。由于将一个核与一个 delta 脉冲进行卷积会得到核本身（经过适当的填充或截断），训练过程实际上是在教导核 $\\mathbf{w}$ 成为系统真实脉冲响应 $\\mathbf{u}^{\\text{train}}$ 的一个近似。因为 CNN 已经学到了底层 LTI 算子核的一个近似，所以预期它能正确地泛化到任何输入，包括平移的脉冲和不同频率的输入。\n\n**4. 模型评估与预期结果**\n\n两种模型都在四个测试用例上进行评估：\n1.  索引 0 处的训练脉冲：测试拟合训练数据的能力。\n2.  索引 16 处的平移脉冲：测试对平移的泛化能力。\n3.  低频余弦波 ($\\cos(2\\pi \\cdot 4x)$)：测试对不同频率的泛化能力。\n4.  常数输入（零频波）：测试对最低频率模式的泛化能力。\n\n预期的结果将清晰地展示归纳偏置的力量。缺乏这种偏置的 MLP 应该只在第一个测试用例（其训练数据）上表现良好，而在所有其他用例上失败。拥有正确平移不变性偏置的 CNN 应该在所有四个测试用例上都表现良好，从而展示出从单个训练样本中获得的强大泛化能力。数值结果将通过相对 $\\ell_2$ 误差来量化这种性能差异。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport scipy.fft\n\ndef solve():\n    \"\"\"\n    Implements and compares MLP and CNN surrogates for a 1D elliptic PDE.\n    \"\"\"\n    # -- 1. Define constants and problem parameters --\n    N = 64\n    h = 1.0 / N\n    a = 1.0\n    K = 31\n    m = (K - 1) // 2\n    T_MLP = 2000\n    eta_MLP = 0.2\n    T_CNN = 2000\n    eta_CNN = 0.05\n\n    # -- 2. Define helper functions --\n    def exact_solver(f_vec):\n        \"\"\"\n        Computes the exact discrete solution using Fourier diagonalization.\n        \"\"\"\n        k = np.arange(N)\n        lambda_k = a + (4 / h**2) * np.sin(np.pi * k / N)**2\n        # Handle the case where a mode might be numerically zero, though not here.\n        # Add a small epsilon to the denominator for stability if needed.\n        f_hat = scipy.fft.fft(f_vec)\n        u_hat = f_hat / lambda_k\n        u_vec = scipy.fft.ifft(u_hat)\n        # The solution for a real forcing must be real.\n        return u_vec.real\n\n    def circular_conv(w_kernel, f_vec):\n        \"\"\"\n        Performs 1D circular convolution.\n        w_kernel has length K, f_vec has length N.\n        \"\"\"\n        y = np.zeros_like(f_vec, dtype=np.float64)\n        s_vals = np.arange(-m, m + 1)\n        for s_idx, s in enumerate(s_vals):\n            y += w_kernel[s_idx] * np.roll(f_vec, s)\n        return y\n\n    # -- 3. Generate training data --\n    f_train = np.zeros(N)\n    f_train[0] = 1.0\n    u_train = exact_solver(f_train)\n\n    # -- 4. Train the MLP model --\n    W = np.zeros((N, N))\n    for _ in range(T_MLP):\n        u_pred_mlp = W @ f_train\n        error_mlp = u_pred_mlp - u_train\n        # Gradient update for L = (1/N) * ||Wf - u||^2\n        grad_W = (2.0 / N) * np.outer(error_mlp, f_train)\n        W -= eta_MLP * grad_W\n\n    # -- 5. Train the CNN model --\n    w = np.zeros(K)\n    for _ in range(T_CNN):\n        u_pred_cnn = circular_conv(w, f_train)\n        error_cnn = u_pred_cnn - u_train\n        grad_w = np.zeros(K)\n        s_vals = np.arange(-m, m + 1)\n        \n        # Gradient for L = (1/N) * ||w*f - u||^2\n        # dL/dw_j = (2/N) * sum_i ( (w*f)_i - u_i ) * f_{i-j_s}\n        # where j_s is the shift corresponding to kernel element j.\n        for s_idx, s in enumerate(s_vals):\n            # The sum is the cross-correlation of the error and the input signal\n            grad_w[s_idx] = np.dot(error_cnn, np.roll(f_train, s))\n        \n        grad_w *= (2.0 / N)\n        w -= eta_CNN * grad_w\n\n    # -- 6. Define test cases and evaluate models --\n    # Case 1: Unit impulse at index 0 (training case)\n    f1 = np.zeros(N); f1[0] = 1.0\n    # Case 2: Unit impulse at index 16\n    f2 = np.zeros(N); f2[16] = 1.0\n    # Case 3: Cosine wave\n    i_indices = np.arange(N)\n    f3 = np.cos(2 * np.pi * 4 * i_indices / N)\n    # Case 4: Constant function (zero-frequency wave)\n    f4 = np.ones(N)\n\n    test_forcings = [f1, f2, f3, f4]\n    test_solutions = [exact_solver(f) for f in test_forcings]\n    \n    results = []\n    for f_test, u_exact in zip(test_forcings, test_solutions):\n        # Prevent division by zero, although not expected here\n        norm_u_exact = np.linalg.norm(u_exact)\n        if norm_u_exact == 0:\n            norm_u_exact = 1.0\n\n        # MLP prediction and error\n        u_mlp = W @ f_test\n        err_mlp = np.linalg.norm(u_mlp - u_exact) / norm_u_exact\n        results.append(err_mlp)\n        \n        # CNN prediction and error\n        u_cnn = circular_conv(w, f_test)\n        err_cnn = np.linalg.norm(u_cnn - u_exact) / norm_u_exact\n        results.append(err_cnn)\n\n    # -- 7. Print final results in the specified format --\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2417315"}, {"introduction": "在理解了良好架构带来的好处之后，我们必须面对函数逼近中的一个基本挑战：“维度灾难”（curse of dimensionality）。当一个函数的输入变量数量（即维度）增加时，其定义域的“体积”会呈指数级增长，这使得有效采样变得异常困难。这个实践将让你通过经验性测量来量化这一效应，直观地展示为了达到固定的逼近精度，所需的数据量是如何随着维度的增加而急剧膨胀的 [@problem_id:2417291]。", "problem": "要求您通过衡量达到固定逼近精度所需的训练样本数量如何随输入维度增长，来经验性地展示神经网络函数逼近中的维度灾难。请实现一个完整、可运行的程序，完成以下任务。\n\n从基本原理出发，考虑以下设置。\n\n- 定义域和目标函数：\n  - 对于每个输入维度 $D \\in \\{\\,1,2,4,8\\,\\}$，将定义域定义为单位超立方体 $[0,1]^D$。\n  - 将目标函数 $f_D:[0,1]^D \\to \\mathbb{R}$ 定义为\n    $$f_D(\\mathbf{x}) \\;=\\; \\prod_{i=1}^{D} \\sqrt{2}\\,\\sin\\!\\big(2\\pi x_i\\big),$$\n    其中角度以弧度为单位。乘法因子 $\\sqrt{2}$ 确保了在均匀分布下，$f_D$ 的均方幅值不会随着 $D$ 的变化而消失或爆炸。\n\n- 神经网络模型和训练方案：\n  - 使用一个带有整流线性单元（ReLU）激活函数的单隐藏层前馈神经网络。隐藏层有 $M$ 个固定的随机特征，仅通过岭回归正则化的线性最小二乘法训练输出层。这是一个有效的神经网络函数逼近器，也是使用随机特征逼近核机器的一种著名方法。\n  - 令 $\\phi:\\mathbb{R}^D \\to \\mathbb{R}^M$ 为隐藏特征映射，定义为\n    $$\\phi_j(\\mathbf{x}) \\;=\\; \\max\\!\\big(0,\\,\\mathbf{w}_j^\\top \\mathbf{x} + b_j\\big), \\quad j\\in\\{1,\\dots,M\\},$$\n    其中每个 $\\mathbf{w}_j \\in \\mathbb{R}^D$ 和 $b_j \\in \\mathbb{R}$ 均从标准正态分布中独立抽取。整体模型为\n    $$\\hat{f}_D(\\mathbf{x}) \\;=\\; \\mathbf{a}^\\top \\phi(\\mathbf{x}) + c,$$\n    其中 $\\mathbf{a}\\in\\mathbb{R}^M$ 和 $c\\in\\mathbb{R}$ 通过在大小为 $N$ 的训练集上最小化岭回归正则化的经验均方误差来训练。\n  - 训练过程简化为求解岭回归正则化的正规方程\n    $$(\\Phi^\\top \\Phi + \\lambda I)\\,\\boldsymbol{\\theta} \\;=\\; \\Phi^\\top \\mathbf{y},$$\n    其中 $\\Phi\\in\\mathbb{R}^{N\\times(M+1)}$ 是设计矩阵，其第 $n$ 行为 $[\\phi(\\mathbf{x}^{(n)})^\\top,\\;1]$，$\\mathbf{y}\\in\\mathbb{R}^N$ 包含目标值 $f_D(\\mathbf{x}^{(n)})$，$\\boldsymbol{\\theta} \\in \\mathbb{R}^{M+1}$ 连接了 $\\mathbf{a}$ 和 $c$，$\\lambda>0$ 是岭回归正则化强度。在所有实验中使用固定的 $\\lambda$。\n  - 使用从 $[0,1]^D$ 上的均匀分布中抽取的独立同分布（i.i.d.）样本进行训练和验证。\n\n- 精度标准：\n  - 在独立同分布的验证集上衡量均方根误差 (RMSE)：\n    $$\\mathrm{RMSE} \\;=\\; \\sqrt{\\frac{1}{N_{\\mathrm{val}}}\\sum_{n=1}^{N_{\\mathrm{val}}}\\big(\\hat{f}_D(\\mathbf{x}^{(n)}_{\\mathrm{val}})-f_D(\\mathbf{x}^{(n)}_{\\mathrm{val}})\\big)^2}.$$\n  - 如果在验证集上 $\\mathrm{RMSE} \\le \\varepsilon$，则宣布给定的训练集大小 $N$ 是足够的。\n\n- 固定的超参数和可复现性：\n  - 使用 $M=256$ 个随机隐藏单元。\n  - 使用岭参数 $\\lambda=10^{-3}$。\n  - 使用验证集大小 $N_{\\mathrm{val}}=2000$。\n  - 使用候选训练集大小 $N \\in \\{\\,32,64,128,256,512,1024,2048\\,\\}$，并选择满足精度标准的最小 $N$。\n  - 使用精度阈值 $\\varepsilon=0.20$。\n  - 使用带有固定种子的独立随机数生成器，以确保结果完全可复现。正弦函数中的角度以弧度为单位。\n\n- 测试套件和输出规范：\n  - 对于每个 $D \\in \\{\\,1,2,4,8\\,\\}$，在固定的 $M$、$\\lambda$、 $N_{\\mathrm{val}}$ 和 $\\varepsilon$（如上所述）下，从候选列表中确定达到 $\\mathrm{RMSE}\\le\\varepsilon$ 的最小 $N$。如果对于给定的 $D$，没有候选 $N$ 能达到该阈值，则为该 $D$ 返回 $-1$。\n  - 您的程序应生成单行输出，其中包含按 $D=\\{\\,1,2,4,8\\,\\}$ 顺序排列的结果，格式为方括号内由逗号分隔的整数列表，例如：\n    $$[N_1,N_2,N_3,N_4],$$\n    其中每个 $N_k$ 要么是该维度的最小足够训练集大小，要么在没有足够大小的情况下为 $-1$。\n\n科学真实性和基础依据：您必须从均方逼近误差的定义、带有ReLU激活的单隐藏层神经网络的结构，以及随机特征和岭回归正则化最小二乘法的性质出发进行推理。除指定内容外，不要使用任何数据依赖的启发式方法。所有角度都必须以弧度为单位。此问题不涉及任何物理单位。如果出现任何内部百分比，请以小数或分数表示，不要使用百分号。", "solution": "我们设计了一项经验性研究，通过量化在使用固定容量的神经网络逼近器时，为达到固定的精度目标，所需的训练集大小如何随输入维度增长，来揭示维度灾难。其基础依据是均方误差的定义、单隐藏层神经网络的结构以及岭回归正则化的最小二乘法。\n\n1. 函数和定义域。对于每个维度 $D \\in \\{\\,1,2,4,8\\,\\}$，我们考虑目标函数\n$$f_D(\\mathbf{x}) \\;=\\; \\prod_{i=1}^{D} \\sqrt{2}\\,\\sin\\!\\big(2\\pi x_i\\big), \\quad \\mathbf{x}\\in[0,1]^D.$$\n角度以弧度为单位。因子 $\\sqrt{2}$ 确保了在均匀测度下，期望平方幅值不随 $D$ 变化：\n$$\\mathbb{E}\\!\\left[f_D(\\mathbf{x})^2\\right] \\;=\\; \\prod_{i=1}^{D} \\mathbb{E}\\!\\left[2\\sin^2(2\\pi X_i)\\right] \\;=\\; \\prod_{i=1}^{D} 1 \\;=\\; 1,$$\n其中 $X_i \\sim \\mathrm{Uniform}[0,1]$ 独立同分布，并且我们使用了 $\\mathbb{E}[\\sin^2(2\\pi X_i)] = \\tfrac{1}{2}$。这种归一化消除了随 $D$ 变化的平凡尺度效应，并着重于表示和采样方面的困难。\n\n2. 带随机特征的神经网络逼近器。考虑一个带有整流线性单元（ReLU）激活的单隐藏层网络，定义为\n$$\\phi_j(\\mathbf{x}) \\;=\\; \\max\\!\\big(0,\\,\\mathbf{w}_j^\\top \\mathbf{x} + b_j\\big), \\quad j=1,\\dots,M,$$\n其中 $\\mathbf{w}_j \\in \\mathbb{R}^D$ 且 $b_j\\in\\mathbb{R}$。该模型在这些特征上是线性的：\n$$\\hat{f}_D(\\mathbf{x}) \\;=\\; \\mathbf{a}^\\top \\phi(\\mathbf{x}) + c.$$\n我们通过从标准正态分布中进行独立同分布采样来固定隐藏参数 $\\{\\mathbf{w}_j,b_j\\}_{j=1}^{M}$。这产生了一个随机特征模型，它在 $M$ 增长时保留了泛逼近性质，并允许对输出层进行快速的凸训练。\n\n3. 通过岭回归正则化的最小二乘法进行训练。给定一个训练集 $\\{(\\mathbf{x}^{(n)}, y^{(n)})\\}_{n=1}^{N}$，其中 $y^{(n)} = f_D(\\mathbf{x}^{(n)})$，通过用一列1来增广隐藏特征，定义设计矩阵 $\\Phi \\in \\mathbb{R}^{N\\times(M+1)}$：\n$$\\Phi_{n,:} \\;=\\; \\big[\\phi(\\mathbf{x}^{(n)})^\\top,\\,1\\big].$$\n令 $\\boldsymbol{\\theta} \\in \\mathbb{R}^{M+1}$ 为输出权重和偏置的拼接，$\\mathbf{y} \\in \\mathbb{R}^{N}$ 为目标向量。岭回归正则化的经验风险最小化问题\n$$\\min_{\\boldsymbol{\\theta}} \\;\\frac{1}{N}\\|\\Phi \\boldsymbol{\\theta} - \\mathbf{y}\\|_2^2 \\;+\\; \\lambda \\|\\boldsymbol{\\theta}\\|_2^2$$\n的唯一解由正规方程给出\n$$(\\Phi^\\top \\Phi + \\lambda I)\\,\\boldsymbol{\\theta} \\;=\\; \\Phi^\\top \\mathbf{y},$$\n其中 $I$ 是 $(M+1)\\times(M+1)$ 的单位矩阵，$\\lambda>0$ 是岭参数。求解这个线性系统可以得到训练好的输出层参数。\n\n4. 精度和验证。为了量化逼近质量，我们在一个大小为 $N_{\\mathrm{val}}$ 的独立验证集上使用均方根误差（RMSE）：\n$$\\mathrm{RMSE} \\;=\\; \\sqrt{\\frac{1}{N_{\\mathrm{val}}}\\sum_{n=1}^{N_{\\mathrm{val}}}\\big(\\hat{f}_D(\\mathbf{x}^{(n)}_{\\mathrm{val}}) - f_D(\\mathbf{x}^{(n)}_{\\mathrm{val}})\\big)^2}.$$\n如果 $\\mathrm{RMSE} \\le \\varepsilon$，我们则宣布对于给定的 $N$ 取得了成功。\n\n5. 实验方案。对于每个 $D \\in \\{\\,1,2,4,8\\,\\}$：\n   - 将隐藏单元数固定为 $M = 256$，岭参数固定为 $\\lambda = 10^{-3}$。\n   - 使用从 $[0,1]^D$ 上均匀独立同分布抽取的 $N_{\\mathrm{val}} = 2000$ 个验证样本。\n   - 考虑候选训练集大小 $N \\in \\{\\,32,64,128,256,512,1024,2048\\,\\}$。\n   - 按升序对每个 N：\n     - 从 $[0,1]^D$ 中独立同分布抽取 N 个训练输入，使用 $f_D$ 计算目标值。\n     - 构建特征矩阵 $\\Phi$ 并求解 $(\\Phi^\\top \\Phi + \\lambda I)\\boldsymbol{\\theta} = \\Phi^\\top \\mathbf{y}$。\n     - 计算验证集上的 RMSE。\n     - 如果 $\\mathrm{RMSE} \\le \\varepsilon$ 且 $\\varepsilon = 0.20$，则记录此 $N$ 为足够大小，并停止为该 $D$ 增加 $N$。\n   - 如果没有候选 $N$ 满足标准，则为该 $D$ 记录 $-1$。\n   - 为特征初始化和数据采样使用固定的随机种子，以确保跨次运行的可复现性。\n\n6. 关于维度灾难的原理说明。维度灾难之所以体现出来，是因为输入空间的体积随维度 $D$ 指数级增长，因此要以固定的样本密度覆盖整个定义域，就需要指数级数量的样本点。此外，像振荡因子乘积这样的函数，随着 $D$ 的增加，在 $[0,1]^D$ 上会表现出越来越复杂的变化。在模型容量 $M$ 固定的情况下，达到目标 $\\mathrm{RMSE}$ 所需的样本量 $N$ 通常随 $D$ 增加而增加，并且可能存在某些维度，在其实际范围内，没有任何 $N$ 能够达到阈值，因为逼近受限于 $M$ 的表示能力。\n\n7. 输出。程序将每个 $D$（按 $D=\\{\\,1,2,4,8\\,\\}$ 的顺序）的最小足够 $N$ 汇总到单行中，格式为方括号内由逗号分隔的整数列表：\n$$[N_1,N_2,N_3,N_4].$$\n如果对于某个特定的 $D$ 没有足够的候选 $N$，则对应的条目为 $-1$。\n\n此设计从均方误差的定义和岭回归正则化最小二乘的线性代数解出发，通过随机ReLU特征指定了一个具体的神经网络函数逼近器，并实现了一个经验性方案来衡量所需数据大小如何随维度变化，从而展示了维度灾难。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef target_function(X):\n    \"\"\"\n    Compute f_D(x) = prod_i sqrt(2) * sin(2*pi*x_i) for a batch of inputs X.\n    X: array of shape (N, D)\n    Returns: array of shape (N,)\n    \"\"\"\n    return np.prod(np.sqrt(2.0) * np.sin(2.0 * np.pi * X), axis=1)\n\ndef generate_random_features(D, M, seed):\n    \"\"\"\n    Generate random ReLU feature parameters (W, b) for dimension D and M features.\n    W: shape (M, D), entries ~ N(0,1)\n    b: shape (M,), entries ~ N(0,1)\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    W = rng.standard_normal(size=(M, D))\n    b = rng.standard_normal(size=(M,))\n    return W, b\n\ndef relu_features(X, W, b):\n    \"\"\"\n    Compute ReLU features Phi = max(0, X @ W^T + b) for batch X.\n    X: (N, D), W: (M, D), b: (M,)\n    Returns: Phi of shape (N, M)\n    \"\"\"\n    Z = X @ W.T + b  # broadcasting b over rows\n    return np.maximum(0.0, Z)\n\ndef ridge_solve(Phi, y, lam):\n    \"\"\"\n    Solve (Phi^T Phi + lam I) theta = Phi^T y for theta.\n    Phi should already include a bias column (i.e., augmented with ones).\n    \"\"\"\n    # Normal equations with Tikhonov regularization\n    A = Phi.T @ Phi\n    # Add ridge on all parameters (including bias for simplicity)\n    A.flat[::A.shape[0]+1] += lam  # add lam to diagonal\n    b = Phi.T @ y\n    theta = np.linalg.solve(A, b)\n    return theta\n\ndef rmse(y_true, y_pred):\n    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n\ndef minimal_training_size_for_dimension(D, M, lam, eps, N_candidates, N_val,\n                                        seed_features, seed_train_base, seed_val_base):\n    \"\"\"\n    For a given input dimension D, find the minimal N in N_candidates such that\n    RMSE <= eps on a validation set, using fixed random features and ridge training.\n    Returns the minimal N, or -1 if none suffices.\n    \"\"\"\n    # Fixed random features for this D\n    W, b = generate_random_features(D, M, seed_features + D)\n\n    # Fixed validation set for this D\n    rng_val = np.random.default_rng(seed_val_base + D)\n    X_val = rng_val.random(size=(N_val, D))\n    y_val = target_function(X_val)\n    Phi_val = relu_features(X_val, W, b)\n    Phi_val_aug = np.hstack([Phi_val, np.ones((N_val, 1))])\n\n    for N in N_candidates:\n        rng_train = np.random.default_rng(seed_train_base + D + N)\n        X_train = rng_train.random(size=(N, D))\n        y_train = target_function(X_train)\n        Phi_train = relu_features(X_train, W, b)\n        Phi_train_aug = np.hstack([Phi_train, np.ones((N, 1))])\n\n        theta = ridge_solve(Phi_train_aug, y_train, lam)\n        y_pred_val = Phi_val_aug @ theta\n        e = rmse(y_val, y_pred_val)\n        if e <= eps:\n            return N\n    return -1\n\ndef solve():\n    # Experimental settings as specified\n    dims = [1, 2, 4, 8]              # D values\n    M = 256                          # number of random hidden units\n    lam = 1e-3                       # ridge regularization strength\n    eps = 0.20                       # RMSE threshold\n    N_candidates = [32, 64, 128, 256, 512, 1024, 2048]\n    N_val = 2000\n\n    # Fixed seeds for reproducibility\n    seed_features = 12345\n    seed_train_base = 54321\n    seed_val_base = 99999\n\n    results = []\n    for D in dims:\n        N_min = minimal_training_size_for_dimension(\n            D=D, M=M, lam=lam, eps=eps, N_candidates=N_candidates, N_val=N_val,\n            seed_features=seed_features, seed_train_base=seed_train_base, seed_val_base=seed_val_base\n        )\n        results.append(N_min)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2417291"}, {"introduction": "最后，我们将探索一个更前沿且功能强大的应用：利用神经网络直接求解物理方程。这个练习超越了简单的数据拟合，展示了如何寻找一个满足特定微分方程的函数，这一思想是物理知识启发的神经网络（PINNs）的核心。你将动手实践，逼近统计力学中的一个基石——玻尔兹曼输运方程的非平衡稳态解，并见证函数逼近的原理如何应用于复杂的物理系统建模中 [@problem_id:2417301]。", "problem": "考虑导体中由恒定电场驱动的空间均匀的一维电子气。设单粒子分布函数为 $f(v)$，其变量为速度 $v$（单位：$\\mathrm{m/s}$）。在温度为 $T$ 的平衡态下，该分布为一维 Maxwell–Boltzmann 分布\n$$\nf_0(v) = n \\sqrt{\\frac{m}{2\\pi k_{\\mathrm{B}} T}} \\exp\\!\\left(-\\frac{m v^2}{2 k_{\\mathrm{B}} T}\\right),\n$$\n其中 $n$ 是数密度（单位：$\\mathrm{m}^{-3}$），$m$ 是电子质量（单位：$\\mathrm{kg}$），$k_{\\mathrm{B}}$ 是 Boltzmann 常数（单位：$\\mathrm{J/K}$）。在均匀电场 $E$（单位：$\\mathrm{V/m}$）作用的稳态下，并采用弛豫时间为 $\\tau$（单位：$\\mathrm{s}$）的弛豫时间近似（RTA），空间均匀系统的一维 Boltzmann 输运方程可写作\n$$\n\\frac{qE}{m}\\,\\frac{\\partial f}{\\partial v} = -\\frac{f - f_0}{\\tau},\n$$\n其中 $q$ 是带符号的电子电荷（$q=-e$，$e>0$）。在线性响应（小电场）范围内，标准的近似方法是在平衡分布下计算速度导数，即用 $\\partial f_0/\\partial v$ 替换左侧的 $\\partial f/\\partial v$。这样可以得到一个非齐次线性关系式，由此可确定非平衡稳态分布函数 $f(v)$。\n\n您的任务是：\n- 从弛豫时间近似和线性响应假设下的稳态 Boltzmann 输运方程出发，推导非平衡稳态分布函数 $f(v)$ 的闭式表达式，该表达式应使用 $f_0(v)$、其速度导数 $\\partial f_0/\\partial v$ 以及参数 $q$、$E$、$m$、$\\tau$ 来表示。\n- 在有限速度区间 $v\\in[-v_{\\max},v_{\\max}]$ 上，实现一个用于 $f(v)$ 的神经网络函数逼近器 $\\hat f(v)$，其中 $v_{\\max} = 5 \\sqrt{k_{\\mathrm{B}} T/m}$（单位：$\\mathrm{m/s}$）。使用一个单隐藏层网络，该网络包含 $N_h$ 个固定的随机特征和双曲正切激活函数（这些固定特征构成隐藏层），并通过在 $[-v_{\\max},v_{\\max}]$ 的一组配置点上最小化线性化稳态 Boltzmann 方程的均方残差来仅训练线性输出层，同时辅以微小的 $\\ell_2$ 岭正则化。具体来说，使用热速度 $v_{\\mathrm{th}} = \\sqrt{k_{\\mathrm{B}} T/m}$ 通过 $x = v / v_{\\mathrm{th}}$ 对输入进行标准化，定义特征 $\\phi_j(v) = \\tanh(w_j x + b_j)$（其中 $j=1,\\dots,N_h$，$w_j$ 和 $b_j$ 为固定的随机值），并拟合系数，使得 $\\hat f$ 在配置点上近似满足线性化方程。\n- 通过计算相对 $L^2$ 误差来评估逼近的质量\n$$\n\\varepsilon = \\frac{\\left(\\int_{-v_{\\max}}^{v_{\\max}} \\left[\\hat f(v) - f(v)\\right]^2 \\, dv\\right)^{1/2}}{\\left(\\int_{-v_{\\max}}^{v_{\\max}} f(v)^2 \\, dv\\right)^{1/2}},\n$$\n该误差在 $[-v_{\\max},v_{\\max}]$ 的一个精细均匀网格上使用复合梯形法则进行数值逼近。此误差为无量纲，且必须以浮点数形式报告。\n\n在您的实现中应使用的基本常数：\n- 电子电荷大小 $e = 1.602176634\\times 10^{-19}$（单位：$\\mathrm{C}$），带符号的电子电荷 $q = -e$。\n- 电子质量 $m = 9.1093837015\\times 10^{-31}$（单位：$\\mathrm{kg}$）。\n- Boltzmann 常数 $k_{\\mathrm{B}} = 1.380649\\times 10^{-23}$（单位：$\\mathrm{J/K}$）。\n\n使用数密度 $n = 1.0\\times 10^{21}$（单位：$\\mathrm{m}^{-3}$）。\n\n不使用角度单位。所有需要单位的物理量均以国际单位制（SI）指定。相对 $L^2$ 误差 $\\varepsilon$ 是无量纲的。\n\n测试套件。您的程序必须运行神经网络逼近，并报告以下参数集的相对 $L^2$ 误差，每个参数集表示为一个三元组 $(E, \\tau, T)$：\n- 情况 1（基准平衡）：$(E, \\tau, T) = (0.0,\\ 1.0\\times 10^{-14},\\ 300.0)$。\n- 情况 2（中等驱动）：$(E, \\tau, T) = (1.0\\times 10^{4},\\ 1.0\\times 10^{-14},\\ 300.0)$。\n- 情况 3（短弛豫）：$(E, \\tau, T) = (1.0\\times 10^{4},\\ 5.0\\times 10^{-16},\\ 300.0)$。\n- 情况 4（反向电场，更高温度）：$(E, \\tau, T) = (-5.0\\times 10^{3},\\ 2.0\\times 10^{-14},\\ 600.0)$。\n\n神经网络设计要求：\n- 使用 $N_h = 128$ 个具有 $\\tanh$ 激活函数的固定随机特征和一个单一的线性输出神经元。在应用随机仿射映射和非线性激活函数之前，通过 $v_{\\mathrm{th}}$ 将输入标准化为 $x=v/v_{\\mathrm{th}}$。\n- 仅训练线性输出层，方法是在 $[-v_{\\max},v_{\\max}]$ 的均匀配置点网格上最小化线性化稳态 Boltzmann 方程的均方残差，并使用一个小的岭惩罚项 $\\lambda = 10^{-8}$。\n- 使用至少 $N_c=801$ 个点的均匀配置，并在一个至少有 $N_e=2001$ 个点的更精细的均匀网格上评估误差。\n\n最终输出规范：\n- 您的程序应生成单行输出，其中包含案例 1-4 的四个相对 $L^2$ 误差，四舍五入到六位小数，以逗号分隔的列表形式并用方括号括起。例如，一个允许的输出格式如 $[\\varepsilon_1,\\varepsilon_2,\\varepsilon_3,\\varepsilon_4]$，其中每个 $\\varepsilon_i$ 是一个小数点后有六位数字的浮点数。", "solution": "问题陈述已经过严格验证，并被判定为**有效**。它在科学上基于统计力学和输运理论的原理，特别是 Boltzmann 输运方程。该问题定义明确、客观，并提供了获得唯一、可验证解所需的所有信息。\n\n任务首先是推导在一维情况下，恒定电场作用下的电子分布函数 $f(v)$ 的解析形式，然后训练一个基于神经网络的函数逼近器 $\\hat f(v)$ 来学习此函数。训练目标是最小化控制物理方程的残差。最后，通过与解析解比较的相对 $L^2$ 误差来量化逼近器的准确性。\n\n**1. 分布函数的解析解**\n\n出发点是在弛豫时间近似（RTA）下，稳态、空间均匀的一维 Boltzmann 输运方程：\n$$\n\\frac{qE}{m}\\,\\frac{\\partial f}{\\partial v} = -\\frac{f - f_0}{\\tau}\n$$\n此处，$f(v)$ 是非平衡分布函数，$f_0(v)$ 是平衡 Maxwell-Boltzmann 分布，$q$ 是电子电荷，$E$ 是电场，$m$ 是电子质量，$\\tau$ 是弛豫时间。\n\n问题指定使用线性响应近似，该近似假设电场 $E$ 很小。在此假设下，$f$ 与 $f_0$ 的偏差很小，方程左侧的导数 $\\partial f/\\partial v$ 可以用平衡分布的导数 $\\partial f_0/\\partial v$ 来近似。方程变为：\n$$\n\\frac{qE}{m}\\,\\frac{\\partial f_0}{\\partial v} \\approx -\\frac{f - f_0}{\\tau}\n$$\n这现在是一个关于 $f(v)$ 的代数方程，可以直接求解：\n$$\nf - f_0 = -\\tau \\left( \\frac{qE}{m} \\right) \\frac{\\partial f_0}{\\partial v}\n$$\n$$\nf(v) = f_0(v) - \\frac{qE\\tau}{m} \\frac{\\partial f_0}{\\partial v}\n$$\n这是线性响应范围内非平衡分布函数的闭式表达式。为了使其完全明确，我们需要一维 Maxwell-Boltzmann 分布 $f_0(v)$ 的导数：\n$$\nf_0(v) = n \\sqrt{\\frac{m}{2\\pi k_{\\mathrm{B}} T}} \\exp\\!\\left(-\\frac{m v^2}{2 k_{\\mathrm{B}} T}\\right)\n$$\n关于速度 $v$ 的导数是：\n$$\n\\frac{\\partial f_0}{\\partial v} = \\left( n \\sqrt{\\frac{m}{2\\pi k_{\\mathrm{B}} T}} \\exp\\!\\left(-\\frac{m v^2}{2 k_{\\mathrm{B}} T}\\right) \\right) \\cdot \\frac{d}{dv} \\left(-\\frac{m v^2}{2 k_{\\mathrm{B}} T}\\right)\n$$\n$$\n\\frac{\\partial f_0}{\\partial v} = f_0(v) \\cdot \\left(-\\frac{2mv}{2 k_{\\mathrm{B}} T}\\right) = -f_0(v) \\frac{mv}{k_{\\mathrm{B}} T}\n$$\n将此导数代回到 $f(v)$ 的表达式中：\n$$\nf(v) = f_0(v) - \\frac{qE\\tau}{m} \\left( -f_0(v) \\frac{mv}{k_{\\mathrm{B}} T} \\right)\n$$\n通过消去质量 $m$ 进行简化：\n$$\nf(v) = f_0(v) + f_0(v) \\frac{qE\\tau v}{k_{\\mathrm{B}} T}\n$$\n$$\nf(v) = f_0(v) \\left( 1 + \\frac{qE\\tau v}{k_{\\mathrm{B}} T} \\right)\n$$\n这个最终表达式是用于评估神经网络逼近的解析基准真相（analytical ground truth）。\n\n**2. 神经网络逼近与训练**\n\n问题要求用一个由单隐藏层神经网络（具有固定的随机特征）实现的函数 $\\hat f(v)$ 来逼近 $f(v)$。该逼近器的结构为：\n$$\n\\hat f(v) = c_0 + \\sum_{j=1}^{N_h} c_j \\phi_j(v)\n$$\n其中 $\\{c_j\\}_{j=0}^{N_h}$ 是线性输出层的可训练系数，$\\phi_j(v)$ 是 $N_h = 128$ 个固定的基函数。这些基函数定义为：\n$$\n\\phi_j(v) = \\tanh(w_j x + b_j), \\quad \\text{where} \\quad x = \\frac{v}{v_{\\mathrm{th}}} \\quad \\text{and} \\quad v_{\\mathrm{th}} = \\sqrt{\\frac{k_{\\mathrm{B}} T}{m}}\n$$\n权重 $w_j$ 和偏置 $b_j$ 从一个随机分布中抽取，并在训练期间保持恒定。\n\n训练目标是最小化线性化 Boltzmann 方程的均方残差，并附加一个 $\\ell_2$ 正则化项。逼近器 $\\hat f(v)$ 的残差为：\n$$\n\\mathcal{R}(v) = \\frac{qE}{m}\\,\\frac{\\partial f_0}{\\partial v} + \\frac{\\hat f(v) - f_0(v)}{\\tau}\n$$\n很明显，将残差设为零，即 $\\mathcal{R}(v)=0$，会恢复出 $\\hat f(v)$ 的解析解。因此，最小化平方残差等价于寻找 $\\hat f(v)$ 对解析解 $f(v)$ 的最佳拟合。要在 $N_c$ 个配置点 $\\{v_i\\}$ 的集合上最小化的损失函数是：\n$$\n\\mathcal{L} = \\frac{1}{N_c} \\sum_{i=1}^{N_c} \\left[ \\frac{\\hat f(v_i) - f_0(v_i)}{\\tau} + \\frac{qE}{m}\\frac{\\partial f_0}{\\partial v_i} \\right]^2 + \\lambda \\sum_{j=0}^{N_h} c_j^2\n$$\n这是一个标准的岭回归问题。令 $\\mathbf{C}$ 为系数向量 $[c_0, c_1, \\dots, c_{N_h}]^T$。令 $\\mathbf{A}$ 为大小 $N_c \\times (N_h+1)$ 的设计矩阵，其中 $A_{i,0} = 1$ 且当 $j > 0$ 时 $A_{i,j} = \\phi_j(v_i)$。令 $\\mathbf{y}$ 为目标向量，其元素为 $y_i = f_0(v_i) - \\frac{qE\\tau}{m}\\frac{\\partial f_0}{\\partial v_i} = f(v_i)$。目标是找到最小化 $||\\mathbf{AC} - \\mathbf{y}||_2^2 + \\lambda' ||\\mathbf{C}||_2^2$ 的 $\\mathbf{C}$（其中 $\\lambda'$ 包含了像 $\\tau$ 和 $N_c$ 这样的缩放因子）。解由岭回归的正规方程给出：\n$$\n(\\mathbf{A}^T\\mathbf{A} + \\lambda \\mathbf{I})\\mathbf{C} = \\mathbf{A}^T\\mathbf{y}\n$$\n求解此线性系统可得到系数向量 $\\mathbf{C}$，从而完全定义训练好的逼近器 $\\hat f(v)$。\n\n**3. 误差评估**\n\n逼近的质量通过在区间 $[-v_{\\max}, v_{\\max}]$ 上的相对 $L^2$ 误差 $\\varepsilon$ 来衡量，其中 $v_{\\max} = 5 v_{\\mathrm{th}}$。该误差定义为：\n$$\n\\varepsilon = \\frac{\\left(\\int_{-v_{\\max}}^{v_{\\max}} \\left[\\hat f(v) - f(v)\\right]^2 \\, dv\\right)^{1/2}}{\\left(\\int_{-v_{\\max}}^{v_{\\max}} f(v)^2 \\, dv\\right)^{1/2}}\n$$\n在数值上，这些积分是在一个横跨该速度区间的、包含 $N_e = 2001$ 个点的精细均匀网格上，使用复合梯形法则计算。对每个测试案例重复此过程，以获得所需的误差值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Boltzmann transport equation for an electron gas using a neural\n    network approximator and evaluates the approximation error.\n    \"\"\"\n    # Define fundamental constants\n    e = 1.602176634e-19  # Electron charge magnitude in C\n    q = -e               # Signed electron charge in C\n    m = 9.1093837015e-31 # Electron mass in kg\n    k_B = 1.380649e-23   # Boltzmann constant in J/K\n\n    # Define problem parameters\n    n = 1.0e21           # Number density in m^-3\n\n    # Neural network design requirements\n    N_h = 128            # Number of hidden neurons (fixed random features)\n    lambda_reg = 1e-8    # L2 ridge regularization parameter\n    N_c = 801            # Number of collocation points for training\n    N_e = 2001           # Number of evaluation points for error calculation\n\n    # For reproducibility of the fixed random features, a seed is used.\n    rng = np.random.default_rng(42)\n    W = rng.normal(size=N_h)  # Weights of the random features\n    b = rng.normal(size=N_h)  # Biases of the random features\n\n    # Test suite: parameter sets (E in V/m, tau in s, T in K)\n    test_cases = [\n        # Case 1 (baseline equilibrium)\n        (0.0, 1.0e-14, 300.0),\n        # Case 2 (moderate drive)\n        (1.0e4, 1.0e-14, 300.0),\n        # Case 3 (short relaxation)\n        (1.0e4, 5.0e-16, 300.0),\n        # Case 4 (opposite field, hotter)\n        (-5.0e3, 2.0e-14, 600.0),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        E, tau, T = case\n\n        # Calculate derived physical quantities for the current case\n        v_th = np.sqrt(k_B * T / m)\n        v_max = 5 * v_th\n\n        # Define the velocity grids for collocation (training) and evaluation (testing)\n        v_collocation = np.linspace(-v_max, v_max, N_c)\n        v_evaluation = np.linspace(-v_max, v_max, N_e)\n\n        # Pre-calculate constants for f0(v) to improve efficiency\n        _f0_const = n * np.sqrt(m / (2 * np.pi * k_B * T))\n        _f0_exp_factor = -m / (2 * k_B * T)\n\n        def f0(v):\n            \"\"\"Computes the equilibrium Maxwell-Boltzmann distribution f_0(v).\"\"\"\n            return _f0_const * np.exp(_f0_exp_factor * v**2)\n\n        def f_analytic(v):\n            \"\"\"Computes the analytical non-equilibrium distribution f(v) in the linear response regime.\"\"\"\n            correction_factor = 1 + (q * E * tau * v) / (k_B * T)\n            return f0(v) * correction_factor\n\n        # --- Training Phase: Solve for the linear output layer coefficients ---\n        \n        # 1. Construct the design matrix A (also known as the feature matrix)\n        # The input velocity is first standardized by the thermal velocity.\n        x_c = v_collocation / v_th\n        \n        # A has shape (N_c, N_h + 1). The first column of ones corresponds to the bias term c_0.\n        A = np.ones((N_c, N_h + 1))\n        \n        # Vectorized computation of features for all points and neurons\n        # Broadcasting W (N_h,) and x_c (N_c,) gives a feature block of shape (N_c, N_h).\n        A[:, 1:] = np.tanh(W[None, :] * x_c[:, None] + b[None, :])\n        \n        # 2. Construct the target vector y from the analytical solution\n        y = f_analytic(v_collocation)\n        \n        # 3. Solve the ridge regression system (A.T @ A + lambda * I) @ C = A.T @ y\n        # to find the coefficient vector C = [c_0, c_1, ..., c_N_h].\n        LHS = A.T @ A + lambda_reg * np.identity(N_h + 1)\n        RHS = A.T @ y\n        C = np.linalg.solve(LHS, RHS)\n\n        # --- Evaluation Phase: Compute the relative L2 error ---\n        \n        def f_nn(v):\n            \"\"\"Evaluates the trained neural network approximator.\"\"\"\n            x = v / v_th\n            # features has shape (len(v), N_h)\n            features = np.tanh(W[None, :] * x[:, None] + b[None, :])\n            # C[1:] has shape (N_h,); result of @ is a vector of length len(v)\n            return C[0] + features @ C[1:]\n        \n        # Get predictions and true values on the fine evaluation grid\n        f_hat_predictions = f_nn(v_evaluation)\n        f_analytic_values = f_analytic(v_evaluation)\n        \n        # Compute the numerator and denominator of the relative L2 error\n        # using the composite trapezoidal rule (implemented by np.trapz).\n        integrand_numerator = (f_hat_predictions - f_analytic_values)**2\n        integrand_denominator = f_analytic_values**2\n        \n        integral_numerator = np.trapz(integrand_numerator, x=v_evaluation)\n        integral_denominator = np.trapz(integrand_denominator, x=v_evaluation)\n        \n        # Calculate the final error.\n        if integral_denominator == 0:\n            error = 0.0 if integral_numerator == 0 else np.inf\n        else:\n            error = np.sqrt(integral_numerator / integral_denominator)\n            \n        results.append(error)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "2417301"}]}