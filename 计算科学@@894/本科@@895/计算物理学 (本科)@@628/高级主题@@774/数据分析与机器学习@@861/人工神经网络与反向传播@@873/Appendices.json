{"hands_on_practices": [{"introduction": "在复杂的损失函数地形中，优化算法常常会陷入局部最小值，这阻碍了模型找到全局最优解。本练习将物理学中的概念引入这个问题，通过在梯度更新中加入热噪声来模拟郎之万动力学（Langevin dynamics）。您将亲手实现并观察噪声如何帮助参数“跳出”能量势阱，这与统计物理学中的模拟退火过程有着深刻的类比，从而加深对优化动力学和随机过程的理解。[@problem_id:2373958]", "problem": "您将实现并分析一个计算实验，该实验将通过反向传播进行的人工神经网络训练与热噪声辅助的局部最小值逃逸联系起来，并与模拟退火进行类比。考虑一个标量人工神经网络，它只有一个参数 $w \\in \\mathbb{R}$，对于单个固定输入，其输出为 $y(w) = w$。定义一个非凸训练目标函数（解释为能量景观）如下：\n$$\nU(w) = (w^2 - 1)^2 + \\alpha\\, w + \\beta \\cos(k\\, w),\n$$\n其中 $\\alpha$、$\\beta$ 和 $k$ 是固定的已知常数。假设该标量网络通过反向传播最小化 $U(w)$ 进行训练，这意味着训练动态由相对于 $w$ 的梯度 $\\frac{dU}{dw}$ 驱动。您需要将热噪声引入梯度信号中以模拟热涨落，这与过阻尼朗之万动力学和模拟退火直接类似。\n\n您必须使用的基本原理：\n- 标量参数基于梯度的训练的定义：标量参数的反向传播梯度为 $\\frac{dU}{dw}$。\n- 在温度为 $T$、能量景观为 $U(w)$ 的环境中，坐标 $w$ 的过阻尼朗之万动力学具有连续时间形式 $dw/dt = -\\mu \\frac{dU}{dw} + \\sqrt{2 D}\\,\\xi(t)$，其中迁移率 $\\mu$ 和扩散常数 $D$ 通过 Einstein 关系式 $D = \\mu k_{\\mathrm{B}} T$ 关联，$k_{\\mathrm{B}}$ 是 Boltzmann 常数，$\\xi(t)$ 是标准白噪声。在离散时间中，步长为 $\\Delta t$，并且将学习率等同于 $\\eta = \\mu \\Delta t$，这会得到一个时间离散化的更新，该更新带有一个经过热缩放的高斯噪声项。\n\n您的任务：\n1. 从上述基本原理出发，推导从 $w_t$ 到 $w_{t+1}$ 的离散时间更新规则，用于模拟类似梯度下降的反向传播，并带有一个在（可能随时间变化的）温度 $T(t)$ 下的附加热噪声项。使用恒定的学习率 $\\eta > 0$。明确论证高斯噪声项的精确方差（用 $\\eta$ 和 $T(t)$ 表示）。\n2. 使用您推导的更新规则，实现一个模拟。该模拟将参数从 $w_0 = 0.95$ 开始，并执行固定步数 $N_{\\mathrm{steps}}$ 的更新。定义一个成功的逃逸事件为最终进入 $w \\approx -1$ 附近的更深的全局势阱，您必须将其操作化为条件 $w_{N_{\\mathrm{steps}}} \\le -0.9$。\n3. 对每个实验，使用不同的随机种子运行 $N_{\\mathrm{trials}}$ 次独立试验，并报告成功逃逸的比例，形式为 $[0,1]$ 范围内的浮点数。每次试验必须使用相同的初始条件 $w_0 = 0.95$ 和相同的方案 $T(t)$，但高斯热噪声的实现是独立的。使用以下固定常数：$\\alpha = 0.3$, $\\beta = 0.1$, $k = 5.0$, $\\eta = 0.01$, $N_{\\mathrm{steps}} = 4000$, $N_{\\mathrm{trials}} = 64$。\n4. 考虑以下模拟不同训练机制的温度方案 $T(t)$。对每种方案，按上述要求计算并报告逃逸比例。\n   - 情况 A（无热噪声）：对于所有 $t$，$T(t) = 0$。\n   - 情况 B（恒定热浴）：对于所有 $t$，$T(t) = 0.02$。\n   - 情况 C（指数模拟退火，慢速）：$T(t) = T_0 \\exp(-t/\\tau)$，其中 $T_0 = 0.05$ 且 $\\tau = 1500$。\n   - 情况 D（指数模拟退火，高温且快速）：$T(t) = T_0 \\exp(-t/\\tau)$，其中 $T_0 = 0.15$ 且 $\\tau = 300$。\n5. 您的程序必须按 A-D 的顺序计算这四种情况的逃逸比例，并将它们作为用方括号括起来的逗号分隔列表，在单行上输出。每个值必须四舍五入到恰好三位小数。例如，四个情况的输出应如下所示：[0.000,0.312,0.516,0.203]。\n\n附加说明：\n- 三角函数中的角度是无量纲的弧度。\n- 输出中不需要物理单位；报告纯数字。\n- 随机数生成必须是可复现的：选择一个主种子，然后为每次独立试验派生出不同的种子。\n- 根据给定的 $U(w)$ 精确实现梯度 $\\frac{dU}{dw}$。不要使用数值方法近似求导。\n- 您的代码必须是自包含的，并且不得读取任何输入。\n\n测试套件规范：\n- 使用上面列出的确切常数和方案。这些构成了您的程序要运行的测试用例。\n- 输出格式要求：您的程序应生成单行输出，其中包含按 [情况A,情况B,情况C,情况D] 顺序排列的结果，格式为用方括号括起来、逗号分隔且四舍五入到三位小数的列表。\n- 四个数字中的每一个都必须是浮点类型，并且必须在 $[0,1]$ 范围内。", "solution": "问题要求推导一个参数 $w$ 的离散时间更新规则，该参数的训练过程类似于过阻尼朗之万动力学，然后使用此规则来模拟从能量景观 $U(w)$ 的局部最小值中逃逸的过程。\n\n**第 1 部分：离散时间更新规则的推导**\n\n我们从描述在温度 $T$ 和势能 $U(w)$ 中参数 $w$ 的连续时间过阻尼朗之万方程开始：\n$$\n\\frac{dw}{dt} = -\\mu \\frac{dU}{dw} + \\sqrt{2 D}\\,\\xi(t)\n$$\n此处，$\\mu$ 是迁移率，$D$ 是扩散常数，$\\xi(t)$ 是标准高斯白噪声，满足 $\\langle \\xi(t) \\rangle = 0$ 和 $\\langle \\xi(t)\\xi(t') \\rangle = \\delta(t-t')$。迁移率和扩散通过 Einstein 关系式关联，我们将其取为 $D = \\mu T$，这是通过将 Boltzmann 常数 $k_B$ 吸收到温度 $T$ 的定义中（将 $T$ 视为具有能量单位）。\n\n为了获得离散时间更新规则，我们采用 Euler-Maruyama 方法，对此随机微分方程在一个微小时间步长 $\\Delta t$ 上进行积分。$w$ 从时间 $t$ 到 $t+\\Delta t$ 的变化近似为：\n$$\nw_{t+1} - w_t = \\Delta w \\approx \\int_{t}^{t+\\Delta t} \\left( -\\mu \\frac{dU}{dw}\\bigg|_{w_s} + \\sqrt{2 D}\\,\\xi(s) \\right) ds\n$$\n假设 $\\Delta t$ 很小，梯度项在该区间内可被视为常数，在 $w_t = w(t)$ 处求值。噪声项的积分是一个维纳过程增量，$\\Delta W_t = \\int_{t}^{t+\\Delta t} \\xi(s) ds$。该增量是一个均值为 $0$、方差为 $\\Delta t$ 的高斯随机变量。我们可以写成 $\\Delta W_t = \\sqrt{\\Delta t} Z_t$，其中 $Z_t$ 是从标准正态分布 $\\mathcal{N}(0, 1)$ 中抽取的随机变量。\n\n因此，离散化的更新方程变为：\n$$\nw_{t+1} = w_t - \\mu \\Delta t \\frac{dU}{dw}\\bigg|_{w_t} + \\sqrt{2 D} \\sqrt{\\Delta t} Z_t\n$$\n问题陈述通过 $\\eta = \\mu \\Delta t$ 提供了学习率 $\\eta$ 与物理参数之间的类比。将这个关系以及 $D = \\mu T$ 代入方程，得到：\n$$\nw_{t+1} = w_t - \\eta \\frac{dU}{dw}\\bigg|_{w_t} + \\sqrt{2 (\\mu T)} \\sqrt{\\Delta t} Z_t\n$$\n为了纯粹用 $\\eta$ 和 $T$ 来表示噪声项，我们重排 $\\eta = \\mu \\Delta t$ 以求得 $\\mu = \\eta / \\Delta t$。将其代入平方根中：\n$$\n\\sqrt{2 (\\mu T) \\Delta t} = \\sqrt{2 \\left(\\frac{\\eta}{\\Delta t} T\\right) \\Delta t} = \\sqrt{2 \\eta T}\n$$\n因此，$w_t$ 的最终离散时间更新规则是：\n$$\nw_{t+1} = w_t - \\eta \\frac{dU}{dw}\\bigg|_{w_t} + \\sqrt{2 \\eta T(t)} Z_t\n$$\n其中 $Z_t \\sim \\mathcal{N}(0, 1)$ 且温度 $T(t)$ 可以随时间变化。第一项 $-\\eta \\frac{dU}{dw}$ 对应于机器学习中的标准梯度下降更新。第二项 $\\sqrt{2 \\eta T(t)} Z_t$ 是附加的热噪声。此项是一个均值为 $0$、方差为 $(\\sqrt{2\\eta T(t)})^2 = 2\\eta T(t)$ 的高斯随机变量。\n\n**第 2 部分：梯度计算**\n\n能量景观（或训练目标函数）由以下公式给出：\n$$\nU(w) = (w^2 - 1)^2 + \\alpha w + \\beta \\cos(k w)\n$$\n为了实现更新规则，我们必须计算 $U(w)$ 相对于 $w$ 的解析梯度：\n$$\n\\frac{dU}{dw} = \\frac{d}{dw} \\left( w^4 - 2w^2 + 1 + \\alpha w + \\beta \\cos(k w) \\right)\n$$\n应用微分法则，我们得到：\n$$\n\\frac{dU}{dw} = 4w^3 - 4w + \\alpha - \\beta k \\sin(k w)\n$$\n此表达式将与提供的常数 $\\alpha=0.3$、$\\beta=0.1$ 和 $k=5.0$ 一起直接在模拟中使用。\n\n**第 3 部分：模拟流程**\n\n模拟旨在计算参数 $w$ 从 $w \\approx 1$ 附近的局部最小值开始，最终逃逸到 $w \\approx -1$ 附近的全局最小值盆地的比例。\n\n对于四种指定的温度方案中的每一种，我们执行以下流程：\n1.  将成功逃逸计数器初始化为 $0$。\n2.  运行 $N_{\\mathrm{trials}} = 64$ 次独立试验。对于每次试验：\n    a. 将参数 $w$ 初始化为其起始值 $w_0 = 0.95$。\n    b. 为热噪声生成一个独立的随机数序列。这是通过为每次试验使用从主生成器派生的唯一种子来设定的随机数生成器来确保的，以确保试验间的独立性以及所有试验的可复现性。\n    c. 执行 $N_{\\mathrm{steps}} = 4000$ 次更新步骤。对于从 $0$ 到 $N_{\\mathrm{steps}}-1$ 的每一步 $t$：\n        i. 根据给定的方案计算温度 $T_t = T(t)$。\n        ii. 使用上述推导的公式计算梯度 $g_t = \\frac{dU}{dw}\\big|_{w_t}$。\n        iii. 计算噪声项的标准差 $\\sigma_t = \\sqrt{2 \\eta T_t}$。如果 $T_t = 0$，则 $\\sigma_t=0$。\n        iv. 从标准正态分布 $\\mathcal{N}(0, 1)$ 中抽取一个随机样本 $Z_t$。\n        v. 更新参数：$w_{t+1} = w_t - \\eta g_t + \\sigma_t Z_t$。\n    d. 在 $N_{\\mathrm{steps}}$ 步之后，检查参数的最终状态 $w_{N_{\\mathrm{steps}}}$。如果 $w_{N_{\\mathrm{steps}}} \\le -0.9$，该试验被计为一次成功的逃逸。\n3.  该温度方案的最终逃逸比例是成功逃逸的总次数除以 $N_{\\mathrm{trials}}$。\n\n对所有四种情况重复此流程：\n- 情况 A（无噪声）：$T(t) = 0$。\n- 情况 B（恒定温度）：$T(t) = 0.02$。\n- 情况 C（慢速退火）：$T(t) = 0.05 \\exp(-t/1500)$。\n- 情况 D（快速退火）：$T(t) = 0.15 \\exp(-t/300)$。\n\n实现将直接遵循此逻辑，并使用提供的常数：$\\alpha = 0.3$、$\\beta = 0.1$、$k = 5.0$ 和 $\\eta = 0.01$。最终结果四舍五入到三位小数，并以指定格式呈现。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Global constants from the problem statement\nALPHA = 0.3\nBETA = 0.1\nK = 5.0\nETA = 0.01\nW0 = 0.95\nN_STEPS = 4000\nN_TRIALS = 64\nSUCCESS_THRESHOLD = -0.9\nMASTER_SEED = 42 # For reproducible results\n\ndef gradient_U(w):\n    \"\"\"\n    Computes the analytical gradient of the potential U(w).\n    dU/dw = 4*w^3 - 4*w + alpha - beta*k*sin(k*w)\n    \"\"\"\n    return 4 * w**3 - 4 * w + ALPHA - BETA * K * np.sin(K * w)\n\ndef calculate_escape_fraction(T_func, master_rng):\n    \"\"\"\n    Runs N_trials simulations for a given temperature schedule T_func.\n    Returns the fraction of successful escapes.\n    \"\"\"\n    success_count = 0\n    for i in range(N_TRIALS):\n        # Derive a distinct seed for each trial from the master RNG for independence\n        trial_seed = master_rng.integers(2**32)\n        trial_rng = np.random.default_rng(seed=trial_seed)\n\n        w = W0\n        for t in range(N_STEPS):\n            # Calculate gradient\n            grad = gradient_U(w)\n\n            # Calculate temperature and noise term\n            temp = T_func(t)\n            noise = 0.0\n            if temp > 0:\n                # Variance of noise is 2*eta*T, so std_dev is sqrt(2*eta*T)\n                std_dev = np.sqrt(2 * ETA * temp)\n                noise = trial_rng.normal(0.0, std_dev)\n\n            # Update w using the derived discrete-time Langevin equation\n            # w_{t+1} = w_t - eta * dU/dw + noise\n            w = w - ETA * grad + noise\n        \n        # Check for successful escape\n        if w = SUCCESS_THRESHOLD:\n            success_count += 1\n\n    return success_count / N_TRIALS\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run simulations, and print results.\n    \"\"\"\n    # Master RNG for generating trial seeds, ensuring independence between cases A, B, C, D\n    master_rng = np.random.default_rng(seed=MASTER_SEED)\n\n    # Define the four temperature schedules (test cases)\n    # Case A: No thermal noise\n    T_A = lambda t: 0.0\n    # Case B: Constant thermal bath\n    T_B = lambda t: 0.02\n    # Case C: Exponential simulated annealing, slow\n    T0_C, TAU_C = 0.05, 1500.0\n    T_C = lambda t: T0_C * np.exp(-t / TAU_C)\n    # Case D: Exponential simulated annealing, hot and fast\n    T0_D, TAU_D = 0.15, 300.0\n    T_D = lambda t: T0_D * np.exp(-t / TAU_D)\n\n    schedules = [T_A, T_B, T_C, T_D]\n\n    results = []\n    for schedule in schedules:\n        fraction = calculate_escape_fraction(schedule, master_rng)\n        results.append(fraction)\n\n    # Format the final output string exactly as required\n    formatted_results = [f\"{res:.3f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the solver\nsolve()\n\n```", "id": "2373958"}, {"introduction": "我们将关注点从优化的物理过程转向计算的物理实现。为了提高效率，现代计算硬件经常使用低精度算术，但这可能影响学习算法的稳定性。本练习通过系统性地降低梯度的数值精度，来探究训练过程的临界点，即学习在何处“失效”。我们将这种学习的突然崩溃类比为物理学中的“相变”（phase transition），为您提供一个从计算物理角度审视神经网络训练极限的独特视角。[@problem_id:2373947]", "problem": "构建一个定量实验，通过训练一个具有量化梯度的单一线性神经元，来研究有限精度反向传播下的学习。考虑一个数据集，其输入矩阵为 $X \\in \\mathbb{R}^{N \\times d}$，输出向量为 $y \\in \\mathbb{R}^{N}$，其中 $N=d$ 且 $X$ 是单位矩阵 $I_{d}$。设目标参数向量为 $w^{\\star} \\in \\mathbb{R}^{d}$，其各项为 $w^{\\star} = [1,-2,3,-4,5]^{\\top}$，初始参数向量为 $w_{0}=\\mathbf{0} \\in \\mathbb{R}^{d}$。定义经验均方误差损失\n$$\nL(w) = \\frac{1}{N}\\,\\lVert Xw - y \\rVert_{2}^{2}.\n$$\n使用固定的学习率 $\\eta$ 执行全批量梯度下降，其中梯度 $Q_{b}(\\nabla L(w))$ 被量化，逐分量地应用 $b$ 个带符号的精度位数（包括符号位）。设 $N=5$, $d=5$, $X = I_{5}$, $y = w^{\\star}$, $w_{0}=\\mathbf{0}$ 以及 $\\eta=1$。以 $T=2000$ 的迭代次数进行精确训练。\n\n量化器定义如下。计算初始全精度梯度 $g_{0} = \\nabla L(w_{0})$，并设置固定的动态范围 $s_{\\mathrm{fixed}} = \\max_{j} |(g_{0})_{j}|$。对于 $b \\ge 2$，定义均匀量化步长\n$$\n\\Delta_{b} = \\frac{s_{\\mathrm{fixed}}}{2^{\\,b-1}-1},\n$$\n并应用逐分量的向零舍入量化\n$$\n\\left[Q_{b}(g)\\right]_{j} = \\operatorname{sign}(g_{j}) \\cdot \\Delta_{b}\\,\\left\\lfloor \\frac{|g_{j}|}{\\Delta_{b}} \\right\\rfloor,\n$$\n约定 $\\operatorname{sign}(0)=0$。对于 $b=1$ 的边界情况，使用一位符号量化\n$$\n\\left[Q_{1}(g)\\right]_{j} = s_{\\mathrm{fixed}} \\cdot \\operatorname{sign}(g_{j}),\n$$\n约定 $\\operatorname{sign}(0)=0$。训练更新规则为\n$$\nw_{t+1} = w_{t} - \\eta \\, Q_{b}\\big(\\nabla L(w_{t})\\big),\n$$\n对 $t=0,1,\\dots,T-1$ 应用。\n\n定义学习崩溃标准为在第 $T$ 次迭代时未能达到目标损失阈值 $\\tau$，即如果 $L(w_{T})  \\tau$，则发生崩溃。使用固定的阈值 $\\tau = 0.002$。\n\n测试套件和要求输出：\n- 使用位精度测试用例 $b \\in \\{32,16,8,7,5,3,2,1\\}$。\n- 对于该集合中的每个 $b$，运行上述训练并记录一个指示崩溃的布尔值：如果 $L(w_{T})  \\tau$ 则输出 $\\mathrm{True}$，否则输出 $\\mathrm{False}$。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，\"[False,False,False,True,True,True,True,True]\"），顺序与测试用例 $[32,16,8,7,5,3,2,1]$ 完全一致。\n\n所有量均为无单位实数。不使用角度。最终输出格式要求是单行，其中包含指定格式的布尔值列表，无任何附加文本。", "solution": "该问题陈述具有科学依据，提法恰当且客观。它提出了一个关于优化算法中数值精度研究的、清晰且可验证的计算实验。所有参数和函数都有明确定义，且前提与数学和机器学习原理一致。该问题是有效的。\n\n任务是模拟使用全批量梯度下降法训练一个单一线性神经元，其中梯度被量化。我们将分析不同位精度下的行为，并确定训练过程何时崩溃，崩溃定义为未能达到特定的损失阈值。\n\n首先，我们构建损失函数及其梯度。模型是一个线性神经元，对于输入矩阵 $X$ 和权重向量 $w$，其输出为 $Xw$。损失函数是经验均方误差：\n$$\nL(w) = \\frac{1}{N} \\lVert Xw - y \\rVert_2^2\n$$\n问题指定 $N=5$, $d=5$, $X = I_5$（$5 \\times 5$ 单位矩阵），并且目标输出 $y$ 被设置为等于目标权重 $w^{\\star}$。因此，损失函数简化为：\n$$\nL(w) = \\frac{1}{5} \\lVert I_5 w - w^\\star \\rVert_2^2 = \\frac{1}{5} \\lVert w - w^\\star \\rVert_2^2\n$$\n此损失的全局最小值在 $w = w^\\star$ 处取得，为 $L=0$。损失函数关于 $w$ 的梯度是：\n$$\n\\nabla L(w) = \\frac{2}{N} X^{\\top}(Xw - y) = \\frac{2}{5} I_5^{\\top}(I_5 w - w^\\star) = \\frac{2}{5} (w - w^\\star)\n$$\n\n训练过程从初始参数向量 $w_0 = \\mathbf{0}$ 开始。因此，初始梯度为：\n$$\ng_0 = \\nabla L(w_0) = \\frac{2}{5} (w_0 - w^\\star) = -\\frac{2}{5} w^\\star\n$$\n给定 $w^{\\star} = [1, -2, 3, -4, 5]^{\\top}$，我们计算 $g_0$：\n$$\ng_0 = -\\frac{2}{5} [1, -2, 3, -4, 5]^{\\top} = [-0.4, 0.8, -1.2, 1.6, -2.0]^{\\top}\n$$\n量化方案使用一个固定的动态范围 $s_{\\mathrm{fixed}}$，该范围由初始梯度分量的最大绝对值确定：\n$$\ns_{\\mathrm{fixed}} = \\max_{j} |(g_0)_j| = \\max\\{0.4, 0.8, 1.2, 1.6, 2.0\\} = 2.0\n$$\n\n训练使用更新规则进行 $T=2000$ 次迭代：\n$$\nw_{t+1} = w_t - \\eta \\, Q_b(\\nabla L(w_t))\n$$\n其中学习率 $\\eta=1$，而 $Q_b$ 是 $b$ 位精度的量化算子。\n\n对于 $b \\ge 2$，均匀量化步长 $\\Delta_b$ 定义为：\n$$\n\\Delta_b = \\frac{s_{\\mathrm{fixed}}}{2^{b-1}-1} = \\frac{2.0}{2^{b-1}-1}\n$$\n逐分量量化函数为：\n$$\n[Q_b(g)]_j = \\operatorname{sign}(g_j) \\cdot \\Delta_b \\cdot \\left\\lfloor \\frac{|g_j|}{\\Delta_b} \\right\\rfloor\n$$\n此函数将任意梯度分量 $g_j$ 映射到最接近零的 $\\Delta_b$ 的倍数。零附近存在一个“死区”：如果 $|g_j|  \\Delta_b$，则 $[Q_b(g)]_j = 0$。\n\n对于 $b=1$ 的边界情况，量化为：\n$$\n[Q_1(g)]_j = s_{\\mathrm{fixed}} \\cdot \\operatorname{sign}(g_j) = 2.0 \\cdot \\operatorname{sign}(g_j)\n$$\n这将任何非零梯度分量映射到 $+2.0$ 或 $-2.0$。\n\n模拟将针对每个位精度 $b \\in \\{32, 16, 8, 7, 5, 3, 2, 1\\}$ 运行。在 $T=2000$ 次迭代后，我们计算最终损失 $L(w_T)$ 并检查是否发生崩溃，即 $L(w_T)  \\tau=0.002$。\n\n导致崩溃的一个关键原因是量化死区。如果初始梯度分量的绝对值 $|(g_0)_j|$ 小于量化步长 $\\Delta_b$，那么该分量从一开始就会被量化为零。相应的权重 $w_j$ 将永远不会从其初始值 $0$ 更新。$g_0$ 中分量的最小绝对值为 $|(g_0)_0| = 0.4$。如果 $\\Delta_b  0.4$，则必定发生崩溃：\n$$\n\\frac{2.0}{2^{b-1}-1}  0.4 \\implies 5  2^{b-1}-1 \\implies 6  2^{b-1} \\implies b-1  \\log_2(6) \\approx 2.585 \\implies b  3.585\n$$\n因此，对于 $b=3, 2, 1$，我们预计会发生崩溃，因为至少有一个权重分量将无法学习。\n\n对于较高的 $b$ 值，其中 $\\Delta_b$ 足够小以使所有分量都能开始学习，量化误差仍然限制了最终的精度。当每个分量的梯度大小 $|\\nabla L(w_t)_j|$ 降至 $\\Delta_b$ 以下时，训练实际上会停滞。这会在权重中留下一个残余误差 $w_T - w^\\star$，其中每个分量大致受 $\\frac{N}{2}\\Delta_b$ 的限制。最终损失大约与 $\\Delta_b^2$ 成正比。如果这个可达到的最小损失大于阈值 $\\tau$，则会发生崩溃。我们预计这会发生在像 $7$ 和 $5$ 这样的中等 $b$ 值上。\n\n对于大的 $b$（例如，$32, 16$），$\\Delta_b$ 非常小，量化误差可以忽略不计，算法收敛良好，最终损失低于 $\\tau$。所提供的程序精确地实现了这一模拟，以找出崩溃点。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates training a single linear neuron with quantized gradients\n    to study learning under finite precision.\n    \"\"\"\n    # Define problem parameters\n    N = 5.0\n    d = 5\n    w_star = np.array([1.0, -2.0, 3.0, -4.0, 5.0])\n    w0 = np.zeros(d)\n    eta = 1.0\n    T = 2000\n    tau = 0.002\n\n    # Define the bit-precision test cases\n    b_cases = [32, 16, 8, 7, 5, 3, 2, 1]\n    \n    results = []\n\n    # Calculate the fixed dynamic range s_fixed from the initial gradient.\n    # This is constant for all test cases.\n    g0 = (2.0 / N) * (w0 - w_star)\n    s_fixed = np.max(np.abs(g0))\n\n    # Iterate over each bit-precision case\n    for b in b_cases:\n        # Initialize weights for the current simulation\n        w = w0.copy()\n\n        # Define the quantizer for the current bit precision b\n        def quantize_gradient(g: np.ndarray, b_val: int) -> np.ndarray:\n            \"\"\"\n            Applies component-wise quantization to the gradient vector.\n            \"\"\"\n            # Case b=1: Sign quantization\n            if b_val == 1:\n                return s_fixed * np.sign(g)\n\n            # Case b >= 2: Uniform quantization\n            # The denominator 2**(b-1) - 1 is greater than 0 for b >= 2.\n            denominator = (2**(b_val - 1)) - 1\n            \n            # For very large b like 32, delta_b is very small but non-zero.\n            # If hypothetically b was so large that denominator overflows,\n            # delta_b would tend to 0, meaning no quantization.\n            if denominator == 0:  # Should not happen for b>=2\n                return g\n            \n            delta_b = s_fixed / denominator\n\n            # The quantizer function is Q_b(g)_j = sign(g_j) * delta_b * floor(|g_j|/delta_b)\n            # This is equivalent to rounding towards zero (truncation).\n            sign_g = np.sign(g)\n            abs_g = np.abs(g)\n            \n            # np.floor implements the rounding-to-zero logic when combined with sign.\n            quantized_magnitude = delta_b * np.floor(abs_g / delta_b)\n            \n            return sign_g * quantized_magnitude\n\n        # Main training loop\n        for _ in range(T):\n            # 1. Calculate the full-precision gradient\n            gradient = (2.0 / N) * (w - w_star)\n            \n            # 2. Quantize the gradient\n            quantized_grad = quantize_gradient(gradient, b)\n            \n            # 3. Update the weights\n            w = w - eta * quantized_grad\n            \n        # After training, calculate the final loss\n        # L(w) = (1/N) * ||w - w_star||^2\n        final_loss = (1.0 / N) * np.linalg.norm(w - w_star)**2\n        \n        # Determine if a breakdown occurred based on the threshold tau\n        breakdown = final_loss > tau\n        results.append(breakdown)\n\n    # Final print statement in the exact required format.\n    # The output format is a string representation of a Python list of booleans.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2373947"}, {"introduction": "传统的梯度下降假设参数存在于一个平坦的欧几里得空间中。然而，在许多高级应用中，参数可能受到特定约束，从而生活在一个弯曲的流形上。本练习将带领您进入几何深度学习的核心，通过在一个单位球面上约束权重向量，实践黎曼优化（Riemannian optimization）。您将学习如何将反向传播推广到非欧几里得几何结构，这不仅是一个强大的技术，也为您理解更广义的优化理论奠定了基础。[@problem_id:2373877]", "problem": "构造一个完整且可运行的程序，该程序训练一个具有二元逻辑输出的单神经元人工神经网络（ANN），同时将权重向量约束在单位球面上。该神经元将一个输入向量 $\\mathbf{x} \\in \\mathbb{R}^3$ 映射为一个概率 $\\hat{y} = \\sigma(z)$，其中 $z = \\mathbf{w} \\cdot \\mathbf{x} + b$，$\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$。权重 $\\mathbf{w} \\in \\mathbb{R}^3$ 被约束在单位球面 $S^2 = \\{\\mathbf{w} \\in \\mathbb{R}^3 \\mid \\|\\mathbf{w}\\|_2 = 1\\}$ 上，而偏置 $b \\in \\mathbb{R}$ 不受约束。对于一个数据集 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^m$（其中 $y_i \\in \\{0,1\\}$），其损失函数为平均二元交叉熵\n$$\n\\mathcal{L}(\\mathbf{w}, b) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y_i \\log \\sigma(z_i) - (1 - y_i) \\log (1 - \\sigma(z_i)) \\right],\n$$\n其中 $z_i = \\mathbf{w} \\cdot \\mathbf{x}_i + b$，$m$ 是样本数量。\n\n单位球面 $S^2$ 被赋予了由 $\\mathbb{R}^3$ 上的标准欧几里得内积所导出的黎曼度量。在点 $\\mathbf{w} \\in S^2$ 处，其切空间为\n$$\nT_{\\mathbf{w}} S^2 = \\left\\{ \\mathbf{v} \\in \\mathbb{R}^3 \\ \\big| \\ \\mathbf{w} \\cdot \\mathbf{v} = 0 \\right\\}。\n$$\n设 $\\nabla_{\\!E} \\mathcal{L}(\\mathbf{w}, b)$ 表示关于 $\\mathbf{w}$ 的欧几里得梯度。$S^2$ 上的黎曼梯度是 $\\nabla_{\\!E} \\mathcal{L}$ 在 $T_{\\mathbf{w}} S^2$ 上的正交投影：\n$$\n\\operatorname{grad}_{\\!S^2} \\mathcal{L}(\\mathbf{w}, b) = \\nabla_{\\!E} \\mathcal{L}(\\mathbf{w}, b) - \\left( \\mathbf{w} \\cdot \\nabla_{\\!E} \\mathcal{L}(\\mathbf{w}, b) \\right) \\mathbf{w}.\n$$\n对于一个步长 $\\alpha  0$，一个离散时间学习步骤必须将 $\\mathbf{w}$ 沿从 $\\mathbf{w}$ 出发、方向为 $-\\operatorname{grad}_{\\!S^2} \\mathcal{L}$ 的测地线移动测地线距离 $\\alpha \\|\\operatorname{grad}_{\\!S^2} \\mathcal{L}\\|_2$。也就是说，下一个迭代点是黎曼指数映射：\n$$\n\\mathbf{w}_{\\text{next}} =\n\\begin{cases}\n\\cos(\\|\\mathbf{v}\\|_2)\\, \\mathbf{w} + \\sin(\\|\\mathbf{v}\\|_2) \\dfrac{\\mathbf{v}}{\\|\\mathbf{v}\\|_2},  \\text{if } \\|\\mathbf{v}\\|_2 \\neq 0, \\\\\n\\mathbf{w},  \\text{if } \\|\\mathbf{v}\\|_2 = 0,\n\\end{cases}\n\\quad \\text{with } \\mathbf{v} = -\\alpha \\, \\operatorname{grad}_{\\!S^2} \\mathcal{L}(\\mathbf{w}, b).\n$$\n偏置 $b$ 在欧几里得意义上更新，即 $b_{\\text{next}} = b - \\alpha \\, \\dfrac{\\partial \\mathcal{L}}{\\partial b}$。\n\n使用 $\\mathbb{R}^3$ 中包含 $m=4$ 个样本的数据集：\n- $\\mathbf{x}_1 = (1, 0, 0)$，$y_1 = 1$，\n- $\\mathbf{x}_2 = (0, 1, 0)$，$y_2 = 0$，\n- $\\mathbf{x}_3 = (0, 0, 1)$，$y_3 = 1$，\n- $\\mathbf{x}_4 = (-1, 0, 0)$，$y_4 = 0$。\n\n您的程序必须：\n- 实现上述模型、损失函数和几何结构，\n- 从第一性原理出发计算 $\\nabla_{\\!E} \\mathcal{L}$ 和 $\\dfrac{\\partial \\mathcal{L}}{\\partial b}$，\n- 对每个测试用例，执行指定次数的离散时间学习动态迭代，\n- 报告每个测试用例的最终平均损失值 $\\mathcal{L}$（一个浮点数）、最终欧几里得范数 $\\|\\mathbf{w}\\|_2$（一个浮点数）和最终偏置 $b$（一个浮点数）。\n\n测试套件：\n- 情况 A (一般情况): 步长 $\\alpha = 0.1$, 迭代次数 $T = 50$, 初始权重 $\\mathbf{w}_0 = \\left(\\dfrac{1}{\\sqrt{3}}, \\dfrac{1}{\\sqrt{3}}, \\dfrac{1}{\\sqrt{3}}\\right)$, 初始偏置 $b_0 = 0$。\n- 情况 B (边界情况): 步长 $\\alpha = 0$, 迭代次数 $T = 50$, 初始权重 $\\mathbf{w}_0 = (1, 0, 0)$, 初始偏置 $b_0 = 0$。\n- 情况 C (大步长边缘情况): 步长 $\\alpha = 1$, 迭代次数 $T = 10$, 初始权重 $\\mathbf{w}_0 = (0, 0, 1)$, 初始偏置 $b_0 = 0.5$。\n\n三角函数中的角度使用弧度制。本问题不涉及物理单位。您的程序应生成单行输出，其中包含一个由方括号括起来的、逗号分隔的结果列表，每个测试用例贡献一个按 $[\\text{final\\_loss}, \\|\\mathbf{w}\\|_2, b]$ 顺序排列的、包含三个浮点数的列表。例如，输出格式必须严格遵循 $[[\\ell_A, n_A, b_A],[\\ell_B, n_B, b_B],[\\ell_C, n_C, b_C]]$ 的形式，不得包含任何额外文本。", "solution": "问题陈述已经过验证，被认为是有效的。这是一个计算物理和机器学习领域的良定问题，具体涉及黎曼流形上的优化。该问题具有科学依据，无内部矛盾，并包含推导出唯一解所需的所有必要信息。\n\n任务是为一个使用逻辑激活函数的单神经元实现一个训练算法。其关键约束是权重向量 $\\mathbf{w} \\in \\mathbb{R}^3$ 必须位于单位球面上，即 $\\|\\mathbf{w}\\|_2 = 1$。这一约束将优化问题从标准的欧几里得设定转变为黎曼流形 $S^2$ 上的问题。\n\n首先，我们为优化过程推导必要的梯度。模型对输入 $\\mathbf{x}_i$ 的预测为 $\\hat{y}_i = \\sigma(z_i)$，其中 $z_i = \\mathbf{w} \\cdot \\mathbf{x}_i + b$ 且 $\\sigma(z) = (1 + e^{-z})^{-1}$ 是 sigmoid 函数。损失函数是 $m$ 个样本数据集上的平均二元交叉熵：\n$$\n\\mathcal{L}(\\mathbf{w}, b) = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}_i = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y_i \\log \\sigma(z_i) - (1 - y_i) \\log (1 - \\sigma(z_i)) \\right]\n$$\n为了求得关于参数 $\\mathbf{w}$ 和 $b$ 的梯度，我们应用链式法则。单个样本的损失 $\\mathcal{L}_i$ 关于线性激活值 $z_i$ 的导数是一个标准结果：\n$$\n\\frac{\\partial \\mathcal{L}_i}{\\partial z_i} = \\frac{\\partial \\mathcal{L}_i}{\\partial \\hat{y}_i} \\frac{\\partial \\hat{y}_i}{\\partial z_i}\n$$\n各分量为：\n$$\n\\frac{\\partial \\mathcal{L}_i}{\\partial \\hat{y}_i} = -\\frac{y_i}{\\hat{y}_i} + \\frac{1-y_i}{1-\\hat{y}_i} = \\frac{\\hat{y}_i - y_i}{\\hat{y}_i(1-\\hat{y}_i)}\n$$\n$$\n\\frac{\\partial \\hat{y}_i}{\\partial z_i} = \\frac{d\\sigma(z_i)}{dz_i} = \\sigma(z_i)(1 - \\sigma(z_i)) = \\hat{y}_i(1-\\hat{y}_i)\n$$\n将它们结合起来，我们得到了一个非常简洁的形式：\n$$\n\\frac{\\partial \\mathcal{L}_i}{\\partial z_i} = \\hat{y}_i - y_i\n$$\n现在，我们可以求出总损失 $\\mathcal{L}$ 关于 $\\mathbf{w}$ 的欧几里得梯度（记为 $\\nabla_{\\!E} \\mathcal{L}$）以及关于偏置 $b$ 的偏导数。\n$$\n\\nabla_{\\!E} \\mathcal{L}(\\mathbf{w}, b) = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}} = \\frac{1}{m} \\sum_{i=1}^m \\frac{\\partial \\mathcal{L}_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial \\mathbf{w}} = \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}_i - y_i) \\mathbf{x}_i\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m \\frac{\\partial \\mathcal{L}_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}_i - y_i) \\cdot 1 = \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}_i - y_i)\n$$\n该优化算法以迭代方式进行。偏置 $b$ 不受约束，可以使用标准欧几里得梯度下降法进行更新：\n$$\nb_{k+1} = b_k - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}\n$$\n然而，权重向量 $\\mathbf{w}$ 必须保持在单位球面 $S^2$ 上。一个朴素的欧几里得更新 $\\mathbf{w}_{k+1} = \\mathbf{w}_k - \\alpha \\nabla_{\\!E} \\mathcal{L}$ 会使向量偏离球面。正确的步骤包括两个步骤：\n\n1.  **梯度投影**: 流形上的最速下降方向由黎曼梯度 $\\operatorname{grad}_{\\!S^2} \\mathcal{L}$ 给出。这是通过将环境欧几里得梯度 $\\nabla_{\\!E} \\mathcal{L}$ 投影到当前点 $\\mathbf{w}$ 的切空间 $T_{\\mathbf{w}} S^2$ 上得到的。投影公式如下：\n    $$\n    \\operatorname{grad}_{\\!S^2} \\mathcal{L}(\\mathbf{w}, b) = \\nabla_{\\!E} \\mathcal{L} - (\\mathbf{w} \\cdot \\nabla_{\\!E} \\mathcal{L}) \\mathbf{w}\n    $$\n    此操作减去了欧几里得梯度中垂直于球面的分量，只保留了切向分量。\n\n2.  **收缩（Retraction）**: 更新步骤是通过从 $\\mathbf{w}$ 点出发，沿着负黎曼梯度的方向在测地线（球面上的大圆）上移动来执行的。这个操作称为指数映射。我们首先在切空间中定义更新向量 $\\mathbf{v} = -\\alpha \\, \\operatorname{grad}_{\\!S^2} \\mathcal{L}$。其模长 $\\|\\mathbf{v}\\|_2$ 是我们移动所沿弧的长度。更新后的权重向量 $\\mathbf{w}_{\\text{next}}$ 由指定的黎曼指数映射公式给出：\n    $$\n    \\mathbf{w}_{\\text{next}} =\n    \\begin{cases}\n    \\cos(\\|\\mathbf{v}\\|_2)\\, \\mathbf{w} + \\sin(\\|\\mathbf{v}\\|_2) \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|_2},  \\text{if } \\|\\mathbf{v}\\|_2  0, \\\\\n    \\mathbf{w},  \\text{if } \\|\\mathbf{v}\\|_2 = 0.\n    \\end{cases}\n    $$\n    如果 $\\|\\mathbf{w}\\|_2 = 1$，此更新能稳健地确保 $\\|\\mathbf{w}_{\\text{next}}\\|_2 = 1$，从而保持了约束条件。\n\n实现将包含一个运行固定迭代次数 $T$ 的循环。在每次迭代中，程序将为所有数据点计算预测值 $\\hat{y}_i$，计算欧几里得梯度，投影权重梯度以获得黎曼梯度，然后使用各自的更新规则更新 $b$ 和 $\\mathbf{w}$。最终的损失、权重向量范数和偏置将根据问题陈述中指定的每个测试用例进行报告。为提高效率，所有计算都将使用 `numpy` 中的向量化操作来执行。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem by running the specified test cases.\n    \"\"\"\n    # Define the dataset\n    X = np.array([\n        [1.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0],\n        [0.0, 0.0, 1.0],\n        [-1.0, 0.0, 0.0]\n    ])\n    y = np.array([1.0, 0.0, 1.0, 0.0])\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"alpha\": 0.1, \n            \"T\": 50, \n            \"w0\": np.array([1.0, 1.0, 1.0]) / np.sqrt(3), \n            \"b0\": 0.0\n        },\n        {\n            \"alpha\": 0.0, \n            \"T\": 50, \n            \"w0\": np.array([1.0, 0.0, 0.0]), \n            \"b0\": 0.0\n        },\n        {\n            \"alpha\": 1.0, \n            \"T\": 10, \n            \"w0\": np.array([0.0, 0.0, 1.0]), \n            \"b0\": 0.5\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        final_loss, final_w_norm, final_b = train_riemannian_neuron(\n            X, y, case[\"w0\"], case[\"b0\"], case[\"alpha\"], case[\"T\"]\n        )\n        results.append([final_loss, final_w_norm, final_b])\n\n    # Final print statement in the exact required format.\n    # The string representation is manipulated to remove spaces for exact format matching.\n    final_output_str = str(results).replace(\" \", \"\")\n    print(final_output_str)\n\ndef _sigmoid(z):\n    \"\"\"Computes the sigmoid function.\"\"\"\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef _compute_loss(y, y_hat):\n    \"\"\"Computes the average binary cross-entropy loss.\"\"\"\n    m = len(y)\n    # Adding a small epsilon for numerical stability in log, though not strictly\n    # necessary as sigmoid output is always in (0,1).\n    epsilon = 1e-9\n    loss = -np.sum(y * np.log(y_hat + epsilon) + (1 - y) * np.log(1 - y_hat + epsilon)) / m\n    return loss\n\ndef train_riemannian_neuron(X, y, w0, b0, alpha, T):\n    \"\"\"\n    Trains a single neuron with a spherical weight constraint using Riemannian optimization.\n    \n    Args:\n        X (np.ndarray): Input data, shape (m, n).\n        y (np.ndarray): Target labels, shape (m,).\n        w0 (np.ndarray): Initial weight vector on the unit sphere, shape (n,).\n        b0 (float): Initial bias.\n        alpha (float): Learning rate (step size).\n        T (int): Number of training iterations.\n\n    Returns:\n        tuple: A tuple containing the final loss (float), final weight norm (float), \n               and final bias (float).\n    \"\"\"\n    w = np.copy(w0)\n    b = b0\n    m = X.shape[0]\n\n    for _ in range(T):\n        # Forward pass\n        z = X @ w + b\n        y_hat = _sigmoid(z)\n\n        # Gradient calculation (from first principles)\n        error = y_hat - y\n        \n        # Euclidean gradient w.r.t. w\n        grad_w_euclidean = (X.T @ error) / m\n        \n        # Gradient w.r.t. b\n        grad_b = np.mean(error)\n        \n        # Riemannian gradient for w (projection onto tangent space)\n        grad_w_riemannian = grad_w_euclidean - np.dot(w, grad_w_euclidean) * w\n\n        # Update bias (standard gradient descent)\n        b = b - alpha * grad_b\n        \n        # Update weights (Riemannian exponential map)\n        v = -alpha * grad_w_riemannian\n        v_norm = np.linalg.norm(v)\n        \n        # Geodesic update\n        if v_norm > 1e-15: # To avoid division by zero\n            w = np.cos(v_norm) * w + np.sin(v_norm) * v / v_norm\n\n    # Final computation after all iterations\n    z_final = X @ w + b\n    y_hat_final = _sigmoid(z_final)\n    final_loss = _compute_loss(y, y_hat_final)\n    final_w_norm = np.linalg.norm(w)\n    \n    return final_loss, final_w_norm, b\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2373877"}]}