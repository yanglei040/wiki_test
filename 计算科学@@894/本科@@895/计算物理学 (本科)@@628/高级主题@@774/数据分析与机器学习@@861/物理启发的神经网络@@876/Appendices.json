{"hands_on_practices": [{"introduction": "这第一个练习为理解物理启发神经网络（PINN）奠定了基础。我们将一个经典的物理问题——静电学的泊松方程——转化为机器学习的语言，通过构建 PINN 的核心组成部分：其损失函数。这个实践将巩固你对物理定律和边界条件如何被编码为可训练目标的理解。[@problem_id:2126324]", "problem": "一位研究人员正在构建一个物理信息神经网络（PINN），以寻找一个二维方形区域内的静电势 $V(x,y)$ 的近似解。该电势的物理行为由泊松方程描述：\n$$\n\\nabla^2 V(x,y) = -f(x,y)\n$$\n其中 $f(x,y)$ 表示给定的电荷分布密度，而 $\\nabla^2 = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}$ 是拉普拉斯算子。该电势定义在域 $D = \\{(x,y) \\mid -L \\le x \\le L, -L \\le y \\le L\\}$ 上。该域的边界 $\\partial D$ 保持在零电势（接地），这施加了边界条件 $V(x,y) = 0$ 对所有 $(x,y) \\in \\partial D$ 成立。\n\nPINN模型，记为 $\\hat{V}(x,y; \\theta)$，通过最小化一个包含了问题物理信息的损失函数 $L(\\theta)$ 来学习近似 $V(x,y)$。此处，$\\theta$ 代表神经网络的所有可训练参数。损失函数的计算使用两组离散点集：\n1.  一组位于域 $D$ 内部的 $N_{pde}$ 个配置点，$S_{pde} = \\{(x_i, y_i) \\mid i=1, \\dots, N_{pde}\\}$。\n2.  一组位于边界 $\\partial D$ 上的 $N_{bc}$ 个边界点，$S_{bc} = \\{(x_j, y_j) \\mid j=1, \\dots, N_{bc}\\}$。\n\n总损失函数 $L(\\theta)$ 是两个均方误差项之和：一个用于控制偏微分方程 ($L_{pde}$)，另一个用于边界条件 ($L_{bc}$)。\n\n构建总损失函数 $L(\\theta) = L_{pde} + L_{bc}$ 的数学表达式。你的表达式应使用网络输出 $\\hat{V}$、其二阶偏导数、函数 $f$、给定的点集及其各自的大小 $N_{pde}$ 和 $N_{bc}$ 来表示。", "solution": "我们从控制泊松方程和边界条件开始：\n$$\n\\nabla^{2}V(x,y)=-f(x,y), \\quad V(x,y)=0 \\text{ for } (x,y)\\in \\partial D.\n$$\n物理信息神经网络用 $\\hat{V}(x,y;\\theta)$ 来近似 $V$。在内部配置点 $(x_{i},y_{i})\\in S_{pde}$ 处的偏微分方程（PDE）残差通过将泊松方程施加于 $\\hat{V}$ 来定义：\n$$\nr_{i}(\\theta)=\\nabla^{2}\\hat{V}(x_{i},y_{i};\\theta)+f(x_{i},y_{i}).\n$$\n使用拉普拉斯算子在二维中的定义，这等价于\n$$\nr_{i}(\\theta)=\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i}).\n$$\n在 $S_{pde}$ 上强制执行PDE的均方误差则为\n$$\nL_{pde}(\\theta)=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(r_{i}(\\theta)\\right)^{2}=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}.\n$$\n边界 $\\partial D$ 上的边界条件 $V=0$ 是通过惩罚 $\\hat{V}$ 在边界点 $(x_{j},y_{j})\\in S_{bc}$ 处与零的偏差来强制执行的：\n$$\nL_{bc}(\\theta)=\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)-0\\right)^{2}=\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}.\n$$\n因此，总损失是这两个均方误差项的和：\n$$\nL(\\theta)=L_{pde}(\\theta)+L_{bc}(\\theta)=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}.\n$$", "answer": "$$\\boxed{\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}}$$", "id": "2126324"}, {"introduction": "在掌握了基础知识之后，我们现在来解决一个更具动态性和挑战性的问题：无粘性伯格斯方程，该方程可用于模拟冲击波的形成。这个练习将教你如何将 PINN 框架扩展到时间依赖和非线性系统，并将初始条件与边界条件和基于物理的损失项结合起来。为这个问题构建损失函数，展示了 PINN 在处理复杂流体动力学问题时的通用性。[@problem_id:2126315]", "problem": "物理信息神经网络 (PINN) 是一种神经网络，它在训练以解决监督学习任务的同时，遵循由一般非线性偏微分方程 (PDE) 描述的给定物理定律。我们的目标是为一种旨在模拟冲击波形成的PINN构建训练目标，即损失函数。\n\n该物理系统由一维、瞬态、无粘性Burgers方程控制：\n$$\n\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = 0\n$$\n解$u(t, x)$在由$x \\in [-1, 1]$和$t \\in [0, T]$（对于某个最终时间$T > 0$）定义的时空域内求解。\n\n系统从给定的光滑初始条件演化而来：\n$$\nu(0, x) = -\\sin(\\pi x) \\quad \\text{for } x \\in [-1, 1]\n$$\n系统满足周期性边界条件：\n$$\nu(t, -1) = u(t, 1) \\quad \\text{for } t \\in [0, T]\n$$\n\n我们使用一个神经网络（表示为$\\hat{u}(t, x; \\theta)$）来近似真实解$u(t, x)$。这里，$\\theta$表示网络的所有可训练参数（权重和偏置）。训练过程涉及最小化一个在离散样本点集上评估的损失函数$\\mathcal{L}(\\theta)$：\n1.  **初始条件点**：在初始时间切片上的一组$N_{IC}$个点$S_{IC} = \\{(0, x_i)\\}_{i=1}^{N_{IC}}$。\n2.  **边界条件点**：在不同时刻的空间边界上的一组$N_{BC}$对点$S_{BC} = \\{(t_j, -1), (t_j, 1)\\}_{j=1}^{N_{BC}}$。\n3.  **配置点**：在域内部$(t,x) \\in (0, T] \\times (-1, 1)$分布的一组$N_{PDE}$个点$S_{PDE} = \\{(t_k, x_k)\\}_{k=1}^{N_{PDE}}$。\n\n您的任务是构建总损失函数$\\mathcal{L}(\\theta)$。该损失函数应被形式化为对应于初始条件、边界条件和PDE残差的均方误差之和。假设这三个损失分量的所有权重因子都等于1。请给出$\\mathcal{L}(\\theta)$的最终表达式。", "solution": "我们用神经网络$\\hat{u}(t,x;\\theta)$来近似无粘性Burgers方程$\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = 0$的解$u(t,x)$。通过将$\\hat{u}$代入PDE得到的残差驱动至零来施加物理约束，而通过匹配初始和边界条件来施加数据约束。\n\n在任意点$(t,x)$处的PDE残差通过将$\\hat{u}$代入控制方程来定义：\n$$\nr_{\\theta}(t,x) \\equiv \\frac{\\partial \\hat{u}}{\\partial t}(t,x;\\theta) + \\hat{u}(t,x;\\theta)\\,\\frac{\\partial \\hat{u}}{\\partial x}(t,x;\\theta).\n$$\n在配置点$S_{PDE} = \\{(t_{k},x_{k})\\}_{k=1}^{N_{PDE}}$处，均方残差为\n$$\n\\mathcal{L}_{PDE}(\\theta) = \\frac{1}{N_{PDE}} \\sum_{k=1}^{N_{PDE}} \\left( \\frac{\\partial \\hat{u}}{\\partial t}(t_{k},x_{k};\\theta) + \\hat{u}(t_{k},x_{k};\\theta)\\,\\frac{\\partial \\hat{u}}{\\partial x}(t_{k},x_{k};\\theta) \\right)^{2}.\n$$\n\n初始条件为$u(0,x) = -\\sin(\\pi x)$。在初始条件点$S_{IC} = \\{(0,x_{i})\\}_{i=1}^{N_{IC}}$处，初始条件均方误差为\n$$\n\\mathcal{L}_{IC}(\\theta) = \\frac{1}{N_{IC}} \\sum_{i=1}^{N_{IC}} \\left( \\hat{u}(0,x_{i};\\theta) + \\sin(\\pi x_{i}) \\right)^{2}.\n$$\n\n周期性边界条件为$u(t,-1) = u(t,1)$。在边界点$S_{BC} = \\{(t_{j},-1),(t_{j},1)\\}_{j=1}^{N_{BC}}$处，边界条件均方误差为\n$$\n\\mathcal{L}_{BC}(\\theta) = \\frac{1}{N_{BC}} \\sum_{j=1}^{N_{BC}} \\left( \\hat{u}(t_{j},-1;\\theta) - \\hat{u}(t_{j},1;\\theta) \\right)^{2}.\n$$\n\n当所有分量的权重均为1时，总损失函数是各项之和\n$$\n\\mathcal{L}(\\theta) = \\mathcal{L}_{IC}(\\theta) + \\mathcal{L}_{BC}(\\theta) + \\mathcal{L}_{PDE}(\\theta).\n$$\n代入各分量的定义，得到显式表达式\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N_{IC}} \\sum_{i=1}^{N_{IC}} \\left( \\hat{u}(0,x_{i};\\theta) + \\sin(\\pi x_{i}) \\right)^{2} + \\frac{1}{N_{BC}} \\sum_{j=1}^{N_{BC}} \\left( \\hat{u}(t_{j},-1;\\theta) - \\hat{u}(t_{j},1;\\theta) \\right)^{2} + \\frac{1}{N_{PDE}} \\sum_{k=1}^{N_{PDE}} \\left( \\frac{\\partial \\hat{u}}{\\partial t}(t_{k},x_{k};\\theta) + \\hat{u}(t_{k},x_{k};\\theta)\\,\\frac{\\partial \\hat{u}}{\\partial x}(t_{k},x_{k};\\theta) \\right)^{2}.\n$$", "answer": "$$\\boxed{\\mathcal{L}(\\theta)=\\frac{1}{N_{IC}}\\sum_{i=1}^{N_{IC}}\\left(\\hat{u}(0,x_{i};\\theta)+\\sin(\\pi x_{i})\\right)^{2}+\\frac{1}{N_{BC}}\\sum_{j=1}^{N_{BC}}\\left(\\hat{u}(t_{j},-1;\\theta)-\\hat{u}(t_{j},1;\\theta)\\right)^{2}+\\frac{1}{N_{PDE}}\\sum_{k=1}^{N_{PDE}}\\left(\\frac{\\partial \\hat{u}}{\\partial t}(t_{k},x_{k};\\theta)+\\hat{u}(t_{k},x_{k};\\theta)\\frac{\\partial \\hat{u}}{\\partial x}(t_{k},x_{k};\\theta)\\right)^{2}}$$", "id": "2126315"}, {"introduction": "这最后一个实践从理论转向代码，动手演示了 PINN 的一个关键特性，即“谱偏差”（spectral bias）。通过实现并训练一个神经网络来求解一个特殊设计的常微分方程，你将定量地观察到 PINN 倾向于先学习低频模式，后学习高频模式。这个高级练习能让你深入洞察这些强大模型在实际训练中的动态特性和局限性。[@problem_id:2427229]", "problem": "你将实现一个完整、可运行的程序，以展示物理信息神经网络（PINN）的光谱偏差（spectral bias）。其核心思想是训练一个 PINN 来求解一个一维边值问题，该问题的已知解是一个低频正弦波和一个高频正弦波的叠加，即 $u(x) = \\sin(x) + \\sin(25x)$，并定量观察在训练过程中哪个频率分量被首先学习到。整个过程中角度必须使用弧度制。\n\n从以下具有周期性边界条件的物理一致常微分方程（ODE）开始：\n给定定义域 $x \\in [0, 2\\pi]$，考虑\n$$\nu''(x) + u(x) = -624 \\sin(25x),\n$$\n其周期性边界条件为\n$$\nu(0) = u(2\\pi), \\quad u'(0) = u'(2\\pi).\n$$\n经验证，如果 $u(x) = \\sin(x) + \\sin(25x)$，则 $u''(x) + u(x) = -624 \\sin(25x)$ 且周期性边界条件成立。除了边界条件外，你不得使用任何关于 $u(x)$ 的标记训练数据；相反，应在损失函数中使用 ODE 残差和边界残差，这是物理信息神经网络（PINN）的标准做法。\n\n构建一个具有 $H$ 个隐藏单元和双曲正切激活函数的单隐层神经网络 $u_{\\theta}(x)$ 作为试探解。将隐藏层预激活定义为 $z_i(x) = w_i x + b_i$（其中 $i \\in \\{1,\\dots,H\\}$），隐藏层激活定义为 $h_i(x) = \\tanh(z_i(x))$，输出定义为\n$$\nu_{\\theta}(x) = \\sum_{i=1}^{H} a_i h_i(x) + c.\n$$\n使用链式法则和乘积法则，以闭合形式计算 $u_{\\theta}(x)$ 关于 $x$ 的一阶和二阶导数。回顾双曲正切函数及其导数的标准恒等式：\n$$\n\\tanh'(z) = \\operatorname{sech}^2(z), \\quad \\frac{d}{dz}\\operatorname{sech}^2(z) = -2\\operatorname{sech}^2(z)\\tanh(z), \\quad \\operatorname{sech}^2(z) = 1 - \\tanh^2(z).\n$$\n将配置点 $\\{x_n\\}_{n=1}^{N}$ 的逐点物理残差定义为\n$$\nr_{\\text{phys}}(x_n;\\theta) = u_{\\theta}''(x_n) + u_{\\theta}(x_n) - \\left(-624 \\sin(25 x_n)\\right),\n$$\n以及周期性边界残差定义为\n$$\nr_{\\text{bc},1}(\\theta) = u_{\\theta}(0) - u_{\\theta}(2\\pi), \\quad r_{\\text{bc},2}(\\theta) = u_{\\theta}'(0) - u_{\\theta}'(2\\pi).\n$$\n使用带有边界权重 $\\lambda_{\\text{bc}}$ 的均方残差损失函数：\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n;\\theta)^2 + \\lambda_{\\text{bc}}\\left(r_{\\text{bc},1}(\\theta)^2 + r_{\\text{bc},2}(\\theta)^2\\right).\n$$\n通过基于梯度的优化方法，从随机初始化开始训练参数 $\\theta = \\{a_i, w_i, b_i, c\\}_{i=1}^{H}$。为了定量评估光谱偏差，在较短的训练预算结束时，通过最小二乘法将学习到的函数 $u_{\\theta}(x)$ 投影到 $[0, 2\\pi)$ 上密集均匀网格上的两个基函数 $\\sin(x)$ 和 $\\sin(25x)$ 上。也就是说，找到最小化以下表达式的系数 $(\\hat{\\alpha}_1, \\hat{\\alpha}_{25})$：\n$$\n\\sum_{m=1}^{M}\\left(u_{\\theta}(x_m) - \\hat{\\alpha}_1 \\sin(x_m) - \\hat{\\alpha}_{25}\\sin(25 x_m)\\right)^2,\n$$\n其中 $x_m$ 在 $[0, 2\\pi)$ 上均匀分布。将学习到的振幅定义为 $A_1 = |\\hat{\\alpha}_1|$ 和 $A_{25} = |\\hat{\\alpha}_{25}|$。如果在早期训练中 $A_1 > A_{25}$，则认为存在光谱偏差。\n\n使用完全向量化的训练循环和关于所有网络参数的闭合形式梯度来实现该程序，仅使用 ODE 残差和边界残差。不要使用任何外部自动微分库。\n\n测试套件和输出规范：\n- 使用以下三个测试用例来检验不同体系。每个用例指定 $(H, N, K, \\eta)$，其中 $H$ 是隐藏单元数， $N$ 是配置点数， $K$ 是梯度步数，$\\eta$ 是学习率。在所有用例中均使用 $\\lambda_{\\text{bc}} = 1$。角度以弧度为单位。\n  1. 用例 1： $(H, N, K, \\eta) = (20, 128, 60, 0.01)$。\n  2. 用例 2： $(H, N, K, \\eta) = (10, 64, 80, 0.01)$。\n  3. 用例 3： $(H, N, K, \\eta) = (5, 128, 120, 0.01)$。\n- 对于每个用例，使用固定的种子初始化参数，以确保结果是确定性的。训练 $K$ 步后，通过在包含 $M=4096$ 个点的密集网格上进行最小二乘投影来计算 $A_1$ 和 $A_{25}$。记录用例的布尔结果，定义如下\n$$\n\\text{result} = \\begin{cases}\n\\text{True}, & \\text{if } A_1 > A_{25},\\\\\n\\text{False}, & \\text{otherwise.}\n\\end{cases}\n$$\n- 最终输出格式：你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，“[True,True,False]”）。\n\n你的程序必须是自包含的，不接收任何输入，并且可以直接运行。角度必须以弧度为单位。所有数值答案都是无量纲的，最终输出是布尔值。训练和投影必须使用上述公式，通过纯线性代数实现，不使用任何外部机器学习框架。目标是通过这些测试用例证明，物理信息神经网络（PINN）会先学习到低频分量 $\\sin(x)$，后学习到高频分量 $\\sin(25x)$，这与光谱偏差的特性一致。", "solution": "目标是训练一个神经网络 $u_{\\theta}(x)$ 来近似求解一维常微分方程（ODE）\n$$\nu''(x) + u(x) = -624 \\sin(25x)\n$$\n在定义域 $x \\in [0, 2\\pi]$ 上，并满足周期性边界条件 $u(0) = u(2\\pi)$ 和 $u'(0) = u'(2\\pi)$。其解析解 $u(x) = \\sin(x) + \\sin(25x)$ 是一个低频分量和一个高频分量的叠加。我们将证明，对 PINN 损失函数进行基于梯度的优化，会使网络学习低频分量 $\\sin(x)$ 的速度快于学习高频分量 $\\sin(25x)$ 的速度。\n\n首先，我们定义神经网络拟设（ansatz），这是一个具有 $H$ 个神经元和 $\\tanh$ 激活函数的单隐层感知机：\n$$\nu_{\\theta}(x) = \\sum_{i=1}^{H} a_i \\tanh(w_i x + b_i) + c\n$$\n该网络的参数为 $\\theta = \\{a_i, w_i, b_i, c\\}_{i=1}^{H}$。为了强制满足该 ODE，我们必须计算 $u_{\\theta}(x)$ 关于 $x$ 的一阶和二阶导数。使用链式法则以及恒等式 $\\frac{d}{dz}\\tanh(z) = \\operatorname{sech}^2(z)$ 和 $\\frac{d}{dz}\\operatorname{sech}^2(z) = -2\\operatorname{sech}^2(z)\\tanh(z)$，我们得到：\n$$\nu'_{\\theta}(x) = \\frac{d u_{\\theta}}{dx} = \\sum_{i=1}^{H} a_i w_i \\operatorname{sech}^2(w_i x + b_i)\n$$\n$$\nu''_{\\theta}(x) = \\frac{d^2 u_{\\theta}}{dx^2} = -2 \\sum_{i=1}^{H} a_i w_i^2 \\operatorname{sech}^2(w_i x + b_i) \\tanh(w_i x + b_i)\n$$\n\n通过最小化一个由 ODE 残差和边界条件残差的均方误差组成的损失函数来训练网络。在一组 $N$ 个配置点 $\\{x_n\\}$ 上的物理残差为：\n$$\nr_{\\text{phys}}(x_n;\\theta) = u_{\\theta}''(x_n) + u_{\\theta}(x_n) + 624 \\sin(25 x_n)\n$$\n周期性边界条件残差为：\n$$\nr_{\\text{bc},1}(\\theta) = u_{\\theta}(0) - u_{\\theta}(2\\pi)\n$$\n$$\nr_{\\text{bc},2}(\\theta) = u_{\\theta}'(0) - u_{\\theta}'(2\\pi)\n$$\n总损失函数是一个加权和：\n$$\n\\mathcal{L}(\\theta) = \\mathcal{L}_{\\text{phys}} + \\lambda_{\\text{bc}} \\mathcal{L}_{\\text{bc}} = \\frac{1}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n;\\theta)^2 + \\lambda_{\\text{bc}}\\left(r_{\\text{bc},1}(\\theta)^2 + r_{\\text{bc},2}(\\theta)^2\\right)\n$$\n其中 $\\lambda_{\\text{bc}}$ 是一个用于平衡各项的超参数，给定值为 $\\lambda_{\\text{bc}} = 1$。\n\n训练使用梯度下降法进行。参数根据 $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} \\mathcal{L}(\\theta)$ 进行更新，其中 $\\eta$ 是学习率。我们必须推导出解析梯度 $\\nabla_{\\theta} \\mathcal{L}(\\theta)$。损失函数关于任意参数 $p \\in \\theta$ 的梯度为：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p} = \\frac{2}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n) \\left(\\frac{\\partial u''_{\\theta}(x_n)}{\\partial p} + \\frac{\\partial u_{\\theta}(x_n)}{\\partial p}\\right) + 2\\lambda_{\\text{bc}}r_{\\text{bc},1}\\left(\\frac{\\partial u_{\\theta}(0)}{\\partial p} - \\frac{\\partial u_{\\theta}(2\\pi)}{\\partial p}\\right) + 2\\lambda_{\\text{bc}}r_{\\text{bc},2}\\left(\\frac{\\partial u'_{\\theta}(0)}{\\partial p} - \\frac{\\partial u'_{\\theta}(2\\pi)}{\\partial p}\\right)\n$$\n网络输出及其空间导数相对于参数 $\\{a_k, w_k, b_k, c\\}$ 的导数通过链式法则计算。这些推导虽然繁琐但系统，并且为了计算效率以向量化形式实现。例如，关于输出权重 $a_k$ 的梯度包含诸如 $\\frac{\\partial u_{\\theta}(x)}{\\partial a_k} = \\tanh(w_k x + b_k)$ 的项。所有梯度的完整表达式都在代码中实现。\n\n在训练了指定的步数后，我们量化学习到的频率分量。我们在 $[0, 2\\pi)$ 上的一个包含 $M$ 个点的密集网格 $\\{x_m\\}$ 上评估训练好的网络 $u_{\\theta}(x)$。然后，我们通过求解一个线性最小二乘问题，将这个学习到的函数投影到基函数 $\\sin(x)$ 和 $\\sin(25x)$ 上，以找到最小化以下表达式的系数 $(\\hat{\\alpha}_1, \\hat{\\alpha}_{25})$：\n$$\n\\sum_{m=1}^{M}\\left(u_{\\theta}(x_m) - \\hat{\\alpha}_1 \\sin(x_m) - \\hat{\\alpha}_{25}\\sin(25 x_m)\\right)^2\n$$\n这个问题的解由 $\\hat{\\boldsymbol{\\alpha}} = (\\mathbf{B}^T\\mathbf{B})^{-1}\\mathbf{B}^T\\mathbf{y}$ 给出，其中 $\\mathbf{y}$ 是网络预测值 $u_{\\theta}(x_m)$ 的向量，$\\mathbf{B}$ 是以 $\\sin(x_m)$ 和 $\\sin(25x_m)$ 为列的设计矩阵。学习到的振幅为 $A_1 = |\\hat{\\alpha}_1|$ 和 $A_{25} = |\\hat{\\alpha}_{25}|$。如果 $A_1 > A_{25}$，我们则断定观察到了光谱偏差。\n\n该实现将遵循这些原则，使用 `numpy` 进行向量化的数值计算，包括完全解析的梯度计算和标准的梯度下降循环。参数初始化将使用固定的随机种子和 Glorot/Xavier 缩放，以确保可复现性和训练稳定性。", "answer": "```python\nimport numpy as np\n\nclass PINN:\n    \"\"\"\n    A Physics-Informed Neural Network to demonstrate spectral bias.\n    The implementation is fully vectorized and uses analytical gradients.\n    \"\"\"\n    def __init__(self, H, N, seed):\n        \"\"\"\n        Initializes the PINN.\n        H: number of hidden units\n        N: number of collocation points\n        seed: random seed for parameter initialization\n        \"\"\"\n        self.H = H\n        self.N = N\n        self.lambda_bc = 1.0\n        self.rng = np.random.default_rng(seed)\n\n        # Xavier/Glorot initialization\n        # For weights w, n_in=1, n_out=1 (conceptual). limit = sqrt(6 / (1+1)) = sqrt(3)\n        limit_w = np.sqrt(3.0)\n        self.w = self.rng.uniform(-limit_w, limit_w, size=(1, self.H))\n        \n        # For weights a, n_in=H, n_out=1. limit = sqrt(6 / (H+1))\n        limit_a = np.sqrt(6.0 / (self.H + 1.0))\n        self.a = self.rng.uniform(-limit_a, limit_a, size=(self.H, 1))\n\n        self.b = np.zeros((1, self.H))\n        self.c = np.zeros((1, 1))\n\n        self.x_colloc = np.linspace(0, 2 * np.pi, self.N, endpoint=False).reshape(-1, 1)\n\n    def forward(self, x):\n        \"\"\"\n        Computes the network output u and its derivatives u', u'' w.r.t. x.\n        x: input points, shape (num_points, 1)\n        \"\"\"\n        z = x @ self.w + self.b\n        h = np.tanh(z)\n        s = 1.0 - h**2  # sech^2(z)\n\n        u = h @ self.a + self.c\n        u_prime = (s * self.w) @ self.a\n        u_double_prime = (-2.0 * s * h * (self.w**2)) @ self.a\n\n        return u, u_prime, u_double_prime, h, s\n\n    def _compute_gradients(self):\n        \"\"\"\n        Computes the loss and the gradients of the loss w.r.t. all parameters.\n        All calculations are vectorized.\n        \"\"\"\n        # --- Physics Loss and Gradients ---\n        u, _, u_pp, H_c, S_c = self.forward(self.x_colloc)\n        \n        f_term = -624.0 * np.sin(25.0 * self.x_colloc)\n        r_phys = u_pp + u - f_term\n        loss_phys = np.mean(r_phys**2)\n\n        # Common factor for physics gradients\n        grad_common_phys = (2.0 / self.N) * r_phys\n        \n        # Gradient w.r.t. a\n        d_u_da = H_c\n        d_u_pp_da = -2.0 * S_c * H_c * self.w**2\n        grad_a_phys = (d_u_da + d_u_pp_da).T @ grad_common_phys\n\n        # Gradient w.r.t. b\n        d_u_db = self.a.T * S_c\n        d_u_pp_db = self.a.T * (4.0 * self.w**2 * S_c * H_c**2 - 2.0 * self.w**2 * S_c**2)\n        grad_b_phys = np.sum(grad_common_phys * (d_u_db + d_u_pp_db), axis=0)\n\n        # Gradient w.r.t. w\n        d_u_dw = self.a.T * self.x_colloc * S_c\n        d_u_pp_dw = self.a.T * (\n            -4.0 * self.w * S_c * H_c + \n            self.x_colloc * (4.0 * self.w**2 * S_c * H_c**2 - 2.0 * self.w**2 * S_c**2)\n        )\n        grad_w_phys = np.sum(grad_common_phys * (d_u_dw + d_u_pp_dw), axis=0)\n        \n        # Gradient w.r.t. c\n        grad_c_phys = np.sum(grad_common_phys)\n\n        # --- Boundary Loss and Gradients ---\n        x_bc = np.array([[0.0], [2 * np.pi]])\n        u_bc, u_p_bc, _, H_bc, S_bc = self.forward(x_bc)\n        \n        u0, u2pi = u_bc[0], u_bc[1]\n        u0_p, u2pi_p = u_p_bc[0], u_p_bc[1]\n\n        r_bc1 = u0 - u2pi\n        r_bc2 = u0_p - u2pi_p\n        loss_bc = r_bc1**2 + r_bc2**2\n        \n        H0, H2pi = H_bc[0:1, :], H_bc[1:2, :]\n        S0, S2pi = S_bc[0:1, :], S_bc[1:2, :]\n        \n        # Common factors for BC gradients\n        common1 = 2.0 * self.lambda_bc * r_bc1\n        common2 = 2.0 * self.lambda_bc * r_bc2\n\n        # Gradient w.r.t. a\n        delta_u_da = (H0 - H2pi).T\n        delta_u_p_da = (self.w * (S0 - S2pi)).T\n        grad_a_bc = common1 * delta_u_da + common2 * delta_u_p_da\n        \n        # Gradient w.r.t. b\n        delta_u_db = self.a.T * (S0 - S2pi)\n        delta_u_p_db = -2.0 * self.a.T * self.w * (S0 * H0 - S2pi * H2pi)\n        grad_b_bc = common1 * delta_u_db + common2 * delta_u_p_db\n\n        # Gradient w.r.t. w\n        delta_u_dw = -2.0 * np.pi * self.a.T * S2pi\n        delta_u_p_dw = self.a.T * (S0 - S2pi) + 4.0 * np.pi * self.a.T * self.w * S2pi * H2pi\n        grad_w_bc = common1 * delta_u_dw + common2 * delta_u_p_dw\n\n        # Gradient w.r.t. c (is zero)\n        grad_c_bc = 0.0\n\n        # --- Total Loss and Gradients ---\n        loss = loss_phys + self.lambda_bc * loss_bc\n        grad_a = grad_a_phys + grad_a_bc\n        grad_w = grad_w_phys.reshape(1, -1) + grad_w_bc\n        grad_b = grad_b_phys.reshape(1, -1) + grad_b_bc\n        grad_c = grad_c_phys + grad_c_bc\n\n        return loss, grad_a, grad_w, grad_b, grad_c\n\n    def train(self, K, eta):\n        \"\"\"\n        Trains the network using gradient descent.\n        K: number of training steps\n        eta: learning rate\n        \"\"\"\n        for _ in range(K):\n            loss, grad_a, grad_w, grad_b, grad_c = self._compute_gradients()\n            \n            self.a -= eta * grad_a\n            self.w -= eta * grad_w\n            self.b -= eta * grad_b\n            self.c -= eta * grad_c\n\n    def project_and_analyze(self):\n        \"\"\"\n        Projects the learned function onto sin(x) and sin(25x) and checks for spectral bias.\n        \"\"\"\n        M = 4096\n        x_dense = np.linspace(0, 2 * np.pi, M, endpoint=False).reshape(-1, 1)\n        \n        u_pred, _, _, _, _ = self.forward(x_dense)\n        u_pred = u_pred.flatten()\n        \n        # Create design matrix for least squares\n        B = np.zeros((M, 2))\n        B[:, 0] = np.sin(x_dense.flatten())\n        B[:, 1] = np.sin(25.0 * x_dense.flatten())\n        \n        # Solve least squares problem: B * alpha = u_pred\n        alpha, _, _, _ = np.linalg.lstsq(B, u_pred, rcond=None)\n        \n        A1 = np.abs(alpha[0])\n        A25 = np.abs(alpha[1])\n        \n        return A1 > A25\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        (20, 128, 60, 0.01),  # Case 1: (H, N, K, eta)\n        (10, 64, 80, 0.01),   # Case 2\n        (5, 128, 120, 0.01),  # Case 3\n    ]\n\n    results = []\n    base_seed = 42\n\n    for i, (H, N, K, eta) in enumerate(test_cases):\n        seed = base_seed + i\n        pinn = PINN(H=H, N=N, seed=seed)\n        pinn.train(K=K, eta=eta)\n        result = pinn.project_and_analyze()\n        results.append(result)\n\n    # Format the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2427229"}]}