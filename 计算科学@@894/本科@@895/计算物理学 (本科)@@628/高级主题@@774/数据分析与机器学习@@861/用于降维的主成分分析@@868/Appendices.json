{"hands_on_practices": [{"introduction": "理论与实践的结合是掌握任何计算方法的关键。本练习旨在通过“第一性原理”来巩固您对主成分分析 (PCA) 的理解。我们将从一个具有已知协方差矩阵 $\\Sigma$ 的多元高斯分布中生成合成数据，然后应用 PCA 来检验我们能否准确地恢复其底层的特征向量和特征值。这个过程不仅验证了样本协方差与总体协方差之间的理论联系，还使我们能够深刻理解样本量如何影响 PCA 结果的准确性 [@problem_id:2430049]。", "problem": "您将编写一个完整的程序，该程序从具有已知协方差矩阵的多元高斯分布中构建合成数据，执行主成分分析，并验证经验主成分在指定容差内与总体主成分匹配。请完全基于概率论和线性代数的第一性原理进行工作。最终程序必须按如下规定生成单行输出。\n\n定义与设置：\n- 令 $d \\in \\mathbb{N}$ 表示维度，令 $N \\in \\mathbb{N}$ 表示独立样本的数量。\n- 令 $\\Sigma \\in \\mathbb{R}^{d \\times d}$ 是一个对称正定协方差矩阵，其特征分解为\n$$\n\\Sigma = Q \\,\\Lambda\\, Q^\\top,\n$$\n其中 $Q \\in \\mathbb{R}^{d \\times d}$ 是一个正交矩阵，其列是总体特征向量 $\\{q_i\\}_{i=1}^d$，$ \\Lambda = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_d)$，其中总体特征值满足 $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_d \\gt 0$。\n- 从均值为零、协方差为 $\\Sigma$ 的多元高斯分布（记为 $\\mathcal{N}(0,\\Sigma)$）中生成 $N$ 个独立样本 $x^{(1)},\\dots,x^{(N)} \\in \\mathbb{R}^d$。\n- 通过减去样本均值并使用无偏估计量，从样本中构建经验协方差矩阵 $S \\in \\mathbb{R}^{d \\times d}$：\n$$\n\\bar{x} = \\frac{1}{N} \\sum_{n=1}^{N} x^{(n)}, \\quad\nS = \\frac{1}{N-1} \\sum_{n=1}^{N} \\big(x^{(n)} - \\bar{x}\\big)\\big(x^{(n)} - \\bar{x}\\big)^\\top.\n$$\n- 通过计算 $S$ 的特征分解来对其执行主成分分析 (PCA)\n$$\nS = U \\, M \\, U^\\top,\n$$\n其中 $U \\in \\mathbb{R}^{d \\times d}$ 具有标准正交列 $\\{u_i\\}_{i=1}^d$（经验特征向量），$M = \\mathrm{diag}(\\mu_1,\\dots,\\mu_d)$，其经验特征值按 $\\mu_1 \\ge \\mu_2 \\ge \\cdots \\ge \\mu_d \\ge 0$ 排序。\n\n验证标准：\n- 特征值：定义最大相对误差\n$$\n\\varepsilon_\\lambda = \\max_{1 \\le i \\le d} \\frac{|\\mu_i - \\lambda_i|}{\\lambda_i}.\n$$\n- 非简并特征值的特征向量：对于任何索引 $i$，如果其对应的总体特征值 $\\lambda_i$ 与所有其他总体特征值都不同，则通过以下方式定义其对齐性\n$$\nc_i = |q_i^\\top u_i|, \\quad s_i = \\sqrt{1 - c_i^2}.\n$$\n- 简并特征值的特征子空间：对于任何大小为 $|C| \\ge 2$ 的索引集 $C \\subset \\{1,\\dots,d\\}$，其中所有的 $\\{\\lambda_i\\}_{i \\in C}$ 都相等，定义总体投影算子 $P_{\\mathrm{true}}(C) = Q_{[:,C]} Q_{[:,C]}^\\top$ 和经验投影算子 $P_{\\mathrm{est}}(C) = U_{[:,C]} U_{[:,C]}^\\top$，并度量\n$$\n\\Delta_P(C) = \\| P_{\\mathrm{est}}(C) - P_{\\mathrm{true}}(C) \\|_F,\n$$\n其中 $\\|\\cdot\\|_F$ 是弗罗贝尼乌斯范数。\n\n对于每个测试用例，当且仅当以下所有条件同时成立时，才判定为成功：\n- $\\varepsilon_\\lambda \\le \\tau_\\lambda$，\n- 对于所有单元素索引集 $C = \\{i\\}$，$s_i \\le \\tau_v$，\n- 对于所有简并索引集 $C$（$|C| \\ge 2$），$\\Delta_P(C) \\le \\tau_P$。\n\n对所有用例使用以下全局容差：$\\tau_\\lambda = 0.06$，$\\tau_v = 0.10$，$\\tau_P = 0.20$。\n\n为确保可复现性的构造细节：\n- 对于每个测试用例，通过对一个 $d \\times d$ 矩阵进行 $\\mathrm{QR}$ 分解来构造正交矩阵 $Q$。该矩阵的元素是使用指定整数种子初始化的伪随机数生成器生成的独立标准正态分布条目。调整列的符号，以使来自 $\\mathrm{QR}$ 分解的 $R$ 矩阵的对角线元素为非负。根据指定的总体特征值构造 $\\Lambda$，并设置 $\\Sigma = Q \\Lambda Q^\\top$。\n- 使用相同的随机数生成器（使用该用例指定的相同种子进行初始化）抽取具有独立标准正态分布条目的矩阵 $Z \\in \\mathbb{R}^{N \\times d}$，并设置 $X = Z \\, B^\\top$（其中 $B = Q \\,\\Lambda^{1/2}$，$\\Lambda^{1/2} = \\mathrm{diag}(\\sqrt{\\lambda_1},\\dots,\\sqrt{\\lambda_d})$），从而生成 $N$ 个样本。\n\n测试套件：\n- 用例 1（非简并谱，3维）：\n  - $d = 3$, $N = 80000$, 总体特征值 $[\\lambda_1,\\lambda_2,\\lambda_3] = [4.0, 1.0, 0.25]$, 种子 $= 11$, 简并索引集 $= \\{\\{1\\},\\{2\\},\\{3\\}\\}$.\n- 用例 2（简并主子空间，5维）：\n  - $d = 5$, $N = 100000$, 总体特征值 $[\\lambda_1,\\dots,\\lambda_5] = [5.0, 5.0, 2.0, 1.0, 0.5]$, 种子 $= 22$, 简并索引集 $= \\{\\{1,2\\},\\{3\\},\\{4\\},\\{5\\}\\}$.\n- 用例 3（高各向异性，2维）：\n  - $d = 2$, $N = 40000$, 总体特征值 $[\\lambda_1,\\lambda_2] = [9.0, 0.09]$, 种子 $= 33$, 简并索引集 $= \\{\\{1\\},\\{2\\}\\}$.\n- 用例 4（完全各向同性，4维）：\n  - $d = 4$, $N = 20000$, 总体特征值 $[\\lambda_1,\\dots,\\lambda_4] = [1.0, 1.0, 1.0, 1.0]$, 种子 $= 44$, 简并索引集 $= \\{\\{1,2,3,4\\}\\}$.\n\n所有公式中的索引都如书写所示是基于 1 的；当使用基于 0 的索引的语言实现时，请相应地调整索引。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，该列表按上述顺序包含测试用例的结果，每个条目都是一个布尔值，表示该用例是否成功，例如，`[True,False,True,True]`。", "solution": "目标是验证对经验数据进行的主成分分析，能否在指定容差内恢复多元高斯分布的总体协方差矩阵的特征结构。解决方案从多元高斯分布、协方差和特征分解的定义出发。\n\n1. 总体构造。对于每个用例，我们给定一个维度 $d$、一组非递增排序的总体特征值 $\\{\\lambda_i\\}_{i=1}^d$ 和一个种子。我们构造一个正交矩阵 $Q \\in \\mathbb{R}^{d \\times d}$ 来定义特征向量，并设置\n$$\n\\Lambda = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_d), \\quad \\Sigma = Q \\Lambda Q^\\top.\n$$\n根据构造，$\\Sigma$ 是对称正定的，因为对于所有 $i$，$\\lambda_i \\gt 0$ 且 $Q$ 是正交的。\n\n为了获得 $Q$，我们从一个具有独立标准正态分布条目的随机矩阵 $A \\in \\mathbb{R}^{d \\times d}$ 中抽样，并计算其 $\\mathrm{QR}$ 分解，$A = \\tilde{Q} R$，其中 $\\tilde{Q}$ 是正交的，$R$ 是上三角矩阵。为消除列符号的模糊性并使映射具有确定性，我们调整符号，使得 $R$ 的对角线元素为非负：如果 $R_{ii} \\lt 0$，我们将 $\\tilde{Q}$ 的第 $i$ 列和 $R$ 的第 $i$ 行乘以 $-1$。最终得到的 $Q$ 是正交的，并且其列在施蒂费尔流形上均匀分布。\n\n2. 数据生成。对于一个多元高斯分布 $\\mathcal{N}(0,\\Sigma)$，如果 $Z \\in \\mathbb{R}^{N \\times d}$ 具有独立标准正态分布条目且 $B \\in \\mathbb{R}^{d \\times d}$ 满足 $B B^\\top = \\Sigma$，则 $X = Z B^\\top$ 包含来自 $\\mathcal{N}(0,\\Sigma)$ 的 $N$ 个独立样本。使用 $\\Sigma$ 的特征分解，我们设置\n$$\nB = Q \\Lambda^{1/2}, \\quad \\Lambda^{1/2} = \\mathrm{diag}(\\sqrt{\\lambda_1},\\dots,\\sqrt{\\lambda_d}),\n$$\n这满足 $B B^\\top = Q \\Lambda^{1/2} (Q \\Lambda^{1/2})^\\top = Q \\Lambda Q^\\top = \\Sigma$。我们使用为该用例指定的种子初始化的伪随机数生成器来生成 $Z$，以确保可复现性。因此，我们获得数据矩阵 $X \\in \\mathbb{R}^{N \\times d}$，其行是样本向量 $x^{(n)}$。\n\n3. 经验协方差。我们计算样本均值\n$$\n\\bar{x} = \\frac{1}{N} \\sum_{n=1}^{N} x^{(n)},\n$$\n和无偏经验协方差矩阵\n$$\nS = \\frac{1}{N-1} \\sum_{n=1}^{N} \\big(x^{(n)} - \\bar{x}\\big)\\big(x^{(n)} - \\bar{x}\\big)^\\top.\n$$\n根据强大数定律，当 $N \\to \\infty$ 时，$S \\to \\Sigma$ 几乎必然成立。\n\n4. 主成分分析。我们计算 $S$ 的特征分解：\n$$\nS = U M U^\\top,\n$$\n其中 $U$ 具有标准正交列，$M$ 是具有非负条目的对角矩阵。我们将经验特征值排序为 $\\mu_1 \\ge \\mu_2 \\ge \\cdots \\ge \\mu_d \\ge 0$，并一致地排列相应的特征向量。\n\n5. 比较指标。\n- 特征值：我们计算最大相对误差\n$$\n\\varepsilon_\\lambda = \\max_{1 \\le i \\le d} \\frac{|\\mu_i - \\lambda_i|}{\\lambda_i}.\n$$\n由于 $S \\to \\Sigma$，我们期望 $\\mu_i \\to \\lambda_i$，因此对于足够大的 $N$，$\\varepsilon_\\lambda$ 会很小。\n\n- 非简并特征值的特征向量：当 $\\lambda_i$ 是单重的（与所有其他总体特征值不同）时，经典的矩阵扰动理论结果表明，相关的经验特征向量 $u_i$ 会收敛到 $q_i$（相差一个任意符号）。我们通过计算下式来处理符号模糊性\n$$\nc_i = |q_i^\\top u_i|, \\quad s_i = \\sqrt{1 - c_i^2}.\n$$\n这里 $s_i$ 是由 $q_i$ 和 $u_i$ 张成的一维子空间之间的主角的正弦值。我们要求 $s_i \\le \\tau_v$。\n\n- 简并特征值的特征子空间：当一组总体特征值相等时，相应的不变子空间是可识别的，但该子空间内的任何标准正交基都是可接受的。令 $C$ 为一个索引集，其大小 $|C| \\ge 2$，且对于所有 $i \\in C$，$\\lambda_i$ 都相等。我们比较这些子空间上的经验投影算子和总体投影算子：\n$$\nP_{\\mathrm{true}}(C) = Q_{[:,C]} Q_{[:,C]}^\\top, \\quad P_{\\mathrm{est}}(C) = U_{[:,C]} U_{[:,C]}^\\top,\n$$\n并通过弗罗贝尼乌斯范数来度量其差异\n$$\n\\Delta_P(C) = \\| P_{\\mathrm{est}}(C) - P_{\\mathrm{true}}(C) \\|_F.\n$$\n该度量对于张成子空间的标准正交基的选择是不变的。随着 $N$ 的增长，$\\Delta_P(C) \\to 0$。\n\n6. 容差与通过标准。我们强制执行三个容差：特征值相对误差的容差 $\\tau_\\lambda = 0.06$，由 $s_i$ 度量的一维子空间失准的容差 $\\tau_v = 0.10$，以及简并子空间投影算子差异的容差 $\\tau_P = 0.20$。一个测试用例在对相应索引和子空间的所有三个条件都同时满足时通过。\n\n7. 测试套件。我们评估四个用例：\n- 用例 1：$d = 3$，$N = 80000$，非简并谱，特征值为 $[\\lambda_1,\\lambda_2,\\lambda_3] = [4.0, 1.0, 0.25]$，种子 $= 11$，简并索引集 $\\{\\{1\\},\\{2\\},\\{3\\}\\}$。\n- 用例 2：$d = 5$, $N = 100000$，具有简并主特征值对，特征值为 $[\\lambda_1,\\dots,\\lambda_5] = [5.0, 5.0, 2.0, 1.0, 0.5]$，种子 $= 22$，简并索引集 $\\{\\{1,2\\},\\{3\\},\\{4\\},\\{5\\}\\}$。\n- 用例 3：$d = 2$, $N = 40000$，高各向异性，特征值为 $[\\lambda_1,\\lambda_2] = [9.0, 0.09]$，种子 $= 33$，简并索引集 $\\{\\{1\\},\\{2\\}\\}$。\n- 用例 4：$d = 4$, $N = 20000$，各向同性，特征值为 $[\\lambda_1,\\dots,\\lambda_4] = [1.0, 1.0, 1.0, 1.0]$，种子 $= 44$，简并索引集 $\\{\\{1,2,3,4\\}\\}$。在这种情况下，对于全空间聚类，$P_{\\mathrm{true}}(C) = I$ 且 $P_{\\mathrm{est}}(C) = I$，因此 $\\Delta_P(C) = 0$，这反映了特征向量无法识别，但子空间（整个空间）完全匹配的事实。\n\n8. 输出。对于每个用例，计算一个布尔值，指示是否所有标准都通过，并按顺序打印这四个布尔值的列表，格式化为单行“[b1,b2,b3,b4]”。\n\n该方法是合理的，其依据是样本协方差向总体协方差的收敛性，以及在对称矩阵扰动下特征值和不变子空间的连续性。指定的容差考虑了量级约为 $N^{-1/2}$ 的有限样本波动，而投影算子度量为简并特征空间提供了一种基不变的比较方法。", "answer": "```python\nimport numpy as np\n\ndef orthogonal_from_seed(d: int, seed: int) -> np.ndarray:\n    \"\"\"\n    Construct a deterministic orthogonal matrix Q in R^{d x d} from a given seed\n    by QR decomposition of a standard normal matrix with sign correction to\n    ensure R has nonnegative diagonal entries.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    A = rng.standard_normal((d, d))\n    Q, R = np.linalg.qr(A)\n    # Ensure deterministic sign: make diagonal of R nonnegative\n    signs = np.sign(np.diag(R))\n    signs[signs == 0.0] = 1.0\n    Q = Q * signs  # broadcast column scaling\n    return Q\n\ndef generate_samples(N: int, lambdas: np.ndarray, Q: np.ndarray, seed: int) -> np.ndarray:\n    \"\"\"\n    Generate N samples from N(0, Sigma) where Sigma = Q diag(lambdas) Q^T.\n    Uses X = Z B^T with B = Q diag(sqrt(lambdas)), Z ~ N(0, I).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    d = len(lambdas)\n    Z = rng.standard_normal((N, d))\n    sqrt_lambdas = np.sqrt(lambdas)\n    B = Q * sqrt_lambdas  # each column i of Q scaled by sqrt(lambdas[i])\n    X = Z @ B.T\n    return X\n\ndef empirical_covariance(X: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute unbiased empirical covariance matrix S from data matrix X (N x d),\n    by centering and using factor 1/(N-1).\n    \"\"\"\n    Xc = X - X.mean(axis=0, keepdims=True)\n    N = X.shape[0]\n    S = (Xc.T @ Xc) / (N - 1)\n    return S\n\ndef pca_eigendecomposition(S: np.ndarray):\n    \"\"\"\n    Compute eigen-decomposition of symmetric matrix S, returning eigenvalues\n    and eigenvectors sorted in descending order of eigenvalues.\n    \"\"\"\n    w, V = np.linalg.eigh(S)\n    idx = np.argsort(w)[::-1]\n    w = w[idx]\n    V = V[:, idx]\n    return w, V\n\ndef max_relative_eigenvalue_error(mu: np.ndarray, lam: np.ndarray) -> float:\n    return float(np.max(np.abs(mu - lam) / lam))\n\ndef singleton_vector_misalignment(U: np.ndarray, Q: np.ndarray, singletons: list) -> float:\n    \"\"\"\n    Compute max s_i = sqrt(1 - |q_i^T u_i|^2) over singleton index sets {i}.\n    Indices are 0-based.\n    \"\"\"\n    if not singletons:\n        return 0.0\n    s_vals = []\n    for i in singletons:\n        ci = float(abs(Q[:, i].T @ U[:, i]))\n        ci = max(min(ci, 1.0), 0.0)\n        si = float(np.sqrt(max(0.0, 1.0 - ci * ci)))\n        s_vals.append(si)\n    return float(np.max(s_vals)) if s_vals else 0.0\n\ndef max_projector_discrepancy(U: np.ndarray, Q: np.ndarray, clusters: list) -> float:\n    \"\"\"\n    For each cluster C with |C| >= 2, compute Frobenius norm of projector difference.\n    Return the maximum over such clusters; return 0.0 if none.\n    \"\"\"\n    deltas = []\n    for C in clusters:\n        if len(C) >= 2:\n            Uc = U[:, C]\n            Qc = Q[:, C]\n            Pest = Uc @ Uc.T\n            Ptrue = Qc @ Qc.T\n            D = Pest - Ptrue\n            delta = float(np.linalg.norm(D, ord='fro'))\n            deltas.append(delta)\n    return float(np.max(deltas)) if deltas else 0.0\n\ndef run_case(d: int, N: int, lambdas_list: list, seed: int, clusters_1based: list,\n             tau_lambda: float, tau_v: float, tau_P: float) -> bool:\n    \"\"\"\n    Run a single test case with specified parameters and tolerances.\n    clusters_1based is a list of index lists using 1-based indices; convert to 0-based.\n    \"\"\"\n    lambdas = np.array(lambdas_list, dtype=float)\n    # Build Q and Sigma\n    Q = orthogonal_from_seed(d, seed)\n    # Generate samples\n    X = generate_samples(N, lambdas, Q, seed)\n    # Empirical covariance and PCA\n    S = empirical_covariance(X)\n    mu, U = pca_eigendecomposition(S)\n    # Ensure eigenvalues are in descending order corresponding to lambdas (which are given descending)\n    # Prepare index clusters\n    clusters0 = [[i - 1 for i in C] for C in clusters_1based]\n    # Split clusters into singletons and degenerate\n    singleton_indices = [C[0] for C in clusters0 if len(C) == 1]\n    # Metrics\n    eps_lambda = max_relative_eigenvalue_error(mu, lambdas)\n    s_max = singleton_vector_misalignment(U, Q, singleton_indices)\n    deltaP_max = max_projector_discrepancy(U, Q, clusters0)\n    # Pass criteria\n    return (eps_lambda <= tau_lambda) and (s_max <= tau_v) and (deltaP_max <= tau_P)\n\ndef solve():\n    # Global tolerances\n    tau_lambda = 0.06\n    tau_v = 0.10\n    tau_P = 0.20\n\n    # Define the test cases in the specified order\n    test_cases = [\n        # Case 1: d=3, N=80000, lambdas=[4.0,1.0,0.25], seed=11, clusters={{1},{2},{3}}\n        {\"d\": 3, \"N\": 80000, \"lambdas\": [4.0, 1.0, 0.25], \"seed\": 11, \"clusters\": [[1], [2], [3]]},\n        # Case 2: d=5, N=100000, lambdas=[5.0,5.0,2.0,1.0,0.5], seed=22, clusters={{1,2},{3},{4},{5}}\n        {\"d\": 5, \"N\": 100000, \"lambdas\": [5.0, 5.0, 2.0, 1.0, 0.5], \"seed\": 22, \"clusters\": [[1, 2], [3], [4], [5]]},\n        # Case 3: d=2, N=40000, lambdas=[9.0,0.09], seed=33, clusters={{1},{2}}\n        {\"d\": 2, \"N\": 40000, \"lambdas\": [9.0, 0.09], \"seed\": 33, \"clusters\": [[1], [2]]},\n        # Case 4: d=4, N=20000, lambdas=[1.0,1.0,1.0,1.0], seed=44, clusters={{1,2,3,4}}\n        {\"d\": 4, \"N\": 20000, \"lambdas\": [1.0, 1.0, 1.0, 1.0], \"seed\": 44, \"clusters\": [[1, 2, 3, 4]]},\n    ]\n\n    results = []\n    for case in test_cases:\n        ok = run_case(\n            d=case[\"d\"],\n            N=case[\"N\"],\n            lambdas_list=case[\"lambdas\"],\n            seed=case[\"seed\"],\n            clusters_1based=case[\"clusters\"],\n            tau_lambda=tau_lambda,\n            tau_v=tau_v,\n            tau_P=tau_P\n        )\n        results.append(ok)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2430049"}, {"introduction": "尽管 PCA 功能强大，但若未经审慎的数据预处理而直接应用，其结果可能会产生误导。本实践探讨了一个至关重要的数据预处理步骤：标准化。我们将通过一个具体案例，展示具有巨大尺度差异的特征变量如何不成比例地主导分析结果，以及标准化如何通过将所有特征置于同等尺度上来解决这个问题，从而揭示数据中真实的潜在结构 [@problem_id:2430028]。", "problem": "给定一系列数据矩阵，其各列的数值尺度差异巨大。对于每种情况，考虑一个实值数据矩阵 $X \\in \\mathbb{R}^{n \\times d}$，该矩阵包含 $d=3$ 个特征和 $n$ 个样本。对于下文的每个测试用例，通过显式公式确定性地定义 $X$。任务是比较对均值中心化数据和标准化数据分别执行主成分分析 (PCA) 的结果，并量化主导主成分方向及其解释方差比例因标准化而发生的变化。此处，主成分分析 (PCA) 定义为对转换后数据的样本协方差矩阵进行特征分解。此处，特征标准化定义为对每一列进行中心化（减去其样本均值）并除以其样本标准差；如果某列的标准差为零，则在中心化后，该标准化列应恒为零。\n\n使用的定义：\n- 给定 $X \\in \\mathbb{R}^{n \\times d}$，令 $\\bar{\\mathbf{x}} \\in \\mathbb{R}^{d}$ 为列样本均值，并令 $X_c = X - \\mathbf{1}\\bar{\\mathbf{x}}^\\top$ 表示中心化数据，其中 $\\mathbf{1} \\in \\mathbb{R}^{n}$ 是全为 1 的向量。样本协方差矩阵为 $S = \\frac{1}{n-1} X_c^\\top X_c$。\n- PCA 的特征值和特征向量是 $S$ 的特征对 $\\{(\\lambda_k,\\mathbf{v}_k)\\}_{k=1}^d$，其中 $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_d \\ge 0$，特征向量 $\\{\\mathbf{v}_k\\}$ 是标准正交的。第一主成分的解释方差比率为 $r_1 = \\frac{\\lambda_1}{\\sum_{j=1}^d \\lambda_j}$。\n- 对于标准化数据，计算 $X_c$ 的列样本标准差 $\\sigma_j$。通过对所有 $\\sigma_j \\neq 0$ 的 $j$ 计算 $Z_{:,j} = X_{c,:,j}/\\sigma_j$ 来构造 $Z$，而对于任何 $\\sigma_j = 0$ 的 $j$，则设 $Z_{:,j} = \\mathbf{0}$。然后定义 $S^{(z)} = \\frac{1}{n-1} Z^\\top Z$ 及其特征对 $\\{(\\mu_k,\\mathbf{u}_k)\\}_{k=1}^d$，排序为 $\\mu_1 \\ge \\mu_2 \\ge \\mu_3 \\ge 0$，其解释方差比率为 $r_1^{(z)} = \\frac{\\mu_1}{\\sum_{j=1}^d \\mu_j}$。\n- 两个单位主方向 $\\mathbf{v}_1$ 和 $\\mathbf{u}_1$ 之间的一致性通过 $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1| \\in [0,1]$ 来衡量。绝对值处理了特征向量的符号不确定性。\n- 为了量化原始坐标轴对主成分的支配程度，定义 $i_{\\mathrm{before}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{v}_1)_j|$ 和 $i_{\\mathrm{after}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{u}_1)_j|$，特征索引使用从零开始的编号。\n\n对于每个测试用例，您必须计算以下有序量列表：\n- 从 $S$ 计算的 $r_1$，\n- 从 $S^{(z)}$ 计算的 $r_1^{(z)}$，\n- $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1|$，\n- $i_{\\mathrm{before}}$，\n- $i_{\\mathrm{after}}$。\n\n测试套件（每个用例定义了 $n$，然后为 $i \\in \\{0,\\dots,n-1\\}$ 定义了 $t_i$，以及作为 $t_i$ 的函数的三个特征）：\n- 用例 1：$n=200$。对于每个 $i \\in \\{0,\\dots,199\\}$，令 $t_i = \\frac{i}{199}$，并定义\n  - $x_{i1} = 1000 \\cos(2\\pi t_i)$，\n  - $x_{i2} = \\sin(2\\pi t_i) + 0.1 \\cos(4\\pi t_i)$，\n  - $x_{i3} = 0.001\\, t_i$。\n  通过将这三个特征作为列堆叠来组装 $X$。\n- 用例 2：$n=100$。对于每个 $i \\in \\{0,\\dots,99\\}$，令 $t_i = \\frac{i}{99}$，并定义\n  - $x_{i1} = 10^6\\, t_i$，\n  - $x_{i2} = 0$，\n  - $x_{i3} = 10\\,(t_i - 0.5)$。\n  通过将这三个特征作为列堆叠来组装 $X$。\n- 用例 3：$n=150$。对于每个 $i \\in \\{0,\\dots,149\\}$，令 $t_i = \\frac{i}{149}$，并定义\n  - $x_{i1} = 1000\\,(2 t_i - 1)$，\n  - $x_{i2} = (2 t_i - 1) + 0.1 \\sin(3\\pi t_i)$，\n  - $x_{i3} = 0.01 \\cos(5\\pi t_i)$。\n  通过将这三个特征作为列堆叠来组装 $X$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。对于每个用例，输出有序列表 $[r_1, r_1^{(z)}, a, i_{\\mathrm{before}}, i_{\\mathrm{after}}]$。将三个用例的结果聚合到一个包含三个列表的列表中，顺序为用例 1、2、3。例如，总体的打印结构必须是 $[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot]]$ 的形式。\n\n所有答案都是指定类型的无量纲实数或整数。不需要物理单位或角度单位，因为所有要求的量都是纯数。", "solution": "用户提供了一个有效且需要解答的问题。问题陈述在科学上基于线性代数和统计学领域，特别是主成分分析 (PCA)。该问题定义良好，为构建数据提供了确定性的指令，定义了所有必要的数学对象和程序，并要求一组特定的、可计算的量。其语言客观且无歧义。因此，可以构建一个合理的、分步的解决方案。\n\n该问题要求在三种不同情况下，比较对均值中心化数据与标准化数据执行 PCA 的结果。问题的核心在于观察缩放如何影响 PCA 的结果。PCA 识别数据集中的最大方差方向。当特征（数据矩阵的列）具有迥然不同的尺度时，无论底层数据结构如何，方差最大的特征将主导第一主成分。标准化通过将每个特征重新缩放到均值为 0、标准差为 1，将所有特征置于同等地位，从而防止了这种情况。\n\n总体步骤如下：\n1. 对于每个测试用例，构建 $n \\times d$ 数据矩阵 $X$，其中 $d=3$。\n2. 对均值中心化数据 $X_c$ 执行 PCA。\n    a. 计算列样本均值向量 $\\bar{\\mathbf{x}}$。\n    b. 数据中心化：$X_c = X - \\mathbf{1}\\bar{\\mathbf{x}}^\\top$。\n    c. 计算样本协方差矩阵 $S = \\frac{1}{n-1} X_c^\\top X_c$。\n    d. 通过求解特征问题 $S\\mathbf{v}_k = \\lambda_k\\mathbf{v}_k$ 来找到 $S$ 的特征值 $\\lambda_k$ 和特征向量 $\\mathbf{v}_k$。特征值被排序为 $\\lambda_1 \\ge \\lambda_2 \\ge \\lambda_3$，特征向量 $\\{\\mathbf{v}_k\\}$ 是标准正交的。第一主成分方向是 $\\mathbf{v}_1$。\n    e. 计算第一主成分的解释方差比率：$r_1 = \\lambda_1 / (\\sum_{j=1}^d \\lambda_j)$。\n    f. 识别主导 $\\mathbf{v}_1$ 的原始特征：$i_{\\mathrm{before}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{v}_1)_j|$。\n\n3. 对标准化数据 $Z$ 执行 PCA。\n    a. 使用 $n-1$ 作为除数，计算 $X$ 的列样本标准差 $\\sigma_j$。\n    b. 构建标准化数据矩阵 $Z$。每一列 $Z_{:,j}$ 是通过将相应的中心化列 $X_{c,:,j}$ 按 $1/\\sigma_j$ 缩放得到的。如果 $\\sigma_j=0$，则列 $Z_{:,j}$ 被设为零向量。\n    c. 计算 $Z$ 的样本协方差矩阵：$S^{(z)} = \\frac{1}{n-1} Z^\\top Z$。该矩阵等价于 $X$ 的样本相关矩阵。对于任何非常数特征，其对角线元素为 1。\n    d. 找到 $S^{(z)}$ 的特征值 $\\mu_k$ 和特征向量 $\\mathbf{u}_k$，排序使 $\\mu_1 \\ge \\mu_2 \\ge \\mu_3$。标准化数据的第一个主成分方向是 $\\mathbf{u}_1$。\n    e. 计算相应的解释方差比率：$r_1^{(z)} = \\mu_1 / (\\sum_{j=1}^d \\mu_j)$。分母中的和 $\\text{Tr}(S^{(z)})$ 等于非常数特征的数量。\n    f. 识别主导 $\\mathbf{u}_1$ 的原始特征：$i_{\\mathrm{after}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{u}_1)_j|$。\n\n4. 通过计算对齐度量 $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1|$（它衡量两个主方向之间夹角的余弦值）来比较两种分析的结果。\n\n5. 对于每种情况，最终输出是有序列表 $[r_1, r_1^{(z)}, a, i_{\\mathrm{before}}, i_{\\mathrm{after}}]$。\n\n具体用例分析：\n- **用例 1**：数据包含三个特征，其尺度分别为 $O(10^3)$、$O(1)$ 和 $O(10^{-3})$。第一个特征 $x_1 = 1000 \\cos(2\\pi t_i)$ 的方差将远远大于其他特征。因此，未标准化数据的第一个主成分 $\\mathbf{v}_1$ 预计将几乎与第一个特征轴完全对齐。这将得到 $r_1 \\approx 1$ 和 $i_{\\mathrm{before}} = 0$。标准化后，所有特征的方差均为单位方差，其结构关系（$(x_1, x_2)$ 平面中的椭圆轨迹）将变得明显。方差将更均匀地分布，导致 $r_1^{(z)}$ 变小，并且 $\\mathbf{u}_1$ 将是特征 1 和 2 的组合。\n- **用例 2**：第一个特征 $x_1 = 10^6 t_i$ 具有巨大的尺度。第二个特征 $x_2=0$ 是常数，方差为零。第三个特征 $x_3 = 10(t_i-0.5)$ 的尺度远小于第一个特征。对于未标准化的数据，PCA 将由特征 1 主导，得到 $r_1 \\approx 1$ 和 $i_{\\mathrm{before}}=0$。标准化后，常数特征 $x_2$ 仍然是一个零向量。特征 1 和 3 都是 $t_i$ 的线性函数，在中心化和缩放后将变得完全相关。它们的标准化版本将是相同的，$Z_{:,1} = Z_{:,3}$。数据将塌陷到 $(Z_1, Z_3)$ 平面中的单个方向上。这将导致 $r_1^{(z)}=1$（因为在非常数特征中，有效秩为 1）和一个形式为 $[1/\\sqrt{2}, 0, 1/\\sqrt{2}]^\\top$ 的特征向量 $\\mathbf{u}_1$。\n- **用例 3**：第一个特征 $x_1 = 1000(2t_i-1)$ 具有大尺度。第二个特征 $x_2 = (2t_i-1) + 0.1 \\sin(3\\pi t_i)$ 与第一个特征高度相关，但尺度小得多。第三个特征的尺度可以忽略不计。与其他情况一样，未标准化的 PCA 将由第一个特征的尺度决定，因此 $r_1 \\approx 1$ 且 $i_{\\mathrm{before}}=0$。标准化后，特征 1 和 2 之间的强线性关系将成为最显著的特征。第一个主成分 $\\mathbf{u}_1$ 将捕捉到这种共享方差，代表了相关数据云长轴的方向，大致在特征 1 和 2 的标准化轴之间成 45 度角。\n\n实现将利用 `numpy` 进行所有数值计算，特别是使用 `numpy.linalg.eigh` 对对称协方差矩阵进行特征分解。将注意处理特征值按降序排序以及标准差为零的情况。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PCA comparison problem for three given test cases.\n    \"\"\"\n\n    def generate_case_1_data():\n        n = 200\n        t = np.linspace(0, 1, n)\n        x1 = 1000 * np.cos(2 * np.pi * t)\n        x2 = np.sin(2 * np.pi * t) + 0.1 * np.cos(4 * np.pi * t)\n        x3 = 0.001 * t\n        return np.stack([x1, x2, x3], axis=1)\n\n    def generate_case_2_data():\n        n = 100\n        t = np.linspace(0, 1, n)\n        x1 = 1e6 * t\n        x2 = np.zeros(n)\n        x3 = 10 * (t - 0.5)\n        return np.stack([x1, x2, x3], axis=1)\n\n    def generate_case_3_data():\n        n = 150\n        t = np.linspace(0, 1, n)\n        x1 = 1000 * (2 * t - 1)\n        x2 = (2 * t - 1) + 0.1 * np.sin(3 * np.pi * t)\n        x3 = 0.01 * np.cos(5 * np.pi * t)\n        return np.stack([x1, x2, x3], axis=1)\n\n    def perform_pca_analysis(X):\n        \"\"\"\n        Performs PCA on both mean-centered and standardized data, returning the required metrics.\n        \"\"\"\n        n, d = X.shape\n        \n        # --- PCA on Mean-Centered Data ---\n        X_c = X - X.mean(axis=0)\n        S = (X_c.T @ X_c) / (n - 1)\n        \n        # Eigendecomposition. eigh returns sorted eigenvalues (ascending).\n        # We reverse them to get principal components in descending order of variance.\n        evals, evecs = np.linalg.eigh(S)\n        evals = evals[::-1]\n        evecs = evecs[:, ::-1]\n        \n        lambda_1 = evals[0]\n        v1 = evecs[:, 0]\n        \n        # Explained variance ratio\n        total_variance = np.sum(evals)\n        r1 = lambda_1 / total_variance if total_variance > 0 else 0\n        \n        # Dominant feature index\n        i_before = np.argmax(np.abs(v1))\n\n        # --- PCA on Standardized Data ---\n        stds = np.std(X_c, axis=0, ddof=1)\n        \n        # Handle features with zero standard deviation\n        Z = np.zeros_like(X_c)\n        non_zero_std_mask = stds > 0\n        if np.any(non_zero_std_mask):\n            Z[:, non_zero_std_mask] = X_c[:, non_zero_std_mask] / stds[non_zero_std_mask]\n\n        S_z = (Z.T @ Z) / (n - 1)\n        \n        # Eigendecomposition of the correlation matrix\n        mu, u = np.linalg.eigh(S_z)\n        mu = mu[::-1]\n        u = u[:, ::-1]\n        \n        mu_1 = mu[0]\n        u1 = u[:, 0]\n        \n        # Explained variance ratio for standardized data\n        total_variance_z = np.sum(mu)\n        r1_z = mu_1 / total_variance_z if total_variance_z > 0 else 0\n        \n        # Dominant feature index after standardization\n        i_after = np.argmax(np.abs(u1))\n        \n        # Alignment between the first principal components\n        alignment = np.abs(np.dot(v1, u1))\n        \n        return [r1, r1_z, alignment, int(i_before), int(i_after)]\n\n    test_cases = [\n        generate_case_1_data,\n        generate_case_2_data,\n        generate_case_3_data\n    ]\n    \n    all_results = []\n    for case_generator in test_cases:\n        X = case_generator()\n        result = perform_pca_analysis(X)\n        all_results.append(result)\n\n    # Format the output string without spaces as [[...],[...],[...]]\n    inner_strings = [f\"[{','.join(map(str, r))}]\" for r in all_results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "2430028"}, {"introduction": "数据质量是任何数据分析成功的先决条件，而离群值是影响数据质量的一个重要因素。由于 PCA 的目标是最大化方差，因此它对远离数据中心的极端值点（即离群值）尤为敏感。本练习提供了一个简单直观的二维场景，让您亲手量化单个离群值如何能够显著地扭曲主成分的方向，从而强调了在应用 PCA 之前进行数据清洗和离群值检测的必要性 [@problem_id:2430058]。", "problem": "您需要量化一个孤立的远处离群点如何影响一个$2$维数据集中第一主成分的方向。考虑一个由$M$个点组成的确定性基础点云，这些点位于一个以原点为中心的椭圆上，由以下参数集定义\n$$\n\\mathbf{x}_k = \\begin{bmatrix} r_x \\cos\\left(\\tfrac{2\\pi k}{M}\\right) \\\\ r_y \\sin\\left(\\tfrac{2\\pi k}{M}\\right) \\end{bmatrix}, \\quad k = 0,1,2,\\dots,M-1,\n$$\n其中 $r_x \\gt 0$ 且 $r_y \\gt 0$。通过在以下坐标处添加一个额外的点（即离群点）来构成一个增广数据集：\n$$\n\\mathbf{z} = \\begin{bmatrix} o_x \\\\ o_y \\end{bmatrix}.\n$$\n设 $\\mathbf{S}_0$ 为基础点云的样本协方差矩阵，$\\mathbf{S}_1$ 为包含$M$个椭圆点和该离群点的增广数据集的样本协方差矩阵。第一主成分定义为相应样本协方差矩阵的与其最大特征值关联的单位特征向量。用 $\\mathbf{u}_0 \\in \\mathbb{R}^2$ 表示对应于 $\\mathbf{S}_0$ 最大特征值的单位特征向量，用 $\\mathbf{u}_1 \\in \\mathbb{R}^2$ 表示 $\\mathbf{S}_1$ 的类似特征向量。由于特征向量的定义可相差一个符号，因此将两个主方向之间的锐角差 $\\Delta$ 定义为\n$$\n\\Delta = \\arccos\\!\\left(\\left|\\mathbf{u}_0^\\top \\mathbf{u}_1\\right|\\right).\n$$\n您必须以弧度为单位计算 $\\Delta$。所有最终数值答案必须以弧度表示，并四舍五入到$6$位小数。\n\n测试套件。对于下方的每个参数元组 $(r_x, r_y, M, o_x, o_y)$，请按规定构建基础点云和增广数据集，计算 $\\Delta$，并报告结果：\n- 案例$1$（一般情况）：$(r_x, r_y, M, o_x, o_y) = (3.0, 1.0, 60, 10.0, 10.0)$。\n- 案例$2$（边界情况，无影响）：$(r_x, r_y, M, o_x, o_y) = (3.0, 1.0, 60, 0.0, 0.0)$。\n- 案例$3$（极端情况，离群点主导正交方向）：$(r_x, r_y, M, o_x, o_y) = (2.0, 1.0, 40, 0.0, 1000.0)$。\n- 案例$4$（近各向同性基础，中等离群点）：$(r_x, r_y, M, o_x, o_y) = (1.05, 1.0, 50, 5.0, 0.2)$。\n\n您的程序应生成单行输出，其中包含一个以逗号分隔的列表形式的结果，并用方括号括起来，顺序与上述案例相同，每个值四舍五入到$6$位小数。例如，一个有效的输出格式为 `[x_1,x_2,x_3,x_4]`。", "solution": "该问题已经过验证，被认为是有效的。这是一个计算物理学和线性代数中的适定问题，基于已确立的科学原理。所有术语的定义都足够严谨，所提供的数据一致且完整。\n\n任务是计算当引入单个离群点时，二维数据集中第一主成分的角度偏差 $\\Delta$。基础数据集是由$M$个在椭圆上均匀分布的点组成，增广数据集则额外包含一个离群点。\n\n数据集的第一主成分是最大方差的方向，它对应于样本协方差矩阵最大特征值所关联的单位特征向量。设$N$个数据点的集合为 $\\{\\mathbf{p}_i\\}_{i=1}^N$，其中每个 $\\mathbf{p}_i \\in \\mathbb{R}^2$。样本均值为 $\\bar{\\mathbf{p}} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{p}_i$。样本协方差矩阵 $\\mathbf{S}$ 由下式给出：\n$$\n\\mathbf{S} = \\frac{1}{N-1}\\sum_{i=1}^N (\\mathbf{p}_i - \\bar{\\mathbf{p}})(\\mathbf{p}_i - \\bar{\\mathbf{p}})^\\top\n$$\n请注意，分母的选择（无论是$N$还是$N-1$）是无关紧要的，因为它只对协方差矩阵进行缩放，而不会改变其特征向量。数值实现将使用分母为$N-1$的常规无偏估计量。\n\n首先，我们分析由以下公式定义的$M$个点的基础点云 $\\{\\mathbf{x}_k\\}_{k=0}^{M-1}$：\n$$\n\\mathbf{x}_k = \\begin{bmatrix} r_x \\cos\\left(\\frac{2\\pi k}{M}\\right) \\\\ r_y \\sin\\left(\\frac{2\\pi k}{M}\\right) \\end{bmatrix}, \\quad k = 0, 1, \\dots, M-1\n$$\n由于余弦和正弦函数在一个完整周期内的对称性，当$M > 1$时，基础点云的样本均值 $\\bar{\\mathbf{x}}_0$ 是零向量：\n$$\n\\bar{\\mathbf{x}}_0 = \\frac{1}{M}\\sum_{k=0}^{M-1} \\mathbf{x}_k = \\begin{bmatrix} \\frac{r_x}{M}\\sum_{k=0}^{M-1}\\cos(\\frac{2\\pi k}{M}) \\\\ \\frac{r_y}{M}\\sum_{k=0}^{M-1}\\sin(\\frac{2\\pi k}{M}) \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n$$\n那么，基础点云的样本协方差矩阵 $\\mathbf{S}_0$ 为：\n$$\n\\mathbf{S}_0 = \\frac{1}{M-1}\\sum_{k=0}^{M-1} \\mathbf{x}_k \\mathbf{x}_k^\\top = \\frac{1}{M-1} \\sum_{k=0}^{M-1} \\begin{bmatrix} r_x^2 \\cos^2(\\theta_k) & r_x r_y \\cos(\\theta_k)\\sin(\\theta_k) \\\\ r_x r_y \\cos(\\theta_k)\\sin(\\theta_k) & r_y^2 \\sin^2(\\theta_k) \\end{bmatrix}\n$$\n其中 $\\theta_k = \\frac{2\\pi k}{M}$。当 $M > 2$ 时，非对角项的和为零。对角项的和为 $\\sum \\cos^2(\\theta_k) = M/2$ 和 $\\sum \\sin^2(\\theta_k) = M/2$。因此，$\\mathbf{S}_0$ 是一个对角矩阵：\n$$\n\\mathbf{S}_0 = \\frac{M}{2(M-1)} \\begin{bmatrix} r_x^2 & 0 \\\\ 0 & r_y^2 \\end{bmatrix}\n$$\n对角矩阵的特征向量是标准基向量。最大特征值对应于$r_x^2$和$r_y^2$中较大的一个。如果$r_x > r_y$，第一主成分是 $\\mathbf{u}_0 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$。如果$r_y > r_x$，则是 $\\mathbf{u}_0 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$。这与椭圆的半长轴方向一致。\n\n接下来，我们分析增广数据集，它包含基础点云的$M$个点以及离群点 $\\mathbf{z} = \\begin{bmatrix} o_x \\\\ o_y \\end{bmatrix}$。总点数为 $N = M+1$。增广数据集的均值 $\\bar{\\mathbf{x}}_1$ 为：\n$$\n\\bar{\\mathbf{x}}_1 = \\frac{1}{M+1}\\left(\\sum_{k=0}^{M-1}\\mathbf{x}_k + \\mathbf{z}\\right) = \\frac{1}{M+1}\\mathbf{z}\n$$\n协方差矩阵 $\\mathbf{S}_1$ 是基于这$M+1$个点计算的。\n$$\n\\mathbf{S}_1 = \\frac{1}{M} \\left( \\sum_{k=0}^{M-1}(\\mathbf{x}_k - \\bar{\\mathbf{x}}_1)(\\mathbf{x}_k - \\bar{\\mathbf{x}}_1)^\\top + (\\mathbf{z} - \\bar{\\mathbf{x}}_1)(\\mathbf{z} - \\bar{\\mathbf{x}}_1)^\\top \\right)\n$$\n一般而言，$\\mathbf{S}_1$ 不是一个对角矩阵。离群点，特别是当其坐标 $(o_x, o_y)$ 很大时，将显著改变均值并为协方差和贡献一个大项，从而旋转数据分布的主轴。第一主成分 $\\mathbf{u}_1$ 是对应于 $\\mathbf{S}_1$ 最大特征值的单位特征向量。这可以通过对数值计算出的 $\\mathbf{S}_1$ 进行特征分解来找到。\n\n角度差 $\\Delta$ 计算为两个主成分向量 $\\mathbf{u}_0$ 和 $\\mathbf{u}_1$ 之间的锐角：\n$$\n\\Delta = \\arccos(|\\mathbf{u}_0^\\top \\mathbf{u}_1|)\n$$\n点积的绝对值确保了角度为锐角，这考虑到了一个特征向量与其负向量等价的事实。\n\n解决每个测试案例的算法流程如下：\n$1$. 根据 $r_x$ 和 $r_y$ 构建椭圆上的 $M$ 个点的基础数据集。\n$2$. 计算基础数据集的样本协方差矩阵 $\\mathbf{S}_0$。\n$3$. 求 $\\mathbf{S}_0$ 的特征向量和特征值。与最大特征值对应的特征向量 $\\mathbf{u}_0$ 即为第一主成分。\n$4$. 通过将离群点 $\\mathbf{z}=(o_x, o_y)$ 添加到基础数据集中来构建增广数据集。\n$5$. 计算增广数据集的样本协方差矩阵 $\\mathbf{S}_1$。\n$6$. 求 $\\mathbf{S}_1$ 的特征向量和特征值。与最大特征值对应的特征向量 $\\mathbf{u}_1$ 即为新的第一主成分。\n$7$. 以弧度为单位计算角度差 $\\Delta = \\arccos(|\\mathbf{u}_0 \\cdot \\mathbf{u}_1|)$。点积的参数应被裁剪到 $[-1, 1]$ 范围内，以避免数值误差。\n$8$. 将结果四舍五入到$6$位小数。\n为每个提供的参数集实现此流程。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the angular difference in the first principal component of a 2D dataset\n    due to the addition of an outlier.\n    \"\"\"\n    test_cases = [\n        # (r_x, r_y, M, o_x, o_y)\n        (3.0, 1.0, 60, 10.0, 10.0),\n        (3.0, 1.0, 60, 0.0, 0.0),\n        (2.0, 1.0, 40, 0.0, 1000.0),\n        (1.05, 1.0, 50, 5.0, 0.2),\n    ]\n\n    results = []\n    \n    for r_x, r_y, M, o_x, o_y in test_cases:\n        # Step 1: Construct the base dataset\n        theta = (2 * np.pi / M) * np.arange(M)\n        x_coords = r_x * np.cos(theta)\n        y_coords = r_y * np.sin(theta)\n        base_cloud = np.stack((x_coords, y_coords), axis=1)\n\n        # Step 2 & 3: PCA on the base cloud\n        # Covariance matrix for a centered ellipse is diagonal, so we can determine u0 analytically.\n        # This is more robust and faster than numerical computation for this specific geometry.\n        # Although numerical computation would yield the same result.\n        # Let's use numerical computation for generality and to match the full algorithm described.\n        # The choice of divisor (N or N-1) does not affect the eigenvectors.\n        # np.cov uses N-1 by default.\n        cov_0 = np.cov(base_cloud, rowvar=False)\n        eigvals_0, eigvecs_0 = np.linalg.eigh(cov_0)\n        # eigh sorts eigenvalues in ascending order, so the last eigenvector is the principal one.\n        u_0 = eigvecs_0[:, -1]\n\n        # Step 4: Construct the augmented dataset\n        outlier = np.array([[o_x, o_y]])\n        augmented_cloud = np.vstack((base_cloud, outlier))\n\n        # Step 5 & 6: PCA on the augmented dataset\n        cov_1 = np.cov(augmented_cloud, rowvar=False)\n        eigvals_1, eigvecs_1 = np.linalg.eigh(cov_1)\n        u_1 = eigvecs_1[:, -1]\n\n        # Step 7: Compute the angular difference\n        # Dot product between the two unit eigenvectors\n        dot_product = np.dot(u_0, u_1)\n        \n        # Take the absolute value to find the acute angle\n        abs_dot_product = np.abs(dot_product)\n        \n        # Clip to handle potential floating point inaccuracies > 1.0\n        clipped_dot = np.clip(abs_dot_product, -1.0, 1.0)\n        \n        delta = np.arccos(clipped_dot)\n\n        # Step 8: Append rounded result\n        results.append(round(delta, 6))\n\n    # Format the final output string\n    output_str = \"[\" + \",\".join([f\"{res:.6f}\" for res in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "2430058"}]}