{"hands_on_practices": [{"introduction": "为了建立对间接推断核心思想的直观理解，我们从一个物理模型——高尔顿板（或称“柏青哥”弹珠台）——开始。在这个练习中，你将通过观察大量弹珠的最终位置分布，来推断出弹珠在每一层隔板上向左或向右偏转的微观概率 $p$ 和步长 $d$。这个实践完美地诠释了间接推断的精髓：即利用一个简单的辅助模型（在此为正态分布）的统计量，来匹配并估计一个我们无法直接观察其内部机制的复杂结构模型。[@problem_id:2401820]", "id": "2401820", "problem": "考虑一个简化的 Galton 式“plinko”板模型，该模型用于阐释随机过程。大量相同的球从顶部逐一落下。每个球会遇到 $T$ 排钉子。在每个钉子处，球会向右或向左偏转。令 $S_{it}\\in\\{-d, +d\\}$ 表示球 $i$ 在第 $t$ 排的水平增量，其中 $d&gt;0$ 是固定的水平步长。假设 $\\mathbb{P}(S_{it}=+d)=p$ 和 $\\mathbb{P}(S_{it}=-d)=1-p$，其中 $0\\le p\\le 1$，且球与球之间、排与排之间均独立。经过 $T$ 排后，球 $i$ 的最终水平位置为 $X_i=\\sum_{t=1}^{T} S_{it}$。\n\n令 $\\theta=(p,d)$ 为未知的结构参数向量。您观察到一个由 $N_{\\text{obs}}$ 个独立最终位置组成的数据集 $\\{X_i\\}_{i=1}^{N_{\\text{obs}}}$，该数据集由真实参数 $\\theta_0=(p_0,d_0)$ 生成。您的任务是使用间接推断（Indirect Inference, II）方法估计 $\\theta$，该方法定义如下。选择一个辅助模型：正态分布 $\\mathcal{N}(\\mu,\\sigma^2)$，其辅助参数是由最终位置计算出的样本均值和样本方差。将辅助统计量映射 $m(\\cdot)$ 定义为 $m(\\{x_i\\}) = \\big(\\bar{x}, s^2\\big)$，其中 $\\bar{x}$ 是样本均值，$s^2$ 是除数为 $N$（总体方差）的样本方差。对于一个候选 $\\theta$，令 $m_{\\text{sim}}(\\theta)$ 表示在参数为 $\\theta$ 的结构模型下生成的数据所计算出的辅助统计量。II 估计量最小化二次距离\n$$\nQ(\\theta)=\\big(m_{\\text{obs}}-m_{\\text{sim}}(\\theta)\\big)^{\\top} W \\big(m_{\\text{obs}}-m_{\\text{sim}}(\\theta)\\big),\n$$\n其中权重矩阵 $W=I_2$，$m_{\\text{obs}}=m(\\{X_i\\}_{i=1}^{N_{\\text{obs}}})$。\n\n您的任务是编写一个完整的程序，该程序需基于基本原理，对下面指定的每个测试用例执行以下所有操作：\n- 使用给定的真实参数 $\\theta_0$ 和指定的随机种子，通过结构模型生成观测数据集 $\\{X_i\\}_{i=1}^{N_{\\text{obs}}}$。\n- 计算观测辅助统计量 $m_{\\text{obs}}$。\n- 计算间接推断估计值 $\\hat{\\theta}=(\\hat{p},\\hat{d})$，作为 $Q(\\theta)$ 在可行集 $\\{(p,d): 0\\le p\\le 1, \\ d&gt;0\\}$ 上的最小化子。\n- 按照最终输出格式的要求，为每个测试用例返回 $\\hat{\\theta}$。\n\n所有概率必须表示为 $[0,1]$ 区间内的小数。不涉及任何物理单位。所有角度（若有）均与本问题无关。估计必须严格按照上述定义进行。\n\n测试套件。对于每个项目，您将获得行数 $T$、真实概率 $p_0$、真实步长 $d_0$、观测样本量 $N_{\\text{obs}}$ 以及用于生成观测数据的随机种子。生成观测数据时，必须使用提供的种子初始化随机数生成器。最终的样本方差必须使用除数 $N_{\\text{obs}}$（总体方差）。\n- 测试用例 1：$T=20$，$p_0=0.6$，$d_0=1.2$，$N_{\\text{obs}}=50000$，种子 $=13579$。\n- 测试用例 2：$T=30$，$p_0=0.5$，$d_0=1.0$，$N_{\\text{obs}}=50000$，种子 $=24680$。\n- 测试用例 3：$T=25$，$p_0=0.9$，$d_0=0.8$，$N_{\\text{obs}}=50000$，种子 $=11235$。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个列表的列表形式的结果，每个内部列表对应一个测试用例，顺序与上文所列一致。每个内部列表必须包含两个小数 $[\\hat{p},\\hat{d}]$，四舍五入到小数点后恰好六位。例如，输出行必须如下所示：\n[[p1,d1],[p2,d2],[p3,d3]]\n其中不插入任何空格，每个 $p_j$ 和 $d_j$ 都是四舍五入到六位小数的十进制数。", "solution": "所提出的问题是一个有效且适定的参数估计练习，该练习使用间接推断方法。它在科学上基于概率论和统计估计，且解决该问题所需的所有信息均已提供。我们将给出一个完整的解法。\n\n问题的核心是估计一个随机过程的结构参数 $\\theta=(p,d)$，给定该过程最终状态的一组观测值。结构模型描述了一个在 Galton 板上运动的小球，该板有 $T$ 排。在每一排 $t \\in \\{1, \\dots, T\\}$，小球的水平位置发生增量 $S_t$，其以概率 $p$ 取值为 $+d$，以概率 $1-p$ 取值为 $-d$。经过 $T$ 排后的最终水平位置为 $X = \\sum_{t=1}^{T} S_t$。\n\n首先，我们建立结构参数 $\\theta=(p,d)$ 与最终位置 $X$ 的矩之间的解析关系。令 $K$ 为一个随机变量，表示向右偏转的次数，它服从二项分布，$K \\sim \\text{Binomial}(T,p)$。总偏转次数为 $T$，因此有 $T-K$ 次向左偏转。最终位置 $X$ 可以表示为：\n$$\nX = K \\cdot (+d) + (T-K) \\cdot (-d) = d(K - (T-K)) = d(2K - T)\n$$\n二项变量 $K$ 的均值和方差分别为 $\\mathbb{E}[K] = Tp$ 和 $\\text{Var}(K) = Tp(1-p)$。利用期望和方差的性质，我们可以求出最终位置 $X$ 的均值 $\\mu_X$ 和方差 $\\sigma_X^2$：\n$$\n\\mu(p, d) = \\mathbb{E}[X] = \\mathbb{E}[d(2K - T)] = d(2\\mathbb{E}[K] - T) = d(2Tp - T) = Td(2p-1)\n$$\n$$\n\\sigma^2(p, d) = \\text{Var}(X) = \\text{Var}(d(2K - T)) = d^2 \\text{Var}(2K) = 4d^2\\text{Var}(K) = 4d^2Tp(1-p)\n$$\n这两个方程构成了将结构参数 $\\theta=(p,d)$ 映射到可观测数据理论矩的约束函数。\n\n问题要求通过间接推断（II）进行估计。给定一个辅助模型，即正态分布 $\\mathcal{N}(\\mu, \\sigma^2)$，其辅助统计量为样本均值 $\\bar{x}$ 和样本方差 $s^2$（除数为 $N_{\\text{obs}}$）。我们有一组由真实参数 $\\theta_0=(p_0,d_0)$ 生成的 $N_{\\text{obs}}$ 个观测值 $\\{X_i\\}_{i=1}^{N_{\\text{obs}}}$。我们计算观测到的辅助统计量向量 $m_{\\text{obs}} = (\\bar{x}_{\\text{obs}}, s^2_{\\text{obs}})$。\n\nII 估计量 $\\hat{\\theta}$ 通过最小化目标函数得到：\n$$\nQ(\\theta) = (m_{\\text{obs}} - m_{\\text{sim}}(\\theta))^{\\top} W (m_{\\text{obs}} - m_{\\text{sim}}(\\theta))\n$$\n其中权重矩阵为单位矩阵，$W=I_2$。项 $m_{\\text{sim}}(\\theta)$ 表示由参数为 $\\theta$ 的模型产生的辅助统计量。一个关键点是如何评估 $m_{\\text{sim}}(\\theta)$。虽然可以在优化过程中为每个 $\\theta$ 值进行模拟，但这会给目标函数引入随机性，使最小化过程变得复杂，并且需要指定一个未提供的内部模拟大小。在这种情况下，正确的标准方法是使用上面推导出的解析矩，这对应于当模拟样本量趋于无穷时辅助统计量的期望值。\n因此，$m_{\\text{sim}}(\\theta) = (\\mu(p,d), \\sigma^2(p,d))$。目标函数变为：\n$$\nQ(p,d) = (\\bar{x}_{\\text{obs}} - \\mu(p,d))^2 + (s^2_{\\text{obs}} - \\sigma^2(p,d))^2\n$$\n$$\nQ(p,d) = (\\bar{x}_{\\text{obs}} - Td(2p-1))^2 + (s^2_{\\text{obs}} - 4Td^2p(1-p))^2\n$$\n估计任务是找到使函数 $Q(p,d)$ 最小化的值 $(\\hat{p}, \\hat{d})$，并满足约束条件 $p \\in [0,1]$ 和 $d > 0$。这是一个标准的非线性约束优化问题。\n\n解决每个测试用例的算法流程如下：\n1. **生成观测数据**：对于给定的测试用例，其参数为 $T$、$p_0$、$d_0$、$N_{\\text{obs}}$ 和一个特定的随机种子，生成 $N_{\\text{obs}}$ 个观测值。这可以通过首先从 $\\text{Binomial}(T, p_0)$ 分布中抽样 $N_{\\text{obs}}$ 个值 $\\{K_i\\}_{i=1}^{N_{\\text{obs}}}$ 来高效实现。然后，观测到的最终位置计算为 $X_i = d_0(2K_i - T)$。\n2. **计算观测统计量**：从生成的数据集 $\\{X_i\\}$ 中，计算观测辅助统计量 $m_{\\text{obs}}=(\\bar{x}_{\\text{obs}}, s^2_{\\text{obs}})$。具体来说，$\\bar{x}_{\\text{obs}} = \\frac{1}{N_{\\text{obs}}} \\sum_{i=1}^{N_{\\text{obs}}} X_i$ 且 $s^2_{\\text{obs}} = \\frac{1}{N_{\\text{obs}}} \\sum_{i=1}^{N_{\\text{obs}}} (X_i - \\bar{x}_{\\text{obs}})^2$。\n3. **数值优化**：关于 $p$ 和 $d$ 最小化目标函数 $Q(p,d)$。我们采用适合约束问题的数值优化算法，例如带箱型约束的拟牛顿法（如 L-BFGS-B）。搜索空间由边界 $p \\in [0,1]$ 和 $d \\in (0, \\infty)$ 定义。为了数值稳定性，对 $d$ 使用一个小的正数下界。\n4. **报告估计值**：最小化 $Q(p,d)$ 的值对 $(\\hat{p}, \\hat{d})$ 构成了给定测试用例的间接推断估计。然后收集所有测试用例的结果，并按要求格式化。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to solve the Indirect Inference estimation problem for all test cases.\n    \"\"\"\n\n    # Test cases as specified in the problem statement.\n    test_cases = [\n        # (T, p0, d0, N_obs, seed)\n        (20, 0.6, 1.2, 50000, 13579),\n        (30, 0.5, 1.0, 50000, 24680),\n        (25, 0.9, 0.8, 50000, 11235),\n    ]\n\n    all_results = []\n\n    for T, p0, d0, N_obs, seed in test_cases:\n        # Step 1: Generate the observed dataset {X_i}\n        # Use a more efficient method based on the Binomial distribution.\n        # k ~ Binomial(T, p) represents the number of right steps.\n        # X = k * (+d) + (T - k) * (-d) = d * (2k - T)\n        rng = np.random.default_rng(seed)\n        k_obs = rng.binomial(T, p0, size=N_obs)\n        x_obs = d0 * (2 * k_obs - T)\n\n        # Step 2: Compute the observed auxiliary statistics m_obs = (mean, variance)\n        mu_obs = np.mean(x_obs)\n        # Use population variance (divisor N), as specified (ddof=0 is default for np.var)\n        var_obs = np.var(x_obs)\n        m_obs = (mu_obs, var_obs)\n        \n        # Step 3: Define the objective function Q(theta) for minimization.\n        # theta is a tuple (p, d).\n        def objective_function(theta, T_val, m_obs_val):\n            p, d = theta\n            mu_obs_val, var_obs_val = m_obs_val\n\n            # Analytical moments from the structural model\n            mu_sim = T_val * d * (2 * p - 1)\n            var_sim = 4 * T_val * (d**2) * p * (1 - p)\n            \n            # Quadratic objective function Q(theta)\n            q_val = (mu_obs_val - mu_sim)**2 + (var_obs_val - var_sim)**2\n            return q_val\n\n        # Step 4: Perform numerical optimization to find the II estimate.\n        # Initial guess for the parameters (p, d)\n        initial_guess = [0.5, 1.0]\n\n        # Bounds for the parameters: 0 <= p <= 1 and d > 0.\n        # Use a small positive number for the lower bound of d for numerical stability.\n        bounds = [(0.0, 1.0), (1e-9, None)]\n\n        # Minimize the objective function. L-BFGS-B is suitable for box constraints.\n        result = minimize(\n            fun=objective_function,\n            x0=initial_guess,\n            args=(T, m_obs),\n            method='L-BFGS-B',\n            bounds=bounds\n        )\n\n        # The optimized parameters are the II estimates\n        p_hat, d_hat = result.x\n        all_results.append([p_hat, d_hat])\n\n    # Final print statement in the exact required format.\n    # Create the list of lists with rounded values.\n    # e.g., [[0.600012, 1.199998], [0.500001, 1.000003], [0.900005, 0.799989]]\n    # Then convert to string and remove spaces to match the output format.\n    final_list_formatted = [[round(p, 6), round(d, 6)] for p, d in all_results]\n    output_str = str(final_list_formatted).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n\n```"}, {"introduction": "在掌握了基本概念后，我们将把间接推断应用于经济学中更常见的场景：时间序列分析。此练习要求你为一个自回归模型（AR(1)）估计参数 $\\theta$，但其中的复杂之处在于，辅助模型本身的参数是通过广义矩估计（GMM）得到的。这个案例不仅让你实践一个更真实的计量经济学工作流程，还引入了“共同随机数”这一至关重要的模拟技巧，以提高估计的稳定性和效率。[@problem_id:2401827]", "id": "2401827", "problem": "给定一个结构时间序列模型和一个通过广义矩估计 (GMM) 估计参数的辅助模型。您的任务是实现一个间接推断估计量，其中辅助参数是通过 GMM 而非最大似然估计 (MLE) 获得的，并为指定的一套数据生成过程和模拟设置生成间接推断估计值。\n\n结构模型为一阶自回归模型，其创新方差已知为1。对于每个时间指数 $t$，该过程满足\n$$\ny_t = \\theta \\, y_{t-1} + \\varepsilon_t,\n$$\n其中 $\\varepsilon_t \\sim \\mathcal{N}(0,1)$ 是独立同分布的。在收集 $T$ 个观测值之前，使用初始条件 $y_0 = 0$ 和 $B = 200$ 个周期的预烧期。对于观测样本路径和辅助样本路径的模拟，角度单位不适用，也不涉及任何物理单位。\n\n辅助模型是线性回归\n$$\ny_t = \\beta_0 + \\beta_1 \\, y_{t-1} + u_t,\n$$\n辅助参数 $\\beta = (\\beta_0,\\beta_1)^\\top$ 是通过基于工具变量向量的过度识别矩条件定义的\n$$\nz_t = \\begin{bmatrix} 1 \\\\ y_{t-1} \\\\ y_{t-2} \\end{bmatrix}.\n$$\n对于每个样本 $y_1,\\dots,y_T$，将 $t=3,\\dots,T$ 的样本矩函数定义为\n$$\ng_T(\\beta) = \\frac{1}{n} \\sum_{t=3}^T z_t \\, \\left(y_t - \\beta_0 - \\beta_1 \\, y_{t-1}\\right),\n$$\n其中 $n = T-2$。使用维度为 $3 \\times 3$ 的单位权重矩阵，$\\beta$ 的 GMM 估计量定义为\n$$\n\\widehat{\\beta}(y_{1:T}) = \\arg \\min_{\\beta \\in \\mathbb{R}^2} \\; g_T(\\beta)^\\top g_T(\\beta).\n$$\n\n针对候选结构参数 $\\theta$ 的间接推断目标函数使用了从结构模型生成的 $S$ 个长度为 $T$ 的独立模拟样本，并采用相同的预烧期 $B$。令 $\\widehat{\\beta}^{\\text{obs}}$ 表示从观测数据集获得的辅助GMM估计值。对于给定的 $\\theta$，定义\n$$\n\\overline{\\beta}(\\theta) = \\frac{1}{S} \\sum_{s=1}^S \\widehat{\\beta}(y^{(s)}_{1:T}(\\theta)),\n$$\n其中 $y^{(s)}_{1:T}(\\theta)$ 是在参数 $\\theta$ 下生成的第 $s$ 个长度为 $T$ 的模拟样本。间接推断准则为\n$$\nQ(\\theta) = \\left(\\widehat{\\beta}^{\\text{obs}} - \\overline{\\beta}(\\theta)\\right)^\\top W \\left(\\widehat{\\beta}^{\\text{obs}} - \\overline{\\beta}(\\theta)\\right),\n$$\n其中 $W = I_2$。对于下述每个测试用例，将 $\\theta$ 限制在离散搜索网格上\n$$\n\\mathcal{G}_i = \\{-0.95, -0.95 + \\delta_i, \\dots, 0.95\\},\n$$\n其中 $\\delta_i$ 是测试用例中指定的网格步长。间接推断估计值为\n$$\n\\widehat{\\theta}_i \\in \\arg \\min_{\\theta \\in \\mathcal{G}_i} Q(\\theta),\n$$\n若有多个解，则选择最小的 $\\theta$。\n\n模拟和可复现性要求如下：\n- 对于每个测试用例 $i$，使用真实参数 $\\theta^{\\star}_i$、样本大小 $T_i$、等于 $1$ 的创新方差、 $B=200$ 的预烧期以及指定的观测数据种子 $s^{\\text{obs}}_i$ 来生成观测数据集。\n- 为了在测试用例 $i$ 的网格上评估 $Q(\\theta)$，通过为每个重复索引 $s \\in \\{1,\\dots,S_i\\}$ 使用种子 $s^{\\text{sim}}_i + s$ 预先生成一个长度为 $T_i + B$ 的标准正态创新序列，从而在不同的 $\\theta$ 值之间使用公共随机数。对于所有 $\\theta \\in \\mathcal{G}_i$，重复使用这些预先生成的创新。\n\n实现上述过程并为以下四个测试用例计算 $\\widehat{\\theta}_i$：\n- 测试用例 1：$T_1 = 300$, $\\theta^{\\star}_1 = 0.6$, $S_1 = 20$, $\\delta_1 = 0.01$, $s^{\\text{obs}}_1 = 1729$, $s^{\\text{sim}}_1 = 20231$。\n- 测试用例 2：$T_2 = 300$, $\\theta^{\\star}_2 = 0.9$, $S_2 = 25$, $\\delta_2 = 0.01$, $s^{\\text{obs}}_2 = 1730$, $s^{\\text{sim}}_2 = 20232$。\n- 测试用例 3：$T_3 = 150$, $\\theta^{\\star}_3 = 0.0$, $S_3 = 40$, $\\delta_3 = 0.02$, $s^{\\text{obs}}_3 = 1731$, $s^{\\text{sim}}_3 = 20233$。\n- 测试用例 4：$T_4 = 300$, $\\theta^{\\star}_4 = -0.5$, $S_4 = 20$, $\\delta_4 = 0.01$, $s^{\\text{obs}}_4 = 1732$, $s^{\\text{sim}}_4 = 20234$。\n\n您的程序应生成单行输出，其中包含四个估计值 $\\widehat{\\theta}_1, \\widehat{\\theta}_2, \\widehat{\\theta}_3, \\widehat{\\theta}_4$，格式为逗号分隔的列表并用方括号括起来，每个值四舍五入到小数点后四位，例如 $[\\widehat{\\theta}_1,\\widehat{\\theta}_2,\\widehat{\\theta}_3,\\widehat{\\theta}_4]$。", "solution": "该问题要求为一阶自回归过程 AR(1) 的参数 $\\theta$ 实现一个间接推断估计量。该估计过程基于一个辅助模型，其参数通过广义矩估计 (GMM) 进行估计。首先评估问题陈述的有效性。\n\n### 第 1 步：提取给定条件\n\n- **结构模型**：$y_t = \\theta \\, y_{t-1} + \\varepsilon_t$，其中 $\\varepsilon_t \\sim \\mathcal{N}(0,1)$ 是 i.i.d. 创新。\n- **初始条件**：$y_0 = 0$。\n- **模拟参数**：预烧期 $B = 200$。预烧后收集的样本大小为 $T$ 个观测值。\n- **辅助模型**：$y_t = \\beta_0 + \\beta_1 \\, y_{t-1} + u_t$，参数为 $\\beta = (\\beta_0,\\beta_1)^\\top$。\n- **工具变量向量**：$z_t = [1, y_{t-1}, y_{t-2}]^\\top$。\n- **GMM 样本矩函数**：$g_T(\\beta) = \\frac{1}{n} \\sum_{t=3}^T z_t \\, (y_t - \\beta_0 - \\beta_1 y_{t-1})$，其中 $n = T-2$。\n- **GMM 估计量**：$\\widehat{\\beta}(y_{1:T}) = \\arg \\min_{\\beta \\in \\mathbb{R}^2} \\; g_T(\\beta)^\\top g_T(\\beta)$。这对应于使用单位权重矩阵。\n- **间接推断准则**：$Q(\\theta) = (\\widehat{\\beta}^{\\text{obs}} - \\overline{\\beta}(\\theta))^\\top W (\\widehat{\\beta}^{\\text{obs}} - \\overline{\\beta}(\\theta))$，权重矩阵 $W = I_2$。\n- **模拟的辅助参数**：$\\overline{\\beta}(\\theta) = \\frac{1}{S} \\sum_{s=1}^S \\widehat{\\beta}(y^{(s)}_{1:T}(\\theta))$，其中 $y^{(s)}_{1:T}(\\theta)$ 是从参数为 $\\theta$ 的结构模型生成的 $S$ 个独立样本。\n- **估计网格**：$\\mathcal{G}_i = \\{-0.95, -0.95 + \\delta_i, \\dots, 0.95\\}$。\n- **间接推断估计量**：$\\widehat{\\theta}_i \\in \\arg \\min_{\\theta \\in \\mathcal{G}_i} Q(\\theta)$，若有多个解，则选择最小的 $\\theta$。\n- **可复现性**：评估不同 $\\theta$ 值的 $Q(\\theta)$ 时需使用公共随机数。对于测试用例 $i$，使用种子 $s^{\\text{obs}}_i$ 生成观测数据。使用种子 $s^{\\text{sim}}_i + 1, \\dots, s^{\\text{sim}}_i + S_i$ 生成 $S_i$ 条模拟路径。\n- **测试用例**：\n    1.  $T_1 = 300$，$\\theta^{\\star}_1 = 0.6$， $S_1 = 20$，$\\delta_1 = 0.01$，$s^{\\text{obs}}_1 = 1729$，$s^{\\text{sim}}_1 = 20231$。\n    2.  $T_2 = 300$，$\\theta^{\\star}_2 = 0.9$， $S_2 = 25$，$\\delta_2 = 0.01$，$s^{\\text{obs}}_2 = 1730$，$s^{\\text{sim}}_2 = 20232$。\n    3.  $T_3 = 150$，$\\theta^{\\star}_3 = 0.0$， $S_3 = 40$，$\\delta_3 = 0.02$，$s^{\\text{obs}}_3 = 1731$，$s^{\\text{sim}}_3 = 20233$。\n    4.  $T_4 = 300$，$\\theta^{\\star}_4 = -0.5$，$S_4 = 20$，$\\delta_4 = 0.01$，$s^{\\text{obs}}_4 = 1732$，$s^{\\text{sim}}_4 = 20234$。\n\n### 第 2 步：使用提取的给定条件进行验证\n\n- **科学依据**：该问题在计量经济学和统计学原理方面有坚实的基础。AR(1) 模型、GMM 估计和间接推断都是标准且广泛使用的方法论。\n- **良定性**：该问题是良定的。目标函数 $Q(\\theta)$ 定义清晰，其在离散网格上的最小化保证了解的存在性。指定的平局打破规则确保了解的唯一性。\n- **客观性**：问题以精确、客观的数学语言陈述，没有任何主观或模棱两可的术语。\n- **完整性与一致性**：所有必需的模型、参数、种子和程序步骤都已明确提供。整个设置内部一致，没有矛盾。GMM 使用单位权重矩阵是一种有效（尽管可能效率不高）的选择，它能导出一个明确定义的估计量。\n\n### 第 3 步：结论与行动\n\n问题陈述是有效的，具有科学合理性、良定性和自洽性。将提供一个合理的解决方案。\n\n### 解决方案的原理性设计\n\n解决方案是通过实现指定的间接推断程序来构建的。这涉及几个不同的逻辑步骤：数据模拟、辅助参数估计、间接推断准则的评估以及对结构参数的网格搜索。\n\n1.  **辅助参数的解析 GMM 估计量**\n    辅助参数 $\\beta = (\\beta_0, \\beta_1)^\\top$ 是通过最小化 $J(\\beta) = g_T(\\beta)^\\top g_T(\\beta)$ 来估计的。令样本表示为 $y_1, \\dots, y_T$。矩条件对 $t=3, \\dots, T$ 进行评估，共包含 $n=T-2$ 个观测值。我们可以用矩阵形式表示该问题。令\n    $$\n    y_{\\text{vec}} = \\begin{bmatrix} y_3 \\\\ y_4 \\\\ \\vdots \\\\ y_T \\end{bmatrix}, \\quad\n    X = \\begin{bmatrix} 1 & y_2 \\\\ 1 & y_3 \\\\ \\vdots & \\vdots \\\\ 1 & y_{T-1} \\end{bmatrix}, \\quad\n    Z = \\begin{bmatrix} 1 & y_2 & y_1 \\\\ 1 & y_3 & y_2 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & y_{T-1} & y_{T-2} \\end{bmatrix}.\n    $$\n    忽略常数因子 $1/n^2$，GMM 目标函数是最小化 $(Z'y_{\\text{vec}} - Z'X\\beta)^\\top (Z'y_{\\text{vec}} - Z'X\\beta)$。这是一个形如 $\\min_{\\beta} \\|b - A\\beta \\|_2^2$ 的线性最小二乘问题，其中 $A = Z'X$，$b = Z'y_{\\text{vec}}$。解由正规方程给出：$\\beta = (A^\\top A)^{-1} A^\\top b$。代入回去，GMM 估计量为\n    $$\n    \\widehat{\\beta} = \\left( (X'Z)(Z'X) \\right)^{-1} (X'Z)(Z'y_{\\text{vec}}).\n    $$\n    对于非共线数据，此解析公式计算效率高且数值稳定，并将用于估计 $\\widehat{\\beta}$。\n\n2.  **从结构模型进行数据模拟**\n    需要一个函数从结构模型 $y_t = \\theta y_{t-1} + \\varepsilon_t$ 生成时间序列数据。生成单个长度为 $T$ 的样本的步骤如下：\n    - 初始化一个长度为 $B+T+1$ 的路径，其中 $y_0 = 0$。\n    - 使用一个预先生成的长度为 $B+T$ 的 i.i.d. 标准正态创新序列 $\\varepsilon_1, \\dots, \\varepsilon_{B+T}$。\n    - 从 $t=1$ 迭代到 $B+T$：$y_t = \\theta y_{t-1} + \\varepsilon_t$。\n    - 最终样本由最后 $T$ 个观测值组成，即 $\\{y_{B+1}, \\dots, y_{B+T}\\}$，有效地丢弃了最初的 $B$ 个预烧期和 $y_0$。\n\n3.  **间接推断过程**\n    问题的核心是针对每个测试用例的四步过程：\n    - **步骤 A：“观测”数据和参数**。对于给定的测试用例 $i$，其真实参数为 $\\theta^\\star_i$，样本大小为 $T_i$，种子为 $s^{\\text{obs}}_i$，生成一个“观测”时间序列 $y^{\\text{obs}}$。然后对该序列应用 GMM 估计量以获得观测的辅助参数向量 $\\widehat{\\beta}^{\\text{obs}}$。\n    - **步骤 B：生成公共随机数**。对于模拟部分，使用种子 $s^{\\text{sim}}_i + 1, \\dots, s^{\\text{sim}}_i + S_i$ 预先生成 $S_i$ 组 i.i.d. 标准正态创新，每组长度为 $B+T_i$。在评估网格 $\\mathcal{G}_i$ 中所有候选 $\\theta$ 值的 $Q(\\theta)$ 时，这些创新组保持固定（公共）。这是一种方差缩减技术，可以提高估计量的稳定性。\n    - **步骤 C：评估准则函数 $Q(\\theta)$**。对于网格 $\\mathcal{G}_i$ 中的每个候选 $\\theta$，执行以下操作：\n        - 使用参数为 $\\theta$ 的结构模型和 $S_i$ 组预生成的创新来模拟 $S_i$ 个时间序列。\n        - 对于 $S_i$ 个模拟序列中的每一个，使用解析GMM公式估计辅助参数向量 $\\widehat{\\beta}^{(s)}(\\theta)$。\n        - 计算平均辅助参数向量 $\\overline{\\beta}(\\theta) = \\frac{1}{S_i} \\sum_{s=1}^{S_i} \\widehat{\\beta}^{(s)}(\\theta)$。\n        - 将间接推断准则计算为欧几里得距离的平方：$Q(\\theta) = \\|\\widehat{\\beta}^{\\text{obs}} - \\overline{\\beta}(\\theta)\\|_2^2$。\n    - **步骤 D：网格搜索**。为每个 $\\theta \\in \\mathcal{G}_i$ 计算 $Q(\\theta)$ 的值。估计值 $\\widehat{\\theta}_i$ 是使 $Q(\\theta)$ 最小化的 $\\theta$ 值。指定的平局打破规则（选择最小的 $\\theta$）通过按升序遍历网格，并且仅在找到严格更小的 $Q(\\theta)$ 值时才更新最小值，可以自然地处理。\n\n4.  **实现**\n    将使用 Python 中的 `numpy` 库进行数值运算来开发一个实现。一个主 `solve` 函数将协调整个过程。将为以下任务创建辅助函数：\n    - 模拟 AR(1) 过程 (`generate_ar1`)。\n    - 估计 GMM 参数 (`gmm_estimator`)。\n    - 为单个测试用例运行整个过程 (`run_test_case`)。\n    主函数将遍历四个指定的测试用例，调用用例运行程序，收集估计的 $\\widehat{\\theta}_i$，并按规定格式化最终输出。", "answer": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef generate_ar1(theta, T, B, innovations):\n    \"\"\"\n    Generates a time series from an AR(1) model.\n    \n    Args:\n        theta (float): The AR(1) parameter.\n        T (int): The number of observations in the final sample.\n        B (int): The number of burn-in periods.\n        innovations (np.ndarray): A vector of T+B standard normal innovations.\n\n    Returns:\n        np.ndarray: The simulated time series of length T.\n    \"\"\"\n    path_len = T + B\n    path = np.zeros(path_len + 1)  # path[0] is y_0\n    \n    for t in range(path_len):\n        path[t + 1] = theta * path[t] + innovations[t]\n        \n    return path[B + 1:] # Returns y_{B+1}, ..., y_{B+T} (length T)\n\ndef gmm_estimator(y):\n    \"\"\"\n    Computes the GMM estimator for the auxiliary model.\n    y_t = beta_0 + beta_1 * y_{t-1} + u_t\n    z_t = [1, y_{t-1}, y_{t-2}]\n    \n    Args:\n        y (np.ndarray): The time series data (y_1, ..., y_T), length T.\n        \n    Returns:\n        np.ndarray: The GMM estimate [beta_0_hat, beta_1_hat].\n    \"\"\"\n    T = len(y)\n    n = T - 2 # Number of observations for moment conditions (t=3 to T)\n    \n    if n <= 0:\n        raise ValueError(\"Time series is too short for GMM estimation.\")\n\n    # y_vec corresponds to y_t for t in {3..T}\n    y_vec = y[2:T] # shape (n,)\n    \n    # X corresponds to [1, y_{t-1}] for t in {3..T}\n    X = np.zeros((n, 2))\n    X[:, 0] = 1.0\n    X[:, 1] = y[1:T-1] # y_2, ..., y_{T-1}\n    \n    # Z corresponds to [1, y_{t-1}, y_{t-2}] for t in {3..T}\n    Z = np.zeros((n, 3))\n    Z[:, 0] = 1.0\n    Z[:, 1] = y[1:T-1] # y_2, ..., y_{T-1}\n    Z[:, 2] = y[0:T-2] # y_1, ..., y_{T-2}\n    \n    # Analytical solution for GMM with W = I:\n    # beta_hat = ( (X'Z Z'X)^-1 ) * ( X'Z Z'y )\n    XT_Z = X.T @ Z\n    ZT_X = Z.T @ X\n    ZT_y = Z.T @ y_vec\n    \n    # Matrix to be inverted\n    M = XT_Z @ ZT_X\n    \n    try:\n        M_inv = np.linalg.inv(M)\n    except np.linalg.LinAlgError:\n        # Fallback to pseudo-inverse if matrix is singular\n        M_inv = np.linalg.pinv(M)\n        \n    beta_hat = M_inv @ (XT_Z @ ZT_y)\n    \n    return beta_hat\n\ndef run_test_case(params):\n    \"\"\"\n    Performs the full indirect inference estimation for one test case.\n    \n    Args:\n        params (dict): A dictionary with all parameters for the test case.\n        \n    Returns:\n        float: The indirect inference estimate of theta.\n    \"\"\"\n    T, theta_star, S, delta, s_obs, s_sim = params.values()\n    B = 200\n\n    # 1. Generate \"observed\" data and estimate beta_obs\n    rng_obs = np.random.default_rng(s_obs)\n    innovs_obs = rng_obs.standard_normal(T + B)\n    y_obs = generate_ar1(theta_star, T, B, innovs_obs)\n    beta_obs = gmm_estimator(y_obs)\n\n    # 2. Pre-generate common random numbers for simulations\n    sim_innovations = []\n    for s in range(1, S + 1):\n        rng_sim = np.random.default_rng(s_sim + s)\n        sim_innovations.append(rng_sim.standard_normal(T + B))\n\n    # 3. Grid search for theta\n    grid_start = -0.95\n    grid_end = 0.95\n    num_points = int(round((grid_end - grid_start) / delta)) + 1\n    theta_grid = np.linspace(grid_start, grid_end, num_points)\n\n    min_Q = np.inf\n    best_theta = None\n\n    for theta_candidate in theta_grid:\n        beta_sim_list = []\n        for s in range(S):\n            y_sim = generate_ar1(theta_candidate, T, B, sim_innovations[s])\n            beta_sim = gmm_estimator(y_sim)\n            beta_sim_list.append(beta_sim)\n        \n        beta_bar = np.mean(beta_sim_list, axis=0)\n        \n        Q = np.sum((beta_obs - beta_bar)**2)\n        \n        # Tie-breaking: select the smallest theta, so only update on strictly smaller Q\n        if Q < min_Q:\n            min_Q = Q\n            best_theta = theta_candidate\n            \n    return best_theta\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {'T': 300, 'theta_star': 0.6, 'S': 20, 'delta': 0.01, 's_obs': 1729, 's_sim': 20231},\n        {'T': 300, 'theta_star': 0.9, 'S': 25, 'delta': 0.01, 's_obs': 1730, 's_sim': 20232},\n        {'T': 150, 'theta_star': 0.0, 'S': 40, 'delta': 0.02, 's_obs': 1731, 's_sim': 20233},\n        {'T': 300, 'theta_star': -0.5, 'S': 20, 'delta': 0.01, 's_obs': 1732, 's_sim': 20234},\n    ]\n\n    results = []\n    for case in test_cases:\n        estimated_theta = run_test_case(case)\n        results.append(estimated_theta)\n\n    print(f\"[{','.join(f'{x:.4f}' for x in results)}]\")\n\nsolve()\n```"}, {"introduction": "最后，我们将挑战一个前沿应用，以展示间接推断的强大功能和灵活性。在这个练习中，你将面对一个混沌动力系统——逻辑斯蒂映射（logistic map）——并尝试估计其关键参数 $r$。由于这类“黑箱”模型的似然函数通常是不可解的，直接的最大似然估计变得不可能，而间接推断为此类问题提供了优雅的解决方案：只要你能模拟一个模型，你就能尝试估计它。[@problem_id:2401774]", "id": "2401774", "problem": "构建一个独立的程序，实现一个基于间接推断 (Indirect Inference, II) 的估计器，以恢复一个混沌离散时间模型的结构参数。真实数据生成过程 (DGP) 由具有单一未知参数的 logistic 映射定义。状态方程为\n$$\nx_{t+1} = r \\, x_t \\, (1 - x_t), \\quad t = 0,1,2,\\dots,T-1,\n$$\n其中初始条件 $x_0 \\in (0,1)$ 为固定已知值，观测方程为\n$$\ny_t = x_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2), \\quad t = 0,1,\\dots,T-1,\n$$\n其中 $\\varepsilon_t$ 是独立同分布的高斯扰动，其已知标准差为 $\\sigma > 0$（也可能 $\\sigma=0$）。唯一的未知结构参数是 $r \\in \\mathbb{R}$。目标是通过匹配观测数据计算出的辅助统计量与结构模型模拟数据计算出的辅助统计量，使用间接推断 (II) 来估计 $r$。\n\n将辅助模型定义为带截距的一阶自回归模型 $AR(1)$：\n$$\ny_t = a_0 + a_1 \\, y_{t-1} + u_t, \\quad t=1,2,\\dots,T-1,\n$$\n其中 $u_t$ 是均值为零的残差。对于任意序列 $\\{y_t\\}_{t=0}^{T-1}$，定义辅助统计向量为\n$$\n\\hat{b}(y) = \\big(\\hat{a}_0(y), \\hat{a}_1(y), \\hat{s}_u(y)\\big),\n$$\n其中 $\\hat{a}_0(y)$ 和 $\\hat{a}_1(y)$ 是 $ a_0 $ 和 $ a_1 $ 的普通最小二乘 (OLS) 估计量，且\n$$\n\\hat{s}_u(y) = \\sqrt{\\frac{1}{T-1}\\sum_{t=1}^{T-1} \\hat{u}_t(y)^2},\n$$\n其中 $\\hat{u}_t(y)$ 是 OLS 残差。$r$ 的间接推断估计量定义为观测数据计算的辅助统计量与使用候选参数值的结构模型生成的模拟数据计算的辅助统计量之间二次距离的最小值点。设 $K \\in \\mathbb{N}$ 是用于求平均的独立模拟数据集的数量。对于一个候选参数 $r$，使用相同的样本量 $T$、相同的初始条件 $x_0$ 和具有相同已知 $\\sigma$ 的高斯观测噪声，定义 $K$ 个模拟数据集 $\\{y^{(k)}(r)\\}_{k=1}^K$。将平均模拟辅助统计量定义为\n$$\n\\bar{b}(r) = \\frac{1}{K} \\sum_{k=1}^K \\hat{b}\\!\\left(y^{(k)}(r)\\right).\n$$\n使用单位权重矩阵 $W = I_3$，准则函数为\n$$\nQ(r) = \\left(\\hat{b}(y^{obs}) - \\bar{b}(r) \\right)^{\\top} W \\left(\\hat{b}(y^{obs}) - \\bar{b}(r) \\right),\n$$\n其中 $y^{obs}$ 表示观测数据序列。间接推断估计量为\n$$\n\\hat{r} \\in \\arg\\min_{r \\in \\mathcal{R}} Q(r),\n$$\n该最小值在预先指定的有限网格 $\\mathcal{R}$ 上求得。\n\n您的程序必须完全按照上述定义实现此估计器，并使用以下固定的设计元素以确保确定性和可复现性：\n\n- 参数网格：\n$$\n\\mathcal{R} = \\left\\{ 3.50 + 0.0025 \\, j \\,:\\, j = 0,1,2,\\dots,200 \\right\\}.\n$$\n- 模拟重复次数：$K = 15$。\n- 初始条件：$x_0 = 0.123456789$。\n- 权重矩阵：$W = I_3$。\n- 为保证在不同候选值 $r \\in \\mathcal{R}$ 间的可复现性，请对模拟数据集使用公共随机数：对于每个测试案例，使用固定的种子 $s_{sim}$ 从伪随机数生成器生成恰好 $K$ 个长度为 $T$ 的独立高斯噪声序列，并将这 $K$ 个序列重用于所有的 $r \\in \\mathcal{R}$。对于观测数据，使用固定的种子 $s_{obs}$ 从伪随机数生成器生成其高斯噪声序列。在所有情况下，将 $\\varepsilon_t$ 抽样为独立的 $\\mathcal{N}(0,\\sigma^2)$ 变量。所有模拟中的初始条件 $x_0$ 必须相同。\n\n测试套件。为以下每个测试案例实现并求解估计器，其中真实结构参数、样本量、噪声水平和种子均已指定：\n\n- 案例 A（一般情况）：$r^{\\star} = 3.8000$, $T = 1000$, $\\sigma = 0.0200$, $s_{obs} = 1729$, $s_{sim} = 2468$。\n- 案例 B（接近混沌边缘，较低噪声）：$r^{\\star} = 3.5700$, $T = 800$, $\\sigma = 0.0100$, $s_{obs} = 1730$, $s_{sim} = 2469$。\n- 案例 C（最大混沌，无测量噪声）：$r^{\\star} = 4.0000$, $T = 1200$, $\\sigma = 0.0000$, $s_{obs} = 1731$, $s_{sim} = 2470$。\n- 案例 D（较短序列，较高噪声）：$r^{\\star} = 3.9500$, $T = 300$, $\\sigma = 0.0500$, $s_{obs} = 1732$, $s_{sim} = 2471$。\n\n对于每个案例，通过使用指定的 $r^{\\star}$、$x_0$ 和 $T$ 模拟 logistic 映射，并使用指定的 $s_{obs}$ 添加具有指定 $\\sigma$ 的高斯噪声，来生成观测数据 $y^{obs}$。然后，如上所定义，在网格 $\\mathcal{R}$ 上计算间接推断估计量 $\\hat{r}$，每个网格点使用 $K$ 个模拟数据集，这些数据集使用来自指定 $s_{sim}$ 的公共随机数生成。\n\n最终输出格式。您的程序应生成单行输出，其中包含案例 A-D 的四个估计值 $\\hat{r}$，按此顺序排列，形式为用方括号括起来的逗号分隔列表。每个值必须打印为四舍五入到小数点后恰好六位的小数。例如，输出格式如“[rA,rB,rC,rD]”，其中 $rA$、$rB$、$rC$ 和 $rD$ 是四个四舍五入后的估计值。不涉及单位，也不应打印任何额外文本。", "solution": "该问题要求构建一个基于间接推断 (Indirect Inference, II) 定义的估计器，并将其应用于一个混沌结构模型。结构 DGP 是带观测噪声的 logistic 映射。该估计器由以下要素定义：\n\n1. 结构模型。状态方程为 $x_{t+1} = r \\, x_t \\, (1 - x_t)$，其中 $t = 0,1,\\dots,T-1$，初始条件 $x_0 = 0.123456789$。观测数据为 $y_t = x_t + \\varepsilon_t$，其中 $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ 对每个 $t$ 独立。\n\n2. 辅助模型。辅助模型是带截距的 $AR(1)$ 模型：$y_t = a_0 + a_1 \\, y_{t-1} + u_t$，其中 $t = 1,\\dots,T-1$。对于任意序列 $y = (y_0,\\dots,y_{T-1})$，使用正规方程定义 OLS 估计。令 $Y = (y_1,\\dots,y_{T-1})^{\\top}$，$X$ 是一个 $(T-1) \\times 2$ 矩阵，其第一列为全1向量，第二列为 $(y_0,\\dots,y_{T-2})^{\\top}$。OLS 估计量为\n$$\n\\hat{\\beta}(y) = \\begin{pmatrix} \\hat{a}_0(y) \\\\ \\hat{a}_1(y) \\end{pmatrix} = (X^{\\top}X)^{-1} X^{\\top} Y,\n$$\n残差为 $\\hat{u}(y) = Y - X \\hat{\\beta}(y)$。残差离散度统计量为\n$$\n\\hat{s}_u(y) = \\sqrt{ \\frac{1}{T-1} \\sum_{t=1}^{T-1} \\hat{u}_t(y)^2 }.\n$$\n将辅助统计量收集为 $\\hat{b}(y) = \\big(\\hat{a}_0(y), \\hat{a}_1(y), \\hat{s}_u(y)\\big)$。\n\n3. 间接推断准则。对于一个候选参数 $r$，使用结构模型、初始状态 $x_0$ 和已知 $\\sigma$ 的高斯噪声，模拟 $K = 15$ 个长度为 $T$ 的数据集 $y^{(k)}(r)$。计算模拟数据中辅助统计量的均值，\n$$\n\\bar{b}(r) = \\frac{1}{K} \\sum_{k=1}^K \\hat{b}\\!\\left(y^{(k)}(r)\\right),\n$$\n并使用单位权重矩阵 $W = I_3$ 定义二次距离为\n$$\nQ(r) = \\left(\\hat{b}(y^{obs}) - \\bar{b}(r)\\right)^{\\top} \\left(\\hat{b}(y^{obs}) - \\bar{b}(r)\\right).\n$$\n估计量是 $\\hat{r} \\in \\arg\\min_{r \\in \\mathcal{R}} Q(r)$，在有限网格 $\\mathcal{R} = \\{3.50 + 0.0025 \\, j : j = 0,1,\\dots,200\\}$ 上求最小。\n\n4. 可复现性与公共随机数。对于每个测试案例，固定观测噪声种子 $s_{obs}$ 以生成单一的观测序列 $\\{\\varepsilon_t^{obs}\\}$，并固定模拟种子 $s_{sim}$ 以生成恰好 $K = 15$ 个独立的高斯噪声序列 $\\{\\varepsilon_t^{(k)}\\}_{k=1}^K$。对所有候选 $r \\in \\mathcal{R}$ 使用这相同的 $K$ 个序列。这就实现了公共随机数，使得 $Q(r)$ 的变化是由于结构动态的改变，而不是由于模拟噪声的变化。初始条件 $x_0 = 0.123456789$ 在所有模拟中保持不变。\n\n5. 与基本原理一致的实现计划。对每个测试案例：\n- 通过模拟 $x_{t+1} = r^{\\star} x_t (1-x_t)$（使用 $x_0$），然后对 $t = 0,\\dots,T-1$ 加上 $\\varepsilon_t^{obs} \\sim \\mathcal{N}(0,\\sigma^2)$，来生成观测序列 $y^{obs}$。\n- 通过上述 OLS 公式计算 $\\hat{b}(y^{obs})$。\n- 使用指定的 $s_{sim}$ 生成 $K$ 个独立的、每个长度为 $T$ 的高斯噪声序列。对于每个候选 $r \\in \\mathcal{R}$，使用 $x_0$ 从 logistic 映射模拟结构状态路径 $x(r)$，通过 $y^{(k)}(r) = x(r) + \\varepsilon^{(k)}$ 形成 $K$ 个模拟数据集，为每个 $k$ 计算 $\\hat{b}\\!\\left(y^{(k)}(r)\\right)$，得到 $\\bar{b}(r)$，评估 $Q(r)$，并在 $\\mathcal{R}$ 上选择最小值点。如果由于数值上的并列而存在多个最小值点，则选择在 $\\mathcal{R}$ 中达到最小值的最小 $r$，这是一个明确定义的规则。\n\n6. 数值与统计考量。对于 $r \\in [3.50,4.00]$，logistic 映射表现出复杂且通常是混沌的行为。间接推断利用辅助模型来比较观测数据和模拟数据之间的显著特征（截距、持续性、残差离散度）。通过在一组使用公共随机数的模拟中匹配这些特征，可以得到一个指导 $r$ 选择的准则。有限网格 $\\mathcal{R}$ 定义了对连续参数空间的筛分式近似；网格分辨率 $0.0025$ 意味着在理想条件下，$\\hat{r}$ 的可达精度在 $0.0025$ 之内。测量噪声 $\\sigma$ 和样本量 $T$ 的存在会影响辅助统计量的可变性，从而影响 $\\hat{r}$ 的精度。\n\n7. 输出。最终程序计算四个指定案例（按 A, B, C, D 顺序）的 $\\hat{r}$，并打印单行输出，该行包含一个由方括号括起来的、逗号分隔的四个估计值列表，每个估计值四舍五入到小数点后六位，无额外输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef simulate_logistic_path(r: float, T: int, x0: float) -> np.ndarray:\n    \"\"\"\n    Simulate the logistic map x_{t+1} = r x_t (1 - x_t) for t=0,...,T-2 with x_0 = x0.\n    Returns an array x of length T with x[0]=x0.\n    \"\"\"\n    x = np.empty(T, dtype=float)\n    x[0] = x0\n    for t in range(T - 1):\n        x[t + 1] = r * x[t] * (1.0 - x[t])\n    return x\n\ndef auxiliary_stats_ar1(y: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute auxiliary statistics from AR(1) with intercept:\n    y_t = a0 + a1 y_{t-1} + u_t, t=1..T-1.\n    Returns [a0_hat, a1_hat, s_u_hat] where s_u_hat = sqrt( (1/(T-1)) sum u_t^2 ).\n    \"\"\"\n    # Ensure 1D float array\n    y = np.asarray(y, dtype=float).ravel()\n    if y.size < 2:\n        # Degenerate case: not enough observations; return NaNs (should not occur in this problem)\n        return np.array([np.nan, np.nan, np.nan], dtype=float)\n    y_curr = y[1:]\n    y_lag = y[:-1]\n    # Design matrix with intercept\n    X = np.column_stack((np.ones_like(y_lag), y_lag))\n    # OLS via normal equations\n    XtX = X.T @ X\n    XtY = X.T @ y_curr\n    beta = np.linalg.solve(XtX, XtY)\n    resid = y_curr - X @ beta\n    s_u = np.sqrt(np.mean(resid ** 2))\n    return np.array([beta[0], beta[1], s_u], dtype=float)\n\ndef indirect_inference_estimate_r(\n    r_grid: np.ndarray,\n    K: int,\n    x0: float,\n    T: int,\n    sigma: float,\n    obs_seed: int,\n    sim_seed: int,\n    r_true: float\n) -> float:\n    \"\"\"\n    Compute the indirect inference estimate of r over the given grid r_grid.\n    - Generate observed data using r_true, x0, T, sigma with RNG seed obs_seed.\n    - Generate K simulation noise sequences using seed sim_seed (common random numbers).\n    - For each r in r_grid, simulate x path and form K simulated datasets by adding the fixed noise sequences.\n    - Compute auxiliary statistics for observed and average over simulated datasets.\n    - Minimize squared Euclidean distance between observed and average simulated auxiliary stats.\n    Returns the minimizing r (tie broken by smallest r).\n    \"\"\"\n    # Generate observed data\n    x_obs = simulate_logistic_path(r_true, T, x0)\n    rng_obs = np.random.default_rng(obs_seed)\n    eps_obs = rng_obs.normal(loc=0.0, scale=sigma, size=T)\n    y_obs = x_obs + eps_obs\n    b_obs = auxiliary_stats_ar1(y_obs)\n\n    # Pre-generate K noise sequences (common random numbers)\n    rng_sim = np.random.default_rng(sim_seed)\n    eps_bank = rng_sim.normal(loc=0.0, scale=sigma, size=(K, T))\n\n    # For each r, compute Q(r)\n    best_r = None\n    best_Q = np.inf\n\n    # Preallocate buffer for speed\n    # Loop over candidate r\n    for r in r_grid:\n        # Simulate state path once for this r\n        x_sim = simulate_logistic_path(r, T, x0)\n        # Add fixed noise sequences to produce K simulated datasets\n        # Each y_k has shape (T,)\n        Q_components = []\n        b_sum = np.zeros(3, dtype=float)\n        # Loop over K replications\n        for k in range(K):\n            y_k = x_sim + eps_bank[k]\n            b_k = auxiliary_stats_ar1(y_k)\n            b_sum += b_k\n        b_bar = b_sum / K\n        diff = b_obs - b_bar\n        Q_r = float(diff @ diff)  # Identity weighting\n        # Update best\n        if Q_r < best_Q - 1e-18 or (np.isclose(Q_r, best_Q) and (best_r is None or r < best_r)):\n            best_Q = Q_r\n            best_r = r\n\n    return float(best_r)\n\ndef solve():\n    # Fixed design elements\n    x0 = 0.123456789\n    K = 15\n    # Grid: {3.50 + 0.0025 * j, j=0..200}\n    r_grid = 3.50 + 0.0025 * np.arange(201, dtype=float)\n\n    # Define the test cases from the problem statement.\n    # Each tuple: (r_true, T, sigma, s_obs, s_sim)\n    test_cases = [\n        (3.8000, 1000, 0.0200, 1729, 2468),  # Case A\n        (3.5700,  800, 0.0100, 1730, 2469),  # Case B\n        (4.0000, 1200, 0.0000, 1731, 2470),  # Case C\n        (3.9500,  300, 0.0500, 1732, 2471),  # Case D\n    ]\n\n    results = []\n    for r_true, T, sigma, s_obs, s_sim in test_cases:\n        r_hat = indirect_inference_estimate_r(\n            r_grid=r_grid,\n            K=K,\n            x0=x0,\n            T=T,\n            sigma=sigma,\n            obs_seed=s_obs,\n            sim_seed=s_sim,\n            r_true=r_true\n        )\n        # Round to 6 decimals in final output formatting\n        results.append(r_hat)\n\n    # Format each result to exactly 6 decimal places\n    formatted = \",\".join(f\"{val:.6f}\" for val in results)\n    # Final print statement in the exact required format.\n    print(f\"[{formatted}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"}]}