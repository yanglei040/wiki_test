{"hands_on_practices": [{"introduction": "要掌握协整的实际应用，第一步是精通其基础检验程序。本练习将引导您完成经典的Engle-Granger两步法。您将亲手模拟存在（或不存在）长期均衡关系的时间序列数据，并利用普通最小二乘法（OLS）和增广Dickey-Fuller（ADF）检验来判断协整关系是否存在，从而为更复杂的分析打下坚实的基础。[@problem_id:2380057]", "id": "2380057", "problem": "给定一个决策问题，该问题使用纯粹的数学术语表述，内容是关于代表公司季度研发支出及其滞后一季度的季度收入的两个随机过程之间的长期均衡关系。对于所提供测试套件中的每一组参数，请根据以下定义构建两个序列 $\\{x_t\\}_{t=0}^{T}$ 和 $\\{y_t\\}_{t=1}^{T}$。\n\n1. 数据生成部分与记法。\n   - 令 $T \\in \\mathbb{N}$ 表示以季度为单位的样本量。\n   - 令 $x_0 = 0$。对于 $t \\in \\{1,2,\\dots,T\\}$，令\n     $$x_t = x_{t-1} + \\varepsilon_t,$$\n     其中 $\\{\\varepsilon_t\\}_{t=1}^{T}$ 是均值为 $0$、方差为 $\\sigma_{\\varepsilon}^2$ 的独立同分布高斯随机变量。\n   - 将收入关系式中使用的滞后输入定义为 $\\{x_{t-1}\\}_{t=1}^{T}$。\n   - 收入关系式中的扰动项有两种备选设定，由类型指示符 $\\text{residual\\_type} \\in \\{\\text{stationary}, \\text{random\\_walk}\\}$ 参数化：\n     - 如果 $\\text{residual\\_type} = \\text{stationary}$，定义 $v_0 = 0$，且对于 $t \\in \\{1,2,\\dots,T\\}$，\n       $$v_t = \\phi\\, v_{t-1} + \\eta_t,$$\n       其中 $|\\phi| &lt; 1$，且 $\\{\\eta_t\\}_{t=1}^{T}$ 是均值为 $0$、方差为 $\\sigma_{\\eta}^2$ 的独立同分布高斯随机变量。在这种情况下，设\n       $$y_t = c + \\beta\\, x_{t-1} + v_t.$$\n     - 如果 $\\text{residual\\_type} = \\text{random\\_walk}$，定义 $s_0 = 0$，且对于 $t \\in \\{1,2,\\dots,T\\}$，\n       $$s_t = s_{t-1} + \\eta_t,$$\n       其中 $\\{\\eta_t\\}_{t=1}^{T}$ 是均值为 $0$、方差为 $\\sigma_{\\eta}^2$ 的独立同分布高斯随机变量。在这种情况下，设\n       $$y_t = c + \\beta\\, x_{t-1} + s_t.$$\n\n2. 应用于每个参数集的决策规则。\n   - 考虑静态长期关系\n     $$y_t = \\alpha + \\beta^{\\ast} x_{t-1} + u_t,$$\n     并将 $\\{\\hat{u}_t\\}_{t=1}^{T}$ 定义为将 $\\{y_t\\}_{t=1}^{T}$ 对一个常数项和 $\\{x_{t-1}\\}_{t=1}^{T}$ 进行普通最小二乘 (OLS) 回归所得的残差。\n   - 计算不含确定性项且带有一阶差分的一阶滞后项的增广 Dickey-Fuller (ADF) 回归：\n     $$\\Delta \\hat{u}_t = \\varphi\\, \\hat{u}_{t-1} + \\gamma\\, \\Delta \\hat{u}_{t-1} + \\varepsilon_t^{\\ast}, \\quad t = 3,4,\\dots,T.$$\n     令 $\\tau$ 表示从该回归计算出的、用于检验原假设 $H_0: \\varphi = 0$ 与单侧备择假设 $H_1: \\varphi &lt; 0$ 的 t-统计量。\n   - 使用显著性水平 $\\alpha = 0.05$，以及适用于协整回归中含截距项、ADF 回归中不含确定性项模型的 Engle-Granger 基于残差的临界值 $c_{0.05} = -3.34$。当且仅当 $\\tau \\le c_{0.05}$ 时，判定 $\\{y_t\\}$ 和 $\\{x_{t-1}\\}$ 是协整的。\n\n3. 测试套件。对于每个参数集 $\\theta$，使用上述定义、给定的参数和一个以指定种子初始化的独立伪随机数生成器来生成序列。参数如下：\n   - 案例 A：$\\left(T, \\text{seed}, c, \\beta, \\sigma_{\\varepsilon}, \\sigma_{\\eta}, \\phi, \\text{residual\\_type}\\right) = \\left(400, 101, 0.5, 1.3, 1.0, 0.3, 0.4, \\text{stationary}\\right)$。\n   - 案例 B：$\\left(T, \\text{seed}, c, \\beta, \\sigma_{\\varepsilon}, \\sigma_{\\eta}, \\phi, \\text{residual\\_type}\\right) = \\left(400, 202, 0.5, 1.3, 1.0, 0.8, 0.0, \\text{random\\_walk}\\right)$。\n   - 案例 C：$\\left(T, \\text{seed}, c, \\beta, \\sigma_{\\varepsilon}, \\sigma_{\\eta}, \\phi, \\text{residual\\_type}\\right) = \\left(400, 303, -0.2, -0.8, 0.8, 0.5, 0.7, \\text{stationary}\\right)$。\n\n4. 要求的最终输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的、逗号分隔的布尔值列表，其顺序与上述测试案例的顺序一致。如果根据第 2 项中的规则判定存在协整关系，则对应的布尔值为 $\\text{True}$，否则为 $\\text{False}$。例如，包含三个案例的输出应如下所示\n$$[\\text{True},\\text{False},\\text{True}].$$", "solution": "问题陈述提出了一个时间序列计量经济学中有效且适定的计算练习。它要求实现 Engle-Granger 两步法，以检验两个模拟时间序列之间的协整关系。该问题有科学依据，经过形式化指定，并且为每个测试案例提供了获得唯一解所需的所有参数和步骤。我们将着手求解。\n\n问题的核心是确定两个随机过程 $\\{x_t\\}$ 和 $\\{y_t\\}$ 之间是否存在称为协整的长期均衡关系。为解决此问题，需要根据指定的数据生成过程 (DGP) 生成序列，然后应用统计假设检验。对测试套件中提供的每一组参数都执行这整个过程。\n\n**步骤 1：数据生成**\n\n对于每个测试案例，我们首先模拟时间序列数据。使用一个特定的种子来初始化伪随机数生成器，以保证结果的可复现性。\n设 T 为样本量。我们生成两个独立同分布 (i.i.d.) 的高斯随机冲击序列：$\\{\\varepsilon_t\\}_{t=1}^{T} \\sim N(0, \\sigma_{\\varepsilon}^2)$ 和 $\\{\\eta_t\\}_{t=1}^{T} \\sim N(0, \\sigma_{\\eta}^2)$。\n\n第一个序列 $\\{x_t\\}_{t=0}^{T}$ 代表研发支出，被构建为一个纯随机游走：\n$$x_t = x_{t-1} + \\varepsilon_t, \\quad \\text{for } t=1, \\dots, T$$\n初始条件为 $x_0 = 0$。像 $\\{x_t\\}$ 这样的过程是非平稳的，被称为一阶单整，记为 $I(1)$，因为它的一阶差分 $\\Delta x_t = \\varepsilon_t$ 是平稳的。\n\n第二个序列 $\\{y_t\\}_{t=1}^{T}$ 代表收入，其构建基于与滞后支出 $\\{x_{t-1}\\}_{t=1}^{T}$ 的关系以及一个性质取决于 `residual_type` 参数的误差项。\n\n- **情况 1：`residual_type` = `stationary`**。误差项 $\\{v_t\\}_{t=1}^{T}$ 遵循一个平稳的一阶自回归 (AR(1)) 过程：\n$$v_t = \\phi v_{t-1} + \\eta_t, \\quad \\text{for } t=1, \\dots, T$$\n其中 $v_0 = 0$ 且自回归参数 $|\\phi| < 1$。那么收入序列为：\n$$y_t = c + \\beta x_{t-1} + v_t$$\n在这种情况下，线性组合 $y_t - \\beta x_{t-1} - c = v_t$ 是平稳的。根据定义，如果两个 $I(1)$ 变量的线性组合是平稳的，那么这两个变量就是协整的。因此，我们预期检验会发现协整关系。\n\n- **情况 2：`residual_type` = `random_walk`**。误差项 $\\{s_t\\}_{t=1}^{T}$ 本身是一个随机游走：\n$$s_t = s_{t-1} + \\eta_t, \\quad \\text{for } t=1, \\dots, T$$\n其中 $s_0 = 0$。收入序列为：\n$$y_t = c + \\beta x_{t-1} + s_t$$\n在这里，线性组合 $y_t - \\beta x_{t-1} - c = s_t$ 是一个随机游走，因此是非平稳的 ($I(1)$)。这两个变量不是协整的。这是一个伪回归的经典案例，即尽管不存在有意义的长期关系，但仍可能观察到高度相关性。\n\n**步骤 2：Engle-Granger 协整检验**\n\n该检验分两个阶段进行。\n\n**阶段 1：估计长期关系**\n我们使用普通最小二乘法 (OLS) 估计假设的长期协整关系的参数：\n$$y_t = \\alpha + \\beta^{\\ast} x_{t-1} + u_t, \\quad \\text{for } t=1, \\dots, T$$\n我们将向量 $Y = (y_1, \\dots, y_T)^T$ 对一个维度为 $T \\times 2$ 的设计矩阵 $X_{ols}$ 进行回归，其中第一列是元素全为1的向量，第二列是向量 $(x_0, \\dots, x_{T-1})^T$。OLS 系数估计值由 $\\hat{\\theta} = (\\hat{\\alpha}, \\hat{\\beta}^{\\ast})^T = (X_{ols}^T X_{ols})^{-1} X_{ols}^T Y$ 给出。\n\n然后计算此回归的残差：\n$$\\hat{u}_t = y_t - (\\hat{\\alpha} + \\hat{\\beta}^{\\ast} x_{t-1}), \\quad \\text{for } t=1, \\dots, T$$\n如果 $y_t$ 和 $x_{t-1}$ 是协整的，这些残差应该是平稳的，即 $I(0)$。如果它们不是协整的，残差将是非平稳的，即 $I(1)$。\n\n**阶段 2：检验残差的单位根**\n为了检验残差 $\\{\\hat{u}_t\\}$ 的平稳性，我们进行增广 Dickey-Fuller (ADF) 检验。原假设是残差含有单位根（即非平稳），这意味着不存在协整关系。备择假设是平稳性，这意味着存在协整关系。\n\n需要估计的特定 ADF 回归模型是：\n$$\\Delta \\hat{u}_t = \\varphi \\hat{u}_{t-1} + \\gamma \\Delta \\hat{u}_{t-1} + \\varepsilon_t^{\\ast}, \\quad \\text{for } t=3, \\dots, T$$\n这是一个将残差的变化量 $\\Delta \\hat{u}_t$ 对残差的滞后水平 $\\hat{u}_{t-1}$ 和滞后的变化量 $\\Delta \\hat{u}_{t-1}$ 进行的 OLS 回归。该回归基于 $T-2$ 个观测值运行。\n\n令 $Z$ 为因变量 $(\\Delta \\hat{u}_3, \\dots, \\Delta \\hat{u}_T)^T$ 的 $(T-2) \\times 1$ 维向量。令 $W$ 为自变量的 $(T-2) \\times 2$ 维矩阵，其行向量为 $(\\hat{u}_{t-1}, \\Delta \\hat{u}_{t-1})$，其中 $t=3, \\dots, T$。$(\\varphi, \\gamma)$ 的 OLS 估计值为 $\\hat{\\psi} = (\\hat{\\varphi}, \\hat{\\gamma})^T = (W^T W)^{-1} W^T Z$。\n\n检验统计量是系数 $\\varphi$ 的 t-统计量：\n$$\\tau = \\frac{\\hat{\\varphi}}{SE(\\hat{\\varphi})}$$\n其中 $SE(\\hat{\\varphi})$ 是估计值 $\\hat{\\varphi}$ 的标准误。其计算方法为系数协方差矩阵 $\\hat{\\sigma}_{\\varepsilon^{\\ast}}^2 (W^T W)^{-1}$ 第一个对角线元素的平方根。回归误差项 $\\varepsilon_t^{\\ast}$ 的方差估计为：\n$$\\hat{\\sigma}_{\\varepsilon^{\\ast}}^2 = \\frac{1}{T-2-2} \\sum_{t=3}^T (\\varepsilon_t^{\\ast})^2 = \\frac{SSR}{T-4}$$\n其中 $SSR$ 是 ADF 回归的残差平方和。\n\n**步骤 3：决策规则**\n\n检验原假设 $H_0: \\varphi = 0$（残差中存在单位根，无协整）与单侧备择假设 $H_1: \\varphi < 0$（平稳，有协整）。\n\n将计算出的 t-统计量 $\\tau$ 与一个特定的临界值进行比较。因为该检验是针对 $I(1)$ 变量回归得到的残差进行的，所以 $\\tau$ 的分布是非标准的，被称为 Engle-Granger 分布。问题为 $\\alpha = 0.05$ 的显著性水平提供了相应的临界值，即 $c_{0.05} = -3.34$。\n\n决策规则是：\n- 如果 $\\tau \\le -3.34$，则拒绝 $H_0$。我们断定序列是协整的。该测试案例的结果为 `True`。\n- 如果 $\\tau > -3.34$，则不拒绝 $H_0$。我们断定没有证据表明存在协整关系。该测试案例的结果为 `False`。\n\n将此完整过程应用于三个测试案例中的每一个，以确定最终的布尔值输出。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the Engle-Granger two-step cointegration test for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # (T, seed, c, beta, sigma_eps, sigma_eta, phi, residual_type)\n        (400, 101, 0.5, 1.3, 1.0, 0.3, 0.4, \"stationary\"),\n        (400, 202, 0.5, 1.3, 1.0, 0.8, 0.0, \"random_walk\"),\n        (400, 303, -0.2, -0.8, 0.8, 0.5, 0.7, \"stationary\"),\n    ]\n\n    results = []\n    critical_value = -3.34\n\n    for case in test_cases:\n        T, seed, c, beta, sigma_eps, sigma_eta, phi, residual_type = case\n\n        # Step 1: Data Generation\n        rng = np.random.default_rng(seed)\n        eps = rng.normal(loc=0.0, scale=sigma_eps, size=T)\n        eta = rng.normal(loc=0.0, scale=sigma_eta, size=T)\n\n        # Generate x_t as a random walk\n        x = np.zeros(T + 1)\n        for t in range(1, T + 1):\n            x[t] = x[t - 1] + eps[t - 1]\n\n        # Generate y_t based on the residual type\n        # In the main vector y, y[t-1] corresponds to the mathematical y_t\n        if residual_type == \"stationary\":\n            v = np.zeros(T + 1)\n            for t in range(1, T + 1):\n                v[t] = phi * v[t-1] + eta[t-1]\n            error_term = v[1:T+1]\n        elif residual_type == \"random_walk\":\n            s = np.zeros(T + 1)\n            for t in range(1, T + 1):\n                s[t] = s[t-1] + eta[t-1]\n            error_term = s[1:T+1]\n        \n        y = c + beta * x[0:T] + error_term\n\n        # Step 2: Engle-Granger Stage 1 - OLS regression to get residuals\n        # Regress y_t on a constant and x_{t-1}\n        X_ols1 = np.vstack((np.ones(T), x[0:T])).T\n        \n        # Using lstsq is numerically more stable than direct matrix inversion\n        coeffs_ols1, _, _, _ = np.linalg.lstsq(X_ols1, y, rcond=None)\n        u_hat = y - X_ols1 @ coeffs_ols1\n\n        # Step 3: Engle-Granger Stage 2 - ADF test on residuals\n        # ADF model: Delta(u_hat)_t = phi*u_hat_{t-1} + gamma*Delta(u_hat)_{t-1} + e_t\n        # for t = 3, ..., T. This gives T-2 observations.\n        delta_u_hat = u_hat[1:] - u_hat[:-1]  # Corresponds to Delta(u_hat)_2, ..., Delta(u_hat)_T\n\n        # Dependent variable: Delta(u_hat)_t for t=3,...,T\n        Y_adf = delta_u_hat[1:]  # Corresponds to Delta(u_hat)_3, ..., Delta(u_hat)_T\n\n        # Independent variables\n        # u_hat_{t-1} for t=3,...,T\n        X_adf_col1 = u_hat[1:-1]\n        # Delta(u_hat)_{t-1} for t=3,...,T\n        X_adf_col2 = delta_u_hat[:-1]\n        \n        X_adf = np.vstack((X_adf_col1, X_adf_col2)).T\n\n        # Perform OLS for the ADF regression\n        coeffs_adf, ssr_adf, _, _ = np.linalg.lstsq(X_adf, Y_adf, rcond=None)\n        phi_hat = coeffs_adf[0]\n\n        # Calculate the t-statistic for phi_hat\n        n_obs_adf = T - 2\n        k_params_adf = 2\n        df = n_obs_adf - k_params_adf\n\n        # Estimate of the regression error variance\n        sigma2_hat_adf = ssr_adf[0] / df\n\n        # Variance-covariance matrix of the ADF coefficients\n        try:\n            cov_matrix = sigma2_hat_adf * np.linalg.inv(X_adf.T @ X_adf)\n            se_phi_hat = np.sqrt(cov_matrix[0, 0])\n            tau_statistic = phi_hat / se_phi_hat\n        except np.linalg.LinAlgError:\n            # Handle cases of singular matrix, which is unlikely but possible\n            # A very large t-stat would not reject the null, which is a safe default\n            tau_statistic = 0.0\n\n        # Step 4: Decision\n        is_cointegrated = tau_statistic <= critical_value\n        results.append(is_cointegrated)\n        \n    # Format and print the final output\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```"}, {"introduction": "经典的Engle-Granger检验是一个强大的工具，但其对预先制定的临界值的依赖性在小样本或存在复杂短期动态时可能成为一个弱点。本练习将介绍一种更为稳健的方法：参数自助法（parametric bootstrap）。您将学习如何通过模拟“无协整”的原假设来生成检验统计量的经验分布，从而为您的特定数据量身定制临界值，这不仅能提高统计推断的可靠性，还能让您对假设检验的原理有更深刻的理解。[@problem_id:2380066]", "id": "2380066", "problem": "你的任务是实现一个完整的、自包含的程序，该程序为恩格尔-格兰杰（Engle–Granger）协整检验统计量计算定制的、基于自助法（bootstrap）的临界值，并判定在几种受控的数据生成场景中是否拒绝“无协整”的原假设。程序必须是完全确定性的，并按如下规定生成单行输出。\n\n你必须基于以下基本原理构建解决方案：\n- 具有单个单位根的随机过程是一阶单整的，记为 $I(1)$，这意味着其一阶差分是平稳的。\n- 两个 $I(1)$ 序列 $\\{x_t\\}$ 和 $\\{y_t\\}$ 是协整的，如果存在一个标量 $\\beta$ 使得 $y_t - \\beta x_t$ 是平稳的。\n- 恩格尔-格兰杰（Engle–Granger）检验协整的过程如下：\n  1) 通过普通最小二乘法估计 $y_t$ 对 $x_t$ 的静态协整回归，以获得残差 $\\hat e_t$。\n  2) 使用不含确定性项的增广迪基-富勒（Augmented Dickey–Fuller, ADF）回归检验 $\\hat e_t$ 是否存在单位根，并使用滞后水平项系数的 $t$-统计量作为恩格尔-格兰杰检验统计量。\n- 步骤2中基于残差的单位根检验在原假设下具有非标准的极限分布，因此在小样本或某些短期动态条件下，查表得到的临界值可能无效。一种自助法（bootstrap）程序可以通过为两个序列生成随机游走来模拟“无协整”的原假设，其中随机游走的生成采用了适当估计的新息协方差，从而可以提供适应于样本和短期特征的定制临界值。\n\n你的程序必须遵守以下计算定义和要求：\n- 恩格尔-格兰杰（Engle–Granger）检验统计量：\n  - 首先，通过普通最小二乘法将 $y_t$ 对一个常数项和 $x_t$ 进行回归，$y_t = \\alpha + \\beta x_t + \\varepsilon_t$，对于 $t = 1,\\dots,T$，并获得残差 $\\hat e_t$。\n  - 然后，为残差估计一个不含截距项和趋势项的增广迪基-富勒回归：\n    $$\\Delta \\hat e_t = \\rho \\hat e_{t-1} + \\sum_{i=1}^{k} \\gamma_i \\Delta \\hat e_{t-i} + u_t,$$\n    其中 $k$ 是一个固定的非负整数。计算系数 $\\rho$ 的普通最小二乘 $t$-统计量。该 $t$-统计量即为恩格尔-格兰杰检验统计量 $\\tau$。\n- 在“无协整”原假设下的自助法：\n  - 估计观测到的一阶差分 $\\Delta x_t$ 和 $\\Delta y_t$ 的 $2\\times 2$ 协方差矩阵 $\\Sigma$。\n  - 对于每次自助法重复 $b = 1,\\dots,B$：生成一个均值为零、协方差为 $\\Sigma$ 的二元高斯新息序列 $\\{(\\varepsilon_t^{x,(b)}, \\varepsilon_t^{y,(b)})\\}_{t=1}^{T-1}$；构建自助法随机游走\n    $$x_t^{(b)} = x_{t-1}^{(b)} + \\varepsilon_t^{x,(b)}, \\quad y_t^{(b)} = y_{t-1}^{(b)} + \\varepsilon_t^{y,(b)},$$\n    初始值为 $x_0^{(b)} = x_0$ 和 $y_0^{(b)} = y_0$，并使用与上面相同的 $k$ 计算 $(y^{(b)}, x^{(b)})$ 的恩格尔-格兰杰统计量 $\\tau^{(b)}$。\n  - 在水平 $a$ 下的定制临界值是 $\\{\\tau^{(b)}\\}_{b=1}^B$ 的经验 $a$-分位数。由于恩格尔-格兰杰统计量在备择假设下通常为负，当 $\\tau \\le q_a$ 时拒绝原假设，其中 $q_a$ 是 $a$-分位数。\n- 在所有情况下，增广迪基-富勒回归均使用 $k = 1$ 的滞后阶数。\n- 在所有情况下，检验水平均使用 $a = 0.05$，自助法重复次数均使用 $B = 299$。\n\n数据生成过程和测试套件：\n- 共有3个测试用例。对于每个用例，你必须使用指定的种子生成长度为 $T$ 且初始值为 $x_0 = 0$ 和 $y_0 = 0$ 的 $(y_t, x_t)$ 以保证可复现性。你的代码必须使用为数据生成和自助法重复提供的确切随机种子。对于自助法，其种子通过将用例种子加上整数 $1000003$ 来确定性地派生。\n- 情况 A (协整)：\n  - 参数：$T = 220$，$\\beta = 1.3$，$\\phi = 0.4$，\\hbox{$\\sigma_x = 1.0$}，\\hbox{$\\sigma_u = 0.4$}，种子 $= 12345$。\n  - 生成 $x_t$ 作为随机游走：$x_t = x_{t-1} + \\varepsilon_t^x$，其中 $\\varepsilon_t^x \\sim \\mathcal N(0, \\sigma_x^2)$。\n  - 为协整关系生成平稳扰动项：$u_t = \\phi u_{t-1} + \\eta_t$，其中 $\\eta_t \\sim \\mathcal N(0, \\sigma_u^2)$ 且 $u_0 = 0$。\n  - 定义 $y_t = \\beta x_t + u_t$。\n- 情况 B (无协整，相关的随机游走)：\n  - 参数：$T = 220$，\\hbox{$\\sigma_x = 1.0$}，\\hbox{$\\sigma_y = 1.0$}，$\\rho = 0.6$，种子 $= 54321$。\n  - 生成一阶差分向量 $(\\Delta x_t, \\Delta y_t)$ 作为来自一个二元正态分布的独立同分布抽样，其均值为零，协方差为\n    $$\\begin{pmatrix}\\sigma_x^2 & \\rho \\sigma_x \\sigma_y \\\\ \\rho \\sigma_x \\sigma_y & \\sigma_y^2 \\end{pmatrix}。$$\n  - 构建随机游走 $x_t = x_{t-1} + \\Delta x_t$ 和 $y_t = y_{t-1} + \\Delta y_t$。\n- 情况 C (协整，较小样本)：\n  - 参数：$T = 80$，$\\beta = 1.0$，$\\phi = 0.5$，\\hbox{$\\sigma_x = 0.8$}，\\hbox{$\\sigma_u = 0.6$}，种子 $= 24680$。\n  - 使用给定参数，如情况 A 那样生成 $x_t$、$u_t$ 和 $y_t$。\n\n你的程序必须：\n- 实现上文定义的恩格尔-格兰杰统计量 $\\tau$，其中 $k = 1$。\n- 实现“无协整”原假设下的自助法程序，重复 $B = 299$ 次，并为每个用例计算定制的 $0.05$ 临界值。\n- 对于每个测试用例，返回一个布尔值，指示是否在 $0.05$ 水平上使用定制临界值拒绝原假设，即如果 $\\tau \\le q_{0.05}$ 则返回 $true$，否则返回 $false$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表，顺序为情况 A、情况 B、情况 C。例如，输出必须类似于\n  `[true,false,true]`\n- 所有布尔值必须为小写字母。\n\n不涉及角度。该问题中没有物理单位。所有数值量都是纯数字。所有随机数生成都必须使用指定的种子以确保可复现性。\n\n你的解决方案必须实现为单个独立的 Python 程序，该程序仅按所述的确切格式打印最终结果行。", "solution": "我们从时间序列计量经济学的核心定义开始。一个单变量过程如果其一阶差分是平稳的，则称其为一阶单整的，记为 $I(1)$。如果存在一个线性组合 $y_t - \\beta x_t$ 是平稳的，则两个 $I(1)$ 过程 $\\{x_t\\}$ 和 $\\{y_t\\}$ 是协整的。恩格尔-格兰杰（Engle–Granger）程序通过首先使用普通最小二乘法估计长期关系，然后检验残差的单位根，将此定义付诸实践。\n\n基于原理构建恩格尔-格兰杰统计量：\n- 步骤1（静态协整回归）：对于 $t = 1,\\dots,T$，通过普通最小二乘法估计 $y_t = \\alpha + \\beta x_t + \\varepsilon_t$，以获得拟合参数和残差 $\\hat e_t = y_t - \\hat \\alpha - \\hat \\beta x_t$。\n- 步骤2（残差单位根检验）：使用不含确定性项的增广迪基-富勒回归对残差进行建模，\n  $$\\Delta \\hat e_t = \\rho \\hat e_{t-1} + \\sum_{i=1}^k \\gamma_i \\Delta \\hat e_{t-i} + u_t,$$\n  其中 $k$ 是一个固定的增广滞后阶数，用于捕捉残差一阶差分中的短期序列相关。通过普通最小二乘法估计此回归。恩格尔-格兰杰检验统计量 $\\tau$ 是与滞后水平项 $\\hat e_{t-1}$ 的系数 $\\rho$ 相关的普通最小二乘 $t$-统计量。$\\tau$ 的一个足够负的值表明 $\\hat e_t$ 是平稳的，因此存在协整。\n\n为何需要自助法：\n- 在无协整的原假设下，残差 $\\hat e_t$ 并非平稳过程的固定线性变换；它从 $I(1)$ 回归量继承了单位根，并且基于残差的单位根统计量的分布是非标准的，以复杂的方式依赖于讨厌参数和有限样本特征。因此，在有限样本中或存在短期相关性或内生性的情况下，列表形式的渐近临界值可能不可靠。\n- 参数自助法可以通过生成合成的随机游走对 $(x_t^{(b)}, y_t^{(b)})$ 来模拟无协整的原假设，这些随机游走的协方差结构与观测到的一阶差分相匹配。通过在这些自助法序列上重新计算恩格尔-格兰杰统计量，我们在以观测到的短期特征为条件下，近似了原假设下 $\\tau$ 的抽样分布。\n\n基于第一性原理的自助法设计：\n- 使用无偏样本协方差估计观测到的一阶差分 $(\\Delta x_t, \\Delta y_t)$ 的协方差矩阵 $\\Sigma$，这是一个经过充分检验的、能在大样本中匹配二阶矩的估计量。\n- 在无协整的原假设下，$x_t$ 和 $y_t$ 的行为都类似于由均值为零的新息驱动的 $I(1)$ 过程。我们通过将 $\\Delta x_t^{(b)}, \\Delta y_t^{(b)}$ 模拟为均值为零、协方差为 $\\Sigma$ 的独立同分布高斯向量，然后累加构建随机游走 $x_t^{(b)}$ 和 $y_t^{(b)}$ 来模仿这一点，并以原始的 $x_0$ 和 $y_0$ 初始化以对齐水平值。\n- 对于每次自助法重复 $b = 1,\\dots,B$，我们通过在 $(y^{(b)}, x^{(b)})$ 上重复两步法并使用相同的增广滞后阶数 $k$ 来计算恩格尔-格兰杰统计量 $\\tau^{(b)}$。\n\n基于分位数的临界值和拒绝法则：\n- 令 $\\{\\tau^{(b)}\\}_{b=1}^B$ 为自助法统计量集合。在水平 $a$ 下的定制临界值是该集合的经验 $a$-分位数 $q_a$。由于更负的 $\\tau$ 值表示反对原假设的证据更强，因此当 $\\tau \\le q_a$ 时，检验拒绝无协整的原假设。\n- 我们固定 $a = 0.05$ 和 $B = 299$，这在高等本科教学环境中，在准确性和计算成本之间取得了平衡。\n\n实现的算法细节：\n- 普通最小二乘法通过线性代数实现如下。对于回归 $y = X \\beta + \\varepsilon$，普通最小二乘估计量为 $\\hat \\beta = (X^\\top X)^{-1} X^\\top y$。残差方差估计量为 $\\hat \\sigma^2 = \\frac{1}{n - p} \\sum_{i=1}^n \\hat \\varepsilon_i^2$，其中 $n$ 是样本量，p 是回归量数量。$\\hat \\beta$ 的协方差矩阵是 $\\hat \\sigma^2 (X^\\top X)^{-1}$，任何分量的普通最小二乘 $t$-统计量是估计系数除以其估计的标准误。\n- 对于不含确定性项的增广迪基-富勒回归，设计矩阵包括滞后残差水平 $\\hat e_{t-1}$ 和 $k$ 个滞后一阶差分 $\\Delta \\hat e_{t-1}, \\dots, \\Delta \\hat e_{t-k}$。当 $k = 1$ 时，回归量是 $\\hat e_{t-1}$ 和 $\\Delta \\hat e_{t-1}$，响应变量是 $\\Delta \\hat e_t$。对齐滞后项后，有效样本量为 $T - 1 - k$。\n- 为确保可复现性，我们为数据生成使用固定的随机种子，并通过将用例种子加上 $1000003$ 来为每个用例派生自助法种子。我们在观测到的 $x_0$ 和 $y_0$ 处初始化自助法随机游走。\n\n测试套件覆盖范围的基本原理：\n- 情况 A 是一个具有明确协整关系的“理想路径”：$T = 220$，$\\beta = 1.3$，具有中度平稳偏差 $u_t$（$\\phi = 0.4$）和相对于 $\\sigma_x = 1.0$ 较小的方差 $\\sigma_u = 0.4$。我们预期会得到一个足够低于自助法临界值的负 $\\tau$ 值，因此拒绝原假设。\n- 情况 B 是一个无协整的原假设情况，但随机游走新息同期相关，这是一个已知的、伪回归风险在渐近意义上可通过定制临界值缓解的场景。匹配新息协方差的自助法应产生与有限样本行为一致的临界值，我们通常预期不拒绝原假设。\n- 情况 C 是一个较小样本的协整情况，其中 $T = 80$，$\\beta = 1.0$，且 $u_t$ 具有中度持续性（$\\phi = 0.5$）。尽管样本较小，但自助法能适应有限样本特征，并且当协整存在时通常会得出拒绝的结论。\n\n输出：\n- 对于每个用例，计算一个布尔值，指示是否在 $0.05$ 水平上使用定制临界值拒绝原假设。程序打印单行，其中包含一个按情况 A、情况 B、情况 C 顺序排列的小写布尔值列表，例如 `[true,false,true]`。\n\n所有的数学定义和计算都源于核心的线性回归和时间序列原理，不依赖于预先制表的临界值。该算法尊重恩格尔-格兰杰检验的基于残差的性质，并通过模拟具有匹配的短期协方差的二元随机游走，构建了一个科学上合理的、在无协整原假设下的自助法。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Execution environment: Python 3.12, numpy 1.23.5, scipy 1.11.4 (not used below).\nimport numpy as np\n\ndef ols_beta_and_se(X: np.ndarray, y: np.ndarray):\n    \"\"\"\n    Ordinary Least Squares estimation.\n    Returns:\n        beta_hat: (p,) array of coefficients\n        se_beta:  (p,) array of standard errors\n        resid:    (n,) residual vector\n    \"\"\"\n    # Solve for beta using least squares\n    beta_hat, residuals, rank, s = np.linalg.lstsq(X, y, rcond=None)\n    # Compute residuals explicitly for robust se\n    resid = y - X @ beta_hat\n    n, p = X.shape\n    # Degrees of freedom\n    dof = max(n - p, 1)  # guard against division by zero; not expected in tests\n    sigma2 = (resid @ resid) / dof\n    # Compute (X'X)^{-1} using pseudo-inverse for numerical stability\n    XtX = X.T @ X\n    XtX_inv = np.linalg.pinv(XtX)\n    cov_beta = sigma2 * XtX_inv\n    se_beta = np.sqrt(np.diag(cov_beta))\n    return beta_hat, se_beta, resid\n\ndef engle_granger_tau(y: np.ndarray, x: np.ndarray, k: int = 1) -> float:\n    \"\"\"\n    Compute the Engle-Granger test statistic (ADF t-stat on residual lag) with k ADF lags, no intercept.\n    Steps:\n      1) OLS of y on [1, x], get residuals e.\n      2) ADF regression: Δe_t = rho * e_{t-1} + sum_{i=1}^k gamma_i Δe_{t-i} + u_t\n         No intercept or trend. Return t-statistic for rho.\n    \"\"\"\n    T = len(y)\n    # Step 1: cointegrating regression with intercept\n    X_coint = np.column_stack([np.ones(T), x])\n    beta_hat, se_beta, resid = ols_beta_and_se(X_coint, y)\n    e = resid\n    # Step 2: ADF without intercept/trend\n    de = e[1:] - e[:-1]  # length T-1\n    if k < 0:\n        raise ValueError(\"k must be nonnegative\")\n    m = (T - 1) - k\n    if m <= 1:\n        # Not enough data; return NaN or a neutral large value\n        return np.nan\n    y_adf = de[k:]  # length m\n    e_lag = e[:-1][k:]  # length m\n    Z_cols = [e_lag]\n    # add lagged differences Δe_{t-1} ... Δe_{t-k}\n    for j in range(1, k + 1):\n        Z_cols.append(de[k - j:-j])\n    Z = np.column_stack(Z_cols)\n    beta_adf, se_adf, resid_adf = ols_beta_and_se(Z, y_adf)\n    rho_hat = beta_adf[0]\n    se_rho = se_adf[0]\n    # Guard against zero standard error (degenerate); return 0 t-stat if degenerate\n    if se_rho <= 0 or not np.isfinite(se_rho):\n        return 0.0\n    t_stat = rho_hat / se_rho\n    return float(t_stat)\n\ndef bootstrap_critical_value(y: np.ndarray, x: np.ndarray, k: int, B: int, alpha: float, seed: int) -> float:\n    \"\"\"\n    Parametric bootstrap under the null of no cointegration:\n      - Estimate covariance of [Δx, Δy]\n      - Generate bivariate Gaussian innovations with that covariance\n      - Construct random walks starting at original initial values\n      - Compute Engle-Granger tau for each bootstrap sample\n      - Return empirical alpha-quantile (left tail)\n    \"\"\"\n    T = len(y)\n    dx = x[1:] - x[:-1]\n    dy = y[1:] - y[:-1]\n    # Stack differences to estimate covariance\n    diffs = np.column_stack([dx, dy])\n    # Unbiased covariance estimate\n    Sigma = np.cov(diffs, rowvar=False, ddof=1)\n    # Ensure Sigma is positive semidefinite; numerical guard\n    # If covariance estimation issues, regularize slightly\n    eigvals = np.linalg.eigvalsh(Sigma)\n    if np.min(eigvals) < 1e-10:\n        Sigma = Sigma + 1e-8 * np.eye(2)\n    rng = np.random.default_rng(seed)\n    taus = []\n    for b in range(B):\n        # Generate T-1 innovations\n        innov = rng.multivariate_normal(mean=np.zeros(2), cov=Sigma, size=T - 1)\n        x_star = np.empty(T)\n        y_star = np.empty(T)\n        x_star[0] = x[0]\n        y_star[0] = y[0]\n        # Build random walks\n        for t in range(1, T):\n            x_star[t] = x_star[t - 1] + innov[t - 1, 0]\n            y_star[t] = y_star[t - 1] + innov[t - 1, 1]\n        tau_b = engle_granger_tau(y_star, x_star, k=k)\n        taus.append(tau_b)\n    taus = np.array(taus, dtype=float)\n    # Empirical alpha-quantile (left tail)\n    q_alpha = float(np.quantile(taus, alpha, method=\"linear\"))\n    return q_alpha\n\ndef simulate_cointegrated(T: int, seed: int, beta: float, phi: float, sigma_x: float, sigma_u: float):\n    \"\"\"\n    Simulate:\n      x_t = x_{t-1} + eps_x, eps_x ~ N(0, sigma_x^2)\n      u_t = phi * u_{t-1} + eta, eta ~ N(0, sigma_u^2)\n      y_t = beta * x_t + u_t\n    with x_0 = 0, u_0 = 0, y_0 accordingly.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    x = np.empty(T)\n    u = np.empty(T)\n    y = np.empty(T)\n    x[0] = 0.0\n    u[0] = 0.0\n    for t in range(1, T):\n        x[t] = x[t - 1] + rng.normal(0.0, sigma_x)\n        u[t] = phi * u[t - 1] + rng.normal(0.0, sigma_u)\n    y = beta * x + u\n    return y, x\n\ndef simulate_null_rw(T: int, seed: int, sigma_x: float, sigma_y: float, rho: float):\n    \"\"\"\n    Simulate correlated random walks under null:\n      [Δx_t, Δy_t]' ~ iid N(0, Sigma), Sigma set by sigma_x, sigma_y, rho\n      x_0 = y_0 = 0\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    cov_xy = rho * sigma_x * sigma_y\n    Sigma = np.array([[sigma_x ** 2, cov_xy],\n                      [cov_xy, sigma_y ** 2]], dtype=float)\n    # Numerical guard for positive semidefinite Sigma\n    eigvals = np.linalg.eigvalsh(Sigma)\n    if np.min(eigvals) < 1e-10:\n        Sigma = Sigma + 1e-8 * np.eye(2)\n    innov = rng.multivariate_normal(mean=np.zeros(2), cov=Sigma, size=T - 1)\n    x = np.empty(T)\n    y = np.empty(T)\n    x[0] = 0.0\n    y[0] = 0.0\n    for t in range(1, T):\n        x[t] = x[t - 1] + innov[t - 1, 0]\n        y[t] = y[t - 1] + innov[t - 1, 1]\n    return y, x\n\ndef decide_reject(y: np.ndarray, x: np.ndarray, k: int, B: int, alpha: float, case_seed: int) -> bool:\n    \"\"\"\n    Compute observed tau, bootstrap critical value, and return True if tau <= q_alpha.\n    \"\"\"\n    tau_obs = engle_granger_tau(y, x, k=k)\n    # Bootstrap seed derived from case seed\n    boot_seed = case_seed + 1000003\n    q_alpha = bootstrap_critical_value(y, x, k=k, B=B, alpha=alpha, seed=boot_seed)\n    # Reject if observed tau is less than or equal to critical value (left tail)\n    return bool(tau_obs <= q_alpha)\n\ndef solve():\n    # Define constants\n    k = 1          # ADF lags\n    B = 299        # Bootstrap replications\n    alpha = 0.05   # Test size\n\n    # Test cases as specified:\n    # Case A: Cointegrated\n    T_A = 220\n    beta_A = 1.3\n    phi_A = 0.4\n    sigma_x_A = 1.0\n    sigma_u_A = 0.4\n    seed_A = 12345\n\n    # Case B: Null (no cointegration), correlated random walks\n    T_B = 220\n    sigma_x_B = 1.0\n    sigma_y_B = 1.0\n    rho_B = 0.6\n    seed_B = 54321\n\n    # Case C: Cointegrated, smaller sample\n    T_C = 80\n    beta_C = 1.0\n    phi_C = 0.5\n    sigma_x_C = 0.8\n    sigma_u_C = 0.6\n    seed_C = 24680\n\n    # Generate data and decisions\n    # Case A\n    yA, xA = simulate_cointegrated(T_A, seed_A, beta_A, phi_A, sigma_x_A, sigma_u_A)\n    reject_A = decide_reject(yA, xA, k=k, B=B, alpha=alpha, case_seed=seed_A)\n\n    # Case B\n    yB, xB = simulate_null_rw(T_B, seed_B, sigma_x_B, sigma_y_B, rho_B)\n    reject_B = decide_reject(yB, xB, k=k, B=B, alpha=alpha, case_seed=seed_B)\n\n    # Case C\n    yC, xC = simulate_cointegrated(T_C, seed_C, beta_C, phi_C, sigma_x_C, sigma_u_C)\n    reject_C = decide_reject(yC, xC, k=k, B=B, alpha=alpha, case_seed=seed_C)\n\n    results = [reject_A, reject_B, reject_C]\n    # Print in exact required format: lower-case booleans\n    def bool_to_str(b): return \"true\" if b else \"false\"\n    print(f\"[{','.join(bool_to_str(r) for r in results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"}, {"introduction": "经济关系并非一成不变，一个关键的技能是识别长期均衡关系何时可能发生破裂。本练习将协整概念应用于结构突变检测这一动态问题。您将通过实现一个递归估算程序来实时监控协整参数的稳定性，并利用一步向前预测误差来精确定位潜在的突变点，这项技术从静态检验扩展到了动态监测，在金融风险管理和宏观经济政策分析中具有广泛的应用。[@problem_id:2380077]", "id": "2380077", "problem": "给定两个合成的时间序列，它们被构建为遵循一个可能在未知时间点发生结构性变化的长期关系。令 $\\{x_t\\}_{t=0}^{T-1}$ 和 $\\{y_t\\}_{t=0}^{T-1}$ 定义如下。对于所有 $t \\in \\{0,1,\\dots,T-1\\}$，令 $x_0 = 0$ 且当 $t \\ge 1$ 时，\n$$\nx_t = x_{t-1} + \\sigma_e \\,\\varepsilon_t,\n$$\n且\n$$\ny_t = \\alpha + \\beta(t)\\, x_t + \\sigma_u \\,\\eta_t,\n$$\n其中 $\\{\\varepsilon_t\\}$ 和 $\\{\\eta_t\\}$ 是独立同分布标准正态随机变量的独立序列，而 $\\beta(t)$ 是一个分段常数函数\n$$\n\\beta(t) = \\begin{cases}\n\\beta_1, & t \\le T_b,\\\\\n\\beta_2, & t > T_b.\n\\end{cases}\n$$\n常数 $T_b$ 表示长期协整斜率从 $\\beta_1$ 变为 $\\beta_2$ 的（潜在）断点指数。如果不希望出现断点，则取 $T_b \\ge T$，使得对于所有 $t$ 都有 $\\beta(t) = \\beta_1$。截距 $\\alpha$ 是恒定的。\n\n扰动项 $\\{\\varepsilon_t\\}$ 和 $\\{\\eta_t\\}$ 必须使用线性同余生成器从一个指定的均匀伪随机序列中确定性地生成，以确保可复现性。定义整数上的乘法线性同余序列 $\\{U_k\\}$ 为\n$$\nU_{k} = (a \\cdot U_{k-1}) \\bmod m,\\quad k \\ge 1,\\quad U_0 = s,\n$$\n模为 $m = 2^{31}-1$，乘数为 $a = 16807$，整数种子为 $s \\in \\{1,2,\\dots,m-1\\}$。通过 $R_k = U_k/m$ 映射到均匀分布 $R_k \\in (0,1)$。使用 Box–Muller 变换将其转换为独立的标准正态新息：对于每一对 $(R_{2j-1}, R_{2j})$，\n$$\nZ_{2j-1} = \\sqrt{-2\\ln R_{2j-1}}\\,\\cos(2\\pi R_{2j}),\\quad\nZ_{2j} = \\sqrt{-2\\ln R_{2j-1}}\\,\\sin(2\\pi R_{2j}),\n$$\n其中三角函数中的所有角度均以弧度为单位。使用前 $T$ 个正态抽样值作为 $\\{\\varepsilon_t\\}$，后 $T$ 个正态抽样值作为 $\\{\\eta_t\\}$。\n\n为了进行检测，为协整关系 $y_s = \\alpha + \\beta x_s + \\text{error}$ 定义一个在扩展样本上计算的普通最小二乘（Ordinary Least Squares (OLS)）估计量的递归序列。对于每个 $t \\in \\{T_{\\min}, T_{\\min}+1, \\dots, T-2\\}$，通过最小化 $\\sum_{s=0}^{t} (y_s - \\alpha - \\beta x_s)^2$ 来估计 $(\\widehat{\\alpha}_t,\\widehat{\\beta}_t)$。等价地，设 $n_t = t+1$，样本均值为 $\\overline{x}_t = \\frac{1}{n_t}\\sum_{s=0}^{t} x_s$ 和 $\\overline{y}_t = \\frac{1}{n_t}\\sum_{s=0}^{t} y_s$，中心化和为 $S_{xx,t} = \\sum_{s=0}^{t} (x_s-\\overline{x}_t)^2$，$S_{xy,t} = \\sum_{s=0}^{t} (x_s-\\overline{x}_t)(y_s-\\overline{y}_t)$，\n$$\n\\widehat{\\beta}_t = \\frac{S_{xy,t}}{S_{xx,t}},\\quad \\widehat{\\alpha}_t = \\overline{y}_t - \\widehat{\\beta}_t\\,\\overline{x}_t.\n$$\n令残差标准差为\n$$\n\\widehat{\\sigma}_t = \\sqrt{\\frac{1}{n_t - 2}\\sum_{s=0}^{t} \\left(y_s - \\widehat{\\alpha}_t - \\widehat{\\beta}_t x_s\\right)^2},\n$$\n定义于 $n_t \\ge 3$。构建在时间 $t+1$ 的超前一步标准化预测误差，\n$$\nz_{t+1} = \\frac{y_{t+1} - \\widehat{\\alpha}_t - \\widehat{\\beta}_t x_{t+1}}{\\widehat{\\sigma}_t}.\n$$\n给定一个阈值 $c > 0$ 和一个游程长度 $L \\in \\mathbb{N}$，为 $s \\in \\{T_{\\min}+1, \\dots, T-1\\}$ 定义指示符 $I_s = \\mathbf{1}\\{|z_s| \\ge c\\}$。检测到的断点指数是满足 $I_{s^\\star} = I_{s^\\star+1} = \\dots = I_{s^\\star+L-1} = 1$ 的最小指数 $s^\\star$。如果不存在这样的指数，则返回 $-1$。以从零开始的时间索引报告检测到的指数 $s^\\star$。\n\n为以下参数集测试套件，将上述数据构建和检测过程实现为一个完整的程序，每个测试用例指定为一个元组\n$$\n(T, T_b, \\alpha, \\beta_1, \\beta_2, \\sigma_e, \\sigma_u, T_{\\min}, c, L, s).\n$$\n- 测试用例 A: $(240, 150, 0.5, 1.0, 1.6, 1.0, 0.05, 40, 3.0, 2, 12345)$。\n- 测试用例 B: $(240, 10^9, 0.5, 1.0, 1.0, 1.0, 0.05, 40, 3.0, 2, 54321)$。\n- 测试用例 C: $(220, 210, 0.3, 0.8, 1.9, 0.9, 0.05, 30, 2.5, 2, 20231107)$。\n- 测试用例 D: $(180, 60, 0.2, 1.2, 1.4, 0.8, 0.1, 25, 2.0, 3, 424242)$。\n\n对于每个测试用例，你的程序应该计算并返回一个整数：如果根据规则检测到断点，则为断点指数 $s^\\star$（以从零开始的索引）；如果未检测到断点，则为 $-1$。你的程序应该生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[rA,rB,rC,rD]”），其中 $rA$、$rB$、$rC$ 和 $rD$ 分别是对应于测试用例 A、B、C 和 D 的整数。", "solution": "该问题提出了一个计算计量经济学中明确定义的任务：检测两个非平稳时间序列之间线性协整关系中的结构性断点。从数据生成到断点检测的整个过程都是算法化的，这使得解是唯一且可验证的。严谨的分析分两个阶段进行：首先，确定性地生成合成数据；其次，应用递归监测算法。\n\n本练习的基础是一个可复现的数据生成过程。为确保这一点，我们不使用系统级的随机源，而是使用乘法线性同余生成器（LCG）从指定的种子 $s$ 构建所需的随机序列。整数序列 $\\{U_k\\}$ 由递推关系 $U_{k} = (a \\cdot U_{k-1}) \\bmod m$（$k \\ge 1$，$U_0 = s$）给出。问题指定了行业标准参数 $m = 2^{31}-1$ 和 $a = 16807$。这些整数通过变换 $R_k = U_k/m$ 映射到区间 $(0, 1)$ 内的均匀伪随机数序列 $\\{R_k\\}$。\n\n从这个均匀序列中，我们必须生成独立的标准正态随机变量。为此，问题指定了 Box-Muller 变换。对于每对均匀变量 $(R_{2j-1}, R_{2j})$，我们生成一对独立的标准正态变量 $(Z_{2j-1}, Z_{2j})$：\n$$\nZ_{2j-1} = \\sqrt{-2\\ln R_{2j-1}}\\,\\cos(2\\pi R_{2j})\n$$\n$$\nZ_{2j} = \\sqrt{-2\\ln R_{2j-1}}\\,\\sin(2\\pi R_{2j})\n$$\n问题需要总共 $2T$ 次正态抽样来构建序列。前 $T$ 次抽样构成序列 $\\{\\varepsilon_t\\}_{t=0}^{T-1}$，随后的 $T$ 次抽样构成序列 $\\{\\eta_t\\}_{t=0}^{T-1}$。\n\n利用这些扰动序列，构建时间序列。序列 $\\{x_t\\}$ 是一个 I($1$) 过程，具体来说是一个从 $x_0 = 0$ 开始的无漂移随机游走：\n$$\nx_t = x_{t-1} + \\sigma_e \\,\\varepsilon_t, \\quad \\text{for } t \\in \\{1, 2, \\dots, T-1\\}.\n$$\n序列 $\\{y_t\\}$ 被构建为与 $\\{x_t\\}$ 协整，并可能存在结构性断点。其定义为：\n$$\ny_t = \\alpha + \\beta(t)\\, x_t + \\sigma_u \\,\\eta_t, \\quad \\text{for } t \\in \\{0, 1, \\dots, T-1\\}.\n$$\n参数 $\\beta(t)$ 是一个阶跃函数，其值在断点指数 $T_b$ 处发生变化：\n$$\n\\beta(t) = \\beta_1 \\text{ if } t \\le T_b, \\quad \\text{and} \\quad \\beta(t) = \\beta_2 \\text{ if } t > T_b.\n$$\n$T_b \\ge T$ 的值表示在样本期内没有发生断点，因此对于所有 $t$，$\\beta(t) = \\beta_1$。\n\n第二阶段是断点的检测。这是通过一个基于扩展数据窗口上的普通最小二乘（OLS）估计的递归监测方案来完成的。对于从起点 $T_{\\min}$ 到 $T-2$ 的每个时间指数 $t$，我们使用从 $s=0$ 到 $s=t$ 的所有可用数据来估计静态协整回归 $y_s = \\alpha + \\beta x_s + \\text{error}$ 的系数。\n\n为了高效地执行此估计，我们不为每次回归重新计算所需的和。相反，我们维护相关量的滚动和：$\\sum x_s$、$\\sum y_s$、$\\sum x_s^2$、$\\sum y_s^2$ 和 $\\sum x_s y_s$。在每一步 $t$，我们用新的数据点 $(x_t, y_t)$ 更新这些和。令 $n_t = t+1$ 为样本大小。然后使用从这些和导出的标准公式计算 OLS 估计值 $(\\widehat{\\alpha}_t, \\widehat{\\beta}_t)$：\n$$\n\\widehat{\\beta}_t = \\frac{\\sum_{s=0}^{t} x_s y_s - n_t \\overline{x}_t \\overline{y}_t}{\\sum_{s=0}^{t} x_s^2 - n_t \\overline{x}_t^2}\n\\quad \\text{and} \\quad\n\\widehat{\\alpha}_t = \\overline{y}_t - \\widehat{\\beta}_t \\overline{x}_t,\n$$\n其中 $\\overline{x}_t$ 和 $\\overline{y}_t$ 是在 $\\{0, \\dots, t\\}$ 上的样本均值。\n\n检测机制的核心是超前一步标准化预测误差 $z_{t+1}$。该统计量衡量在给定截至时间 $t$ 所估计的模型下，下一个观测值 $(x_{t+1}, y_{t+1})$ 的意外程度。其定义为：\n$$\nz_{t+1} = \\frac{y_{t+1} - (\\widehat{\\alpha}_t + \\widehat{\\beta}_t x_{t+1})}{\\widehat{\\sigma}_t}\n$$\n分母 $\\widehat{\\sigma}_t$ 是回归残差的估计标准差，用于标准化预测误差。其计算方法如下：\n$$\n\\widehat{\\sigma}_t = \\sqrt{\\frac{1}{n_t - 2}\\sum_{s=0}^{t} (y_s - \\widehat{\\alpha}_t - \\widehat{\\beta}_t x_s)^2}\n$$\n分子中的残差平方和可以高效地计算为 $S_{yy,t} - \\widehat{\\beta}_t S_{xy,t}$，其中 $S_{yy,t}$ 和 $S_{xy,t}$ 分别是中心化的平方和与交叉乘积和。\n\n当结构性断点发生时，基于可能跨越断点前后两种机制的数据的 OLS 估计会变得有偏。这种偏差会对后续时期产生系统性的不良预测，导致一系列大的预测误差。检测规则将这种直觉形式化。对于给定的阈值 $c$，我们定义一个指示符，如果 $|z_s| \\ge c$，则 $I_s = 1$，否则 $I_s = 0$。当第一个时间指数 $s^\\star$ 引发了至少 $L$ 个连续的大标准化预测误差序列时，即 $I_{s^\\star} = I_{s^\\star+1} = \\dots = I_{s^\\star+L-1} = 1$，就发出了一个断点信号。对 $s^\\star$ 的搜索从指数 $T_{\\min}+1$ 开始。如果在观测窗口内没有找到这样的序列，我们断定没有检测到断点，并报告 $-1$。对于每个提供的参数集，整个过程都是确定性地实现的。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        # (T, T_b, alpha, beta_1, beta_2, sigma_e, sigma_u, T_min, c, L, s)\n        (240, 150, 0.5, 1.0, 1.6, 1.0, 0.05, 40, 3.0, 2, 12345),\n        (240, 10**9, 0.5, 1.0, 1.0, 1.0, 0.05, 40, 3.0, 2, 54321),\n        (220, 210, 0.3, 0.8, 1.9, 0.9, 0.05, 30, 2.5, 2, 20231107),\n        (180, 60, 0.2, 1.2, 1.4, 0.8, 0.1, 25, 2.0, 3, 424242),\n    ]\n\n    results = []\n    for params in test_cases:\n        result = _run_case(params)\n        results.append(result)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _run_case(params):\n    \"\"\"\n    Processes a single test case.\n    \"\"\"\n    T, T_b, alpha, beta_1, beta_2, sigma_e, sigma_u, T_min, c, L, s = params\n\n    # --- 1. Data Generation ---\n\n    # LCG parameters\n    m = 2**31 - 1\n    a = 16807\n    \n    # Generate 2*T uniform random numbers\n    uniforms = np.zeros(2 * T)\n    U_k = s\n    for k in range(2 * T):\n        U_k = (a * U_k) % m\n        uniforms[k] = U_k / m\n\n    # Generate 2*T standard normal random numbers using Box-Muller transform\n    normals = np.zeros(2 * T)\n    for j in range(T):\n        R1 = uniforms[2*j]\n        R2 = uniforms[2*j+1]\n        \n        log_R1 = np.log(R1)\n        term1 = np.sqrt(-2.0 * log_R1)\n        term2 = 2.0 * np.pi * R2\n\n        normals[2*j] = term1 * np.cos(term2)\n        normals[2*j+1] = term1 * np.sin(term2)\n    \n    epsilons = normals[:T]\n    etas = normals[T:]\n\n    # Generate time series x_t and y_t\n    x = np.zeros(T, dtype=np.float64)\n    y = np.zeros(T, dtype=np.float64)\n    \n    # x_0 = 0 is default\n    for t in range(1, T):\n        x[t] = x[t-1] + sigma_e * epsilons[t]\n\n    for t in range(T):\n        beta_t = beta_1 if t <= T_b else beta_2\n        y[t] = alpha + beta_t * x[t] + sigma_u * etas[t]\n\n    # --- 2. Break Detection ---\n    \n    # Array to store indicators of threshold exceedance\n    z_indicators = np.zeros(T, dtype=int)\n    \n    # Initialize running sums for recursive OLS\n    s_x, s_y, s_x2, s_y2, s_xy = 0.0, 0.0, 0.0, 0.0, 0.0\n    \n    # Pre-computation for the initial window up to T_min\n    for t in range(T_min):\n        s_x += x[t]\n        s_y += y[t]\n        s_x2 += x[t]**2\n        s_y2 += y[t]**2\n        s_xy += x[t] * y[t]\n\n    # Recursive estimation and forecast error calculation\n    for t in range(T_min, T - 1):\n        # Update sums with the value at time t\n        s_x += x[t]\n        s_y += y[t]\n        s_x2 += x[t]**2\n        s_y2 += y[t]**2\n        s_xy += x[t] * y[t]\n        \n        n_t = t + 1\n        \n        # Calculate OLS estimates\n        x_bar = s_x / n_t\n        y_bar = s_y / n_t\n        \n        s_xx = s_x2 - n_t * x_bar**2\n        s_yy = s_y2 - n_t * y_bar**2\n        s_xy_t = s_xy - n_t * x_bar * y_bar\n        \n        # Avoid division by zero, though unlikely with a random walk\n        if s_xx == 0:\n            continue\n            \n        beta_hat = s_xy_t / s_xx\n        alpha_hat = y_bar - beta_hat * x_bar\n        \n        # Calculate residual standard deviation\n        ssr = s_yy - beta_hat * s_xy_t\n        # Ensure non-negativity due to potential floating point inaccuracies\n        ssr = max(0, ssr)\n        \n        df = n_t - 2\n        if df <= 0:\n            continue\n            \n        sigma_hat_sq = ssr / df\n        \n        if sigma_hat_sq <= 0:\n            continue \n        \n        sigma_hat = np.sqrt(sigma_hat_sq)\n        \n        # Calculate standardized one-step-ahead forecast error for time t+1\n        forecast_error = y[t+1] - (alpha_hat + beta_hat * x[t+1])\n        z_t_plus_1 = forecast_error / sigma_hat if sigma_hat > 0 else np.inf\n        \n        if np.abs(z_t_plus_1) >= c:\n            z_indicators[t + 1] = 1\n\n    # --- 3. Search for Break Index ---\n    \n    # Search for the first run of L consecutive indicators\n    # The search range for the start of the run (s_star)\n    # is from T_min+1 to T-L (inclusive).\n    for s_star in range(T_min + 1, T - L + 1):\n        # Check for a run of length L\n        if np.all(z_indicators[s_star : s_star + L] == 1):\n            return s_star\n\n    return -1\n\nif __name__ == '__main__':\n    solve()\n```"}]}