{"hands_on_practices": [{"introduction": "本练习将引导您从零开始构建一个多元线性回归模型。您将运用基本的线性代数原理，而不是依赖“黑箱”软件包，来估计一个移动应用程序的需求。这项实践旨在加深您对普通最小二乘法（OLS）核心机制的理解，并涵盖从模型拟合、预测到评估和诊断等一系列基本技能。[@problem_id:2413185]", "id": "2413185", "problem": "要求您在计算金融情景下实现并应用多元线性回归。任务是建立一个移动应用需求的普通最小二乘法（OLS）模型，其中目标变量是下载量。预测变量是价格（单位：美元，USD）、用户评分（$1$至$5$分制）以及社交媒体广告支出（单位：美元，USD）。模型必须包含一个截距项。所有预测的下载量必须以计数（无单位）表示，所有货币输入均为美元。不涉及任何角度。所有输出必须是指定的布尔、整数或浮点数类型的数值。\n\n您必须从基本定义出发，将OLS视为最小化训练数据上残差平方和的解。除了数值线性代数的基本功能外，您不得依赖任何预先封装的黑盒模型。您必须使用提供的训练数据来估计模型，然后回答接下来的测试套件。\n\n训练数据：每行给出 $(\\text{价格}, \\text{用户评分}, \\text{社交媒体广告支出}) \\to \\text{下载量}$，其中下载量是计数。\n\n- 第 $1$ 行：$(0.99, 4.5, 12000) \\to 31320$\n- 第 $2$ 行：$(1.99, 4.2, 8000) \\to 24620$\n- 第 $3$ 行：$(0.00, 3.8, 5000) \\to 25700$\n- 第 $4$ 行：$(2.99, 4.7, 15000) \\to 29620$\n- 第 $5$ 行：$(4.99, 4.9, 20000) \\to 30970$\n- 第 $6$ 行：$(1.49, 3.5, 0) \\to 17370$\n- 第 $7$ 行：$(0.00, 4.0, 7000) \\to 27950$\n- 第 $8$ 行：$(3.99, 4.4, 11000) \\to 23820$\n- 第 $9$ 行：$(2.49, 3.9, 6000) \\to 21570$\n- 第 $10$ 行：$(0.99, 4.8, 18000) \\to 36720$\n- 第 $11$ 行：$(1.99, 4.1, 4000) \\to 21440$\n- 第 $12$ 行：$(0.49, 3.6, 2000) \\to 21540$\n\n用于样本外评估的验证数据：每行给出 $(\\text{价格}, \\text{用户评分}, \\text{社交媒体广告支出}) \\to \\text{下载量}$。\n\n- V$1$：$(2.99, 4.0, 9000) \\to 23320$\n- V$2$：$(0.00, 4.9, 16000) \\to 37300$\n- V$3$：$(1.49, 3.7, 4000) \\to 21370$\n\n您必须基于多元线性回归的第一性原理，实现以下内容：\n\n- 通过最小化训练集上的残差平方和，拟合一个将下载量作为截距、价格、用户评分和广告支出的函数的线性模型。\n- 仅使用训练集计算估计的系数向量和残差方差。\n- 使用拟合好的模型计算以下测试套件所需的量。\n\n测试套件和要求输出：\n\n设 $n$ 表示训练观测点的数量，$k$ 表示模型参数的数量（包括截距）。这里 $n=12$，$k=4$。\n\n您的程序必须按顺序计算以下五个结果：\n\n$1.$ 预测任务（正常路径）：使用拟合的模型，预测一个新应用在 $(\\text{价格} = 1.99, \\text{用户评分} = 4.5, \\text{社交媒体广告支出} = 10000)$ 情况下的下载量。输出四舍五入到最接近的整数（计数）的预测值。\n\n$2.$ 系数提取：输出价格的估计系数，以浮点数形式表示，保留 $2$ 位小数。\n\n$3.$ 样本外拟合优度：计算验证集上的决定系数，定义为 $R^{2}_{\\text{test}} = 1 - \\dfrac{\\sum_{i \\in \\text{test}} (y_i - \\hat{y}_i)^2}{\\sum_{i \\in \\text{test}} (y_i - \\bar{y}_{\\text{test}})^2}$，其中 $\\bar{y}_{\\text{test}}$ 是验证目标的均值。以浮点数形式输出，保留 $4$ 位小数。\n\n$4.$ 价格的统计显著性（边缘情况：小残差方差）：使用双边 Student's $t$检验，在显著性水平 $\\alpha = 0.05$ 和自由度为 $n-k$ 的条件下，检验价格系数等于 $0$ 的原假设，其备择假设为价格系数不等于 $0$。输出一个布尔值：如果显著则为 $True$，否则为 $False$。\n\n$5.$ 数值稳定性诊断（多重共线性检查）：使用 $2$-范数计算带截距项的训练设计矩阵的谱条件数，即其最大奇异值与最小奇异值之比。以浮点数形式输出，保留 $2$ 位小数。\n\n最终输出格式：\n\n您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，“[result1,result2,result3,result4,result5]”）。结果必须按指定的确切顺序出现：预测值（整数）、价格系数（保留 $2$ 位小数的浮点数）、测试集 $R^{2}$（保留 $4$ 位小数的浮点数）、价格显著性布尔值、条件数（保留 $2$ 位小数的浮点数）。", "solution": "此问题需要进行验证。\n\n### 步骤 1：提取已知信息\n- **任务**：使用普通最小二乘法（OLS）实现并应用多元线性回归，以对移动应用需求进行建模。\n- **目标变量 ($y$)**：下载量（计数）。\n- **预测变量 ($x_j$)**：价格（美元）、用户评分（$1-5$ 分制）、广告支出（美元）。\n- **模型设定**：包含截距项的线性模型。\n- **方法**：最小化残差平方和。不得使用“黑盒”模型，仅限使用线性代数基本功能。\n- **训练数据**（$n=12$ 个观测值）：\n  - $(\\text{价格}, \\text{用户评分}, \\text{广告支出}) \\to \\text{下载量}$\n  - $(0.99, 4.5, 12000) \\to 31320$\n  - $(1.99, 4.2, 8000) \\to 24620$\n  - $(0.00, 3.8, 5000) \\to 25700$\n  - $(2.99, 4.7, 15000) \\to 29620$\n  - $(4.99, 4.9, 20000) \\to 30970$\n  - $(1.49, 3.5, 0) \\to 17370$\n  - $(0.00, 4.0, 7000) \\to 27950$\n  - $(3.99, 4.4, 11000) \\to 23820$\n  - $(2.49, 3.9, 6000) \\to 21570$\n  - $(0.99, 4.8, 18000) \\to 36720$\n  - $(1.99, 4.1, 4000) \\to 21440$\n  - $(0.49, 3.6, 2000) \\to 21540$\n- **验证数据**（$3$ 个观测值）：\n  - $(2.99, 4.0, 9000) \\to 23320$\n  - $(0.00, 4.9, 16000) \\to 37300$\n  - $(1.49, 3.7, 4000) \\to 21370$\n- **常数**：训练观测数量 $n=12$，模型参数数量 $k=4$。\n- **测试套件**：\n  1.  预测 $(\\text{价格}=1.99, \\text{用户评分}=4.5, \\text{广告支出}=10000)$ 时的下载量。输出：整数。\n  2.  提取价格的估计系数。输出：浮点数，保留 $2$ 位小数。\n  3.  在验证集上计算 $R^{2}_{\\text{test}} = 1 - \\dfrac{\\sum_{i \\in \\text{test}} (y_i - \\hat{y}_i)^2}{\\sum_{i \\in \\text{test}} (y_i - \\bar{y}_{\\text{test}})^2}$。输出：浮点数，保留 $4$ 位小数。\n  4.  使用双边 t 检验，在显著性水平 $\\alpha = 0.05$ 和自由度为 $n-k$ 的条件下，检验价格系数的显著性。输出：布尔值。\n  5.  计算训练设计矩阵的谱条件数。输出：浮点数，保留 $2$ 位小数。\n\n### 步骤 2：使用提取的已知信息进行验证\n该问题是多元线性回归的一个标准、明确定义的应用，这是统计学和计量经济学中的一个基本方法。\n- **科学依据**：该问题基于普通最小二乘法（OLS），这是一个核心且成熟的统计理论。其背景对于计算经济学和金融学而言是现实的。\n- **适定性**：该问题提供了足够的数据（12 个观测值用于 4 个参数）来估计模型系数。测试套件中的任务具体、明确，并且基于标准统计公式有唯一的解。\n- **客观性**：问题陈述使用精确、客观的语言编写。数据是数值型的，所需的计算基于既定的数学公式，而非主观解释。\n- 该问题没有违反任何无效标准。它不基于错误的前提，是可形式化的，数据是完整的，并且任务是可验证的。\n\n### 步骤 3：结论与行动\n该问题被判定为**有效**。将提供解决方案。\n\n### 解决方案\n该问题要求使用普通最小二乘法（OLS）拟合一个多元线性回归模型。该模型形式如下：\n$$ \\text{downloads} = \\beta_0 + \\beta_1 \\cdot \\text{price} + \\beta_2 \\cdot \\text{user\\_rating} + \\beta_3 \\cdot \\text{ad\\_spend} + \\epsilon $$\n以矩阵形式表示，即 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$，其中 $\\mathbf{y}$ 是观测下载量的向量，$\\mathbf{X}$ 是设计矩阵，$\\boldsymbol{\\beta}$ 是系数向量，$\\boldsymbol{\\epsilon}$ 是误差向量。\n\nOLS 方法旨在找到能最小化残差平方和（SSR）$S = \\sum_{i=1}^{n} \\epsilon_i^2$ 的系数向量 $\\hat{\\boldsymbol{\\beta}}$。\n$$ S(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) $$\n为最小化此量，我们对 $\\boldsymbol{\\beta}$ 求梯度并令其为零：\n$$ \\frac{\\partial S}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{0} $$\n这得到了正规方程组：\n$$ (\\mathbf{X}^T\\mathbf{X})\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{y} $$\nOLS 估计量 $\\hat{\\boldsymbol{\\beta}}$ 的解由下式给出：\n$$ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} $$\n该方程将使用数值稳定的线性代数程序来求解。\n\n首先，我们根据 $n=12$ 个训练观测值构建设计矩阵 $\\mathbf{X}_{\\text{train}}$ 和目标向量 $\\mathbf{y}_{\\text{train}}$。矩阵 $\\mathbf{X}_{\\text{train}}$ 有 $k=4$ 列：一列用于截距（$\\beta_0$），其余三列分别对应三个预测变量。\n$$\n\\mathbf{y}_{\\text{train}} = \\begin{pmatrix} 31320 \\\\ 24620 \\\\ 25700 \\\\ 29620 \\\\ 30970 \\\\ 17370 \\\\ 27950 \\\\ 23820 \\\\ 21570 \\\\ 36720 \\\\ 21440 \\\\ 21540 \\end{pmatrix},\n\\quad\n\\mathbf{X}_{\\text{train}} = \\begin{pmatrix}\n1 & 0.99 & 4.5 & 12000 \\\\\n1 & 1.99 & 4.2 & 8000 \\\\\n1 & 0.00 & 3.8 & 5000 \\\\\n1 & 2.99 & 4.7 & 15000 \\\\\n1 & 4.99 & 4.9 & 20000 \\\\\n1 & 1.49 & 3.5 & 0 \\\\\n1 & 0.00 & 4.0 & 7000 \\\\\n1 & 3.99 & 4.4 & 11000 \\\\\n1 & 2.49 & 3.9 & 6000 \\\\\n1 & 0.99 & 4.8 & 18000 \\\\\n1 & 1.99 & 4.1 & 4000 \\\\\n1 & 0.49 & 3.6 & 2000 \\\\\n\\end{pmatrix}\n$$\n求解 $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2, \\hat{\\beta}_3]^T$ 的正规方程组，可以得到估计的系数向量。接下来我们着手解决测试套件中的问题。\n\n**1. 预测任务**\n一个新的观测值由预测向量 $(\\text{价格}=1.99, \\text{用户评分}=4.5, \\text{广告支出}=10000)$ 给出。我们构建一个带截距的设计向量 $\\mathbf{x}_{\\text{new}}$：\n$$ \\mathbf{x}_{\\text{new}} = \\begin{pmatrix} 1 & 1.99 & 4.5 & 10000 \\end{pmatrix}^T $$\n预测下载量 $\\hat{y}_{\\text{new}}$ 计算如下：\n$$ \\hat{y}_{\\text{new}} = \\mathbf{x}_{\\text{new}}^T \\hat{\\boldsymbol{\\beta}} $$\n结果四舍五入到最接近的整数。\n\n**2. 系数提取**\n价格的估计系数 $\\hat{\\beta}_1$ 是向量 $\\hat{\\boldsymbol{\\beta}}$ 的第二个元素。提取此值并保留 $2$ 位小数。\n\n**3. 样本外拟合优度**\n验证集的决定系数 $R^{2}_{\\text{test}}$ 计算如下：\n$$ R^{2}_{\\text{test}} = 1 - \\frac{SSR_{\\text{test}}}{SST_{\\text{test}}} $$\n其中，$SSR_{\\text{test}} = \\sum_{i \\in \\text{test}} (y_i - \\hat{y}_i)^2$ 是验证集上的残差平方和，$SST_{\\text{test}} = \\sum_{i \\in \\text{test}} (y_i - \\bar{y}_{\\text{test}})^2$ 是验证集的总平方和。$\\bar{y}_{\\text{test}}$ 是验证数据中观测下载量的均值。预测值 $\\hat{y}_i$ 使用拟合模型生成：$\\hat{\\mathbf{y}}_{\\text{test}} = \\mathbf{X}_{\\text{test}}\\hat{\\boldsymbol{\\beta}}$。结果保留 $4$ 位小数。\n\n**4. 价格的统计显著性**\n我们使用双边 Student's t 检验来检验原假设 $H_0: \\beta_1 = 0$ 与备择假设 $H_1: \\beta_1 \\neq 0$。t 统计量为：\n$$ t = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)} $$\n系数的标准误 $SE(\\hat{\\beta}_1)$ 是系数协方差矩阵 $\\text{Cov}(\\hat{\\boldsymbol{\\beta}}) = \\hat{\\sigma}^2 (\\mathbf{X}_{\\text{train}}^T \\mathbf{X}_{\\text{train}})^{-1}$ 中相应对角线元素的平方根。残差方差 $\\hat{\\sigma}^2$ 是误差方差 $\\sigma^2$ 的一个无偏估计量，由训练数据计算得出：\n$$ \\hat{\\sigma}^2 = \\frac{SSR_{\\text{train}}}{n-k} = \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{n-k} $$\nt 分布的自由度为 $df = n-k = 12-4=8$。我们将计算出的 t 统计量的绝对值 $|t|$ 与在显著性水平 $\\alpha = 0.05$ 和自由度 $df=8$ 下的 t 分布临界值 $t_{\\text{crit}}$ 进行比较。如果 $|t| > t_{\\text{crit}}$，我们拒绝 $H_0$，则该系数在统计上是显著的。\n\n**5. 数值稳定性诊断**\n训练设计矩阵 $\\mathbf{X}_{\\text{train}}$ 的谱条件数是其最大奇异值 ($\\sigma_{\\max}$)与最小奇异值 ($\\sigma_{\\min}$)之比：\n$$ \\kappa_2(\\mathbf{X}_{\\text{train}}) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}} $$\n该值是衡量预测变量多重共线性的一种度量。高值表示在估计系数时存在潜在的数值不稳定性。结果保留 $2$ 位小数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Implements and applies a multiple linear regression model from first principles.\n    \"\"\"\n    # Define the problem data as specified.\n    # Training Data (n=12)\n    y_train_list = [31320, 24620, 25700, 29620, 30970, 17370, 27950, 23820, 21570, 36720, 21440, 21540]\n    X_train_predictors_list = [\n        [0.99, 4.5, 12000], [1.99, 4.2, 8000], [0.00, 3.8, 5000], [2.99, 4.7, 15000],\n        [4.99, 4.9, 20000], [1.49, 3.5, 0], [0.00, 4.0, 7000], [3.99, 4.4, 11000],\n        [2.49, 3.9, 6000], [0.99, 4.8, 18000], [1.99, 4.1, 4000], [0.49, 3.6, 2000]\n    ]\n\n    # Validation Data\n    y_val_list = [23320, 37300, 21370]\n    X_val_predictors_list = [\n        [2.99, 4.0, 9000], [0.00, 4.9, 16000], [1.49, 3.7, 4000]\n    ]\n\n    # New app data for prediction\n    x_new_predictors_list = [1.99, 4.5, 10000]\n    \n    # Convert lists to numpy arrays\n    y_train = np.array(y_train_list, dtype=float)\n    X_train_predictors = np.array(X_train_predictors_list, dtype=float)\n    y_val = np.array(y_val_list, dtype=float)\n    X_val_predictors = np.array(X_val_predictors_list, dtype=float)\n    x_new_predictors = np.array(x_new_predictors_list, dtype=float)\n\n    # Add intercept column to design matrices\n    n_train = X_train_predictors.shape[0]\n    n_val = X_val_predictors.shape[0]\n    X_train = np.hstack([np.ones((n_train, 1)), X_train_predictors])\n    X_val = np.hstack([np.ones((n_val, 1)), X_val_predictors])\n    x_new = np.hstack([1, x_new_predictors])\n    \n    # --- Model Fitting: Solve Normal Equations ---\n    # (X.T @ X) @ beta = X.T @ y\n    XTX = X_train.T @ X_train\n    XTy = X_train.T @ y_train\n    beta_hat = np.linalg.solve(XTX, XTy)\n\n    # --- Test Suite Computations ---\n    results = []\n\n    # 1. Prediction task\n    y_pred_new = x_new @ beta_hat\n    result1 = int(round(y_pred_new))\n    results.append(result1)\n\n    # 2. Coefficient extraction (price)\n    price_coeff = beta_hat[1]\n    result2 = round(price_coeff, 2)\n    results.append(result2)\n\n    # 3. Out-of-sample R-squared\n    y_pred_val = X_val @ beta_hat\n    residuals_val = y_val - y_pred_val\n    ssr_val = np.sum(residuals_val**2)\n    y_mean_val = np.mean(y_val)\n    sst_val = np.sum((y_val - y_mean_val)**2)\n    r2_val = 1 - (ssr_val / sst_val)\n    result3 = round(r2_val, 4)\n    results.append(result3)\n    \n    # 4. Statistical significance of price\n    n = n_train\n    k = X_train.shape[1] # number of parameters\n    df = n - k\n    y_pred_train = X_train @ beta_hat\n    residuals_train = y_train - y_pred_train\n    ssr_train = np.sum(residuals_train**2)\n    sigma_sq_hat = ssr_train / df\n    XTX_inv = np.linalg.inv(XTX)\n    var_beta_hat = sigma_sq_hat * XTX_inv\n    se_price_coeff = np.sqrt(var_beta_hat[1, 1])\n    t_statistic = price_coeff / se_price_coeff\n    alpha = 0.05\n    t_critical = t.ppf(1 - alpha/2, df)\n    result4 = bool(abs(t_statistic) > t_critical)\n    results.append(result4)\n    \n    # 5. Numerical stability diagnostic (condition number)\n    cond_num = np.linalg.cond(X_train, 2)\n    result5 = round(cond_num, 2)\n    results.append(result5)\n\n    # Final print statement in the exact required format.\n    # Example format: [28132,-1803.97,0.9744,True,1367.65]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "在回归分析中，正确编码分类变量至关重要，错误的编码方式可能导致被称为“虚拟变量陷阱”的致命问题。本练习通过在薪资模型中尝试不同的方式引入性别虚拟变量，来探讨完全多重共线性这一概念。通过分析设计矩阵的秩及其对系数估计的影响，您将深刻地理解为何某些模型设定无效，并掌握正确处理分类数据的方法。[@problem_id:2413177]", "id": "2413177", "problem": "您正在研究一个关于横截面数据集中小时工资的线性模型，该模型包含一个二元性别指示变量。设 $y \\in \\mathbb{R}^n$ 是工资向量，设 $X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，其列是一个截距项（如果存在，则为一列1）、男性虚拟变量和女性虚拟变量，具体取决于不同情况。考虑普通最小二乘法 (OLS) 问题，即在 $\\beta \\in \\mathbb{R}^p$ 上最小化残差平方和 $S(\\beta) = \\lVert y - X \\beta \\rVert_2^2$。如果存在唯一的最小化子，则用 $\\hat{\\beta}$ 表示。如果最小化子集合不是单元素集，则将该情况报告的系数向量定义为 $S(\\beta)$ 的所有最小化子中具有最小欧几里得范数 $\\lVert \\beta \\rVert_2$ 的那个。对于下面的每个测试用例，您必须计算 $X$ 的列秩 $r$，确定格拉姆矩阵 $X^\\top X$ 是否可逆（等价于 $r=p$ 是否成立），并按规定计算系数向量。如果 $X^\\top X$ 可逆，则将可逆性指标报告为整数 $1$，否则报告为 $0$。所有系数值必须四舍五入到小数点后4位。\n\n测试套件。对于每种情况，设计矩阵 $X$ 和响应向量 $y$ 都已明确给出。系数的顺序必须与该情况下 $X$ 的列顺序相匹配。\n\n- 情况 A（尝试在同时存在两种性别时，对截距项、男性虚拟变量和女性虚拟变量进行回归）。此处 $n=6$ 且 $p=3$，其中\n$$\nX_A=\\begin{bmatrix}\n1 & 1 & 0\\\\\n1 & 0 & 1\\\\\n1 & 1 & 0\\\\\n1 & 0 & 1\\\\\n1 & 1 & 0\\\\\n1 & 0 & 1\n\\end{bmatrix},\\quad\ny_A=\\begin{bmatrix}\n21\\\\\n19\\\\\n23\\\\\n18\\\\\n22\\\\\n20\n\\end{bmatrix}.\n$$\n\n- 情况 B（使用截距项和单个虚拟变量进行正确编码）。此处 $n=6$ 且 $p=2$，其中\n$$\nX_B=\\begin{bmatrix}\n1 & 1\\\\\n1 & 0\\\\\n1 & 1\\\\\n1 & 0\\\\\n1 & 1\\\\\n1 & 0\n\\end{bmatrix},\\quad\ny_B=\\begin{bmatrix}\n21\\\\\n19\\\\\n23\\\\\n18\\\\\n22\\\\\n20\n\\end{bmatrix}.\n$$\n系数顺序为 $[\\text{截距项},\\text{男性}]$。\n\n- 情况 C（当所有观测值均为男性时，尝试对截距项、男性虚拟变量和女性虚拟变量进行回归）。此处 $n=4$ 且 $p=3$，其中\n$$\nX_C=\\begin{bmatrix}\n1 & 1 & 0\\\\\n1 & 1 & 0\\\\\n1 & 1 & 0\\\\\n1 & 1 & 0\n\\end{bmatrix},\\quad\ny_C=\\begin{bmatrix}\n21\\\\\n22\\\\\n20\\\\\n23\n\\end{bmatrix}.\n$$\n\n- 情况 D（无截距项，但存在两个虚拟变量）。此处 $n=6$ 且 $p=2$，其中\n$$\nX_D=\\begin{bmatrix}\n1 & 0\\\\\n0 & 1\\\\\n1 & 0\\\\\n0 & 1\\\\\n1 & 0\\\\\n0 & 1\n\\end{bmatrix},\\quad\ny_D=\\begin{bmatrix}\n21\\\\\n19\\\\\n23\\\\\n18\\\\\n22\\\\\n20\n\\end{bmatrix}.\n$$\n系数顺序为 $[\\text{男性},\\text{女性}]$。\n\n要求的最终输出格式。您的程序应生成单行输出，其中包含4个测试用例的结果，格式为方括号括起来的逗号分隔列表。每个结果本身必须是一个形式为 $[r, \\text{invertibility\\_indicator}, [\\beta_1,\\dots,\\beta_p]]$ 的列表，其中 $r$ 是一个整数，可逆性指标是整数 $1$ 或 $0$，系数子列表的条目是四舍五入到小数点后4位的浮点数。例如，整体输出必须如下所示\n$$\n[[r_A, \\text{inv}_A, [\\dots]], [r_B, \\text{inv}_B, [\\dots]], [r_C, \\text{inv}_C, [\\dots]], [r_D, \\text{inv}_D, [\\dots]]].\n$$", "solution": "该问题要求分析线性回归模型的四种不同设定。目标是为每种情况确定设计矩阵 $X$ 的秩、格拉姆矩阵 $X^\\top X$ 的可逆性以及估计的系数向量 $\\beta$。该模型是标准的普通最小二乘法 (OLS) 设置，我们旨在找到一个向量 $\\beta \\in \\mathbb{R}^p$ 来最小化残差平方和 $S(\\beta)$：\n$$ S(\\beta) = \\lVert y - X\\beta \\rVert_2^2 = (y - X\\beta)^\\top (y - X\\beta) $$\n其中 $y \\in \\mathbb{R}^n$ 是观测向量，$X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵。\n\n$S(\\beta)$ 的一个最小化子 $\\hat{\\beta}$ 必须满足正规方程组：\n$$ (X^\\top X)\\hat{\\beta} = X^\\top y $$\n解的性质取决于矩阵 $X^\\top X$。\n\n如果 $X$ 的列是线性无关的，则矩阵 $X$ 是满列秩的，即其秩 $r$ 等于列数 $p$。在这种情况下，格拉姆矩阵 $X^\\top X$ 是一个 $p \\times p$ 的正定矩阵，因此是可逆的。正规方程组有唯一解，由下式给出：\n$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y $$\n\n如果 $X$ 的列是线性相关的，则矩阵 $X$ 是秩亏的，即 $r < p$。这种情况被称为完全多重共线性。格拉姆矩阵 $X^\\top X$ 是奇异的且不可逆。正规方程组是相容的，但有无穷多个解。问题为这种情况指定了一个唯一的解选择标准：我们必须选择 $S(\\beta)$ 的最小化子中欧几里得范数 $\\lVert \\beta \\rVert_2$ 最小的那个。这个特定的解由下式给出：\n$$ \\hat{\\beta} = X^+ y $$\n其中 $X^+$ 是 $X$ 的 Moore-Penrose 伪逆。这是稳健的数值线性代数程序为最小二乘问题提供的标准解。\n\n我们现在开始分析每种情况。\n\n**情况 A**\n设计矩阵 $X_A$ 和响应向量 $y_A$ 由下式给出：\n$$ X_A=\\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 0 \\\\ 1 & 0 & 1 \\end{bmatrix},\\quad y_A=\\begin{bmatrix} 21 \\\\ 19 \\\\ 23 \\\\ 18 \\\\ 22 \\\\ 20 \\end{bmatrix} $$\n$X_A$ 的列对应于一个截距项、一个男性虚拟变量和一个女性虚拟变量。参数数量为 $p=3$。一个线性相关关系立即可见：第一列（截距项）是第二列和第三列（男性和女性虚拟变量）的和。这是经典的“虚拟变量陷阱”。由于这种多重共线性，这些列是线性相关的。$X_A$ 的秩，记为 $r_A$，必须小于 $p$。由于第二列和第三列是线性无关的，所以秩为 $r_A = 2$。\n由于 $r_A < p$，格拉姆矩阵 $X_A^\\top X_A$ 是奇异的，因此其可逆性指标为 $0$。系数向量 $\\hat{\\beta}_A$ 是最小范数解。所有最小二乘解的集合由方程组 $\\hat{\\beta}_0 + \\hat{\\beta}_1 = \\bar{y}_{\\text{male}} = 22$ 和 $\\hat{\\beta}_0 + \\hat{\\beta}_2 = \\bar{y}_{\\text{female}} = 19$ 定义。解集可以用 $\\beta_0$ 参数化：$[\\beta_0, 22-\\beta_0, 19-\\beta_0]^\\top$。最小化该向量的范数 $\\beta_0^2 + (22-\\beta_0)^2 + (19-\\beta_0)^2$，得到 $\\beta_0 = 41/3$。对应的最小范数系数向量是 $\\hat{\\beta}_A = [41/3, 25/3, 16/3]^\\top$。\n数值上，我们发现：\n- $r_A = 2$\n- 可逆性指标：$0$\n- $\\hat{\\beta}_A \\approx [13.6667, 8.3333, 5.3333]$\n\n**情况 B**\n设计矩阵 $X_B$ 和响应向量 $y_B$ 为：\n$$ X_B=\\begin{bmatrix} 1 & 1 \\\\ 1 & 0 \\\\ 1 & 1 \\\\ 1 & 0 \\\\ 1 & 1 \\\\ 1 & 0 \\end{bmatrix},\\quad y_B=\\begin{bmatrix} 21 \\\\ 19 \\\\ 23 \\\\ 18 \\\\ 22 \\\\ 20 \\end{bmatrix} $$\n这里，$p=2$。这些列对应于一个截距项和一个男性虚拟变量。女性类别是参照组。这两列是线性无关的。因此，该矩阵具有满列秩，$r_B = 2 = p$。\n因此，$X_B^\\top X_B$ 是可逆的，可逆性指标为 $1$。计算唯一的 OLS 解。$\\hat{\\beta}_0$ 估计了参照组（女性）的平均工资，而 $\\hat{\\beta}_1$ 估计了男性和女性平均工资之间的差异。\n女性平均工资：$(\\frac{19+18+20}{3}) = 19$。因此，$\\hat{\\beta}_0 = 19$。\n男性平均工资：$(\\frac{21+23+22}{3}) = 22$。因此，$\\hat{\\beta}_0 + \\hat{\\beta}_1 = 22$，这意味着 $\\hat{\\beta}_1 = 22 - 19 = 3$。\n- $r_B = 2$\n- 可逆性指标：$1$\n- $\\hat{\\beta}_B = [19.0000, 3.0000]$\n\n**情况 C**\n设计矩阵 $X_C$ 和响应向量 $y_C$ 为：\n$$ X_C=\\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 1 & 1 & 0 \\end{bmatrix},\\quad y_C=\\begin{bmatrix} 21 \\\\ 22 \\\\ 20 \\\\ 23 \\end{bmatrix} $$\n参数数量为 $p=3$。所有观测值都是男性的，这导致了两种形式的完全多重共线性。首先，截距项列与男性虚拟变量列相同。其次，女性虚拟变量列是一个零向量。列空间由一个全为1的向量张成。因此，秩为 $r_C = 1$。\n由于 $r_C < p$，矩阵 $X_C^\\top X_C$ 是奇异的，可逆性指标为 $0$。我们必须找到最小范数解。该模型实际上是试图使用预测变量的线性组合来估计平均工资 $\\bar{y}_C = \\frac{21+22+20+23}{4} = 21.5$。任何观测值的预测值为 $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1$。因此，任何解都必须满足 $\\hat{\\beta}_0 + \\hat{\\beta}_1 = 21.5$。系数 $\\hat{\\beta}_2$ 不影响残差，因此为了最小范数，它必须为 $0$。然后我们在约束条件 $\\beta_0 + \\beta_1 = 21.5$ 下最小化 $\\beta_0^2 + \\beta_1^2$，这得到 $\\hat{\\beta}_0 = \\hat{\\beta}_1 = 21.5 / 2 = 10.75$。\n- $r_C = 1$\n- 可逆性指标：$0$\n- $\\hat{\\beta}_C = [10.7500, 10.7500, 0.0000]$\n\n**情况 D**\n设计矩阵 $X_D$ 和响应向量 $y_D$ 为：\n$$ X_D=\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 0 & 1 \\end{bmatrix},\\quad y_D=\\begin{bmatrix} 21 \\\\ 19 \\\\ 23 \\\\ 18 \\\\ 22 \\\\ 20 \\end{bmatrix} $$\n这个模型省略了截距项，并为每个类别（男性和女性）都包含了一个虚拟变量。参数数量为 $p=2$。这些列是正交的，因此是线性无关的。该矩阵具有满列秩，$r_D = 2 = p$。\n因此，$X_D^\\top X_D$ 是可逆的（实际上，它是一个对角矩阵），可逆性指标为 $1$。存在唯一的 OLS 解。在这种参数化中，系数直接估计了每个组的条件均值。\n男性的系数 $\\hat{\\beta}_1$ 是男性的平均工资：$\\bar{y}_{\\text{male}} = 22$。\n女性的系数 $\\hat{\\beta}_2$ 是女性的平均工资：$\\bar{y}_{\\text{female}} = 19$。\n- $r_D = 2$\n- 可逆性指标：$1$\n- $\\hat{\\beta}_D = [22.0000, 19.0000]$", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the OLS problem for four different regression model specifications.\n\n    For each case, it computes:\n    1. The column rank 'r' of the design matrix X.\n    2. An invertibility indicator for the Gram matrix X.T @ X (1 if invertible, 0 otherwise).\n    3. The OLS coefficient vector beta. If the solution is not unique, the minimum\n       Euclidean norm solution is computed.\n\n    The results are formatted into a list of lists as required.\n    \"\"\"\n\n    def analyze_case(X, y):\n        \"\"\"\n        Analyzes a single OLS regression case.\n        \n        Args:\n            X (np.ndarray): The design matrix of shape (n, p).\n            y (np.ndarray): The response vector of shape (n,).\n            \n        Returns:\n            list: A list containing [rank, invertibility_indicator, coefficients].\n        \"\"\"\n        # Get the number of parameters p (number of columns)\n        p = X.shape[1]\n\n        # 1. Compute the column rank of X\n        r = np.linalg.matrix_rank(X)\n\n        # 2. Determine if X.T @ X is invertible\n        # This is true if and only if X has full column rank (r == p).\n        invertibility_indicator = 1 if r == p else 0\n\n        # 3. Compute the coefficient vector beta\n        # np.linalg.lstsq correctly handles both full rank and rank-deficient cases.\n        # For rank-deficient cases, it returns the minimum L2-norm solution, which is\n        # the solution obtained via the Moore-Penrose pseudoinverse.\n        # rcond=None is specified to use the default machine precision-based cutoff.\n        beta, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n\n        # Round coefficients to 4 decimal places\n        beta_rounded = [round(b, 4) for b in beta]\n\n        return [r, invertibility_indicator, beta_rounded]\n\n    # Test Suite Definition\n    test_cases = [\n        # Case A: Intercept, male dummy, female dummy (dummy variable trap)\n        (\n            np.array([[1, 1, 0], [1, 0, 1], [1, 1, 0], [1, 0, 1], [1, 1, 0], [1, 0, 1]]),\n            np.array([21, 19, 23, 18, 22, 20])\n        ),\n        # Case B: Intercept and one dummy (correct specification)\n        (\n            np.array([[1, 1], [1, 0], [1, 1], [1, 0], [1, 1], [1, 0]]),\n            np.array([21, 19, 23, 18, 22, 20])\n        ),\n        # Case C: Intercept, male dummy, female dummy (all obs are male)\n        (\n            np.array([[1, 1, 0], [1, 1, 0], [1, 1, 0], [1, 1, 0]]),\n            np.array([21, 22, 20, 23])\n        ),\n        # Case D: No intercept, two dummies (alternative correct specification)\n        (\n            np.array([[1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1]]),\n            np.array([21, 19, 23, 18, 22, 20])\n        )\n    ]\n\n    results = []\n    for X, y in test_cases:\n        result = analyze_case(X, y)\n        results.append(result)\n        \n    # The str() representation of a list in Python matches the required format.\n    # Ex: str([2, 0, [13.6667, 8.3333, 5.3333]]) gives '[2, 0, [13.6667, 8.3333, 5.3333]]'\n    # We join these string representations with commas.\n    results_str = ','.join(map(str, results))\n\n    # Print the final output in the exact required format.\n    print(f\"[{results_str}]\")\n\nsolve()\n```"}, {"introduction": "经典线性回归模型的关键假设之一是同方差性，即误差项的方差保持不变。本练习通过蒙特卡洛模拟，来展示当这一假设被违背时（即出现异方差性，一种在经济数据中常见的情况）会发生什么。您将比较经典标准误与异方差稳健（HC）标准误的表现，并亲眼见证为何在现代应用计量经济学中，稳健性方法对于获得可靠的统计推断至关重要。[@problem_id:2413193]", "id": "2413193", "problem": "考虑一个包含异方差扰动的线性回归模型。对于每个观测值 $i \\in \\{1,\\dots,n\\}$，设数据生成过程为\n$$\ny_i = \\beta_0 + \\beta_1 \\cdot \\text{income}_i + \\beta_2 \\cdot \\text{educ}_i + \\varepsilon_i,\n$$\n满足外生性条件 $E[\\varepsilon_i \\mid X] = 0$ 和异方差结构\n$$\n\\operatorname{Var}(\\varepsilon_i \\mid X) = \\sigma^2 \\cdot \\text{income}_i,\n$$\n其中 $X$ 表示包含截距项的完整回归变量矩阵。目标是使用蒙特卡洛模拟来证明，在异方差性存在时，经典的普通最小二乘法 (OLS) 标准误会失效，而异方差稳健 (HC) 标准误则能渐近地保持正确的覆盖率。\n\n您必须编写一个完整、可运行的程序，该程序能够：\n- 根据上述描述生成合成数据。\n- 通过 OLS 估计回归模型，并计算经典的仅同方差标准误和异方差稳健 (HC) 标准误。\n- 使用标准正态临界值为 $\\beta_1$ 构建名义覆盖率为 $0.95$ 的双侧置信区间。\n- 将实验重复 $R$ 次独立实验，并报告每种标准误方法的经验覆盖率（即包含真实 $\\beta_1$ 值的置信区间的比例）。\n\n使用以下基本原理进行设计和论证：\n- OLS 估计量定义为残差平方和的最小化器。残差是观测值 $y_i$ 与回归变量线性组合所蕴含的拟合值之间的偏差。\n- 经典的仅同方差标准误要求假设 $\\operatorname{Var}(\\varepsilon \\mid X) = \\sigma^2 I$，当方差依赖于回归变量时，该标准误是不一致的。\n- 异方差稳健 (HC) 标准误用一个在 $\\operatorname{Var}(\\varepsilon \\mid X)$ 是对角矩阵但与单位矩阵不成比例时仍保持一致性的估计量来替代不正确的同方差残差协方差。\n\n每次重复实验的数据生成细节：\n- 从形状参数 $k = 4$、尺度参数 $\\theta = 12$ 的伽马分布中独立抽取 $\\text{income}_i$（因此 $E[\\text{income}_i] = k \\theta = 48$）。这一选择确保了 $\\text{income}_i$ 严格为正且具有符合现实的右偏性。\n- 从均值为 $14$、标准差为 $2$ 的正态分布中独立抽取 $\\text{educ}_i$，然后将其裁剪到区间 $[8, 22]$ 内以避免不切实际的数值。\n- 将真实参数向量设为 $(\\beta_0,\\beta_1,\\beta_2) = (1, 0.8, 0.5)$。\n- 对于每个 $i$，从均值为 $0$、方差为 $\\sigma^2 \\cdot \\text{income}_i$ 的正态分布中抽取 $\\varepsilon_i$。\n- 相应地构造 $y_i$。\n\n置信区间：\n- 在每次重复实验中，通过 OLS 估计模型后，为 $\\beta_1$ 计算两个置信区间：一个使用经典的仅同方差标准误，另一个使用异方差稳健 (HC) 标准误。在两种情况下，都使用对应于名义覆盖率 $0.95$ 的标准正态临界值（即双侧临界值 $z_{0.975}$）。\n- 记录每个区间是否包含真实的 $\\beta_1$。\n\n测试套件：\n对以下参数集 $(n, \\sigma^2, R)$ 运行蒙特卡洛模拟：\n- 情况 A (标准情形): $(n, \\sigma^2, R) = (500, 1.0, 1000)$。\n- 情况 B (小样本): $(n, \\sigma^2, R) = (60, 1.0, 1000)$。\n- 情况 C (更严重的异方差性): $(n, \\sigma^2, R) = (500, 3.0, 1000)$。\n- 情况 D (较温和的异方差性): $(n, \\sigma^2, R) = (500, 0.2, 1000)$。\n\n随机性与可复现性：\n- 为伪随机数生成器使用固定的种子 $20240519$，以确保结果可以精确复现。\n\n程序输出要求：\n- 对于每个测试用例，报告三项内容：使用仅同方差标准误的经验覆盖率（一个浮点数），使用 HC 稳健标准误的经验覆盖率（一个浮点数），以及一个布尔值，指示稳健覆盖率是否比同方差覆盖率更接近名义值 $0.95$（即 $|\\text{robust} - 0.95| < |\\text{classical} - 0.95|$）。\n- 您的程序应生成单行输出，其中包含一个由方括号括起来的、逗号分隔的结果列表，每个测试用例对应一个内部列表，例如：\n\"[[cA_classical,cA_robust,bA],[cB_classical,cB_robust,bB],[cC_classical,cC_robust,bC],[cD_classical,cD_robust,bD]]\"\n- 所有覆盖率必须以小数（而非百分比）形式报告。\n\n角度单位不适用。不需要物理单位。\n\n您的程序必须是完整且可直接运行的，无需任何用户输入或外部文件，并且必须严格遵守指定的输出格式。", "solution": "所述问题是有效的。它提出了一个定义明确的标准计量经济学练习，旨在展示异方差性对普通最小二乘法 (OLS) 估计量所做推断的影响。其数据生成过程有完整的规定，理论概念合理，目标清晰且可量化。任务是执行一次蒙特卡洛模拟，这是计算统计学和计量经济学中用于评估估计量和统计检验的有限样本性质的一项基本技术。\n\n我们首先将指定的线性回归模型形式化。对于每个观测值 $i=1, \\dots, n$，模型为：\n$$\ny_i = \\beta_0 + \\beta_1 \\cdot \\text{income}_i + \\beta_2 \\cdot \\text{educ}_i + \\varepsilon_i\n$$\n用矩阵表示法，可以写成 $y = X\\beta + \\varepsilon$，其中 $y$ 是因变量观测值的 $n \\times 1$ 向量，$X$ 是回归变量的 $n \\times p$ 矩阵（$p=3$），$\\beta$ 是真实参数的 $p \\times 1$ 向量，$\\varepsilon$ 是扰动的 $n \\times 1$ 向量。观测值 $i$ 的回归变量矩阵为 $x_i^T = [1, \\text{income}_i, \\text{educ}_i]$。\n\n问题对误差项 $\\varepsilon$ 提出了两个关键假设：\n$1$. 外生性条件：$E[\\varepsilon \\mid X] = 0$。这确保了 OLS 估计量是无偏的。\n$2$. 一种特定形式的异方差性：$\\operatorname{Var}(\\varepsilon_i \\mid X) = \\sigma_i^2 = \\sigma^2 \\cdot \\text{income}_i$。这意味着误差项的方差不是恒定的，而是依赖于其中一个回归变量。误差向量的完整协方差矩阵为 $\\Omega = \\operatorname{Var}(\\varepsilon \\mid X) = \\operatorname{diag}(\\sigma_1^2, \\dots, \\sigma_n^2)$，它与单位矩阵 $I_n$ 不成比例。\n\n$\\beta$ 的 OLS 估计量是通过最小化残差平方和 $S(\\beta) = (y - X\\beta)^T(y - X\\beta)$ 推导出来的。其众所周知的解是：\n$$\n\\hat{\\beta}_{OLS} = (X^T X)^{-1} X^T y\n$$\n在外生性假设下，此估计量是无偏的，即 $E[\\hat{\\beta}_{OLS} \\mid X] = \\beta$。然而，为了进行有效的统计推断（构建置信区间和执行假设检验），我们需要一个关于 $\\hat{\\beta}_{OLS}$ 协方差矩阵的一致估计量。在以 $X$ 为条件时，OLS 估计量的真实协方差矩阵是：\n$$\n\\operatorname{Var}(\\hat{\\beta}_{OLS} \\mid X) = (X^T X)^{-1} X^T \\Omega X (X^T X)^{-1}\n$$\n\n该模拟比较了该协方差矩阵的两种估计量。\n\n首先，是经典的、仅同方差的方差估计量。该估计量错误地假设 $\\Omega = \\sigma^2 I_n$。其公式为：\n$$\n\\widehat{\\operatorname{Var}}_{classical}(\\hat{\\beta}) = \\hat{s}^2 (X^T X)^{-1}\n$$\n其中 $\\hat{s}^2 = \\frac{1}{n-p} \\sum_{i=1}^n \\hat{\\varepsilon}_i^2$ 是误差方差的标准无偏估计量，而 $\\hat{\\varepsilon}_i = y_i - x_i^T \\hat{\\beta}$ 是 OLS 残差。当存在异方差性时，此公式是不正确的，由此产生的标准误是有偏且不一致的。使用这些标准误构建的置信区间将不具有正确的名义覆盖概率。\n\n其次，是异方差稳健 (HC) 方差估计量，特别是 White (1980) 提出的 HC0 估计量。该估计量不要求同方差性假设，并为真实的协方差矩阵提供了一个一致的估计。它用 OLS 残差平方构成的对角矩阵 $\\hat{\\Omega} = \\operatorname{diag}(\\hat{\\varepsilon}_1^2, \\dots, \\hat{\\varepsilon}_n^2)$ 来替代真实公式中未知的 $\\Omega$。HC0 估计量是：\n$$\n\\widehat{\\operatorname{Var}}_{HC0}(\\hat{\\beta}) = (X^T X)^{-1} (X^T \\hat{\\Omega} X) (X^T X)^{-1}\n$$\n项 $X^T \\hat{\\Omega} X$ 通常被称为“三明治夹心项”。从该估计量派生出的标准误对于未知形式的异方差性是“稳健的”。渐近地，基于 HC 标准误的置信区间将达到正确的名义覆盖率。\n\n对于每个测试用例 $(n, \\sigma^2, R)$，模拟将按以下步骤进行：\n$1$. 该过程重复 $R$ 次实验。\n$2$. 在每次重复实验中，根据指定的数据生成过程，使用真实参数 $(\\beta_0, \\beta_1, \\beta_2) = (1, 0.8, 0.5)$ 生成一个大小为 $n$ 的合成数据集 $(y, X)$。\n$3$. 使用 OLS 估计模型以获得 $\\hat{\\beta}$ 和残差 $\\hat{\\varepsilon}$。\n$4$. 使用经典公式和 HC0 稳健公式计算 $\\hat{\\beta}_1$ 的标准误。这涉及取相应估计协方差矩阵的第二个对角元素（索引为1）的平方根。\n$5$. 为 $\\beta_1$ 构建两个 $95\\%$ 置信区间：$[\\hat{\\beta}_1 \\pm z_{0.975} \\cdot SE(\\hat{\\beta}_1)]$，其中 $z_{0.975}$ 是来自标准正态分布的临界值 ($z_{0.975} \\approx 1.95996$)，$SE(\\hat{\\beta}_1)$ 是相应的标准误估计值。\n$6$. 对每个区间，我们检查它是否包含真实值 $\\beta_1 = 0.8$。\n$7$. 所有重复实验结束后，计算每种方法的经验覆盖率，即包含真实参数的区间的比例。将这些覆盖率与名义水平 $0.95$ 进行比较。\n预期结果是，稳健置信区间的经验覆盖率将接近 $0.95$，而经典区间的覆盖率将显著偏离，从而证明了它们在异方差性下的失效。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef run_simulation(n, sigma2, R, true_beta, rng):\n    \"\"\"\n    Runs a single Monte Carlo simulation for a given parameter set.\n    \"\"\"\n    beta1_true = true_beta[1]\n    num_params = len(true_beta)\n    \n    # Pre-calculate the critical value for 95% CI\n    # z_crit = 1.959964 is norm.ppf(0.975)\n    z_crit = norm.ppf(1 - 0.05 / 2.0)\n\n    # Counters for coverage\n    classical_hits = 0\n    robust_hits = 0\n\n    for _ in range(R):\n        # 1. Generate data\n        income = rng.gamma(shape=4, scale=12, size=n)\n        educ = rng.normal(loc=14, scale=2, size=n)\n        educ = np.clip(educ, 8, 22)\n        \n        X = np.column_stack((np.ones(n), income, educ))\n        \n        # Generate heteroskedastic errors\n        error_variances = sigma2 * income\n        epsilon = rng.normal(loc=0, scale=np.sqrt(error_variances))\n        \n        y = X @ true_beta + epsilon\n\n        # 2. OLS Estimation\n        try:\n            XTX = X.T @ X\n            XTX_inv = np.linalg.inv(XTX)\n            beta_hat = XTX_inv @ X.T @ y\n        except np.linalg.LinAlgError:\n            continue # Skip iteration if X is singular\n\n        beta1_hat = beta_hat[1]\n        residuals = y - X @ beta_hat\n\n        # 3. Calculate Classical (Homoskedastic) Standard Errors\n        s2_classical = np.sum(residuals**2) / (n - num_params)\n        var_beta_classical = s2_classical * XTX_inv\n        se_beta1_classical = np.sqrt(var_beta_classical[1, 1])\n\n        # 4. Calculate HC0 (Robust) Standard Errors\n        # Efficiently compute the sandwich meat: X^T * diag(res^2) * X\n        sandwich_meat = X.T @ (X * (residuals**2)[:, np.newaxis])\n        var_beta_hc0 = XTX_inv @ sandwich_meat @ XTX_inv\n        \n        # Ensure variance is non-negative due to potential floating point errors\n        var_beta1_hc0 = var_beta_hc0[1, 1]\n        if var_beta1_hc0 < 0:\n            var_beta1_hc0 = 0\n        se_beta1_hc0 = np.sqrt(var_beta1_hc0)\n\n        # 5. Construct Confidence Intervals and check coverage\n        # Classical CI\n        ci_classical_lower = beta1_hat - z_crit * se_beta1_classical\n        ci_classical_upper = beta1_hat + z_crit * se_beta1_classical\n        if ci_classical_lower <= beta1_true <= ci_classical_upper:\n            classical_hits += 1\n\n        # Robust CI\n        ci_robust_lower = beta1_hat - z_crit * se_beta1_hc0\n        ci_robust_upper = beta1_hat + z_crit * se_beta1_hc0\n        if ci_robust_lower <= beta1_true <= ci_robust_upper:\n            robust_hits += 1\n\n    # 6. Calculate empirical coverages\n    coverage_classical = classical_hits / R\n    coverage_robust = robust_hits / R\n\n    # 7. Compare performance\n    is_robust_better = abs(coverage_robust - 0.95) < abs(coverage_classical - 0.95)\n\n    return [coverage_classical, coverage_robust, is_robust_better]\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: (n, sigma^2, R) = (500, 1.0, 1000)\n        (500, 1.0, 1000),\n        # Case B: (n, sigma^2, R) = (60, 1.0, 1000)\n        (60, 1.0, 1000),\n        # Case C: (n, sigma^2, R) = (500, 3.0, 1000)\n        (500, 3.0, 1000),\n        # Case D: (n, sigma^2, R) = (500, 0.2, 1000)\n        (500, 0.2, 1000),\n    ]\n\n    true_beta = np.array([1.0, 0.8, 0.5])\n    seed = 20240519\n    rng = np.random.default_rng(seed)\n\n    results = []\n    for n, sigma2, R in test_cases:\n        result = run_simulation(n, sigma2, R, true_beta, rng)\n        results.append(result)\n\n    # Format the output string exactly as required\n    output_str = \"[\"\n    for i, res in enumerate(results):\n        cov_c, cov_r, is_better = res\n        # Format boolean as lowercase 'true'/'false'\n        bool_str = 'true' if is_better else 'false'\n        output_str += f\"[{cov_c},{cov_r},{bool_str}]\"\n        if i < len(results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```"}]}