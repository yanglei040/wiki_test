{"hands_on_practices": [{"introduction": "在处理高度相关的预测变量时，岭回归（$L_2$ 正则化）和 LASSO（$L_1$ 正则化）的行为存在根本差异。本练习提供了一个清晰的计算场景，让你能够亲手验证 LASSO 的变量选择特性以及岭回归在面对共线性时的稳定性。通过这个具体的数值实践，你将对这两种流行正则化方法的核心工作原理获得更深入的直观理解。[@problem_id:2426291]", "id": "2426291", "problem": "一位分析师正在使用两个高度相关的预测变量，对一个投资组合的单期超额收益建立一个无截距项的线性预测模型，数据涵盖两个时期。设响应向量为 $y \\in \\mathbb{R}^{2}$，预测变量矩阵为 $X \\in \\mathbb{R}^{2 \\times 2}$，其列向量为 $x_{1}$ 和 $x_{2}$，由下式给出\n$$\ny=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}, \\quad x_{1}=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}, \\quad x_{2}=\\begin{pmatrix}2 \\\\ 1\\end{pmatrix}, \\quad X=\\begin{pmatrix}1 & 2 \\\\ 0 & 1\\end{pmatrix}.\n$$\n数据生成过程为 $y=x_{1}$ (无噪声)。考虑两种正则化估计量：\n- 最小绝对收缩和选择算子 (LASSO) 估计量 $\\hat{\\beta}^{\\text{LASSO}}(\\lambda)$，定义为下式的任意一个最小化子：\n$$\n\\min_{\\beta \\in \\mathbb{R}^{2}} \\; \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\n其中惩罚参数为 $\\lambda=\\frac{6}{5}$。\n- 岭回归估计量 $\\hat{\\beta}^{\\text{Ridge}}(\\alpha)$，定义为下式的唯一最小化子：\n$$\n\\min_{\\beta \\in \\mathbb{R}^{2}} \\; \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\alpha}{2} \\|\\beta\\|_{2}^{2},\n$$\n其中惩罚参数为 $\\alpha=\\frac{1}{2}$。\n\n精确计算这两个系数向量，并以单个行向量 $\\big[\\hat{\\beta}_{1}^{\\text{LASSO}}, \\hat{\\beta}_{2}^{\\text{LASSO}}, \\hat{\\beta}_{1}^{\\text{Ridge}}, \\hat{\\beta}_{2}^{\\text{Ridge}}\\big]$ 的形式报告。给出精确值，不要进行四舍五入。你的最终答案必须是这个单个行向量。", "solution": "该问题要求计算两种正则化线性回归的系数向量：岭估计量和LASSO估计量。问题定义明确，科学上合理，且所有必要数据均已提供。我们将分别求解每个估计量。\n\n给定的数据是：\n响应向量 $y = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n预测变量矩阵 $X = \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix}$。\n系数向量为 $\\beta = \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix}$。\n\n首先，我们计算一些必要的矩阵乘积。\n$X$ 的转置是 $X^T = \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\end{pmatrix}$。\nGram矩阵是 $X^T X = \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 2 \\\\ 2 & 5 \\end{pmatrix}$。\n乘积 $X^T y$ 是 $X^T y = \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$。\n\n**1. 岭回归估计量**\n\n岭回归的目标函数由下式给出：\n$$L_{\\text{Ridge}}(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\alpha}{2} \\|\\beta\\|_{2}^{2}$$\n其中惩罚参数为 $\\alpha = \\frac{1}{2}$。\n该目标函数是严格凸且可微的。通过将关于 $\\beta$ 的梯度设为零，可以找到最小化子 $\\hat{\\beta}^{\\text{Ridge}}$：\n$$\\nabla_{\\beta} L_{\\text{Ridge}} = -X^T(y - X\\beta) + \\alpha\\beta = 0$$\n整理各项可得岭回归的正规方程：\n$$(X^T X + \\alpha I) \\beta = X^T y$$\n其中 $I$ 是 $2 \\times 2$ 的单位矩阵。因此，解为：\n$$\\hat{\\beta}^{\\text{Ridge}} = (X^T X + \\alpha I)^{-1} X^T y$$\n我们代入给定值：\n$$X^T X + \\alpha I = \\begin{pmatrix} 1 & 2 \\\\ 2 & 5 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 + \\frac{1}{2} & 2 \\\\ 2 & 5 + \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} & 2 \\\\ 2 & \\frac{11}{2} \\end{pmatrix}$$\n为了求该矩阵的逆，我们首先计算其行列式：\n$$\\det(X^T X + \\alpha I) = \\left(\\frac{3}{2}\\right)\\left(\\frac{11}{2}\\right) - (2)(2) = \\frac{33}{4} - 4 = \\frac{33 - 16}{4} = \\frac{17}{4}$$\n逆矩阵为：\n$$(X^T X + \\alpha I)^{-1} = \\frac{1}{\\frac{17}{4}} \\begin{pmatrix} \\frac{11}{2} & -2 \\\\ -2 & \\frac{3}{2} \\end{pmatrix} = \\frac{4}{17} \\begin{pmatrix} \\frac{11}{2} & -2 \\\\ -2 & \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{22}{17} & -\\frac{8}{17} \\\\ -\\frac{8}{17} & \\frac{6}{17} \\end{pmatrix}$$\n现在我们可以计算岭估计量：\n$$\\hat{\\beta}^{\\text{Ridge}} = \\begin{pmatrix} \\frac{22}{17} & -\\frac{8}{17} \\\\ -\\frac{8}{17} & \\frac{6}{17} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{22}{17} - \\frac{16}{17} \\\\ -\\frac{8}{17} + \\frac{12}{17} \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{17} \\\\ \\frac{4}{17} \\end{pmatrix}$$\n因此，$\\hat{\\beta}_{1}^{\\text{Ridge}} = \\frac{6}{17}$ 且 $\\hat{\\beta}_{2}^{\\text{Ridge}} = \\frac{4}{17}$。\n\n**2. LASSO 估计量**\n\nLASSO的目标函数为：\n$$L_{\\text{LASSO}}(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$$\n其中惩罚参数为 $\\lambda = \\frac{6}{5}$。由于 $L_1$ 范数的不可微性，我们使用次梯度最优性条件。最小化子 $\\hat{\\beta}^{\\text{LASSO}}$ 必须满足 $0 \\in \\partial L_{\\text{LASSO}}(\\hat{\\beta})$。\n次梯度条件为 $X^T(y - X\\hat{\\beta})_j \\in \\lambda \\cdot \\partial |\\beta_j| |_{\\hat{\\beta}_j}$，对每个分量 $j \\in \\{1, 2\\}$ 均成立。这可以写作：\n$$\n\\begin{cases}\n(X^T(y - X\\hat{\\beta}))_j = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_j) & \\text{if } \\hat{\\beta}_j \\neq 0 \\\\\n|(X^T(y - X\\hat{\\beta}))_j| \\le \\lambda & \\text{if } \\hat{\\beta}_j = 0\n\\end{cases}\n$$\n让我们计算与残差的相关性向量，$c(\\beta) = X^T(y - X\\beta)$：\n$$c(\\beta) = X^T y - X^T X \\beta = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 1 & 2 \\\\ 2 & 5 \\end{pmatrix}\\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix} = \\begin{pmatrix} 1 - \\beta_1 - 2\\beta_2 \\\\ 2 - 2\\beta_1 - 5\\beta_2 \\end{pmatrix}$$\n我们分析系数激活集的可能情况。\n\n情况1：$\\hat{\\beta}_1 = 0$，$\\hat{\\beta}_2 \\ne 0$。\n条件是 $|c_1(\\hat{\\beta})| \\le \\lambda$ 和 $c_2(\\hat{\\beta}) = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_2)$。\n当 $\\hat\\beta_1 = 0$ 时，我们有 $c_2 = 2 - 5\\hat{\\beta}_2$。\n如果 $\\hat{\\beta}_2 > 0$：$2 - 5\\hat{\\beta}_2 = \\lambda = \\frac{6}{5} \\implies 5\\hat{\\beta}_2 = 2 - \\frac{6}{5} = \\frac{4}{5} \\implies \\hat{\\beta}_2 = \\frac{4}{25}$。这与 $\\hat{\\beta}_2 > 0$ 的假设一致。\n我们必须检查 $\\hat{\\beta}_1=0$ 的条件：$|c_1| \\le \\lambda$。\n$|c_1| = |1 - 0 - 2\\hat{\\beta}_2| = |1 - 2(\\frac{4}{25})| = |1 - \\frac{8}{25}| = |\\frac{17}{25}| = \\frac{17}{25}$。\n条件是 $\\frac{17}{25} \\le \\frac{6}{5}$。因为 $\\frac{6}{5} = \\frac{30}{25}$，所以我们有 $\\frac{17}{25} \\le \\frac{30}{25}$，这是成立的。\n所以, $\\hat{\\beta}^{\\text{LASSO}} = \\begin{pmatrix} 0 \\\\ \\frac{4}{25} \\end{pmatrix}$ 是一个有效解。\n\n如果 $\\hat{\\beta}_2 < 0$：$2 - 5\\hat{\\beta}_2 = -\\lambda = -\\frac{6}{5} \\implies 5\\hat{\\beta}_2 = 2 + \\frac{6}{5} = \\frac{16}{5} \\implies \\hat{\\beta}_2 = \\frac{16}{25}$。这与 $\\hat{\\beta}_2 < 0$ 的假设矛盾。\n\n情况2：$\\hat{\\beta}_1 \\ne 0$，$\\hat{\\beta}_2 = 0$。\n条件是 $c_1(\\hat{\\beta}) = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_1)$ 和 $|c_2(\\hat{\\beta})| \\le \\lambda$。\n当 $\\hat\\beta_2 = 0$ 时，我们有 $c_1 = 1 - \\hat\\beta_1$。\n如果 $\\hat{\\beta}_1 > 0$：$1 - \\hat{\\beta}_1 = \\lambda = \\frac{6}{5} \\implies \\hat{\\beta}_1 = 1 - \\frac{6}{5} = -\\frac{1}{5}$。矛盾。\n如果 $\\hat{\\beta}_1 < 0$：$1 - \\hat{\\beta}_1 = -\\lambda = -\\frac{6}{5} \\implies \\hat{\\beta}_1 = 1 + \\frac{6}{5} = \\frac{11}{5}$。矛盾。\n这种情况不产生解。\n\n情况3：$\\hat{\\beta}_1 \\ne 0$，$\\hat{\\beta}_2 \\ne 0$。\n这需要求解 $c_j(\\hat{\\beta}) = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_j)$ 对 $j=1,2$ 成立。\n1) $\\hat{\\beta}_1 > 0, \\hat{\\beta}_2 > 0$：$\\hat{\\beta}_1 + 2\\hat{\\beta}_2 = 1 - \\frac{6}{5} = -\\frac{1}{5}$。两个正数之和不能为负。矛盾。\n2) $\\hat{\\beta}_1 > 0, \\hat{\\beta}_2 < 0$：$\\hat{\\beta}_1 + 2\\hat{\\beta}_2 = 1 - \\frac{6}{5} = -\\frac{1}{5}$ 且 $2\\hat{\\beta}_1 + 5\\hat{\\beta}_2 = 2 - (-\\frac{6}{5}) = \\frac{16}{5}$。解此方程组得到 $\\hat{\\beta}_2 = \\frac{18}{5}$，这与 $\\hat{\\beta}_2 < 0$ 矛盾。\n3) $\\hat{\\beta}_1 < 0, \\hat{\\beta}_2 > 0$：$\\hat{\\beta}_1 + 2\\hat{\\beta}_2 = 1 - (-\\frac{6}{5}) = \\frac{11}{5}$ 且 $2\\hat{\\beta}_1 + 5\\hat{\\beta}_2 = 2 - \\frac{6}{5} = \\frac{4}{5}$。解此方程组得到 $\\hat{\\beta}_2 = -\\frac{18}{5}$，这与 $\\hat{\\beta}_2 > 0$ 矛盾。\n4) $\\hat{\\beta}_1 < 0, \\hat{\\beta}_2 < 0$：$\\hat{\\beta}_1 + 2\\hat{\\beta}_2 = 1 - (-\\frac{6}{5}) = \\frac{11}{5}$。两个负数之和不能为正。矛盾。\n两个系数都非零时不存在解。\n\n由于LASSO目标函数是严格凸的（因为 $X$ 是满秩的），所以必须存在唯一的最小化子。我们对所有可能激活集的分析只得出了一个有效解。\n因此，LASSO估计量为 $\\hat{\\beta}^{\\text{LASSO}} = \\begin{pmatrix} 0 \\\\ \\frac{4}{25} \\end{pmatrix}$。\n所以，$\\hat{\\beta}_{1}^{\\text{LASSO}} = 0$ 且 $\\hat{\\beta}_{2}^{\\text{LASSO}} = \\frac{4}{25}$。\n\n**最终答案汇总**\n问题要求最终答案为单个行向量 $\\big[\\hat{\\beta}_{1}^{\\text{LASSO}}, \\hat{\\beta}_{2}^{\\text{LASSO}}, \\hat{\\beta}_{1}^{\\text{Ridge}}, \\hat{\\beta}_{2}^{\\text{Ridge}}\\big]$。\n代入计算出的值：\n$$\\begin{pmatrix} 0 & \\frac{4}{25} & \\frac{6}{17} & \\frac{4}{17} \\end{pmatrix}$$\n这就是最终结果。", "answer": "$$\n\\boxed{\\begin{pmatrix} 0 & \\frac{4}{25} & \\frac{6}{17} & \\frac{4}{17} \\end{pmatrix}}\n$$"}, {"introduction": "标准的岭回归将系数向零收缩，但这在有先验经济理论支持特定非零系数的情况下可能并非最优。本练习将指导你实现一种更通用的岭回归，它将系数收缩到一个由理论驱动的目标向量 $\\beta_0$。这个实践能让你学会如何将领域知识融入到计量经济模型中，使其更具解释性和现实意义。[@problem_id:2426295]", "id": "2426295", "problem": "考虑一个线性模型，其中经济产出向量 $y \\in \\mathbb{R}^n$ 通过系数 $\\beta \\in \\mathbb{R}^p$ 与回归元矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 相关联。您的任务是，对于给定的输入，计算能够使以下目标函数最小化的系数向量 $\\beta^\\star \\in \\mathbb{R}^p$：\n$$\nJ(\\beta) \\;=\\; \\frac{1}{n}\\,\\lVert y - X\\beta \\rVert_2^2 \\;+\\; \\lambda \\,\\lVert \\beta - \\beta_0 \\rVert_2^2,\n$$\n其中 $\\lambda \\in \\mathbb{R}_{\\ge 0}$ 是一个正则化参数，$\\beta_0 \\in \\mathbb{R}^p$ 是一个由经济学理论启示的固定目标系数向量。\n\n您的程序必须为以下每个测试用例计算唯一的最小化向量 $\\beta^\\star$。对于每个用例，将系数向量作为四舍五入到六位小数的实数列表返回。将所有测试用例的结果汇总到单行中，格式为一个逗号分隔的列表，用方括号括起，每个内部列表代表一个用例的系数，顺序如下。不应打印任何其他文本。\n\n测试套件：\n\n- 案例 $1$ (一般情况，向非零目标收缩): \n  - $n = 5$, $p = 2$,\n  - $X = \\begin{bmatrix}\n  1 & 0.5 \\\\\n  1 & 1.0 \\\\\n  1 & 1.5 \\\\\n  1 & 2.0 \\\\\n  1 & 3.0\n  \\end{bmatrix}$,\n  - $y = \\begin{bmatrix} 0.3 \\\\ 1.9 \\\\ 2.1 \\\\ 3.2 \\\\ 4.8 \\end{bmatrix}$,\n  - $\\beta_0 = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}$,\n  - $\\lambda = 0.1$.\n\n- 案例 $2$ (无正则化的边界条件): \n  - 与案例 $1$ 相同的 $X$, $y$, 和 $\\beta_0$,\n  - $\\lambda = 0$.\n\n- 案例 $3$ (强正则化，系数接近目标): \n  - 与案例 $1$ 相同的 $X$, $y$, 和 $\\beta_0$,\n  - $\\lambda = 100$.\n\n- 案例 $4$ (由正则化稳定的共线回归元): \n  - $n = 4$, $p = 3$,\n  - $X = \\begin{bmatrix}\n  1 & 2 & 4 \\\\\n  1 & -1 & -2 \\\\\n  1 & 0.5 & 1.0 \\\\\n  1 & 3 & 6\n  \\end{bmatrix}$,\n  - $y = \\begin{bmatrix} 2.0 \\\\ -1.0 \\\\ 0.5 \\\\ 3.0 \\end{bmatrix}$,\n  - $\\beta_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$,\n  - $\\lambda = 0.5$.\n\n最终输出格式：\n\n- 您的程序必须生成单行，其中包含结果，格式为一个无空格的列表之列表，例如，对于四个案例，它应如下所示：\n  $[ [\\cdot,\\cdot], [\\cdot,\\cdot], [\\cdot,\\cdot], [\\cdot,\\cdot,\\cdot] ]$\n  但没有任何空格。具体来说，请打印：\n  $[[\\beta^\\star_{1,1},\\ldots,\\beta^\\star_{1,p_1}],[\\beta^\\star_{2,1},\\ldots,\\beta^\\star_{2,p_2}],[\\beta^\\star_{3,1},\\ldots,\\beta^\\star_{3,p_3}],[\\beta^\\star_{4,1},\\ldots,\\beta^\\star_{4,p_4}]]]$,\n  其中每个 $\\beta^\\star_{k,j}$ 都四舍五入到六位小数。", "solution": "所提供的问题经过了严格的验证。\n\n**第1步：提取已知条件**\n该问题定义了一个线性模型，其中产出向量 $y \\in \\mathbb{R}^n$ 通过一个系数向量 $\\beta \\in \\mathbb{R}^p$ 与一个回归元矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 相关。任务是找到目标函数\n$$\nJ(\\beta) \\;=\\; \\frac{1}{n}\\,\\lVert y - X\\beta \\rVert_2^2 \\;+\\; \\lambda \\,\\lVert \\beta - \\beta_0 \\rVert_2^2\n$$\n的最小化向量 $\\beta^\\star$，其中 $\\lambda \\in \\mathbb{R}_{\\ge 0}$ 是一个正则化参数，$\\beta_0 \\in \\mathbb{R}^p$ 是一个固定的目标系数向量。\n\n四个测试用例的已知条件如下：\n- 案例 $1$：$n = 5$, $p = 2$, $X = \\begin{bmatrix} 1 & 0.5 \\\\ 1 & 1.0 \\\\ 1 & 1.5 \\\\ 1 & 2.0 \\\\ 1 & 3.0 \\end{bmatrix}$, $y = \\begin{bmatrix} 0.3 \\\\ 1.9 \\\\ 2.1 \\\\ 3.2 \\\\ 4.8 \\end{bmatrix}$, $\\beta_0 = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}$, $\\lambda = 0.1$。\n- 案例 $2$：与案例 $1$ 相同的 $X, y, \\beta_0$，但 $\\lambda = 0$。\n- 案例 $3$：与案例 $1$ 相同的 $X, y, \\beta_0$，但 $\\lambda = 100$。\n- 案例 $4$：$n = 4$, $p = 3$, $X = \\begin{bmatrix} 1 & 2 & 4 \\\\ 1 & -1 & -2 \\\\ 1 & 0.5 & 1.0 \\\\ 1 & 3 & 6 \\end{bmatrix}$, $y = \\begin{bmatrix} 2.0 \\\\ -1.0 \\\\ 0.5 \\\\ 3.0 \\end{bmatrix}$, $\\beta_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$, $\\lambda = 0.5$。\n\n**第2步：使用提取的已知条件进行验证**\n根据验证标准对问题进行评估。\n1.  **科学基础**：该问题是正则化线性回归的一个标准练习，这是统计学、机器学习和计量经济学中的一个基本课题。该目标函数是岭回归（Ridge regression）的一种推广，有时也称为带有非零先验或目标的吉洪诺夫正则化（Tikhonov regularization）。这是一种成熟且科学合理的技术。\n2.  **适定性**：目标函数 $J(\\beta)$ 是两个平方$\\ell_2$范数之和，它们是严格凸函数。因此，对于任何 $\\lambda > 0$，$J(\\beta)$ 都是严格凸的；对于 $\\lambda=0$，$J(\\beta)$ 是凸的。一个严格凸函数有唯一的最小化子。对于 $\\lambda = 0$，问题简化为普通最小二乘法 (Ordinary Least Squares, OLS)，当且仅当矩阵 $X^T X$ 可逆（即 $X$ 具有满列秩）时，它有唯一解。为案例2提供的数据，其设计矩阵 $X$ 具有满列秩，确保了唯一解的存在。对于案例4，回归元是共线的，但 $\\lambda > 0$ 的正则化项确保了问题仍然是适定的并且有唯一解。\n3.  **客观性**：该问题使用精确的数学定义和数值数据进行陈述。它要求一个特定的、可计算的量，没有主观解释的余地。\n4.  **完整性与一致性**：每个案例都提供了所有必要的数据（$X, y, \\beta_0, \\lambda, n, p$）。矩阵和向量的维度是一致的（$y \\in \\mathbb{R}^n, X \\in \\mathbb{R}^{n \\times p}, \\beta \\in \\mathbb{R}^p$）。\n\n**第3步：结论与行动**\n该问题被判定为**有效**。它科学合理、适定、客观且完整。我们可以继续推导解决方案。\n\n**最优系数向量的推导**\n\n需要最小化的目标函数是：\n$$\nJ(\\beta) = \\frac{1}{n}\\,\\lVert y - X\\beta \\rVert_2^2 + \\lambda \\,\\lVert \\beta - \\beta_0 \\rVert_2^2\n$$\n为了找到最小化向量 $\\beta^\\star$，我们必须找到 $J(\\beta)$ 关于 $\\beta$ 的梯度为零的点。首先，我们使用向量转置展开平方范数项：\n$$\nJ(\\beta) = \\frac{1}{n}(y - X\\beta)^T(y - X\\beta) + \\lambda(\\beta - \\beta_0)^T(\\beta - \\beta_0)\n$$\n展开乘积可得：\n$$\nJ(\\beta) = \\frac{1}{n}(y^T y - y^T X \\beta - \\beta^T X^T y + \\beta^T X^T X \\beta) + \\lambda(\\beta^T \\beta - \\beta^T \\beta_0 - \\beta_0^T \\beta + \\beta_0^T \\beta_0)\n$$\n由于 $\\beta^T X^T y$ 是一个标量，它等于其自身的转置 $(y^T X \\beta)^T$。因此，$y^T X \\beta = \\beta^T X^T y$。同样地，$\\beta_0^T \\beta = \\beta^T \\beta_0$。表达式简化为：\n$$\nJ(\\beta) = \\frac{1}{n}(y^T y - 2\\beta^T X^T y + \\beta^T X^T X \\beta) + \\lambda(\\beta^T \\beta - 2\\beta^T \\beta_0 + \\beta_0^T \\beta_0)\n$$\n现在，我们使用标准的矩阵微积分恒等式 $\\nabla_{\\beta}(\\beta^T a) = a$ 和 $\\nabla_{\\beta}(\\beta^T A \\beta) = 2A\\beta$（对于对称矩阵 $A$）来计算 $J(\\beta)$ 关于向量 $\\beta$ 的梯度：\n$$\n\\nabla_\\beta J(\\beta) = \\frac{1}{n}(-2X^T y + 2X^T X \\beta) + \\lambda(2\\beta - 2\\beta_0)\n$$\n将梯度设为零以求最优的 $\\beta^\\star$：\n$$\n\\nabla_\\beta J(\\beta^\\star) = \\frac{2}{n}(X^T X \\beta^\\star - X^T y) + 2\\lambda(\\beta^\\star - \\beta_0) = 0\n$$\n两边除以 $2$ 并重新整理各项以求解 $\\beta^\\star$：\n$$\n\\frac{1}{n}X^T X \\beta^\\star - \\frac{1}{n}X^T y + \\lambda I \\beta^\\star - \\lambda \\beta_0 = 0\n$$\n其中 $I$ 是 $p \\times p$ 的单位矩阵。我们将包含 $\\beta^\\star$ 的项组合在一起：\n$$\n\\left(\\frac{1}{n}X^T X + \\lambda I\\right) \\beta^\\star = \\frac{1}{n}X^T y + \\lambda \\beta_0\n$$\n为简化起见，我们将整个方程乘以 $n$：\n$$\n(X^T X + n\\lambda I) \\beta^\\star = X^T y + n\\lambda \\beta_0\n$$\n对于 $\\lambda > 0$，矩阵 $(X^T X + n\\lambda I)$ 是可逆的，因为 $X^T X$ 是半正定的，而 $n\\lambda I$ 是正定的，使其和为正定矩阵，因此可逆。如果 $\\lambda = 0$，则该矩阵可逆当且仅当 $X$ 具有满列秩。\n求解 $\\beta^\\star$ 得到闭式解：\n$$\n\\beta^\\star = (X^T X + n\\lambda I)^{-1} (X^T y + n\\lambda \\beta_0)\n$$\n这个通用解适用于所提供的所有测试用例。它表示一个形如 $A \\beta^\\star = b$ 的线性方程组，其中 $A = X^T X + n\\lambda I$，$b = X^T y + n\\lambda \\beta_0$。在数值计算上，直接求解这个线性系统比显式计算矩阵的逆更可取。所提供的Python解决方案将实现这一逻辑。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal coefficient vector for several regularized regression problems.\n    \"\"\"\n\n    test_cases = [\n        {\n            # Case 1 (general case, shrinkage toward a nonzero target)\n            \"n\": 5, \"p\": 2,\n            \"X\": np.array([\n                [1, 0.5], [1, 1.0], [1, 1.5], [1, 2.0], [1, 3.0]\n            ]),\n            \"y\": np.array([0.3, 1.9, 2.1, 3.2, 4.8]),\n            \"beta0\": np.array([0.0, 1.0]),\n            \"lambda\": 0.1\n        },\n        {\n            # Case 2 (boundary condition with no regularization)\n            \"n\": 5, \"p\": 2,\n            \"X\": np.array([\n                [1, 0.5], [1, 1.0], [1, 1.5], [1, 2.0], [1, 3.0]\n            ]),\n            \"y\": np.array([0.3, 1.9, 2.1, 3.2, 4.8]),\n            \"beta0\": np.array([0.0, 1.0]),\n            \"lambda\": 0.0\n        },\n        {\n            # Case 3 (large regularization, coefficients close to the target)\n            \"n\": 5, \"p\": 2,\n            \"X\": np.array([\n                [1, 0.5], [1, 1.0], [1, 1.5], [1, 2.0], [1, 3.0]\n            ]),\n            \"y\": np.array([0.3, 1.9, 2.1, 3.2, 4.8]),\n            \"beta0\": np.array([0.0, 1.0]),\n            \"lambda\": 100.0\n        },\n        {\n            # Case 4 (collinear regressors stabilized by regularization)\n            \"n\": 4, \"p\": 3,\n            \"X\": np.array([\n                [1, 2, 4], [1, -1, -2], [1, 0.5, 1.0], [1, 3, 6]\n            ]),\n            \"y\": np.array([2.0, -1.0, 0.5, 3.0]),\n            \"beta0\": np.array([0.0, 0.0, 0.0]),\n            \"lambda\": 0.5\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        X = case[\"X\"]\n        y = case[\"y\"]\n        beta0 = case[\"beta0\"]\n        lam = case[\"lambda\"]\n        n = case[\"n\"]\n        p = case[\"p\"]\n\n        # The solution is beta_star = (X^T*X + n*lambda*I)^-1 * (X^T*y + n*lambda*beta_0)\n        # This is equivalent to solving the linear system A*beta_star = b where:\n        # A = X^T*X + n*lambda*I\n        # b = X^T*y + n*lambda*beta_0\n\n        XTX = X.T @ X\n        XTy = X.T @ y\n        I = np.identity(p)\n\n        A = XTX + n * lam * I\n        b = XTy + n * lam * beta0\n\n        # Solve the linear system A*beta = b for beta\n        beta_star = np.linalg.solve(A, b)\n\n        # Round the result to six decimal places and convert to a list\n        results.append(np.round(beta_star, 6).tolist())\n\n    # Format the output as a string representation of a list of lists, with no spaces.\n    final_output = str(results).replace(\" \", \"\")\n    print(final_output)\n\nsolve()\n```"}, {"introduction": "弹性网（Elastic Net）通过结合 $L_1$ 和 $L_2$ 惩罚项，提供了在岭回归和 LASSO 之间的灵活折衷，并在实践中表现出色。本练习的核心任务是实现求解弹性网问题的关键优化算法——坐标下降法。通过从零开始编写代码并在一系列测试案例中验证你的实现，你将掌握解决复杂正则化问题的核心计算技术，并为你的计量经济学工具箱增添一个强大的工具。[@problem_id:2426260]", "id": "2426260", "problem": "给定如下定义的弹性网络 (Elastic Net) 回归问题。对于一个数据矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和一个响应向量 $y \\in \\mathbb{R}^{n}$，考虑以下优化问题：\n$$\n\\min_{b \\in \\mathbb{R}^p} \\; \\frac{1}{2n} \\lVert y - X b \\rVert_2^2 \\;+\\; \\lambda_1 \\lVert b \\rVert_1 \\;+\\; \\frac{\\lambda_2}{2} \\lVert b \\rVert_2^2,\n$$\n其中 $n$ 和 $p$ 是正整数，$X$ 和 $y$ 是给定的，$\\lambda_1 \\ge 0$ 和 $\\lambda_2 \\ge 0$ 是惩罚参数。该任务的动机源于计算经济学和金融学中的模型选择和过拟合控制，在这些领域中，预测变量可能高度相关，并且变量数量可能与观测数量相当甚至更多。\n\n您的程序必须针对下面指定的测试实例，使用一种每次更新一个系数同时保持其他系数固定的方法来计算所述优化问题的解，并且必须展示该按坐标更新的方法相对于岭回归 (Ridge regression) 和最小绝对值收敛和选择算子 (LASSO) 的混合性质。程序必须产生下述规定的输出，并且不得要求任何用户输入。\n\n所有计算纯粹是数值计算，不涉及物理单位。本问题中不出现角度和百分比。\n\n请使用以下包含四个案例的测试套件。在每个案例中，$n$ 和 $p$ 由 $X$ 和 $y$ 的形状指定。\n\n- 案例1 (在通用设计上的纯岭回归一致性):\n  - 数据:\n    $$\n    X_1 = \\begin{bmatrix}\n    1 & 0 & 1 \\\\\n    1 & 1 & 1 \\\\\n    1 & 2 & 1 \\\\\n    1 & 3 & 2 \\\\\n    1 & 4 & 3 \\\\\n    1 & 5 & 5\n    \\end{bmatrix}, \\quad\n    b^{\\mathrm{true}} = \\begin{bmatrix} 0.7 \\\\ 1.3 \\\\ -0.8 \\end{bmatrix}, \\quad\n    y_1 = X_1 b^{\\mathrm{true}}.\n    $$\n  - 惩罚项: $\\lambda_1 = 0$, $\\lambda_2 = 0.8$。\n  - 此案例所需的标量结果: 一个布尔值 $r_1$，如果您的算法获得的解与唯一的岭回归闭式解 $b^{\\mathrm{ridge}}$（通过求解 $(X^\\top X / n + \\lambda_2 I) b = (X^\\top y)/n$ 定义）在每个系数上的绝对容差在 $10^{-9}$ 以内匹配，则为真，否则为假。\n\n- 案例2 (在正交设计上的纯LASSO):\n  - 数据:\n    $$\n    X_2 = \\begin{bmatrix}\n    2 & 0 & 0 \\\\\n    0 & 2 & 0 \\\\\n    0 & 0 & 2 \\\\\n    0 & 0 & 0\n    \\end{bmatrix}, \\quad\n    y_2 = \\begin{bmatrix} 3.2 \\\\ -0.5 \\\\ 1.1 \\\\ 0 \\end{bmatrix}.\n    $$\n    注意，当 $n = 4$ 时，有 $(1/n) X_2^\\top X_2 = I_3$。\n  - 惩罚项: $\\lambda_1 = 0.6$, $\\lambda_2 = 0$。\n  - 此案例所需的标量结果: 一个布尔值 $r_2$，如果您的算法获得的解与已知的正交设计下的LASSO解（即普通最小二乘估计的逐系数软阈值）在每个系数上的绝对容差在 $10^{-12}$ 以内匹配，则为真，否则为假。\n\n- 案例3 (在案例1的通用设计上的一般弹性网络系数):\n  - 数据: $X_3 = X_1$, $y_3 = y_1$。\n  - 惩罚项: $\\lambda_1 = 0.5$, $\\lambda_2 = 0.3$。\n  - 此案例所需的结果: 此案例的弹性网络解 $b^{\\mathrm{EN}}$ 的三个系数，每个系数需四舍五入到六位小数。将它们表示为 $b^{(3)}_1, b^{(3)}_2, b^{(3)}_3$。\n\n- 案例4 (通过大的 $\\ell_1$ 惩罚实现的零解边缘情况):\n  - 数据: $X_4 = X_1$, $y_4 = y_1$。\n  - 惩罚项: $\\lambda_1 = 8.0$, $\\lambda_2 = 0.1$。\n  - 此案例所需的标量结果: 一个布尔值 $r_4$，如果计算出的弹性网络解在每个系数的绝对容差 $10^{-10}$ 内是零向量，则为真，否则为假。\n\n最终输出格式:\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表的结果。该列表必须按顺序包含:\n$$\n\\big[ r_1, \\; r_2, \\; b^{(3)}_1, \\; b^{(3)}_2, \\; b^{(3)}_3, \\; r_4 \\big].\n$$", "solution": "该问题陈述已经过验证，并被认定为有效。它在科学上基于成熟的正则化线性回归理论，问题适定、客观，并为一项可解的数值任务提供了完整且一致的定义和数据。\n\n该问题要求解弹性网络 (Elastic Net) 优化问题，其目标函数定义为：\n$$\nL(b) = \\frac{1}{2n} \\lVert y - X b \\rVert_2^2 \\;+\\; \\lambda_1 \\lVert b \\rVert_1 \\;+\\; \\frac{\\lambda_2}{2} \\lVert b \\rVert_2^2\n$$\n这里，$y \\in \\mathbb{R}^{n}$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是数据矩阵，$b \\in \\mathbb{R}^{p}$ 是待优化的系数向量，$\\lambda_1 \\ge 0$, $\\lambda_2 \\ge 0$ 是非负的正则化参数。$\\lVert \\cdot \\rVert_1$ 是 $\\ell_1$-范数（绝对值之和），$\\lVert \\cdot \\rVert_2$ 是 $\\ell_2$-范数（欧几里得范数）。\n\n问题指定使用一种按坐标更新的方法，通常称为坐标下降法 (coordinate descent)。该方法每次只针对单个系数 $b_j$ 迭代优化目标函数，同时保持所有其他系数 $b_{k \\neq j}$ 固定。\n\n为了推导单个系数 $b_j$ 的更新规则，我们将目标函数 $L(b)$ 仅看作 $b_j$ 的函数，并将 $b$ 的所有其他分量视为常数。\n令 $r = y - Xb$ 为当前残差向量。我们可以将平方和项表示为：\n$$\n\\lVert y - X b \\rVert_2^2 = \\left\\lVert y - \\sum_{k=1}^p X_k b_k \\right\\rVert_2^2 = \\left\\lVert \\left(y - \\sum_{k \\neq j} X_k b_k\\right) - X_j b_j \\right\\rVert_2^2\n$$\n令部分残差为 $r^{(j)} = y - \\sum_{k \\neq j} X_k b_k$。这可以根据完整残差 $r$ 和 $b_j$ 的前一个值（记为 $b_j^{\\text{old}}$）计算得出，$r^{(j)} = r + X_j b_j^{\\text{old}}$。\n作为 $b_j$ 的函数，目标函数为：\n$$\nL(b_j) = \\frac{1}{2n} \\lVert r^{(j)} - X_j b_j \\rVert_2^2 + \\lambda_1 |b_j| + \\frac{\\lambda_2}{2} b_j^2 + \\text{常数}\n$$\n展开平方范数，我们得到：\n$$\nL(b_j) = \\frac{1}{2n} \\left( (r^{(j)})^\\top r^{(j)} - 2b_j (X_j)^\\top r^{(j)} + b_j^2 (X_j)^\\top X_j \\right) + \\lambda_1 |b_j| + \\frac{\\lambda_2}{2} b_j^2 + \\text{常数}\n$$\n为了找到最小值，我们求 $L(b_j)$ 关于 $b_j$ 的次梯度，并令其为 $0$。次梯度 $\\partial L(b_j) / \\partial b_j$ 是：\n$$\n\\frac{\\partial L(b_j)}{\\partial b_j} = \\frac{1}{n} \\left( - (X_j)^\\top r^{(j)} + b_j (X_j)^\\top X_j \\right) + \\lambda_1 \\text{sgn}(b_j) + \\lambda_2 b_j = 0\n$$\n这里 $\\text{sgn}(b_j)$ 是 $|b_j|$ 的次梯度的一部分，当 $b_j=0$ 时取值在 $[-1, 1]$ 内。\n令 $\\rho_j = \\frac{1}{n} (X_j)^\\top r^{(j)}$ 和 $a_j = \\frac{1}{n} (X_j)^\\top X_j$。方程变为：\n$$\n-\\rho_j + b_j a_j + \\lambda_2 b_j + \\lambda_1 \\text{sgn}(b_j) = 0 \\implies b_j (a_j + \\lambda_2) = \\rho_j - \\lambda_1 \\text{sgn}(b_j)\n$$\n这是一个一维的LASSO类型问题。解由软阈值算子 $S(z, \\gamma) = \\text{sgn}(z) \\max(|z|-\\gamma, 0)$ 给出。$b_j$ 的更新规则是：\n$$\nb_j \\leftarrow \\frac{S(\\rho_j, \\lambda_1)}{a_j + \\lambda_2}\n$$\n为了计算效率，$\\rho_j = \\frac{1}{n} (X_j)^\\top (y - \\sum_{k \\neq j} X_k b_k)$ 可以计算为 $\\rho_j = \\frac{1}{n}(X^\\top y)_j - \\sum_{k \\neq j} \\frac{1}{n}(X^\\top X)_{jk} b_k$。通过预计算矩阵 $\\frac{1}{n}X^\\top X$ 和 $\\frac{1}{n}X^\\top y$，每个 $b_j$ 的更新在迭代循环内的计算成本变得很低。算法会重复遍历所有系数 $j=1, \\dots, p$，直到系数向量 $b$ 收敛。\n\n该算法将应用于指定的四个测试案例。\n\n案例1：纯岭回归一致性。\n当惩罚项为 $\\lambda_1 = 0$ 和 $\\lambda_2 = 0.8$ 时，问题简化为岭回归。更新规则简化为 $b_j \\leftarrow \\rho_j / (a_j + \\lambda_2)$，因为 $S(\\rho_j, 0) = \\rho_j$。坐标下降算法预期会收敛到岭回归目标函数的唯一全局最小值。该解将与岭回归的解析闭式解 $b^{\\mathrm{ridge}} = ( \\frac{1}{n} X^\\top X + \\lambda_2 I )^{-1} (\\frac{1}{n} X^\\top y)$进行比较。如果计算出的系数与解析解在 $10^{-9}$ 的容差内匹配，布尔结果 $r_1$ 将为真。\n\n案例2：在正交设计上的纯LASSO。\n当惩罚项为 $\\lambda_1 = 0.6$ 和 $\\lambda_2 = 0$ 时，问题是LASSO回归。设计矩阵 $X_2$ 具有性质 $\\frac{1}{n} X_2^\\top X_2 = I_3$，其中 $n=4$。对于这样的设计，目标函数在各系数之间解耦，其精确解已知是普通最小二乘法 (OLS) 估计的软阈值。OLS估计为 $b^{\\mathrm{OLS}} = (\\frac{1}{n}X_2^\\top X_2)^{-1}(\\frac{1}{n}X_2^\\top y_2) = \\frac{1}{n}X_2^\\top y_2$。因此LASSO解为 $b^{\\mathrm{LASSO}}_j = S((b^{\\mathrm{OLS}})_j, \\lambda_1)$。实现的坐标下降算法预期将收敛到此精确解。如果匹配在 $10^{-12}$ 的容差内，布尔结果 $r_2$ 将为真。\n\n案例3：一般弹性网络。\n当 $\\lambda_1 = 0.5$ 和 $\\lambda_2 = 0.3$ 时，这是一个一般的弹性网络问题。应用推导出的完整坐标下降算法。所得向量 $b^{\\mathrm{EN}}$ 的系数，记为 $b^{(3)}_1, b^{(3)}_2, b^{(3)}_3$，是所需的输出，四舍五入到六位小数。\n\n案例4：零解边缘情况。\n当有一个大的 $\\ell_1$ 惩罚 $\\lambda_1 = 8.0$ 和 $\\lambda_2 = 0.1$ 时，解可能为零向量 $b=0$。$b=0$ 为最优解的条件是目标函数在 $b=0$ 处的次梯度必须包含零向量。此条件是对于所有 $j=1, \\dots, p$ 都有 $| \\frac{1}{n}(X^\\top y)_j | \\le \\lambda_1$。对于给定的数据和 $\\lambda_1 = 8.0$，我们计算 $\\frac{1}{n} X_1^\\top y_1$ 并发现该条件对所有三个分量都成立。因此，坐标下降算法预期会收敛到 $b=0$。如果计算出的每个系数的范数小于容差 $10^{-10}$，布尔结果 $r_4$ 将为真。\n\n实现将首先定义所有案例的数据，然后为每个参数集执行坐标下降求解器，并执行指定的验证检查或提取所需的系数值。", "answer": "```python\nimport numpy as np\n\ndef soft_threshold(rho, lam):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(rho) * np.maximum(np.abs(rho) - lam, 0.)\n\ndef solve_elastic_net(X, y, lambda1, lambda2, max_iter=100000, tol=1e-15):\n    \"\"\"\n    Solves the Elastic Net problem using coordinate descent.\n\n    min_b (1/2n) ||y - Xb||^2_2 + lambda1 ||b||_1 + (lambda2/2) ||b||^2_2\n    \"\"\"\n    n, p = X.shape\n    b = np.zeros(p)\n    \n    # Pre-compute matrices for efficiency\n    XTX = (X.T @ X) / n\n    XTy = (X.T @ y) / n\n    \n    # Denominators for the update rule\n    denominators = np.diag(XTX) + lambda2\n\n    for _ in range(max_iter):\n        b_old_cycle = b.copy()\n        \n        for j in range(p):\n            # Calculate rho_j using pre-computed matrices\n            # rho_j = XTy[j] - (XTX_row_j . b - XTX_jj * b_j)\n            rho_j = XTy[j] - (np.dot(XTX[j, :], b) - XTX[j, j] * b[j])\n            \n            if denominators[j] == 0:\n                 # This case is unlikely with non-zero columns in X or lambda2 > 0\n                b[j] = 0.\n            else:\n                b[j] = soft_threshold(rho_j, lambda1) / denominators[j]\n        \n        # Check for convergence\n        max_change = np.max(np.abs(b - b_old_cycle))\n        if max_change < tol:\n            break\n            \n    return b\n\ndef solve():\n    # --- Define common data ---\n    X1 = np.array([\n        [1.0, 0.0, 1.0],\n        [1.0, 1.0, 1.0],\n        [1.0, 2.0, 1.0],\n        [1.0, 3.0, 2.0],\n        [1.0, 4.0, 3.0],\n        [1.0, 5.0, 5.0]\n    ])\n    b_true = np.array([0.7, 1.3, -0.8])\n    y1 = X1 @ b_true  # As per problem, y1 is defined by this product\n\n    # --- Case 1: Ridge-only consistency ---\n    lambda1_c1, lambda2_c1 = 0.0, 0.8\n    b1_my = solve_elastic_net(X1, y1, lambda1_c1, lambda2_c1)\n    \n    n1, p1 = X1.shape\n    A = (X1.T @ X1) / n1 + lambda2_c1 * np.identity(p1)\n    v = (X1.T @ y1) / n1\n    b1_ridge = np.linalg.solve(A, v)\n    \n    r1 = np.allclose(b1_my, b1_ridge, atol=1e-9)\n\n    # --- Case 2: LASSO-only on an orthonormal design ---\n    X2 = np.array([\n        [2.0, 0.0, 0.0],\n        [0.0, 2.0, 0.0],\n        [0.0, 0.0, 2.0],\n        [0.0, 0.0, 0.0]\n    ])\n    y2 = np.array([3.2, -0.5, 1.1, 0.0])\n    lambda1_c2, lambda2_c2 = 0.6, 0.0\n    \n    b2_my = solve_elastic_net(X2, y2, lambda1_c2, lambda2_c2)\n    \n    n2 = X2.shape[0]\n    b_ols = (X2.T @ y2) / n2\n    b2_lasso = soft_threshold(b_ols, lambda1_c2)\n    \n    r2 = np.allclose(b2_my, b2_lasso, atol=1e-12)\n\n    # --- Case 3: General Elastic Net coefficients ---\n    X3, y3 = X1, y1\n    lambda1_c3, lambda2_c3 = 0.5, 0.3\n    \n    b3_en = solve_elastic_net(X3, y3, lambda1_c3, lambda2_c3)\n    b3_1 = round(b3_en[0], 6)\n    b3_2 = round(b3_en[1], 6)\n    b3_3 = round(b3_en[2], 6)\n\n    # --- Case 4: Zero-solution edge case ---\n    X4, y4 = X1, y1\n    lambda1_c4, lambda2_c4 = 8.0, 0.1\n    \n    b4_en = solve_elastic_net(X4, y4, lambda1_c4, lambda2_c4)\n    r4 = np.allclose(b4_en, np.zeros(X4.shape[1]), atol=1e-10)\n    \n    # --- Final Output Formatting ---\n    results = [\n        str(r1).lower(),\n        str(r2).lower(),\n        f\"{b3_1:.6f}\",\n        f\"{b3_2:.6f}\",\n        f\"{b3_3:.6f}\",\n        str(r4).lower()\n    ]\n    \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"}]}