{"hands_on_practices": [{"introduction": "GARCH模型的一个核心特性是协方差平稳性，它是确保模型的长期预测行为稳定且有意义的基础。本练习旨在弥合抽象的数学条件（即$ \\alpha_1 + \\beta_1 < 1 $）与时间序列数据中可观察到的后果之间的鸿沟。通过模拟，您将亲手验证当此条件满足、临界或被违反时，过程的行为有何不同，从而对长期稳定性建立直观而扎实的理解。[@problem_id:2373513]", "id": "2373513", "problem": "考虑一个零均值收益率过程 $\\left\\{r_t\\right\\}$ 的 $\\left(1,1\\right)$ 阶广义自回归条件异方差（GARCH）模型，其由以下随机差分方程定义：\n$$r_t \\;=\\; \\sigma_t\\,\\varepsilon_t,$$\n$$\\sigma_t^2 \\;=\\; \\omega \\;+\\; \\alpha_1\\,r_{t-1}^2 \\;+\\; \\beta_1\\,\\sigma_{t-1}^2,$$\n其中 $\\left\\{\\varepsilon_t\\right\\}$ 是独立同分布（IID）的，满足 $\\mathbb{E}\\!\\left[\\varepsilon_t\\right]=0$ 和 $\\mathbb{E}\\!\\left[\\varepsilon_t^2\\right]=1$，并且参数满足 $\\omega \\gt 0$, $\\alpha_1 \\ge 0$, $\\beta_1 \\ge 0$。在计算经济学和金融学中，一个关键问题是协方差平稳性，即是否存在一个有限的、不随时间变化的无条件二阶矩 $\\mathbb{E}\\!\\left[r_t^2\\right]$。\n\n您的任务如下。\n\n任务 A：仅从模型定义、迭代期望定律和独立性出发，推导无条件二阶矩 $\\mu_t \\equiv \\mathbb{E}\\!\\left[r_t^2\\right] = \\mathbb{E}\\!\\left[\\sigma_t^2\\right]$ 所满足的标量线性递归关系，并确定 $\\mu_t$ 在 $t \\to \\infty$ 时收敛到一个有限极限的充要条件（用 $\\alpha_1$ 和 $\\beta_1$ 表示）。如果极限存在，请用 $\\omega$、$\\alpha_1$ 和 $\\beta_1$ 符号化地表示它。\n\n任务 B：设计一个算法，给定参数 $\\left(\\omega,\\alpha_1,\\beta_1\\right)$，通过对 GARCH$\\left(1,1\\right)$ 过程进行长时模拟，并通过验证以下两个属性来经验性地评估其协方差平稳性：\n- 收敛性：当理论无条件二阶矩存在时，$r_t^2$ 的长期样本均值接近于任务 A 中得到的理论值。\n- 稳定性：在预烧期（burn-in）后的样本上，计算大小相等、时间上连续的分块中 $r_t^2$ 的均值，这些分块均值应相互之间保持稳定，不表现出持续的漂移。您的算法应通过一个定量诊断来操作化稳定性，该诊断比较序列的早期和晚期部分，并使用一个跨分块的离散度度量。\n\n任务 C：实现一个完整的、可运行的程序，该程序：\n- 对以下测试套件中的每组参数，使用独立同分布的标准正态新息来模拟模型，总长度 $T = 120{,}000$，并在诊断前丢弃 $B = 20{,}000$ 个观测值作为预烧期。将 $\\sigma_0^2$ 初始化为一个与所有案例兼容的严格正有限值。使用固定的随机种子以确保可复现性。\n- 在预烧期后的样本上使用 $K = 8$ 个大小相等的块来计算 $r_t^2$ 的分块均值。\n- 根据以下定量标准，将每个测试案例分类为协方差平稳或非协方差平稳：\n  - 令 $\\widehat{m}$ 表示预烧期后样本中 $r_t^2$ 的样本均值，令 $m^\\star$ 表示任务 A 中得到的理论无条件二阶矩（若存在）。定义相对误差为 $\\left|\\widehat{m} - m^\\star\\right|/m^\\star$。\n  - 令 $m_{\\text{first}}$ 和 $m_{\\text{last}}$ 分别为预烧期后样本的第一个和最后一个四分之一时段内 $r_t^2$ 的样本均值，并定义增长率 $d \\equiv m_{\\text{last}}/m_{\\text{first}}$。\n  - 令 $m_1,\\dots,m_K$ 为分块均值，并定义变异系数 $\\mathrm{CV} \\equiv \\mathrm{sd}\\!\\left(m_1,\\dots,m_K\\right)/\\overline{m}$，其中 $\\overline{m}$ 是这 $K$ 个分块均值的算术平均值。\n  - 当且仅当理论无条件二阶矩存在，并且同时满足以下所有条件时，才将一组参数分类为协方差平稳：相对误差最多为 $\\tau = 0.10$；增长率满足 $d \\le \\rho = 1.20$；分块稳定性满足 $\\mathrm{CV} \\le \\kappa = 0.20$。\n- 生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个条目都是一个布尔值，按下方给出的顺序对应于相关测试案例的分类结果，中间无空格。\n\n测试套件：\n- 案例 A（和略小于 $1$）：$\\omega = 0.1$, $\\alpha_1 = 0.05$, $\\beta_1 = 0.90$。\n- 案例 B（和等于 $1$）：$\\omega = 0.1$, $\\alpha_1 = 0.05$, $\\beta_1 = 0.95$。\n- 案例 C（和略大于 $1$）：$\\omega = 0.1$, $\\alpha_1 = 0.05$, $\\beta_1 = 0.98$。\n\n答案规范：\n- 最终输出必须是单行文本，包含一个含有三个布尔值的列表，顺序严格为 $\\left[\\text{案例 A},\\text{案例 B},\\text{案例 C}\\right]$，例如 $\\left[\\text{True},\\text{False},\\text{False}\\right]$。\n- 不涉及物理单位。所有数值阈值必须严格按照上述规定实现。", "solution": "此问题要求对 GARCH$\\left(1,1\\right)$ 模型进行两部分分析：首先，理论推导协方差平稳性的条件；其次，设计并实现一个基于模拟的经验性检验来验证此性质。\n\n我们首先进行任务 A 中指定的理论推导。GARCH$\\left(1,1\\right)$ 模型由以下方程定义：\n$$r_t = \\sigma_t \\varepsilon_t$$\n$$\\sigma_t^2 = \\omega + \\alpha_1 r_{t-1}^2 + \\beta_1 \\sigma_{t-1}^2$$\n其中 $\\left\\{\\varepsilon_t\\right\\}$ 是一个独立同分布（IID）过程，满足 $\\mathbb{E}[\\varepsilon_t] = 0$ 和 $\\mathbb{E}[\\varepsilon_t^2] = 1$。参数满足 $\\omega > 0$, $\\alpha_1 \\ge 0$ 和 $\\beta_1 \\ge 0$。我们关心的是无条件二阶矩 $\\mu_t \\equiv \\mathbb{E}[r_t^2]$。\n\n令 $\\mathcal{F}_{t-1}$ 表示在时间 $t-1$ 可用的信息所构成的 sigma-代数，它包含 $\\varepsilon_t$ 的所有过去值。根据定义，$\\sigma_t^2$ 是过去收益率和方差的函数，因此是 $\\mathcal{F}_{t-1}$-可测的。新息 $\\varepsilon_t$ 独立于 $\\mathcal{F}_{t-1}$。\n\n首先，我们证明恒等式 $\\mu_t = \\mathbb{E}[\\sigma_t^2]$。使用迭代期望定律：\n$$\\mu_t = \\mathbb{E}[r_t^2] = \\mathbb{E}[\\sigma_t^2 \\varepsilon_t^2] = \\mathbb{E}\\left[\\mathbb{E}[\\sigma_t^2 \\varepsilon_t^2 | \\mathcal{F}_{t-1}]\\right]$$\n因为 $\\sigma_t^2$ 是 $\\mathcal{F}_{t-1}$-可测的，且 $\\varepsilon_t$ 独立于 $\\mathcal{F}_{t-1}$：\n$$\\mathbb{E}[\\sigma_t^2 \\varepsilon_t^2 | \\mathcal{F}_{t-1}] = \\sigma_t^2 \\mathbb{E}[\\varepsilon_t^2 | \\mathcal{F}_{t-1}] = \\sigma_t^2 \\mathbb{E}[\\varepsilon_t^2] = \\sigma_t^2 \\cdot 1 = \\sigma_t^2$$\n将此代回，我们得到：\n$$\\mu_t = \\mathbb{E}[\\sigma_t^2]$$\n这证实了问题描述中提供的恒等式。\n\n接下来，我们推导 $\\mu_t$ 的递归关系。我们对条件方差方程取无条件期望：\n$$\\mathbb{E}[\\sigma_t^2] = \\mathbb{E}[\\omega + \\alpha_1 r_{t-1}^2 + \\beta_1 \\sigma_{t-1}^2]$$\n根据期望算子的线性性质：\n$$\\mathbb{E}[\\sigma_t^2] = \\omega + \\alpha_1 \\mathbb{E}[r_{t-1}^2] + \\beta_1 \\mathbb{E}[\\sigma_{t-1}^2]$$\n使用我们的定义 $\\mu_t = \\mathbb{E}[\\sigma_t^2]$ 和 $\\mu_{t-1} = \\mathbb{E}[r_{t-1}^2] = \\mathbb{E}[\\sigma_{t-1}^2]$，我们得到无条件二阶矩的标量线性递归关系：\n$$\\mu_t = \\omega + (\\alpha_1 + \\beta_1) \\mu_{t-1}$$\n这是一个一阶线性非齐次递归关系。为了使 $\\mu_t$ 在 $t \\to \\infty$ 时收敛到一个有限的、不随时间变化的极限 $\\mu$，动态项的系数的绝对值必须小于 1。考虑到非负约束 $\\alpha_1 \\ge 0$ 和 $\\beta_1 \\ge 0$，存在有限的、不随时间变化的二阶矩的充要条件是：\n$$\\alpha_1 + \\beta_1 < 1$$\n如果此条件成立，则称该过程是弱平稳或协方差平稳的。极限无条件方差（我们记作 $m^\\star$）可以通过在递归关系中令 $\\mu_t = \\mu_{t-1} = m^\\star$ 来找到，从而得到不动点方程：\n$$m^\\star = \\omega + (\\alpha_1 + \\beta_1) m^\\star$$\n解出 $m^\\star$ 得到理论无条件二阶矩的表达式：\n$$m^\\star = \\frac{\\omega}{1 - \\alpha_1 - \\beta_1}$$\n这个理论基础直接为任务 B 和 C 的算法设计提供了依据。\n\n经验性评估协方差平稳性的算法步骤如下。\n1.  **理论预检验**：对于给定的一组参数 $(\\omega, \\alpha_1, \\beta_1)$，首先检查条件 $\\alpha_1 + \\beta_1 < 1$ 是否满足。如果不满足，则该过程立即被分类为非协方差平稳，因为理论无条件二阶矩不存在。\n2.  **模拟**：如果通过了预检验，则模拟一个长度为 $T = 120,000$ 的 GARCH$\\left(1,1\\right)$ 过程。新息 $\\varepsilon_t$ 从独立同分布的标准正态分布中抽取。为确保可复现性，使用固定的随机种子。模拟从一个初始方差 $\\sigma_0^2 > 0$ 开始。\n3.  **数据准备**：丢弃前 $B = 20,000$ 个数据点作为预烧期，以消除对初始条件的依赖。分析在后续的 $N = 100,000$ 个平方收益率样本 $\\{r_t^2\\}_{t=B}^{T-1}$ 上进行。\n4.  **诊断评估**：计算三个定量指标来评估收敛性和稳定性。\n    a.  **收敛性准则**：计算预烧期后平方收益率的样本均值 $\\widehat{m}$。其相对于理论值 $m^\\star$ 的相对误差 $|\\widehat{m} - m^\\star|/m^\\star$ 必须小于或等于容差 $\\tau = 0.10$。\n    b.  **稳定性准则（趋势）**：将预烧期后的样本分成四个相等的季度。计算最后一个季度的均值 ($m_{\\text{last}}$) 与第一个季度的均值 ($m_{\\text{first}}$) 的比率，记为 $d = m_{\\text{last}}/m_{\\text{first}}$。一个平稳序列不应表现出显著的漂移，因此该比率必须小于或等于阈值 $\\rho = 1.20$。\n    c.  **稳定性准则（离散度）**：将预烧期后的样本划分为 $K=8$ 个大小相等的不重叠块。计算每个块的均值 $m_k$。通过这些分块均值的变异系数 (CV) 来评估过程的稳定性，$\\mathrm{CV} = \\mathrm{sd}(m_1, \\dots, m_K) / \\overline{m}$，其中 $\\mathrm{sd}(\\cdot)$ 是样本标准差，$\\overline{m}$ 是分块均值的均值。较低的 CV 表示方差在整个样本中是稳定的。该值必须小于或等于阈值 $\\kappa = 0.20$。\n5.  **最终分类**：当且仅当理论条件 $\\alpha_1 + \\beta_1 < 1$ 满足，并且所有三个经验准则（相对误差、增长率、分块 CV）都满足其指定的阈值时，才将一组参数分类为协方差平稳。否则，将其分类为非协方差平稳。对每个测试案例执行此完整过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_garch_analysis(omega, alpha_1, beta_1, T, B, K, tau, rho, kappa, eps_series):\n    \"\"\"\n    Simulates a GARCH(1,1) process and assesses covariance stationarity.\n\n    Args:\n        omega (float): GARCH parameter.\n        alpha_1 (float): GARCH parameter.\n        beta_1 (float): GARCH parameter.\n        T (int): Total length of the simulation.\n        B (int): Length of the burn-in period.\n        K (int): Number of blocks for stability analysis.\n        tau (float): Relative error threshold.\n        rho (float): Growth ratio threshold.\n        kappa (float): Block stability (CV) threshold.\n        eps_series (np.ndarray): Pre-generated standard normal innovations.\n\n    Returns:\n        bool: True if classified as covariance-stationary, False otherwise.\n    \"\"\"\n    # Step 1: Theoretical Pre-check\n    if alpha_1 + beta_1 >= 1:\n        return False\n\n    # Theoretical unconditional second moment\n    m_star = omega / (1 - alpha_1 - beta_1)\n\n    # Step 2: Simulation\n    sigma_sq = np.zeros(T)\n    r_sq = np.zeros(T)\n    \n    # Initialization: choose a positive finite value compatible with all cases,\n    # as required by the problem.\n    sigma_sq[0] = 0.1\n\n    for t in range(T - 1):\n        r_sq[t] = sigma_sq[t] * eps_series[t]**2\n        sigma_sq[t+1] = omega + alpha_1 * r_sq[t] + beta_1 * sigma_sq[t]\n    \n    # Compute the last value of r_sq\n    r_sq[T-1] = sigma_sq[T-1] * eps_series[T-1]**2\n\n    # Step 3: Data Preparation\n    analysis_sample = r_sq[B:]\n    N = T - B\n    \n    # Step 4: Diagnostic Evaluation\n    \n    # a. Convergence Criterion\n    m_hat = np.mean(analysis_sample)\n    relative_error = np.abs(m_hat - m_star) / m_star\n    \n    # b. Stability Criterion (Trend)\n    n_quarter = N // 4\n    m_first = np.mean(analysis_sample[:n_quarter])\n    m_last = np.mean(analysis_sample[-n_quarter:])\n    \n    # To avoid division by zero if m_first happens to be pathologically small\n    if m_first <= 0:\n        growth_ratio = float('inf')\n    else:\n        growth_ratio = m_last / m_first\n\n    # c. Stability Criterion (Dispersion)\n    block_size = N // K\n    block_means = np.array([\n        np.mean(analysis_sample[k*block_size : (k+1)*block_size]) for k in range(K)\n    ])\n    mean_of_block_means = np.mean(block_means) # This is equivalent to m_hat\n    \n    if mean_of_block_means <= 0:\n        cv = float('inf')\n    else:\n        # Use ddof=1 for sample standard deviation\n        cv = np.std(block_means, ddof=1) / mean_of_block_means\n\n    # Step 5: Final Classification\n    is_conv_ok = relative_error <= tau\n    is_growth_ok = growth_ratio <= rho\n    is_stability_ok = cv <= kappa\n\n    return is_conv_ok and is_growth_ok and is_stability_ok\n\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases.\n    \"\"\"\n    # Define parameters from the problem statement.\n    T = 120000\n    B = 20000\n    K = 8\n    tau = 0.10\n    rho = 1.20\n    kappa = 0.20\n\n    # Define the test cases.\n    test_cases = [\n        {'name': 'Case A', 'omega': 0.1, 'alpha_1': 0.05, 'beta_1': 0.90},\n        {'name': 'Case B', 'omega': 0.1, 'alpha_1': 0.05, 'beta_1': 0.95},\n        {'name': 'Case C', 'omega': 0.1, 'alpha_1': 0.05, 'beta_1': 0.98},\n    ]\n\n    # Set a fixed random seed for reproducibility of the entire program run.\n    # The same stream of random numbers will be used sequentially for all cases.\n    np.random.seed(42)\n    \n    # Pre-generate all random numbers needed.\n    eps_series = np.random.standard_normal(T)\n\n    results = []\n    for case in test_cases:\n        is_stationary = run_garch_analysis(\n            omega=case['omega'],\n            alpha_1=case['alpha_1'],\n            beta_1=case['beta_1'],\n            T=T, B=B, K=K,\n            tau=tau, rho=rho, kappa=kappa,\n            eps_series=eps_series\n        )\n        results.append(is_stationary)\n\n    # Final print statement in the exact required format.\n    # The required format is boolean values, so convert True/False to lowercase strings\n    # as per standard Python `str()` behavior for booleans.\n    print(f\"[{','.join(map(lambda x: str(x).lower(), results))}]\")\n\nsolve()\n```"}, {"introduction": "在理解了GARCH模型的基本属性后，一个实际的挑战是如何为给定的数据集选择合适的模型。本练习将探讨模型选择中一个微妙但重要的问题：如何在GARCH(1,1)模型和更简单的ARCH模型之间做出抉择。您将学习像贝叶斯信息准则（BIC）这样的工具如何在模型拟合优度与复杂性之间进行权衡，并观察到一个更简约但可能错误（misspecified）的模型在某些情况下（尤其是当波动率持续性较低时）如何会被优先选择。[@problem_id:2373512]", "id": "2373512", "problem": "你需要编写一个完整的、可运行的程序，该程序构建模拟的金融回报序列，并使用高斯准最大似然估计（QMLE）在两个波动率模型之间进行模型选择评估。该任务的背景是自回归条件异方差性，具体来说，是在数据由一个低持续性的GARCH(1,1)过程生成时，比较一个一阶自回归条件异方差模型（ARCH(1)）与一个（1,1）阶广义自回归条件异方差模型（GARCH(1,1)）。\n\n从以下核心定义和公认事实开始：\n\n- 一个具有条件异方差性的零均值回报序列 $\\{r_t\\}_{t=1}^T$ 被建模为 $r_t = \\sigma_t z_t$，其中 $\\{z_t\\}$ 是独立同分布的标准正态变量，$\\sigma_t^2$ 是条件方差。\n- ARCH($p$) 模型将条件方差指定为 $\\sigma_t^2 = \\omega + \\sum_{i=1}^p \\alpha_i r_{t-i}^2$，其中为保证协方差平稳性，需满足 $\\omega > 0$，$\\alpha_i \\ge 0$ 和 $\\sum_{i=1}^p \\alpha_i &lt; 1$。\n- GARCH(1,1) 模型将条件方差指定为 $\\sigma_t^2 = \\omega + \\alpha r_{t-1}^2 + \\beta \\sigma_{t-1}^2$，其中为保证协方差平稳性，需满足 $\\omega > 0$，$\\alpha \\ge 0$，$\\beta \\ge 0$ 和 $\\alpha + \\beta &lt; 1$。\n- 在高斯QMLE下，对于给定的参数 $\\theta$ 和一个产生 $\\{\\sigma_t^2(\\theta)\\}$ 的参数化波动率递归，其对数似然为\n$$\n\\ell_T(\\theta) = -\\frac{1}{2} \\sum_{t=1}^T \\left( \\log(2\\pi) + \\log\\left(\\sigma_t^2(\\theta)\\right) + \\frac{r_t^2}{\\sigma_t^2(\\theta)} \\right).\n$$\n- 一个具有 $k$ 个自由参数和样本量为 $n$ 的模型的贝叶斯信息准则（BIC）是\n$$\n\\mathrm{BIC} = -2 \\ell_T(\\hat{\\theta}) + k \\log(n),\n$$\n其中 $\\hat{\\theta}$ 是使对数似然最大化的参数。\n\n你的程序必须：\n\n1. 对于每个测试用例，使用给定的参数从一个GARCH(1,1)过程中模拟一个回报序列 $\\{r_t\\}_{t=1}^T$，该过程具有低持续性（即 $\\alpha + \\beta$ 严格小于1且数值上很小）。使用标准正态新息 $\\{z_t\\}$ 和一个固定的 $B = 1000$ 个观测值的预烧期，在保留最后 $T$ 个观测值之前丢弃这些数据，以减轻初始条件的影响。使用无条件方差作为初始方差，即 $\\sigma_0^2 = \\omega / (1 - \\alpha - \\beta)$。\n2. 通过高斯QMLE拟合以下两个模型：\n   - 一个指定阶数 $p$ 的ARCH($p$)模型。\n   - 一个GARCH(1,1)模型。\n   估计过程必须遵守上述的正值性和平稳性约束。你可以使用任何数值稳定、可微的重参数化方法，在最优化过程中施加这些约束。\n3. 对于每个拟合的模型，计算其最大化对数似然和BIC。对于ARCH($p$)模型，使用 $k = 1 + p$；对于GARCH(1,1)模型，使用 $k = 3$；样本量为 $n = T$。\n4. 当数据由GARCH(1,1)生成时，如果ARCH($p$)的BIC严格小于GARCH(1,1)的BIC，则定义一个指标，表示模拟数据集“欺骗”了标准ARCH($p$)模型，该指标为一个布尔值且为真。即，如果 $\\mathrm{BIC}_{\\mathrm{ARCH}(p)} &lt; \\mathrm{BIC}_{\\mathrm{GARCH}(1,1)}$，则输出true。\n\n实现要求与数值细节：\n\n- 在对两个模型进行似然评估时，使用与当前参数猜测值相关联的无条件方差来初始化递归。对于ARCH($p$)的似然递归，使用此无条件方差初始化样本前的平方回报 $r_{t-i}^2$（对于 $i &gt; t$）。\n- 使用标准正态新息进行模拟。\n- 在模拟和似然评估的所有时间步中，确保条件方差保持严格为正。\n- 对每个模型，使用任何确定性数值最优化程序来最大化高斯对数似然，并通过对无约束参数进行光滑变换来强制施加约束。\n\n测试套件：\n\n对以下三个参数集中的每一个，运行上述完整的模拟-估计-选择流程，并返回一个布尔结果，该结果指示BIC是否选择了ARCH($p$)模型而不是GARCH(1,1)模型：\n\n- 测试用例1（理想情况，低持续性）：$T = 1200$，$\\omega = 0.05$，$\\alpha = 0.08$，$\\beta = 0.18$，$p = 1$，模拟种子 $= 11$。\n- 测试用例2（较小样本）：$T = 800$，$\\omega = 0.10$，$\\alpha = 0.05$，$\\beta = 0.25$，$p = 1$，模拟种子 $= 22$。\n- 测试用例3（接近白噪声的波动性）：$T = 1000$，$\\omega = 0.02$，$\\alpha = 0.03$，$\\beta = 0.05$，$p = 1$，模拟种子 $= 33$。\n\n最终输出格式：\n\n- 你的程序应生成单行输出，其中包含与三个测试用例相对应的三个布尔结果，格式为方括号内以逗号分隔的列表（例如，“[True,False,True]”）。", "solution": "该问题要求在金融计量经济学领域实现一个数值实验。目标是评估贝叶斯信息准则（BIC）在数据实际上由一个低持续性的GARCH($1,1$)过程生成时，在GARCH($1,1$)模型与更简单的ARCH($p$)模型之间进行选择的性能。这个练习对于理解模型设定错误以及模型简约性与拟合优度之间的权衡至关重要。\n\n该过程将针对每个指定的测试用例执行，遵循一个严格的三步流程：数据模拟、模型估计和模型选择。\n\n**1. 数据模拟**\n\n本分析的基础是一个模拟的金融回报序列 $\\{r_t\\}_{t=1}^T$。该序列由一个GARCH($1,1$)过程生成，由以下方程定义：\n$$\nr_t = \\sigma_t z_t\n$$\n$$\n\\sigma_t^2 = \\omega + \\alpha r_{t-1}^2 + \\beta \\sigma_{t-1}^2\n$$\n其中 $\\{z_t\\}_{t=1}^T$ 是从标准正态分布 $z_t \\sim \\mathcal{N}(0, 1)$ 中抽取的独立同分布（i.i.d.）随机变量序列。每个测试用例都提供了参数 $(\\omega, \\alpha, \\beta)$，它们满足协方差平稳性条件：$\\omega > 0$、$\\alpha \\ge 0$、$\\beta \\ge 0$ 和 $\\alpha + \\beta < 1$。\n\n为了减轻初始条件对生成序列的影响，我们首先模拟一个长度为 $T + B$ 的更长序列，其中 $T$ 是期望的样本量，$B = 1000$ 是预烧期。递归使用过程的无条件方差进行初始化，即 $\\sigma_0^2 = \\frac{\\omega}{1 - \\alpha - \\beta}$。然后，丢弃前 $B$ 个模拟观测值，留下最终的序列 $\\{r_t\\}_{t=1}^T$ 进行分析。\n\n**2. 通过准最大似然（QMLE）进行参数估计**\n\n对于每个模拟序列，我们拟合两个竞争模型：一个ARCH($p$)模型和一个GARCH($1,1$)模型。估计是通过最大化高斯准对数似然函数来执行的。假设新息为高斯分布，给定参数向量 $\\theta$ 的大小为 $T$ 的样本的对数似然为：\n$$\n\\ell_T(\\theta) = -\\frac{1}{2} \\sum_{t=1}^T \\left( \\log(2\\pi) + \\log\\left(\\sigma_t^2(\\theta)\\right) + \\frac{r_t^2}{\\sigma_t^2(\\theta)} \\right)\n$$\n最大化 $\\ell_T(\\theta)$ 等价于最小化其负值。为最优化目的，常数项 $-\\frac{T}{2}\\log(2\\pi)$ 可以忽略，因为它不影响最大值的位置。因此，要最小化的目标函数是：\n$$\n\\mathcal{L}(\\theta) = \\sum_{t=1}^T \\left( \\log\\left(\\sigma_t^2(\\theta)\\right) + \\frac{r_t^2}{\\sigma_t^2(\\theta)} \\right)\n$$\n估计的一个关键方面是强制执行方差正值性和平稳性所需的参数约束。这是通过光滑、可微的重参数化实现的，它允许无约束的数值优化器在有效的参数空间内进行搜索。\n\n对于GARCH($1,1$)模型，参数为 $\\theta_{GARCH} = (\\omega, \\alpha, \\beta)$。约束条件是 $\\omega > 0$、$\\alpha \\ge 0$、$\\beta \\ge 0$ 和 $\\alpha + \\beta < 1$。我们通过以下方式从一个无约束向量 $\\psi = (\\psi_0, \\psi_1, \\psi_2) \\in \\mathbb{R}^3$ 映射到约束空间：\n- $\\omega = \\exp(\\psi_0)$\n- $\\alpha = \\frac{\\psi_1^2}{1 + \\psi_1^2}$\n- $\\beta = \\left(1 - \\alpha\\right) \\frac{\\psi_2^2}{1 + \\psi_2^2}$\n这种构造保证了所有约束都得到满足。\n\n对于ARCH($p$)模型，在所有测试用例中 $p=1$，参数为 $\\theta_{ARCH} = (\\omega, \\alpha_1)$。约束条件是 $\\omega > 0$ 和 $0 \\le \\alpha_1 < 1$。我们从一个无约束向量 $\\phi = (\\phi_0, \\phi_1) \\in \\mathbb{R}^2$ 进行映射：\n- $\\omega = \\exp(\\phi_0)$\n- $\\alpha_1 = \\frac{\\exp(\\phi_1)}{1 + \\exp(\\phi_1)}$\n这种变换确保了 $\\omega > 0$ 且 $\\alpha_1 \\in (0, 1)$。\n\n在最优化程序中每次评估似然函数时，都必须计算条件方差序列 $\\{\\sigma_t^2(\\theta)\\}_{t=1}^T$。递归使用*当前*参数猜测值 $\\theta$ 所隐含的无条件方差进行初始化。对于GARCH($1,1$)，样本前的值 $r_0^2$ 和 $\\sigma_0^2$ 被设置为 $\\frac{\\omega}{1-\\alpha-\\beta}$。对于ARCH($1$)，样本前的值 $r_0^2$ 被设置为 $\\frac{\\omega}{1-\\alpha_1}$。\n\n**3. 通过贝叶斯信息准则（BIC）进行模型选择**\n\n在获得每个模型的最大化对数似然值 $\\ell_T(\\hat{\\theta})$ 后，我们计算BIC：\n$$\n\\mathrm{BIC} = -2 \\ell_T(\\hat{\\theta}) + k \\log(n)\n$$\n其中 $\\hat{\\theta}$ 是估计参数的向量， $k$ 是模型中自由参数的数量， $n=T$ 是样本量。BIC对模型的复杂度进行惩罚，其中 $k \\log(n)$ 项代表惩罚。较低的BIC值表示模型更优。\n- 对于ARCH($p$)模型，$k = p + 1$。由于 $p=1$，$k_{ARCH} = 2$。\n- 对于GARCH($1,1$)模型，$k_{GARCH} = 3$。\n\n每个测试用例的最终输出是一个布尔值，指示ARCH($p$)模型是否“欺骗”了选择准则。如果其BIC严格小于GARCH($1,1$)模型的BIC，即 $\\mathrm{BIC}_{\\mathrm{ARCH}(p)} < \\mathrm{BIC}_{\\mathrm{GARCH}(1,1)}$，则该值为真。这一结果表明，尽管数据源自GARCH过程，但由于BIC对GARCH模型额外参数的惩罚，更简约的ARCH模型反而被偏好。当真实的GARCH过程具有低持续性（即 $\\alpha+\\beta$ 较小）时，这种情况更有可能发生，这使得它难以与更简单的ARCH过程区分开来，尤其是在样本量较小的情况下。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef simulate_garch11(T, B, omega, alpha, beta, seed):\n    \"\"\"\n    Simulates a GARCH(1,1) process.\n    \"\"\"\n    np.random.seed(seed)\n    total_len = T + B\n    \n    # Generate standard normal innovations\n    z = np.random.randn(total_len)\n    \n    r = np.zeros(total_len)\n    sigma2 = np.zeros(total_len)\n    \n    # Initial variance (unconditional variance)\n    uncond_var = omega / (1 - alpha - beta)\n    sigma2_t_minus_1 = uncond_var\n    # Use E[r_{t-1}^2] = sigma_{t-1}^2 for initialization of first r_t\n    r_t_minus_1_sq = uncond_var\n    \n    for t in range(total_len):\n        sigma2[t] = omega + alpha * r_t_minus_1_sq + beta * sigma2_t_minus_1\n        r[t] = np.sqrt(sigma2[t]) * z[t]\n        \n        sigma2_t_minus_1 = sigma2[t]\n        r_t_minus_1_sq = r[t]**2\n        \n    # Discard burn-in period\n    return r[B:]\n\ndef garch11_neg_loglike_factory(r):\n    \"\"\"\n    Factory for the negative log-likelihood of a GARCH(1,1) model.\n    \"\"\"\n    T = len(r)\n    r_sq = r**2\n\n    def neg_loglike(unconstrained_params):\n        # 1. Reparameterize to enforce constraints\n        psi_0, psi_1, psi_2 = unconstrained_params\n        omega = np.exp(psi_0)\n        # alpha is in [0, 1)\n        alpha = psi_1**2 / (1 + psi_1**2)\n        # beta is in [0, 1-alpha)\n        beta = (1 - alpha) * (psi_2**2 / (1 + psi_2**2))\n        \n        # 2. Check for stationarity to avoid division by zero\n        if (alpha + beta) >= 1.0:\n            return 1e9 # Return a large value if non-stationary\n\n        # 3. Calculate conditional variances\n        sigma2 = np.zeros(T)\n        uncond_var = omega / (1 - alpha - beta)\n        \n        # Initialize with unconditional variance\n        sigma2[0] = omega + alpha * uncond_var + beta * uncond_var\n        \n        for t in range(1, T):\n            sigma2[t] = omega + alpha * r_sq[t-1] + beta * sigma2[t-1]\n\n        # Prevent numerical issues with very small variances\n        sigma2 = np.maximum(sigma2, 1e-9)\n\n        # 4. Calculate log-likelihood\n        log_likelihood_sum = np.sum(np.log(sigma2) + r_sq / sigma2)\n        \n        return 0.5 * log_likelihood_sum\n\n    return neg_loglike\n\ndef arch1_neg_loglike_factory(r):\n    \"\"\"\n    Factory for the negative log-likelihood of an ARCH(1) model.\n    \"\"\"\n    T = len(r)\n    r_sq = r**2\n\n    def neg_loglike(unconstrained_params):\n        # 1. Reparameterize to enforce constraints\n        phi_0, phi_1 = unconstrained_params\n        omega = np.exp(phi_0)\n        # alpha1 is in (0, 1)\n        alpha1 = np.exp(phi_1) / (1 + np.exp(phi_1))\n\n        # 2. Check for stationarity\n        if alpha1 >= 1.0:\n            return 1e9\n        \n        # 3. Calculate conditional variances\n        sigma2 = np.zeros(T)\n        uncond_var = omega / (1 - alpha1)\n        \n        # Initialize with unconditional variance for pre-sample r_0^2\n        sigma2[0] = omega + alpha1 * uncond_var\n        \n        for t in range(1, T):\n            sigma2[t] = omega + alpha1 * r_sq[t-1]\n        \n        # Prevent numerical issues\n        sigma2 = np.maximum(sigma2, 1e-9)\n\n        # 4. Calculate log-likelihood\n        log_likelihood_sum = np.sum(np.log(sigma2) + r_sq / sigma2)\n        \n        return 0.5 * log_likelihood_sum\n\n    return neg_loglike\n    \ndef solve():\n    \"\"\"\n    Main function to run test cases and generate final output.\n    \"\"\"\n    test_cases = [\n        # T, omega, alpha, beta, p, seed\n        (1200, 0.05, 0.08, 0.18, 1, 11),\n        (800, 0.10, 0.05, 0.25, 1, 22),\n        (1000, 0.02, 0.03, 0.05, 1, 33),\n    ]\n\n    results = []\n    B = 1000 # Burn-in period\n\n    for case in test_cases:\n        T, omega_true, alpha_true, beta_true, p, seed = case\n        \n        # 1. Simulate data from GARCH(1,1)\n        r_sim = simulate_garch11(T, B, omega_true, alpha_true, beta_true, seed)\n        \n        # 2. Fit GARCH(1,1) model\n        garch_objective = garch11_neg_loglike_factory(r_sim)\n        # Initial guess for unconstrained params (psi_0, psi_1, psi_2)\n        # Corresponds roughly to omega=0.1, alpha=0.1, beta=0.8\n        x0_garch = [-2.3, 0.33, 2.8] \n        garch_res = minimize(garch_objective, x0_garch, method='BFGS')\n        \n        # maximized log-likelihood value (without constant)\n        min_neg_ll_garch = garch_res.fun\n        \n        # 3. Fit ARCH(p) model (here p=1)\n        arch_objective = arch1_neg_loglike_factory(r_sim)\n        # Initial guess for unconstrained params (phi_0, phi_1)\n        # Corresponds roughly to omega=0.1, alpha1=0.2\n        x0_arch = [-2.3, -1.38] \n        arch_res = minimize(arch_objective, x0_arch, method='BFGS')\n        \n        min_neg_ll_arch = arch_res.fun\n\n        # 4. Compute BIC for both models\n        k_garch = 3\n        # We need 2 * min_neg_ll since optimizer minimizes 0.5 * sum(...)\n        # The constant part of log-likelihood cancels out in comparison\n        bic_garch = 2 * min_neg_ll_garch + k_garch * np.log(T)\n        \n        k_arch = 1 + p\n        bic_arch = 2 * min_neg_ll_arch + k_arch * np.log(T)\n        \n        # 5. Compare BICs and record result\n        # True if ARCH(p) is wrongly selected\n        results.append(bic_arch < bic_garch)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "选择并拟合一个模型并非建模工作的终点，我们还必须验证其充分性。本练习将指导您实现一个关键的诊断检验——Ljung-Box检验，以检测模型残差中是否存在任何未被捕捉的波动率聚集现象。通过从零开始构建此检验，您将更深入地理解模型设定错误的概念，并体会到模型后诊断（post-estimation diagnostics）在构建可靠金融模型中的重要性。[@problem_id:2395745]", "id": "2395745", "problem": "你的任务是构建一个完整的程序，用于估计一个广义自回归条件异方差（GARCH）模型，并对标准化残差的平方应用 Ljung-Box 诊断检验，以检测残留的自回归条件异方差（ARCH）效应。该程序必须从基本原理出发，基于核心定义实现以下组成部分：(i) 在高斯假设下通过最大似然法估计 GARCH 模型，(ii) 使用估计的条件方差序列构建标准化残差，以及 (iii) 使用从标准化残差派生的转换序列的样本自相关来计算 Ljung-Box 检验统计量。该程序不得依赖任何预打包的 GARCH 或诊断例程，并且必须通过固定的伪随机种子保证可复现性。\n\n考虑一个单变量零均值收益率序列 $\\{r_t\\}_{t=1}^T$，其条件方差为 $\\{\\sigma_t^2\\}_{t=1}^T$。GARCH 模型将条件方差指定为一个由过去的新息平方和过去的条件方差驱动的递归。标准化残差的定义是将每个新息除以其条件方差的平方根。Ljung-Box 诊断是一种联合检验（portmanteau test），用于评估一个序列的样本自相关在一组滞后阶数上是否整体为零。在合适的原假设和渐近条件下，Ljung-Box 统计量服从一个参考分布，从而可以计算用于在给定显著性水平下制定决策规则的尾部概率。\n\n你的程序必须对下面测试套件中的每个测试用例执行以下操作：\n1. 使用指定的条件方差递归和正态新息来模拟一个收益率序列。在收集最后的 $T$ 个观测值之前，使用固定的伪随机种子 $12345$ 和一个正的预热期（burn-in）以减少初始化效应。\n2. 在高斯似然下，通过准最大似然估计（QMLE）对模拟序列拟合一个 GARCH($1,1$) 模型，并对参数施加正值约束和严格平稳性类型的界限。\n3. 通过将每个新息除以该时刻拟合的条件方差的平方根来计算标准化残差。\n4. 对标准化残差的平方应用 Ljung-Box 检验，使用该测试用例指定的最大滞后阶数 $m$。使用基于样本协方差的样本自相关的标准定义，并利用大样本参考分布来获得尾部概率。如果该尾部概率严格小于显著性水平 $\\alpha = 0.05$，则判定为拒绝原假设。\n5. 为每个测试用例记录一个布尔结果，指明诊断检验是否在指定的滞后阶数上拒绝了“平方标准化残差中无残差自相关”的原假设。\n\n测试套件：\n- 用例 A（设定正确的基线，理想路径）：\n  - 数据生成过程：GARCH($1,1$)，参数为 $(\\omega,\\alpha,\\beta) = (0.05,0.05,0.90)$。\n  - 样本量：$T=3000$。\n  - Ljung-Box 最大滞后阶数：$m=20$。\n- 用例 B（模型设定错误，存在应被检测出的残留 ARCH 效应）：\n  - 数据生成过程：GARCH($2,1$)，参数为 $(\\omega,\\alpha_1,\\alpha_2,\\beta) = (0.02,0.04,0.08,0.86)$。\n  - 样本量：$T=4000$。\n  - Ljung-Box 最大滞后阶数：$m=20$。\n- 用例 C（模型设定错误，由于滞后截断选择不当和样本量有限，Ljung-Box 检验可能无法检测出残留的 ARCH 效应）：\n  - 数据生成过程：一个在滞后 $L=10$ 处具有单一长滞后效应的 ARCH 过程，定义为 $\\sigma_t^2 = \\omega + a_L \\cdot r_{t-L}^2$，参数为 $(\\omega,a_L)=(0.05,0.80)$。\n  - 样本量：$T=800$。\n  - Ljung-Box 最大滞后阶数：$m=1$。\n\n实现细节：\n- 估计使用高斯似然，模拟使用高斯新息。显式约束 GARCH($1,1$) 参数 $(\\omega,\\alpha,\\beta)$ 满足 $\\omega>0$、$\\alpha\\ge 0$、$\\beta\\ge 0$ 和 $\\alpha+\\beta<1$。\n- 使用从估计的 GARCH($1,1$) 模型得到的拟合条件方差路径来标准化残差。\n- 基于中心化数据使用样本自相关的标准定义来实现 Ljung-Box 统计量，并在 $m$ 阶滞后的大样本参考分布下推导尾部概率。\n- 在所有模拟中，保留最后的 $T$ 个观测值之前，使用至少 $1000$ 个观测值的预热期（burn-in）。\n\n最终输出格式：\n- 你的程序应生成单行输出，按顺序包含用例 A、B 和 C 的三个布尔决策，格式为方括号括起来的逗号分隔列表，例如 $[{\\rm True},{\\rm False},{\\rm True}]$。\n\n你的答案必须是一个完整的、可运行的程序。不需要也不允许用户输入。此问题不涉及任何物理单位；所有数值输出均为指定列表格式的无量纲布尔值。", "solution": "所提出的问题是计算计量经济学中一个明确定义的练习，要求实现 GARCH 模型分析的完整流程。它在科学上基于成熟的时间序列分析理论，内容自洽，并提出了一个客观、可形式化的任务。问题没有可识别的缺陷，因此有必要提供一个严谨的解决方案。\n\n该解决方案涉及几个顺序步骤：数据模拟、通过准最大似然估计（QMLE）进行模型估计、计算标准化残差，以及使用 Ljung-Box 检验进行诊断性检查。\n\n一个针对零均值收益率序列 $\\{r_t\\}$ 的通用 GARCH($p,q$) 过程由以下方程定义：\n$$ r_t = \\sigma_t z_t $$\n$$ \\sigma_t^2 = \\omega + \\sum_{i=1}^{q} \\alpha_i r_{t-i}^2 + \\sum_{j=1}^{p} \\beta_j \\sigma_{t-j}^2 $$\n此处，$\\{z_t\\}$ 是一个零均值和单位方差的独立同分布（i.i.d.）随机变量序列，我们假设其为标准正态分布，$z_t \\sim N(0,1)$。参数必须满足 $\\omega > 0$、$\\alpha_i \\ge 0$ 和 $\\beta_j \\ge 0$，以确保方差为非负。为使条件方差过程是弱平稳的，需要满足条件 $\\sum_{i=1}^{q} \\alpha_i + \\sum_{j=1}^{p} \\beta_j < 1$。\n\n该问题要求估计一个 GARCH($1,1$) 模型，其条件方差递归简化为：\n$$ \\sigma_t^2 = \\omega + \\alpha r_{t-1}^2 + \\beta \\sigma_{t-1}^2 $$\n待估计的参数向量为 $\\theta = (\\omega, \\alpha, \\beta)$。\n\n估计是使用 QMLE 进行的。假设新息是条件正态的，在信息集 $\\mathcal{F}_{t-1}$ 的条件下，观测值 $t$ 的对数似然为：\n$$ l_t(\\theta) = -\\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}\\log(\\sigma_t^2(\\theta)) - \\frac{r_t^2}{2\\sigma_t^2(\\theta)} $$\n样本量为 $T$ 的总对数似然是总和 $\\mathcal{L}(\\theta) = \\sum_{t=1}^T l_t(\\theta)$。QMLE 估计量 $\\hat{\\theta}$ 是在参数约束下最大化 $\\mathcal{L}(\\theta)$ 的 $\\theta$ 值。这等同于最小化负对数似然 $-\\mathcal{L}(\\theta)$。优化过程通过数值方法执行，通常使用诸如 L-BFGS-B 的拟牛顿法，该方法可以处理参数的箱形约束。严格平稳性类型的约束 $\\alpha + \\beta < 1$ 在目标函数中通过在其被违反时返回一个无穷大惩罚来强制执行。$\\sigma_t^2$ 的递归通过将 $\\sigma_1^2$ 设置为收益率序列的样本方差来初始化，这是一种常见且稳健的做法。\n\n一旦参数 $\\hat{\\theta} = (\\hat{\\omega}, \\hat{\\alpha}, \\hat{\\beta})$ 被估计出来，就使用这些估计值和 GARCH($1,1$) 递归来构建拟合的条件方差序列 $\\{\\hat{\\sigma}_t^2\\}_{t=1}^T$。然后，标准化残差计算如下：\n$$ \\hat{\\epsilon}_t = \\frac{r_t}{\\hat{\\sigma}_t} $$\n如果 GARCH($1,1$) 模型设定正确，序列 $\\{\\hat{\\epsilon}_t\\}$ 应近似为具有单位方差的独立同分布序列。因此，平方标准化残差 $\\{\\hat{\\epsilon}_t^2\\}$ 不应表现出显著的自相关。\n\n为检验这一假设，我们对序列 $x_t = \\hat{\\epsilon}_t^2$ 采用 Ljung-Box 检验。首先，我们计算 $\\{x_t\\}$ 直至指定最大滞后阶数 $m$ 的样本自相关。滞后 $k > 0$ 的样本自相关定义为：\n$$ \\hat{\\rho}_k = \\frac{\\sum_{t=k+1}^T (x_t - \\bar{x})(x_{t-k} - \\bar{x})}{\\sum_{t=1}^T (x_t - \\bar{x})^2} $$\n其中 $\\bar{x}$ 是 $\\{x_t\\}$ 的样本均值。\n\n然后，Ljung-Box Q-统计量计算如下：\n$$ Q = T(T+2) \\sum_{k=1}^m \\frac{\\hat{\\rho}_k^2}{T-k} $$\n在序列中没有自相关（即 $\\rho_1 = \\dots = \\rho_m = 0$）的原假设（$H_0$）下，统计量 $Q$ 渐近服从自由度为 $m$ 的卡方分布，即 $Q \\sim \\chi^2(m)$。我们计算 p 值为 $P(\\chi^2(m) > Q_{\\text{obs}})$，其中 $Q_{\\text{obs}}$ 是该统计量的观测值。如果此 p 值严格小于指定的显著性水平 $\\alpha = 0.05$，我们就拒绝原假设，这表明存在残留的 ARCH 效应。\n\n这整个过程将应用于三个测试用例：\n1.  **用例 A**：数据由 GARCH($1,1$) 过程生成，并拟合 GARCH($1,1$) 模型。这是一个设定正确的用例，我们预期诊断检验将不会拒绝原假设。\n2.  **用例 B**：数据由 GARCH($2,1$) 过程生成，但拟合的是 GARCH($1,1$) 模型。这种模型设定错误预计会留下未捕捉到的动态，导致平方标准化残差中出现自相关。我们预期 Ljung-Box 检验将拒绝原假设。\n3.  **用例 C**：数据由一个在滞后 $L=10$ 处具有特定滞后结构的 ARCH 过程生成。我们拟合一个 GARCH($1,1$) 模型，并应用最大滞后阶数仅为 $m=1$ 的 Ljung-Box 检验。由于该检验只检查滞后 1 阶，因此它不适合检测这种特定形式的模型设定错误。我们预期该检验将不会拒绝原假设，从而展示了诊断工具在配置不当时的局限性。\n\n该实现将把这些组件综合到一个程序中，对所有三个用例执行分析，并为每个用例报告布尔决策（拒绝/不拒绝）。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import chi2\n\ndef simulate_series(omega, alpha_coeffs, beta_coeffs, T, burn_in, seed):\n    \"\"\"\n    Simulates a time series from a general GARCH(p,q) process.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    q = len(alpha_coeffs)\n    p = len(beta_coeffs)\n    total_T = T + burn_in\n\n    r = np.zeros(total_T)\n    sigma2 = np.zeros(total_T)\n    z = rng.normal(size=total_T)\n\n    # Initialize with unconditional variance, assuming stationarity\n    uncond_var_denom = 1.0 - np.sum(alpha_coeffs) - np.sum(beta_coeffs)\n    # Ensure denominator is positive, preventing division by zero for non-stationary cases\n    uncond_var = omega / uncond_var_denom if uncond_var_denom > 0 else 1.0\n\n    # max_lag is the number of past values needed for the recursion\n    max_lag = max(p, q)\n    if max_lag == 0: # handle simple case of constant variance\n        r = np.sqrt(omega) * z\n        return r[burn_in:]\n\n    sigma2[:max_lag] = uncond_var\n    r[:max_lag] = np.sqrt(sigma2[:max_lag]) * z[:max_lag]\n    \n    # Convert lists to numpy arrays for vectorized operations\n    alpha_arr = np.array(alpha_coeffs)\n    beta_arr = np.array(beta_coeffs)\n\n    for t in range(max_lag, total_T):\n        arch_term = np.sum(alpha_arr * np.flip(r[t-q:t]**2)) if q > 0 else 0\n        garch_term = np.sum(beta_arr * np.flip(sigma2[t-p:t])) if p > 0 else 0\n        sigma2[t] = omega + arch_term + garch_term\n        r[t] = np.sqrt(max(1e-9, sigma2[t])) * z[t] # Failsafe for numerical stability\n\n    return r[burn_in:]\n\n\ndef garch11_neg_log_likelihood(params, r_series):\n    \"\"\"\n    Computes the negative of the log-likelihood for a GARCH(1,1) model.\n    \"\"\"\n    omega, alpha, beta = params\n    \n    # Parameter constraints\n    if omega <= 0 or alpha < 0 or beta < 0 or (alpha + beta) >= 1.0:\n        return np.inf\n\n    T = len(r_series)\n    sigma2 = np.zeros(T)\n    \n    # Initialize variance with sample variance\n    sigma2[0] = np.var(r_series)\n\n    for t in range(1, T):\n        sigma2[t] = omega + alpha * r_series[t-1]**2 + beta * sigma2[t-1]\n    \n    # Add a small constant to sigma2 to avoid log(0)\n    sigma2[sigma2 <= 0] = 1e-9\n\n    log_likelihood = -0.5 * np.sum(np.log(2 * np.pi) + np.log(sigma2) + r_series**2 / sigma2)\n    \n    return -log_likelihood\n\n\ndef estimate_garch11(r_series):\n    \"\"\"\n    Estimates GARCH(1,1) parameters using QMLE.\n    \"\"\"\n    initial_params = np.array([np.var(r_series)*0.05, 0.05, 0.9])\n    bounds = [(1e-9, None), (0, 1), (0, 1)]\n    \n    result = minimize(garch11_neg_log_likelihood, initial_params, args=(r_series,),\n                      method='L-BFGS-B', bounds=bounds)\n    \n    return result.x\n\n\ndef ljung_box_test(series, m):\n    \"\"\"\n    Computes the Ljung-Box Q-statistic and its p-value.\n    \"\"\"\n    T = len(series)\n    x_mean = np.mean(series)\n    \n    # Sample variance (autocovariance at lag 0)\n    gamma0 = np.sum((series - x_mean)**2) / T\n    if gamma0 == 0:\n        return 1.0 # No variation, so no autocorrelation\n    \n    acf_sq_terms = []\n    for k in range(1, m + 1):\n        # Sample autocovariance at lag k\n        gamma_k = np.sum((series[k:] - x_mean) * (series[:-k] - x_mean)) / T\n        rho_k = gamma_k / gamma0\n        acf_sq_terms.append(rho_k**2 / (T - k))\n        \n    Q = T * (T + 2) * np.sum(acf_sq_terms)\n    \n    # p-value from chi-squared distribution\n    p_value = chi2.sf(Q, df=m)\n    return p_value\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    burn_in = 1000\n    seed = 12345\n    alpha_level = 0.05\n\n    test_cases = [\n        {\n            'name': 'Case A',\n            'dgp_params': {'omega': 0.05, 'alpha': [0.05], 'beta': [0.90]},\n            'T': 3000,\n            'm': 20\n        },\n        {\n            'name': 'Case B',\n            'dgp_params': {'omega': 0.02, 'alpha': [0.04, 0.08], 'beta': [0.86]},\n            'T': 4000,\n            'm': 20\n        },\n        {\n            'name': 'Case C',\n            'dgp_params': {'omega': 0.05, 'alpha': [0]*9 + [0.80], 'beta': []},\n            'T': 800,\n            'm': 1\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # 1. Simulate return series\n        dgp = case['dgp_params']\n        r_series = simulate_series(dgp['omega'], dgp['alpha'], dgp['beta'], case['T'], burn_in, seed)\n        \n        # 2. Fit GARCH(1,1) model\n        est_omega, est_alpha, est_beta = estimate_garch11(r_series)\n        \n        # 3. Compute standardized residuals\n        T = case['T']\n        sigma2_hat = np.zeros(T)\n        sigma2_hat[0] = np.var(r_series)\n        for t in range(1, T):\n            sigma2_hat[t] = est_omega + est_alpha * r_series[t-1]**2 + est_beta * sigma2_hat[t-1]\n        \n        std_residuals = r_series / np.sqrt(np.maximum(1e-9, sigma2_hat))\n        sq_std_residuals = std_residuals**2\n        \n        # 4. Apply Ljung-Box test\n        p_value = ljung_box_test(sq_std_residuals, case['m'])\n        \n        # 5. Record boolean decision\n        reject_null = p_value < alpha_level\n        results.append(reject_null)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}]}