{"hands_on_practices": [{"introduction": "在经济学中，最优化是理解企业和个人战略决策的核心工具。这个练习将多维无约束最优化方法应用于一个经典的伯特兰双头垄断模型，其中两家公司通过定价进行竞争。通过求解每个公司各自的利润最大化问题，你将实践如何找到纳什均衡，这体现了在战略互动环境中如何运用最优化的思想。[@problem_id:2445317]", "id": "2445317", "problem": "两家公司，索引为 $i \\in \\{1,2\\}$，生产差异化产品并同时选择价格 $p_1$ 和 $p_2$，其中 $p_1, p_2 \\in \\mathbb{R}$。市场需求系统是线性的，由下式给出：\n$$\nq_1 = 100 - 3 p_1 + p_2, \\quad q_2 = 100 - 3 p_2 + p_1,\n$$\n其中当收取的价格向量为 $(p_1, p_2)$ 时，$q_i$ 是对公司 $i$ 的需求量。公司 $1$ 的边际成本为常数 $c_1 = 10$，公司 $2$ 的边际成本为常数 $c_2 = 20$。每家公司的利润为\n$$\n\\pi_1(p_1,p_2) = (p_1 - c_1) q_1, \\quad \\pi_2(p_1,p_2) = (p_2 - c_2) q_2.\n$$\n纯策略纳什均衡是一个价格向量 $(p_1^\\star, p_2^\\star)$，使得 $p_1^\\star$ 在 $p_1 \\in \\mathbb{R}$ 的范围内最大化了 $\\pi_1(p_1, p_2^\\star)$，并且 $p_2^\\star$ 在 $p_2 \\in \\mathbb{R}$ 的范围内最大化了 $\\pi_2(p_1^\\star, p_2)$。\n\n求唯一的纯策略纳什均衡价格向量 $(p_1^\\star, p_2^\\star)$。提供精确值（不要四舍五入）。最终答案必须是价格的有序对。", "solution": "该问题要求在一个具有差异化产品和非对称成本的伯特兰竞争模型中，找到唯一的纯策略纳什均衡。对问题陈述进行验证。\n\n**步骤1：提取已知条件**\n提供了以下信息：\n-   公司：$i \\in \\{1,2\\}$\n-   价格：$p_1, p_2 \\in \\mathbb{R}$\n-   需求函数：$q_1 = 100 - 3 p_1 + p_2$ 和 $q_2 = 100 - 3 p_2 + p_1$。\n-   边际成本：$c_1 = 10$ 和 $c_2 = 20$。\n-   利润函数：$\\pi_1(p_1,p_2) = (p_1 - c_1) q_1$ 和 $\\pi_2(p_1,p_2) = (p_2 - c_2) q_2$。\n-   纳什均衡的定义：一个价格向量 $(p_1^\\star, p_2^\\star)$，其中 $p_1^\\star$ 最大化 $\\pi_1(p_1, p_2^\\star)$ 且 $p_2^\\star$ 最大化 $\\pi_2(p_1^\\star, p_2)$。\n\n**步骤2：使用提取的已知条件进行验证**\n该问题是微观经济博弈论中的一个标准练习。\n-   **科学依据**：该模型是一个经典的差异化产品伯特兰双寡头模型，是产业组织理论中的一个基本概念。它在科学上和数学上都是合理的。\n-   **适定性**：该问题是适定的。目标陈述清晰，并且为找到唯一解提供了所有必要的函数和参数，正如利润函数的严格凹性所表明的那样。\n-   **客观性**：该问题使用精确的数学语言陈述，没有主观或模糊的术语。\n-   **完整性**：该问题是自洽的，并提供了所有必要的信息。不存在矛盾。\n\n**步骤3：结论与行动**\n该问题有效。将构建解答。\n\n每家公司 $i$ 的利润函数由 $\\pi_i = (p_i - c_i) q_i$ 给出。代入给定的需求函数和成本参数，即可得出公司1和公司2的利润函数。\n\n对于公司1：\n$$\n\\pi_1(p_1, p_2) = (p_1 - 10)(100 - 3p_1 + p_2)\n$$\n对于公司2：\n$$\n\\pi_2(p_1, p_2) = (p_2 - 20)(100 - 3p_2 + p_1)\n$$\n在纳什均衡中，每家公司选择其价格以最大化自身利润，同时将另一家公司的价格视为给定。这个无约束优化问题可以通过找到每家公司的一阶条件来解决。\n\n首先，我们求公司1的最佳反应函数。我们将 $\\pi_1$ 对 $p_1$ 求导，并将导数设为零。\n$$\n\\frac{\\partial \\pi_1}{\\partial p_1} = (1)(100 - 3p_1 + p_2) + (p_1 - 10)(-3) = 0\n$$\n$$\n100 - 3p_1 + p_2 - 3p_1 + 30 = 0\n$$\n$$\n130 - 6p_1 + p_2 = 0\n$$\n对 $p_1$ 求解，得出公司1的最佳反应函数 $p_1(p_2)$：\n$$\np_1 = \\frac{130 + p_2}{6}\n$$\n为确认这是一个最大值，我们检查二阶条件：\n$$\n\\frac{\\partial^2 \\pi_1}{\\partial p_1^2} = -6 < 0\n$$\n利润函数相对于 $p_1$ 是严格凹的，因此对于任何给定的 $p_2$，一阶条件都能确定一个唯一的利润最大化价格。\n\n接下来，我们求公司2的最佳反应函数。我们将 $\\pi_2$ 对 $p_2$ 求导，并将导数设为零。\n$$\n\\frac{\\partial \\pi_2}{\\partial p_2} = (1)(100 - 3p_2 + p_1) + (p_2 - 20)(-3) = 0\n$$\n$$\n100 - 3p_2 + p_1 - 3p_2 + 60 = 0\n$$\n$$\n160 - 6p_2 + p_1 = 0\n$$\n对 $p_2$ 求解，得出公司2的最佳反应函数 $p_2(p_1)$：\n$$\np_2 = \\frac{160 + p_1}{6}\n$$\n二阶条件确认了这是一个最大值：\n$$\n\\frac{\\partial^2 \\pi_2}{\\partial p_2^2} = -6 < 0\n$$\n纳什均衡是同时满足两个最佳反应函数的价格向量 $(p_1^\\star, p_2^\\star)$。我们必须解以下线性方程组：\n$$\n\\begin{cases}\np_1^\\star = \\frac{130 + p_2^\\star}{6} \\\\\np_2^\\star = \\frac{160 + p_1^\\star}{6}\n\\end{cases}\n$$\n将第二个方程代入第一个方程：\n$$\np_1^\\star = \\frac{1}{6} \\left( 130 + \\frac{160 + p_1^\\star}{6} \\right)\n$$\n两边乘以 $6$：\n$$\n6p_1^\\star = 130 + \\frac{160 + p_1^\\star}{6}\n$$\n再次乘以 $6$ 以消去分数：\n$$\n36p_1^\\star = 6(130) + 160 + p_1^\\star\n$$\n$$\n36p_1^\\star = 780 + 160 + p_1^\\star\n$$\n$$\n35p_1^\\star = 940\n$$\n$$\np_1^\\star = \\frac{940}{35} = \\frac{188 \\times 5}{7 \\times 5} = \\frac{188}{7}\n$$\n现在，将 $p_1^\\star$ 的值代入公司2的最佳反应函数以求得 $p_2^\\star$：\n$$\np_2^\\star = \\frac{160 + p_1^\\star}{6} = \\frac{1}{6} \\left( 160 + \\frac{188}{7} \\right)\n$$\n$$\np_2^\\star = \\frac{1}{6} \\left( \\frac{160 \\times 7}{7} + \\frac{188}{7} \\right) = \\frac{1}{6} \\left( \\frac{1120 + 188}{7} \\right)\n$$\n$$\np_2^\\star = \\frac{1}{6} \\left( \\frac{1308}{7} \\right) = \\frac{1308}{42}\n$$\n分子和分母同除以 $6$：\n$$\np_2^\\star = \\frac{1308 \\div 6}{42 \\div 6} = \\frac{218}{7}\n$$\n因此，唯一的纯策略纳什均衡价格向量是 $(p_1^\\star, p_2^\\star) = (\\frac{188}{7}, \\frac{218}{7})$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{188}{7} & \\frac{218}{7}\n\\end{pmatrix}\n}\n$$"}, {"introduction": "最优化不仅适用于公司行为，也同样可以为个人决策提供深刻见解。本问题将求职者的最优申请策略构建成一个二次型最优化问题，其中的挑战在于推导出用参数表示的通用解析解。这个练习旨在锻炼你的抽象问题解决能力，并展示了如何利用矩阵代数这一强大工具来系统性地求解多维最优化问题。[@problem_id:2445351]", "id": "2445351", "problem": "一位求职者在一周内考虑两种求职渠道：正式渠道和非正式渠道。设 $n_F$ 表示通过正式渠道投递申请的强度，$n_I$ 表示通过非正式渠道投递申请的强度。将强度解释为短期内申请数量的连续代理变量，在独立且到达概率较小的条件下，获得工作的周期望薪资可以通过关于 $(n_F,n_I)$ 的二阶展开式来近似：\n$$\n\\mathbb{E}[S(n_F,n_I)] \\;=\\; \\beta_0 \\;+\\; \\beta_F \\, n_F \\;+\\; \\beta_I \\, n_I \\;-\\; \\frac{1}{2}\\Big( a_F \\, n_F^{2} \\;+\\; 2 a_{FI} \\, n_F n_I \\;+\\; a_I \\, n_I^{2} \\Big),\n$$\n其中 $\\beta_0$、$\\beta_F$、$\\beta_I$、$a_F$、$a_{FI}$ 和 $a_I$ 是参数。假设 $a_F \\gt 0$、$a_I \\gt 0$、$a_F a_I - a_{FI}^{2} \\gt 0$，并且 $\\beta_F \\gt 0$ 和 $\\beta_I \\gt 0$。这些条件确保了该近似函数在 $(n_F,n_I)$ 上是严格凹的，并存在唯一的内部最大化点。\n\n确定关于 $n_F$ 和 $n_I$ 的无约束优化量 $(n_F^{\\star}, n_I^{\\star})$，以最大化 $\\mathbb{E}[S(n_F,n_I)]$。将您的最终答案表示为单一的封闭形式解析表达式。无需四舍五入。", "solution": "该问题要求解周期望薪资函数的无约束优化量，该函数是关于两个变量的二次函数：通过正式渠道的申请强度 $n_F$ 和通过非正式渠道的申请强度 $n_I$。\n\n首先，需要对问题陈述进行验证。\n\n步骤1：提取已知条件。\n需要最大化的目标函数是：\n$$\n\\mathbb{E}[S(n_F,n_I)] = \\beta_0 + \\beta_F n_F + \\beta_I n_I - \\frac{1}{2}\\Big( a_F n_F^{2} + 2 a_{FI} n_F n_I + a_I n_I^{2} \\Big)\n$$\n优化的变量是 $n_F$ 和 $n_I$。\n参数是 $\\beta_0$、$\\beta_F$、$\\beta_I$、$a_F$、$a_{FI}$ 和 $a_I$。\n给定的条件如下：\n$a_F > 0$\n$a_I > 0$\n$a_F a_I - a_{FI}^{2} > 0$\n$\\beta_F > 0$\n$\\beta_I > 0$\n\n步骤2：使用提取的已知条件进行验证。\n该问题在科学上和数学上是合理的。它提出了一个针对二次函数的标准无约束优化问题。该函数代表一个二阶泰勒近似，这是包括计算经济学在内的许多科学领域中常用且有效的技术。该问题是适定的；所给定的参数 $a_F$、$a_I$ 和 $a_{FI}$ 的条件正是确保二次项的海森矩阵(Hessian matrix)为正定矩阵所需的条件，这又使得目标函数是严格凹的，从而保证了唯一的全局最大值。问题陈述客观，数学术语和约束定义精确。它是自洽的，不违反任何基本原则。\n\n步骤3：结论和行动。\n问题有效。将推导求解。\n\n设目标函数为 $J(n_F, n_I)$。为了找到无约束最大值，我们必须应用一阶必要条件，即函数在最优点 $(n_F^{\\star}, n_I^{\\star})$ 的梯度必须为零向量。\n$J(n_F, n_I)$ 的梯度是其偏导数向量：$\\nabla J = \\begin{pmatrix} \\frac{\\partial J}{\\partial n_F} & \\frac{\\partial J}{\\partial n_I} \\end{pmatrix}^T$。\n\n我们计算偏导数：\n$$\n\\frac{\\partial J}{\\partial n_F} = \\frac{\\partial}{\\partial n_F} \\left( \\beta_0 + \\beta_F n_F + \\beta_I n_I - \\frac{1}{2} a_F n_F^{2} - a_{FI} n_F n_I - \\frac{1}{2} a_I n_I^{2} \\right) = \\beta_F - a_F n_F - a_{FI} n_I\n$$\n$$\n\\frac{\\partial J}{\\partial n_I} = \\frac{\\partial}{\\partial n_I} \\left( \\beta_0 + \\beta_F n_F + \\beta_I n_I - \\frac{1}{2} a_F n_F^{2} - a_{FI} n_F n_I - \\frac{1}{2} a_I n_I^{2} \\right) = \\beta_I - a_{FI} n_F - a_I n_I\n$$\n\n将这些偏导数设为零，得到关于两个未知数 $n_F$ 和 $n_I$ 的二元线性方程组：\n$$\na_F n_F + a_{FI} n_I = \\beta_F\n$$\n$$\na_{FI} n_F + a_I n_I = \\beta_I\n$$\n\n该方程组可以用矩阵形式表示为：\n$$\n\\begin{pmatrix} a_F & a_{FI} \\\\ a_{FI} & a_I \\end{pmatrix} \\begin{pmatrix} n_F \\\\ n_I \\end{pmatrix} = \\begin{pmatrix} \\beta_F \\\\ \\beta_I \\end{pmatrix}\n$$\n\n最大值的二阶充分条件要求二阶偏导数的海森矩阵 $H$ 在临界点是负定的。我们来计算海森矩阵：\n$$\nH = \\begin{pmatrix} \\frac{\\partial^2 J}{\\partial n_F^2} & \\frac{\\partial^2 J}{\\partial n_F \\partial n_I} \\\\ \\frac{\\partial^2 J}{\\partial n_I \\partial n_F} & \\frac{\\partial^2 J}{\\partial n_I^2} \\end{pmatrix} = \\begin{pmatrix} -a_F & -a_{FI} \\\\ -a_{FI} & -a_I \\end{pmatrix}\n$$\n如果一个矩阵的顺序主子式符号交替，并且以负号开始，那么该矩阵是负定的。\n一阶主子式是 $H_1 = -a_F$。由于问题陈述 $a_F > 0$，所以有 $H_1 < 0$。\n二阶主子式是 $H$ 的行列式：\n$$\n\\det(H) = (-a_F)(-a_I) - (-a_{FI})^2 = a_F a_I - a_{FI}^2\n$$\n问题陈述 $a_F a_I - a_{FI}^2 > 0$。\n由于顺序主子式按要求符号交替（-，+），海森矩阵是负定的。这证实了一阶条件的解对应于一个严格的局部最大值。由于函数是全局凹的，因此这也是唯一的全局最大值。\n\n为了求解关于 $(n_F^{\\star}, n_I^{\\star})$ 的线性方程组，我们可以使用矩阵求逆法。设系数矩阵为 $A = \\begin{pmatrix} a_F & a_{FI} \\\\ a_{FI} & a_I \\end{pmatrix}$。其行列式为 $\\det(A) = a_F a_I - a_{FI}^2$，根据已知条件，该值为正。$A$ 的逆矩阵是：\n$$\nA^{-1} = \\frac{1}{a_F a_I - a_{FI}^2} \\begin{pmatrix} a_I & -a_{FI} \\\\ -a_{FI} & a_F \\end{pmatrix}\n$$\n然后，通过将常数向量左乘 $A^{-1}$ 来找到解向量：\n$$\n\\begin{pmatrix} n_F^{\\star} \\\\ n_I^{\\star} \\end{pmatrix} = A^{-1} \\begin{pmatrix} \\beta_F \\\\ \\beta_I \\end{pmatrix} = \\frac{1}{a_F a_I - a_{FI}^2} \\begin{pmatrix} a_I & -a_{FI} \\\\ -a_{FI} & a_F \\end{pmatrix} \\begin{pmatrix} \\beta_F \\\\ \\beta_I \\end{pmatrix}\n$$\n执行矩阵-向量乘法，得到最优强度的表达式：\n$$\n\\begin{pmatrix} n_F^{\\star} \\\\ n_I^{\\star} \\end{pmatrix} = \\frac{1}{a_F a_I - a_{FI}^2} \\begin{pmatrix} a_I \\beta_F - a_{FI} \\beta_I \\\\ a_F \\beta_I - a_{FI} \\beta_F \\end{pmatrix}\n$$\n因此，优化量的分量是：\n$$\nn_F^{\\star} = \\frac{a_I \\beta_F - a_{FI} \\beta_I}{a_F a_I - a_{FI}^2}\n$$\n$$\nn_I^{\\star} = \\frac{a_F \\beta_I - a_{FI} \\beta_F}{a_F a_I - a_{FI}^2}\n$$\n这些表达式代表了最大化期望薪资的唯一最优申请强度。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{a_I \\beta_F - a_{FI} \\beta_I}{a_F a_I - a_{FI}^{2}} & \\frac{a_F \\beta_I - a_{FI} \\beta_F}{a_F a_I - a_{FI}^{2}} \\end{pmatrix}}$$"}, {"introduction": "当我们面临无法轻易求得解析解的复杂现实问题时，数值方法就显得至关重要。这个编程练习将带你从解析方法过渡到数值方法，实现梯度下降算法——这是计算经济学和金融学中最基本的迭代优化技术之一。通过将其应用于不同性质的目标函数（包括良态、病态二次函数以及非二次的对数似然函数），你将获得宝贵的实践经验，并深入理解该算法在不同场景下的表现和收敛特性。[@problem_id:2445371]", "id": "2445371", "problem": "给定多个计算经济学和金融学领域中的多维平滑目标函数。对于每个指定的测试用例，构建一个迭代无约束最小化方法，根据 $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{d}_k$（其中下降方向为 $\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k)$）生成序列 $\\{\\mathbf{x}_k\\}_{k \\ge 0}$。在每次迭代 $k$ 中，步长 $\\alpha_k$ 必须从几何序列 $\\{\\alpha_0 \\rho^m: m \\in \\{0,1,2,\\dots\\}\\}$ 中选取，其中 $\\alpha_0 \\in (0,\\infty)$ 和 $\\rho \\in (0,1)$ 为固定值且对所有测试用例通用，以满足 Armijo 充分下降条件：\n$$\nf(\\mathbf{x}_k + \\alpha_k \\mathbf{d}_k) \\le f(\\mathbf{x}_k) + c\\,\\alpha_k \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k,\n$$\n其中 $c \\in (0,1)$ 是一个对所有测试用例通用的固定常数。从给定的起始点开始初始化，当 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\varepsilon$ 或当迭代次数 $k$ 达到指定的最大迭代次数时终止。所有计算都在实数域上进行。不使用角度。不涉及物理单位。\n\n对所有测试用例使用以下固定参数：$\\alpha_0 = 1$，$\\rho = \\tfrac{1}{2}$，$c = 10^{-4}$，$\\varepsilon = 10^{-8}$ 以及 $\\text{max\\_iter} = 10000$。\n\n测试套件（所有矩阵和向量均已明确写出）：\n\n- 测试用例 1（金融学中的均值-方差二次型问题）：\n  - 决策变量 $\\mathbf{w} \\in \\mathbb{R}^2$。\n  - 目标函数\n    $$\n    f_1(\\mathbf{w}) = \\tfrac{1}{2}\\,\\mathbf{w}^\\top \\boldsymbol{\\Sigma}\\,\\mathbf{w} - \\boldsymbol{\\mu}^\\top \\mathbf{w},\n    \\quad\n    \\boldsymbol{\\Sigma} =\n    \\begin{bmatrix}\n    2 & 0.8 \\\\\n    0.8 & 1.5\n    \\end{bmatrix},\n    \\quad\n    \\boldsymbol{\\mu} =\n    \\begin{bmatrix}\n    0.5 \\\\\n    0.3\n    \\end{bmatrix}.\n    $$\n  - 初始条件 $\\mathbf{w}_0 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$。\n\n- 测试用例 2（病态二次型问题）：\n  - 决策变量 $\\mathbf{x} \\in \\mathbb{R}^2$。\n  - 目标函数\n    $\n    f_2(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^\\top \\mathbf{H}\\,\\mathbf{x} - \\mathbf{b}^\\top \\mathbf{x},\n    \\quad\n    \\mathbf{H} =\n    \\begin{bmatrix}\n    1000 & 0 \\\\\n    0 & 1\n    \\end{bmatrix},\n    \\quad\n    \\mathbf{b} =\n    \\begin{bmatrix}\n    1 \\\\\n    1\n    \\end{bmatrix}.\n    $\n  - 初始条件 $\\mathbf{x}_0 = \\begin{bmatrix} 10 \\\\ 10 \\end{bmatrix}$。\n\n- 测试用例 3（二元选择的负对数似然函数，使用 logistic 连接函数，非线性可分）：\n  - 参数向量 $\\boldsymbol{\\theta} \\in \\mathbb{R}^2$。\n  - 数据矩阵\n    $\n    \\mathbf{X} =\n    \\begin{bmatrix}\n    1 & -2 \\\\\n    1 & -1 \\\\\n    1 & 1 \\\\\n    1 & 2\n    \\end{bmatrix},\n    $\n    标签向量\n    $\n    \\mathbf{y} =\n    \\begin{bmatrix}\n    -1 \\\\\n    1 \\\\\n    -1 \\\\\n    1\n    \\end{bmatrix}。\n    $\n  - 目标函数\n    $$\n    f_3(\\boldsymbol{\\theta}) = \\sum_{i=1}^{4} \\log\\!\\big(1 + \\exp(-y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta})\\big),\n    $$\n    其中 $\\mathbf{x}_i^\\top$ 是 $\\mathbf{X}$ 的第 $i$ 行，$y_i$ 是 $\\mathbf{y}$ 的第 $i$ 个元素。\n  - 初始条件 $\\boldsymbol{\\theta}_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n\n对于每个测试用例，使用上述通用参数运行迭代最小化过程，并报告终止时最小化的目标函数值 $f(\\mathbf{x}^\\star)$。你的程序不能读取任何输入。你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按测试用例的顺序给出结果，每个条目四舍五入到六位小数，例如 $[0.123456,1.234567,2.345678]$。", "solution": "问题陈述经过了严格评估，并被确定为有效。它在无约束优化领域提出了一组清晰、数学上合理且适定的任务。这些目标函数是计算经济学和金融学中的标准示例，所有必需的参数和初始条件都已明确无误地提供，没有任何歧义或矛盾。所指定的算法，即基于 Armijo 条件并采用回溯线搜索的梯度下降法，是解决此类问题的基础且合适的方法。我们现在将进行形式化的推导和求解。\n\n问题的核心是实现最速下降法来最小化一个平滑函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$。这是一个迭代算法，它使用以下更新规则生成一个点序列 $\\{\\mathbf{x}_k\\}_{k \\ge 0}$：\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{d}_k\n$$\n方向 $\\mathbf{d}_k$ 被选为目标函数在当前迭代点 $\\mathbf{x}_k$ 处的负梯度，即最速下降方向：\n$$\n\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k)\n$$\n步长 $\\alpha_k > 0$ 通过回溯线搜索过程确定，以确保目标函数值有充分的下降。从初始猜测 $\\alpha = \\alpha_0$ 开始，步长被连续乘以一个因子 $\\rho \\in (0,1)$ 进行缩减，直到满足 Armijo 条件：\n$$\nf(\\mathbf{x}_k + \\alpha \\mathbf{d}_k) \\le f(\\mathbf{x}_k) + c\\,\\alpha \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k\n$$\n其中 $c \\in (0,1)$ 是一个小常数。对于所有测试用例，给定参数固定为 $\\alpha_0 = 1$, $\\rho = \\frac{1}{2}$ 和 $c = 10^{-4}$。\n\n当梯度的欧几里得范数低于指定的容差 $\\varepsilon = 10^{-8}$，或当迭代次数 $k$ 达到最大限制 $\\text{max\\_iter} = 10000$ 时，算法终止，并将当前点 $\\mathbf{x}_k$ 报告为近似最小化点 $\\mathbf{x}^\\star$。\n\n我们现在将此通用过程应用于三个指定的测试用例。\n\n**测试用例 1：均值-方差二次型**\n目标函数是投资组合优化中的一个标准二次型：\n$$\nf_1(\\mathbf{w}) = \\tfrac{1}{2}\\,\\mathbf{w}^\\top \\boldsymbol{\\Sigma}\\,\\mathbf{w} - \\boldsymbol{\\mu}^\\top \\mathbf{w}\n$$\n其中 $\\mathbf{w} \\in \\mathbb{R}^2$, $\\boldsymbol{\\Sigma} = \\begin{bmatrix} 2 & 0.8 \\\\ 0.8 & 1.5 \\end{bmatrix}$, 且 $\\boldsymbol{\\mu} = \\begin{bmatrix} 0.5 \\\\ 0.3 \\end{bmatrix}$。对于对称矩阵 $\\mathbf{A}$，一般二次函数 $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} - \\mathbf{b}^\\top \\mathbf{x}$ 的梯度为 $\\nabla f(\\mathbf{x}) = \\mathbf{A}\\mathbf{x} - \\mathbf{b}$。因此，$f_1$ 的梯度为：\n$$\n\\nabla f_1(\\mathbf{w}) = \\boldsymbol{\\Sigma}\\,\\mathbf{w} - \\boldsymbol{\\mu}\n$$\n$f_1$ 的 Hessian 矩阵为 $\\nabla^2 f_1(\\mathbf{w}) = \\boldsymbol{\\Sigma}$。矩阵 $\\boldsymbol{\\Sigma}$ 是对称的，其特征值约为 $2.55$ 和 $0.95$，均为正数。因此，$\\boldsymbol{\\Sigma}$ 是正定的，这意味着 $f_1$ 是严格凸的，并拥有唯一的全局最小值。最速下降算法保证收敛到该最小值。算法从 $\\mathbf{w}_0 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ 初始化。\n\n**测试用例 2：病态二次型**\n目标函数是另一个二次型：\n$$\nf_2(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^\\top \\mathbf{H}\\,\\mathbf{x} - \\mathbf{b}^\\top \\mathbf{x}\n$$\n其中 $\\mathbf{x} \\in \\mathbb{R}^2$, $\\mathbf{H} = \\begin{bmatrix} 1000 & 0 \\\\ 0 & 1 \\end{bmatrix}$, 且 $\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$。其梯度为：\n$$\n\\nabla f_2(\\mathbf{x}) = \\mathbf{H}\\,\\mathbf{x} - \\mathbf{b}\n$$\nHessian 矩阵 $\\mathbf{H}$ 是正定的，因为其特征值为 $1000$ 和 $1$。因此，$f_2$ 是严格凸的，具有唯一的最小值。$\\mathbf{H}$ 的条件数是其最大特征值与最小特征值之比，即 $1000/1 = 1000$。这个高条件数意味着 $f_2$ 的水平集是高度拉长的椭圆，这通常会减慢最速下降法的收敛速度。算法从 $\\mathbf{x}_0 = \\begin{bmatrix} 10 \\\\ 10 \\end{bmatrix}$ 开始。\n\n**测试用例 3：二元选择的负对数似然**\n目标函数是逻辑回归模型的负对数似然：\n$$\nf_3(\\boldsymbol{\\theta}) = \\sum_{i=1}^{4} \\log\\!\\big(1 + \\exp(-y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta})\\big)\n$$\n其中 $\\boldsymbol{\\theta} \\in \\mathbb{R}^2$。为了求梯度，我们对 $\\boldsymbol{\\theta}$ 的一个分量 $\\theta_j$ 求导：\n$$\n\\frac{\\partial f_3}{\\partial \\theta_j} = \\sum_{i=1}^{4} \\frac{1}{1 + \\exp(-y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta})} \\cdot \\frac{\\partial}{\\partial \\theta_j} \\left(1 + \\exp(-y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta})\\right)\n$$\n$$\n= \\sum_{i=1}^{4} \\frac{\\exp(-y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta})}{1 + \\exp(-y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta})} \\cdot (-y_i x_{ij})\n$$\n令 $\\sigma(z) = 1/(1+e^{-z})$ 为 logistic sigmoid 函数。表达式 $\\frac{\\exp(-z)}{1+\\exp(-z)}$ 可以重写为 $\\frac{1}{e^z+1} = \\sigma(-z)$。\n因此，梯度向量 $\\nabla f_3(\\boldsymbol{\\theta})$ 的分量为：\n$$\n[\\nabla f_3(\\boldsymbol{\\theta})]_j = \\sum_{i=1}^{4} \\sigma(-y_i \\mathbf{x}_i^\\top \\boldsymbol{\\theta}) (-y_i x_{ij})\n$$\n这可以写成紧凑的向量形式。设 $\\mathbf{p}$ 是一个向量，其元素为 $p_i = \\sigma(y_i \\mathbf{x}_i^\\top \\boldsymbol{\\theta})$。使用恒等式 $\\sigma(-z) = 1 - \\sigma(z)$，梯度为：\n$$\n\\nabla f_3(\\boldsymbol{\\theta}) = \\sum_{i=1}^{4} (1-p_i) (-y_i \\mathbf{x}_i) = \\sum_{i=1}^{4} y_i(p_i-1) \\mathbf{x}_i = \\mathbf{X}^\\top (\\mathbf{y} \\odot (\\mathbf{p} - \\mathbf{1}))\n$$\n其中 $\\odot$ 表示逐元素乘法，$\\mathbf{1}$ 是元素全为 1 的向量。该函数的 Hessian 矩阵可以被证明是半正定的。由于数据矩阵 $\\mathbf{X}$ 具有线性无关的列（满列秩），Hessian 矩阵实际上是正定的，确保了 $f_3$ 是严格凸的，并具有唯一的最小化点。算法从原点 $\\boldsymbol{\\theta}_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ 初始化。为了数值稳定性，$\\log(1+e^z)$ 的计算采用 log-sum-exp 模式。\n\n该算法根据这些推导在软件中实现，以计算每个用例的最终目标值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit, logsumexp\n\ndef solve():\n    \"\"\"\n    Solves the unconstrained optimization problems defined in the test suite\n    using the gradient descent method with backtracking line search.\n    \"\"\"\n    \n    # Define common parameters for the optimization algorithm\n    alpha0 = 1.0\n    rho = 0.5\n    c = 1e-4\n    epsilon = 1e-8\n    max_iter = 10000\n\n    def gradient_descent(f, grad_f, x0):\n        \"\"\"\n        Generic implementation of gradient descent with backtracking line search.\n\n        Args:\n            f (callable): The objective function.\n            grad_f (callable): The gradient of the objective function.\n            x0 (np.ndarray): The initial starting point.\n\n        Returns:\n            float: The minimized objective function value at termination.\n        \"\"\"\n        x = np.copy(x0).astype(np.float64)\n        \n        for k in range(max_iter):\n            grad = grad_f(x)\n            grad_norm = np.linalg.norm(grad)\n\n            # Termination condition: norm of the gradient is small enough\n            if grad_norm <= epsilon:\n                break\n            \n            d = -grad  # Steepest descent direction\n            \n            # Backtracking line search to find step length alpha\n            alpha = alpha0\n            fx = f(x)\n            grad_dot_d = np.dot(grad, d)\n            \n            # Armijo condition check\n            while f(x + alpha * d) > fx + c * alpha * grad_dot_d:\n                alpha = rho * alpha\n            \n            # Update the iterate\n            x = x + alpha * d\n            \n        return f(x)\n\n    results = []\n\n    # Test Case 1: Mean-variance quadratic\n    Sigma = np.array([[2.0, 0.8], [0.8, 1.5]])\n    mu = np.array([0.5, 0.3])\n    w0 = np.array([1.0, 1.0])\n    \n    def f1(w):\n        return 0.5 * w.T @ Sigma @ w - mu.T @ w\n    \n    def grad_f1(w):\n        return Sigma @ w - mu\n        \n    result1 = gradient_descent(f1, grad_f1, w0)\n    results.append(result1)\n\n    # Test Case 2: Ill-conditioned quadratic\n    H = np.array([[1000.0, 0.0], [0.0, 1.0]])\n    b = np.array([1.0, 1.0])\n    x0_2 = np.array([10.0, 10.0])\n\n    def f2(x):\n        return 0.5 * x.T @ H @ x - b.T @ x\n    \n    def grad_f2(x):\n        return H @ x - b\n        \n    result2 = gradient_descent(f2, grad_f2, x0_2)\n    results.append(result2)\n\n    # Test Case 3: Negative log-likelihood for binary choice\n    X = np.array([[1.0, -2.0], [1.0, -1.0], [1.0, 1.0], [1.0, 2.0]])\n    y = np.array([-1.0, 1.0, -1.0, 1.0])\n    theta0 = np.array([0.0, 0.0])\n\n    def f3(theta):\n        # log(1+exp(z)) is computed robustly as log(exp(0)+exp(z))\n        z = -y * (X @ theta)\n        return np.sum(logsumexp(np.vstack((np.zeros_like(z), z)), axis=0))\n\n    def grad_f3(theta):\n        # Gradient of sum_i log(1+exp(-y_i*x_i'theta)) is sum_i y_i(p_i-1)x_i\n        # where p_i = sigmoid(y_i*x_i'theta)\n        h = y * (X @ theta)\n        p = expit(h)  # Numerically stable sigmoid function\n        # Vectorized gradient calculation\n        return X.T @ (y * (p - 1.0))\n        \n    result3 = gradient_descent(f3, grad_f3, theta0)\n    results.append(result3)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"}]}