{"hands_on_practices": [{"introduction": "在计算实践中，仅仅知道一个算法的数学公式是远远不够的。真正的挑战在于当算法未能按预期工作时，能够诊断并修复问题。本练习将您置于一个真实世界的调试场景中：一个旨在通过牛顿法最小化目标函数的程序正在发散。您的任务是扮演一名“算法侦探”，仔细审查程序的诊断输出，并运用您对下降方向、赫森矩阵正定性以及 Armijo 条件等核心概念的理解，找出导致失败的根本原因。[@problem_id:2414722] 这项实践将极大地磨练您在应用数值方法时至关重要的批判性思维和问题解决能力。", "id": "2414722", "problem": "一家金融机构通过最小化逻辑斯谛模型（logistic specification）的负对数似然来估计一个二元违约概率模型，其方法是使用带回溯线搜索的 Newton 方法，该方法旨在强制满足 Armijo 充分下降条件。参数向量表示为 $\\beta \\in \\mathbb{R}^p$，目标函数为 $f(\\beta)$，梯度为 $\\nabla f(\\beta)$，Hessian 矩阵为 $\\nabla^2 f(\\beta)$。该实现打印出以下诊断信息。\n\n在第 $k=0$ 次迭代时：\n- $f(\\beta_0) = 120.35$。\n- $\\lVert \\nabla f(\\beta_0) \\rVert_2 = 85.7$。\n- $\\nabla^2 f(\\beta_0)$ 的最小和最大特征值分别为 $\\lambda_{\\min} = 3.1$ 和 $\\lambda_{\\max} = 214.2$。\n- 求解了 Newton 方向的线性系统，报告的内积为 $\\nabla f(\\beta_0)^\\top s_0 = -28.9$。\n- 参数为 $c_1 = 10^{-4}$ 的回溯线搜索接受了步长 $t_0 = 1$，并显示消息“Armijo satisfied”，代码报告 $f(\\beta_0 + t_0 s_0) = 134.7$。\n\n在第 $k=1$ 次迭代时：\n- $f(\\beta_1) = 134.7$。\n- $\\lVert \\nabla f(\\beta_1) \\rVert_2 = 92.4$。\n- 报告的 $\\nabla^2 f(\\beta_1)$ 的最小特征值为 $\\lambda_{\\min} = 2.7$。\n- 计算出的方向满足 $\\nabla f(\\beta_1)^\\top s_1 = -31.4$。\n- 线搜索再次接受了步长 $t_1 = 1$，并显示消息“Armijo satisfied”，且 $f(\\beta_1 + t_1 s_1) = 150.2$。\n\n假设逻辑斯谛模型的负对数似然被正确指定且二阶连续可微，并且数据集的规模和条件适中，因此当 Hessian 矩阵为正定时，数值线性代数是稳定的。\n\n根据这些诊断信息以及带 Armijo 回溯的无约束最小化 Newton 方法的标准属性，哪一个实现问题与观察到的不收敛现象最一致？\n\nA. Hessian 矩阵在组装时没有包含交叉偏导数项，导致其变为不定矩阵并产生非下降方向。\n\nB. Newton 系统求解时符号错误，实际上使用了 $s_k = \\nabla^2 f(\\beta_k)^{-1} \\nabla f(\\beta_k)$，这是一个上升方向。\n\nC. 线搜索中的 Armijo 充分下降测试在实现时不等号方向反了，因此错误地接受了使 $f$ 增加的步长。\n\nD. 算法对 $\\lVert \\nabla f(\\beta_k) \\rVert_2$ 的停止准则过于严格，导致其在接近最小值点时继续迭代而不是终止。", "solution": "用户提供了一个问题陈述，描述了一个用于最小化负对数似然函数的 Newton 方法的错误实现的输出。我必须验证该陈述，然后根据所提供的数据诊断出最可能的实现错误。\n\n**问题验证**\n\n首先，我将提取给定的信息并验证问题陈述。\n\n**已知条件：**\n-   **算法**：用于最小化目标函数 $f(\\beta)$ 的 Newton 方法。\n-   **更新规则**：$\\beta_{k+1} = \\beta_k + t_k s_k$，其中 $s_k$ 是 Newton 方向。\n-   **Newton 方向**：$s_k$ 是 $\\nabla^2 f(\\beta_k) s_k = - \\nabla f(\\beta_k)$ 的解。\n-   **线搜索**：使用回溯法强制满足 Armijo 充分下降条件，参数为 $c_1 = 10^{-4}$。\n-   **Armijo 条件**：$f(\\beta_k + t_k s_k) \\le f(\\beta_k) + c_1 t_k \\nabla f(\\beta_k)^\\top s_k$。\n-   **假设**：函数 $f$ 是逻辑斯谛模型的二阶连续可微的负对数似然。假设当 Hessian 矩阵为正定时，数值计算是稳定的。\n\n**第 $k=0$ 次迭代的数据：**\n-   $f(\\beta_0) = 120.35$\n-   $\\lVert \\nabla f(\\beta_0) \\rVert_2 = 85.7$\n-   $\\nabla^2 f(\\beta_0)$ 的特征值：$\\lambda_{\\min} = 3.1$, $\\lambda_{\\max} = 214.2$。\n-   $\\nabla f(\\beta_0)^\\top s_0 = -28.9$\n-   接受的步长：$t_0 = 1$。\n-   得到的函数值：$f(\\beta_1) = f(\\beta_0 + t_0 s_0) = 134.7$。\n-   算法消息：“Armijo satisfied”。\n\n**第 $k=1$ 次迭代的数据：**\n-   $f(\\beta_1) = 134.7$\n-   $\\lVert \\nabla f(\\beta_1) \\rVert_2 = 92.4$\n-   $\\nabla^2 f(\\beta_1)$ 的最小特征值：$\\lambda_{\\min} = 2.7$。\n-   $\\nabla f(\\beta_1)^\\top s_1 = -31.4$\n-   接受的步长：$t_1 = 1$。\n-   得到的函数值：$f(\\beta_2) = f(\\beta_1 + t_1 s_1) = 150.2$。\n-   算法消息：“Armijo satisfied”。\n\n**验证分析：**\n问题陈述在数值优化和统计学方面具有科学依据。所提供的数据似乎与算法的既定目标（通过 Armijo 规则进行最小化）相矛盾，但这是诊断问题的基础，而非问题陈述本身的缺陷。该问题是适定的、客观的，并包含足够的信息以进行逻辑诊断。对于逻辑斯谛回归的负对数似然，Hessian 矩阵保证是半正定的，对于非退化数据集，则是正定的。给定的正特征值（$\\lambda_{\\min} = 3.1$ 和 $\\lambda_{\\min} = 2.7$）与此属性一致。因此，该问题是有效的。\n\n**求解推导**\n\n该算法的目标是最小化 $f(\\beta)$。这要求函数值在每一步都减小，即 $f(\\beta_{k+1}) < f(\\beta_k)$。Armijo 条件旨在保证这一点，甚至确保“充分”的下降。\n\n我们来分析第 $k=0$ 次迭代时的 Armijo 条件：\n条件为 $f(\\beta_0 + t_0 s_0) \\le f(\\beta_0) + c_1 t_0 \\nabla f(\\beta_0)^\\top s_0$。\n代入给定值：\n-   左侧 (LHS)：$f(\\beta_0 + s_0) = 134.7$。\n-   右侧 (RHS)：$f(\\beta_0) + c_1 t_0 \\nabla f(\\beta_0)^\\top s_0 = 120.35 + (10^{-4})(1)(-28.9) = 120.35 - 0.00289 = 120.34711$。\n\n正确的 Armijo 条件要求 $134.7 \\le 120.34711$，这是不成立的。尽管如此，算法仍然报告“Armijo satisfied”。此外，函数值增加了：$f(\\beta_1) = 134.7 > f(\\beta_0) = 120.35$。对于一个最小化算法来说，这是一个失败。\n\n我们对第 $k=1$ 次迭代进行同样的分析：\n条件为 $f(\\beta_1 + t_1 s_1) \\le f(\\beta_1) + c_1 t_1 \\nabla f(\\beta_1)^\\top s_1$。\n代入给定值：\n-   LHS：$f(\\beta_1 + s_1) = 150.2$。\n-   RHS：$f(\\beta_1) + c_1 t_1 \\nabla f(\\beta_1)^\\top s_1 = 134.7 + (10^{-4})(1)(-31.4) = 134.7 - 0.00314 = 134.69686$。\n\n正确的 Armijo 条件要求 $150.2 \\le 134.69686$，这同样不成立。算法再次错误地报告“Armijo satisfied”，并且函数值增加：$f(\\beta_2) = 150.2 > f(\\beta_1) = 134.7$。\n\n算法正在系统性地失败。它采取的步骤增加了目标函数值，并正在远离最小值点，梯度范数的增加也表明了这一点（$\\lVert \\nabla f(\\beta_1) \\rVert_2 > \\lVert \\nabla f(\\beta_0) \\rVert_2$）。在函数值增加的情况下，唯一可能打印出“Armijo satisfied”的原因是该条件的逻辑测试实现不正确。\n\n让我们考虑实现中不等号方向反转的可能性：\n$f(\\beta_k + t_k s_k) \\ge f(\\beta_k) + c_1 t_k \\nabla f(\\beta_k)^\\top s_k$。\n\n-   在 $k=0$ 时，这将测试 $134.7 \\ge 120.34711$ 是否成立。这是成立的。\n-   在 $k=1$ 时，这将测试 $150.2 \\ge 134.69686$ 是否成立。这也是成立的。\n\n这个反向的不等式完美地解释了为什么步长 $t=1$ 在两次迭代中都被接受，尽管这导致了目标函数值的大幅增加。\n\n**逐项分析**\n\nA. Hessian 矩阵在组装时没有包含交叉偏导数项，导致其变为不定矩阵并产生非下降方向。\n**分析**：这一说法与数据相矛盾。在两次迭代中，Hessian 矩阵的最小特征值分别为 $\\lambda_{\\min} = 3.1$ 和 $\\lambda_{\\min} = 2.7$。由于最小特征值为正，Hessian 矩阵是正定的，而非不定的。此外，计算出的方向是下降方向，因为它们与梯度的内积 $\\nabla f(\\beta_0)^\\top s_0 = -28.9$ 和 $\\nabla f(\\beta_1)^\\top s_1 = -31.4$ 均为负数。\n**结论**：不正确。\n\nB. Newton 系统求解时符号错误，实际上使用了 $s_k = \\nabla^2 f(\\beta_k)^{-1} \\nabla f(\\beta_k)$，这是一个上升方向。\n**分析**：如果是这种情况，搜索方向 $s_k$ 将是一个上升方向。对于正定的 Hessian 矩阵 $\\nabla^2 f(\\beta_k)$，方向导数 $\\nabla f(\\beta_k)^\\top s_k$ 将是 $\\nabla f(\\beta_k)^\\top [\\nabla^2 f(\\beta_k)]^{-1} \\nabla f(\\beta_k) > 0$。然而，所提供的数据显示，两次迭代的 $\\nabla f(\\beta_k)^\\top s_k$ 都小于 0。因此，计算出的方向是下降方向，而不是上升方向。\n**结论**：不正确。\n\nC. 线搜索中的 Armijo 充分下降测试在实现时不等号方向反了，因此错误地接受了使 $f$ 增加的步长。\n**分析**：如上文推导所示，如果 Armijo 条件检查被实现为 $f(\\beta_{k+1}) \\ge f(\\beta_k) + c_1 t_k \\nabla f(\\beta_k)^\\top s_k$，算法将接受步长 $t_0=1$ 和 $t_1=1$，并在每次迭代时报告“Armijo satisfied”。这一假设与观察到的函数值增加（$120.35 \\to 134.7 \\to 150.2$）完全一致，并与所有提供的数值数据相匹配。\n**结论**：正确。\n\nD. 算法对 $\\lVert \\nabla f(\\beta_k) \\rVert_2$ 的停止准则过于严格，导致其在接近最小值点时继续迭代而不是终止。\n**分析**：停止准则的性质与在*每次迭代内部*观察到的错误行为无关。核心问题是算法正在发散（函数值和梯度范数都在增加），而不是收敛缓慢且未能停止。一个严格的停止准则不会导致发散；如果算法正在收敛，它只意味着算法运行时间更长。观察到的行为是优化步骤本身的根本性失败。\n**结论**：不正确。", "answer": "$$\\boxed{C}$$"}, {"introduction": "理论上的牛顿法虽然优雅，但在实践中可能因非正定赫森矩阵或步长过大而失效。一个稳健的优化器必须包含“安全保障”机制。本练习将指导您从头开始实现一个混合优化算法，它优先尝试牛顿步，但如果牛顿方向不是一个有效的下降方向，或无法满足充分下降条件，算法会智能地切换到更可靠的梯度下降方向。[@problem_id:2414720] 通过解决这个涵盖了凸、非凸和病态问题的编码挑战，您将掌握构建能够应对各种复杂情况的、生产级别的优化代码的核心技术。", "id": "2414720", "problem": "考虑光滑实值函数的无约束最小化问题，该问题采用一种在每次迭代中强制执行充分下降条件的更新规则。设 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 为一个二次连续可微函数。在迭代点 $x_k \\in \\mathbb{R}^n$ 处，定义梯度 $\\nabla f(x_k)$ 和Hessian矩阵 $\\nabla^2 f(x_k)$。一个候选的更新 $x_{k+1} = x_k + \\alpha_k p_k$ 必须满足带有常数 $c_1 \\in (0,1)$ 和 $\\rho \\in (0,1)$ 的Armijo充分下降条件，即\n$$\nf(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top p_k,\n$$\n其中 $\\alpha_k$ 通过形式为 $\\alpha_k \\in \\{ \\rho^j : j \\in \\{0,1,2,\\ldots\\} \\}$ 的回溯法从 $\\alpha_k = 1$ 开始选择。对于给定的 $x_k$，首先尝试选择线性系统\n$$\n\\nabla^2 f(x_k)\\, p_k = - \\nabla f(x_k)\n$$\n的解作为 $p_k$。如果该候选方向未能在回溯序列中产生一个满足充分下降条件的步长 $\\alpha_k$，则拒绝该方向，转而选择 $p_k = -\\nabla f(x_k)$，并通过相同的回溯充分下降条件确定 $\\alpha_k$。当 $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$ 或达到最大迭代次数 $K$ 时，迭代终止。\n\n使用以下固定常数实现此迭代方案：$c_1 = 10^{-4}$，$\\rho = 0.5$，容差 $\\varepsilon = 10^{-8}$，每次迭代的回溯限制为 $M = 50$ 次缩减，以及最大迭代次数 $K = 50$。如果线性系统在某个 $x_k$ 处是奇异的或病态的，则将由 $\\nabla^2 f(x_k)\\, p_k = - \\nabla f(x_k)$ 定义的方向视为失败，并如上所述继续使用 $p_k = -\\nabla f(x_k)$。所有计算都应在实数算术中执行，无需任何外部数据输入。\n\n将此方案应用于以下测试套件。在所有情况下，使用欧几里得范数 $\\|\\cdot\\|_2$ 作为终止准则，并在每次迭代时从步长 $\\alpha_k = 1$ 开始回溯。报告每种情况下由终止条件返回的最小化点的估计值。将报告的每个数字表示为四舍五入到小数点后六位的十进制数。\n\n测试用例 A（两个参数的二元选择负对数似然）。设参数为 $x = (b_0, b_1)^\\top \\in \\mathbb{R}^2$。设数据为 $\\{(x_i,y_i)\\}_{i=1}^5$，其中\n$$\n(x_1,y_1) = (-2,0),\\quad (x_2,y_2) = (-1,0),\\quad (x_3,y_3) = (0,1),\\quad (x_4,y_4) = (1,1),\\quad (x_5,y_5) = (2,0).\n$$\n定义负对数似然\n$$\nf(x) = \\sum_{i=1}^5 \\left[ \\log\\!\\left(1 + e^{b_0 + b_1 x_i}\\right) - y_i\\,(b_0 + b_1 x_i) \\right].\n$$\n使用初始点 $x_0 = (0,0)^\\top$。\n\n测试用例 B（病态二次型）。设 $x \\in \\mathbb{R}^2$。定义\n$$\nf(x) = \\tfrac{1}{2} x^\\top Q x - b^\\top x,\\quad Q = \\begin{bmatrix} 10^{-6} & 0 \\\\ 0 & 1 \\end{bmatrix},\\quad b = \\begin{bmatrix} 10^{-6} \\\\ 1 \\end{bmatrix}.\n$$\n使用初始点 $x_0 = (10,-10)^\\top$。\n\n测试用例 C（非凸一维四次函数）。设 $x \\in \\mathbb{R}$。定义\n$$\nf(x) = x^4 - 3 x^2 + x.\n$$\n使用初始点 $x_0 = 0.2$。\n\n输出规格。您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表，每个测试用例的结果按顺序显示如下：\n- 对于测试用例 A：一个包含两个数字的列表 $[b_0^\\star,b_1^\\star]$，\n- 对于测试用例 B：一个包含两个数字的列表 $[x_1^\\star,x_2^\\star]$，\n- 对于测试用例 C：一个数字 $x^\\star$，\n每个数字都四舍五入到小数点后六位。例如，格式必须是\n$$\n\\big[ [b_0^\\star,b_1^\\star], [x_1^\\star,x_2^\\star], x^\\star \\big].\n$$\n打印的行中不允许有空格。所有数字都应表示为普通小数。", "solution": "所提出的问题是数值优化领域中一个明确定义的任务。它要求实现一个结合了牛顿法和最速下降法的混合迭代算法，并辅以回溯线搜索以确保每一步都有充分的函数值下降。该问题在科学上是合理的，基于非线性优化的既定原则，并为三个不同的测试用例提供了所有必要的参数、初始条件和目标函数。因此，该问题被认为是有效的。\n\n要实现的算法是针对一个二次连续可微函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$ 的无约束最小化过程。给定一个迭代点 $x_k$，下一个迭代点 $x_{k+1}$ 通过 $x_{k+1} = x_k + \\alpha_k p_k$ 找到，其中 $p_k$ 是搜索方向，$\\alpha_k$ 是步长。\n\n该算法的核心是搜索方向 $p_k$ 的选择。主要选择是牛顿方向，通过求解以下线性系统获得：\n$$\n\\nabla^2 f(x_k) p_k = - \\nabla f(x_k)\n$$\n其中 $\\nabla f(x_k)$ 是 $f$ 在 $x_k$ 处的梯度，$\\nabla^2 f(x_k)$ 是 $f$ 在 $x_k$ 处的Hessian矩阵。对于二次函数，这个方向是最优的，并且在Hessian矩阵是正定的最小值附近提供快速的局部收敛（二次收敛）。然而，如果Hessian矩阵是奇异的，牛顿方向可能未定义；如果Hessian矩阵不是正定的，它也可能不是一个下降方向。如果 $\\nabla f(x_k)^\\top p_k < 0$，则方向 $p_k$ 是一个下降方向。对于牛顿方向，$\\nabla f(x_k)^\\top p_k = -\\nabla f(x_k)^\\top (\\nabla^2 f(x_k))^{-1} \\nabla f(x_k)$，仅当 $\\nabla^2 f(x_k)$ 是正定时，该值才为负。如果牛顿方向不是下降方向或未能产生合适的步长，算法必须切换到一个稳健的备用方案。\n\n备用方向是最速下降方向，$p_k = -\\nabla f(x_k)$。只要梯度非零，这个方向保证是下降方向，因为 $\\nabla f(x_k)^\\top p_k = -\\nabla f(x_k)^\\top \\nabla f(x_k) = -\\|\\nabla f(x_k)\\|_2^2 < 0$。虽然其收敛通常较慢（线性收敛），但它能确保从任何 $\\nabla f(x_k) \\neq 0$ 的点取得进展。\n\n步长 $\\alpha_k$ 由回溯线搜索确定。从试探步长 $\\alpha = 1$ 开始，步长被连续乘以一个因子 $\\rho \\in (0,1)$，直到满足Armijo充分下降条件：\n$$\nf(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k)^\\top p_k\n$$\n其中 $c_1 \\in (0,1)$ 是一个常数。问题设定 $c_1 = 10^{-4}$ 和 $\\rho = 0.5$。回溯步骤被限制在 $M=50$ 次以内。\n\n整个迭代过程如下：\n对于 $k = 0, 1, 2, \\ldots, K-1$:\n1. 计算梯度 $g_k = \\nabla f(x_k)$。如果 $\\|g_k\\|_2 \\le \\varepsilon$，则终止。\n2. 尝试计算牛顿方向 $p_{\\text{Newton}}$。如果 $\\nabla^2f(x_k)$ 是奇异的，或者得到的方向不是下降方向（$\\nabla f(x_k)^\\top p_{\\text{Newton}} \\ge 0$），则此尝试失败。\n3. 如果牛顿方向有效，则执行回溯线搜索。如果在 $M$ 次缩减内找到步长 $\\alpha_k$，则使用 $p_k = p_{\\text{Newton}}$ 和 $\\alpha_k$ 执行更新。\n4. 如果牛顿方向步骤因任何原因失败（奇异性、不是下降方向或回溯失败），算法将退回到最速下降方向 $p_k = -\\nabla f(x_k)$，并执行另一次回溯线搜索以找到 $\\alpha_k$。\n5. 更新迭代点：$x_{k+1} = x_k + \\alpha_k p_k$。\n如果梯度范数低于容差 $\\varepsilon=10^{-8}$ 或达到最大迭代次数 $K=50$，则过程终止。\n\n此实现将把该逻辑应用于三个测试用例。对于每个用例，我们必须提供目标函数 $f(x)$、其梯度 $\\nabla f(x)$ 和其Hessian矩阵 $\\nabla^2 f(x)$ 的解析形式。\n\n**测试用例 A：二元选择负对数似然**\n目标函数是逻辑斯蒂回归模型的负对数似然。\n$f(b_0, b_1) = \\sum_{i=1}^5 \\left[ \\log(1 + e^{b_0 + b_1 x_i}) - y_i(b_0 + b_1 x_i) \\right]$。\n设 $u_i(b) = b_0 + b_1 x_i$。sigmoid函数为 $\\sigma(u) = 1/(1+e^{-u})$。\n梯度分量为：\n$$\n\\frac{\\partial f}{\\partial b_0} = \\sum_{i=1}^5 (\\sigma(u_i) - y_i)\n$$\n$$\n\\frac{\\partial f}{\\partial b_1} = \\sum_{i=1}^5 x_i (\\sigma(u_i) - y_i)\n$$\nHessian矩阵分量为（使用 $\\sigma'(u) = \\sigma(u)(1-\\sigma(u))$）：\n$$\n\\frac{\\partial^2 f}{\\partial b_0^2} = \\sum_{i=1}^5 \\sigma(u_i)(1-\\sigma(u_i))\n$$\n$$\n\\frac{\\partial^2 f}{\\partial b_1 \\partial b_0} = \\sum_{i=1}^5 x_i \\sigma(u_i)(1-\\sigma(u_i))\n$$\n$$\n\\frac{\\partial^2 f}{\\partial b_1^2} = \\sum_{i=1}^5 x_i^2 \\sigma(u_i)(1-\\sigma(u_i))\n$$\n该Hessian矩阵总是半正定的，确保牛顿方向（如果定义了）是一个下降方向。\n\n**测试用例 B：病态二次型**\n目标函数是 $f(x) = \\tfrac{1}{2} x^\\top Q x - b^\\top x$。\n梯度是 $\\nabla f(x) = Qx - b$。\nHessian矩阵是常数：$\\nabla^2 f(x) = Q$。\n由于 $Q$ 是一个常数正定矩阵，使用完整步长（$\\alpha=1$）的牛顿法将在单次迭代中找到精确的最小值 $x^*=Q^{-1}b$。\n\n**测试用例 C：非凸一维四次函数**\n目标函数是 $f(x) = x^4 - 3x^2 + x$。\n梯度（导数）是 $f'(x) = 4x^3 - 6x + 1$。\nHessian矩阵（二阶导数）是 $f''(x) = 12x^2 - 6$。\n在初始点 $x_0 = 0.2$ 处，Hessian值为 $f''(0.2) = 12(0.04) - 6 = -5.52$，为负数。因此，在 $x_0$ 处的牛顿方向将是一个上升方向，算法必须在第一次迭代时回退到最速下降法。\n\n以下Python代码实现了所描述的算法，并将其应用于三个测试用例，按规定格式化输出。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the optimization for all test cases and print results.\n    \"\"\"\n    # Fixed constants for the algorithm\n    C1 = 1e-4\n    RHO = 0.5\n    EPS = 1e-8\n    MAX_ITER = 50\n    BT_LIMIT = 50\n\n    def optimizer(f, grad_f, hess_f, x0):\n        \"\"\"\n        Implements the hybrid Newton/Gradient-Descent optimization algorithm.\n        \"\"\"\n        x = np.array(x0, dtype=float)\n\n        for _ in range(MAX_ITER):\n            g = grad_f(x)\n            if np.linalg.norm(g) <= EPS:\n                return x\n\n            p = None\n            alpha = None\n            newton_step_succeeded = False\n\n            # --- Attempt Newton's Method ---\n            try:\n                H = hess_f(x)\n                p_newton = np.linalg.solve(H, -g)\n\n                # Newton direction must be a descent direction\n                if np.dot(g, p_newton) < 0:\n                    # Backtracking line search for Newton's direction\n                    alpha_bt = 1.0\n                    fk = f(x)\n                    g_dot_p = np.dot(g, p_newton)\n                    for _ in range(BT_LIMIT):\n                        if f(x + alpha_bt * p_newton) <= fk + C1 * alpha_bt * g_dot_p:\n                            p = p_newton\n                            alpha = alpha_bt\n                            newton_step_succeeded = True\n                            break\n                        alpha_bt *= RHO\n            except np.linalg.LinAlgError:\n                # Hessian is singular or ill-posed\n                pass\n\n            # --- Fallback to Steepest Descent ---\n            if not newton_step_succeeded:\n                p_sd = -g\n                \n                # Backtracking line search for steepest descent\n                alpha_bt = 1.0\n                fk = f(x)\n                g_dot_p = np.dot(g, p_sd)\n                for _ in range(BT_LIMIT):\n                    if f(x + alpha_bt * p_sd) <= fk + C1 * alpha_bt * g_dot_p:\n                        p = p_sd\n                        alpha = alpha_bt\n                        break\n                    alpha_bt *= RHO\n            \n            # If no step could be found (highly unlikely for steepest descent)\n            if p is None or alpha is None:\n                return x\n\n            # Update x\n            x = x + alpha * p\n        \n        return x\n\n    # --- Test Case A: Logistic Regression ---\n    x_data_A = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    y_data_A = np.array([0.0, 0.0, 1.0, 1.0, 0.0])\n    x0_A = np.array([0.0, 0.0])\n\n    def f_A(b):\n        u = b[0] + b[1] * x_data_A\n        # Use np.logaddexp for numerical stability: log(1+exp(x)) = logaddexp(0,x)\n        return np.sum(np.logaddexp(0, u) - y_data_A * u)\n\n    def grad_f_A(b):\n        u = b[0] + b[1] * x_data_A\n        # Sigmoid function: 1 / (1 + exp(-u))\n        sigma = 1 / (1 + np.exp(-u))\n        diff = sigma - y_data_A\n        return np.array([np.sum(diff), np.sum(diff * x_data_A)])\n\n    def hess_f_A(b):\n        u = b[0] + b[1] * x_data_A\n        sigma = 1 / (1 + np.exp(-u))\n        weights = sigma * (1 - sigma)\n        h00 = np.sum(weights)\n        h01 = np.sum(weights * x_data_A)\n        h11 = np.sum(weights * x_data_A**2)\n        return np.array([[h00, h01], [h01, h11]])\n\n    result_A = optimizer(f_A, grad_f_A, hess_f_A, x0_A)\n\n    # --- Test Case B: Ill-Conditioned Quadratic ---\n    Q_B = np.array([[1e-6, 0.0], [0.0, 1.0]])\n    b_vec_B = np.array([1e-6, 1.0])\n    x0_B = np.array([10.0, -10.0])\n\n    def f_B(x):\n        return 0.5 * x.T @ Q_B @ x - b_vec_B.T @ x\n    \n    def grad_f_B(x):\n        return Q_B @ x - b_vec_B\n    \n    def hess_f_B(x):\n        return Q_B\n\n    result_B = optimizer(f_B, grad_f_B, hess_f_B, x0_B)\n\n    # --- Test Case C: Nonconvex Quartic ---\n    x0_C = np.array([0.2])\n\n    def f_C(x):\n        xv = x[0]\n        return xv**4 - 3 * xv**2 + xv\n    \n    def grad_f_C(x):\n        xv = x[0]\n        return np.array([4 * xv**3 - 6 * xv + 1])\n    \n    def hess_f_C(x):\n        xv = x[0]\n        return np.array([[12 * xv**2 - 6]])\n    \n    result_C = optimizer(f_C, grad_f_C, hess_f_C, x0_C)\n\n    # --- Format Output ---\n    results_list = [result_A, result_B, result_C]\n    formatted_parts = []\n    for res in results_list:\n        if res.size > 1:\n            num_strs = [f\"{val:.6f}\" for val in res]\n            formatted_parts.append(f\"[{','.join(num_strs)}]\")\n        else:\n            formatted_parts.append(f\"{res.item():.6f}\")\n\n    final_output = f\"[{','.join(formatted_parts)}]\"\n    print(final_output)\n\nsolve()\n```"}, {"introduction": "现实世界中的经济模型，如空间经济学或大规模资产定价模型，通常涉及数千甚至数百万个变量，这对计算效率提出了极高的要求。本高级练习旨在展示如何将牛顿法扩展到这类大规模问题，其关键在于利用问题的内在结构——即赫森矩阵的稀疏性。[@problem_id:2414738] 您将实现一个能够识别并利用这种稀疏性的求解器，从而大幅降低内存消耗和计算时间。这项实践将使您掌握处理高维优化任务的关键策略，这是现代计算经济学和金融工程领域的一项必备技能。", "id": "2414738", "problem": "考虑一个源于二维网格上离散化空间经济势的严格凸目标函数的最小化问题。设该网格有 $N_x$ 行和 $N_y$ 列，且 $N = N_x N_y$。位置由 $i \\in \\{0,1,\\dots,N-1\\}$ 索引。网格坐标 $(r,c)$（其中 $r \\in \\{0,1,\\dots,N_x-1\\}$ 和 $c \\in \\{0,1,\\dots,N_y-1\\}$）与向量索引之间的映射关系为 $i = r N_y + c$。通过$4$-邻域邻接性定义无向边集 $\\mathcal{E}$：节点 $(r,c)$ 和 $(r',c')$ 相邻当且仅当 $|r-r'|+|c-c'|=1$。设图拉普拉斯矩阵为 $L \\in \\mathbb{R}^{N \\times N}$，其定义为：如果 $(i,j)\\in \\mathcal{E}$，则 $L_{ii} = \\deg(i)$ 且 $L_{ij} = -1$；否则 $L_{ij}=0$。对于给定的参数 $a_i>0$、$b_i \\in \\mathbb{R}$、$c_i \\ge 0$ 和 $k \\ge 0$，定义目标函数\n$$\nf(x) \\;=\\; \\frac{1}{2}\\sum_{i=0}^{N-1} a_i \\left(x_i - b_i\\right)^2 \\;+\\; \\frac{k}{2}\\sum_{(i,j)\\in \\mathcal{E}} \\left(x_i - x_j\\right)^2 \\;+\\; \\sum_{i=0}^{N-1} c_i e^{x_i},\n$$\n对于 $x \\in \\mathbb{R}^N$。下文中三角函数所使用的所有角度均以弧度为单位。\n\n您的任务是编写一个程序，为下文测试套件中的每一组参数，高精度地计算 $f(x)$ 的一个最小化子 $x^\\star \\in \\mathbb{R}^N$，并返回所达到的目标值 $f(x^\\star)$。该实现必须对所有给定案例保持数值稳定，并且必须利用由$4$-邻域网格所隐含的稀疏结构。在所有案例中，请使用以下通用的初始化和停止准则：\n- 初始化：$x^{(0)} = 0 \\in \\mathbb{R}^N$。\n- 停止准则：当梯度的无穷范数满足 $\\|\\nabla f(x)\\|_\\infty \\le 10^{-8}$ 时，或当一个合适的二阶减量代理指标表明在接近 $10^{-12}$ 的水平上达到近似平稳点时，或在最多 $50$ 次迭代后，终止程序。步长的选择必须确保 $f(x)$ 下降。\n\n用于覆盖多种情况的参数集测试套件：\n- 案例 A（中等耦合，非线性局部项）：$N_x=4$, $N_y=4$, $k=0.5$。对于 $i \\in \\{0,\\dots,N-1\\}$，设 $a_i = 2.0 + 0.1\\,(i+1)$, $b_i = 0.2 \\sin(i+1)$, $c_i = 0.3\\,\\bigl((i \\bmod 3)+1\\bigr)$。\n- 案例 B（无耦合；对角海森结构）：$N_x=5$, $N_y=5$, $k=0.0$。对于 $i \\in \\{0,\\dots,N-1\\}$，设 $a_i = 1.5 + 0.05\\,(i+1)$, $b_i = -0.1 \\cos\\bigl(2\\,(i+1)\\bigr)$, $c_i = 0.2\\,\\bigl(1 + (i \\bmod 2)\\bigr)$。\n- 案例 C（纯二次型，强耦合）：$N_x=8$, $N_y=8$, $k=1.0$。对于 $i \\in \\{0,\\dots,N-1\\}$，设 $a_i = 3.0$, $b_i = 0.1\\,\\bigl((i \\bmod 7)-3\\bigr)$, $c_i = 0.0$。\n- 案例 D（更大网格，中等耦合，弱非线性）：$N_x=20$, $N_y=20$, $k=0.2$。对于 $i \\in \\{0,\\dots,N-1\\}$，设 $a_i = 1.0 + 0.001\\,(i+1)$, $b_i = 0.05 \\sin\\bigl(0.1\\,(i+1)\\bigr)$, $c_i = 0.05 + 0.05\\,\\frac{(i \\bmod 5)}{5}$。\n\n所有的指数、正弦和余弦函数都应被解释为标准的实值函数。本问题不涉及物理单位。每个案例都必须从共享的初始化 $x^{(0)}=0$ 开始独立求解。\n\n要求的最终输出格式：您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。具体来说，输出 $[F_A,F_B,F_C,F_D]$，其中 $F_\\bullet = f(x^\\star)$ 是对应案例的结果，每个值都精确到小数点后 $6$ 位（使用标准的四舍五入到最近的值，如果您的语言有定义，则当出现平局时舍入到最近的可表示的偶数十进制数）。例如，格式必须严格如 $[1.234567,8.765432,0.000001,2.500000]$，行内没有任何空格。", "solution": "该问题要求最小化一个多维目标函数 $f(x)$，其中向量 $x \\in \\mathbb{R}^N$。遵循科学规范，我们首先验证问题的陈述。\n\n目标函数陈述如下\n$$\nf(x) \\;=\\; \\frac{1}{2}\\sum_{i=0}^{N-1} a_i \\left(x_i - b_i\\right)^2 \\;+\\; \\frac{k}{2}\\sum_{(i,j)\\in \\mathcal{E}} \\left(x_i - x_j\\right)^2 \\;+\\; \\sum_{i=0}^{N-1} c_i e^{x_i}.\n$$\n参数被指定为约束条件 $a_i > 0$、$b_i \\in \\mathbb{R}$、$c_i \\ge 0$ 和 $k \\ge 0$。集合 $\\mathcal{E}$ 表示具有标准$4$-邻域结构的图中的边。这些定义在数学上是精确且一致的。该问题是一个标准的无约束非线性优化问题，其基础是计算经济学、物理学和图像分析中常见的既定数学原理。因此，该问题是科学上合理且适定的。\n\n为了选择一个合适的最小化算法，我们必须分析 $f(x)$ 的性质。该函数是二次连续可微的。其凸性由其海森矩阵 $\\nabla^2 f(x)$ 的正定性决定。\n\n让我们将该函数表示为紧凑的矩阵-向量形式。设 $A = \\text{diag}(a_0, \\dots, a_{N-1})$ 是由系数 $a_i$ 构成的对角矩阵，$L$ 是图拉普拉斯矩阵。二次项 $\\frac{1}{2}\\sum_{(i,j)\\in \\mathcal{E}} (x_i - x_j)^2$ 等价于 $\\frac{1}{2} x^T L x$，这是谱图论中的一个标准结果。因此，该函数可以写为\n$$\nf(x) = \\frac{1}{2}(x-b)^T A (x-b) + \\frac{k}{2} x^T L x + \\sum_{i=0}^{N-1} c_i e^{x_i}.\n$$\n$f(x)$ 的梯度是向量 $\\nabla f(x) \\in \\mathbb{R}^N$，由下式给出\n$$\n\\nabla f(x) = A(x-b) + kLx + G(x),\n$$\n其中 $G(x)$ 是一个向量，其分量为 $(G(x))_i = c_i e^{x_i}$。\n海森矩阵 $\\nabla^2 f(x) \\in \\mathbb{R}^{N \\times N}$ 为\n$$\n\\nabla^2 f(x) = A + kL + \\text{diag}(c_0e^{x_0}, \\dots, c_{N-1}e^{x_{N-1}}).\n$$\n我们分析此海森矩阵的正定性以确定 $f(x)$ 的凸性：\n1. 矩阵 $A$ 是对角矩阵，其对角元 $a_i > 0$，因此它是一个正定矩阵。\n2. 图拉普拉斯矩阵 $L$ 是一个已知的半正定矩阵。给定 $k \\ge 0$，矩阵 $kL$ 也是半正定的。\n3. 矩阵 $\\text{diag}(c_i e^{x_i})$ 是对角矩阵。由于对于所有 $x_i \\in \\mathbb{R}$ 都有 $c_i \\ge 0$ 和 $e^{x_i} > 0$，其对角元是非负的，这使其成为一个半正定矩阵。\n\n海森矩阵 $\\nabla^2 f(x)$ 是一个正定矩阵（$A$）和两个半正定矩阵之和。这样的和必定是正定的。对于所有 $x \\in \\mathbb{R}^N$ 海森矩阵都是正定的，这证实了 $f(x)$ 是一个严格凸函数。在 $\\mathbb{R}^N$ 上的一个严格凸且强制（coercive）的函数（强制性由包含 $a_i > 0$ 的二次项保证）有唯一的全局最小化子 $x^\\star$。\n\n对于梯度和海森矩阵都可解析获得的严格凸、二次可微函数，牛顿法因其在最小化子附近的二次收敛速度而成为首选。迭代过程从一个初始点 $x^{(0)}$ 开始，并生成一系列迭代点 $x^{(k)}$。牛顿方向 $\\Delta x^{(k)}$ 通过求解线性系统得到\n$$\n\\nabla^2 f(x^{(k)}) \\Delta x^{(k)} = -\\nabla f(x^{(k)}).\n$$\n一个纯牛顿步 $(x^{(k+1)} = x^{(k)} + \\Delta x^{(k)})$ 不能保证从一个较远的起始点收敛。因此，我们必须结合线搜索来确定一个步长 $\\alpha_k \\in (0, 1]$ 以确保目标函数值的下降。我们采用满足 Armijo 条件的回溯线搜索：\n$$\nf(x^{(k)} + \\alpha_k \\Delta x^{(k)}) \\le f(x^{(k)}) + \\sigma \\alpha_k (\\nabla f(x^{(k)}))^T \\Delta x^{(k)},\n$$\n对于一个小的常数 $\\sigma \\in (0, 0.5)$，例如 $\\sigma = 10^{-4}$。完整的更新规则则为 $x^{(k+1)} = x^{(k)} + \\alpha_k \\Delta x^{(k)}$。\n\n实现的一个关键方面是牛顿系统的有效求解。海森矩阵 $\\nabla^2 f(x)$ 是稀疏的，由两个对角矩阵和稀疏拉普拉斯矩阵 $L$ 的和组成。对于一个$4$-邻域网格，海森矩阵的每一行最多有 $5$ 个非零元。应通过使用稀疏矩阵数据结构和专门的稀疏线性求解器来利用这种稀疏性。\n\n该算法实现如下：\n1. 按照规定，初始化迭代计数器 $k=0$ 和解向量 $x^{(0)} = 0 \\in \\mathbb{R}^N$。\n2. 对于 $k$ 从 $0$ 到最大 $49$：\n    a. 计算梯度向量 $g = \\nabla f(x^{(k)})$ 和稀疏海森矩阵 $H = \\nabla^2 f(x^{(k)})$。\n    b. 检查第一个停止准则：如果梯度的无穷范数满足 $\\|g\\|_\\infty \\le 10^{-8}$，则过程终止。\n    c. 求解稀疏线性系统 $H \\Delta x = -g$ 以获得牛顿方向 $\\Delta x$。\n    d. 检查二阶停止准则。牛顿减量的一半，$\\lambda^2/2 = -\\frac{1}{2} g^T \\Delta x$，是衡量接近最小值程度的指标。如果 $-\\frac{1}{2} g^T \\Delta x \\le 10^{-12}$，则认为解已收敛，我们执行最后一步 $x^{(k+1)} = x^{(k)} + \\Delta x$ 并终止。\n    e. 从 $\\alpha=1$ 开始，执行回溯线搜索以找到满足 Armijo 条件的步长 $\\alpha_k$。\n    f. 更新解：$x^{(k+1)} = x^{(k)} + \\alpha_k \\Delta x$。\n3. 如果循环在未收敛的情况下完成，则程序在达到最大迭代次数时停止。\n\n这个完整的算法被应用于每个测试案例。然后计算并报告在确定的最小化子 $x^\\star$ 处的目标函数最终值 $f(x^\\star)$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import diags, lil_matrix\nfrom scipy.sparse.linalg import spsolve\n\ndef solve():\n    \"\"\"\n    Main solver function that iterates through test cases and prints the final results.\n    \"\"\"\n\n    def build_laplacian(Nx, Ny):\n        \"\"\"\n        Constructs the sparse graph Laplacian matrix for a 2D grid with 4-neighborhood.\n        \n        Args:\n            Nx (int): Number of rows in the grid.\n            Ny (int): Number of columns in the grid.\n            \n        Returns:\n            scipy.sparse.csr_matrix: The sparse Laplacian matrix L.\n        \"\"\"\n        N = Nx * Ny\n        # Use LIL format for efficient incremental construction of the sparse matrix.\n        L = lil_matrix((N, N))\n        for r in range(Nx):\n            for c in range(Ny):\n                i = r * Ny + c\n                degree = 0\n                # Neighbor up\n                if r > 0:\n                    j = (r - 1) * Ny + c\n                    L[i, j] = -1\n                    degree += 1\n                # Neighbor down\n                if r < Nx - 1:\n                    j = (r + 1) * Ny + c\n                    L[i, j] = -1\n                    degree += 1\n                # Neighbor left\n                if c > 0:\n                    j = r * Ny + (c - 1)\n                    L[i, j] = -1\n                    degree += 1\n                # Neighbor right\n                if c < Ny - 1:\n                    j = r * Ny + (c + 1)\n                    L[i, j] = -1\n                    degree += 1\n                L[i, i] = degree\n        # Convert to CSR format for fast matrix-vector products.\n        return L.tocsr()\n\n    def objective_function(x, a, b, c, k, L_sparse):\n        \"\"\"Calculates the value of the objective function f(x).\"\"\"\n        term1 = 0.5 * np.sum(a * (x - b)**2)\n        # The term (k/2) * x.T @ L @ x is computationally correct for the sum expression\n        term2 = 0.5 * k * (x.T @ (L_sparse @ x))\n        term3 = np.sum(c * np.exp(x))\n        return term1 + term2 + term3\n\n    def solve_one_case(Nx, Ny, k_param, a_vec, b_vec, c_vec):\n        \"\"\"\n        Solves the minimization problem for a single set of parameters using Newton's method.\n        \"\"\"\n        N = Nx * Ny\n        L_sparse = build_laplacian(Nx, Ny)\n        A_sparse = diags(a_vec, 0, format='csr')\n\n        x = np.zeros(N)\n        \n        # Newton's method parameters\n        max_iter = 50\n        grad_tol = 1e-8\n        decrement_tol = 1e-12\n        \n        # Line search parameters\n        beta = 0.5\n        sigma = 1e-4\n\n        for _ in range(max_iter):\n            exp_x = np.exp(x)\n            c_exp_x = c_vec * exp_x\n            \n            # Gradient: g = A(x-b) + k*L*x + c*exp(x)\n            g = A_sparse @ (x - b_vec) + k_param * (L_sparse @ x) + c_exp_x\n            \n            if np.max(np.abs(g)) <= grad_tol:\n                break\n\n            # Hessian: H = A + k*L + diag(c*exp(x))\n            H_diag = diags(c_exp_x, 0, format='csr')\n            H = A_sparse + k_param * L_sparse + H_diag\n\n            # Solve the Newton system: H*dx = -g.\n            # Convert to CSC format for efficiency with spsolve.\n            dx = spsolve(H.tocsc(), -g)\n            \n            newton_decrement_half = -0.5 * g.T @ dx\n            if newton_decrement_half <= decrement_tol:\n                x = x + dx  # Take full final step\n                break\n            \n            # Backtracking line search\n            alpha = 1.0\n            f_x = objective_function(x, a_vec, b_vec, c_vec, k_param, L_sparse)\n            slope = g.T @ dx\n\n            while True:\n                x_new = x + alpha * dx\n                f_x_new = objective_function(x_new, a_vec, b_vec, c_vec, k_param, L_sparse)\n                if f_x_new <= f_x + sigma * alpha * slope:\n                    break\n                alpha *= beta\n                if alpha < 1e-12: # Safety break\n                    break\n            \n            x = x + alpha * dx\n        \n        return objective_function(x, a_vec, b_vec, c_vec, k_param, L_sparse)\n\n    test_cases = [\n        # Case A\n        {'Nx': 4, 'Ny': 4, 'k': 0.5, \n         'a_func': lambda i: 2.0 + 0.1 * (i + 1),\n         'b_func': lambda i: 0.2 * np.sin(i + 1),\n         'c_func': lambda i: 0.3 * ((i % 3) + 1)},\n        # Case B\n        {'Nx': 5, 'Ny': 5, 'k': 0.0,\n         'a_func': lambda i: 1.5 + 0.05 * (i + 1),\n         'b_func': lambda i: -0.1 * np.cos(2 * (i + 1)),\n         'c_func': lambda i: 0.2 * (1 + (i % 2))},\n        # Case C\n        {'Nx': 8, 'Ny': 8, 'k': 1.0,\n         'a_func': lambda i: 3.0,\n         'b_func': lambda i: 0.1 * ((i % 7) - 3),\n         'c_func': lambda i: 0.0},\n        # Case D\n        {'Nx': 20, 'Ny': 20, 'k': 0.2,\n         'a_func': lambda i: 1.0 + 0.001 * (i + 1),\n         'b_func': lambda i: 0.05 * np.sin(0.1 * (i + 1)),\n         'c_func': lambda i: 0.05 + 0.05 * ((i % 5) / 5)}\n    ]\n\n    results = []\n    for case in test_cases:\n        Nx, Ny = case['Nx'], case['Ny']\n        N = Nx * Ny\n        i_vals = np.arange(N)\n        \n        a = case['a_func'](i_vals)\n        if np.isscalar(a): a = np.full(N, a, dtype=float)\n\n        b = case['b_func'](i_vals)\n        if np.isscalar(b): b = np.full(N, b, dtype=float)\n\n        c = case['c_func'](i_vals)\n        if np.isscalar(c): c = np.full(N, c, dtype=float)\n        \n        f_val = solve_one_case(Nx, Ny, case['k'], a, b, c)\n        results.append(f\"{f_val:.6f}\")\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"}]}