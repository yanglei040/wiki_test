{"hands_on_practices": [{"introduction": "最速下降法的每一步都始于一个基本问题：我们应该朝哪个方向移动？这个练习将带你回到最核心的原理，即搜索方向是函数值下降最快的方向。通过为一个在优化领域中广泛使用的测试函数计算初始搜索方向，你将掌握最速下降法最基础的第一步：在给定的起点计算负梯度 $p = -\\nabla f(x)$。[@problem_id:2221567]", "id": "2221567", "problem": "在数值优化领域，新算法的性能通常使用一组标准测试函数进行基准测试。Rosenbrock 函数就是其中之一，由于其狭窄的抛物线形山谷，该函数极具最小化挑战。\n\n考虑二维 Rosenbrock 函数，其形式为\n$$f(x,y) = (a-x)^2 + b(y-x^2)^2$$\n其中 $a$ 和 $b$ 是正实数常量。\n\n一个迭代最小化算法从点 $(x_0, y_0) = (0, 0)$ 开始。该算法的第一步是确定初始搜索方向。该方向定义为函数值从起始点下降最快的向量。\n\n确定这个初始搜索方向向量 $\\mathbf{d}_0$。用常数 $a$ 和 $b$ 将您的答案表示为列向量。", "solution": "函数 $f$ 在某点下降最快的方向是负梯度方向，因此从 $(x_{0},y_{0})=(0,0)$ 出发的初始搜索方向为 $\\mathbf{d}_{0}=-\\nabla f(0,0)$。\n\n计算 $f(x,y)=(a-x)^{2}+b(y-x^{2})^{2}$ 的梯度：\n- 关于 $x$ 的偏导数为\n$$\n\\frac{\\partial f}{\\partial x}=2(x-a)-4bx(y-x^{2}).\n$$\n- 关于 $y$ 的偏导数为\n$$\n\\frac{\\partial f}{\\partial y}=2b(y-x^{2}).\n$$\n\n在 $(0,0)$ 点计算这些偏导数的值：\n$$\n\\frac{\\partial f}{\\partial x}(0,0)=-2a,\\qquad \\frac{\\partial f}{\\partial y}(0,0)=0.\n$$\n因此，\n$$\n\\nabla f(0,0)=\\begin{pmatrix}-2a\\\n$$4pt]0\\end{pmatrix},\n\\quad\n\\mathbf{d}_{0}=-\\nabla f(0,0)=\\begin{pmatrix}2a\\\n$$4pt]0\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}2a\\\\0\\end{pmatrix}}$$"}, {"introduction": "为什么最速下降法有时收敛缓慢？这个练习超越了简单的计算，探讨了算法在“病态”问题上的行为，而这类问题在经济和金融模型（如存在多重共线性）中很常见。通过分析目标函数的橢圆形等高线及其与海森矩阵 (Hessian matrix) 特征值的关系，你将从几何上理解算法产生标志性“之”字形（zigzagging）路径并缓慢收敛的根本原因。[@problem_id:2434016] 这种洞察力对于诊断优化性能和理解更高级算法的优势至关重要。", "id": "2434016", "problem": "考虑一个有两个回归量的线性回归的普通最小二乘法 (OLS) 目标函数，\n$$\nf(\\beta) \\;=\\; \\frac{1}{2n}\\,\\lVert y - X\\beta\\rVert_2^2,\n$$\n其中 $X\\in\\mathbb{R}^{n\\times 2}$ 的列为 $x_1$ 和 $x_2$，且 $y\\in\\mathbb{R}^n$。假设回归量是标准化的，使得 $x_1^\\top x_1 = x_2^\\top x_2 = n$ 且 $x_1^\\top x_2 = \\rho n$，其中 $\\rho\\in(0,1)$ 且接近于 $1$，即 $x_1$ 和 $x_2$ 近似共线性。令 $\\beta^\\star$ 表示 $f(\\beta)$ 的唯一最小值点，并考虑从某个 $\\beta^{(0)}\\neq \\beta^\\star$ 开始，对 $f(\\beta)$ 应用带有精确线搜索的最速下降法（梯度下降法）。\n\n在这种近似共线性的设定下，关于 $f(\\beta)$ 的水平集的几何形状以及最速下降法产生的路径，下列哪个/哪些陈述是正确的？\n\nA. $f(\\beta)$ 的水平集是细长的椭圆，其长轴与 $(1/n)X^\\top X$ 的较小特征值所对应的特征向量对齐；最速下降法在峡谷两侧交替方向，沿着这个长轴进展缓慢。\n\nB. 对于这个带有精确线搜索的严格凸二次目标函数，连续的梯度是正交的；当 $\\rho$ 接近 $1$ 时，这种正交性会在一个狭窄的峡谷中产生明显的Z字形路径。\n\nC. 因为目标函数在共线性方向上近似平坦，精确线搜索在该方向上会选择一个无界的步长，因此最速下降法会过冲并发散。\n\nD. 任何迭代点的负梯度方向都等于 $-(\\beta-\\beta^\\star)$，因此路径是一条直接指向最小值点的直线。", "solution": "对问题陈述进行验证。\n\n### 步骤1：提取已知条件\n- 目标函数: $f(\\beta) = \\frac{1}{2n}\\,\\lVert y - X\\beta\\rVert_2^2$。\n- 数据矩阵: $X \\in \\mathbb{R}^{n\\times 2}$，列为 $x_1$ 和 $x_2$。\n- 响应向量: $y \\in \\mathbb{R}^n$。\n- 参数向量: $\\beta \\in \\mathbb{R}^2$。\n- 回归量性质: $x_1^\\top x_1 = n$, $x_2^\\top x_2 = n$, 且 $x_1^\\top x_2 = \\rho n$。\n- 共线性参数: $\\rho \\in (0,1)$ 且 $\\rho$ 接近于 $1$。\n- 最小值点: $\\beta^\\star$ 是 $f(\\beta)$ 的唯一最小值点。\n- 算法: 最速下降法（梯度下降法）与精确线搜索。\n- 初始点: $\\beta^{(0)} \\neq \\beta^\\star$。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题是数值优化中的一个标准练习，具体来说是关于最速下降法在普通最小二乘法 (OLS) 问题上的应用。所有组成部分都定义明确，并且是统计学和优化领域的标准内容。\n\n1.  **科学依据**：该问题基于线性代数、多元微积分和数值优化的既有理论。OLS 目标函数和梯度下降是基本概念。\n2.  **适定性**：可以分析目标函数以确定其性质。$f(\\beta)$ 的梯度为 $\\nabla f(\\beta) = \\frac{1}{n}(X^\\top X \\beta - X^\\top y)$。Hessian 矩阵为 $\\nabla^2 f(\\beta) = H = \\frac{1}{n}X^\\top X$。使用给定的性质，我们计算 $X^\\top X$：\n    $$\n    X^\\top X = \\begin{pmatrix} x_1^\\top x_1 & x_1^\\top x_2 \\\\ x_2^\\top x_1 & x_2^\\top x_2 \\end{pmatrix} = \\begin{pmatrix} n & \\rho n \\\\ \\rho n & n \\end{pmatrix} = n \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}\n    $$\n    因此，Hessian 矩阵是一个常数矩阵：\n    $$\n    H = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}\n    $$\n    $H$ 的特征值 $\\lambda$ 由特征方程 $\\det(H - \\lambda I) = (1-\\lambda)^2 - \\rho^2 = 0$ 给出，解得 $\\lambda = 1 \\pm \\rho$。由于 $\\rho \\in (0,1)$，特征值为 $\\lambda_1 = 1-\\rho > 0$ 和 $\\lambda_2 = 1+\\rho > 0$。因为两个特征值都为正，所以 Hessian 矩阵是正定的。这证实了 $f(\\beta)$ 是一个严格凸二次函数，从而保证了问题中所述的唯一最小值点 $\\beta^\\star$ 的存在。该问题是适定的。\n3.  **客观性**：该问题以精确的数学语言陈述，没有歧义或主观性。\n\n### 步骤3：结论与行动\n问题陈述是有效的。这是一个适定的、科学上合理的的问题。将推导一个完整的解。\n\n### 推导\n目标函数可以根据其最小值点 $\\beta^\\star$ 和 Hessian 矩阵 $H$ 写成：\n$$\nf(\\beta) = f(\\beta^\\star) + \\frac{1}{2}(\\beta - \\beta^\\star)^\\top H (\\beta - \\beta^\\star)\n$$\n$f$ 的一个水平集是点集 $\\{\\beta \\in \\mathbb{R}^2 \\mid f(\\beta) = c\\}$，其中 $c$ 是某个常数且 $c > f(\\beta^\\star)$。这对应一个以 $\\beta^\\star$ 为中心的椭圆方程：\n$$\n(\\beta - \\beta^\\star)^\\top H (\\beta - \\beta^\\star) = 2(c - f(\\beta^\\star))\n$$\n这些椭圆水平集的几何形状由 Hessian 矩阵 $H = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ 的特征系统决定。\n特征值为 $\\lambda_1 = 1-\\rho$ 和 $\\lambda_2 = 1+\\rho$。\nHessian 矩阵的条件数为 $\\kappa(H) = \\frac{\\lambda_{\\text{max}}}{\\lambda_{\\text{min}}} = \\frac{\\lambda_2}{\\lambda_1} = \\frac{1+\\rho}{1-\\rho}$。由于 $\\rho$ 接近 $1$，令 $\\rho = 1-\\epsilon$，其中 $\\epsilon>0$ 是一个小量。那么 $\\kappa(H) = \\frac{2-\\epsilon}{\\epsilon} \\gg 1$。大的条件数意味着水平集是高度细长的椭圆。\n\n椭圆的轴由 $H$ 的特征向量决定。\n- 对于较小的特征值 $\\lambda_1 = 1-\\rho$，特征向量 $v_1$ 满足 $(H - \\lambda_1 I)v_1 = 0$：\n  $$\n  \\begin{pmatrix} \\rho & \\rho \\\\ \\rho & \\rho \\end{pmatrix} v_1 = 0 \\implies v_1 \\propto \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n  $$\n- 对于较大的特征值 $\\lambda_2 = 1+\\rho$，特征向量 $v_2$ 满足 $(H - \\lambda_2 I)v_2 = 0$：\n  $$\n  \\begin{pmatrix} -\\rho & \\rho \\\\ \\rho & -\\rho \\end{pmatrix} v_2 = 0 \\implies v_2 \\propto \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n  $$\n椭圆半轴的长度与相应特征值的平方根成反比。长轴对应于较小的特征值 $\\lambda_1$，并与特征向量 $v_1$ 对齐。短轴对应于较大的特征值 $\\lambda_2$，并与 $v_2$ 对齐。这些高度细长的椭圆形成了一个狭长的“峡谷”，其方向为 $v_1 = [1, -1]^\\top$。\n\n最速下降算法执行更新 $\\beta^{(k+1)} = \\beta^{(k)} - \\alpha_k \\nabla f(\\beta^{(k)})$。方向是负梯度 $d_k = -\\nabla f(\\beta^{(k)})$，它与在 $\\beta^{(k)}$ 处的水平集正交。使用精确线搜索时，选择步长 $\\alpha_k$ 以最小化 $f(\\beta^{(k)} + \\alpha d_k)$。这意味着新点处的梯度 $\\nabla f(\\beta^{(k+1)})$ 与搜索方向 $d_k$ 正交。\n$$\n\\nabla f(\\beta^{(k+1)})^\\top d_k = 0 \\implies \\nabla f(\\beta^{(k+1)})^\\top (-\\nabla f(\\beta^{(k)})) = 0\n$$\n这表明连续的梯度 $\\nabla f(\\beta^{(k)})$ 和 $\\nabla f(\\beta^{(k+1)})$ 是正交的。\n\n当该算法应用于具有细长水平集的函数时，在“峡谷”一侧的点 $\\beta^{(k)}$ 处的梯度向量几乎垂直于峡谷底部（长轴）。精确线搜索将迭代点移动到峡谷的另一侧。新的梯度与前一个梯度正交，并再次陡峭地指向峡谷对面。这会产生一个典型的Z字形轨迹，算法沿着峡谷底部向最小值点 $\\beta^\\star$ 缓慢进展。\n\n### 逐项分析\n\n**A. $f(\\beta)$ 的水平集是细长的椭圆，其长轴与 $(1/n)X^\\top X$ 的较小特征值所对应的特征向量对齐；最速下降法在峡谷两侧交替方向，沿着这个长轴进展缓慢。**\n这个陈述与我们的推导完全一致。\n- 水平集确实是椭圆，并且由于条件数 $\\kappa(H) = \\frac{1+\\rho}{1-\\rho}$ 很大，它们是细长的。\n- Hessian 矩阵是 $H = (1/n)X^\\top X$。椭圆水平集的长轴对应于曲率最小的方向，即与最小特征值相关联的特征向量。这一点陈述正确。\n- 将最速下降路径描述为“在峡谷两侧交替方向”（Z字形前进）并“沿着这个长轴进展缓慢”，是该算法在病态问题上的经典行为。\n结论：**正确**。\n\n**B. 对于这个带有精确线搜索的严格凸二次目标函数，连续的梯度是正交的；当 $\\rho$ 接近 $1$ 时，这种正交性会在一个狭窄的峡谷中产生明显的Z字形路径。**\n这个陈述也是正确的。\n- 如上所示，目标函数是一个严格凸二次函数。\n- 对带有精确线搜索的最速下降法，连续梯度是正交的，这是一个基本结果。\n- 条件 $\\rho$ 接近 1 意味着 Hessian 矩阵是病态的，这对应于一个“狭窄峡谷”的几何形状。梯度的正交性正是迫使在这种几何形状中产生“Z字形”路径的机制。\n结论：**正确**。\n\n**C. 因为目标函数在共线性方向上近似平坦，精确线搜索在该方向上会选择一个无界的步长，因此最速下降法会过冲并发散。**\n这个陈述是错误的。虽然目标函数在 $v_1 = [1, -1]^\\top$ 方向上“近似平坦”（曲率小），但它仍然是一个严格凸二次函数。沿着任何直线，该函数都是一个开口向上的抛物线。沿此线的最小值点是唯一且有限的。因此，精确线搜索将始终选择一个有限的步长 $\\alpha_k > 0$。该算法保证会收敛到唯一的最小值点 $\\beta^\\star$，并且不会发散。\n结论：**错误**。\n\n**D. 任何迭代点的负梯度方向都等于 $-(\\beta-\\beta^\\star)$，因此路径是一条直接指向最小值点的直线。**\n这个陈述是错误的。梯度为 $\\nabla f(\\beta) = H(\\beta - \\beta^\\star)$。负梯度方向是 $-H(\\beta - \\beta^\\star)$。只有当 $H$ 是单位矩阵的标量倍数时，这个方向才等于 $-(\\beta-\\beta^\\star)$（或与 $\\beta^\\star - \\beta$ 成比例）。在我们的例子中，$H = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ 且 $\\rho \\neq 0$，所以 $H$ 不是单位矩阵的倍数。梯度通常不指向最小值点。因此，路径不是一条直线。（直线路徑是牛顿法作用于二次函数时的特征，而不是最速下降法的特征）。\n结论：**错误**。\n\n陈述 A 和 B 都是对该系统的正确描述。陈述 A 侧重于几何形状和由此产生的路径，而陈述 B 解释了导致路径形状的算法属性。问题要求选出所有正确的陈述。", "answer": "$$\\boxed{AB}$$"}, {"introduction": "掌握一个算法的最终环节是亲手实现它。这个练习将引导你编写一个完整的最速下降求解器，并整合一个在实践中非常重要的技术——回溯线搜索 (backtracking line search)，用以自适应地确定每一步的步长 $\\alpha_k$。通过将你的代码应用于一个经济学中典型的效用最大化问题，你将把理论概念、算法逻辑与计算实践无缝连接起来，体会数值优化的强大功能。[@problem_id:2434090]", "id": "2434090", "problem": "考虑一个由二次效用函数给出的双变量无约束效用最大化问题\n$$u(x) = b^{\\top} x - \\tfrac{1}{2} x^{\\top} Q x,$$\n其中 $x \\in \\mathbb{R}^{2}$，$b \\in \\mathbb{R}^{2}$，且 $Q \\in \\mathbb{R}^{2 \\times 2}$ 是对称正定矩阵。该问题可以等价地表示为目标函数的无约束最小化问题\n$$f(x) = -u(x) = \\tfrac{1}{2} x^{\\top} Q x - b^{\\top} x.$$\n设迭代过程定义为 $x^{(k+1)} = x^{(k)} - \\alpha_{k} \\nabla f\\left(x^{(k)}\\right)$，其中 $\\nabla f(x) = Qx - b$。在每次迭代中，步长 $\\alpha_{k}$ 必须满足充分下降条件\n$$f\\left(x^{(k)} - \\alpha_{k} \\nabla f\\left(x^{(k)}\\right)\\right) \\le f\\left(x^{(k)}\\right) + c \\, \\alpha_{k} \\, \\nabla f\\left(x^{(k)}\\right)^{\\top}\\left(-\\nabla f\\left(x^{(k)}\\right)\\right),$$\n其中 $\\alpha_{k}$ 从几何序列 $\\{\\alpha_{0} \\rho^{m} : m \\in \\mathbb{N} \\cup \\{0\\}\\}$ 中选取，$\\alpha_{0} \\in \\mathbb{R}_{++}$、$\\rho \\in (0,1)$ 和 $c \\in (0,1)$ 为固定常数。所有范数计算均使用欧几里得范数 $\\|\\cdot\\|_{2}$。当 $\\|\\nabla f(x^{(k)})\\|_{2} \\le \\varepsilon$ 或迭代次数达到最大值 $N_{\\max}$ 时，算法必须终止。\n\n对于每个测试用例，计算所述过程产生的最终迭代点 $x^{(T)}$，并计算 $u(x)$ 的唯一最大化点 $x^{\\star}$，它等价于 $f(x)$ 的唯一最小化点且满足 $Q x^{\\star} = b$。对于每个测试用例，报告标量欧几里得误差 $\\|x^{(T)} - x^{\\star}\\|_{2}$。\n\n所有测试用例均使用以下固定参数值：初始步长参数 $\\alpha_{0} = 1$，回溯收缩因子 $\\rho = 0.5$，充分下降常数 $c = 10^{-4}$，容差 $\\varepsilon = 10^{-8}$，以及最大迭代次数 $N_{\\max} = 10000$。所有矩阵和向量的具体值如下所示。\n\n测试套件：\n- 测试用例 $1$：$Q = \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix}$，$b = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$，$x^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 测试用例 $2$：$Q = \\begin{bmatrix} 100 & 0 \\\\ 0 & 1 \\end{bmatrix}$，$b = \\begin{bmatrix} 100 \\\\ 1 \\end{bmatrix}$，$x^{(0)} = \\begin{bmatrix} 10 \\\\ -10 \\end{bmatrix}$。\n- 测试用例 $3$：$Q = \\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\end{bmatrix}$，$b = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$，$x^{(0)} = \\begin{bmatrix} \\tfrac{2}{5} \\\\ -\\tfrac{1}{5} \\end{bmatrix}$。\n- 测试用例 $4$：$Q = \\begin{bmatrix} 4 & 1.5 \\\\ 1.5 & 1 \\end{bmatrix}$，$b = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$，$x^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按测试用例的顺序列出结果，即 $[\\|x^{(T)}_{1} - x^{\\star}_{1}\\|_{2}, \\|x^{(T)}_{2} - x^{\\star}_{2}\\|_{2}, \\|x^{(T)}_{3} - x^{\\star}_{3}\\|_{2}, \\|x^{(T)}_{4} - x^{\\star}_{4}\\|_{2}]$。结果必须是实数（浮点数）。本问题不涉及物理单位。", "solution": "所述问题是有效的。这是一个数值优化领域内定义明确、有科学依据的问题，具体重点在于将最速下降法应用于二次目标函数。所有必要的参数和数据均已提供，术语精确，且没有内部矛盾或逻辑缺陷。该问题涉及最小化一个严格凸的二次函数，对此，使用回溯线搜索的最速下降法是一种标准的收敛算法。我们现在开始求解。\n\n问题是最小化二次目标函数 $f(x) = \\frac{1}{2} x^{\\top} Q x - b^{\\top} x$，其中 $x \\in \\mathbb{R}^{2}$，$b \\in \\mathbb{R}^{2}$，且 $Q \\in \\mathbb{R}^{2 \\times 2}$ 是一个对称正定矩阵。\n\n首先，我们确定解的存在性和唯一性。目标函数的Hessian矩阵是 $\\nabla^2 f(x) = Q$。由于对于所有测试用例，$Q$ 都被给定为正定矩阵，因此函数 $f(x)$ 是严格凸的。在 $\\mathbb{R}^{n}$ 上的严格凸函数至多有一个最小化点。由于 $f(x)$ 是强制的（即，当 $\\|x\\|_2 \\to \\infty$ 时，$f(x) \\to \\infty$），因此保证存在一个唯一的全局最小化点，我们将其表示为 $x^{\\star}$。\n\n最优性的一阶必要条件指出，在最小化点处，目标函数的梯度必须为零。$f(x)$ 的梯度为 $\\nabla f(x) = Qx - b$。将梯度设为零，得到最优性条件：\n$$\n\\nabla f(x^{\\star}) = Qx^{\\star} - b = 0\n$$\n这是一个线性方程组 $Qx^{\\star} = b$。由于 $Q$ 是正定的，因此它是可逆的。所以，唯一的解析解由以下公式给出：\n$$\nx^{\\star} = Q^{-1} b\n$$\n\n求解的数值过程是最速下降法。这是一种迭代算法，它生成一个收敛于 $x^{\\star}$ 的点序列 $\\{x^{(k)}\\}_{k=0}^{\\infty}$。迭代定义为：\n$$\nx^{(k+1)} = x^{(k)} + \\alpha_k p^{(k)}\n$$\n其中 $p^{(k)}$ 是搜索方向，$\\alpha_k > 0$ 是步长。对于最速下降法，搜索方向选择为当前迭代点的负梯度方向，因为这是函数局部下降最快的方向。\n$$\np^{(k)} = -\\nabla f(x^{(k)}) = -(Qx^{(k)} - b) = b - Qx^{(k)}\n$$\n因此，更新规则为：\n$$\nx^{(k+1)} = x^{(k)} - \\alpha_k \\nabla f(x^{(k)})\n$$\n\n步长 $\\alpha_k$ 是通过回溯线搜索过程来确定的，以满足充分下降条件，也称为 Armijo 条件。这确保了每一步都朝着最小值取得了有意义的进展。该条件是：\n$$\nf(x^{(k+1)}) \\le f(x^{(k)}) + c \\, \\alpha_k \\, \\nabla f(x^{(k)})^{\\top} p^{(k)}\n$$\n代入 $p^{(k)} = -\\nabla f(x^{(k)})$，条件变为：\n$$\nf\\left(x^{(k)} - \\alpha_k \\nabla f\\left(x^{(k)}\\right)\\right) \\le f\\left(x^{(k)}\\right) - c \\, \\alpha_k \\, \\|\\nabla f\\left(x^{(k)}\\right)\\|^{2}_{2}\n$$\n在每次迭代 $k$ 中选择 $\\alpha_k$ 的过程如下：\n$1$. 从初始步长 $\\alpha = \\alpha_0 = 1$ 开始。\n$2$. 当 Armijo 条件不满足时，缩短步长：$\\alpha \\leftarrow \\rho \\alpha$。此处 $\\rho = 0.5$。\n$3$. 一旦条件满足，则设 $\\alpha_k = \\alpha$。\n参数给定为 $c = 10^{-4}$，$\\alpha_0 = 1$ 和 $\\rho = 0.5$。\n\n当满足以下两个条件之一时，算法终止：\n$1$. 梯度的范数小于指定的容差 $\\varepsilon = 10^{-8}$：$\\|\\nabla f(x^{(k)})\\|_{2} \\le \\varepsilon$。这表明迭代点非常接近梯度为零的最优解 $x^{\\star}$。\n$2$. 迭代次数 $k$ 达到允许的最大次数 $N_{\\max} = 10000$。\n\n设 $x^{(T)}$ 是算法产生的最终迭代点。每个测试用例最终要求的输出是欧几里得误差 $\\|x^{(T)} - x^{\\star}\\|_{2}$。对于给定的每个测试用例（$Q$，$b$，$x^{(0)}$），我们将首先计算解析解 $x^{\\star} = Q^{-1}b$，然后执行所述的迭代算法来找到 $x^{(T)}$。\n\n对于测试用例 3，给定 $Q = \\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\end{bmatrix}$，$b = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ 和 $x^{(0)} = \\begin{bmatrix} 2/5 \\\\ -1/5 \\end{bmatrix}$。解析解为 $x^{\\star} = Q^{-1}b = \\frac{1}{5}\\begin{bmatrix} 2 & -1 \\\\ -1 & 3 \\end{bmatrix}\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2/5 \\\\ -1/5 \\end{bmatrix}$。因此，初始点 $x^{(0)}$ 就是精确解 $x^{\\star}$。在这种情况下，初始梯度为 $\\nabla f(x^{(0)}) = Qx^{(0)} - b = 0$。在第 $k=0$ 次迭代时，终止条件 $\\|\\nabla f(x^{(0)})\\|_{2} = 0 \\le \\varepsilon$ 立即得到满足。算法终止，得到 $x^{(T)} = x^{(0)}$，且误差 $\\|x^{(T)} - x^{\\star}\\|_{2}$ 为 $0$。\n\n现在，实现将对所有测试用例遵循这一逻辑。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the unconstrained utility maximization problem for four test cases\n    using the steepest descent method with backtracking line search.\n    \"\"\"\n    # Fixed parameters for the algorithm\n    alpha0 = 1.0\n    rho = 0.5\n    c = 1e-4\n    epsilon = 1e-8\n    N_max = 10000\n\n    # Test cases defined in the problem statement\n    test_cases = [\n        {\n            \"Q\": np.array([[2.0, 0.0], [0.0, 1.0]]),\n            \"b\": np.array([2.0, 1.0]),\n            \"x0\": np.array([0.0, 0.0]),\n        },\n        {\n            \"Q\": np.array([[100.0, 0.0], [0.0, 1.0]]),\n            \"b\": np.array([100.0, 1.0]),\n            \"x0\": np.array([10.0, -10.0]),\n        },\n        {\n            \"Q\": np.array([[3.0, 1.0], [1.0, 2.0]]),\n            \"b\": np.array([1.0, 0.0]),\n            \"x0\": np.array([2.0 / 5.0, -1.0 / 5.0]),\n        },\n        {\n            \"Q\": np.array([[4.0, 1.5], [1.5, 1.0]]),\n            \"b\": np.array([1.0, 2.0]),\n            \"x0\": np.array([0.0, 0.0]),\n        },\n    ]\n\n    results = []\n\n    def objective_function(x, Q, b):\n        \"\"\"Computes the value of the objective function f(x).\"\"\"\n        return 0.5 * x.T @ Q @ x - b.T @ x\n\n    for case in test_cases:\n        Q = case[\"Q\"]\n        b = case[\"b\"]\n        x_k = case[\"x0\"].copy()\n\n        # Compute the analytical solution x_star\n        x_star = np.linalg.solve(Q, b)\n\n        # Main loop for the steepest descent algorithm\n        for _ in range(N_max):\n            # Compute the gradient at the current iterate x_k\n            grad_f = Q @ x_k - b\n            grad_norm = np.linalg.norm(grad_f)\n\n            # Check for termination based on gradient norm\n            if grad_norm <= epsilon:\n                break\n            \n            # Backtracking line search to find the step size alpha_k\n            alpha = alpha0\n            f_k = objective_function(x_k, Q, b)\n            grad_norm_sq = grad_norm**2 # More efficient than dot product\n\n            while True:\n                # Armijo condition check\n                f_new = objective_function(x_k - alpha * grad_f, Q, b)\n                if f_new <= f_k - c * alpha * grad_norm_sq:\n                    break\n                \n                # Shrink alpha if condition is not met\n                alpha *= rho\n\n            # Update the iterate\n            x_k = x_k - alpha * grad_f\n        \n        # The loop terminates, x_k is the terminal iterate x_T\n        x_terminal = x_k\n\n        # Compute the final Euclidean error\n        error = np.linalg.norm(x_terminal - x_star)\n        results.append(error)\n\n    # Print the results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}]}