{"hands_on_practices": [{"introduction": "在我们编写任何代码之前，让我们先通过一个思想实验来热身。梯度下降算法虽然强大，但其优化路径深受损失函数“地形”的影响。这个练习 [@problem_id:2375265] 探讨了在一个非凸函数上，初始点的微小变动如何可能导致算法走向一个完全不同的终点——其中一个可能是远不如另一个的最优解。通过分析这个场景，你将对“为什么初始化至关重要”以及“局部最优解如何困住算法”建立起关键的直觉。", "id": "2375265", "problem": "一位计算金融领域的研究员正在一个代表性代理人资产定价模型中校准一个标量偏好参数 $\\theta$，其方法是最小化一个程式化的非凸损失函数，该函数捕捉了模型设定误差和一个弱的经验倾向。损失函数为\n$$\nL(\\theta) \\;=\\; (\\theta^{2} - 1)^{2} \\;-\\; 0.2\\,\\theta.\n$$\n该研究员使用恒定步长 $\\alpha$ 的标准梯度下降法，更新规则如下\n$$\n\\theta_{t+1} \\;=\\; \\theta_{t} \\;-\\; \\alpha\\,\\nabla L(\\theta_{t}).\n$$\n考虑步长 $\\alpha = 0.1$ 和两个初始值 $\\theta_{0}^{(-)} = -0.051$ 与 $\\theta_{0}^{(+)} = -0.049$，两者相差 $0.002$。以下哪个选项最能描述这两次梯度下降运行的渐进行为，并指出是否达到了次优最小值？\n\nA. $\\theta_{0}^{(-)}$ 和 $\\theta_{0}^{(+)}$ 都收敛到 $\\theta \\approx -1$ 附近的左侧局部最小值，该值是全局最小值。\n\nB. $\\theta_{0}^{(-)}$ 收敛到 $\\theta \\approx 1$ 附近的右侧局部最小值（全局），而 $\\theta_{0}^{(+)}$ 收敛到 $\\theta \\approx -1$ 附近的左侧局部最小值（次优）。\n\nC. $\\theta_{0}^{(-)}$ 收敛到 $\\theta \\approx -1$ 附近的左侧局部最小值（次优），而 $\\theta_{0}^{(+)}$ 收敛到 $\\theta \\approx 1$ 附近的右侧局部最小值（全局）。\n\nD. 两个序列都发散到 $\\pm\\infty$，因为对于这个损失函数，步长 $\\alpha = 0.1$ 太大，无法保证稳定性。", "solution": "在任何科学探究中，首要任务是验证问题陈述的有效性。一个有缺陷的前提会导致一个毫无意义的结论。\n\n### 步骤 1：提取已知条件\n\n问题提供了以下信息：\n- 损失函数：$L(\\theta) = (\\theta^{2} - 1)^{2} - 0.2\\,\\theta$。\n- 梯度下降更新规则：$\\theta_{t+1} = \\theta_{t} - \\alpha\\,\\nabla L(\\theta_{t})$。\n- 步长：$\\alpha = 0.1$。\n- 初始条件：$\\theta_{0}^{(-)} = -0.051$ 和 $\\theta_{0}^{(+)} = -0.049$。\n\n### 步骤 2：使用提取的已知条件进行验证\n\n根据有效性所需标准对问题进行分析。\n- **科学依据**：该问题描述了使用标准梯度下降算法最小化一个标量多项式函数。这是数值优化和微积分中一个基础且被充分理解的问题。该函数是非凸的，这是优化中的一个常见挑战，使得该问题不平凡且具有现实意义。“计算金融”的背景仅仅是一个设定；核心问题是纯数学的。它是科学上合理的。\n- **适定性**：损失函数被明确定义。更新规则、步长和初始条件都以精确的数值给出。问题要求序列的渐进行为，这是给定设置下的一个确定性结果。该问题是适定的。\n- **客观性**：问题以客观和精确的数学语言陈述。没有主观或模棱两可的术语。\n\n问题陈述没有违反任何有效性标准。这是一个标准的、自洽的、可解的数学问题。\n\n### 步骤 3：结论与行动\n\n问题陈述是**有效的**。我们继续进行求解。\n\n### 求解推导\n\n求解需要分析损失函数 $L(\\theta)$ 的拓扑结构和梯度下降算法的动力学特性。\n\n首先，我们必须通过找到其梯度的根来确定损失函数的临界点。损失函数为 $L(\\theta) = \\theta^4 - 2\\theta^2 + 1 - 0.2\\theta$。\n梯度（一阶导数）为：\n$$\n\\nabla L(\\theta) = L'(\\theta) = \\frac{d}{d\\theta} \\left( \\theta^4 - 2\\theta^2 + 1 - 0.2\\theta \\right) = 4\\theta^3 - 4\\theta - 0.2\n$$\n临界点 $\\theta^*$ 是 $L'(\\theta^*) = 0$ 的解：\n$$\n4(\\theta^*)^3 - 4\\theta^* - 0.2 = 0\n$$\n这是一个三次方程。为了理解其根，我们观察到对于未扰动方程 $4\\theta^3 - 4\\theta = 0$，其根为 $\\theta = 0, \\pm 1$。项 $-0.2$ 是一个小扰动。\n- 在 $\\theta = 0$ 附近，$L'(\\theta) \\approx -4\\theta - 0.2$。令其为 $0$ 得到 $-4\\theta \\approx 0.2$，所以 $\\theta \\approx -0.05$。\n- 在 $\\theta = 1$ 附近，令 $\\theta = 1+\\epsilon$。$L'(1+\\epsilon) = 4(1+\\epsilon)^3 - 4(1+\\epsilon) - 0.2 \\approx 4(1+3\\epsilon) - 4(1+\\epsilon) - 0.2 = 8\\epsilon - 0.2$。令其为 $0$ 得到 $8\\epsilon \\approx 0.2$，所以 $\\epsilon \\approx 0.025$，得出一个根在 $\\theta \\approx 1.025$ 附近。\n- 在 $\\theta = -1$ 附近，令 $\\theta = -1+\\epsilon$。$L'(-1+\\epsilon) = 4(-1+\\epsilon)^3 - 4(-1+\\epsilon) - 0.2 \\approx 4(-1+3\\epsilon) - 4(-1+\\epsilon) - 0.2 = 8\\epsilon - 0.2$。令其为 $0$ 得到 $8\\epsilon \\approx 0.2$，所以 $\\epsilon \\approx 0.025$，得出一个根在 $\\theta \\approx -1 + 0.025 = -0.975$ 附近。\n\n因此，我们有三个临界点，大约为 $\\theta_1^* \\approx -0.975$，$\\theta_2^* \\approx -0.05$ 和 $\\theta_3^* \\approx 1.025$。\n\n接下来，我们使用二阶导数检验来对这些临界点进行分类。二阶导数为：\n$$\nL''(\\theta) = \\frac{d}{d\\theta} (4\\theta^3 - 4\\theta - 0.2) = 12\\theta^2 - 4\n$$\n- 在 $\\theta_1^* \\approx -0.975$ 处：$L''(-0.975) = 12(-0.975)^2 - 4 > 12(0.9)^2 - 4 = 12(0.81) - 4 = 9.72 - 4 > 0$。这是一个局部最小值。\n- 在 $\\theta_2^* \\approx -0.05$ 处：$L''(-0.05) = 12(-0.05)^2 - 4 = 12(0.0025) - 4 = 0.03 - 4 < 0$。这是一个局部最大值。\n- 在 $\\theta_3^* \\approx 1.025$ 处：$L''(1.025) = 12(1.025)^2 - 4 > 12(1)^2 - 4 = 8 > 0$。这是一个局部最小值。\n\n我们有两个局部最小值和一个局部最大值。为了确定全局最小值，我们必须比较这两个最小值点上的 $L(\\theta)$ 值。\n- $L(\\theta_1^*) \\approx L(-0.975) = ((-0.975)^2 - 1)^2 - 0.2(-0.975) \\approx (0.95 - 1)^2 + 0.195 = 0.0025 + 0.195 = 0.1975$。\n- $L(\\theta_3^*) \\approx L(1.025) = ((1.025)^2 - 1)^2 - 0.2(1.025) \\approx (1.05 - 1)^2 - 0.205 = 0.0025 - 0.205 = -0.2025$。\n由于 $L(\\theta_3^*) < L(\\theta_1^*)$，$\\theta \\approx 1.025$ 附近的局部最小值是**全局最小值**，而 $\\theta \\approx -0.975$ 附近的局部最小值是一个**次优局部最小值**。\n\n位于 $\\theta_2^* \\approx -0.05$ 的局部最大值充当了梯度流的分界线，或称“分水岭”。位于该最大值左侧的初始点将下降到左侧的吸引盆地，而右侧的点将下降到右侧的吸引盆地。\n\n初始值为 $\\theta_{0}^{(-)} = -0.051$ 和 $\\theta_{0}^{(+)} = -0.049$。它们被策略性地放置在局部最大值 $\\theta_2^* \\approx -0.05$ 的两侧。\n- 对于 $\\theta_{0}^{(-)} = -0.051$：这个点位于局部最大值的稍左侧。在局部最大值紧邻的左侧区间内，梯度 $L'(\\theta)$ 必须为正。我们来验证一下：$L'(-0.051) = 4(-0.051)^3 - 4(-0.051) - 0.2 \\approx 4(-0.00013) + 0.204 - 0.2 = -0.00052 + 0.004 > 0$。\n梯度下降更新为 $\\theta_{1}^{(-)} = \\theta_{0}^{(-)} - \\alpha L'(\\theta_{0}^{(-)})$。由于 $L'(\\theta_{0}^{(-)}) > 0$，更新将减去一个正量，使参数向左移动：$\\theta_{1}^{(-)} < \\theta_{0}^{(-)}$。该轨迹将收敛到左侧的局部最小值，这是次优的。\n- 对于 $\\theta_{0}^{(+)} = -0.049$：这个点位于局部最大值的稍右侧。在局部最大值紧邻的右侧区间内，梯度 $L'(\\theta)$ 必须为负。我们来验证一下：$L'(-0.049) = 4(-0.049)^3 - 4(-0.049) - 0.2 \\approx 4(-0.00012) + 0.196 - 0.2 = -0.00048 - 0.004 < 0$。\n更新为 $\\theta_{1}^{(+)} = \\theta_{0}^{(+)} - \\alpha L'(\\theta_{0}^{(+)})$。由于 $L'(\\theta_{0}^{(+)}) < 0$，更新将加上一个正量，使参数向右移动：$\\theta_{1}^{(+)} > \\theta_{0}^{(+)}$。该轨迹将收敛到右侧的局部最小值，这是全局最小值。\n\n最后，我们评估关于发散的说法。为使梯度下降收敛到最小值 $\\theta^*$，步长的一个充分条件是 $0 < \\alpha < 2/L''(\\theta^*)$。\n- 在左侧最小值（$\\theta_1^*$）处，$L''(\\theta_1^*) \\approx 7.4$。条件是 $\\alpha < 2/7.4 \\approx 0.27$。我们的 $\\alpha = 0.1$ 满足这个条件。\n- 在右侧最小值（$\\theta_3^*$）处，$L''(\\theta_3^*) \\approx 8.6$。条件是 $\\alpha < 2/8.6 \\approx 0.23$。我们的 $\\alpha = 0.1$ 也满足这个条件。\n步长 $\\alpha = 0.1$ 足够小以确保收敛，因此序列不会发散。\n\n### 逐项分析\n\nA. **$\\theta_{0}^{(-)}$ 和 $\\theta_{0}^{(+)}$ 都收敛到 $\\theta \\approx -1$ 附近的左侧局部最小值，该值是全局最小值。**\n初始点位于局部最大值的两侧，因此落入不同的吸引盆地。它们不会收敛到同一个最小值。此外，左侧的最小值是次优的，而不是全局的。\n结论：**错误**。\n\nB. **$\\theta_{0}^{(-)}$ 收敛到 $\\theta \\approx 1$ 附近的右侧局部最小值（全局），而 $\\theta_{0}^{(+)}$ 收敛到 $\\theta \\approx -1$ 附近的左侧局部最小值（次优）。**\n这与我们推导出的行为相反。$\\theta_{0}^{(-)}$ 被推向左侧，而 $\\theta_{0}^{(+)}$ 被推向右侧。\n结论：**错误**。\n\nC. **$\\theta_{0}^{(-)}$ 收敛到 $\\theta \\approx -1$ 附近的左侧局部最小值（次优），而 $\\theta_{0}^{(+)}$ 收敛到 $\\theta \\approx 1$ 附近的右侧局部最小值（全局）。**\n这与我们的分析完全吻合。$\\theta_{0}^{(-)} = -0.051$ 位于次优的左侧最小值的吸引盆地中。$\\theta_{0}^{(+)} = -0.049$ 位于全局的右侧最小值的吸引盆地中。\n结论：**正确**。\n\nD. **两个序列都发散到 $\\pm\\infty$，因为对于这个损失函数，步长 $\\alpha = 0.1$ 太大，无法保证稳定性。**\n我们对步长条件的分析表明，$\\alpha = 0.1$ 完全在稳定收敛到任一最小值的范围内。关于发散的说法是错误的。\n结论：**错误**。", "answer": "$$\\boxed{C}$$"}, {"introduction": "这个练习将带你从理论走向一个具体的应用。你将亲手实现梯度下降的核心循环，来解决计算金融领域中的一个经典问题 [@problem_id:2375264]：为航空公司寻找最优的燃油套期保值比率。通过这个实践，你将在一个高度相关的实际背景下，巩固对梯度下降算法力学的理解。", "id": "2375264", "problem": "一家航空公司试图通过交易石油期货合约来对冲其航空燃油的成本风险。令时间索引表示为 $t \\in \\{1, \\dots, T\\}$。对于每个时期 $t$ ，令 $s_t$ 表示航空公司每单位体积的航空燃油成本的观测变化（例如，每加仑的美元（USD）变化），令 $f_t$ 表示同期每合约单位的期货价格的相应变化。该航空公司选择一个标量对冲比率 $h \\in \\mathbb{R}$，解释为每单位燃油风险敞口的期货合约单位数，以减少对冲后成本变化 $x_t(h) = s_t - h f_t$ 的波动。为抑制过大的头寸，该航空公司会产生一个系数为 $\\gamma \\ge 0$ 的微小二次头寸惩罚。目标是选择 $h$ 以最小化带惩罚的样本均方对冲后变化\n$$\nJ(h) = \\frac{1}{T} \\sum_{t=1}^{T} \\left(s_t - h f_t\\right)^2 + \\gamma h^2.\n$$\n从给定的初始猜测值 $h_0 \\in \\mathbb{R}$ 开始，使用固定步长 $\\alpha > 0$ 的基本梯度下降法来最小化 $J(h)$。在第 $k$ 次迭代时，更新 $h_{k+1} = h_k - \\alpha \\, \\nabla J(h_k)$，并在迭代值的绝对变化满足 $|h_{k+1} - h_k| < \\varepsilon$ 或达到最大迭代次数 $N$ 时终止。您的实现必须：\n- 基于 $J(h)$ 的定义，从第一性原理推导出梯度 $\\nabla J(h)$。\n- 通过使用提供的容差 $\\varepsilon$ 和迭代上限 $N$ 来确保数值稳健性。\n- 返回最终的迭代值作为估计的最优对冲比率 $h^\\star$。\n\n实现一个程序，为以下每个测试用例计算 $h^\\star$。对于每个案例，$s$ 和 $f$ 作为实数的有序列表给出，并为您提供了 $\\gamma$、$\\alpha$、$h_0$、$\\varepsilon$ 和 $N$。\n\n测试套件：\n- 案例 1：\n  - $s = [\\,\\$1.0,\\,-\\$0.5,\\,\\$0.75,\\,-\\$1.25,\\,\\$0.6,\\,-\\$0.8\\,]$\n  - $f = [\\,\\$0.9,\\,-\\$0.4,\\,\\$0.8,\\,-\\$1.1,\\,\\$0.5,\\,-\\$0.7\\,]$\n  - $\\gamma = 0.01$，$\\alpha = 0.05$，$h_0 = 0.0$，$\\varepsilon = 10^{-12}$，$N = 100000$\n- 案例 2：\n  - $s = [\\,\\$1.0,\\,-\\$1.0,\\,\\$1.0,\\,-\\$1.0,\\,\\$0.5,\\,-\\$0.5\\,]$\n  - $f = [\\,-\\$0.8,\\,\\$0.8,\\,-\\$0.7,\\,\\$0.7,\\,-\\$0.4,\\,\\$0.4\\,]$\n  - $\\gamma = 0.02$，$\\alpha = 0.05$，$h_0 = 0.0$，$\\varepsilon = 10^{-12}$，$N = 100000$\n- 案例 3（弱对冲工具；测试正则化）：\n  - $s = [\\,\\$1.0,\\,\\$0.5,\\,-\\$0.5,\\,-\\$1.0,\\,\\$0.3,\\,-\\$0.2\\,]$\n  - $f = [\\,\\$0.1,\\,-\\$0.1,\\,\\$0.1,\\,-\\$0.1,\\,\\$0.05,\\,-\\$0.05\\,]$\n  - $\\gamma = 0.5$，$\\alpha = 0.02$，$h_0 = 0.0$，$\\varepsilon = 10^{-12}$，$N = 100000$\n- 案例 4（更长样本，惩罚可忽略不计）：\n  - $s = [\\,\\$0.2,\\,-\\$0.1,\\,\\$0.3,\\,-\\$0.25,\\,\\$0.15,\\,-\\$0.05,\\,\\$0.4,\\,-\\$0.35\\,]$\n  - $f = [\\,\\$0.18,\\,-\\$0.12,\\,\\$0.28,\\,-\\$0.22,\\,\\$0.14,\\,-\\$0.06,\\,\\$0.39,\\,-\\$0.33\\,]$\n  - $\\gamma = 0.0$，$\\alpha = 0.08$，$h_0 = 0.0$，$\\varepsilon = 10^{-12}$，$N = 100000$\n\n所有货币量均以美元（USD）为单位，但输出的对冲比率是无单位的。您的程序应生成单行输出，其中包含四个估计的对冲比率，格式为方括号内的逗号分隔列表，每个比率四舍五入到恰好 $6$ 位小数（例如，$[1.234567,-0.123456,0.000001,2.000000]$）。不应打印任何其他文本。", "solution": "所陈述的问题需要进行验证。\n\n**步骤1：提取已知条件**\n\n目标是找到对冲比率 $h \\in \\mathbb{R}$，以最小化以下带惩罚的成本函数：\n$$\nJ(h) = \\frac{1}{T} \\sum_{t=1}^{T} \\left(s_t - h f_t\\right)^2 + \\gamma h^2\n$$\n其中：\n- $t \\in \\{1, \\dots, T\\}$ 是时间索引。\n- $s_t$ 是每单位体积的航空燃油成本的变化。\n- $f_t$ 是每合约单位的期货价格的变化。\n- $\\gamma \\ge 0$ 是二次头寸惩罚系数。\n\n优化将使用基本梯度下降法执行，更新规则为：\n$$\nh_{k+1} = h_k - \\alpha \\, \\nabla J(h_k)\n$$\n其中：\n- $k$ 是迭代索引。\n- $h_k$ 是在第 $k$ 次迭代时对冲比率的估计值。\n- $\\alpha > 0$ 是一个固定的步长。\n- $\\nabla J(h_k)$ 是在 $h_k$ 处计算的 $J(h)$ 的梯度。\n\n算法从初始猜测值 $h_0$ 开始，并在以下两种情况之一时终止：\n1.  收敛：$|h_{k+1} - h_k| < \\varepsilon$，其中 $\\varepsilon$ 是一个指定的容差。\n2.  达到最大迭代次数：迭代次数达到最大值 $N$。\n\n提供的测试用例为：\n- **案例 1**：\n  - $s = [1.0, -0.5, 0.75, -1.25, 0.6, -0.8]$\n  - $f = [0.9, -0.4, 0.8, -1.1, 0.5, -0.7]$\n  - $\\gamma = 0.01$，$\\alpha = 0.05$，$h_0 = 0.0$，$\\varepsilon = 10^{-12}$，$N = 100000$\n- **案例 2**：\n  - $s = [1.0, -1.0, 1.0, -1.0, 0.5, -0.5]$\n  - $f = [-0.8, 0.8, -0.7, 0.7, -0.4, 0.4]$\n  - $\\gamma = 0.02$，$\\alpha = 0.05$，$h_0 = 0.0$，$\\varepsilon = 10^{-12}$，$N = 100000$\n- **案例 3**：\n  - $s = [1.0, 0.5, -0.5, -1.0, 0.3, -0.2]$\n  - $f = [0.1, -0.1, 0.1, -0.1, 0.05, -0.05]$\n  - $\\gamma = 0.5$，$\\alpha = 0.02$，$h_0 = 0.0$，$\\varepsilon = 10^{-12}$，$N = 100000$\n- **案例 4**：\n  - $s = [0.2, -0.1, 0.3, -0.25, 0.15, -0.05, 0.4, -0.35]$\n  - $f = [0.18, -0.12, 0.28, -0.22, 0.14, -0.06, 0.39, -0.33]$\n  - $\\gamma = 0.0$，$\\alpha = 0.08$，$h_0 = 0.0$，$\\varepsilon = 10^{-12}$，$N = 100000$\n\n**步骤2：使用提取的已知条件进行验证**\n\n根据验证标准对问题进行评估。\n- **科学依据**：该问题是正则化线性回归在金融工程（对冲）标准问题中的应用，前者是统计学和机器学习中的一项基本技术。目标函数是岭回归（Ridge regression）的一种形式，梯度下降是其典型的优化方法。该公式在科学和数学上是合理的。\n- **适定性**：目标函数 $J(h)$ 是一个关于 $h$ 的二次函数。具体来说，$J(h) = A h^2 - B h + C$，其中二次项的系数是 $A = (\\frac{1}{T} \\sum_{t=1}^T f_t^2) + \\gamma$。由于 $\\gamma \\ge 0$ 且 $f_t^2 \\ge 0$，只要不是所有的 $f_t$ 都为零或 $\\gamma > 0$，系数 $A$ 就严格为正。在所有测试用例中，向量 $f$ 都不是零向量，因此 $A > 0$。这意味着 $J(h)$ 是一个严格凸函数，并拥有唯一的全局最小值。因此，该问题是适定的。\n- **客观性**：问题以精确的数学术语陈述，没有歧义、主观性或个人观点。\n- **完整性**：每个测试用例所需的所有数据和参数（$s, f, \\gamma, \\alpha, h_0, \\varepsilon, N$）均已提供。该问题是自洽的。\n\n**步骤3：结论与行动**\n\n该问题被判定为**有效**。将提供一个解决方案。\n\n**求解推导**\n\n梯度下降算法的核心是计算目标函数的梯度 $\\nabla J(h)$。我们通过对 $J(h)$ 关于 $h$ 求导，从第一性原理推导出该梯度。\n\n目标函数为：\n$$\nJ(h) = \\frac{1}{T} \\sum_{t=1}^{T} (s_t - h f_t)^2 + \\gamma h^2\n$$\n\n梯度 $\\nabla J(h)$ 是普通导数 $\\frac{dJ}{dh}$：\n$$\n\\nabla J(h) = \\frac{d}{dh} \\left( \\frac{1}{T} \\sum_{t=1}^{T} (s_t - h f_t)^2 + \\gamma h^2 \\right)\n$$\n\n根据微分的线性性质，我们可以逐项求导：\n$$\n\\nabla J(h) = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{d}{dh} (s_t - h f_t)^2 + \\frac{d}{dh} (\\gamma h^2)\n$$\n\n对第一项应用链式法则，对第二项应用幂法则：\n$$\n\\frac{d}{dh} (s_t - h f_t)^2 = 2 (s_t - h f_t) \\cdot \\frac{d}{dh}(s_t - h f_t) = 2 (s_t - h f_t) (-f_t) = -2s_t f_t + 2h f_t^2\n$$\n$$\n\\frac{d}{dh} (\\gamma h^2) = 2 \\gamma h\n$$\n\n将这些代回梯度的表达式中：\n$$\n\\nabla J(h) = \\frac{1}{T} \\sum_{t=1}^{T} (-2s_t f_t + 2h f_t^2) + 2 \\gamma h\n$$\n\n我们可以将求和内的项分开：\n$$\n\\nabla J(h) = \\frac{1}{T} \\left( \\sum_{t=1}^{T} (-2s_t f_t) + \\sum_{t=1}^{T} (2h f_t^2) \\right) + 2 \\gamma h\n$$\n$$\n\\nabla J(h) = -\\frac{2}{T} \\sum_{t=1}^{T} s_t f_t + \\frac{2h}{T} \\sum_{t=1}^{T} f_t^2 + 2 \\gamma h\n$$\n\n提出 $2h$：\n$$\n\\nabla J(h) = 2h \\left( \\frac{1}{T} \\sum_{t=1}^{T} f_t^2 + \\gamma \\right) - \\frac{2}{T} \\sum_{t=1}^{T} s_t f_t\n$$\n这就是梯度的解析表达式。该表达式将用于迭代更新规则中。\n\n算法流程如下：\n1.  用 $h_0$ 初始化 $h_k$。\n2.  对于从 $0$ 到 $N-1$ 的 $k$：\n    a. 使用推导出的公式计算梯度 $\\nabla J(h_k)$。为高效计算，我们首先预计算样本矩 $\\frac{1}{T} \\sum s_t f_t$ 和 $\\frac{1}{T} \\sum f_t^2$。\n    b. 更新估计值：$h_{k+1} = h_k - \\alpha \\nabla J(h_k)$。\n    c. 检查收敛性：如果 $|h_{k+1} - h_k| < \\varepsilon$，算法已收敛。最终估计值为 $h^\\star = h_{k+1}$。终止。\n3.  如果循环完成而未满足收敛标准，则算法因达到最大迭代次数而终止。最终估计值为 $h^\\star = h_N$。\n\n此过程将对每个测试用例实施，以找到最优对冲比率 $h^\\star$。提供的步长 $\\alpha$ 足够小，可以确保向凸目标函数的唯一最小值稳定收敛。\n通过设置 $\\nabla J(h) = 0$ 找到的最小值解析解是 $h^\\star = \\frac{\\sum s_t f_t}{\\sum f_t^2 + T\\gamma}$，这可用于验证迭代算法的收敛性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the optimal hedging ratio using gradient descent for multiple test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"s\": [1.0, -0.5, 0.75, -1.25, 0.6, -0.8],\n            \"f\": [0.9, -0.4, 0.8, -1.1, 0.5, -0.7],\n            \"gamma\": 0.01,\n            \"alpha\": 0.05,\n            \"h0\": 0.0,\n            \"epsilon\": 1e-12,\n            \"N\": 100000,\n        },\n        {\n            \"s\": [1.0, -1.0, 1.0, -1.0, 0.5, -0.5],\n            \"f\": [-0.8, 0.8, -0.7, 0.7, -0.4, 0.4],\n            \"gamma\": 0.02,\n            \"alpha\": 0.05,\n            \"h0\": 0.0,\n            \"epsilon\": 1e-12,\n            \"N\": 100000,\n        },\n        {\n            \"s\": [1.0, 0.5, -0.5, -1.0, 0.3, -0.2],\n            \"f\": [0.1, -0.1, 0.1, -0.1, 0.05, -0.05],\n            \"gamma\": 0.5,\n            \"alpha\": 0.02,\n            \"h0\": 0.0,\n            \"epsilon\": 1e-12,\n            \"N\": 100000,\n        },\n        {\n            \"s\": [0.2, -0.1, 0.3, -0.25, 0.15, -0.05, 0.4, -0.35],\n            \"f\": [0.18, -0.12, 0.28, -0.22, 0.14, -0.06, 0.39, -0.33],\n            \"gamma\": 0.0,\n            \"alpha\": 0.08,\n            \"h0\": 0.0,\n            \"epsilon\": 1e-12,\n            \"N\": 100000,\n        },\n    ]\n\n    def compute_h_star(s_data, f_data, gamma, alpha, h0, epsilon, N):\n        \"\"\"\n        Computes the optimal hedging ratio h* using gradient descent.\n        \"\"\"\n        s = np.array(s_data)\n        f = np.array(f_data)\n        T = len(s)\n\n        # Pre-compute sample moments for efficiency\n        # E[f^2] = (1/T) * sum(f_t^2)\n        mean_f_sq = np.sum(f**2) / T\n        # E[sf] = (1/T) * sum(s_t * f_t)\n        mean_s_f = np.sum(s * f) / T\n        \n        h_k = h0\n\n        for _ in range(N):\n            # Gradient: grad_J(h) = 2 * (h * (E[f^2] + gamma) - E[sf])\n            grad_J = 2 * (h_k * (mean_f_sq + gamma) - mean_s_f)\n            \n            h_k_plus_1 = h_k - alpha * grad_J\n            \n            if np.abs(h_k_plus_1 - h_k) < epsilon:\n                return h_k_plus_1\n            \n            h_k = h_k_plus_1\n            \n        return h_k\n\n    results = []\n    for case in test_cases:\n        h_star = compute_h_star(\n            case[\"s\"],\n            case[\"f\"],\n            case[\"gamma\"],\n            case[\"alpha\"],\n            case[\"h0\"],\n            case[\"epsilon\"],\n            case[\"N\"],\n        )\n        results.append(h_star)\n\n    # Format the final output as a comma-separated list of strings,\n    # with each number rounded to 6 decimal places, enclosed in brackets.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n\n```"}, {"introduction": "最后的这项实践 [@problem_id:2375271] 将挑战你，将技能提升到解决一个受商业选址分析启发的更复杂多维问题。你不仅要实现梯度上升算法，还将引入一种更稳健的步长选择方法——回溯线搜索，以确保算法稳定高效地收敛。这个练习展示了梯度类方法在应对复杂现实世界优化任务时的强大威力与灵活性。", "id": "2375271", "problem": "给定一个由空间需求中心构建的二维平滑无约束目标函数。设有 $m \\in \\mathbb{N}$ 个中心点，每个中心点具有位置 $\\boldsymbol{\\mu}_i \\in \\mathbb{R}^2$、正权重 $w_i \\in \\mathbb{R}_{>0}$ 和各向同性扩展 $s_i \\in \\mathbb{R}_{>0}$。定义客户流量强度函数 $T: \\mathbb{R}^2 \\to \\mathbb{R}$ 如下：\n$$\nT(\\boldsymbol{x}) \\equiv \\sum_{i=1}^{m} w_i \\exp\\left( -\\frac{1}{2 s_i^2} \\left\\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\right\\|_2^2 \\right),\n$$\n其中 $\\left\\| \\cdot \\right\\|_2$ 表示欧几里得范数。您的任务是，对于下方的每个测试实例，首先使用基本原理推导梯度 $\\nabla T(\\boldsymbol{x})$，然后实现带有回溯线搜索的梯度上升法来估计 $T(\\boldsymbol{x})$ 的一个最大化点 $\\boldsymbol{x}^\\star \\in \\mathbb{R}^2$。\n\n从以下基本依据出发：\n- 梯度定义为偏导数向量。\n- 可微函数复合的微分链式法则。\n- 梯度方向上的方向导数给出最大瞬时增长率这一事实。\n- 用于回溯线搜索的 Armijo 条件：对于步长 $\\alpha > 0$ 和上升方向 $\\boldsymbol{d}$，如果满足\n$$\nT(\\boldsymbol{x} + \\alpha \\boldsymbol{d}) \\ge T(\\boldsymbol{x}) + c \\alpha \\, \\nabla T(\\boldsymbol{x})^\\top \\boldsymbol{d}\n$$\n则接受 $\\alpha$，其中 $c \\in (0,1)$ 是一个选定的常数。\n\n算法要求：\n- 使用最速上升法，方向为 $\\boldsymbol{d}_k = \\nabla T(\\boldsymbol{x}_k)$，迭代公式为 $\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k + \\alpha_k \\boldsymbol{d}_k$。\n- 使用回溯线搜索，从初始步长 $\\alpha_0$ 开始，每次按因子 $\\rho \\in (0,1)$ 缩减步长，直到满足 Armijo 条件或步长低于最小阈值时停止。\n- 当梯度范数低于容差 $\\varepsilon$ 或达到最大迭代次数时停止。\n- 除 $\\boldsymbol{x} \\in \\mathbb{R}^2$ 外，对 $\\boldsymbol{x}$ 没有其他约束，也无需报告物理单位。\n\n测试套件：\n- 情况 1（通用情况，峰值分离良好）：\n  - $m = 3$,\n  - $\\boldsymbol{\\mu}_1 = (1, 2)$, $w_1 = 100$, $s_1 = 1.0$;\n  - $\\boldsymbol{\\mu}_2 = (-2, -1)$, $w_2 = 80$, $s_2 = 1.5$;\n  - $\\boldsymbol{\\mu}_3 = (3, -3)$, $w_3 = 120$, $s_3 = 0.7$;\n  - 初始化 $\\boldsymbol{x}_0 = (0, 0)$。\n- 情况 2（对称情况，初始化点梯度为零）：\n  - $m = 2$,\n  - $\\boldsymbol{\\mu}_1 = (-2, 0)$, $w_1 = 50$, $s_1 = 0.5$;\n  - $\\boldsymbol{\\mu}_2 = (2, 0)$, $w_2 = 50$, $s_2 = 0.5$;\n  - 初始化 $\\boldsymbol{x}_0 = (0, 0)$。\n- 情况 3（局部最大值与全局最大值，对初始化的敏感性）：\n  - $m = 2$,\n  - $\\boldsymbol{\\mu}_1 = (5, 5)$, $w_1 = 200$, $s_1 = 2.0$;\n  - $\\boldsymbol{\\mu}_2 = (-5, -5)$, $w_2 = 180$, $s_2 = 1.0$;\n  - 初始化 $\\boldsymbol{x}_0 = (-10, -10)$。\n\n所有情况使用相同的线搜索和停止参数：\n- 初始步长 $\\alpha_0 = 1.0$，\n- 缩减因子 $\\rho = 0.5$，\n- Armijo 系数 $c = 10^{-4}$，\n- 梯度范数容差 $\\varepsilon = 10^{-8}$，\n- 最大迭代次数 $K_{\\max} = 10000$，\n- 回溯的最小步长阈值 $\\alpha_{\\min} = 10^{-12}$。\n\n输出规范：\n- 对于每种情况，以列表 $[\\hat{x}, \\hat{y}, T(\\hat{\\boldsymbol{x}})]$ 的形式返回估计的最大化点和目标值，其中 $\\hat{\\boldsymbol{x}} = (\\hat{x}, \\hat{y})$。\n- 每个标量必须是四舍五入到 $6$ 位小数的浮点数。\n- 您的程序应生成单行输出，其中包含三个情况的结果，形式为逗号分隔的列表，并用方括号括起来，同时保留内部列表结构。具体来说，打印\n  $$\n  \\big[ [\\hat{x}_1,\\hat{y}_1,T_1],\\; [\\hat{x}_2,\\hat{y}_2,T_2],\\; [\\hat{x}_3,\\hat{y}_3,T_3] \\big],\n  $$\n  按指定要求四舍五入，不含多余的空格或文本。例如，一个语法上有效的输出如下所示：\n  $$\n  [[0.123456,-1.234567,9.876543],[\\dots],[\\dots]].\n  $$\n预期的答案类型是浮点数列表，最终输出将这三个列表聚合为单行上的一个顶级列表。", "solution": "所提出的问题是一个标准的无约束非线性优化问题。任务是使用最速上升法（一种梯度上升法的特例）来寻找给定目标函数 $T(\\boldsymbol{x})$ 的一个局部最大值。该问题定义明确，科学上合理，并且为数值求解提供了所有必要的参数和条件。因此，该问题是有效的。\n\n解决方案分两个阶段进行。首先，我们推导目标函数梯度的解析形式。其次，我们实现带有回溯线搜索的梯度上升算法，为每个指定的测试用例数值化地寻找一个最大化点。\n\n**1. 梯度 $\\nabla T(\\boldsymbol{x})$ 的推导**\n\n目标函数由下式给出：\n$$\nT(\\boldsymbol{x}) = \\sum_{i=1}^{m} w_i \\exp\\left( -\\frac{1}{2 s_i^2} \\left\\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\right\\|_2^2 \\right)\n$$\n其中 $\\boldsymbol{x} = (x_1, x_2)^\\top \\in \\mathbb{R}^2$ 是变量位置。\n\n$T(\\boldsymbol{x})$ 的梯度，记作 $\\nabla T(\\boldsymbol{x})$，是其关于 $\\boldsymbol{x}$ 各分量的偏导数组成的向量：\n$$\n\\nabla T(\\boldsymbol{x}) = \\begin{pmatrix} \\frac{\\partial T}{\\partial x_1} \\\\ \\frac{\\partial T}{\\partial x_2} \\end{pmatrix}\n$$\n\n根据微分算子的线性性质，和的梯度等于梯度的和。因此，我们可以独立计算求和式中每一项的梯度：\n$$\n\\nabla T(\\boldsymbol{x}) = \\nabla \\left( \\sum_{i=1}^{m} w_i \\exp\\left( -\\frac{1}{2 s_i^2} \\left\\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\right\\|_2^2 \\right) \\right) = \\sum_{i=1}^{m} \\nabla \\left( w_i \\exp\\left( -\\frac{1}{2 s_i^2} \\left\\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\right\\|_2^2 \\right) \\right)\n$$\n\n我们来分析单项 $T_i(\\boldsymbol{x}) = w_i \\exp(f_i(\\boldsymbol{x}))$，其中指数为 $f_i(\\boldsymbol{x}) = -\\frac{1}{2s_i^2} \\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\|_2^2$。\n使用向量函数的链式法则 $\\nabla (\\exp(f(\\boldsymbol{x}))) = \\exp(f(\\boldsymbol{x})) \\nabla f(\\boldsymbol{x})$。应用此法则，我们得到：\n$$\n\\nabla T_i(\\boldsymbol{x}) = w_i \\exp(f_i(\\boldsymbol{x})) \\nabla f_i(\\boldsymbol{x})\n$$\n\n现在，我们必须计算 $f_i(\\boldsymbol{x})$ 的梯度。欧几里得范数的平方为 $\\| \\boldsymbol{z} \\|_2^2 = \\boldsymbol{z}^\\top \\boldsymbol{z}$。令 $\\boldsymbol{z} = \\boldsymbol{x} - \\boldsymbol{\\mu}_i$。则 $f_i(\\boldsymbol{x}) = -\\frac{1}{2s_i^2} (\\boldsymbol{x} - \\boldsymbol{\\mu}_i)^\\top (\\boldsymbol{x} - \\boldsymbol{\\mu}_i)$。\n二次型 $\\boldsymbol{x}^\\top A \\boldsymbol{x}$ 的梯度是 $(A+A^\\top)\\boldsymbol{x}$。这里我们遇到的是一个更简单的情况。令 $\\boldsymbol{\\mu}_i = (\\mu_{i1}, \\mu_{i2})^\\top$。\n$$\nf_i(\\boldsymbol{x}) = -\\frac{1}{2s_i^2} \\left( (x_1 - \\mu_{i1})^2 + (x_2 - \\mu_{i2})^2 \\right)\n$$\n偏导数是：\n$$\n\\frac{\\partial f_i}{\\partial x_1} = -\\frac{1}{2s_i^2} \\cdot 2(x_1 - \\mu_{i1}) \\cdot 1 = -\\frac{1}{s_i^2}(x_1 - \\mu_{i1})\n$$\n$$\n\\frac{\\partial f_i}{\\partial x_2} = -\\frac{1}{2s_i^2} \\cdot 2(x_2 - \\mu_{i2}) \\cdot 1 = -\\frac{1}{s_i^2}(x_2 - \\mu_{i2})\n$$\n将它们组合成梯度向量 $\\nabla f_i(\\boldsymbol{x})$：\n$$\n\\nabla f_i(\\boldsymbol{x}) = \\begin{pmatrix} -\\frac{1}{s_i^2}(x_1 - \\mu_{i1}) \\\\ -\\frac{1}{s_i^2}(x_2 - \\mu_{i2}) \\end{pmatrix} = -\\frac{1}{s_i^2} (\\boldsymbol{x} - \\boldsymbol{\\mu}_i)\n$$\n\n将此代回 $\\nabla T_i(\\boldsymbol{x})$ 的表达式中：\n$$\n\\nabla T_i(\\boldsymbol{x}) = w_i \\exp\\left( -\\frac{1}{2s_i^2} \\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\|_2^2 \\right) \\left( -\\frac{1}{s_i^2} (\\boldsymbol{x} - \\boldsymbol{\\mu}_i) \\right)\n$$\n\n最后，对所有 $i = 1, \\dots, m$ 求和，得到目标函数 $T(\\boldsymbol{x})$ 的完整梯度：\n$$\n\\nabla T(\\boldsymbol{x}) = \\sum_{i=1}^{m} \\left[ - \\frac{w_i}{s_i^2} \\exp\\left( -\\frac{1}{2s_i^2} \\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\|_2^2 \\right) (\\boldsymbol{x} - \\boldsymbol{\\mu}_i) \\right]\n$$\n这个表达式是实现梯度上升算法的基础。\n\n**2. 算法实现**\n\n最速上升法通过沿梯度 $\\nabla T(\\boldsymbol{x}_k)$ 方向前进一步来迭代更新当前估计值 $\\boldsymbol{x}_k$。更新规则为：\n$$\n\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k + \\alpha_k \\boldsymbol{d}_k, \\quad \\text{其中} \\quad \\boldsymbol{d}_k = \\nabla T(\\boldsymbol{x}_k)\n$$\n步长 $\\alpha_k$ 在每次迭代中使用回溯线搜索来确定，以确保目标函数有足够的增量，这由 Armijo 条件指定。\n\n**算法：带回溯线搜索的梯度上升**\n\n1.  **初始化**：\n    -   设置迭代计数器 $k = 0$。\n    -   设置初始点 $\\boldsymbol{x}_0$。\n    -   设置参数：初始步长 $\\alpha_0$、缩减因子 $\\rho$、Armijo 常数 $c$、容差 $\\varepsilon$、最大迭代次数 $K_{\\max}$、最小步长 $\\alpha_{\\min}$。\n\n2.  **迭代**：对于 $k = 0, 1, 2, \\ldots, K_{\\max}-1$：\n    a.  **计算梯度**：使用推导出的公式计算梯度向量 $\\boldsymbol{g}_k = \\nabla T(\\boldsymbol{x}_k)$。\n    b.  **检查收敛性**：如果 $\\| \\boldsymbol{g}_k \\|_2 < \\varepsilon$，则梯度足够小。终止并返回 $\\boldsymbol{x}_k$ 作为估计的最大化点。\n    c.  **回溯线搜索**：寻找步长 $\\alpha_k$。\n        i.   初始化步长 $\\alpha = \\alpha_0$。\n        ii.  上升方向为 $\\boldsymbol{d}_k = \\boldsymbol{g}_k$。\n        iii. 计算 Armijo 条件的右侧：$RHS = T(\\boldsymbol{x}_k) + c \\alpha \\boldsymbol{g}_k^\\top \\boldsymbol{d}_k = T(\\boldsymbol{x}_k) + c \\alpha \\| \\boldsymbol{g}_k \\|_2^2$。\n        iv.  当 $\\alpha \\ge \\alpha_{\\min}$ 时循环：\n             -   计算候选点：$\\boldsymbol{x}_{\\text{new}} = \\boldsymbol{x}_k + \\alpha \\boldsymbol{d}_k$。\n             -   在新点处评估目标函数：$LHS = T(\\boldsymbol{x}_{\\text{new}})$。\n             -   如果 $LHS \\ge RHS$，则 Armijo 条件满足。设置 $\\alpha_k = \\alpha$ 并跳出内部 while 循环。\n             -   否则，缩减步长：$\\alpha = \\rho \\alpha$。\n        v.   如果循环因 $\\alpha < \\alpha_{\\min}$ 而终止，说明步长太小无法取得进展。终止主算法并返回当前点 $\\boldsymbol{x}_k$。\n    d.  **更新位置**：更新估计值：$\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k + \\alpha_k \\boldsymbol{d}_k$。\n\n3.  **终止**：如果循环因达到 $K_{\\max}$ 而完成，终止并返回最终点 $\\boldsymbol{x}_{K_{\\max}}$。\n\n该算法使用指定的参数应用于三个测试用例中的每一个。\n\n-   **情况 1** 从 $\\boldsymbol{x}_0 = (0,0)$ 开始，预计会收敛到三个局部最大值之一。权重、扩展和与原点的距离的组合决定了算法将遵循哪个吸引盆。\n-   **情况 2** 从 $\\boldsymbol{x}_0 = (0,0)$ 开始，由于设置的对称性，该点是一个鞍点。在该点，$\\nabla T(0,0) = \\boldsymbol{0}$。由于梯度范数低于容差 $\\varepsilon$，算法预计将在第 0 次迭代时立即终止。\n-   **情况 3** 展示了对初始化的敏感性。起始点 $\\boldsymbol{x}_0 = (-10,-10)$ 位于靠近 $\\boldsymbol{\\mu}_2 = (-5,-5)$ 的局部最大值的吸引盆中，尽管由于其更高的权重 $w_1=200 > w_2=180$，全局最大值位于 $\\boldsymbol{\\mu}_1 = (5,5)$ 附近。算法预计将收敛到这个局部最大值。\n\n该实现将包含用于计算 $T(\\boldsymbol{x})$ 和 $\\nabla T(\\boldsymbol{x})$ 的函数，以及一个主求解器函数，该函数为每个测试用例协调上述迭代过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimization problem for all test cases and prints the result.\n    \"\"\"\n\n    # Define test cases as a list of dictionaries.\n    # Each dictionary contains: centers, initial_x, m\n    # 'centers' is a list of tuples (mu, w, s)\n    test_cases = [\n        {\n            \"m\": 3,\n            \"centers\": [\n                (np.array([1.0, 2.0]), 100.0, 1.0),\n                (np.array([-2.0, -1.0]), 80.0, 1.5),\n                (np.array([3.0, -3.0]), 120.0, 0.7),\n            ],\n            \"initial_x\": np.array([0.0, 0.0]),\n        },\n        {\n            \"m\": 2,\n            \"centers\": [\n                (np.array([-2.0, 0.0]), 50.0, 0.5),\n                (np.array([2.0, 0.0]), 50.0, 0.5),\n            ],\n            \"initial_x\": np.array([0.0, 0.0]),\n        },\n        {\n            \"m\": 2,\n            \"centers\": [\n                (np.array([5.0, 5.0]), 200.0, 2.0),\n                (np.array([-5.0, -5.0]), 180.0, 1.0),\n            ],\n            \"initial_x\": np.array([-10.0, -10.0]),\n        },\n    ]\n\n    # Algorithmic parameters\n    params = {\n        \"alpha_0\": 1.0,\n        \"rho\": 0.5,\n        \"c\": 1e-4,\n        \"epsilon\": 1e-8,\n        \"k_max\": 10000,\n        \"alpha_min\": 1e-12,\n    }\n\n    results = []\n    for case in test_cases:\n        result_x = run_gradient_ascent(case, params)\n        final_T = T_func(result_x, case[\"centers\"])\n        # Format output as [x, y, T(x)] rounded to 6 decimal places\n        formatted_result = [\n            round(result_x[0], 6),\n            round(result_x[1], 6),\n            round(final_T, 6),\n        ]\n        results.append(formatted_result)\n    \n    # Final print statement in the exact required format\n    # Example: [[0.123456,-1.234567,9.876543],[...],[...]]\n    print(str(results).replace(\" \", \"\"))\n\n\ndef T_func(x, centers):\n    \"\"\"\n    Computes the objective function T(x).\n    x: np.array of shape (2,)\n    centers: list of tuples (mu, w, s)\n    \"\"\"\n    total_intensity = 0.0\n    for mu_i, w_i, s_i in centers:\n        norm_sq = np.sum((x - mu_i)**2)\n        exponent = -1.0 / (2.0 * s_i**2) * norm_sq\n        total_intensity += w_i * np.exp(exponent)\n    return total_intensity\n\ndef grad_T_func(x, centers):\n    \"\"\"\n    Computes the gradient of the objective function nabla T(x).\n    x: np.array of shape (2,)\n    centers: list of tuples (mu, w, s)\n    \"\"\"\n    grad = np.zeros(2)\n    for mu_i, w_i, s_i in centers:\n        diff = x - mu_i\n        norm_sq = np.sum(diff**2)\n        s_i_sq = s_i**2\n        exp_term = np.exp(-1.0 / (2.0 * s_i_sq) * norm_sq)\n        grad += (-w_i / s_i_sq) * exp_term * diff\n    return grad\n\ndef run_gradient_ascent(case, params):\n    \"\"\"\n    Performs gradient ascent with backtracking line search for a single case.\n    \"\"\"\n    x_k = case[\"initial_x\"].copy()\n    centers = case[\"centers\"]\n    \n    alpha_0 = params[\"alpha_0\"]\n    rho = params[\"rho\"]\n    c = params[\"c\"]\n    epsilon = params[\"epsilon\"]\n    k_max = params[\"k_max\"]\n    alpha_min = params[\"alpha_min\"]\n\n    for _ in range(k_max):\n        grad_k = grad_T_func(x_k, centers)\n        grad_norm = np.linalg.norm(grad_k)\n\n        if grad_norm < epsilon:\n            break\n\n        # Backtracking line search\n        alpha = alpha_0\n        d_k = grad_k\n        T_k = T_func(x_k, centers)\n        armijo_rhs_const = c * np.dot(grad_k, d_k) # same as c * grad_norm**2\n\n        while alpha >= alpha_min:\n            x_new = x_k + alpha * d_k\n            T_new = T_func(x_new, centers)\n            \n            if T_new >= T_k + alpha * armijo_rhs_const:\n                x_k = x_new\n                break\n            \n            alpha *= rho\n        \n        if alpha < alpha_min:\n            # Cannot find a suitable step, terminate.\n            break\n            \n    return x_k\n\nsolve()\n```"}]}