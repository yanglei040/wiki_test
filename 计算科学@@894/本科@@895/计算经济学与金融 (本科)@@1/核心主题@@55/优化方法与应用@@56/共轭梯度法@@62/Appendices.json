{"hands_on_practices": [{"introduction": "本练习深入探讨了共轭梯度（CG）方法的一个关键理论特性：其收敛步数等于系统矩阵 $A$ 的不同特征值的数量。你将分析一个投资组合优化问题，其中协方差矩阵 $\\Sigma$ 暗示了一个单因子经济模型，从而只产生两个不同的特征值。这个练习有力地将 CG 方法的抽象数学特性与具体的金融结构联系起来，展示了潜在的经济模型如何决定计算效率。[@problem_id:2382876]", "id": "2382876", "problem": "考虑一个四资产的均值-方差框架，其中收益向量的协方差矩阵 $\\Sigma \\in \\mathbb{R}^{4 \\times 4}$ 由下式给出\n$$\n\\Sigma \\;=\\; I_{4} \\;+\\; v v^{\\top}, \\quad v \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n设超额收益向量为 $b \\in \\mathbb{R}^{4}$，由下式给出\n$$\nb \\;=\\; \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\\\ -4 \\end{pmatrix}.\n$$\n你需要使用共轭梯度 (CG) 法求解线性系统\n$$\n\\Sigma x \\;=\\; b\n$$\n，初始点为 $x_{0} = 0_{4}$，并假设全程使用精确计算。\n\n任务：\n- 证明 $\\Sigma$ 恰好有两个不同的特征值，并且是对称正定的。\n- 证明在精确计算下，共轭梯度法对此系统恰好在两次迭代后终止，并计算满足 $\\Sigma x = b$ 的收敛解向量 $x$。\n- 简要解释 $\\Sigma$ 的特定形式对收益的联合分布所暗含的经济结构。\n\n仅报告收敛解向量 $x$ 作为最终数值答案，并写成单行矩阵的形式。无需四舍五入。", "solution": "首先将对问题陈述的科学合理性和一致性进行验证。\n\n步骤 1：提取已知条件。\n- 协方差矩阵为 $\\Sigma = I_{4} + v v^{\\top}$，其中 $I_4$ 是 $4 \\times 4$ 的单位矩阵。\n- 向量 $v$ 由 $v = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$ 给出。\n- 超额收益向量为 $b = \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\\\ -4 \\end{pmatrix}$。\n- 待求解的线性系统是 $\\Sigma x = b$。\n- 求解方法是共轭梯度 (CG) 法。\n- 初始猜测是零向量 $x_{0} = 0_{4}$。\n- 计算假设使用精确计算。\n\n任务如下：\n1. 证明 $\\Sigma$ 恰好有两个不同的特征值，并且是对称正定 (SPD) 的。\n2. 证明 CG 法在两次迭代后终止，并计算解向量 $x$。\n3. 解释 $\\Sigma$ 的形式所暗含的经济结构。\n\n步骤 2：使用提取的已知条件进行验证。\n该问题具有科学依据，是计算金融和数值线性代数中的一个标准问题。矩阵 $\\Sigma$ 是对单位矩阵的秩一更新，这是一种研究得很透彻的结构。共轭梯度法适用于矩阵为对称正定的系统。为确认这一点，我们检查 $\\Sigma$ 的性质。\n\n对称性：$\\Sigma^{\\top} = (I_{4} + vv^{\\top})^{\\top} = I_{4}^{\\top} + (vv^{\\top})^{\\top} = I_{4} + (v^{\\top})^{\\top}v^{\\top} = I_{4} + vv^{\\top} = \\Sigma$。该矩阵是对称的。\n\n特征值：设 $u$ 是 $\\Sigma$ 的一个特征向量。特征值方程为 $(I_{4} + vv^{\\top})u = \\lambda u$，可重排为 $(vv^{\\top})u = (\\lambda - 1)u$。这表明 $u$ 也是秩一矩阵 $vv^{\\top}$ 的一个特征向量，其特征值为 $\\lambda - 1$。\n矩阵 $vv^{\\top}$ 至多有一个非零特征值。\n- 如果 $u$ 是 $v$ 的倍数，比如 $u=cv$（其中 $c$ 为某个非零标量），则 $vv^{\\top}(cv) = v(v^{\\top}cv) = c(v^{\\top}v)v$。$vv^{\\top}$ 对应的特征值为 $\\lambda_{vv^{\\top}} = v^{\\top}v = 1^2 + 1^2 + 0^2 + 0^2 = 2$。\n- 如果 $u$ 与 $v$ 正交（$v^{\\top}u = 0$），则 $vv^{\\top}u = v(v^{\\top}u) = v(0) = 0$。特征值为 $0$。在 $\\mathbb{R}^{4}$ 中与 $v$ 正交的向量空间维度为 $4-1=3$。\n因此，$vv^{\\top}$ 的特征值为 $2$（重数为 $1$）和 $0$（重数为 $3$）。\n$\\Sigma = I_4 + vv^{\\top}$ 的特征值由 $\\lambda_{\\Sigma} = 1 + \\lambda_{vv^{\\top}}$ 给出。因此，$\\Sigma$ 的特征值为 $1+2=3$（重数为 $1$）和 $1+0=1$（重数为 $3$）。\n由于 $\\Sigma$ 恰有两个不同的特征值（$1$ 和 $3$），且两者均为正数，因此对称矩阵 $\\Sigma$ 是正定的。\n\n该问题是适定的、完整的和一致的。这是一个有效的问题。\n\n步骤 3：结论与行动。\n问题有效。我现在将提供解答。\n\n按要求，该问题分三部分解答。\n\n第 1 部分：$\\Sigma$ 的特征值和对称正定 (SPD) 性质。\n如在验证步骤中确立的，矩阵 $\\Sigma = I_4 + vv^{\\top}$ 是对称的。其特征值为 $3$ 和 $1$。由于所有特征值均为正，$\\Sigma$ 是对称正定的。这证实了共轭梯度法是适用的。\n\n第 2 部分：共轭梯度法终止与求解。\n共轭梯度法保证能在至多 $k$ 次迭代内找到 $\\Sigma x = b$ 的精确解（假设精确计算），其中 $k$ 是 $\\Sigma$ 不同特征值的数量。由于 $\\Sigma$ 恰有两个不同的特征值，该方法将在两次迭代后精确终止。\n\n要计算解 $x$，可以执行 CG 算法的两个步骤。然而，一种更有洞察力的方法是利用我们刚刚推导出的 $\\Sigma$ 的谱特性。这种方法更稳健，且不易出现计算错误。由于方法在两步内收敛且 $x_0=0$，解 $x$ 必须位于 Krylov 子空间 $\\mathcal{K}_{2}(\\Sigma, r_0)$ 中。该子空间由 $\\{r_0, \\Sigma r_0\\}$ 张成，其中 $r_0=b-\\Sigma x_0=b$。\n\n精确解由 $x = \\Sigma^{-1}b$ 给出。我们可以通过将 $b$ 分解到 $\\Sigma$ 的特征空间来计算它。\n对应于 $\\lambda=3$ 的特征空间由向量 $v = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$ 张成。\n对应于 $\\lambda=1$ 的特征空间是 $v$ 所张成空间的正交补空间，即满足 $v^{\\top}u=0$ 的向量 $u$ 的集合。\n\n我们将 $b$ 分解为一个与 $v$ 平行的分量 $b_{\\parallel}$ 和一个与 $v$ 正交的分量 $b_{\\perp}$。\n$b = b_{\\parallel} + b_{\\perp}$。\n平行分量是 $b$ 在 $v$ 上的投影：\n$$\nb_{\\parallel} = \\frac{b^{\\top}v}{v^{\\top}v} v = \\frac{\\begin{pmatrix} 2 & 1 & 5 & -4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}}{\\begin{pmatrix} 1 & 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}} v = \\frac{2(1)+1(1)}{1^2+1^2} v = \\frac{3}{2} v = \\begin{pmatrix} 3/2 \\\\ 3/2 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n正交分量是 $b_{\\perp} = b - b_{\\parallel}$：\n$$\nb_{\\perp} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\\\ -4 \\end{pmatrix} - \\begin{pmatrix} 3/2 \\\\ 3/2 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ -1/2 \\\\ 5 \\\\ -4 \\end{pmatrix}.\n$$\n我们可以验证 $v^{\\top}b_{\\perp} = 1(1/2) + 1(-1/2) + 0 + 0 = 0$，所以 $b_{\\perp}$ 确实在对应 $\\lambda=1$ 的特征空间中。\n\n现在我们将 $\\Sigma^{-1}$ 应用于 $b = b_{\\parallel} + b_{\\perp}$。由于 $b_{\\parallel}$ 是对应 $\\lambda=3$ 的特征向量，而 $b_{\\perp}$ 是对应 $\\lambda=1$ 的特征向量：\n$$\nx = \\Sigma^{-1}b = \\Sigma^{-1}(b_{\\parallel} + b_{\\perp}) = \\Sigma^{-1}b_{\\parallel} + \\Sigma^{-1}b_{\\perp}\n$$\n$$\nx = \\frac{1}{3} b_{\\parallel} + \\frac{1}{1} b_{\\perp}\n$$\n代入向量：\n$$\nx = \\frac{1}{3} \\begin{pmatrix} 3/2 \\\\ 3/2 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1/2 \\\\ -1/2 \\\\ 5 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1/2 \\\\ -1/2 \\\\ 5 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} 1/2+1/2 \\\\ 1/2-1/2 \\\\ 0+5 \\\\ 0-4 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 5 \\\\ -4 \\end{pmatrix}.\n$$\n这就是精确解，CG 算法在两步内找到它。\n\n我们来验证这个解：\n$$\n\\Sigma x = (I_4 + vv^{\\top})x = x + v(v^{\\top}x)\n$$\n$$\nv^{\\top}x = \\begin{pmatrix} 1 & 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 5 \\\\ -4 \\end{pmatrix} = 1(1) + 1(0) = 1.\n$$\n$$\n\\Sigma x = \\begin{pmatrix} 1 \\\\ 0 \\\\ 5 \\\\ -4 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} (1) = \\begin{pmatrix} 1+1 \\\\ 0+1 \\\\ 5+0 \\\\ -4+0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\\\ -4 \\end{pmatrix} = b.\n$$\n解是正确的。\n\n第 3 部分：经济结构。\n资产收益的协方差矩阵 $\\Sigma = I_4 + vv^{\\top}$ 是单因子资产定价模型的一个典型特征。在此类模型中，资产 $i$ 的收益 $R_i$ 由其对共同市场因子 $F$ 的暴露度以及资产特定的异质性冲击 $\\epsilon_i$ 来描述：$R_i = \\beta_i F + \\epsilon_i$。收益向量 $R$ 的协方差矩阵是 $\\text{Cov}(R) = (\\beta\\beta^{\\top})\\sigma_F^2 + D$，其中 $\\beta$ 是因子载荷向量，$\\sigma_F^2$ 是因子的方差，而 $D$ 是异质性方差的对角矩阵。\n\n在本问题中，$\\Sigma = vv^{\\top} + I_4$ 的结构意味着：\n1.  存在一个单位方差（$\\sigma_F^2=1$）的单一系统性风险因子。\n2.  因子暴露度（载荷）向量是 $v = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$。这意味着只有前两种资产暴露于此共同风险因子，载荷均为 $1$。资产 3 和资产 4 不暴露于此因子。\n3.  异质性风险矩阵是 $D = I_4$。这意味着每种资产都有一个独立的、特定的风险成分，其方差为 $1$。\n\n因此，资产 1 和 2 之间的非零协方差 $(\\Sigma)_{12} = 1$ 完全由它们对共同因子的共同暴露度来解释。所有其他资产对都是不相关的，因为除非 $\\{i,j\\}=\\{1,2\\}$，否则对于 $i \\neq j$ 就有 $(\\Sigma)_{ij} = 0$。资产 1 和 2 的总方差为 $(\\Sigma)_{11} = (\\Sigma)_{22} = 2$，由因子方差（$1^2 \\cdot 1 = 1$）和异质性方差（$1$）组成。资产 3 和 4 的总方差为 $(\\Sigma)_{33} = (\\Sigma)_{44} = 1$，这纯粹是异质性方差，因为它们的因子载荷为零。", "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & 0 & 5 & -4 \\end{pmatrix}}\n$$"}, {"introduction": "我们知道共轭梯度（CG）方法是高效的，但它究竟比更简单的替代方法好多少？本练习针对一个简单的双资产投资组合问题，对 CG 方法和最速下降（SD）法进行了直接的量化比较。通过逐步精确计算两种算法的进程，你将亲眼见证为何 CG 方法选择的 $A$-共轭搜索方向能够避免最速下降法的低效“之”字形路径，从而更快地达到最优解。[@problem_id:2382887]", "id": "2382887", "problem": "考虑一个包含两种风险资产的无约束均值-方差投资组合选择问题。目标是最小化二次函数\n$$f(w) \\;=\\; \\frac{1}{2}\\, w^{\\top} \\Sigma \\, w \\;-\\; \\mu^{\\top} w,$$\n其中协方差矩阵为\n$$\\Sigma \\;=\\; \\begin{pmatrix} 2 & 1 \\\n$$4pt] 1 & 3 \\end{pmatrix},$$\n期望收益向量为\n$$\\mu \\;=\\; \\begin{pmatrix} 1 \\\n$$4pt] 0 \\end{pmatrix},$$\n初始投资组合为\n$$w_{0} \\;=\\; \\begin{pmatrix} 0 \\\n$$4pt] 0 \\end{pmatrix}.$$\n\n在第 $k \\in \\{1,2\\}$ 步，从相同的当前迭代点出发，比较两个搜索方向：最速下降方向 $d_{k}^{\\mathrm{SD}} \\,=\\, -\\nabla f(w_{k-1})$ 和一个共轭梯度 (CG) 方向 $d_{k}^{\\mathrm{CG}}$。该 CG 方向被构造为与前一个 CG 方向 $\\Sigma$-共轭（其中 $d_{1}^{\\mathrm{CG}} \\,=\\, -\\nabla f(w_{0})$ 且 $d_{2}^{\\mathrm{CG}}$ 满足 $\\big(d_{2}^{\\mathrm{CG}}\\big)^{\\top} \\Sigma \\, d_{1}^{\\mathrm{CG}} \\,=\\, 0$）。对于每一步的每个方向，采用精确线搜索步长，该步长能最小化沿对应直线上的 $f$ 值，并将步进后的点分别记为 $w_{k}^{\\mathrm{SD}}$ 和 $w_{k}^{\\mathrm{CG}}$。将第 $k$ 步的次优性定义为\n$$s_{k} \\;=\\; f\\!\\big(w_{k}^{\\mathrm{SD}}\\big) \\;-\\; f\\!\\big(w_{k}^{\\mathrm{CG}}\\big).$$\n\n精确计算 $s_{1}$ 和 $s_{2}$。以单个行向量 $\\big(s_{1} \\;\\; s_{2}\\big)$ 的形式报告你的最终答案，使用精确分数。不要包含单位，也不要四舍五入。", "solution": "问题陈述具有科学依据、是良定的，并且是客观的。它描述了一个标准的二次优化任务，为此指定的数值算法，即最速下降法 (SD) 和共轭梯度法 (CG)，都是良定义的。我们进行形式化推导。\n\n要最小化的目标函数由下式给出\n$$f(w) = \\frac{1}{2} w^{\\top} \\Sigma w - \\mu^{\\top} w$$\n该函数的梯度为\n$$\\nabla f(w) = \\Sigma w - \\mu$$\n初始点为 $w_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。初始梯度为 $\\nabla f(w_0) = \\Sigma w_0 - \\mu = -\\mu = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$。\n残差向量定义为 $r = -\\nabla f(w)$。初始残差为 $r_0 = -\\nabla f(w_0) = \\mu = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n\n从迭代点 $w_k$ 沿方向 $d_k$ 进行精确线搜索的最优步长 $\\alpha$ 由最小化 $f(w_k + \\alpha d_k)$ 的公式给出：\n$$\\alpha_k = -\\frac{\\nabla f(w_k)^{\\top} d_k}{d_k^{\\top} \\Sigma d_k}$$\n\n第 $k=1$ 步：\n\n对于 SD 和 CG 方法，初始搜索方向都是负梯度：\n$$d_1^{\\mathrm{SD}} = d_1^{\\mathrm{CG}} = -\\nabla f(w_0) = r_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\n两种方法的步长 $\\alpha_1$ 相同：\n$$\\alpha_1 = -\\frac{\\nabla f(w_0)^{\\top} d_1}{d_1^{\\top} \\Sigma d_1} = \\frac{r_0^{\\top} r_0}{r_0^{\\top} \\Sigma r_0}$$\n我们计算各项：\n$$r_0^{\\top} r_0 = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1$$\n$$r_0^{\\top} \\Sigma r_0 = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 2$$\n步长为 $\\alpha_1 = \\frac{1}{2}$。\n新的迭代点相同：\n$$w_1^{\\mathrm{SD}} = w_1^{\\mathrm{CG}} = w_0 + \\alpha_1 d_1 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix}$$\n让我们将这个共同的迭代点记为 $w_1$。在该点的目标函数值为：\n$$f(w_1) = \\frac{1}{2} w_1^{\\top} \\Sigma w_1 - \\mu^{\\top} w_1 = \\frac{1}{2} \\begin{pmatrix} \\frac{1}{2} & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix}$$\n$$f(w_1) = \\frac{1}{2} \\begin{pmatrix} 1 & \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} - \\frac{1}{2} = \\frac{1}{2} \\left(\\frac{1}{2}\\right) - \\frac{1}{2} = \\frac{1}{4} - \\frac{1}{2} = -\\frac{1}{4}$$\n由于 $f(w_1^{\\mathrm{SD}}) = f(w_1^{\\mathrm{CG}}) = -\\frac{1}{4}$，第一步的次优性为：\n$$s_1 = f(w_1^{\\mathrm{SD}}) - f(w_1^{\\mathrm{CG}}) = 0$$\n\n第 $k=2$ 步：\n\n我们从共同的迭代点 $w_1 = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix}$ 开始。该点的梯度为：\n$$\\nabla f(w_1) = \\Sigma w_1 - \\mu = \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\end{pmatrix}$$\n新的残差为 $r_1 = -\\nabla f(w_1) = \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix}$。\n\n对 SD 方法，搜索方向为 $d_2^{\\mathrm{SD}} = -\\nabla f(w_1) = \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix}$。\n步长为 $\\alpha_2^{\\mathrm{SD}} = \\frac{r_1^{\\top} r_1}{r_1^{\\top} \\Sigma r_1}$。\n分子：$r_1^{\\top} r_1 = (0)^2 + (-\\frac{1}{2})^2 = \\frac{1}{4}$。\n分母：$r_1^{\\top} \\Sigma r_1 = \\begin{pmatrix} 0 & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 0 & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} -\\frac{1}{2} \\\\ -\\frac{3}{2} \\end{pmatrix} = \\frac{3}{4}$。\n$\\alpha_2^{\\mathrm{SD}} = \\frac{1/4}{3/4} = \\frac{1}{3}$。\nSD 迭代点为 $w_2^{\\mathrm{SD}} = w_1 + \\alpha_2^{\\mathrm{SD}} d_2^{\\mathrm{SD}} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} + \\frac{1}{3} \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{6} \\end{pmatrix}$。\n函数值为 $f(w_2^{\\mathrm{SD}}) = \\frac{1}{2} \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{6} \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{6} \\end{pmatrix}$。\n$f(w_2^{\\mathrm{SD}}) = \\frac{1}{2} \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} 1-\\frac{1}{6} \\\\ \\frac{1}{2}-\\frac{3}{6} \\end{pmatrix} - \\frac{1}{2} = \\frac{1}{2} \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} \\frac{5}{6} \\\\ 0 \\end{pmatrix} - \\frac{1}{2} = \\frac{1}{2}\\left(\\frac{5}{12}\\right) - \\frac{1}{2} = -\\frac{7}{24}$。\n\n对于 CG 方法，搜索方向为 $d_2^{\\mathrm{CG}} = r_1 + \\beta_1 d_1^{\\mathrm{CG}}$，其中 $\\beta_1 = \\frac{r_1^{\\top}r_1}{r_0^{\\top}r_0}$。\n由于 $r_1^{\\top} r_1 = \\frac{1}{4}$ 且 $r_0^{\\top} r_0 = 1$，我们得到 $\\beta_1 = \\frac{1}{4}$。\n$$d_2^{\\mathrm{CG}} = r_1 + \\frac{1}{4}d_1^{\\mathrm{CG}} = \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix} + \\frac{1}{4} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{2} \\end{pmatrix}$$\n步长为 $\\alpha_2^{\\mathrm{CG}} = -\\frac{\\nabla f(w_1)^{\\top} d_2^{\\mathrm{CG}}}{(d_2^{\\mathrm{CG}})^{\\top} \\Sigma d_2^{\\mathrm{CG}}} = \\frac{r_1^{\\top} d_2^{\\mathrm{CG}}}{(d_2^{\\mathrm{CG}})^{\\top} \\Sigma d_2^{\\mathrm{CG}}}$。\n分子：$r_1^{\\top} d_2^{\\mathrm{CG}} = \\begin{pmatrix} 0 & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{2} \\end{pmatrix} = \\frac{1}{4}$。\n分母：$(d_2^{\\mathrm{CG}})^{\\top} \\Sigma d_2^{\\mathrm{CG}} = \\begin{pmatrix} \\frac{1}{4} & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -\\frac{5}{4} \\end{pmatrix} = \\frac{5}{8}$。\n$\\alpha_2^{\\mathrm{CG}} = \\frac{1/4}{5/8} = \\frac{2}{5}$。\nCG 迭代点为 $w_2^{\\mathrm{CG}} = w_1 + \\alpha_2^{\\mathrm{CG}} d_2^{\\mathrm{CG}} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} + \\frac{2}{5} \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2}+\\frac{1}{10} \\\\ -\\frac{1}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{5} \\\\ -\\frac{1}{5} \\end{pmatrix}$。\n这就是精确极小值点 $w^* = \\Sigma^{-1}\\mu$，正如在 $N=2$ 维空间中 CG 法所预期的那样。\n最小函数值为 $f(w_2^{\\mathrm{CG}}) = f(w^*) = -\\frac{1}{2}\\mu^\\top w^* = -\\frac{1}{2}\\begin{pmatrix} 1 & 0 \\end{pmatrix}\\begin{pmatrix} 3/5 \\\\ -1/5 \\end{pmatrix} = -\\frac{3}{10}$。\n\n第二步的次优性为：\n$$s_2 = f(w_2^{\\mathrm{SD}}) - f(w_2^{\\mathrm{CG}}) = -\\frac{7}{24} - \\left(-\\frac{3}{10}\\right) = -\\frac{7}{24} + \\frac{3}{10} = \\frac{-35 + 36}{120} = \\frac{1}{120}$$\n\n最终值为 $s_1 = 0$ 和 $s_2 = \\frac{1}{120}$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix} 0 & \\frac{1}{120} \\end{pmatrix}\n}\n$$"}, {"introduction": "共轭梯度（CG）方法的理论精妙之处在于其搜索方向严格保持 $A$-正交性。但如果这个条件被破坏，哪怕只是轻微的破坏，会发生什么呢？这个动手编程练习将让你通过在一个Markowitz投资组合问题的CG运行过程中，故意扰动一个搜索方向来探究这个问题。通过观察这对收敛造成的剧烈影响，你将对为何保持 $A$-正交性不仅是理论上的精巧设计，更是该算法强大功能和保证在 $n$ 步内收敛的绝对关键，获得深刻而实践性的理解。[@problem_id:2382914]", "id": "2382914", "problem": "给定一个由带惩罚项的均值-方差Markowitz投资组合模型产生的线性系统。考虑$n$个资产，其收益向量为$\\mu \\in \\mathbb{R}^n$，协方差矩阵为$\\Sigma \\in \\mathbb{R}^{n \\times n}$，该矩阵是对称正定(SPD)的。引入一个权重为$\\eta > 0$的软预算惩罚项，得到以下无约束二次目标函数\n$$\nf(x) \\equiv \\tfrac{1}{2} x^{\\top} \\Sigma x - \\mu^{\\top} x + \\tfrac{\\eta}{2}\\left(\\mathbf{1}^{\\top} x - 1\\right)^2,\n$$\n其中$\\mathbf{1} \\in \\mathbb{R}^n$是全1向量。一阶条件$\\nabla f(x) = 0$可简化为如下SPD线性系统\n$$\nA x = b,\\quad\\text{with}\\quad A \\equiv \\Sigma + \\eta\\, \\mathbf{1}\\mathbf{1}^{\\top},\\quad b \\equiv \\mu + \\eta\\, \\mathbf{1}.\n$$\n您的任务是实现共轭梯度(CG)法来求解$A x = b$，并展示扰动单个搜索方向如何破坏搜索方向的$A$-正交性，从而阻止其在至多$n$步内精确收敛。\n\n请使用以下在计算金融学中具有代表性的明确指定的实例：\n- 资产数量：$n = 6$。\n- 常数相关矩阵$R \\in \\mathbb{R}^{6 \\times 6}$，相关系数为$\\rho = 0.2$，定义为$R = \\rho \\mathbf{1}\\mathbf{1}^{\\top} + (1-\\rho) I$。\n- 资产标准差$s = [0.15, 0.20, 0.25, 0.30, 0.22, 0.18]$；定义$\\Sigma = \\operatorname{diag}(s)\\, R\\, \\operatorname{diag}(s)$。\n- 期望收益$\\mu = [0.08, 0.10, 0.12, 0.15, 0.11, 0.09]^{\\top}$。\n- 惩罚权重$\\eta = 10^{-2}$。\n\n算法要求：\n- 从$x_0 = 0$开始实现共轭梯度(CG)法。您必须精确计算$n$次迭代，不得提前终止，并使用在精确算术下能为SPD系统保持$A$-共轭性的、有数学依据的更新方式。\n- 实现一个扰动单个搜索方向的机制。具体来说，如果扰动迭代的索引为$k_{\\mathrm{perturb}} \\in \\{0,1,\\dots,n-1\\}$，标量扰动幅度为$\\varepsilon \\geq 0$，则在第$j = k_{\\mathrm{perturb}}$次迭代时，将当前搜索方向$p_j$替换为\n$$\n\\tilde{p}_j = p_j + \\varepsilon \\,\\lVert p_j \\rVert_2\\, e_1,\n$$\n其中$e_1 = [1,0,\\dots,0]^{\\top} \\in \\mathbb{R}^n$是第一个标准基向量。如果$k_{\\mathrm{perturb}} < 0$ 或 $k_{\\mathrm{perturb}} \\ge n$ 或 $\\varepsilon = 0$，则不应施加扰动。\n- 将所有$n$个搜索方向按CG方法使用的顺序记录为矩阵$P \\in \\mathbb{R}^{n \\times n}$的列。\n\n对于每次运行，在精确执行$n$次迭代后，计算以下两项：\n1. $n$次迭代后的残差范数，定义为$\\lVert b - A x_n \\rVert_2$。\n2. 搜索方向的$A$-正交性缺陷，定义为Gram矩阵$G \\equiv P^{\\top} A P$的非对角线元素绝对值的最大值，即\n$$\n\\max_{i \\ne j} \\left| G_{ij} \\right|.\n$$\n\n测试套件：\n对上述固定的$(A,b)$，使用以下五对参数$(k_{\\mathrm{perturb}}, \\varepsilon)$运行您的实现：\n- 情况1：$(-1, 0)$。\n- 情况2：$(2, 1)$。\n- 情况3：$(0, 1)$。\n- 情况4：$(10, 1)$。\n- 情况5：$(3, 10^{-3})$。\n\n最终输出格式：\n- 您的程序必须打印单行，其中包含一个扁平列表，每个测试用例包含2个数字，按顺序排列：对于每个用例，首先是$n$步后的残差范数，然后是$A$-正交性缺陷。所有数字必须使用标准四舍五入精确到8位小数。\n- 具体而言，输出必须是以下形式的单行：\n$$\n[\\text{res}_1,\\text{def}_1,\\text{res}_2,\\text{def}_2,\\dots,\\text{res}_5,\\text{def}_5],\n$$\n其中每个$\\text{res}_i$和$\\text{def}_i$都是一个小数点后恰好有8位数字的十进制数。", "solution": "该问题要求实现共轭梯度(CG)法来求解一个由均值-方差投资组合优化问题导出的线性系统$Ax=b$。核心任务是通过分析定向扰动的影响，来展示保持搜索方向之间$A$-正交性的重要性。\n\n首先，我们对问题陈述进行形式化验证。\n\n给定项如下：\n- 目标函数：$f(x) \\equiv \\tfrac{1}{2} x^{\\top} \\Sigma x - \\mu^{\\top} x + \\tfrac{\\eta}{2}\\left(\\mathbf{1}^{\\top} x - 1\\right)^2$。\n- 线性系统：$A x = b$。\n- 系统矩阵：$A \\equiv \\Sigma + \\eta\\, \\mathbf{1}\\mathbf{1}^{\\top}$。\n- 右端向量：$b \\equiv \\mu + \\eta\\, \\mathbf{1}$。\n- 系统维度：$n = 6$。\n- 相关系数：$\\rho = 0.2$。\n- 相关矩阵：$R = \\rho \\mathbf{1}\\mathbf{1}^{\\top} + (1-\\rho) I$。\n- 资产标准差：$s = [0.15, 0.20, 0.25, 0.30, 0.22, 0.18]^{\\top}$。\n- 协方差矩阵：$\\Sigma = \\operatorname{diag}(s)\\, R\\, \\operatorname{diag}(s)$。\n- 期望收益向量：$\\mu = [0.08, 0.10, 0.12, 0.15, 0.11, 0.09]^{\\top}$。\n- 惩罚权重：$\\eta = 10^{-2}$。\n- CG起始向量：$x_0 = \\mathbf{0}$。\n- 迭代次数：精确为$n=6$。\n- 扰动规则：对于$j = k_{\\mathrm{perturb}}$，用$\\tilde{p}_j = p_j + \\varepsilon \\,\\lVert p_j \\rVert_2\\, e_1$替换搜索方向$p_j$。此规则在$k_{\\mathrm{perturb}} \\in \\{0, 1, \\dots, n-1\\}$且$\\varepsilon > 0$时生效。\n- 度量指标：$\\lVert b - A x_n \\rVert_2$ 和 $\\max_{i \\ne j} \\left| (P^{\\top} A P)_{ij} \\right|$。\n\n该问题是有效的。它在科学上基于计算金融学和数值线性代数的既定原理。协方差矩阵$\\Sigma$被构造为对称正定(SPD)。系统矩阵$A = \\Sigma + \\eta \\mathbf{1}\\mathbf{1}^{\\top}$是一个SPD矩阵$\\Sigma$与一个对称半正定矩阵$\\eta \\mathbf{1}\\mathbf{1}^{\\top}$（因为$\\eta = 10^{-2} > 0$）之和，这确保了$A$也是SPD的。因此，线性系统$Ax=b$是适定的，并且CG方法是一种合适且理论上可靠的求解方法。所有参数和算法要求都以足够的精度进行了规定，并且内部一致。\n\n我们首先构建系统组件。维度为$n=6$。向量$\\mu \\in \\mathbb{R}^6$和$s \\in \\mathbb{R}^6$是给定的。向量$\\mathbf{1}$是$\\mathbb{R}^6$中的全1向量，$I$是$6 \\times 6$的单位矩阵。\n相关矩阵$R$为：\n$$ R = (1-0.2)I + 0.2 \\cdot \\mathbf{1}\\mathbf{1}^{\\top} = 0.8 I + 0.2 \\cdot \\mathbf{1}\\mathbf{1}^{\\top} $$\n协方差矩阵$\\Sigma$构造如下：\n$$ \\Sigma = \\operatorname{diag}([0.15, 0.20, 0.25, 0.30, 0.22, 0.18]) \\cdot R \\cdot \\operatorname{diag}([0.15, 0.20, 0.25, 0.30, 0.22, 0.18]) $$\n当$\\eta = 0.01$时，系统矩阵$A$和向量$b$为：\n$$ A = \\Sigma + 0.01 \\cdot \\mathbf{1}\\mathbf{1}^{\\top} $$\n$$ b = \\mu + 0.01 \\cdot \\mathbf{1} $$\n\n共轭梯度算法是一种迭代方法，通过生成一系列相互$A$-正交（或共轭）的搜索方向$\\{p_j\\}$（即对于$i \\ne j$，有$p_i^{\\top} A p_j = 0$）来求解$Ax=b$。标准算法从$x_0 = \\mathbf{0}$，$r_0 = b$和$p_0 = r_0$开始，按以下步骤进行：\n对于 $j = 0, 1, \\dots, n-1$:\n1. 计算步长：$\\alpha_j = \\frac{r_j^{\\top} r_j}{p_j^{\\top} A p_j}$\n2. 更新解：$x_{j+1} = x_j + \\alpha_j p_j$\n3. 更新残差：$r_{j+1} = r_j - \\alpha_j A p_j$\n4. 计算改进因子：$\\beta_j = \\frac{r_{j+1}^{\\top} r_{j+1}}{r_j^{\\top} r_j}$\n5. 更新搜索方向：$p_{j+1} = r_{j+1} + \\beta_j p_j$\n\n在精确算术中，此过程会生成一组$A$-正交的搜索方向基，并在至多$n$次迭代内找到精确解。这个问题的核心是破坏这一性质。在指定的迭代$j=k_{\\text{perturb}}$处，搜索方向$p_j$被扰动：\n$$ \\tilde{p}_j = p_j + \\varepsilon \\lVert p_j \\rVert_2 e_1 $$\n其中$e_1 = [1, 0, \\dots, 0]^{\\top}$。这个被扰动的方向$\\tilde{p}_j$随后被用来代替$p_j$更新$x_{j+1}$和$r_{j+1}$。关键的是，后续的搜索方向$p_{j+1}$是使用这个被扰动的方向构建的：\n$$ p_{j+1} = r_{j+1} + \\beta_j \\tilde{p}_j $$\n这种扰动破坏了共轭链。新的方向$\\tilde{p}_j$通常与之前的方向$p_0, \\dots, p_{j-1}$不$A$-正交。这个误差会传播，因为所有后续的方向都是建立在这个被污染的步骤之上。结果，这组执行了$n$次的搜索方向$\\{p_0, \\dots, \\tilde{p}_j, \\dots, p_{n-1}\\}$不再是$A$-正交基，因此在$n$步内收敛的理论保证也随之失效。\n\n我们预期会观察到：\n1. 对于未受扰动的运行（情况1和4），最终的残差范数$\\lVert b - A x_n \\rVert_2$和$A$-正交性缺陷$\\max_{i \\ne j} | (P^{\\top} A P)_{ij} |$将接近机器精度（接近0）。矩阵$P^{\\top} A P$将几乎是完美的对角矩阵。\n2. 对于受扰动的运行（情况2、3和5），这两个度量指标都将显著大于0。这表明算法未能在$n$步内收敛到精确解，并且搜索方向的底层共轭属性已被破坏。误差的大小将取决于扰动强度$\\varepsilon$和迭代索引$k_{\\text{perturb}}$。\n\n该解决方案的实现首先根据问题数据定义系统矩阵。然后创建一个函数来执行CG算法。该函数包含了扰动指定搜索方向并存储所有使用过的方向的逻辑。在精确执行$n=6$次迭代后，它从搜索方向矩阵中计算出最终的残差范数和$A$-正交性缺陷。对五个测试用例中的每一个重复此过程。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Sets up the portfolio optimization problem, runs the Conjugate Gradient (CG)\n    method for several test cases with and without perturbation, and\n    calculates the specified performance metrics.\n    \"\"\"\n    # Problem Constants and Data\n    n = 6\n    rho = 0.2\n    s = np.array([0.15, 0.20, 0.25, 0.30, 0.22, 0.18])\n    mu = np.array([0.08, 0.10, 0.12, 0.15, 0.11, 0.09])\n    eta = 1e-2\n\n    # Construct the linear system Ax = b\n    ones = np.ones((n, 1))\n    R = rho * (ones @ ones.T) + (1 - rho) * np.identity(n)\n    D = np.diag(s)\n    Sigma = D @ R @ D\n    A = Sigma + eta * (ones @ ones.T)\n    b = mu + eta * ones.flatten()\n\n    def run_cg_perturbed(A_mat, b_vec, k_perturb, epsilon, n_iter):\n        \"\"\"\n        Runs the Conjugate Gradient algorithm for n_iter steps with an\n        optional perturbation on a specified search direction.\n        \n        Args:\n            A_mat (np.ndarray): The system matrix (n x n).\n            b_vec (np.ndarray): The right-hand side vector (n,).\n            k_perturb (int): The index of the iteration to perturb.\n            epsilon (float): The magnitude of the perturbation.\n            n_iter (int): The number of iterations to run.\n\n        Returns:\n            tuple: A tuple containing:\n                - residual_norm (float): The L2 norm of the final residual.\n                - defect (float): The A-orthogonality defect.\n        \"\"\"\n        dim = A_mat.shape[0]\n        x = np.zeros(dim)\n        r = b_vec - A_mat @ x\n        p = r\n        rs_old = r.T @ r\n        \n        p_storage = []\n        e1 = np.zeros(dim)\n        e1[0] = 1.0\n\n        for j in range(n_iter):\n            p_effective = p\n            \n            # Apply perturbation if conditions are met\n            if j == k_perturb and epsilon > 0:\n                p_norm = np.linalg.norm(p)\n                perturbation = epsilon * p_norm * e1\n                p_effective = p + perturbation\n\n            p_storage.append(p_effective)\n            \n            Ap = A_mat @ p_effective\n            alpha = rs_old / (p_effective.T @ Ap)\n            \n            x = x + alpha * p_effective\n            r = r - alpha * Ap\n            \n            rs_new = r.T @ r\n            \n            beta = rs_new / rs_old\n            p = r + beta * p_effective\n            rs_old = rs_new\n\n        # 1. Calculate final residual norm\n        final_residual_norm = np.linalg.norm(b_vec - A_mat @ x)\n        \n        # 2. Calculate A-orthogonality defect\n        P = np.array(p_storage).T\n        G = P.T @ A_mat @ P\n        np.fill_diagonal(G, 0.0) # We only care about off-diagonal elements\n        orthogonality_defect = np.max(np.abs(G))\n        \n        return final_residual_norm, orthogonality_defect\n\n    test_cases = [\n        (-1, 0.0),    # Case 1: No perturbation (invalid index)\n        (2, 1.0),     # Case 2: Perturb p_2 with eps=1\n        (0, 1.0),     # Case 3: Perturb p_0 with eps=1\n        (10, 1.0),    # Case 4: No perturbation (index out of bounds)\n        (3, 1e-3),    # Case 5: Perturb p_3 with small eps\n    ]\n\n    results = []\n    for k_perturb, epsilon in test_cases:\n        res_norm, defect = run_cg_perturbed(A, b, k_perturb, epsilon, n)\n        results.append(f\"{res_norm:.8f}\")\n        results.append(f\"{defect:.8f}\")\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"}]}