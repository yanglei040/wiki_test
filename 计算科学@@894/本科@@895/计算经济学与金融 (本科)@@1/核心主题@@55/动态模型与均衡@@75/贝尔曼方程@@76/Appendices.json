{"hands_on_practices": [{"introduction": "贝尔曼方程的威力远不止于传统的经济优化问题，它同样是分析动态战略互动的强大工具。本练习将引导你将一个经典的重复博弈——囚徒困境——建模为一个受控马尔可夫过程。通过构建和求解贝尔曼方程，你将学会如何在一个由对手固定策略（“以牙还牙”）所创造的环境中，推导出自己（玩家1）的最优对策，并确定维持合作所需的关键条件。[@problem_id:2437325]", "id": "2437325", "problem": "考虑一个无限期、离散时间的重复囚徒困境，博弈双方对阶段博弈的收益有共同知识。在每个时期 $t \\in \\{1,2,\\dots\\}$，每个玩家选择一个行动 $a_t \\in \\{C,D\\}$。决策者（玩家1）的阶段收益由标准的囚徒困境排序给出，具体数值如下：如果双方合作 $(C,C)$，收益为 $R=3$；如果玩家1背叛而对手合作 $(D,C)$，收益为 $T=5$；如果双方都背叛 $(D,D)$，收益为 $P=1$；如果玩家1合作而对手背叛 $(C,D)$，收益为 $S=0$。这些数值满足 $T>R>P>S$ 和 $2R>T+S$。对手（玩家2）遵循固定的一报还一报 (TFT) 策略：在时期 $t=1$ 合作，并且对于所有 $t \\geq 2$，采取玩家1在时期 $t-1$ 所采取的行动。玩家1以折扣因子 $\\delta \\in (0,1)$ 对未来收益进行折现，并选择行动以最大化其无限期折扣总和 $\\sum_{t=1}^{\\infty} \\delta^{t-1} u_t$，其中 $u_t$ 是时期 $t$ 的阶段收益。\n\n将玩家1面临的环境视为由对手的TFT策略引出的一个双状态受控马尔可夫过程，请从第一性原理（无限期折扣价值的定义和最优性原理）出发，推导玩家1在每个状态下价值函数的贝尔曼最优性条件。利用这些条件，确定最小折扣因子 $\\delta^{\\ast} \\in (0,1)$，使得玩家1的贝尔曼最优最佳响应是在初始时期 $t=1$ 进行合作。请以精确形式（无四舍五入）给出您的答案。", "solution": "该问题经评估具有科学依据、提法明确、客观且内部一致。这是一个将动态规划应用于博弈论的标准问题。所有必要的数据均已提供。我们可以开始解答。\n\n问题要求两件事：首先，为玩家1的决策问题推导贝尔曼最优性条件；其次，计算使玩家1在第一期进行合作成为最优选择的最小折扣因子 $\\delta^{\\ast}$。\n\n首先，我们从玩家1的视角将问题形式化为一个受控马尔可夫过程。对手，玩家2，遵循固定的一报还一报 (TFT) 策略。这意味着玩家2在时期 $t \\geq 2$ 的行动由玩家1在时期 $t-1$ 的行动决定。玩家2在时期 $t=1$ 的行动是合作 ($C$)。因此，系统在任何时期 $t$ 开始时的状态可以由玩家2即将采取的行动完全刻画。我们定义两个状态：\n\\begin{itemize}\n    \\item 状态 $S_C$：玩家2在本期将要合作。\n    \\item 状态 $S_D$：玩家2在本期将要背叛。\n\\end{itemize}\n在时期 $t=1$ 开始时，系统处于状态 $S_C$。\n\n玩家1在任何状态下的可用行动都是 $\\{C, D\\}$。由于玩家2的TFT策略，状态转移是根据玩家1的行动确定性地发生的。\n\\begin{itemize}\n    \\item 如果在状态 $S_C$，玩家1选择 $C$：结果是 $(C,C)$，玩家1的收益是 $R=3$。玩家2将在下个时期合作。系统转移到状态 $S_C$。\n    \\item 如果在状态 $S_C$，玩家1选择 $D$：结果是 $(D,C)$，玩家1的收益是 $T=5$。玩家2将在下个时期背叛。系统转移到状态 $S_D$。\n    \\item 如果在状态 $S_D$，玩家1选择 $C$：结果是 $(C,D)$，玩家1的收益是 $S=0$。玩家2将在下个时期合作。系统转移到状态 $S_C$。\n    \\item 如果在状态 $S_D$，玩家1选择 $D$：结果是 $(D,D)$，玩家1的收益是 $P=1$。玩家2将在下个时期背叛。系统转移到状态 $S_D$。\n\\end{itemize}\n\n令 $V(S_C)$和 $V(S_D)$ 分别为玩家1从状态 $S_C$ 和 $S_D$ 开始的最大未来收益折扣总和的价值函数。贝尔曼最优性原理指出，最优价值函数必须满足一个递归关系。在给定状态下的价值是所有可能行动中可实现的最大价值，其中一个行动的价值是即时收益加上后续状态的折扣价值。\n\n对于状态 $S_C$，玩家1在合作与背叛之间进行选择：\n\\begin{itemize}\n    \\item 选择 $C$：收益为 $R$。下一个状态是 $S_C$。价值为 $R + \\delta V(S_C)$。\n    \\item 选择 $D$：收益为 $T$。下一个状态是 $S_D$。价值为 $T + \\delta V(S_D)$。\n\\end{itemize}\n状态 $S_C$ 的贝尔曼最优性条件是：\n$$V(S_C) = \\max\\{R + \\delta V(S_C), \\; T + \\delta V(S_D)\\}$$\n\n对于状态 $S_D$，玩家1在合作与背叛之间进行选择：\n\\begin{itemize}\n    \\item 选择 $C$：收益为 $S$。下一个状态是 $S_C$。价值为 $S + \\delta V(S_C)$。\n    \\item 选择 $D$：收益为 $P$。下一个状态是 $S_D$。价值为 $P + \\delta V(S_D)$。\n\\end{itemize}\n状态 $S_D$ 的贝尔曼最优性条件是：\n$$V(S_D) = \\max\\{S + \\delta V(S_C), \\; P + \\delta V(S_D)\\}$$\n这两个方程组构成了玩家1价值函数的贝尔曼最优性条件。\n\n接下来，我们必须找到最小折扣因子 $\\delta^{\\ast}$，使得玩家1在初始时期合作。初始状态是 $S_C$。如果合作的价值至少与背叛的价值一样大，玩家1就会选择合作：\n$$R + \\delta V(S_C) \\geq T + \\delta V(S_D)$$\n为了评估这个不等式，我们需要找到最优价值函数 $V(S_C)$ 和 $V(S_D)$，而它们依赖于最优策略。最优策略本身又依赖于 $\\delta$。我们必须找到在状态 $S_C$ 中合作为最优行动的 $\\delta$ 的范围。\n\n让我们假设最优策略是在状态 $S_C$ 下合作。这意味着 $V(S_C) = R + \\delta V(S_C)$，得出 $V(S_C)(1-\\delta) = R$。这一步是不正确的，因为它假定了 $R + \\delta V(S_C)$ 是最大值。正确的方法是检验候选的平稳策略。一个关键的候选策略是“始终合作”(AC)：在状态 $S_C$ 和 $S_D$ 下都合作。让我们确定AC策略为最优的 $\\delta$ 范围。\n\n在AC策略下，玩家1始终选择 $C$。\n如果从 $S_C$ 开始，玩家1选择 $C$，玩家2也选择 $C$，状态保持为 $S_C$。收益流是 $(R, R, R, \\dots)$。价值为 $V^{AC}(S_C) = \\sum_{t=0}^{\\infty} \\delta^t R = \\frac{R}{1-\\delta}$。\n如果从 $S_D$ 开始，玩家1选择 $C$，玩家2选择 $D$。收益为 $S$。下一个状态是 $S_C$。后续的价值是 $\\delta V^{AC}(S_C)$。所以，$V^{AC}(S_D) = S + \\delta V^{AC}(S_C) = S + \\frac{\\delta R}{1-\\delta}$。\n\n要使AC成为贝尔曼最优策略，它必须在每个状态下都是激励兼容的。\n1.  检查在 $S_C$ 状态下的最优性：必须偏好 $C$ 胜过 $D$。\n    $$R + \\delta V^{AC}(S_C) \\geq T + \\delta V^{AC}(S_D)$$\n    代入数值：\n    $$\\frac{R}{1-\\delta} \\geq T + \\delta \\left(S + \\frac{\\delta R}{1-\\delta}\\right)$$\n    $$R \\geq T(1-\\delta) + \\delta S(1-\\delta) + \\delta^2 R$$\n    $$R \\geq T - T\\delta + S\\delta - S\\delta^2 + R\\delta^2$$\n    $$0 \\geq (T-R) - (T-S)\\delta + (R-S)\\delta^2$$\n    代入具体数值 $R=3, T=5, P=1, S=0$:\n    $$0 \\geq (5-3) - (5-0)\\delta + (3-0)\\delta^2$$\n    $$3\\delta^2 - 5\\delta + 2 \\leq 0$$\n    二次方程 $3x^2 - 5x + 2 = 0$ 的根是 $x = \\frac{5 \\pm \\sqrt{25-24}}{6} = \\frac{5 \\pm 1}{6}$，即 $x_1 = 2/3$ 和 $x_2=1$。由于抛物线开口向上，该不等式在 $\\delta \\in [2/3, 1]$ 上成立。\n\n2.  检查在 $S_D$ 状态下的最优性：必须偏好 $C$ 胜过 $D$。\n    $$S + \\delta V^{AC}(S_C) \\geq P + \\delta V^{AC}(S_D)$$\n    代入数值：\n    $$S + \\frac{\\delta R}{1-\\delta} \\geq P + \\delta \\left(S + \\frac{\\delta R}{1-\\delta}\\right)$$\n    $$V^{AC}(S_D) \\geq P + \\delta V^{AC}(S_D)$$\n    $$V^{AC}(S_D)(1-\\delta) \\geq P$$\n    $$\\left(S + \\frac{\\delta R}{1-\\delta}\\right)(1-\\delta) \\geq P$$\n    $$S(1-\\delta) + \\delta R \\geq P$$\n    代入具体数值：\n    $$0(1-\\delta) + 3\\delta \\geq 1$$\n    $$3\\delta \\geq 1 \\implies \\delta \\geq \\frac{1}{3}$$\n\n要使AC策略最优，两个条件都必须成立。$\\delta \\in [2/3, 1]$ 和 $\\delta \\geq 1/3$ 的交集是 $\\delta \\in [2/3, 1)$。因此，对于任何折扣因子 $\\delta \\geq 2/3$，“始终合作”策略是玩家1的一个最优策略。该策略包含了在初始状态 $S_C$ 下合作。\n\n对于 $\\delta < 2/3$ 的值，可以证明最优策略涉及在状态 $S_C$ 下背叛。例如，对于 $\\delta \\in [1/4, 2/3]$，最优策略是在 $S_C$ 中背叛、在 $S_D$ 中合作（一种“反TFT”策略）。对于 $\\delta \\leq 1/4$，最优策略是始终背叛。在所有 $\\delta < 2/3$ 的情况下，玩家1在初始时期的最优行动是背叛。\n\n问题要求的是在初始时期合作为最优行动的最小折扣因子 $\\delta^{\\ast}$。这对应于最优策略涉及在状态 $S_C$ 中合作的区域的下界。正如我们已经发现的，这发生在 $\\delta \\geq 2/3$ 的情况下。在边界 $\\delta=2/3$ 处，玩家1对于一个以合作开始的策略（AC）和一个以背叛开始的策略（反TFT）是无差异的，这使得合作成为最优行动之一。\n\n因此，最小折扣因子是 $\\delta^{\\ast} = 2/3$。", "answer": "$$\\boxed{\\frac{2}{3}}$$"}, {"introduction": "在掌握了离散状态问题后，我们转向宏观经济学中的一个核心连续状态模型。虽然许多动态规划问题因其复杂性必须依赖数值方法求解，但在某些特定函数形式（如常数相对风险厌恶（CRRA）效用）下，我们可以获得优美的解析解。本练习将介绍一种强大的“猜测与验证”方法，通过假设值函数的特定形式来精确求解其系数，从而让你深入理解值函数在某些经济模型中的结构特性。[@problem_id:2437253]", "id": "2437253", "problem": "考虑一个具有恒定相对风险厌恶（CRRA）偏好的单一个人所面临的无限期消费-储蓄问题。该人的时期效用为 $u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$，其中参数 $\\sigma \\in (0,1)$，贴现因子为 $\\beta \\in (0,1)$。财富 $\\{W_t\\}_{t \\geq 0}$ 根据预算转移方程演变\n$$\nW_{t+1} \\;=\\; R_{t+1}\\,\\bigl(W_t - c_t\\bigr),\n$$\n其中总投资回报 $\\{R_t\\}_{t \\geq 1}$ 是跨时间独立同分布（IID）的，以概率 $p \\in (0,1)$ 取值为 $r_L>0$，以概率 $1-p$ 取值为 $r_H>0$。该人面临借贷约束 $c_t \\in [0, W_t]$。如果 $W_t \\leq 0$，则发生破产，该状态是吸收态，对于所有 $s \\geq 1$ 都有 $W_{t+s}=0$，并且在 $c_t=0$ 时的时期效用等于 $u(0)=0$。\n\n令 $V(W)$ 表示解出 Bellman 方程的值函数\n$$\nV(W) \\;=\\; \\max_{c \\in [0,W]} \\Bigl\\{ u(c) \\;+\\; \\beta \\,\\mathbb{E}\\bigl[\\,V\\bigl(R\\,(W-c)\\bigr)\\,\\bigr] \\Bigr\\},\n$$\n其边界条件为 $V(0)=0$，其中期望是针对 $R \\in \\{r_L, r_H\\}$ 计算的。\n\n假设参数满足有限性条件\n$$\n\\Bigl(\\beta \\,\\bigl[p\\,r_L^{\\,1-\\sigma} + (1-p)\\,r_H^{\\,1-\\sigma}\\bigr]\\Bigr)^{\\frac{1}{\\sigma}} \\;<\\; 1.\n$$\n\n定义在吸收态附近的前导阶系数\n$$\n\\Lambda(\\beta,\\sigma,p,r_L,r_H) \\;\\equiv\\; \\lim_{W \\to 0^+} \\frac{V(W)}{W^{\\,1-\\sigma}}.\n$$\n\n仅使用 Bellman 方程和模型的原素，确定 $\\Lambda(\\beta,\\sigma,p,r_L,r_H)$ 的精确闭式表达式。你的最终答案必须是仅包含 $\\beta$、$\\sigma$、$p$、$r_L$ 和 $r_H$ 的单一解析表达式。", "solution": "问题陈述需经过验证。\n\n**步骤1：提取已知条件**\n- 效用函数：$u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$，其中 $\\sigma \\in (0,1)$。\n- 贴现因子：$\\beta \\in (0,1)$。\n- 预算转移方程：$W_{t+1} = R_{t+1}(W_t - c_t)$。\n- 投资回报 $R$：独立同分布，以概率 $p \\in (0,1)$ 取值为 $r_L > 0$，以概率 $1-p$ 取值为 $r_H > 0$。\n- 约束：$c_t \\in [0, W_t]$。\n- 边界/吸收态：$V(0)=0$。\n- Bellman 方程：$V(W) = \\max_{c \\in [0,W]} \\left\\{ u(c) + \\beta \\mathbb{E}[V(R(W-c))] \\right\\}$。\n- 有限性条件：$(\\beta [p r_L^{1-\\sigma} + (1-p) r_H^{1-\\sigma}])^{\\frac{1}{\\sigma}} < 1$。\n- 目标：找到 $\\Lambda \\equiv \\lim_{W \\to 0^+} \\frac{V(W)}{W^{\\,1-\\sigma}}$ 的闭式表达式。\n\n**步骤2：使用提取的已知条件进行验证**\n该问题是量化经济学中一个典型的无限期动态规划问题。所有组成部分都是标准的，并有严格的定义。\n- **科学依据**：该模型是现代宏观经济学（消费-储蓄问题）的基石。CRRA 效用函数是一个标准假设。Bellman 方程是解决此类问题的基本工具。该设定在数学和经济学上都是合理的。\n- **良态性**：该问题是良态的。所提供的有限性条件正是确保存在一个非平凡、有限值函数所必需的条件。约束条件明确。目标是找到一个定义明确的数学极限。\n- **客观性**：问题使用了精确的数学语言进行陈述，不含任何主观性。\n\n**步骤3：结论与行动**\n该问题是有效的。它是一个定义明确且自洽的数学问题，其基础是成熟的经济理论。将提供完整的解答。\n\n此问题旨在确定系数 $\\Lambda$，该系数描述了当财富 $W$ 趋近于零处的吸收态时，值函数 $V(W)$ 的前导阶行为。鉴于 CRRA 效用函数和财富冲击的乘法性质，这类模型允许一个值函数为 $1-\\sigma$ 次齐次的解。因此，我们推测解的形式为：\n$$\nV(W) = A W^{1-\\sigma}\n$$\n其中 $A > 0$ 为某个常数系数。在此推测下，待求的量正是这个系数：\n$$\n\\Lambda = \\lim_{W \\to 0^+} \\frac{V(W)}{W^{\\,1-\\sigma}} = \\lim_{W \\to 0^+} \\frac{A W^{1-\\sigma}}{W^{\\,1-\\sigma}} = A\n$$\n相关的最优策略函数是财富的线性函数：$c(W) = \\lambda W$，其中 $\\lambda \\in (0,1)$ 为某个恒定的消费-财富比。任务简化为求出常数 $A$。这将通过两个逻辑步骤完成：首先，我们确定最优策略参数 $\\lambda$，其次，我们用它来求出值函数系数 $A$。\n\n为简洁起见，我们定义恒定的期望回报因子：\n$$\n\\mathcal{R} \\equiv \\mathbb{E}[R^{1-\\sigma}] = p r_L^{1-\\sigma} + (1-p) r_H^{1-\\sigma}\n$$\nBellman 方程可以写作：\n$$\nV(W) = \\max_{c \\in [0,W]} \\left\\{ \\frac{c^{1-\\sigma}}{1-\\sigma} + \\beta \\mathbb{E}[V(R(W-c))] \\right\\}\n$$\n代入推测的形式 $V(W) = A W^{1-\\sigma}$：\n$$\nA W^{1-\\sigma} = \\max_{c \\in [0,W]} \\left\\{ \\frac{c^{1-\\sigma}}{1-\\sigma} + \\beta \\mathbb{E}[A (R(W-c))^{1-\\sigma}] \\right\\} = \\max_{c \\in [0,W]} \\left\\{ \\frac{c^{1-\\sigma}}{1-\\sigma} + \\beta A (W-c)^{1-\\sigma} \\mathcal{R} \\right\\}\n$$\n\n**步骤1：确定最优策略参数 $\\lambda$。**\n我们将包络定理应用于 Bellman 方程。对值函数关于状态变量 $W$ 求导，得到：\n$$\nV'(W) = \\frac{\\partial}{\\partial W} \\left( \\frac{c^{1-\\sigma}}{1-\\sigma} + \\beta \\mathbb{E}[V(R(W-c))] \\right) \\Biggr|_{c=c(W)}\n$$\n该定理指出，我们可以忽略 $W$ 对最优选择 $c(W)$ 的影响，因此在对 $W$ 求导时，我们将 $c$ 视为固定的：\n$$\nV'(W) = \\beta \\mathbb{E}[V'(R(W-c(W))) \\cdot R]\n$$\n现在，我们代入推测的形式 $V(W) = A W^{1-\\sigma}$，这意味着 $V'(W) = A(1-\\sigma)W^{-\\sigma}$。最优策略是 $c(W)=\\lambda W$。\n$$\nA(1-\\sigma)W^{-\\sigma} = \\beta \\mathbb{E}\\left[ A(1-\\sigma)(R(W - \\lambda W))^{-\\sigma} \\cdot R \\right]\n$$\n从两边消去项 $A(1-\\sigma)$（因为 $A>0$ 且 $\\sigma \\neq 1$）：\n$$\nW^{-\\sigma} = \\beta \\mathbb{E}\\left[ (R(1-\\lambda)W)^{-\\sigma} R \\right] = \\beta (1-\\lambda)^{-\\sigma} W^{-\\sigma} \\mathbb{E}[R^{-\\sigma} R]\n$$\n$$\nW^{-\\sigma} = \\beta (1-\\lambda)^{-\\sigma} W^{-\\sigma} \\mathbb{E}[R^{1-\\sigma}]\n$$\n消去 $W^{-\\sigma}$（对于 $W>0$）并使用 $\\mathcal{R}$ 的定义：\n$$\n1 = \\beta (1-\\lambda)^{-\\sigma} \\mathcal{R}\n$$\n现在我们可以解出最优消费-财富比 $\\lambda$：\n$$\n(1-\\lambda)^{\\sigma} = \\beta \\mathcal{R}\n$$\n$$\n1-\\lambda = (\\beta \\mathcal{R})^{\\frac{1}{\\sigma}}\n$$\n$$\n\\lambda = 1 - (\\beta \\mathcal{R})^{\\frac{1}{\\sigma}} = 1 - \\left( \\beta \\left[ p r_L^{1-\\sigma} + (1-p) r_H^{1-\\sigma} \\right] \\right)^{\\frac{1}{\\sigma}}\n$$\n问题的有限性条件确保了 $(\\beta \\mathcal{R})^{\\frac{1}{\\sigma}} < 1$，这保证了 $\\lambda \\in (0,1)$，与非平凡的内部解一致。\n\n**步骤2：确定值函数系数 $A$。**\n在求得最优策略参数 $\\lambda$ 后，我们将策略 $c = \\lambda W$ 代回 Bellman 方程。由于该策略是最优的，'max' 算子不再需要。\n$$\nA W^{1-\\sigma} = \\frac{(\\lambda W)^{1-\\sigma}}{1-\\sigma} + \\beta \\mathbb{E}[A (R(W-\\lambda W))^{1-\\sigma}]\n$$\n提出公因式：\n$$\nA W^{1-\\sigma} = \\frac{\\lambda^{1-\\sigma} W^{1-\\sigma}}{1-\\sigma} + \\beta A (1-\\lambda)^{1-\\sigma} W^{1-\\sigma} \\mathbb{E}[R^{1-\\sigma}]\n$$\n将整个方程除以 $W^{1-\\sigma}$（对于 $W>0$）并使用 $\\mathcal{R}$ 的定义：\n$$\nA = \\frac{\\lambda^{1-\\sigma}}{1-\\sigma} + \\beta A (1-\\lambda)^{1-\\sigma} \\mathcal{R}\n$$\n我们求解 $A$：\n$$\nA \\left( 1 - \\beta (1-\\lambda)^{1-\\sigma} \\mathcal{R} \\right) = \\frac{\\lambda^{1-\\sigma}}{1-\\sigma}\n$$\n从步骤1中，我们得到恒等式 $(1-\\lambda)^{\\sigma} = \\beta \\mathcal{R}$。我们用它来简化括号中的项：\n$$\n1 - \\beta (1-\\lambda)^{1-\\sigma} \\mathcal{R} = 1 - (1-\\lambda)^{1-\\sigma} (\\beta \\mathcal{R}) = 1 - (1-\\lambda)^{1-\\sigma} (1-\\lambda)^{\\sigma} = 1 - (1-\\lambda)^{1-\\sigma+\\sigma} = 1 - (1-\\lambda) = \\lambda\n$$\n将此简化结果代回关于 $A$ 的方程中：\n$$\nA \\cdot \\lambda = \\frac{\\lambda^{1-\\sigma}}{1-\\sigma}\n$$\n因为 $\\lambda > 0$，我们可以除以 $\\lambda$：\n$$\nA = \\frac{\\lambda^{-\\sigma}}{1-\\sigma}\n$$\n最后，我们将步骤1中得到的 $\\lambda$ 的表达式代入此方程，以求得 $A=\\Lambda$：\n$$\n\\Lambda = \\frac{1}{1-\\sigma} \\left[ 1 - (\\beta \\mathcal{R})^{\\frac{1}{\\sigma}} \\right]^{-\\sigma}\n$$\n代入 $\\mathcal{R}$ 的定义，得到以模型原素参数表示的最终表达式：\n$$\n\\Lambda(\\beta,\\sigma,p,r_L,r_H) = \\frac{1}{1-\\sigma} \\left[ 1 - \\left( \\beta \\left( p r_L^{1-\\sigma} + (1-p) r_H^{1-\\sigma} \\right) \\right)^{\\frac{1}{\\sigma}} \\right]^{-\\sigma}\n$$\n该表达式即为所求系数的精确闭式解。", "answer": "$$\n\\boxed{\\frac{1}{1-\\sigma} \\left[ 1 - \\left(\\beta \\left(p r_L^{1-\\sigma} + (1-p) r_H^{1-\\sigma}\\right)\\right)^{\\frac{1}{\\sigma}} \\right]^{-\\sigma}}\n$$"}, {"introduction": "理论的优雅是基础，但计算经济学的核心在于将理论付诸实践。由于解析解在现实问题中相当罕见，数值方法成为不可或缺的工具。本练习将指导你动手实现值函数迭代（Value Function Iteration, VFI）算法——它正是贝尔曼算子的数值对应物。通过为一个标准的消费储蓄模型编写代码，你将亲身体验理论如何转化为可执行的解决方案，并直观地理解收敛性等关键计算概念。[@problem_id:2437296]", "id": "2437296", "problem": "考虑一个具有单一资产状态的确定性消费-储蓄问题。偏好由常相对风险规避 (CRRA) 效用函数给出，其风险规避参数为 $\\\\sigma &gt; 0$，每期效用为 $u(c) = \\\\frac{c^{1-\\\\sigma}}{1-\\\\sigma}$（当 $\\\\sigma \\\\neq 1$ 时）。代理人每期获得外生收入 $y$，其持有的资产 $a$ 按确定性方式演变，资产的总回报率为 $1+r$，并且不能借贷。状态变量是资产 $a$，控制变量是下一期的资产 $a'$。单期预算约束为 $c = (1+r)a + y - a'$，同时有非负约束 $c \\\\ge 0$ 和借贷约束 $a' \\\\ge 0$。\n\n在 $[0, A_{\\\\max}]$ 上定义一个包含 $N$ 个均匀间隔资产点的有限网格 $\\\\mathcal{A}_N = \\\\{a_0, a_1, \\\\dots, a_{N-1}\\\\}$。动态规划问题是计算能解出贝尔曼方程的价值函数 $V(a)$\n$$\nV(a) = \\\\max_{a' \\\\in \\\\mathcal{A}_N} \\\\left\\\\{ u\\\\big((1+r)a + y - a'\\\\big) + \\\\beta V(a') \\\\right\\\\},\n$$\n约束条件为 $c = (1+r)a + y - a' \\\\ge 0$ 和 $a' \\\\ge 0$，其中 $\\\\beta \\\\in (0,1)$ 是贴现因子。令 $\\\\mathcal{T}$ 表示由等式右侧定义的贝尔曼算子，它将网格上的函数 $V$ 映射到一个新函数 $\\\\mathcal{T}V$。\n\n任务：\n- 从贝尔曼算子 $\\\\mathcal{T}$ 的基本定义以及有界连续效用和紧凑状态-行动集下的动态规划原理出发，通过对初始猜测 $V_0(a) = 0$ 在网格 $\\\\mathcal{A}_N$ 上重复应用 $\\\\mathcal{T}$ 来实现价值函数迭代，直至在上确界范数下收敛。对于每次迭代 $k$，计算误差 $\\\\epsilon_k = \\\\lVert V_{k+1} - V_k \\\\rVert_\\\\infty = \\\\max_i \\\\left| V_{k+1}(a_i) - V_k(a_i) \\\\right|$。\n- 在上确界范数下，使用 $\\\\text{tol} = 10^{-4}$ 的收敛容差和 $1000$ 次的最大迭代次数。记录每种网格规模下收敛所需的总迭代次数。\n- 通过连续误差之比 $\\\\rho_k = \\\\epsilon_{k+1} / \\\\epsilon_k$ 计算经验收敛率的估计值。将终止前最后 $M$ 个可用比率的中位数作为观测到的收敛率进行报告，其中 $M = 10$，如果可用的比率少于 $M$，则使用所有可用的比率。将此观测率四舍五入到三位小数。\n- 使用参数值 $\\\\beta = 0.90$，$r = 0.03$，$y = 1.0$，$\\\\sigma = 2.0$ 以及 $A_{\\\\max} = 5.0$。对于不可行的消费 $c \\\\le 0$，将 $u(c)$ 视为一个非常大的负值，以在最大化过程中强制执行可行性。\n\n测试组：\n- 针对以下资产网格规模 $N \\\\in \\\\{5, 20, 80, 160\\\\}$ 评估程序。\n\n要求的最终输出：\n- 你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。列表中的每个元素对应一个网格规模 $N$，并且本身必须是 $[N, \\\\text{iterations}, \\\\text{observed\\\\_rate}]$ 形式的列表，其中 $\\\\text{iterations}$ 是收敛所需的整数迭代次数，$\\\\text{observed\\\\_rate}$ 是四舍五入到三位小数的浮点数。例如，包含两个测试用例的输出应如下所示：$[[N_1, I_1, R_1],[N_2, I_2, R_2]]$。\n\n科学真实性与推导基础：\n- 从核心定义出发：贝尔曼算子 $\\\\mathcal{T}$、上确界范数 $\\\\lVert \\\\cdot \\\\rVert_\\\\infty$ 以及贴现下的压缩映射原理。不要假设 $V(a)$ 有任何闭式解，也不要使用任何加速方法；严格使用价值函数迭代，并在 $a' \\\\in \\\\mathcal{A}_N$ 上进行全网格搜索。", "solution": "该问题要求使用价值函数迭代法对一个确定性的消费-储蓄模型进行数值求解。该解法必须从动态规划的基本原理推导得出。\n\n**1. 理论基础：贝尔曼方程与压缩映射**\n\n代理人的问题可以递归地表述。价值函数 $V(a)$ 表示代理人从资产水平 $a$ 开始所能实现的最大终生效用。它必须满足贝尔曼方程，该方程定义了一个泛函不动点问题：\n$$\nV(a) = \\max_{a' \\in [0, (1+r)a+y]} \\left\\{ u\\big((1+r)a + y - a'\\big) + \\beta V(a') \\right\\}\n$$\n这里，$u(c)$ 是单期效用函数，$c$ 是消费，$a$ 是当前资产水平（状态），$a'$ 是下一期资产水平（控制），$r$ 是利率，$y$ 是收入，$\\\\beta \\in (0,1)$ 是贴现因子。最大化受制于预算约束 $c = (1+r)a + y - a'$ และ非负约束 $c \\ge 0$ 和 $a' \\ge 0$。\n\n这个方程可以用贝尔曼算子 $\\\\mathcal{T}$ 来表示，它将一个候选价值函数 $W$ 映射到一个新的函数 $\\\\mathcal{T}W$：\n$$\n(\\mathcal{T}W)(a) = \\max_{a'} \\left\\{ u(c) + \\beta W(a') \\right\\}\n$$\n真实的价值函数 $V$ 是该算子的唯一不动点，即 $V = \\\\mathcal{T}V$。该不动点的存在唯一性，以及迭代方法向其收敛的性质，由压缩映射定理保证。对于一个配备了上确界范数 $\\\\lVert f \\\\rVert_\\\\infty = \\\\sup_x |f(x)|$ 的有界函数构成的完备度量空间，如果存在一个常数 $\\\\gamma \\\\in [0,1)$，使得对于空间中的任意两个函数 $f_1, f_2$，都有 $\\\\lVert \\\\mathcal{T}f_1 - \\\\mathcal{T}f_2 \\\\rVert_\\\\infty \\\\le \\\\gamma \\\\lVert f_1 - f_2 \\\\rVert_\\\\infty$，则算子 $\\\\mathcal{T}$ 是一个压缩映射。由于贴现的存在，贝尔曼算子 $\\\\mathcal{T}$ 是一个模为 $\\\\beta$ 的压缩映射。\n\n此性质保证了对于任意初始有界猜测 $V_0$，由价值函数迭代生成的序列 $V_{k+1} = \\\\mathcal{T}V_k$ 会收敛到唯一解 $V$。收敛速度是几何级的，每一步迭代与不动点的距离至少减少一个因子 $\\\\beta$。这意味着连续迭代之间的距离也会缩小：$\\\\lVert V_{k+1} - V_k \\\\rVert_\\\\infty \\\\le \\\\beta \\\\lVert V_k - V_{k-1} \\\\rVert_\\\\infty$。\n\n**2. 离散化与算法实现**\n\n为了通过计算解决该问题，我们必须将连续的状态空间离散化。资产持有量 $a$ 被限制在一个包含 $N$ 个点的有限网格上，即 $\\\\mathcal{A}_N = \\\\{a_0, a_1, \\\\ldots, a_{N-1}\\\\}$，这些点在 $[0, A_{\\\\max}]$ 上均匀分布。因此，价值函数可以用一个长度为 $N$ 的向量 $\\\\mathbf{V}$ 来表示，其中 $\\\\mathbf{V}[i] = V(a_i)$。\n\n价值函数迭代算法如下：\n- **步骤 1：初始化。** 从价值函数向量的初始猜测开始，通常为 $\\\\mathbf{V}_0 = \\\\mathbf{0}$。设置迭代计数器 $k=0$ 和收敛容差 $\\\\text{tol}$。指定的参数为 $\\\\beta=0.90, r=0.03, y=1.0, \\\\sigma=2.0, A_{\\\\max}=5.0, \\\\text{tol}=10^{-4}$。效用函数为 $u(c) = \\\\frac{c^{1-\\\\sigma}}{1-\\\\sigma} = -c^{-1}$（当 $\\\\sigma=2.0$ 时）。\n- **步骤 2：迭代。** 应用贝尔曼算子来更新价值函数。对于网格中的每个点 $a_i \\\\in \\\\mathcal{A}_N$，求解以下最大化问题：\n$$\n\\mathbf{V}_{k+1}[i] = \\max_{j \\in \\{0, \\ldots, N-1\\}} \\left\\{ u\\big((1+r)a_i + y - a_j\\big) + \\beta \\mathbf{V}_k[j] \\right\\}\n$$\n$a'$ 的选择集也被限制在网格 $\\\\mathcal{A}_N$ 上。导致非正消费（$c_{ij} = (1+r)a_i+y-a_j \\\\le 0$）的选择 $a_j$ 是不可行的。按照要求，这种情况通过为这些选择分配一个非常大的负效用值来处理，从而在最大化过程中有效地排除它们。\n- **步骤 3：收敛性检查。** 在每次完全更新价值函数向量后，计算上确界范数误差：\n$$\n\\epsilon_k = \\lVert \\mathbf{V}_{k+1} - \\mathbf{V}_k \\rVert_\\infty = \\max_{i \\in \\{0, \\ldots, N-1\\}} |\\mathbf{V}_{k+1}[i] - \\mathbf{V}_k[i]|\n$$\n循环持续进行，直到 $\\\\epsilon_k < \\\\text{tol}$ 或达到最大迭代次数。\n\n**3. 高效的向量化计算**\n\n使用嵌套循环的朴素实现计算成本很高，每次迭代的复杂度为 $\\\\mathcal{O}(N^2)$。一种更优的方法是使用向量化。设 $\\\\mathbf{a}$ 是资产网格点的 $N \\\\times 1$ 列向量。每个状态下的“手头现金”是向量 $\\\\mathbf{z} = (1+r)\\\\mathbf{a} + y$。我们可以构建一个潜在消费值的 $N \\\\times N$ 矩阵 $\\\\mathbf{C}$，其中 $\\\\mathbf{C}_{ij} = z_i - a_j$。这可以通过广播计算得出，即 $\\\\mathbf{C} = \\\\mathbf{z} - \\\\mathbf{a}^T$。\n\n然后形成一个相应的 $N \\\\times N$ 效用矩阵 $\\\\mathbf{U}$：\n$$\n\\mathbf{U}_{ij} =\n\\begin{cases}\n- (\\mathbf{C}_{ij})^{-1} & \\text{if } \\mathbf{C}_{ij} > 0 \\\\\n-\\infty & \\text{if } \\mathbf{C}_{ij} \\le 0\n\\end{cases}\n$$\n这样，所有状态的价值函数更新就可以通过一次高效的操作完成：\n$$\n\\mathbf{V}_{k+1} = \\max_{\\text{axis}=1}(\\mathbf{U} + \\beta \\mathbf{V}_k^T)\n$$\n其中 `axis=1` 表示对列（即选择变量 $a'$ 的索引 $j$）进行最大化，而 $\\\\mathbf{V}_k^T$ 则会广播到 $\\\\mathbf{U}$ 的各行。\n\n**4. 经验收敛率**\n\n迭代的理论收敛率是 $\\\\beta$。我们可以从误差序列 \\\\{\\\\epsilon_k\\\\} 中经验地估计这个值。连续误差之比 $\\\\rho_k = \\\\epsilon_{k+1} / \\\\epsilon_k$ 应该收敛到 $\\\\beta$。为了获得一个稳定的估计，我们计算算法终止前最后 $M=10$ 个此类比率的中位数。这就得到了 `observed_rate`。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef vfi_solver(N, beta, sigma, y, r, A_max, tol, max_iter, M):\n    \"\"\"\n    Solves the deterministic consumption-savings problem using Value Function Iteration.\n\n    Args:\n        N (int): Number of points in the asset grid.\n        beta (float): Discount factor.\n        sigma (float): Coefficient of relative risk aversion.\n        y (float): Exogenous income.\n        r (float): Net interest rate.\n        A_max (float): Maximum asset level.\n        tol (float): Convergence tolerance for the supremum norm.\n        max_iter (int): Maximum number of iterations.\n        M (int): Number of recent error ratios to use for rate calculation.\n\n    Returns:\n        list: A list containing [N, number_of_iterations, observed_rate].\n    \"\"\"\n    # 1. Discretize the state space\n    a_grid = np.linspace(0, A_max, N)\n\n    # 2. Initialize value function and error list\n    V = np.zeros(N)\n    errors = []\n\n    # 3. Vectorized setup for efficient computation\n    # Reshape grids for broadcasting\n    a_grid_col = a_grid.reshape(N, 1)  # Current assets 'a'\n    a_grid_row = a_grid.reshape(1, N)  # Next period assets 'a''\n\n    # Calculate cash-on-hand for each current asset level\n    cash_on_hand = (1 + r) * a_grid_col + y\n\n    # Calculate consumption for all (a, a') pairs\n    consumption = cash_on_hand - a_grid_row\n    \n    # Define the utility function\n    # Note: As per problem, sigma is not 1.\n    def u(c):\n        return (c**(1 - sigma)) / (1 - sigma)\n\n    # Pre-calculate utility matrix.\n    # Set utility of non-positive consumption to a very large negative number\n    # to enforce feasibility during maximization.\n    utility = np.full((N, N), -np.inf)\n    positive_c_mask = consumption > 0\n    utility[positive_c_mask] = u(consumption[positive_c_mask])\n    \n    # 4. Value Function Iteration loop\n    num_iterations = 0\n    for k in range(max_iter):\n        # Apply the Bellman operator in a vectorized fashion\n        # V is (N,), broadcasting makes it equivalent to V.reshape(1, N)\n        # So (utility + beta * V) is an (N, N) matrix.\n        next_V = np.max(utility + beta * V, axis=1)\n        \n        # Calculate supremum norm error\n        error = np.max(np.abs(next_V - V))\n        errors.append(error)\n        \n        # Update value function\n        V = next_V\n        num_iterations = k + 1\n        \n        # Check for convergence\n        if error < tol:\n            break\n\n    # 5. Calculate the empirical convergence rate\n    observed_rate = np.nan\n    if len(errors) > 1:\n        # Ratios are epsilon_{k+1} / epsilon_k\n        ratios = np.array(errors[1:]) / np.array(errors[:-1])\n        \n        # Take the median of the last M ratios (or fewer if not enough exist)\n        num_ratios_to_consider = min(M, len(ratios))\n        if num_ratios_to_consider > 0:\n            last_ratios = ratios[-num_ratios_to_consider:]\n            observed_rate = float(np.median(last_ratios))\n\n    # Round the rate to three decimal places\n    if not np.isnan(observed_rate):\n        observed_rate = round(observed_rate, 3)\n\n    return [N, num_iterations, observed_rate]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the parameters from the problem statement.\n    params = {\n        'beta': 0.90,\n        'sigma': 2.0,\n        'y': 1.0,\n        'r': 0.03,\n        'A_max': 5.0,\n        'tol': 1e-4,\n        'max_iter': 1000,\n        'M': 10\n    }\n    \n    # Define the test cases from the problem statement.\n    test_cases = [5, 20, 80, 160]\n\n    results = []\n    for N in test_cases:\n        # Run the solver for one grid size\n        result = vfi_solver(N=N, **params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # e.g., [[N1,I1,R1],[N2,I2,R2]]\n    # Using str().replace() is a reliable way to get this format without spaces.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"}]}