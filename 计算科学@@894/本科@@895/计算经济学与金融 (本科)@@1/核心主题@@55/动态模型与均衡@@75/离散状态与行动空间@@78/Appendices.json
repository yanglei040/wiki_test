{"hands_on_practices": [{"introduction": "库存管理是运筹学和经济学中的一个经典问题，其核心是在持有成本和缺货成本之间找到平衡。本练习将引导你使用动态规划来处理一个经典的库存控制场景，目标是最小化无限时间范围内的预期折扣成本。通过这个实践，你将学习如何通过一个巧妙的对数网格将连续的库存状态离散化，并应用价值迭代算法来找到最优订货策略 [@problem_id:2388580]。", "id": "2388580", "problem": "一家生产单一产品的公司在离散时间、无限期界面临随机需求。库存为非负数，并以单位计量。该公司在每个时期需求实现之前选择一个离散的订货量。未满足的需求会流失（无缺货补给）。库存的状态空间是按对数而非线性离散化的。\n\n模型定义如下。\n\n- 状态空间。库存状态 $s$ 属于一个有限网格 $\\mathcal{S}$，该网格由 $\\{0\\}$ 和一组严格为正的点集构成，这些点按对数间隔分布并四舍五入为整数。给定严格正整数 $s_{\\min}^+$ 和 $s_{\\max}$（其中 $s_{\\min}^+ \\le s_{\\max}$），以及一个正整数 $N_+$（其中 $N_+ \\ge 2$），定义\n$$\n\\tilde{s}_i = \\exp\\!\\left(\\log(s_{\\min}^+) + (i-1)\\frac{\\log(s_{\\max}) - \\log(s_{\\min}^+)}{N_+ - 1}\\right), \\quad i \\in \\{1,\\dots,N_+\\},\n$$\n然后四舍五入为整数 $r_i = \\operatorname{round}(\\tilde{s}_i)$，删除重复值，并将 $\\mathcal{S}$ 设置为 $\\mathcal{S} = \\{0\\} \\cup \\{r_i\\}_{i=1}^{N_+}$ 并按升序排序。网格 $\\mathcal{S}$ 包含 0 和最大（并包括）为 $s_{\\max}$ 的严格正整数。\n\n- 行动空间。公司从离散集合 $\\mathcal{A} = \\{0,1,2,\\dots,A_{\\max}\\}$ 中选择一个订货量 $a$，其中 $A_{\\max}$ 是一个给定的正整数。\n\n- 需求。在每个时期，需求 $D$ 是一个离散随机变量，其支撑集为一个由非负整数构成的有限集合 $\\{d_1,\\dots,d_K\\}$，相应概率为 $\\{p_1,\\dots,p_K\\}$，且满足 $\\sum_{k=1}^K p_k = 1$。\n\n- 时间顺序与状态转移。给定当前库存 $s \\in \\mathcal{S}$ 和行动 $a \\in \\mathcal{A}$，需求 $D$ 实现。销售量为 $\\min\\{s+a, D\\}$。离散化之前的期末库存为\n$$\ns'_{\\text{cont}} = \\max\\{s + a - D, 0\\}。\n$$\n下一状态通过将 $s'_{\\text{cont}}$ 使用最近邻投影到网格 $\\mathcal{S}$ 上获得\n$$\n\\Pi(s'_{\\text{cont}}) = \\operatorname*{arg\\,min}_{g \\in \\mathcal{S}} |g - s'_{\\text{cont}}|,\n$$\n若存在平局，则选择较小的网格点。\n\n- 单期成本。瞬时成本为\n$$\ng(s,a,D) = K \\cdot \\mathbf{1}\\{a > 0\\} + c\\,a + h\\,s'_{\\text{cont}} + p\\,\\max\\{D - (s+a), 0\\},\n$$\n其中 $K$ 是固定订货成本，$c$ 是单位订货成本，$h$ 是针对期末库存 $s'_{\\text{cont}}$ 的单位持有成本，$p$ 是单位缺货损失惩罚。\n\n- 目标。对于折扣因子 $\\beta \\in (0,1)$，公司在所有将 $s_t \\in \\mathcal{S}$ 映射到 $a_t \\in \\mathcal{A}$ 的可测策略中，寻求一个能够最小化无限期界期望折扣成本 $\\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\beta^t g(s_t,a_t,D_t)\\right]$ 的平稳策略。\n\n您的任务是实现一个程序，对于下方测试套件中的每一组参数，计算在上文定义过的精确离散模型（包括投影 $\\Pi(\\cdot)$）下，初始库存 $s_0 = 0$ 时的最优平稳行动。要求输出的为整数形式的最优行动。\n\n测试套件。对于每个测试用例，请精确使用给定的参数。所有以数字形式给出的量均为无量纲量。\n\n- 测试用例 1（一般情况）：\n  - 折扣因子 $\\beta = 0.95$。\n  - 对数网格参数 $s_{\\min}^+ = 1$, $s_{\\max} = 50$, $N_+ = 10$。\n  - 行动上界 $A_{\\max} = 20$。\n  - 成本 $K = 5$, $c = 1$, $h = 0.1$, $p = 2$。\n  - 需求支撑集和概率：$\\{0,1,2,3,4\\}$，概率为 $\\{0.1,0.2,0.3,0.25,0.15\\}$。\n\n- 测试用例 2（无固定订货成本，较高惩罚）：\n  - 折扣因子 $\\beta = 0.95$。\n  - 对数网格参数 $s_{\\min}^+ = 1$, $s_{\\max} = 30$, $N_+ = 8$。\n  - 行动上界 $A_{\\max} = 15$。\n  - 成本 $K = 0$, $c = 1$, $h = 0.5$, $p = 5$。\n  - 需求支撑集和概率：$\\{0,2,5,8\\}$，概率为 $\\{0.2,0.3,0.3,0.2\\}$。\n\n- 测试用例 3（零需求边界情况）：\n  - 折扣因子 $\\beta = 0.9$。\n  - 对数网格参数 $s_{\\min}^+ = 1$, $s_{\\max} = 20$, $N_+ = 6$。\n  - 行动上界 $A_{\\max} = 10$。\n  - 成本 $K = 3$, $c = 1$, $h = 0.2$, $p = 4$。\n  - 需求支撑集和概率：$\\{0\\}$，概率为 $\\{1.0\\}$。\n\n- 测试用例 4（确定性高需求，高缺货损失惩罚）：\n  - 折扣因子 $\\beta = 0.9$。\n  - 对数网格参数 $s_{\\min}^+ = 1$, $s_{\\max} = 40$, $N_+ = 12$。\n  - 行动上界 $A_{\\max} = 20$。\n  - 成本 $K = 1$, $c = 1$, $h = 0.1$, $p = 10$。\n  - 需求支撑集和概率：$\\{10\\}$，概率为 $\\{1.0\\}$。\n\n最终输出格式。您的程序应产生单行输出，按顺序包含测试用例 1 到 4 在初始库存 $s_0=0$ 时的最优行动，格式为一个用方括号括起来的逗号分隔列表（例如，$\\texttt{[a_1,a_2,a_3,a_4]}$）。要求输出的为整数。", "solution": "在尝试解决方案之前，需要对问题进行验证。\n\n### 步骤 1：提取已知条件\n\n- **状态空间 $\\mathcal{S}$**：库存水平的有限网格。它由一个由 $N_+ \\ge 2$ 个严格正整数 $\\{r_i\\}_{i=1}^{N_+}$ 构成的基本集合和状态 $\\{0\\}$ 组成。点 $r_i$ 是在 $s_{\\min}^+ > 0$ 和 $s_{\\max} \\ge s_{\\min}^+$ 之间对数分布的点 $\\tilde{s}_i$ 四舍五入后的整数值。对于 $i \\in \\{1,\\dots,N_+\\}$，$\\tilde{s}_i$ 的公式为：\n$$\n\\tilde{s}_i = \\exp\\!\\left(\\log(s_{\\min}^+) + (i-1)\\frac{\\log(s_{\\max}) - \\log(s_{\\min}^+)}{N_+ - 1}\\right)\n$$\n网格的正数部分是 $\\{r_i = \\operatorname{round}(\\tilde{s}_i)\\}_{i=1}^{N_+}$，并移除了重复值。完整的状态空间是 $\\mathcal{S} = \\{0\\} \\cup \\{r_i\\}$，并按升序排序。\n\n- **行动空间 $\\mathcal{A}$**：可能订货量的集合，由 $\\mathcal{A} = \\{0,1,2,\\dots,A_{\\max}\\}$ 给出，其中 $A_{\\max}$ 是给定的正整数。\n\n- **需求 $D$**：一个离散随机变量，其支撑集为一个有限集合 $\\{d_1,\\dots,d_K\\}$，对应概率为 $\\{p_1,\\dots,p_K\\}$，其中 $\\sum_{k=1}^K p_k = 1$。\n\n- **状态转移**：给定状态 $s \\in \\mathcal{S}$ 和行动 $a \\in \\mathcal{A}$，投影前的期末库存水平是 $s'_{\\text{cont}} = \\max\\{s + a - D, 0\\}$。下一状态 $s' \\in \\mathcal{S}$ 通过使用最近邻投影将 $s'_{\\text{cont}}$ 投影到网格 $\\mathcal{S}$ 上来确定：\n$$\ns' = \\Pi(s'_{\\text{cont}}) = \\operatorname*{arg\\,min}_{g \\in \\mathcal{S}} |g - s'_{\\text{cont}}|\n$$\n若存在平局，则选择较小的网格点。\n\n- **单期成本 $g(s,a,D)$**：单个时期内产生的成本是：\n$$\ng(s,a,D) = K \\cdot \\mathbf{1}\\{a > 0\\} + c\\,a + h\\,s'_{\\text{cont}} + p\\,\\max\\{D - (s+a), 0\\}\n$$\n其中 $K$ 是固定订货成本，$c$ 是单位订货成本，$h$ 是单位持有成本，$p$ 是单位缺货损失惩罚。\n\n- **目标**：以折扣因子 $\\beta \\in (0,1)$ 最小化无限期界期望折扣成本 $\\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\beta^t g(s_t,a_t,D_t)\\right]$。目标是找到一个平稳最优策略 $\\pi^*: \\mathcal{S} \\to \\mathcal{A}$。\n\n- **任务**：对于四个不同的参数集（测试用例），确定在初始状态 $s_0=0$ 时的最优平稳行动 $a^*$。\n\n### 步骤 2：使用提取的已知条件进行验证\n\n- **科学依据**：该问题是单一品项不确定性库存控制问题的标准表述，这是运筹学和计算经济学中的一个经典课题。使用离散状态空间（具体来说是对数网格）和离散行动是使连续状态问题在计算上可行的常用有效技术。该模型基于动态规划的成熟原则。它在科学上是合理的。\n\n- **适定性**：该问题是一个有限状态、有限行动、无限期界的折扣动态规划问题。对于非负参数，单期成本 $g(s,a,D)$ 是非负的，因此它们有下界。折扣因子 $\\beta$ 严格介于 0 和 1 之间。在这些条件下，贝尔曼算子是一个压缩映射。动态规划中的标准定理（例如，来自 Blackwell 的定理）保证了存在唯一有界的值函数，它是贝尔曼算子的不动点，并且存在一个平稳最优策略。诸如值迭代之类的算法保证会收敛到此唯一解。该问题是适定的。\n\n- **客观性**：所有变量、参数和函数都以数学精度进行了定义。语言正式且无歧义。该问题是客观的。\n\n### 步骤 3：结论与行动\n\n问题陈述是有效的。它具有科学依据、适定性和客观性，并具有完整一致的设置。将提供一个解决方案。\n\n### 解决方案\n\n所描述的问题是一个无限期界折扣成本随机动态规划问题。最优策略通过求解贝尔曼方程找到，该方程刻画了每个状态 $s \\in \\mathcal{S}$ 的最小期望成本，即值函数 $V(s)$。值函数必须满足：\n$$\nV(s) = \\min_{a \\in \\mathcal{A}} \\left\\{ \\mathbb{E}_D[g(s,a,D)] + \\beta \\mathbb{E}_D[V(\\Pi(\\max\\{s+a-D, 0\\}))] \\right\\}\n$$\n该方程表明，处于状态 $s$ 的值是在所有可能的行动 $a$ 上取最小值，其内容是即时期望成本与折扣后未来期望成本之和。期望 $\\mathbb{E}_D[\\cdot]$ 是对需求 $D$ 的分布计算的。\n\n我们将最小化项内的表达式定义为状态-行动值函数，或称 $Q$-函数：\n$$\nQ(s,a) = \\mathbb{E}_D[g(s,a,D)] + \\beta \\mathbb{E}_D[V(\\Pi(\\max\\{s+a-D, 0\\}))]\n$$\n贝尔曼方程可写为 $V(s) = \\min_{a \\in \\mathcal{A}} Q(s,a)$。\n\n该问题使用**值迭代**算法求解，该算法保证收敛到最优值函数 $V^*$，因为对于 $\\beta \\in (0,1)$，贝尔曼算子是一个压缩映射。算法流程如下：\n\n1.  **初始化**：\n    - 根据指定的对数网格生成过程构建离散状态空间 $\\mathcal{S}$。令 $N_s = |\\mathcal{S}|$。\n    - 初始化值函数向量 $V_0 \\in \\mathbb{R}^{N_s}$，通常对所有 $s \\in \\mathcal{S}$ 设置 $V_0(s) = 0$。\n    - 设置迭代计数器 $n=0$ 和一个小的收敛容差 $\\epsilon > 0$。\n\n2.  **迭代**：对 $n=0, 1, 2, \\dots$ 重复：\n    - 对于每个状态 $s \\in \\mathcal{S}$，使用前一次迭代的值函数 $V_n$ 求解一步贝尔曼方程，计算更新后的值 $V_{n+1}(s)$：\n    $$\n    V_{n+1}(s) = \\min_{a \\in \\mathcal{A}} \\left\\{ \\mathbb{E}_D[g(s,a,D)] + \\beta \\sum_{k=1}^K p_k V_n\\left(\\Pi(\\max\\{s+a-d_k, 0\\})\\right) \\right\\}\n    $$\n    状态-行动对 $(s,a)$ 的即时期望成本为：\n    $$\n    \\mathbb{E}_D[g(s,a,D)] = K \\cdot \\mathbf{1}\\{a > 0\\} + c\\,a + \\sum_{k=1}^K p_k \\left( h\\,\\max\\{s+a-d_k, 0\\} + p\\,\\max\\{d_k-(s+a), 0\\} \\right)\n    $$\n    - 迭代 $n$ 时的平稳策略 $\\pi_n(s)$ 是对每个状态 $s$ 实现最小值的行动 $a$。\n\n3.  **终止**：当值函数收敛时，迭代停止。这通过检查连续值函数之间差值的上确界范数是否小于容差来确定：\n    $$\n    \\max_{s \\in \\mathcal{S}} |V_{n+1}(s) - V_n(s)| < \\epsilon\n    $$\n    一个足够小的容差，例如 $\\epsilon=10^{-8}$，可确保得到的策略是最优的。\n\n4.  **结果**：收敛后，算法产生最优值函数 $V^*$ 和最优平稳策略 $\\pi^*$。问题要求在特定初始状态 $s_0=0$ 时的最优行动，即 $\\pi^*(0)$。\n\n对于每个测试用例，计算实现包括以下步骤：\n\na.  一个函数生成排序后的状态网格 $\\mathcal{S}$ 以及从状态值到其索引的映射，以实现高效查找。\nb.  实现一个投影函数 $\\Pi(x)$。给定一个值 $x$ 和排序后的网格 $\\mathcal{S}$，它使用 `numpy.argmin(numpy.abs(S - x))` 找到最接近的网格点的索引。平局规则（选择较小的值）由 `argmin` 自动处理，因为它返回最小值的第一个索引。\nc.  执行主值迭代循环。在该循环内部，嵌套循环遍历每个状态 $s \\in \\mathcal{S}$ 和每个行动 $a \\in \\mathcal{A}$。对于每个 $(s,a)$ 对，再通过一个循环遍历所有需求结果 $\\{d_k\\}$，以计算完整的 $Q(s,a)$ 值。\nd.  在找到给定状态下所有行动的 $Q$ 值后，将最小值存储为 $V_{n+1}(s)$，并将相应的行动存储为该状态的当前最优策略。\ne.  遍历所有状态后，检查收敛标准。若不满足，则将 $V_n$ 更新为 $V_{n+1}$，并重复该过程。\nf.  一旦达到收敛，从最终策略数组中提取状态 $s=0$ 的最优行动。对所有提供的测试用例重复此过程。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the inventory management problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"beta\": 0.95, \"s_min_plus\": 1, \"s_max\": 50, \"N_plus\": 10,\n            \"A_max\": 20, \"K\": 5, \"c\": 1, \"h\": 0.1, \"p\": 2,\n            \"demand_support\": [0, 1, 2, 3, 4],\n            \"demand_probs\": [0.1, 0.2, 0.3, 0.25, 0.15]\n        },\n        {\n            \"beta\": 0.95, \"s_min_plus\": 1, \"s_max\": 30, \"N_plus\": 8,\n            \"A_max\": 15, \"K\": 0, \"c\": 1, \"h\": 0.5, \"p\": 5,\n            \"demand_support\": [0, 2, 5, 8],\n            \"demand_probs\": [0.2, 0.3, 0.3, 0.2]\n        },\n        {\n            \"beta\": 0.9, \"s_min_plus\": 1, \"s_max\": 20, \"N_plus\": 6,\n            \"A_max\": 10, \"K\": 3, \"c\": 1, \"h\": 0.2, \"p\": 4,\n            \"demand_support\": [0], \"demand_probs\": [1.0]\n        },\n        {\n            \"beta\": 0.9, \"s_min_plus\": 1, \"s_max\": 40, \"N_plus\": 12,\n            \"A_max\": 20, \"K\": 1, \"c\": 1, \"h\": 0.1, \"p\": 10,\n            \"demand_support\": [10], \"demand_probs\": [1.0]\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        optimal_action = compute_optimal_policy(**params)\n        results.append(optimal_action)\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\ndef create_state_grid(s_min_plus, s_max, N_plus):\n    \"\"\"\n    Generates the logarithmically spaced state grid.\n    \"\"\"\n    if s_min_plus > s_max:\n        raise ValueError(\"s_min_plus must be less than or equal to s_max\")\n    if N_plus < 2:\n        raise ValueError(\"N_plus must be 2 or greater\")\n\n    log_s_min = np.log(s_min_plus)\n    log_s_max = np.log(s_max)\n    \n    # Generate log-spaced points\n    grid_points = np.exp(log_s_min + (np.arange(N_plus)) * (log_s_max - log_s_min) / (N_plus - 1))\n    \n    # Round to nearest integer\n    rounded_points = np.round(grid_points).astype(int)\n    \n    # Remove duplicates and form the final grid with 0\n    positive_grid = sorted(list(set(rounded_points)))\n    \n    final_grid = np.array([0] + positive_grid)\n    \n    return final_grid\n\ndef compute_optimal_policy(beta, s_min_plus, s_max, N_plus, A_max, K, c, h, p, demand_support, demand_probs):\n    \"\"\"\n    Computes the optimal policy for a given set of parameters using value iteration.\n    \"\"\"\n    \n    # 1. Setup\n    S = create_state_grid(s_min_plus, s_max, N_plus)\n    s_to_idx = {val: i for i, val in enumerate(S)}\n    \n    A = np.arange(A_max + 1)\n    D = np.array(demand_support)\n    P_d = np.array(demand_probs)\n    \n    V = np.zeros(len(S))\n    policy = np.zeros(len(S), dtype=int)\n    tol = 1e-8\n    max_iter = 5000\n\n    def project(x_cont, grid):\n        \"\"\" Projects a continuous value onto the discrete grid S. \"\"\"\n        idx = np.argmin(np.abs(grid - x_cont))\n        return grid[idx]\n\n    # Pre-calculate components of one-period cost that depend only on (s,a,D)\n    # This can be vectorized for efficiency.\n    s_plus_a_tensor = S[:, np.newaxis] + A[np.newaxis, :] # Shape (|S|, |A|)\n    s_cont_prime_tensor = np.maximum(s_plus_a_tensor[:, :, np.newaxis] - D[np.newaxis, np.newaxis, :], 0)\n    \n    holding_costs_exp = h * np.sum(P_d * s_cont_prime_tensor, axis=2)\n    penalty_costs_exp = p * np.sum(P_d * np.maximum(D - s_plus_a_tensor[:, :, np.newaxis], 0), axis=2)\n    \n    fixed_costs = K * (A > 0)\n    unit_costs = c * A\n    \n    E_g = fixed_costs[np.newaxis, :] + unit_costs[np.newaxis, :] + holding_costs_exp + penalty_costs_exp\n    \n    # 2. Value Iteration\n    for _ in range(max_iter):\n        V_new = np.copy(V)\n        \n        # Calculate expected future value term\n        E_V = np.zeros((len(S), len(A)))\n        for s_idx, s in enumerate(S):\n            for a_idx, a in enumerate(A):\n                future_value = 0\n                for d_val, p_val in zip(D, P_d):\n                    s_cont_prime = max(s + a - d_val, 0)\n                    s_prime = project(s_cont_prime, S)\n                    s_prime_idx = s_to_idx[s_prime]\n                    future_value += p_val * V[s_prime_idx]\n                E_V[s_idx, a_idx] = future_value\n\n        Q_values = E_g + beta * E_V\n        \n        V_new = np.min(Q_values, axis=1)\n        policy = A[np.argmin(Q_values, axis=1)]\n\n        # 3. Check convergence\n        if np.max(np.abs(V - V_new)) < tol:\n            break\n        \n        V = V_new\n\n    # 4. Return optimal action for s=0\n    return policy[0]\n\nif __name__ == '__main__':\n    solve()\n```"}, {"introduction": "在不确定性下做决策时，探索（exploration）与利用（exploitation）之间的权衡是一个核心挑战。这个问题将带你进入一个资源勘探的世界，你需要为一家矿业公司在有限时间内制定最优的钻探计划 [@problem_id:2388581]。你将通过构建一个基于已知和未知资源计数的动态规划模型，并使用反向归纳法来解决它，从而亲身体验如何量化并解决这一根本性的经济权衡问题。", "id": "2388581", "problem": "一家矿业公司正在考虑一个网格世界地图上的有限期决策问题，地图中的单元具有离散的单元格类型。每个单元格处于以下三种状态之一：未知、贫矿或富矿。时间是离散的，存在一个 $T$ 期的有限期界。在每个时期，公司可以且仅可以钻取一个单元格。钻取一个未知单元格会立即揭示其真实类型，并在该时期产生即时采矿收益；钻取一个已知为贫矿或富矿的单元格，则在该时期产生相应的收益。钻取后，该单元格的状态会保持不变，并可在未来时期再次钻取。公司的目标是最大化各时期收益的预期折现总和。\n\n问题表述。假设网格包含 $N$ 个单元格。对于一个未知单元格，其为富矿的先验概率为 $ \\theta \\in [0,1]$，其为贫矿的概率为 $1-\\theta$。设钻取一个已知贫矿的每期收益为 $r_P \\ge 0$，钻取一个已知富矿的收益为 $r_R \\ge r_P$。钻取一个未知单元格，除了实现的采矿收益外，还会产生每次钻探的勘探成本 $c_U \\ge 0$。折现因子为 $\\beta \\in [0,1]$。在给定类型的情况下，每个单元格的状态与其他单元格相互独立，并且类型不会随时间改变。公司在 $T$ 个时期内，每期钻取且仅钻取一个单元格。初始状态下，所有单元格均为未知。\n\n状态、行动和转移。定义计数 $(k_R, k_P, k_U)$ 分别表示当前已知为富矿、贫矿和未知的单元格数量，使得 $k_R + k_P + k_U = N$。在状态 $(k_R, k_P, k_U)$ 下的行动是从可用的类别中选择一个进行钻取：如果 $k_R > 0$，可钻富矿；如果 $k_P > 0$，可钻贫矿；如果 $k_U > 0$，可钻未知单元格。即时回报为：\n- 如果钻取富矿：$r_R$。\n- 如果钻取贫矿：$r_P$。\n- 如果钻取未知单元格：有 $\\theta$ 的概率单元格被揭示为富矿，并立即产生 $r_R - c_U$ 的收益；有 $1-\\theta$ 的概率被揭示为贫矿，并立即产生 $r_P - c_U$ 的收益。\n\n钻取一个未知单元格后，下一状态以概率 $\\theta$ 更新为 $(k_R+1, k_P, k_U-1)$，或以概率 $1-\\theta$ 更新为 $(k_R, k_P+1, k_U-1)$。钻取一个已知类型的单元格后，计数保持不变。\n\n目标。令 $V_T(k_R,k_P,k_U)$ 表示从状态 $(k_R,k_P,k_U)$ 开始，在剩余 $T$ 个时期内的最优期望价值。目标是计算 $V_T(0,0,N)$，即从所有单元格均为未知状态开始的最优期望折现总收益。期望总收益是各时期 $t = 1, \\dots, T$ 的即时收益乘以 $\\beta^{t-1}$ 后的总和。\n\n任务。编写一个完整的、可运行的程序，针对以下每组参数，通过基于计数的精确动态规划计算最优期望值 $V_T(0,0,N)$，并返回结果。\n\n测试套件。使用以下参数集，每组指定为 $(N, T, \\theta, r_P, r_R, c_U, \\beta)$，其中所有概率都应解释为小数（而非百分比）：\n1. $(3, 3, 0.4, 1.0, 5.0, 0.5, 0.95)$\n2. $(4, 5, 0.5, 0.5, 6.0, 0.0, 0.9)$\n3. $(2, 2, 0.3, 1.0, 4.0, 2.5, 1.0)$\n4. $(1, 3, 0.5, 0.0, 10.0, 1.0, 1.0)$\n5. $(3, 3, 0.7, 1.0, 3.0, 0.2, 0.0)$\n\n最终输出格式。您的程序应生成单行输出，其中包含用逗号分隔的浮点数列表，四舍五入到 $6$ 位小数，并用方括号括起来，顺序与测试套件相同（例如 $[x_1,x_2,x_3,x_4,x_5]$）。不应打印任何其他文本。输出不涉及物理单位，所有概率必须作为小数而非百分比处理。", "solution": "对问题陈述进行验证。\n\n### 步骤1：提取已知信息\n- **领域**：一个网格世界地图上的有限期决策问题。\n- **单元格状态**：未知、贫矿、富矿。\n- **时间期界**：$T$ 个离散时期。\n- **行动**：每期钻取且仅钻取一个单元格。\n- **目标**：最大化各时期收益的预期折现总和，即 $\\sum_{t=1}^{T} \\beta^{t-1} \\times (\\text{第 } t \\text{ 期的收益})$。\n- **网格大小**：$N$ 个单元格。\n- **初始状态**：所有 $N$ 个单元格均为未知。\n- **参数**：\n    - 未知单元格为富矿的先验概率：$\\theta \\in [0,1]$。\n    - 钻取贫矿的收益：$r_P \\ge 0$。\n    - 钻取富矿的收益：$r_R \\ge r_P$。\n    - 钻取未知单元格的勘探成本：$c_U \\ge 0$。\n    - 折现因子：$\\beta \\in [0,1]$。\n- **状态表示**：$(k_R, k_P, k_U)$，表示富矿、贫矿和未知单元格的数量，其中 $k_R + k_P + k_U = N$。\n- **从状态 $(k_R, k_P, k_U)$ 出发的行动**：\n    - 钻取富矿（若 $k_R > 0$）。\n    - 钻取贫矿（若 $k_P > 0$）。\n    - 钻取未知单元格（若 $k_U > 0$）。\n- **即时回报**：\n    - 钻取富矿：$r_R$。\n    - 钻取贫矿：$r_P$。\n    - 钻取未知单元格：以概率 $\\theta$ 获得 $r_R - c_U$，以概率 $1-\\theta$ 获得 $r_P - c_U$。\n- **状态转移**：\n    - 钻取富矿/贫矿：状态 $(k_R, k_P, k_U)$ 保持不变。\n    - 钻取未知单元格：状态以概率 $\\theta$ 变为 $(k_R+1, k_P, k_U-1)$，或以概率 $1-\\theta$ 变为 $(k_R, k_P+1, k_U-1)$。\n- **任务**：使用精确动态规划计算从初始状态出发的最优期望值 $V_T(0,0,N)$。\n\n### 步骤2：使用提取的已知信息进行验证\n根据指定标准对问题进行评估。\n- **科学性基础**：该问题是一个经典的有限期动态规划问题，是运筹学、强化学习和计算经济学中的基础模型。其表述建立在成熟的数学原理之上。该问题有效。\n- **适定性**：该问题是适定的。状态空间虽然较大，但是有限且离散的。行动空间是有限的。时间期界是有限的。目标函数定义明确。这些条件保证了存在一个唯一的、稳定的、有意义的最优值，并且可以通过反向归纳法计算得出。\n- **客观性**：问题使用精确、无歧义的数学语言陈述。所有参数均已定义，目标有明确的公式。它不含主观或非科学的主张。\n\n该问题没有表现出任何诸如科学上不健全、不完整、矛盾或不可行等缺陷。它是其领域内一个标准的、可解的问题。\n\n### 步骤3：结论与行动\n该问题是**有效的**。将提供一个解决方案。\n\n---\n\n该问题是一个有限期离散时间动态规划问题。解决方案是通过计算每个时间点上每个状态的价值函数来找到的，这利用了最优性原理。这一过程通过反向归纳法实现。\n\n设状态由元组 $(t, k_R, k_P)$ 定义，其中 $t$ 是剩余的时间期数，$k_R$ 是已知为富矿的单元格数量，$k_P$ 是已知为贫矿的单元格数量。未知单元格的数量则隐含为 $k_U = N - k_R - k_P$。价值函数 $V_t(k_R, k_P)$ 表示在剩余 $t$ 个时期的情况下，从当前状态出发所能获得的最大期望折现未来收益总和。我们的目标是计算 $V_T(0, 0)$。\n\n动态规划的递归关系由贝尔曼方程定义。\n基准情况是当剩余 $t=0$ 期时，此时无法再采取任何行动，因此价值为 $0$。\n$$V_0(k_R, k_P) = 0 \\quad \\forall k_R, k_P \\text{ such that } k_R+k_P \\le N$$\n\n对于 $t > 0$，价值函数 $V_t(k_R, k_P)$ 是在可用行动集合 $A(k_R, k_P)$ 上可获得的最大价值。\n$$V_t(k_R, k_P) = \\max_{a \\in A(k_R, k_P)} \\left\\{ R(k_R, k_P, a) + \\beta \\, \\mathbb{E}[V_{t-1}(k'_R, k'_P) \\mid k_R, k_P, a] \\right\\}$$\n其中 $R(k_R, k_P, a)$ 是在当前状态下采取行动 $a$ 的即时期望收益，而期望是针对下一状态 $(k'_R, k'_P)$ 计算的。\n\n可能的行动及其对应价值如下：\n$1$. **钻取富矿**（若 $k_R > 0$ 则可用）：\n即时收益为 $r_R$。状态不发生改变。此行动的价值为：\n$$v_{\\text{drill_R}} = r_R + \\beta V_{t-1}(k_R, k_P)$$\n\n$2$. **钻取贫矿**（若 $k_P > 0$ 则可用）：\n即时收益为 $r_P$。状态不发生改变。此行动的价值为：\n$$v_{\\text{drill_P}} = r_P + \\beta V_{t-1}(k_R, k_P)$$\n\n$3$. **钻取未知单元格**（若 $k_U = N - k_R - k_P > 0$ 则可用）：\n即时期望收益为 $\\theta (r_R - c_U) + (1-\\theta) (r_P - c_U) = \\theta r_R + (1-\\theta)r_P - c_U$。下一状态以概率 $\\theta$ 变为 $(k_R+1, k_P)$，或以概率 $1-\\theta$ 变为 $(k_R, k_P+1)$。此行动的价值为：\n$$v_{\\text{drill_U}} = (\\theta r_R + (1-\\theta)r_P - c_U) + \\beta \\left( \\theta V_{t-1}(k_R+1, k_P) + (1-\\theta) V_{t-1}(k_R, k_P+1) \\right)$$\n\n状态 $(t, k_R, k_P)$ 的价值函数是所有可用行动价值中的最大值。例如，如果 $k_R > 0$, $k_P > 0$ 且 $k_U > 0$，则：\n$$V_t(k_R, k_P) = \\max\\{v_{\\text{drill_R}}, v_{\\text{drill_P}}, v_{\\text{drill_U}}\\}$$\n\n算法通过反向归纳法进行：\n$1$. 初始化一个维度为 $(T+1) \\times (N+1) \\times (N+1)$ 的表 $V$，用于存储 $V_t(k_R, k_P)$ 的值。将所有 $V_0(k_R, k_P)$ 设置为 $0$。\n$2$. 从 $1$ 到 $T$ 遍历 $t$。\n$3$. 对于每个 $t$，遍历所有有效状态 $(k_R, k_P)$，其中 $k_R \\in [0, N]$ 且 $k_P \\in [0, N-k_R]$。\n$4$. 对于每个状态，使用 $V_{t-1}$ 表中预先计算的值来计算所有可用行动的价值。\n$5$. 将这些价值的最大值存储在 $V_t(k_R, k_P)$ 中。\n$6$. 最终结果是初始状态的价值：$V_T(0, 0)$。\n\n此过程保证了最优期望折现总收益的计算。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_case(N, T, theta, r_P, r_R, c_U, beta):\n    \"\"\"\n    Computes the optimal expected value for a single set of parameters\n    using dynamic programming.\n    \"\"\"\n    # V[t, k_R, k_P] stores the max expected value with t periods remaining,\n    # given k_R known rich cells and k_P known poor cells.\n    V = np.zeros((T + 1, N + 1, N + 1))\n\n    # Iterate forwards in time remaining, from t=1 to T.\n    # This corresponds to backward induction in calendar time.\n    for t in range(1, T + 1):\n        for k_R in range(N + 1):\n            for k_P in range(N - k_R + 1):\n                k_U = N - k_R - k_P\n                \n                # A list to store the value of each possible action\n                action_values = []\n                \n                # Action: Drill a known Rich Deposit (if available)\n                if k_R > 0:\n                    # Immediate payoff is r_R. State does not change.\n                    # Continuation value depends on V[t-1] for the same state.\n                    val_drill_R = r_R + beta * V[t - 1, k_R, k_P]\n                    action_values.append(val_drill_R)\n                \n                # Action: Drill a known Poor Deposit (if available)\n                if k_P > 0:\n                    # Immediate payoff is r_P. State does not change.\n                    val_drill_P = r_P + beta * V[t - 1, k_R, k_P]\n                    action_values.append(val_drill_P)\n                \n                # Action: Drill an Unknown cell (if available)\n                if k_U > 0:\n                    # Expected immediate payoff from exploration\n                    expected_immediate_payoff = theta * r_R + (1 - theta) * r_P - c_U\n                    \n                    # Expected continuation value, averaged over possible outcomes\n                    # Outcome 1: cell is Rich (prob theta), state becomes (k_R+1, k_P)\n                    # Outcome 2: cell is Poor (prob 1-theta), state becomes (k_R, k_P+1)\n                    expected_continuation_value = beta * (\n                        theta * V[t - 1, k_R + 1, k_P] +\n                        (1 - theta) * V[t - 1, k_R, k_P + 1]\n                    )\n                    val_drill_U = expected_immediate_payoff + expected_continuation_value\n                    action_values.append(val_drill_U)\n                \n                # The value of the current state is the maximum over possible actions.\n                # If N>0 and T>0, there is always at least one action.\n                if action_values:\n                    V[t, k_R, k_P] = max(action_values)\n\n    # The result is the value at the initial state: T periods remaining,\n    # 0 known Rich, 0 known Poor.\n    return V[T, 0, 0]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    # Each test case is a tuple: (N, T, theta, r_P, r_R, c_U, beta)\n    test_cases = [\n        (3, 3, 0.4, 1.0, 5.0, 0.5, 0.95),\n        (4, 5, 0.5, 0.5, 6.0, 0.0, 0.9),\n        (2, 2, 0.3, 1.0, 4.0, 2.5, 1.0),\n        (1, 3, 0.5, 0.0, 10.0, 1.0, 1.0),\n        (3, 3, 0.7, 1.0, 3.0, 0.2, 0.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_case(*case)\n        # Round the result to 6 decimal places as required.\n        results.append(f\"{result:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"}, {"introduction": "当环境模型未知时，我们如何学习最优行为？强化学习为此提供了强大的框架，允许智能体通过与环境的直接互动来学习。在这个实践中，你将扮演一名量化分析师，从零开始构建一个基于Q学习的交易机器人，该机器人通过离散的状态和动作空间在金融市场中进行交易决策 [@problem_id:2388619]。这个练习不仅能让你掌握强化学习的基本原理，还能让你了解如何将这些抽象概念应用于解决实际的金融问题。", "id": "2388619", "problem": "要求您实现一个完整、可运行的程序，该程序使用离散状态和动作空间，基于相对强弱指数（RSI）这一单一技术指标，来训练一个表格型强化学习（RL）智能体进行单只股票交易。该程序必须遵循马尔可夫决策过程（MDP）的形式化表示，并使用从Bellman最优性方程的第一性原理推导出的方法来优化行为，而非使用本说明中给出的简化公式。\n\n环境定义如下。智能体观察到的状态是由RSI区间和其当前头寸组成的一对。RSI区间是根据价格序列计算得出的，它是一个关于回看窗口 $w$ 的函数，使用最近 $w$ 个单周期价格变动的平均收益和平均亏损的标准定义进行计算：\n- 设价格序列为 $\\{P_t\\}_{t=0}^{T-1}$，其中 $T \\geq w + 2$，并定义单步价格变动 $\\Delta_t = P_t - P_{t-1}$，对于 $t \\in \\{1, \\dots, T-1\\}$。\n- 对于每个时间 $t$ 且 $t \\geq w$，将平均收益定义为在 $k \\in \\{t-w+1, \\dots, t\\}$ 范围内的 $\\max(\\Delta_k, 0)$ 的算术平均值，将平均亏损定义为在同一窗口内的 $\\max(-\\Delta_k, 0)$ 的算术平均值。\n- 定义相对强度为 $RS_t = \\dfrac{\\text{平均收益}}{\\text{平均亏损}}$，并采用以下约定以确保数学上的良定性：如果平均亏损为 $0$ 且平均收益为严格正数，则设 $RS_t = +\\infty$，这意味着相对强弱指数（RSI）$RSI_t = 100$；如果平均收益为 $0$ 且平均亏损为严格正数，则设 $RSI_t = 0$；如果两个平均值均为 $0$，则设 $RSI_t = 50$。否则，当两个平均值均为正数时，设 $RSI_t = 100 - \\dfrac{100}{1 + RS_t}$。\n- 使用阈值 $30$ 和 $70$ 将RSI离散化为不同区间，如下所示：当 $RSI_t \\leq 30$ 时为超卖，当 $30 < RSI_t < 70$ 时为中性，当 $RSI_t \\geq 70$ 时为超买。\n\n时间 $t$ 的完整离散状态是 $s_t = (r_t, p_t)$，其中 $r_t \\in \\{\\text{超卖}, \\text{中性}, \\text{超买}\\}$ 是RSI区间，而 $p_t \\in \\{0,1\\}$ 是当前头寸（$0$ 表示空仓，$1$ 表示持有一单位多头）。离散动作空间为 $A = \\{\\text{持有}, \\text{买入}, \\text{卖出}\\}$。\n\n周期内动态如下：\n- 在每个时间 $t$（其中 $t \\in \\{w, \\dots, T-2\\}$），智能体观察状态 $s_t$，选择一个动作 $a_t \\in A$，其头寸确定性地更新：\n  - 如果 $a_t = \\text{买入}$，则 $p_{t+} = 1$。\n  - 如果 $a_t = \\text{卖出}$，则 $p_{t+} = 0$。\n  - 如果 $a_t = \\text{持有}$，则 $p_{t+} = p_t$。\n- 仅当 $p_{t+} \\neq p_t$ 时（即仅当头寸实际发生变化时），才会产生交易成本 $c \\geq 0$。\n- 单步奖励为 $r_t = p_{t+}\\cdot (P_{t+1} - P_t) - \\mathbf{1}\\{p_{t+} \\neq p_t\\}\\, c$。\n- 下一个状态 $s_{t+1}$ 使用下一个RSI区间和更新后的头寸 $p_{t+}$。\n\n回合被定义为对时间索引 $t \\in \\{w, \\dots, T-2\\}$ 的一次从左到右的遍历，起始头寸为 $p_w = 0$。学习智能体必须实现从Bellman最优性原理和样本备份推导出的表格型 $Q$ 学习。您必须使用一个 $\\epsilon$-贪心行为策略，在探索时对三个动作进行均匀随机探索，在利用时通过选择任何能最大化当前动作价值估计的动作来进行贪心动作选择。通过选择索引最小的动作来确定性地打破平局。对所有随机化过程使用固定的伪随机种子 $12345$，以确保结果是可复现的。\n\n在训练指定数量的回合后，在用于训练的相同价格路径上评估贪心策略（在评估期间设置 $\\epsilon = 0$），从 $p_w = 0$ 开始，并对 $t \\in \\{w, \\dots, T-2\\}$ 使用相同的奖励构造。在评估结束时，如果最终头寸为 $1$，则通过卖出强制平仓，并减去一次交易成本 $c$；平仓时没有额外的价格变动。一个测试用例的评估指标是最终累计财富，它等于评估期间的奖励总和加上结束时可能的强制平仓成本。不涉及任何物理单位。\n\n您的程序必须实现上述内容，并在以下测试套件上运行。对于每个测试用例，您将获得价格路径 $\\{P_t\\}$、窗口长度 $w$、学习率 $\\alpha \\in (0,1]$、折扣因子 $\\gamma \\in [0,1]$、探索率 $\\epsilon \\in [0,1]$、训练回合数 $E \\in \\mathbb{N}$ 和交易成本 $c \\geq 0$。\n\n测试套件：\n- 案例 $1$ (有小幅回调的趋势，理想路径)：价格 $[100, 101, 102, 101, 103, 105, 104, 106, 108, 110]$, $w = 3$, $\\alpha = 0.3$, $\\gamma = 0.9$, $\\epsilon = 0.1$, $E = 200$, $c = 0.05$.\n- 案例 $2$ (横盘且有噪声，边界情况 $\\gamma = 0$)：价格 $[100.0, 100.5, 99.8, 100.2, 99.7, 100.1, 99.9, 100.0, 99.8, 100.2]$, $w = 3$, $\\alpha = 0.5$, $\\gamma = 0.0$, $\\epsilon = 0.2$, $E = 300$, $c = 0.05$.\n- 案例 $3$ (从开始即贪心，无探索)：价格 $[10, 11, 12, 13, 14, 15]$, $w = 2$, $\\alpha = 0.3$, $\\gamma = 0.9$, $\\epsilon = 0.0$, $E = 50$, $c = 0.1$.\n- 案例 $4$ (RSI的边界区间，包括 $0$ 和 $100$)：价格 $[50, 49, 48, 47, 46, 47, 48, 49, 50, 49]$, $w = 3$, $\\alpha = 0.4$, $\\gamma = 0.8$, $\\epsilon = 0.15$, $E = 250$, $c = 0.05$.\n\n实现要求：\n- 状态空间是离散的，由 $3$ 个RSI区间和 $2$ 个头寸状态的笛卡尔积构成，恰好有 $6$ 个状态。\n- 动作空间是离散的，恰好有 $3$ 个动作，按 $\\{\\text{持有}, \\text{买入}, \\text{卖出}\\}$ 排序，并索引为 $\\{0, 1, 2\\}$。\n- 将所有动作价值 $Q(s,a)$ 初始化为 $0$。\n- 对所有随机性使用同一个固定的伪随机种子 $12345$。\n- 评估为所述的 $4$ 个案例中的每一个生成最终财富。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含用方括号括起来的逗号分隔的结果列表，例如 $[x_1,x_2,x_3,x_4]$。\n- 每个 $x_i$ 必须是一个浮点数，四舍五入到恰好 $6$ 位小数。\n\n您的任务：完全按照规定实现该程序，使其按顺序运行这 $4$ 个案例，并打印上述所描述的单行内容。不需要用户输入，也不允许使用外部文件。实现应是自包含的且纯算法的。确保所有随机化都使用固定的种子 $12345$ 以确保结果是可复现的。", "solution": "该问题是一个将强化学习应用于金融交易场景的良定问题。它具有科学依据，内部一致，并且规定了足够详细的细节，以允许唯一的算法解。因此，我们将着手对其进行形式化分析和实现。\n\n问题是训练一个智能体，使用从 $Q$ 学习算法派生的策略进行单只股票交易。环境被建模为一个有限状态马尔可夫决策过程（MDP），鉴于状态和动作的离散性质，这是非常合适的。一个MDP由一个元组 $(S, A, P, R, \\gamma)$ 正式定义，其中：\n- $S$ 是状态集合。\n- $A$ 是动作集合。\n- $P$ 是状态转移概率函数，$P(s'|s, a) = \\text{Pr}(S_{t+1}=s' | S_t=s, A_t=a)$。\n- $R$ 是奖励函数，$R(s, a, s') = \\mathbb{E}[R_{t+1} | S_t=s, A_t=a, S_{t+1}=s']$。\n- $\\gamma \\in [0, 1]$ 是未来奖励的折扣因子。\n\n在这个具体问题中，这些组成部分的定义如下：\n\n状态空间 $S$ 是相对强弱指数（RSI）区间集合与可能头寸集合的笛卡尔积。设RSI区间为 $R_{reg} = \\{\\text{超卖}, \\text{中性}, \\text{超买}\\}$，头寸为 $P_{pos} = \\{0, 1\\}$。在时间 $t$ 的状态是 $s_t = (r_t, p_t) \\in R_{reg} \\times P_{pos}$。这导致 $|S| = 3 \\times 2 = 6$ 个不同的状态。\n\n动作空间为 $A = \\{\\text{持有}, \\text{买入}, \\text{卖出}\\}$，这是一个包含 $3$ 个离散动作的集合。\n\n状态转移由智能体的动作和外生价格序列 $\\{P_t\\}_{t=0}^{T-1}$ 决定。给定一个状态 $s_t = (r_t, p_t)$ 和一个动作 $a_t$，智能体的头寸确定性地转移到一个新头寸 $p_{t+}$。下一个RSI区间 $r_{t+1}$ 由直到时间 $t+1$ 的价格序列决定。因此，下一个状态是 $s_{t+1} = (r_{t+1}, p_{t+})$。由于价格序列是固定的，对于任何给定的动作，转移动态都是确定性的。\n\n在时间 $t$，从状态 $s_t=(r_t, p_t)$ 执行动作 $a_t$ 的奖励函数，由价格变动带来的盈利或亏损给出，并根据交易成本进行调整。动作后的头寸 $p_{t+}$ 决定了对价格变动 $P_{t+1} - P_t$ 的敞口。奖励 $r_t$ 是：\n$$r_t = p_{t+} \\cdot (P_{t+1} - P_t) - \\mathbf{1}\\{p_{t+} \\neq p_t\\} \\cdot c$$\n其中 $c$ 是交易成本，$\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n\n智能体的目标是学习一个最优策略 $\\pi^*: S \\to A$，以最大化从任何给定状态开始的期望累计折扣奖励。这是通过学习最优动作价值函数 $Q^*(s, a)$ 来实现的，它表示在状态 $s$ 采取动作 $a$ 并在此后以最优方式行动所能获得的最大期望折扣未来奖励。$Q^*$ 函数满足 Bellman 最优性方程：\n$$Q^*(s, a) = \\mathbb{E} \\left[ R_{t+1} + \\gamma \\max_{a' \\in A} Q^*(S_{t+1}, a') \\mid S_t=s, A_t=a \\right]$$\n对于本问题中的确定性转移（给定价格路径），期望算子是多余的。对于由动作 $a_t$ 导致从 $s_t$ 到 $s_{t+1}$ 的转移，并获得即时奖励 $r_t$，Bellman 方程简化为：\n$$Q^*(s_t, a_t) = r_t + \\gamma \\max_{a' \\in A} Q^*(s_{t+1}, a')$$\n$Q$ 学习是一种无模型的时序差分（TD）控制算法，它迭代地逼近 $Q^*(s, a)$。智能体无需先验地知道转移或奖励函数。它从样本转移 $(s_t, a_t, r_t, s_{t+1})$ 中学习。在每次这样的转移之后，动作价值表中的条目 $Q(s_t, a_t)$ 按以下规则更新：\n$$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left( r_t + \\gamma \\max_{a' \\in A} Q(s_{t+1}, a') - Q(s_t, a_t) \\right)$$\n这里，$\\alpha \\in (0, 1]$ 是学习率，它控制更新的步长。项 $r_t + \\gamma \\max_{a'} Q(s_{t+1}, a')$ 是 TD 目标，它作为 Bellman 方程右侧的基于样本的估计。\n\n为确保对状态-动作空间的充分探索，必须选择一个允许非贪心动作的行为策略。指定的 $\\epsilon$-贪心策略通过以概率 $\\epsilon$ 选择一个随机动作，并以概率 $1-\\epsilon$ 选择贪心动作（即最大化当前 $Q(s, \\cdot)$ 的动作）来实现这一点。\n\n实现将按以下步骤进行：\n$1$. 对于给定的价格序列，将根据指定的公式预先计算 RSI 值，包括对平均收益或亏损为零的约定。然后，这些连续的 RSI 值将被映射到三个离散的区间。\n$2$. 状态空间和动作空间将被映射为整数索引，以便高效地使用 NumPy 数组，该数组将表示 $Q$ 表，并初始化为全零。具体来说，一个状态 $(r, p)$，其中 $r \\in \\{0, 1, 2\\}$ 和 $p \\in \\{0, 1\\}$，将被映射到一个索引 $i_s = r \\cdot 2 + p$。动作将索引为 $0, 1, 2$。$Q$ 表的大小将为 $6 \\times 3$。\n$3$. 训练过程将迭代指定的 dla 回合数 $E$。每个回合包括对价格序列的有效时间步（从 $t=w$ 到 $t=T-2$）的单次遍历。\n$4$. 在每个回合内，智能体从零头寸开始。在每个时间步 $t$，它观察状态 $s_t$，通过 $\\epsilon$-贪心策略选择动作 $a_t$，观察奖励 $r_t$ 和下一状态 $s_{t+1}$，并使用 $Q$ 学习规则更新 $Q$ 表。\n$5$. 训练完成后，评估学习到的策略。评估遵循相同的时间轨迹，但 $\\epsilon=0$，这意味着智能体总是相对于学习到的 $Q$ 值采取贪心行动。在此次遍历中累计总财富。如果在评估期结束时智能体持有头寸，则应用最终的平仓成本。最终累计财富是性能指标。\n\n所有随机化都由一个固定的种子控制，以确保可复现性，这是科学验证所必需的。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs the Q-learning trading agent problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        {\n            \"prices\": [100.0, 101.0, 102.0, 101.0, 103.0, 105.0, 104.0, 106.0, 108.0, 110.0],\n            \"w\": 3, \"alpha\": 0.3, \"gamma\": 0.9, \"epsilon\": 0.1, \"E\": 200, \"c\": 0.05\n        },\n        {\n            \"prices\": [100.0, 100.5, 99.8, 100.2, 99.7, 100.1, 99.9, 100.0, 99.8, 100.2],\n            \"w\": 3, \"alpha\": 0.5, \"gamma\": 0.0, \"epsilon\": 0.2, \"E\": 300, \"c\": 0.05\n        },\n        {\n            \"prices\": [10.0, 11.0, 12.0, 13.0, 14.0, 15.0],\n            \"w\": 2, \"alpha\": 0.3, \"gamma\": 0.9, \"epsilon\": 0.0, \"E\": 50, \"c\": 0.1\n        },\n        {\n            \"prices\": [50.0, 49.0, 48.0, 47.0, 46.0, 47.0, 48.0, 49.0, 50.0, 49.0],\n            \"w\": 3, \"alpha\": 0.4, \"gamma\": 0.8, \"epsilon\": 0.15, \"E\": 250, \"c\": 0.05\n        }\n    ]\n\n    results = []\n    \n    # --- Mappings ---\n    # RSI Regimes: {0: Oversold, 1: Neutral, 2: Overbought}\n    # Positions: {0: Flat, 1: Long}\n    # States: (rsi_regime, position_state) -> rsi_regime * 2 + position_state\n    # Actions: {0: Hold, 1: Buy, 2: Sell}\n    NUM_STATES = 6\n    NUM_ACTIONS = 3\n    \n    for case in test_cases:\n        prices_np = np.array(case[\"prices\"], dtype=np.float64)\n        w = case[\"w\"]\n        alpha = case[\"alpha\"]\n        gamma = case[\"gamma\"]\n        epsilon = case[\"epsilon\"]\n        E = case[\"E\"]\n        c = case[\"c\"]\n        T = len(prices_np)\n\n        # --- 1. Pre-calculate RSI and Regimes ---\n        rsi_values = np.full(T, np.nan)\n        deltas = prices_np[1:] - prices_np[:-1]\n        \n        for t in range(w, T):\n            window_deltas = deltas[t-w:t]\n            gains = np.maximum(window_deltas, 0)\n            losses = np.maximum(-window_deltas, 0)\n            \n            avg_gain = np.mean(gains)\n            avg_loss = np.mean(losses)\n\n            if avg_loss == 0:\n                if avg_gain == 0:\n                    rsi_values[t] = 50.0\n                else:\n                    rsi_values[t] = 100.0\n            else:\n                rs = avg_gain / avg_loss\n                rsi_values[t] = 100.0 - (100.0 / (1.0 + rs))\n\n        rsi_regimes = np.full(T, -1, dtype=int)\n        rsi_regimes[rsi_values <= 30] = 0  # Oversold\n        rsi_regimes[(rsi_values > 30) & (rsi_values < 70)] = 1 # Neutral\n        rsi_regimes[rsi_values >= 70] = 2  # Overbought\n\n        # --- 2. Training Phase ---\n        q_table = np.zeros((NUM_STATES, NUM_ACTIONS))\n        rng = np.random.RandomState(12345)\n\n        for _ in range(E):\n            current_pos = 0\n            for t in range(w, T - 1):\n                # Current state\n                current_rsi_regime = rsi_regimes[t]\n                current_state_idx = current_rsi_regime * 2 + current_pos\n                \n                # Action selection (epsilon-greedy)\n                if rng.rand() < epsilon:\n                    action_idx = rng.randint(0, NUM_ACTIONS)\n                else:\n                    action_idx = np.argmax(q_table[current_state_idx, :])\n\n                # State transition and reward\n                prev_pos = current_pos\n                if action_idx == 0: # Hold\n                    next_pos = prev_pos\n                elif action_idx == 1: # Buy\n                    next_pos = 1\n                else: # Sell\n                    next_pos = 0\n                \n                transaction_cost = c if next_pos != prev_pos else 0.0\n                reward = next_pos * (prices_np[t+1] - prices_np[t]) - transaction_cost\n\n                # Next state\n                next_rsi_regime = rsi_regimes[t+1]\n                next_state_idx = next_rsi_regime * 2 + next_pos\n                \n                # Q-table update\n                old_q_value = q_table[current_state_idx, action_idx]\n                next_max_q = np.max(q_table[next_state_idx, :])\n                td_target = reward + gamma * next_max_q\n                new_q_value = old_q_value + alpha * (td_target - old_q_value)\n                q_table[current_state_idx, action_idx] = new_q_value\n\n                # Update position for next step in episode\n                current_pos = next_pos\n        \n        # --- 3. Evaluation Phase ---\n        total_wealth = 0.0\n        current_pos = 0\n        final_pos = 0\n\n        for t in range(w, T - 1):\n            # Current state\n            current_rsi_regime = rsi_regimes[t]\n            current_state_idx = current_rsi_regime * 2 + current_pos\n\n            # Action selection (greedy)\n            action_idx = np.argmax(q_table[current_state_idx, :])\n\n            # State transition and reward\n            prev_pos = current_pos\n            if action_idx == 0: # Hold\n                next_pos = prev_pos\n            elif action_idx == 1: # Buy\n                next_pos = 1\n            else: # Sell\n                next_pos = 0\n            \n            transaction_cost = c if next_pos != prev_pos else 0.0\n            reward = next_pos * (prices_np[t+1] - prices_np[t]) - transaction_cost\n            total_wealth += reward\n\n            # Update position for next step in evaluation\n            current_pos = next_pos\n        \n        final_pos = current_pos\n        \n        # Final liquidation\n        if final_pos == 1:\n            total_wealth -= c\n\n        results.append(round(total_wealth, 6))\n\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"}]}