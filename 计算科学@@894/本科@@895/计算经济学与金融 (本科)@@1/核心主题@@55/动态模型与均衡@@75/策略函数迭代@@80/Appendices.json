{"hands_on_practices": [{"introduction": "在深入研究无限期界问题之前，让我们先通过一个有限期界动态规划问题来热身。这个练习要求你为一个学生在课程中制定最优的“努力”策略，目标是在考虑努力成本的同时最大化最终成绩的期望效用。通过使用向后归纳法解决这个问题，你将掌握动态规划的核心思想：从最终结果出发，一步步反向推导出每个阶段的最优决策，这为理解无限期界问题中的迭代思想奠定了坚实的基础。[@problem_id:2419717]", "id": "2419717", "problem": "考虑一个有限期决策问题，其中一名学生在课程的每一周选择一个非负整数的努力程度。时间是离散的。状态由数对 $(t,E)$ 表示，其中 $t \\in \\{0,1,\\dots,T\\}$ 是当前周的索引，$E \\in \\{0,1,\\dots,E_{\\max}\\}$ 是截至第 $t$ 周的累积努力程度。时期长度 $T$ 是一个非负整数。在每个非终止周 $t<T$，从有限集 $\\mathcal{A}=\\{0,1,2\\}$ 中选择行动 $a$，产生单位时期成本 $c(a) = \\kappa a^2$，其中 $\\kappa>0$ 是一个已知常数。累积努力程度按照 $E'=\\min\\{E+a,E_{\\max}\\}$ 确定性地演变，其中 $E_{\\max}=2T$。终止周为 $t=T$；此周之后不能再付出更多努力。最终课程成绩在从第 $T$ 周到课后吸收期的过渡中实现，它是一个随机变量，其期望值通过严格递增函数 $p(E)=1-\\exp(-\\theta E)$ 依赖于总努力程度 $E$，其中 $\\theta>0$。学生是风险中性的，除了课程期间努力的负流动成本外，只看重期望终端回报 $R \\cdot p(E)$，其中 $R>0$ 是给定的。目标是从初始状态 $(0,0)$ 开始，最大化期望贴现回报总和，贴现因子为 $\\beta \\in (0,1)$。贴现因子按时期应用；终端期望回报在第 $T$ 周结束时收到，此后不再重复。\n\n形式上，对于 $t<T$，将价值函数 $V(t,E)$ 定义为\n$$\nV(t,E)=\\max_{a\\in\\{0,1,2\\}} \\left\\{ -\\kappa a^2 + \\beta \\, V\\Big(t+1, \\min\\{E+a,2T\\}\\Big) \\right\\},\n$$\n并将 $t=T$ 时的终端价值定义为\n$$\nV(T,E)= R \\left(1-\\exp(-\\theta E)\\right).\n$$\n设 $a^{\\star}(t,E)$ 为一个最优策略映射，它为每个非终止状态达到最大值。\n\n你的任务是编写一个完整的程序，该程序对以下测试套件中的每一组参数，计算最优初始努力 $a^{\\star}(0,0)$，并以整数形式返回结果。\n\n如果没有决策期（即 $T=0$），则初始状态下没有可用的努力选择，按照惯例，你必须为 $a^{\\star}(0,0)$ 输出 $0$。\n\n测试套件包含以下参数集 $(T,\\beta,\\kappa,\\theta,R)$：\n- 案例 A: $(3,\\,0.95,\\,0.2,\\,0.5,\\,10)$\n- 案例 B: $(3,\\,0.95,\\,2.0,\\,0.5,\\,10)$\n- 案例 C: $(3,\\,0.95,\\,0.2,\\,0.1,\\,5)$\n- 案例 D: $(1,\\,0.95,\\,0.5,\\,1.0,\\,20)$\n- 案例 E: $(0,\\,0.95,\\,0.5,\\,1.0,\\,20)$\n\n你的程序应生成单行输出，其中包含五个最优初始努力的结果，以逗号分隔的列表形式呈现，并用方括号括起来，顺序为 A、B、C、D、E（例如，“[x,y,z,w,v]”）。所有输出都必须是整数。", "solution": "该问题陈述已经过验证，并被确定为计算经济学领域中一个适定且有科学依据的问题。这是一个有限期离散时间动态规划问题，其解可以通过后向归纳法原理来构造。\n\n问题的核心是求解价值函数 $V(t,E)$ 的 Bellman 函数方程，该函数代表了从状态 $(t,E)$ 出发的最大期望贴现未来回报。状态由当前时期 $t \\in \\{0, 1, \\dots, T\\}$ 和累积努力 $E \\in \\{0, 1, \\dots, E_{\\max}\\}$ 组成，其中 $E_{\\max} = 2T$。\n\n对于任何非终止时期 $t < T$，价值函数由 Bellman 方程定义：\n$$\nV(t,E)=\\max_{a\\in\\{0,1,2\\}} \\left\\{ -\\kappa a^2 + \\beta \\, V\\Big(t+1, \\min\\{E+a,2T\\}\\Big) \\right\\}\n$$\n该方程表明，状态 $(t,E)$ 的最优值是通过从集合 $\\mathcal{A}=\\{0,1,2\\}$ 中选择一个行动 $a$ 来找到的，该行动能够最大化即时回报（即负成本 $-\\kappa a^2$）与下一时期结果状态的贴现值之和。状态从 $(t,E)$ 确定性地转移到 $(t+1, \\min\\{E+a, 2T\\})$。\n\n在终止时间 $t=T$，不能再采取任何行动。其价值由期望终端回报给出，该回报取决于最终的累积努力 $E$：\n$$\nV(T,E)= R \\left(1-\\exp(-\\theta E)\\right)\n$$\n这个终端条件为后向归纳算法提供了一个出发点。最优策略 $a^\\star(t,E)$ 是在 Bellman 方程中为每个状态 $(t,E)$ 达到最大值的行动 $a$。\n\n求解算法按以下步骤进行：\n\n1.  **状态空间离散化**：我们建立一个网格来表示所有可能的状态 $(t,E)$。该网格可以用一个二维数组表示，按时间 $t$（从 $0$ 到 $T$）和努力 $E$（从 $0$ 到 $2T$）进行索引。我们将需要两个这样的网格：一个用于存储价值函数 $V(t,E)$，另一个用于存储最优策略 $a^\\star(t,E)$。\n\n2.  **在 $t=T$ 处初始化**：后向归纳过程从最终时期 $t=T$ 开始。我们使用给定的终端价值公式，为所有可能的努力水平 $E \\in \\{0, 1, \\dots, 2T\\}$ 计算价值函数 $V(T,E)$。这些值被存储在我们网格中对应于 $t=T$ 的行中。\n\n3.  **后向迭代**：我们从 $t=T-1$ 向下迭代至 $t=0$。对于每个时间步 $t$ 和状态空间中的每个可能努力水平 $E$：\n    a. 我们评估每个可能行动 $a \\in \\{0, 1, 2\\}$ 的潜在结果。对于给定的行动 $a$，其价值计算为 $q(a) = -\\kappa a^2 + \\beta V(t+1, \\min\\{E+a, 2T\\})$。请注意，$V(t+1, \\cdot)$ 的值已从迭代的前一步中得知。\n    b. 我们确定使该值最大化的行动：$a^\\star(t,E) = \\arg\\max_{a \\in \\{0,1,2\\}} q(a)$。\n    c. 我们将这个最优行动 $a^\\star(t,E)$ 存储在我们的策略网格中。\n    d. 我们将相应的最大值 $V(t,E) = q(a^\\star(t,E))$ 存储在我们的价值函数网格中。\n    如果出现多个行动产生相同最大值的平局情况，我们采纳选择最小行动的惯例。\n\n4.  **解的提取**：当循环迭代至 $t=0$ 完成后，策略网格包含了每个可能的非终止状态的最优行动。我们具体关注的量是最优初始努力 $a^\\star(0,0)$，它可以在策略网格的相应条目中找到。\n\n5.  **特殊情况 $T=0$**：如果时期长度 $T$ 为 $0$，则没有决策期。初始状态也就是终止状态。无法付出任何努力。按照惯例，最优行动 $a^\\star(0,0)$ 定义为 $0$。该算法明确处理了这种情况。\n\n这个系统性过程保证能为给定的有限期问题找到最优策略。对于测试套件中提供的每组参数，实现过程将精确地遵循这一逻辑。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases and print the results.\n    \"\"\"\n    \n    def solve_case(T, beta, kappa, theta, R):\n        \"\"\"\n        Solves the dynamic programming problem for a single set of parameters.\n\n        Args:\n            T (int): The horizon length.\n            beta (float): The discount factor.\n            kappa (float): The cost parameter for effort.\n            theta (float): The effectiveness parameter for effort.\n            R (float): The maximum reward constant.\n\n        Returns:\n            int: The optimal initial effort a_star(0, 0).\n        \"\"\"\n        # Per the problem statement's convention for T=0.\n        if T == 0:\n            return 0\n\n        # Define the state space for effort. E_max is 2*T.\n        E_max = 2 * T\n        E_grid = np.arange(E_max + 1)\n\n        # Initialize grids to store the value function and the optimal policy.\n        # V[t, E] stores the value function V(t, E).\n        # policy[t, E] stores the optimal action a_star(t, E).\n        V = np.zeros((T + 1, E_max + 1))\n        policy = np.zeros((T + 1, E_max + 1), dtype=int)\n\n        # Step 1: Compute the terminal value function at t=T.\n        V[T, :] = R * (1 - np.exp(-theta * E_grid))\n\n        # Define the action space and the corresponding immediate costs.\n        actions = np.array([0, 1, 2])\n        costs = -kappa * actions**2\n\n        # Steps 2 & 3: Perform backward induction from t=T-1 down to 0.\n        for t in range(T - 1, -1, -1):\n            for E in E_grid:\n                # Calculate the value associated with each possible action.\n                q_values = np.zeros(len(actions))\n                for i, a in enumerate(actions):\n                    # Determine the next state for accumulated effort.\n                    E_next = min(E + a, E_max)\n                    \n                    # Bellman equation: current cost + discounted future value.\n                    q_values[i] = costs[i] + beta * V[t + 1, E_next]\n                \n                # Find the optimal action and the corresponding value.\n                # np.argmax returns the index of the first maximum value,\n                # which corresponds to the smallest action in case of a tie.\n                best_action_index = np.argmax(q_values)\n                policy[t, E] = actions[best_action_index]\n                V[t, E] = q_values[best_action_index]\n\n        # Step 4: The result is the optimal action at the initial state (t=0, E=0).\n        return policy[0, 0]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (3, 0.95, 0.2, 0.5, 10),  # Case A\n        (3, 0.95, 2.0, 0.5, 10),  # Case B\n        (3, 0.95, 0.2, 0.1, 5),   # Case C\n        (1, 0.95, 0.5, 1.0, 20),  # Case D\n        (0, 0.95, 0.5, 1.0, 20),  # Case E\n    ]\n    \n    results = []\n    for case_params in test_cases:\n        result = solve_case(*case_params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "掌握了有限期界的逻辑后，我们将挑战一个更复杂的无限期界问题。这个练习模拟了一个经典的设备更换决策场景：你需要决定何时更换一台状况会随机变差、且更换成本本身也是随机变化的机器。由于没有明确的“终点”，我们无法再使用简单的向后归纳法，而必须采用策略函数迭代或价值函数迭代这类方法，通过不断迭代更新来逼近最优的静态策略。[@problem_id:2419699]", "id": "2419699", "problem": "考虑一个关于单台生产性机器的无穷期、离散时间决策问题。时间由 $t \\in \\{0,1,2,\\dots\\}$ 索引。在每个时期 $t$ 的开始，决策者观察到机器的状况状态 $s_t \\in \\{1,2,\\dots,S\\}$ 和一个重置成本状态 $i_t \\in \\{1,2,\\dots,M\\}$。状态 $s_t$ 索引了从最佳 ($s_t=1$) 到最差 ($s_t=S$) 的状况。重置成本状态 $i_t$ 决定了该时期的即时重置成本。\n\n行动集：在每个时期，决策者选择一个行动 $a_t \\in \\{0,1\\}$，其中 $a_t=1$ 表示“立即重置”，$a_t=0$ 表示“本期继续运行”。\n\n成本与动态：\n- 若 $a_t=1$，则支付即期成本 $C_{i_t}$，机器在下一时期被重置为最佳状况，使得 $s_{t+1}=1$，且重置成本状态根据一个马尔可夫转移矩阵 $\\Pi \\in \\mathbb{R}^{M \\times M}$ 进行演化，其元素为 $\\pi_{i j} = \\mathbb{P}(i_{t+1}=j \\mid i_t=i)$。\n- 若 $a_t=0$，则支付即期成本 $m_{s_t}$。机器有 $q_{s_t}$ 的概率在本期内发生故障；在此情况下，会产生一笔额外的即期故障惩罚 $f$，并且需要以当前时期的重置成本 $C_{i_t}$ 立即进行重置，以便在下一时期恢复功能。有 $1 - q_{s_t}$ 的概率不发生故障。下一时期的状况遵循：\n  - 若发生故障，$s_{t+1}=1$。\n  - 若未发生故障，$s_{t+1}=\\min\\{s_t+1, S\\}$。\n  重置成本状态 $i_t$ 始终根据 $\\Pi$ 演化，与行动 $a_t$ 和故障的发生与否无关。\n\n目标：使用贴现因子 $\\beta \\in (0,1)$，最小化预期的贴现成本总和：\n$$\n\\min_{\\{a_t\\}_{t \\ge 0}} \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\beta^t \\cdot \\text{cost}(s_t,i_t,a_t)\\right],\n$$\n其中 $\\text{cost}(s_t,i_t,1) = C_{i_t}$ 且 $\\text{cost}(s_t,i_t,0) = m_{s_t} + q_{s_t}\\,(C_{i_t} + f)$。\n\n设价值函数为 $V(s,i)$，表示从一个时期开始时处于状态 $(s,i)$ 的最小预期贴现成本。最优稳态策略是一个映射 $\\pi^{\\star}(s,i) \\in \\{0,1\\}$，它在每个状态 $(s,i)$ 下最小化贝尔曼目标。\n\n报告结果的状态排序：对于一个给定的包含 $S$ 和 $M$ 的测试用例，将状态的有序列表定义为\n$$\n[(1,1), (1,2), \\dots, (1,M), (2,1), (2,2), \\dots, (2,M), \\dots, (S,1), \\dots, (S,M)].\n$$\n您的程序必须按此顺序计算所有状态的最优稳态策略 $\\pi^{\\star}(s,i)$，并将其报告为一个整数列表（其中 $1$ 表示“立即重置”，$0$ 表示“保留”）。\n\n参数集测试套件：\n- 测试用例1：\n  - $S = 4$， $M = 2$。\n  - 贴现因子 $\\beta = 0.95$。\n  - 维护成本 $m = [1.0, 2.0, 3.0, 4.0]$，对应于 $s \\in \\{1,2,3,4\\}$。\n  - 故障概率 $q = [0.05, 0.10, 0.20, 0.40]$，对应于 $s \\in \\{1,2,3,4\\}$。\n  - 故障惩罚 $f = 5.0$。\n  - 重置成本 $C = [8.0, 14.0]$，对应于 $i \\in \\{1,2\\}$。\n  - 重置成本转移矩阵\n    $$\n    \\Pi = \\begin{bmatrix}\n    0.9 & 0.1 \\\\\n    0.2 & 0.8\n    \\end{bmatrix}.\n    $$\n- 测试用例2：\n  - $S = 4$， $M = 2$。\n  - 贴现因子 $\\beta = 0.95$。\n  - 维护成本 $m = [0.0, 1.0, 2.0, 3.0]$，对于 $s \\in \\{1,2,3,4\\}$。\n  - 故障概率 $q = [0.20, 0.40, 0.60, 0.80]$，对于 $s \\in \\{1,2,3,4\\}$。\n  - 故障惩罚 $f = 20.0$。\n  - 重置成本 $C = [8.0, 12.0]$，对于 $i \\in \\{1,2\\}$。\n  - 重置成本转移矩阵\n    $$\n    \\Pi = \\begin{bmatrix}\n    0.85 & 0.15 \\\\\n    0.25 & 0.75\n    \\end{bmatrix}.\n    $$\n- 测试用例3：\n  - $S = 4$， $M = 2$。\n  - 贴现因子 $\\beta = 0.90$。\n  - 维护成本 $m = [0.5, 0.6, 0.7, 0.8]$，对于 $s \\in \\{1,2,3,4\\}$。\n  - 故障概率 $q = [0.0, 0.0, 0.0, 0.0]$，对于 $s \\in \\{1,2,3,4\\}$。\n  - 故障惩罚 $f = 0.0$。\n  - 重置成本 $C = [10.0, 20.0]$，对于 $i \\in \\{1,2\\}$。\n  - 重置成本转移矩阵\n    $$\n    \\Pi = \\begin{bmatrix}\n    0.95 & 0.05 \\\\\n    0.05 & 0.95\n    \\end{bmatrix}.\n    $$\n\n要求输出：对于每个测试用例，输出一个长度为 $S \\cdot M$ 的列表，其中包含按指定状态顺序排列的最优二元决策 $\\pi^{\\star}(s,i)$。将所有测试用例的结果汇总到单行中，作为一个列表的列表，不含空格，例如 $[[\\dots],[\\dots],[\\dots]]$。\n\n您的程序应生成单行输出，其中包含按上述确切格式表示的结果，即由方括号括起来的逗号分隔列表。", "solution": "所述问题是有效的。它构成了一个标准的、良定的无穷期、离散时间动态规划问题，通常称为马尔可夫决策过程 (Markov Decision Process, MDP)。所有参数、状态转移动态和成本结构都已明确定义，从而可以计算出唯一的最优稳态策略。该问题基于最优控制和计算经济学的成熟理论。\n\n目标是找到一个最优稳态策略 $\\pi^{\\star}(s,i)$，以最小化无穷期的总预期贴现成本。系统在任意时间 $t$ 的状态由配对 $(s_t, i_t)$ 给出，其中 $s_t \\in \\{1, 2, \\dots, S\\}$ 是机器状况，$i_t \\in \\{1, 2, \\dots, M\\}$ 是重置成本状态。决策者选择一个行动 $a_t \\in \\{0, 1\\}$，其中 $a_t=0$ 是“保留”，$a_t=1$ 是“重置”。\n\n该问题的解由价值函数 $V(s,i)$ 的贝尔曼方程刻画，该函数表示从状态 $(s,i)$ 开始的最小预期贴现成本。贝尔曼方程为：\n$$\nV(s,i) = \\min \\{ V_0(s,i), V_1(s,i) \\}\n$$\n其中 $V_a(s,i)$ 是在状态 $(s,i)$ 下采取行动 $a$ 的相关价值。这些通常被称为行动价值函数。\n\n让我们将预期的下一期价值定义为，它取决于当前成本状态 $i$ 和下一期机器状况 $s'$：\n$$\nE(s', i) = \\sum_{j=1}^{M} \\pi_{ij} V(s', j)\n$$\n其中 $\\pi_{ij}$ 是从成本状态 $i$ 转移到成本状态 $j$ 的概率，由转移矩阵 $\\Pi$ 给出。\n\n“重置”($a=1$) 的行动价值函数由下式给出：\n$$\nV_1(s,i) = C_i + \\beta E(1, i)\n$$\n这是因为重置机器会产生即期成本 $C_i$，并在下一时期将机器状态转移到最佳状况 $s'=1$。\n\n“保留”($a=0$) 的行动价值函数更为复杂。预期的即期成本是 $m_s + q_s(f+C_i)$。下一时期的机器状态取决于是否发生故障。故障以概率 $q_s$ 发生，并将机器重置为状态 $s'=1$。如果不发生故障（概率为 $1-q_s$），机器状况恶化至状态 $s'=\\min\\{s+1, S\\}$。因此，“保留”的价值是：\n$$\nV_0(s,i) = \\left( m_s + q_s(f+C_i) \\right) + \\beta \\left[ q_s E(1, i) + (1-q_s) E(\\min\\{s+1, S\\}, i) \\right]\n$$\n该问题通过找到贝尔曼算子的不动点来解决，这可以通过价值迭代法实现。该迭代算法的步骤如下：\n1. 初始化所有状态的价值函数，例如，在迭代 $k=0$ 时，对所有 $(s,i)$ 设 $V_k(s,i) = 0$。\n2. 对于随后的每次迭代 $k=1, 2, \\dots$，使用前一次迭代的价值 $V_{k-1}$ 更新所有状态 $(s,i)$ 的价值函数：\n$$\nV_k(s,i) = \\min \\left\\{ \\left( m_s + q_s(f+C_i) \\right) + \\beta \\left[ q_s E_{k-1}(1, i) + (1-q_s) E_{k-1}(\\min\\{s+1, S\\}, i) \\right], \\quad C_i + \\beta E_{k-1}(1, i) \\right\\}\n$$\n其中 $E_{k-1}(s',i) = \\sum_{j=1}^{M} \\pi_{ij} V_{k-1}(s', j)$。\n3. 迭代持续进行，直到价值函数收敛，即所有状态的最大变化小于预定义的容差 $\\epsilon$：$\\max_{s,i} |V_k(s,i) - V_{k-1}(s,i)| < \\epsilon$。由于贴现因子 $\\beta \\in (0,1)$，贝尔曼算子是一个压缩映射，这保证了它会收敛到一个唯一的不动点 $V^\\star$。\n\n一旦获得收敛的价值函数 $V^\\star$，最优稳态策略 $\\pi^\\star(s,i)$ 就可以通过为每个状态选择最小化单期贝尔曼前瞻的行动来确定：\n$$\n\\pi^\\star(s,i) = \\arg\\min_{a \\in \\{0,1\\}} V_a(s,i)\n$$\n具体来说，如果重置的价值小于保留的价值，则策略为重置。遵循在价值相等时选择保留的惯例：\n$$\n\\pi^\\star(s,i) = \\begin{cases} 0 & \\text{if } V_0(s,i) \\le V_1(s,i) \\\\ 1 & \\text{if } V_0(s,i) > V_1(s,i) \\end{cases}\n$$\n该实现将使用 NumPy 的向量化操作以提高计算效率。价值函数 $V(s,i)$ 将存储在一个 $S \\times M$ 矩阵中。预期的未来价值将通过与转移矩阵 $\\Pi$ 的转置进行矩阵乘法来计算。最优策略将从最终的价值函数中导出，并遵循指定的状态排序，格式化为一个扁平化列表。", "answer": "完整的、可运行的 Python 3 代码如下。导入的库必须符合指定的执行环境。\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_optimal_policy(S, M, beta, m, q, f, C, Pi):\n    \"\"\"\n    Solves for the optimal replacement policy using value iteration.\n\n    Args:\n        S (int): Number of machine condition states.\n        M (int): Number of replacement cost states.\n        beta (float): Discount factor.\n        m (np.ndarray): Vector of maintenance costs for each condition state.\n        q (np.ndarray): Vector of breakdown probabilities for each condition state.\n        f (float): Failure penalty.\n        C (np.ndarray): Vector of replacement costs for each cost state.\n        Pi (np.ndarray): Transition matrix for replacement cost states.\n\n    Returns:\n        list: A flattened list of optimal actions (0 for keep, 1 for replace)\n              ordered by state (s, i) in row-major order.\n    \"\"\"\n    # Initialize value function\n    V = np.zeros((S, M))\n\n    # Value iteration parameters\n    tolerance = 1e-9\n    max_iterations = 10000\n\n    for _ in range(max_iterations):\n        V_old = V.copy()\n\n        # Calculate expected future values for all states (s,i)\n        # ExpectedV[s, i] = sum_{j} Pi[i, j] * V[s, j]\n        ExpectedV = V @ Pi.T\n\n        # --- Value of replacing (a=1) ---\n        # Immediate cost C_i + discounted future value from state s=1\n        # The expected future value for s=1 is the first row of ExpectedV.\n        EV_s1_vec = ExpectedV[0, :]\n        # val_replace is a 1xM row vector, will be broadcast to SxM for comparison\n        val_replace = C + beta * EV_s1_vec\n\n        # --- Value of keeping (a=0) ---\n        # Reshape parameter vectors for broadcasting\n        m_col = m.reshape(-1, 1)  # S x 1\n        q_col = q.reshape(-1, 1)  # S x 1\n        C_row = C.reshape(1, -1)  # 1 x M\n\n        # Expected immediate cost: m_s + q_s * (f + C_i)\n        immediate_cost_keep = m_col + q_col * (f + C_row)\n\n        # Expected future cost: beta * [q_s * E[V(1,j)] + (1-q_s) * E[V(min{s+1,S},j)]]\n        s_indices = np.arange(S)\n        s_next_keep_indices = np.minimum(s_indices + 1, S - 1)\n        EV_s_next_keep_matrix = ExpectedV[s_next_keep_indices, :]\n        \n        future_cost_keep = beta * (\n            q_col * EV_s1_vec.reshape(1, -1) + (1 - q_col) * EV_s_next_keep_matrix\n        )\n\n        val_keep = immediate_cost_keep + future_cost_keep\n\n        # Bellman update\n        V = np.minimum(val_keep, val_replace)\n\n        # Check for convergence\n        if np.max(np.abs(V - V_old)) < tolerance:\n            break\n    \n    # After convergence, determine the optimal policy using the final val_keep and val_replace\n    # Policy is 1 (replace) if val_keep > val_replace, 0 (keep) otherwise.\n    policy = (val_keep > val_replace).astype(int)\n\n    # Flatten the policy matrix to a list in row-major order\n    return policy.flatten().tolist()\n\ndef solve():\n    \"\"\"\n    Defines the test cases, solves for the optimal policy for each,\n    and prints the results in the required format.\n    \"\"\"\n    test_cases = [\n        {\n            \"S\": 4, \"M\": 2, \"beta\": 0.95,\n            \"m\": np.array([1.0, 2.0, 3.0, 4.0]),\n            \"q\": np.array([0.05, 0.10, 0.20, 0.40]),\n            \"f\": 5.0,\n            \"C\": np.array([8.0, 14.0]),\n            \"Pi\": np.array([[0.9, 0.1], [0.2, 0.8]]),\n        },\n        {\n            \"S\": 4, \"M\": 2, \"beta\": 0.95,\n            \"m\": np.array([0.0, 1.0, 2.0, 3.0]),\n            \"q\": np.array([0.20, 0.40, 0.60, 0.80]),\n            \"f\": 20.0,\n            \"C\": np.array([8.0, 12.0]),\n            \"Pi\": np.array([[0.85, 0.15], [0.25, 0.75]]),\n        },\n        {\n            \"S\": 4, \"M\": 2, \"beta\": 0.90,\n            \"m\": np.array([0.5, 0.6, 0.7, 0.8]),\n            \"q\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"f\": 0.0,\n            \"C\": np.array([10.0, 20.0]),\n            \"Pi\": np.array([[0.95, 0.05], [0.05, 0.95]]),\n        },\n    ]\n\n    results = []\n    for params in test_cases:\n        policy = compute_optimal_policy(\n            params[\"S\"], params[\"M\"], params[\"beta\"], params[\"m\"],\n            params[\"q\"], params[\"f\"], params[\"C\"], params[\"Pi\"]\n        )\n        results.append(policy)\n\n    # Format the final output string without spaces\n    result_strings = [f\"[{','.join(map(str, res))}]\" for res in results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"}, {"introduction": "最后，我们将策略函数迭代应用于宏观经济学的基石模型之一——新古典增长模型。在这个练习中，你将为一个代表性代理人求解最优的资本积累路径，其中一个关键的复杂性来自于一个偶尔会“绑”住决策的资本下限约束。通过实现策略函数迭代算法，你不仅能找到最优的储蓄策略，还将学习如何通过计算欧拉方程残差等诊断工具来评估解的准确性，这是经济模型数值求解中的一项核心技能。[@problem_id:2419733]", "id": "2419733", "problem": "考虑一个无限期、确定性的动态规划问题，其中一个代表性代理人选择下一期的资本以最大化其生命周期效用。该代理人的消费效用由恒定相对风险规避 (CRRA) 效用函数 $u(c)$ 给出，定义为当 $\\sigma \\neq 1$ 时，$u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$；当 $\\sigma = 1$ 时，$u(c) = \\log(c)$。资源约束为 $c + k' = A k^{\\alpha} + (1-\\delta) k$，其中 $k$ 是当期资本，$k'$ 是下一期资本，$A$ 是全要素生产率，$\\alpha$ 是资本份额，$\\delta$ 是折旧率。代理人以因子 $\\beta \\in (0,1)$ 对未来进行贴现。下一期资本受到一个偶尔紧约束的不等式约束 $k' \\ge \\bar{k}$。可行性还要求非负消费，这意味着 $k' \\le A k^{\\alpha} + (1-\\delta) k$。资本的状态空间被限制在一个网格 $\\mathcal{K} = \\{k_1, k_2, \\dots, k_N\\}$ 上，其中 $k_1 = \\bar{k}$ 且 $k_N = k_{\\max}$，该网格由 $N$ 个均匀间隔的点组成。\n\n对于下方的每一组参数，计算平稳最优策略函数 $g: \\mathcal{K} \\to \\mathcal{K}$，该函数将每个网格点 $k \\in \\mathcal{K}$ 映射到同一网格上的一个最优选择 $k' = g(k)$，并受约束于 $k' \\in [\\bar{k}, A k^{\\alpha} + (1-\\delta) k]$，同时定义以下诊断指标：\n\n- 诊断指标1：下界是紧约束的网格点的比例，计算为满足 $g(k) \\le \\bar{k} + \\tau$ 的 $k \\in \\mathcal{K}$ 所占的比例，其中紧约束容差为 $\\tau = 10^{-10}$。\n\n- 诊断指标2：中位数网格索引处的策略值，即 $g(k_m)$，其中 $m = \\lfloor (N-1)/2 \\rfloor + 1$ 是 $\\{1,2,\\dots,N\\}$ 中的中位数索引。\n\n- 诊断指标3：在计算出的策略下，整个网格上欧拉方程残差的最大绝对值，对每个 $k \\in \\mathcal{K}$ 定义为\n$$\nr(k) \\equiv \\left| u'(c(k)) - \\beta \\, u'(c(g(k))) \\left(\\alpha A \\, g(k)^{\\alpha-1} + 1 - \\delta \\right) \\right|,\n$$\n其中 $c(k) = A k^{\\alpha} + (1-\\delta)k - g(k)$ 且 $u'(c)$ 是消费的边际效用，当 $\\sigma \\neq 1$ 时由 $u'(c) = c^{-\\sigma}$ 给出，当 $\\sigma = 1$ 时由 $u'(c) = \\frac{1}{c}$ 给出。需报告的诊断指标3是 $\\max_{k \\in \\mathcal{K}} r(k)$。\n\n资本网格定义为在 $\\bar{k}$ 和 $k_{\\max}$ 之间（含两端）均匀间隔的 $N$ 个点组成的集合 $\\mathcal{K}$。在所有计算中，对每个 $k \\in \\mathcal{K}$ 使用可行性条件 $k' \\in [\\bar{k}, A k^{\\alpha} + (1-\\delta) k]$，并且仅在 $c > 0$ 时评估 $u(c)$。\n\n测试组：\n- 测试1： $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.33, 0.96, 0.08, 1.0, 2.0, 0.01, 200, 5.0)$。\n- 测试2： $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.33, 0.96, 0.08, 1.0, 2.0, 0.50, 200, 5.0)$。\n- 测试3： $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.35, 0.94, 0.10, 0.90, 1.0, 1.50, 200, 5.0)$。\n\n您的程序必须为每个测试在指定的网格上计算最优平稳策略函数 $g$，并按顺序为每个测试返回上述三个诊断指标（作为实数）。最终输出格式必须将所有测试结果聚合到一行中，形式为包含在方括号内的逗号分隔列表，其中包含测试1的3个诊断指标，接着是测试2的3个诊断指标，最后是测试3的3个诊断指标。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含一个位于方括号内的逗号分隔列表作为结果（例如，“[result1,result2,result3,...]”）。", "solution": "用户提供了一个来自计算宏观经济学的明确定义的问题。我将首先确认其有效性，然后提供一个完整的解决方案。\n\n### 步骤1：提取已知条件\n\n- **效用函数**：当 $\\sigma \\neq 1$ 时为 $u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$，当 $\\sigma = 1$ 时为 $u(c) = \\log(c)$。\n- **资源约束**：$c + k' = A k^{\\alpha} + (1-\\delta) k$。\n- **参数**：全要素生产率 $A$、资本份额 $\\alpha$、折旧率 $\\delta$、CRRA系数 $\\sigma$以及贴现因子 $\\beta \\in (0,1)$。\n- **状态变量和选择变量**：当期资本 $k$ (状态)，下一期资本 $k'$ (选择)。\n- **约束**：$k' \\ge \\bar{k}$ (下界) 以及 $k' \\le A k^{\\alpha} + (1-\\delta) k$ (可行性)。要求 $c>0$。\n- **状态空间**：一个离散网格 $\\mathcal{K} = \\{k_1, k_2, \\dots, k_N\\}$，它是一个从 $k_1 = \\bar{k}$ 到 $k_N = k_{\\max}$ 的包含 $N$ 个均匀间隔点的集合。\n- **策略函数**：一个平稳映射 $g: \\mathcal{K} \\to \\mathcal{K}$，使得 $k' = g(k)$。\n- **诊断指标1**：在网格点 $k \\in \\mathcal{K}$ 中，满足 $g(k) \\le \\bar{k} + \\tau$ 的点的比例，容差 $\\tau = 10^{-10}$。\n- **诊断指标2**：在中位数网格索引 $m = \\lfloor (N-1)/2 \\rfloor + 1$ 处的策略值 $g(k_m)$。\n- **诊断指标3**：欧拉方程残差的最大绝对值，$\\max_{k \\in \\mathcal{K}} r(k)$，其中\n  $r(k) \\equiv \\left| u'(c(k)) - \\beta \\, u'(c(g(k))) \\left(\\alpha A \\, g(k)^{\\alpha-1} + 1 - \\delta \\right) \\right|$，\n  $c(k) = A k^{\\alpha} + (1-\\delta)k - g(k)$ 且 $u'(c)$ 为边际效用。\n- **测试用例**：\n  - 测试1： $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.33, 0.96, 0.08, 1.0, 2.0, 0.01, 200, 5.0)$\n  - 测试2： $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.33, 0.96, 0.08, 1.0, 2.0, 0.50, 200, 5.0)$\n  - 测试3： $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.35, 0.94, 0.10, 0.90, 1.0, 1.50, 200, 5.0)$\n\n### 步骤2：使用提取的已知条件进行验证\n\n- **科学依据**：该问题描述了在离散时间、确定性背景下的标准新古典增长模型（也称为 Ramsey-Cass-Koopmans 模型）。这是宏观经济学中的一个基础模型。所有组成部分——CRRA效用、柯布-道格拉斯生产技术、资本积累动态——都是标准的且经过严格建立的。该问题在科学上是合理的。\n- **良态性**：该问题旨在为一个贴现动态规划问题找到平稳最优策略函数。状态空间 $\\mathcal{K}$ 是一个紧集，对于任何可行策略，效用函数都是有界的。根据压缩映射定理，存在唯一的平稳最优策略函数。该问题是良态的。\n- **客观性**：问题陈述使用了精确的数学定义和客观的语言。没有主观或模糊的术语。\n- **结论**：该问题未违反任何指定的无效标准。它是一个完整、一致且可解的科学问题。\n\n### 步骤3：裁定与行动\n\n问题陈述是**有效**的。将提供一个解决方案。\n\n### 基于原则的解决方案设计\n\n该问题要求为一个无限期动态规划问题找到平稳最优策略函数 $g(k)$。该问题的贝尔曼方程是：\n$$\nV(k) = \\max_{k' \\in \\mathcal{K} \\text{ and feasible}} \\left\\{ u(A k^{\\alpha} + (1-\\delta)k - k') + \\beta V(k') \\right\\}\n$$\n其中 $V(k)$ 是价值函数。最优策略函数 $g(k)$ 是为每个 $k$ 给出能达到最大值的 $k'$ 的函数。\n\n我们将使用**策略函数迭代 (PFI)** 算法来求解策略函数 $g(k)$。这种迭代方法寻找贝尔曼算子的不动点，该不动点对应于平稳最优策略。\n\nPFI算法按以下步骤进行：\n1.  **初始化**：\n    -   将资本网格 $\\mathcal{K}$ 定义为一个从 $\\bar{k}$ 到 $k_{\\max}$ 的包含 $N$ 个点的数组。\n    -   从策略函数 $g_0(k)$ 的一个初始猜测开始。一个简单且可行的初始策略是选择下一期可能的最小资本，即对所有 $k \\in \\mathcal{K}$，设 $g_0(k) = \\bar{k}$。\n\n2.  **迭代**：重复以下两个步骤，直到策略函数收敛。设当前迭代为 $j$。\n    a. **策略评估**：给定当前策略 $g_j(k)$，计算相应的价值函数 $V_j(k)$。该价值函数是以下线性泛函方程的解：\n       $$\n       V_j(k) = u(A k^{\\alpha} + (1-\\delta)k - g_j(k)) + \\beta V_j(g_j(k))\n       $$\n       这代表了一组关于 $N$ 个值 $\\{V_j(k_i)\\}_{i=1}^N$ 的 $N$ 个线性方程组。我们不通过矩阵求逆（计算量大，$O(N^3)$）直接求解此方程组，而是通过对固定策略下的价值函数进行迭代来找到 $V_j$：\n       $$\n       V_{j, s+1}(k) = R_j(k) + \\beta V_{j,s}(g_j(k))\n       $$\n       其中 $R_j(k) = u(A k^{\\alpha} + (1-\\delta)k - g_j(k))$ 是策略 $g_j$ 下的报酬。此内循环一直迭代直到 $V_{j,s}$ 收敛到 $V_j$。\n\n    b. **策略改进**：利用评估步骤得到的价值函数 $V_j$，通过求解每个状态 $k \\in \\mathcal{K}$ 的最大化问题来找到改进后的策略 $g_{j+1}(k)$：\n       $$\n       g_{j+1}(k) = \\underset{k' \\in \\mathcal{K} \\text{ s.t. } \\bar{k} \\le k' < Y(k)}{\\arg\\max} \\left\\{ u(Y(k) - k') + \\beta V_j(k') \\right\\}\n       $$\n       其中 $Y(k) = A k^{\\alpha} + (1-\\delta)k$ 是可用的资源总量。对最优 $k'$ 的搜索是在所有满足可行性约束的网格点上进行的。严格不等式 $k' < Y(k)$ 确保了消费始终为正，这是对数和CRRA效用函数的要求。\n\n3.  **收敛**：当策略函数在两次迭代之间不再变化时，即对所有 $k \\in \\mathcal{K}$ 都有 $g_{j+1}(k) = g_j(k)$ 时，算法终止。得到的策略就是平稳最优策略 $g(k)$。\n\n一旦计算出最优策略函数 $g(k)$，三个诊断指标按如下方式计算：\n- **诊断指标1**：资本下界成为紧约束的网格点的比例。由于策略选择 $g(k)$ 必须位于网格 $\\mathcal{K}$ 上，且第一个网格点是 $\\bar{k}$，因此对于一个很小的 $\\tau$，条件 $g(k) \\le \\bar{k} + \\tau$ 等价于 $g(k) = \\bar{k}$。我们计算使 $g(k) = \\bar{k}$ 的状态 $k \\in \\mathcal{K}$ 所占的比例。\n- **诊断指标2**：在中位数资本网格点 $k_m$ 处的策略函数值。问题中的索引 $m$ 是基于1的，所以对于一个大小为 $N$ 的基于0的索引数组，这对应于索引 $\\lfloor(N-1)/2\\rfloor$。我们报告 $g(k_{\\lfloor(N-1)/2\\rfloor})$。\n- **诊断指标3**：整个网格上的最大绝对欧拉方程误差。对于每个状态 $k$，我们计算消费 $c(k) = Y(k) - g(k)$ 和下一期的消费 $c(g(k)) = Y(g(k)) - g(g(k))$。然后，我们使用提供的公式计算残差 $r(k)$，并找出所有 $k \\in \\mathcal{K}$ 中的最大值。这可以作为我们计算出的策略准确性的一个度量。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the dynamic programming problem for all test cases\n    and print the results in the specified format.\n    \"\"\"\n    test_cases = [\n        (0.33, 0.96, 0.08, 1.0, 2.0, 0.01, 200, 5.0),\n        (0.33, 0.96, 0.08, 1.0, 2.0, 0.50, 200, 5.0),\n        (0.35, 0.94, 0.10, 0.90, 1.0, 1.50, 200, 5.0),\n    ]\n\n    results = []\n    for params in test_cases:\n        alpha, beta, delta, A, sigma, k_bar, N, k_max = params\n        diagnostics = compute_policy_and_diagnostics(\n            alpha, beta, delta, A, sigma, k_bar, int(N), k_max\n        )\n        results.extend(diagnostics)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_policy_and_diagnostics(alpha, beta, delta, A, sigma, k_bar, N, k_max):\n    \"\"\"\n    Computes the stationary optimal policy function using Policy Function Iteration\n    and calculates the required diagnostics.\n    \"\"\"\n    # 1. Setup the economic environment\n    k_grid = np.linspace(k_bar, k_max, N)\n\n    # Define utility and marginal utility functions based on sigma\n    if sigma == 1.0:\n        u = lambda c: np.log(c)\n        u_prime = lambda c: 1.0 / c\n    else:\n        u = lambda c: (c**(1.0 - sigma)) / (1.0 - sigma)\n        u_prime = lambda c: c**(-sigma)\n\n    # 2. Policy Function Iteration (PFI)\n    policy_idx = np.zeros(N, dtype=int)  # Initial policy: g(k) = k_bar for all k\n    \n    max_pfi_iter = 200\n    pfi_tol = 1e-9\n\n    V = np.zeros(N) # Initialize value function\n\n    for pfi_iter in range(max_pfi_iter):\n        # --- Policy Evaluation ---\n        # Iteratively find the value function for the current policy\n        max_v_iter = 5000\n        v_tol = 1e-9\n        \n        k_prime_eval = k_grid[policy_idx]\n        c_eval = A * k_grid**alpha + (1.0 - delta) * k_grid - k_prime_eval\n        \n        # Consumption must be positive\n        R = np.full(N, -np.inf)\n        positive_c_mask = c_eval > 0\n        R[positive_c_mask] = u(c_eval[positive_c_mask])\n        \n        for v_iter in range(max_v_iter):\n            V_next = R + beta * V[policy_idx]\n            if np.max(np.abs(V_next - V)) < v_tol:\n                break\n            V = V_next\n        else: # This 'else' belongs to the for-loop, runs if no 'break'\n            pass # Convergence not reached, but proceed with current V\n\n        # --- Policy Improvement ---\n        new_policy_idx = np.zeros(N, dtype=int)\n        y = A * k_grid**alpha + (1.0 - delta) * k_grid\n\n        for i in range(N):\n            # Find feasible choices for k' (must allow for positive consumption)\n            # k_prime < y[i]\n            valid_k_prime_end_idx = np.searchsorted(k_grid, y[i], side='left')\n\n            if valid_k_prime_end_idx == 0:\n                # No feasible choice leads to positive consumption\n                new_policy_idx[i] = 0 # Default to k_bar\n                continue\n\n            choice_indices = np.arange(valid_k_prime_end_idx)\n            k_prime_choices = k_grid[choice_indices]\n            c_choices = y[i] - k_prime_choices\n            \n            value_choices = u(c_choices) + beta * V[choice_indices]\n            \n            best_choice_local_idx = np.argmax(value_choices)\n            new_policy_idx[i] = choice_indices[best_choice_local_idx]\n\n        # --- Convergence Check ---\n        if np.max(np.abs(new_policy_idx - policy_idx)) < 1:\n            policy_idx = new_policy_idx\n            break\n        \n        policy_idx = new_policy_idx\n\n    # 3. Calculate Diagnostics\n    # Final policy g(k) = k'\n    g_k = k_grid[policy_idx]\n    \n    # Diagnostic 1: Fraction of grid points where the lower bound is binding\n    # Since g(k) is on the grid and k_bar is the first grid point,\n    # g(k) <= k_bar + tau is equivalent to g(k) == k_bar.\n    diag1 = np.mean(policy_idx == 0)\n    \n    # Diagnostic 2: Policy value at the median grid index\n    # Problem: m = floor((N-1)/2) + 1 (1-based), Python: (N-1)//2 (0-based)\n    median_idx = (N - 1) // 2\n    diag2 = g_k[median_idx]\n    \n    # Diagnostic 3: Maximum absolute Euler equation residual\n    c = A * k_grid**alpha + (1.0 - delta) * k_grid - g_k\n    \n    # Find g(g(k))\n    g_of_g_k_policy_idx = policy_idx[policy_idx]\n    g_of_g_k = k_grid[g_of_g_k_policy_idx]\n    \n    # Consumption at the next state, c' = c(g(k))\n    c_prime = A * g_k**alpha + (1.0 - delta) * g_k - g_of_g_k\n    \n    # Ensure all consumptions are positive before taking logs or negative powers\n    if np.any(c <= 0) or np.any(c_prime <= 0):\n        # This indicates a problem, as feasible policies should yield c > 0.\n        # However, to prevent crashes, we'll signal an error with a large residual.\n        # In a well-behaved model, this branch is not taken.\n        return diag1, diag2, np.inf\n\n    u_prime_c = u_prime(c)\n    u_prime_c_prime = u_prime(c_prime)\n    \n    # Marginal return on capital (k')\n    return_on_k_prime = alpha * A * g_k**(alpha - 1.0) + (1.0 - delta)\n    \n    residuals = np.abs(u_prime_c - beta * u_prime_c_prime * return_on_k_prime)\n    diag3 = np.max(residuals)\n    \n    return diag1, diag2, diag3\n\nif __name__ == '__main__':\n    solve()\n```"}]}