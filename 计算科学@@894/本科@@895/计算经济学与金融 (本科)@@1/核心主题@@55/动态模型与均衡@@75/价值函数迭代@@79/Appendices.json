{"hands_on_practices": [{"introduction": "在深入研究值函数迭代的代码实现之前，让我们首先通过一个简单的计算来理解所有数值动态规划方法都面临的一个基本挑战：“维度灾难” (curse of dimensionality)。这个练习将具体展示，随着我们模型中状态变量的增加，问题的规模会如何呈指数级增长。理解这一点对于认识到为何我们需要，以及何时可以使用，诸如值函数迭代这样的巧妙算法至关重要。[@problem_id:2439741]", "id": "2439741", "problem": "在一个计算经济学应用中，考虑通过对一个离散化的连续状态向量进行价值函数迭代，来解决一个动态问题的有限状态马尔可夫决策过程 (MDP)。状态向量是 $d$ 维的，且状态向量的每个分量被离散化为 $10$ 个等距的区间。假设在状态空间上形成了一个规则的笛卡尔网格。\n\n计算在 $d=2$ 和 $d=10$ 的情况下不同网格单元的总数。请以对应于 $\\left(d=2,\\ d=10\\right)$ 的有序对形式提供您的答案。报告精确值，无需四舍五入。无需单位。", "solution": "首先将对问题陈述进行严格的验证过程。\n\n步骤 1：提取已知条件。\n问题陈述中明确提供的数据如下：\n- 背景是计算经济学中的一个有限状态马尔可夫决策过程 (MDP)。\n- 解决方法是对一个离散化的连续状态向量进行价值函数迭代。\n- 状态向量是 $d$ 维的。\n- 状态向量的每个分量被离散化为 $10$ 个等距的区间。\n- 网格结构是状态空间上的一个规则的笛卡尔网格。\n- 任务是计算在维度 $d=2$ 和维度 $d=10$ 两种情况下不同网格单元的总数。\n- 答案必须以对应于 $(d=2, d=10)$ 的有序对形式提供。\n- 数值必须是精确的。\n\n步骤 2：使用提取的已知条件进行验证。\n根据所需标准对问题进行评估。\n- **科学依据**：该问题植根于动态规划和计算经济学这一成熟领域。将连续状态空间离散化为笛卡尔网格是近似求解连续状态 MDP 的一种标准技术。该问题所展示的“维度灾难”是该领域的一个基本概念。因此，该问题在科学上是合理的。\n- **适定性**：该问题是适定的。它提供了计算唯一确定性答案所需的所有信息。每个维度的区间数量和维度本身都已指定，网格的结构也明确定义为笛卡尔网格。\n- **客观性**：该问题以精确、技术性和客观的语言陈述，没有任何主观或模棱两可的术语。\n\n基于此分析，该问题被认为是有效的，可以推导出解决方案。\n\n该问题要求计算离散化状态空间中的状态总数。在此背景下，一个状态是多维网格中的一个单独的网格单元。\n\n设状态向量的维度用 $d$ 表示。\n设每个维度的离散化区间（或单元）数量用 $N$ 表示。\n根据问题陈述，我们已知 $N = 10$。\n\n状态空间由一个规则的笛卡尔网格构成。这意味着对于 $d$ 个维度中的每一个，我们都从 $N$ 个可用区间中选择一个。一个特定的状态由所有维度上区间选择的唯一组合来定义。\n\n对于一个通用维度 $d$，不同网格单元的总数（我们用 $S_d$ 表示）是所有可能组合的总数。由于第一个维度有 $N$ 种选择，第二个维度有 $N$ 种选择，以此类推，直到第 $d$ 个维度，因此组合总数是每个维度选择数量的乘积。\n\n$$S_d = \\underbrace{N \\times N \\times \\dots \\times N}_{d \\text{ times}}$$\n\n这可以用指数形式表示为：\n\n$$S_d = N^{d}$$\n\n状态数量与维度之间的这种指数关系是“维度灾难”的数学基础。随着 $d$ 的增加，状态空间的大小 $S_d$ 会呈指数级增长，使得像价值函数迭代这样的穷举计算方法变得不可行。\n\n问题要求我们计算 $S_d$ 在两个特定 $d$ 值（$d=2$ 和 $d=10$）下的数值。\n\n情况 1：$d=2$\n对于一个 $2$ 维状态空间，我们将 $d=2$ 和 $N=10$ 代入公式：\n$$S_2 = N^{2} = 10^{2} = 100$$\n因此，对于一个 $2$ 维问题，状态空间被离散化为 $100$ 个单元。\n\n情况 2：$d=10$\n对于一个 $10$ 维状态空间，我们将 $d=10$ 和 $N=10$ 代入公式：\n$$S_{10} = N^{10} = 10^{10} = 10,000,000,000$$\n对于一个 $10$ 维问题，状态空间被离散化为一百亿个单元。这种急剧的增长鲜明地说明了高维度带来的计算负担。\n\n最终答案应以有序对 $(S_2, S_{10})$ 的形式呈现。\n因此，该有序对为 $(100, 10^{10})$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n100 & 10^{10}\n\\end{pmatrix}\n}\n$$"}, {"introduction": "现在我们将着手完成核心任务：从零开始实现值函数迭代 (Value Function Iteration)。我们将求解一个带有随机生产率冲击的新古典主义增长模型，这是现代宏观经济学中的一个主力模型。这个练习将指导你完成离散化状态空间、构造贝尔曼算子以及检查收敛性等关键步骤，从而在实践中巩固你对值函数迭代算法的理解。[@problem_id:2446471]", "id": "2446471", "problem": "考虑一个无限期消费—储蓄问题，其中有一个代表性代理人，他面临由物质资本和外生生产率水平组成的二维状态。时间是离散的，由 $t \\in \\{0,1,2,\\dots\\}$ 索引。时期效用为恒定相对风险规避 (CRRA) 形式，为消费 $c_t$ 定义为\n$$\nu(c_t) = \n\\begin{cases}\n\\dfrac{c_t^{1-\\sigma}}{1-\\sigma}, & \\text{若 } \\sigma \\neq 1, \\\\\n\\log(c_t), & \\text{若 } \\sigma = 1,\n\\end{cases}\n$$\n其中 $\\sigma > 0$ 是相对风险厌恶系数。生产技术为 Cobb–Douglas 形式，资本份额参数为 $\\alpha \\in (0,1)$，折旧率为 $\\delta \\in (0,1)$，因此产出为 $y_t = z_t k_t^{\\alpha}$，资本的运动法则满足\n$$\nc_t + k_{t+1} = z_t k_t^{\\alpha} + (1 - \\delta) k_t,\n$$\n带有非负约束 $c_t \\ge 0$ 和借贷约束 $k_{t+1} \\ge 0$。贴现因子为 $\\beta \\in (0,1)$。\n\n外生生产率状态 $z_t$ 在一个有限网格 $\\{z_1,\\dots,z_{N_z}\\}$ 上取值，并遵循一个时间同质的马尔可夫链，其转移矩阵为 $P \\in \\mathbb{R}^{N_z \\times N_z}$，其中 $P_{ij} = \\mathbb{P}(z_{t+1}=z_j \\mid z_t=z_i)$ 且 $P$ 的每行之和为 $1$。\n\n将值函数 $V(k,z)$ 定义为从状态 $(k,z)$ 出发，所有可行计划下预期贴现效用总和的上确界。值函数满足 Bellman 方程\n$$\nV(k,z) = \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\!\\left(z k^{\\alpha} + (1-\\delta)k - k' \\right) + \\beta \\sum_{z' \\in \\mathcal{Z}} \\mathbb{P}(z' \\mid z) V(k', z') \\right\\},\n$$\n其中 $\\mathcal{Z} = \\{z_1,\\dots,z_{N_z}\\}$ 且 $\\mathcal{K}$ 是一个离散的资本选择网格。可行集由满足 $z k^{\\alpha} + (1-\\delta)k - k' \\ge 0$ 的 $k' \\in \\mathcal{K}$ 来刻画。\n\n你的程序必须：\n- 将资本空间离散化为一个包含 $N_k$ 个点的等距网格 $\\mathcal{K} = \\{k_{\\min}, k_{\\min} + \\Delta k, \\dots, k_{\\max}\\}$ (包括端点)。\n- 计算 Bellman 方程在笛卡尔积 $\\mathcal{K} \\times \\mathcal{Z}$ 上的唯一不动点，以及相关的下一期资本的策略函数 $k'(k,z)$，其取值被限制在 $\\mathcal{K}$ 内。\n- 对于下方的每个测试用例，报告以下五个量：\n  1. 满足停止准则所用的迭代次数。\n  2. 在资本网格中位数索引和冲击中位数索引处评估的值函数，即 $V\\!\\left(k_{\\lfloor (N_k-1)/2 \\rfloor+1}, z_{\\lfloor (N_z-1)/2 \\rfloor+1}\\right)$。\n  3. 在最低资本和最低冲击水平下的最优下一期资本，即 $k'(k_{\\min}, z_{\\min})$。\n  4. 一个布尔值，指示对于每个固定的冲击 $z$，策略 $k'(k,z)$ 在整个资本网格上是否关于 $k$ 是弱单调递增的。\n  5. 在所有内部状态上的最大绝对欧拉方程残差（不包括策略在 $(k,z)$ 处选择最低或最高资本网格点，或者任何隐含的消费为非正的状态），其中在状态 $(k,z)$ 的残差定义为\n     $$\n     \\left| 1 - \\dfrac{\\beta \\, \\mathbb{E}\\!\\left[ u'(c') \\left( \\alpha z' k'^{\\alpha - 1} + 1 - \\delta \\right) \\,\\big|\\, z \\right]}{u'(c)} \\right|,\n     $$\n     其中 $c = z k^{\\alpha} + (1-\\delta)k - k'$, $k' = k'(k,z)$, $c' = z' k'^{\\alpha} + (1-\\delta)k' - k''$, 且 $k'' = k'(k', z')$。此处 $u'(c)$ 表示消费的边际效用。\n\n使用以下测试套件。对于每个测试用例，使用指定的参数、冲击网格、转移矩阵和资本网格：\n\n- 测试用例A (基准)：\n  - $\\beta = 0.96$, $\\sigma = 2.0$, $\\alpha = 0.36$, $\\delta = 0.08$。\n  - 冲击网格：$N_z = 3$ 且 $z = [0.9, 1.0, 1.1]$。\n  - 转移矩阵\n    $$\n    P = \\begin{bmatrix}\n    0.90 & 0.09 & 0.01 \\\\\n    0.09 & 0.82 & 0.09 \\\\\n    0.01 & 0.09 & 0.90\n    \\end{bmatrix}.\n    $$\n  - 资本网格：$N_k = 80$, $k_{\\min} = 0.01$, $k_{\\max} = 3.0$。\n\n- 测试用例B (较低的耐心和更易变的冲击)：\n  - $\\beta = 0.90$, $\\sigma = 2.0$, $\\alpha = 0.36$, $\\delta = 0.08$。\n  - 冲击网格：$N_z = 3$ 且 $z = [0.8, 1.0, 1.2]$。\n  - 转移矩阵\n    $$\n    P = \\begin{bmatrix}\n    0.85 & 0.10 & 0.05 \\\\\n    0.10 & 0.80 & 0.10 \\\\\n    0.05 & 0.10 & 0.85\n    \\end{bmatrix}.\n    $$\n  - 资本网格：$N_k = 60$, $k_{\\min} = 0.01$, $k_{\\max} = 3.0$。\n\n- 测试用例C (较高的耐心和较低的折旧)：\n  - $\\beta = 0.985$, $\\sigma = 2.0$, $\\alpha = 0.36$, $\\delta = 0.02$。\n  - 冲击网格：$N_z = 3$ 且 $z = [0.95, 1.0, 1.05]$。\n  - 转移矩阵\n    $$\n    P = \\begin{bmatrix}\n    0.92 & 0.07 & 0.01 \\\\\n    0.07 & 0.86 & 0.07 \\\\\n    0.01 & 0.07 & 0.92\n    \\end{bmatrix}.\n    $$\n  - 资本网格：$N_k = 100$, $k_{\\min} = 0.01$, $k_{\\max} = 4.0$。\n\n停止准则和数值约定：\n- 任意初始化在 $\\mathcal{K} \\times \\mathcal{Z}$ 上的值函数。\n- 迭代 Bellman 方程，直到连续两次值函数迭代之间的上确界范数距离小于 $\\varepsilon = 10^{-6}$，或达到最大迭代次数 $N_{\\text{iter},\\max} = 1000$。\n- 在计算效用时，将任何消费 $c \\le 0$ 视为不可行。你必须强制 $c \\ge 0$；任何不可行的选择都不能在最大化过程中被选中。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个列表，该列表有三个子列表，分别对应测试用例A、B、C的顺序。每个子列表必须按上述指定顺序包含五个量：$[\\text{iterations}, V_{\\text{mid}}, k'_{\\text{min,low}~z}, \\text{is\\_monotone}, \\text{max\\_Euler\\_residual}]$。\n- 所有浮点数必须四舍五入到六位小数；布尔值必须是 True 或 False。\n- 具体来说，输出必须如下所示\n  $$\n  \\big[ [n_A, v_A, k_A, b_A, e_A], [n_B, v_B, k_B, b_B, e_B], [n_C, v_C, k_C, b_C, e_C] \\big],\n  $$\n  以单行形式打印，方括号和逗号需与所示完全一致，其中 $n_\\cdot$ 是整数，$v_\\cdot, k_\\cdot, e_\\cdot$ 是四舍五入到六位小数的浮点数，$b_\\cdot$ 是布尔值。", "solution": "该问题是一个平稳的无限期动态规划问题，具有紧凑的离散化状态空间和在可行集上的有界回报函数。将有界函数映射到有界函数的 Bellman 算子 $T$ 对于 $(k,z) \\in \\mathcal{K} \\times \\mathcal{Z}$ 逐点定义为\n$$\n(TV)(k,z) = \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\!\\left(z k^{\\alpha} + (1 - \\delta)k - k' \\right) + \\beta \\sum_{z' \\in \\mathcal{Z}} \\mathbb{P}(z' \\mid z) V(k', z') \\right\\}.\n$$\n根据 Blackwell 的充分条件，$T$ 是有界函数空间在上确界范数下的一个压缩映射：它具有单调性，因为如果 $V \\le W$，则 $TV \\le TW$；并且它满足贴现条件，因为对于任何常数 $a$，都有 $(TV + a)(k,z) \\le (TV)(k,z) + \\beta a$，其模数为 $\\beta \\in (0,1)$。因此，$T$ 有一个唯一的不动点 $V^\\star$，并且对于任何初始的 $V_0$，序列 $V_{n+1} = TV_n$ 都在上确界范数下收敛到 $V^\\star$。\n\n为了数值计算该不动点，我们将资本状态和决策 $k'$ 都在同一个有限网格 $\\mathcal{K} = \\{k_1,\\dots,k_{N_k}\\}$上进行离散化，并将外生冲击空间离散化在 $\\mathcal{Z} = \\{z_1,\\dots,z_{N_z}\\}$ 上。对于每个冲击 $z_j$ 和每个当前资本 $k_i$，我们枚举所有候选的下一期资本 $k' \\in \\mathcal{K}$，计算隐含的消费 $c = z_j k_i^{\\alpha} + (1 - \\delta) k_i - k'$，丢弃消费 $c \\le 0$ 的不可行候选项，评估时期效用 $u(c)$，并加上贴现后的期望延续值 $\\beta \\sum_{m=1}^{N_z} P_{j m} V(k', z_m)$。对 $k'$ 的最大化器产生更新后的值和相应的策略索引。这种构造以向量化的方式实现了将 Bellman 算子应用于值函数迭代的过程。重复此更新，直到连续迭代之间的上确界范数距离小于 $\\varepsilon = 10^{-6}$ (或达到迭代上限)，即可得到 $V^\\star$ 及其相关策略函数 $k'(k,z)$ 的一个 $\\varepsilon$-精确近似。\n\n一旦获得策略，我们评估额外的诊断指标：\n\n- 中位数索引处的值：令 $i^\\star = \\left\\lfloor \\dfrac{N_k - 1}{2} \\right\\rfloor + 1$ 且 $j^\\star = \\left\\lfloor \\dfrac{N_z - 1}{2} \\right\\rfloor + 1$。报告 $V(k_{i^\\star}, z_{j^\\star})$。\n\n- 边界状态 $(k_{\\min}, z_{\\min})$ 处的下一期最优资本：报告 $k'(k_1, z_1)$。\n\n- 策略在资本上的单调性：对于每个固定的冲击 $z_j$，验证映射 $i \\mapsto k'(k_i, z_j)$ 对于 $i = 1,\\dots,N_k$ 是否是弱单调递增的。这一性质在理论上得到了标准凹增长模型中 Bellman 算子的单调性和凹性的支持，尽管数值离散化可能会引入微小的违规。我们通过检查所有相邻对来计算该布尔值。\n\n- 欧拉方程残差：内部解的一阶条件断言\n$$\nu'(c_t) = \\beta \\, \\mathbb{E} \\left[ u'(c_{t+1}) \\left( \\alpha z_{t+1} k_{t+1}^{\\alpha - 1} + 1 - \\delta \\right) \\bigg| z_t \\right].\n$$\n使用离散策略 $k' = k'(k,z)$ 和关于 $z$ 的马尔可夫链，我们将每个内部状态 $(k_i, z_j)$ 的残差近似为\n$$\nR(k_i,z_j) = \\left| 1 - \\dfrac{\\beta \\sum_{m=1}^{N_z} P_{j m} \\, u'(c'(k_i,z_j; z_m)) \\, \\left( \\alpha z_m \\, k'(k_i,z_j)^{\\alpha - 1} + 1 - \\delta \\right)}{u'(c(k_i,z_j))} \\right|,\n$$\n其中 $c(k_i,z_j) = z_j k_i^{\\alpha} + (1 - \\delta) k_i - k'(k_i,z_j)$ 且 $c'(k_i,z_j; z_m) = z_m k'(k_i,z_j)^{\\alpha} + (1 - \\delta) k'(k_i,z_j) - k''$，其中 $k'' = k'(k'(k_i,z_j), z_m)$。我们排除了策略选择最低或最高 $k'$ 的状态（以避免角点解）以及任何 $c$ 或 $c'$ 为非正的状态。在其余状态上的最大残差衡量了离散策略相对于欧拉条件的全局质量。\n\n数值细节：\n- 对于 $\\sigma \\neq 1$ 的 CRRA 效用，有 $u'(c) = c^{-\\sigma}$。对于 $\\sigma = 1$，可以使用 $u(c) = \\log(c)$ 和 $u'(c) = 1/c$；实现中兼容了这两种情况。\n- 通过在最大化步骤中为不可行的选择分配一个负无穷大的值来强制实现可行性 $c \\ge 0$，确保它们永远不会被选中。\n- 对于一个固定的当前冲击 $z_j$ 和候选的 $k'$，其期望延续值是通过将 $P$ 的行与在 $k'$ 处评估的关于 $z'$ 的堆叠值函数进行行式乘积来计算的。\n\n将此程序应用于三个指定的测试用例中的每一个，使用其给定的参数、冲击网格、转移矩阵和资本网格，可以得到每个案例的五个所要求的输出：迭代次数、中位数索引处的值、在 $(k_{\\min}, z_{\\min})$ 处的策略、单调性布尔值和最大欧拉残差。所有浮点输出都四舍五入到六位小数，最终打印结果是按要求组织的单行列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef crra_utility(c, sigma):\n    \"\"\"CRRA utility u(c); returns -inf for nonpositive c to enforce feasibility.\"\"\"\n    c = np.asarray(c)\n    u = np.full_like(c, -np.inf, dtype=np.float64)\n    positive = c > 0\n    if sigma == 1.0:\n        u[positive] = np.log(c[positive])\n    else:\n        u[positive] = (np.power(c[positive], 1.0 - sigma)) / (1.0 - sigma)\n    return u\n\ndef crra_marginal_utility(c, sigma):\n    \"\"\"Marginal utility u'(c) for CRRA; returns nan for nonpositive c.\"\"\"\n    c = np.asarray(c)\n    mu = np.full_like(c, np.nan, dtype=np.float64)\n    positive = c > 0\n    if sigma == 1.0:\n        mu[positive] = 1.0 / c[positive]\n    else:\n        mu[positive] = np.power(c[positive], -sigma)\n    return mu\n\ndef vfi(k_grid, z_grid, P, beta, sigma, alpha, delta, tol=1e-6, max_iter=1000):\n    \"\"\"\n    Perform value function iteration on discretized grids.\n    Returns:\n        V: value function array shape (Nz, Nk)\n        policy_idx: optimal k' indices shape (Nz, Nk)\n        iters: iterations performed\n    \"\"\"\n    Nz = len(z_grid)\n    Nk = len(k_grid)\n    V = np.zeros((Nz, Nk), dtype=np.float64)\n    V_new = np.empty_like(V)\n    policy_idx = np.zeros((Nz, Nk), dtype=np.int64)\n\n    # Precompute resources y[j, i] = z_j * k_i^alpha + (1 - delta) * k_i\n    k_alpha = np.power(k_grid, alpha)\n    y = (z_grid[:, None] * k_alpha[None, :]) + (1.0 - delta) * k_grid[None, :]\n\n    # For each shock z_j, continuation value over k' is beta * P[j] @ V_old\n    iters = 0\n    diff = np.inf\n    neg_inf = -1.0e12  # numerical -inf surrogate for infeasible choices\n    while iters < max_iter and diff > tol:\n        # Expected continuation values for choosing each k' at current shock j\n        # EV has shape (Nz, Nk): for each j, EV[j, k_idx] = beta * sum_m P[j, m] * V[m, k_idx]\n        EV = beta * (P @ V)\n        # Loop over shocks to form return matrices and maximize w.r.t k'\n        for j in range(Nz):\n            # consumption matrix for current shock j: C[i, kprime] = y[j,i] - k_grid[kprime]\n            C = y[j, :, None] - k_grid[None, :]\n            U = crra_utility(C, sigma)\n            # mask infeasible as neg_inf to avoid NaNs in max\n            U[~np.isfinite(U)] = neg_inf\n            # add continuation value for each k' (broadcast across current k_i)\n            # total return W[i, kprime]\n            W = U + EV[j][None, :]\n            # maximize over k' axis=1\n            pol_idx = np.argmax(W, axis=1)\n            V_new[j, :] = W[np.arange(Nk), pol_idx]\n            policy_idx[j, :] = pol_idx\n\n        diff = np.max(np.abs(V_new - V))\n        V[:, :] = V_new\n        iters += 1\n\n    return V, policy_idx, iters\n\ndef policy_monotone(policy_idx):\n    \"\"\"\n    Check monotonicity of policy in k for each z: nondecreasing indices across k.\n    Returns True if monotone for all z, else False.\n    \"\"\"\n    Nz, Nk = policy_idx.shape\n    for j in range(Nz):\n        if np.any(np.diff(policy_idx[j, :]) < 0):\n            return False\n    return True\n\ndef euler_residual_max(k_grid, z_grid, P, beta, sigma, alpha, delta, policy_idx):\n    \"\"\"\n    Compute the maximum absolute Euler residual over interior states.\n    Excludes states where policy picks boundary k' or any implied c or c' is nonpositive.\n    Residual at (k_i, z_j): |1 - RHS / LHS| with LHS = u'(c), RHS = beta * E[u'(c') * (alpha z' k'^{alpha-1} + 1 - delta)].\n    \"\"\"\n    Nz = len(z_grid)\n    Nk = len(k_grid)\n    # Precompute\n    k_alpha = np.power(k_grid, alpha)\n    y = (z_grid[:, None] * k_alpha[None, :]) + (1.0 - delta) * k_grid[None, :]\n\n    max_resid = 0.0\n    # Iterate over states\n    for j in range(Nz):\n        for i in range(Nk):\n            kp_idx = policy_idx[j, i]\n            # exclude boundary policies\n            if kp_idx == 0 or kp_idx == Nk - 1:\n                continue\n            k = k_grid[i]\n            z = z_grid[j]\n            kp = k_grid[kp_idx]\n            # Current consumption\n            c = z * (k ** alpha) + (1.0 - delta) * k - kp\n            if c <= 0:\n                continue\n            mu_c = (1.0 / c) if sigma == 1.0 else c ** (-sigma)\n            # Next period marginal utility and returns term\n            # For each z' compute c' and k'' = policy(k', z')\n            kp_alpha = kp ** alpha\n            # Precompute gross return at next period given z'\n            R_terms = alpha * z_grid * (kp ** (alpha - 1.0)) + (1.0 - delta)\n            # Compute c' for each z'\n            y_next = z_grid * kp_alpha + (1.0 - delta) * kp  # shape (Nz,)\n            # For each z' state m, need k'' index policy_idx[m, kp_idx]\n            kpp_idx_vec = policy_idx[:, kp_idx]  # shape (Nz,)\n            kpp_vec = k_grid[kpp_idx_vec]        # shape (Nz,)\n            c_next = y_next - kpp_vec            # shape (Nz,)\n            # Exclude if any c' is nonpositive by setting their marginal utility to nan and ignoring via weights\n            mu_c_next = crra_marginal_utility(c_next, sigma)  # nan where nonpositive\n            # Expected RHS conditional on current z_j using P[j, :]\n            valid = np.isfinite(mu_c_next)\n            if not np.any(valid):\n                continue\n            RHS = beta * np.sum(P[j, valid] * mu_c_next[valid] * R_terms[valid])\n            resid = abs(1.0 - RHS / mu_c)\n            if np.isfinite(resid):\n                if resid > max_resid:\n                    max_resid = resid\n    return float(max_resid)\n\ndef run_test_case(params):\n    beta = params[\"beta\"]\n    sigma = params[\"sigma\"]\n    alpha = params[\"alpha\"]\n    delta = params[\"delta\"]\n    z = np.array(params[\"z\"], dtype=np.float64)\n    P = np.array(params[\"P\"], dtype=np.float64)\n    Nk = params[\"Nk\"]\n    k_min = params[\"k_min\"]\n    k_max = params[\"k_max\"]\n    k_grid = np.linspace(k_min, k_max, Nk, dtype=np.float64)\n\n    V, pol_idx, iters = vfi(k_grid, z, P, beta, sigma, alpha, delta, tol=1e-6, max_iter=1000)\n\n    # Median indices\n    mid_k_idx = (Nk - 1) // 2\n    mid_z_idx = (len(z) - 1) // 2\n    V_mid = V[mid_z_idx, mid_k_idx]\n    # Policy at (k_min, z_min)\n    kp_min_lowz = k_grid[pol_idx[0, 0]]\n    # Monotonicity\n    is_mono = policy_monotone(pol_idx)\n    # Euler residual\n    max_euler_resid = euler_residual_max(k_grid, z, P, beta, sigma, alpha, delta, pol_idx)\n\n    # Round floats to six decimals\n    result = [\n        int(iters),\n        round(float(V_mid), 6),\n        round(float(kp_min_lowz), 6),\n        bool(is_mono),\n        round(float(max_euler_resid), 6),\n    ]\n    return result\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"A\",\n            \"beta\": 0.96,\n            \"sigma\": 2.0,\n            \"alpha\": 0.36,\n            \"delta\": 0.08,\n            \"z\": [0.9, 1.0, 1.1],\n            \"P\": [\n                [0.90, 0.09, 0.01],\n                [0.09, 0.82, 0.09],\n                [0.01, 0.09, 0.90],\n            ],\n            \"Nk\": 80,\n            \"k_min\": 0.01,\n            \"k_max\": 3.0,\n        },\n        {\n            \"name\": \"B\",\n            \"beta\": 0.90,\n            \"sigma\": 2.0,\n            \"alpha\": 0.36,\n            \"delta\": 0.08,\n            \"z\": [0.8, 1.0, 1.2],\n            \"P\": [\n                [0.85, 0.10, 0.05],\n                [0.10, 0.80, 0.10],\n                [0.05, 0.10, 0.85],\n            ],\n            \"Nk\": 60,\n            \"k_min\": 0.01,\n            \"k_max\": 3.0,\n        },\n        {\n            \"name\": \"C\",\n            \"beta\": 0.985,\n            \"sigma\": 2.0,\n            \"alpha\": 0.36,\n            \"delta\": 0.02,\n            \"z\": [0.95, 1.0, 1.05],\n            \"P\": [\n                [0.92, 0.07, 0.01],\n                [0.07, 0.86, 0.07],\n                [0.01, 0.07, 0.92],\n            ],\n            \"Nk\": 100,\n            \"k_min\": 0.01,\n            \"k_max\": 4.0,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_test_case(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Ensure booleans and numbers are printed without extra spaces.\n    def format_item(x):\n        if isinstance(x, bool):\n            return \"True\" if x else \"False\"\n        elif isinstance(x, (int, np.integer)):\n            return str(int(x))\n        elif isinstance(x, float) or isinstance(x, np.floating):\n            # Ensure fixed rounding to 6 decimals; preserve trailing zeros\n            return f\"{x:.6f}\"\n        else:\n            return str(x)\n\n    formatted = []\n    for res in results:\n        formatted.append(\"[\" + \",\".join(format_item(x) for x in res) + \"]\")\n    print(\"[\" + \",\".join(formatted) + \"]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"}, {"introduction": "值函数迭代并非求解贝尔曼方程的唯一算法。本练习将介绍霍华德的策略迭代算法 (Howard's Policy Iteration)，这是一种强大的替代方法，通常能以更少的迭代次数（尽管每次迭代的计算量更大）收敛。通过实现该算法并与值函数迭代进行比较，你将对解决动态规划问题的不同计算策略及其所涉及的权衡有更深刻的认识。[@problem_id:2446390]", "id": "2446390", "problem": "考虑如下定义的无穷期界、确定性、离散状态、离散选择的资本积累问题。状态是资本 $k \\in \\mathcal{K}$，其中 $\\mathcal{K}$ 是区间 $[k_{\\min}, k_{\\max}]$ 上的一个有限网格。每期效用为 $u(c) = \\log(c)$（当 $c > 0$ 时），否则为 $u(c) = -\\infty$。生产函数为 $f(k) = z k^{\\alpha}$，其中 $z=1$；资本以速率 $\\delta \\in (0,1)$ 折旧；折扣因子为 $\\beta \\in (0,1)$。其运动定律为\n$$\nk' \\in \\mathcal{K}, \\quad c = f(k) + (1-\\delta)k - k'.\n$$\n令贝尔曼方程为\n$$\nV(k) = \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\big(f(k) + (1-\\delta)k - k'\\big) + \\beta V(k') \\right\\}.\n$$\n使用以下离散近似方案：\n- 网格 $\\mathcal{K}$ 是在 $[k_{\\min}, k_{\\max}]$ 上由 $N_k$ 个等距点组成的集合。\n- 通过一阶条件 $f'(k^{\\star}) = \\frac{1}{\\beta} - 1 + \\delta$ 定义确定性稳态 $k^{\\star}$，即：\n$$\n\\alpha (k^{\\star})^{\\alpha-1} = \\frac{1}{\\beta} - 1 + \\delta, \\quad \\text{所以} \\quad k^{\\star} = \\left(\\frac{\\alpha}{\\frac{1}{\\beta} - 1 + \\delta}\\right)^{\\frac{1}{1-\\alpha}}.\n$$\n- 对于每个测试案例，区间端点为 $k_{\\min} = a \\cdot k^{\\star}$ 和 $k_{\\max} = b \\cdot k^{\\star}$（其中 $a$ 和 $b$ 为给定值），$\\mathcal{K}$ 是该区间上包含 $N_k$ 个点的均匀网格。\n- 初始值函数为 $V_0(k) = 0$，对所有 $k \\in \\mathcal{K}$ 成立。\n- 对于 $c \\le 0$ 的不可行选择，其评估值为 $u(c) = -\\infty$，因此永远不会是最优选择。\n\n定义两种迭代方法来计算贝尔曼方程的近似不动点，两者均使用相同的容差 $\\varepsilon$ 和相同的均匀网格：\n- 方法 A（每次迭代进行完全最大化）：通过 $V_{n+1}(k) = \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\big(f(k) + (1-\\delta)k - k'\\big) + \\beta V_n(k') \\right\\}$（对所有 $k \\in \\mathcal{K}$）构建序列 $\\{V_n\\}_{n \\ge 0}$。当满足 $\\|V_{n+1} - V_n\\|_{\\infty} < \\varepsilon$ 的最小整数 $n \\ge 0$ 出现时停止。令 $M_A$ 为执行的迭代次数，该次数等于所应用的完全逐状态最大化扫描的次数。\n- 方法 B（改进与评估交替）：从 $V_0$ 开始，重复以下步骤直至收敛。首先，对于每个 $k \\in \\mathcal{K}$，计算决策规则 $\\pi(k) \\in \\arg\\max_{k' \\in \\mathcal{K}} \\left\\{ u\\big(f(k) + (1-\\delta)k - k'\\big) + \\beta V(k') \\right\\}$。然后，在保持 $\\pi$ 不变的情况下，执行恰好 $m$ 次策略评估扫描，每次扫描通过 $V_{\\text{new}}(k) = u\\big(f(k) + (1-\\delta)k - \\pi(k)\\big) + \\beta V\\big(\\pi(k)\\big)$（对所有 $k \\in \\mathcal{K}$）同步更新 $V$。每完成一组 $m$ 次评估扫描后，通过将当前值函数与该组开始时的值函数进行比较来检验收敛性，当 $\\ell_{\\infty}$ 距离小于 $\\varepsilon$ 时停止。令 $M_B$ 为执行的总改进步骤数，该数等于决策规则 $\\pi$ 被重新计算的次数（也即完全逐状态最大化扫描的次数）。\n\n对于下方的每个测试案例，计算并报告包含以下各项的元组：\n- $M_A$，\n- $M_B$，\n- 一个布尔值，指示 $M_B < M_A$ 是否成立，\n- 通过方法 A 和方法 B 获得的最终值函数之间的 $\\ell_{\\infty}$ 距离。\n\n所有对数均为自然对数。收敛容差为 $\\varepsilon = 10^{-5}$。如果某个算法在 $I_{\\max}$ 次外部迭代内未收敛，则终止该算法并返回截至终止时所执行的迭代次数。对于方法 A，使用 $I_{\\max} = 10{,}000$；对于方法 B，使用 $I_{\\max} = 10{,}000$ 次改进步骤。此问题不涉及角度。没有需要报告的物理单位。\n\n测试套件：\n- 案例 1：$(\\alpha, \\beta, \\delta, N_k, a, b, m) = (0.33, 0.96, 0.08, 200, 0.5, 1.5, 20)$。\n- 案例 2：$(\\alpha, \\beta, \\delta, N_k, a, b, m) = (0.36, 0.99, 0.025, 300, 0.25, 1.75, 25)$。\n- 案例 3：$(\\alpha, \\beta, \\delta, N_k, a, b, m) = (0.30, 0.95, 0.10, 80, 0.30, 1.70, 1)$。\n- 案例 4：$(\\alpha, \\beta, \\delta, N_k, a, b, m) = (0.40, 0.97, 0.05, 250, 0.40, 1.60, 50)$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。列表中的每个元素对应一个测试案例，其本身也是一个按 $[M_A, M_B, \\text{布尔值}, \\text{距离}]$ 顺序排列的列表。例如，一个语法上有效的输出如下所示：“[ [12,3,True,0.00001], [ ... ], [ ... ], [ ... ] ]”（其中的具体数字必须是您程序计算出的结果）。", "solution": "该问题是一个确定性的无穷期界动态规划模型，通常被称为新古典增长模型的离散状态、离散选择版本。目标是使用两种不同的迭代方案在有限网格上计算贝尔曼方程的近似不动点，并使用明确定义的度量标准来比较它们的性能。\n\n基本设置：\n- 状态空间是区间 $[k_{\\min}, k_{\\max}]$ 上的一个有限网格 $\\mathcal{K} = \\{k_1, k_2, \\ldots, k_{N_k}\\}$，由 $k_{\\min} = a \\cdot k^{\\star}$ 和 $k_{\\max} = b \\cdot k^{\\star}$ 构建。稳态 $k^{\\star}$ 由该确定性背景下欧拉方程所隐含的平稳性要求定义：$f'(k^{\\star}) = \\frac{1}{\\beta} - 1 + \\delta$，从而得到\n$$\nk^{\\star} = \\left(\\frac{\\alpha}{\\frac{1}{\\beta} - 1 + \\delta}\\right)^{\\frac{1}{1-\\alpha}}.\n$$\n- 在有界函数空间 $V:\\mathcal{K} \\to \\mathbb{R}$ 上的贝尔曼算子 $\\mathcal{T}$ 为\n$$\n(\\mathcal{T}V)(k) = \\max_{k' \\in \\mathcal{K}} \\Big\\{ u\\big( f(k) + (1-\\delta)k - k' \\big) + \\beta V(k') \\Big\\}.\n$$\n- 当 $c>0$ 时 $u(c) = \\log(c)$，当 $c \\le 0$ 时 $u(c) = -\\infty$，因此不可行的选择在最大化过程中被自动排除。在有限网格上，当回报有上界时，此算子在上确界范数下是一个模为 $\\beta \\in (0,1)$ 的压缩映射；因此，从任何有界初始猜测开始迭代该算子，都会收敛到唯一的不动点。\n\n离散近似与预计算：\n- 由于 $\\mathcal{K}$ 是有限且固定的，可以预先计算即期效用矩阵 $U \\in \\mathbb{R}^{N_k \\times N_k}$，其元素为\n$$\nU_{ij} = \\begin{cases}\n\\log\\!\\Big( f(k_i) + (1-\\delta)k_i - k_j \\Big), & \\text{若 } f(k_i) + (1-\\delta)k_i - k_j > 0,\\\\\n-\\infty, & \\text{否则。}\n\\end{cases}\n$$\n- 贝尔曼更新随后可以写成向量化形式\n$$\nV_{\\text{new}} = \\max_{j \\in \\{1,\\ldots,N_k\\}} \\left\\{ U_{\\cdot j} + \\beta V_j \\right\\},\n$$\n其中 $U_{\\cdot j}$ 是 $U$ 的第 $j$ 列，$V_j$ 是标量 $V(k_j)$ 并在各个状态间广播。\n\n方法 A（每次迭代进行完全最大化）：\n- 初始化 $V_0(k) = 0$（对所有 $k \\in \\mathcal{K}$）。对于 $n=0,1,2,\\ldots$，通过对每个状态在所有 $k' \\in \\mathcal{K}$ 上进行离散最大化来计算 $V_{n+1} = \\mathcal{T} V_n$。当满足 $\\|V_{n+1} - V_n\\|_{\\infty} < \\varepsilon$ 的最小 $n$ 出现时停止，并设 $M_A = n+1$，即所需完全最大化扫描的次数。\n\n方法 B（改进与评估交替）：\n- 给定当前值 $V$，通过以下方式计算一个改进的决策规则 $\\pi:\\mathcal{K}\\to\\mathcal{K}$\n$$\n\\pi(k_i) \\in \\arg\\max_{k_j \\in \\mathcal{K}} \\left\\{ U_{ij} + \\beta V(k_j) \\right\\}.\n$$\n- 定义与 $\\pi$ 相关联的策略评估算子为\n$$\n(\\mathcal{T}_{\\pi}V)(k_i) = U_{i,\\pi(i)} + \\beta V\\big(\\pi(k_i)\\big),\n$$\n其中 $U_{i,\\pi(i)} = u\\big(f(k_i) + (1-\\delta)k_i - \\pi(k_i)\\big)$。在保持 $\\pi$ 不变的情况下，应用 $\\mathcal{T}_{\\pi}$ 恰好 $m$ 次，每次扫描同步更新所有状态。这是一个模为 $\\beta$ 的压缩映射，因为它对于 $V$ 是仿射的，并且沿 $\\pi$ 诱导的映射斜率为 $\\beta$。\n- 完成 $m$ 次扫描后，使用上确界范数将新的值函数与该组开始时的值进行比较。如果距离小于 $\\varepsilon$，则停止。否则，使用当前值重新计算 $\\pi$，增加改进计数器，然后重复。数值 $M_B$ 是总的改进步骤数，等于完全最大化扫描的总次数。由于每次改进仅需在选择中进行一次最大化，而策略评估不包含最大化，因此该计数精确地分离出了计算成本高昂的最大化操作。\n\n性能度量与比较：\n- 度量 $M_A$ 反映了完全最大化迭代达到容差所需的完全最大化扫描次数。\n- 度量 $M_B$ 反映了在改进步骤之间穿插策略评估扫描时的完全最大化扫描次数。理论和实践表明，将策略固定进行 $m \\ge 1$ 次扫描会加速改进步骤之间值函数的收敛，通常会减少 $M_B$ 相对于 $M_A$ 的计数，因为改进的需求频率降低了。\n- 为验证两种方法计算的是同一个不动点，需要计算它们最终值函数之间的 $\\ell_{\\infty}$ 距离。在离散化、折扣因子 $\\beta \\in (0,1)$ 以及有限网格上的有界回报结构均相同的情况下，两种方法都会在容差范围内收敛到同一个不动点，因此该距离应该很小，通常在 $\\varepsilon$ 的数量级。\n\n与陈述一致的实现细节：\n- 使用提供的 $(\\alpha,\\beta,\\delta,N_k,a,b)$，通过 $k^{\\star}$ 和在 $[a k^{\\star}, b k^{\\star}]$ 上的均匀间距来构建 $\\mathcal{K}$。\n- 安全地预计算 $U$，用一个非常大的负值代替 $-\\infty$ 以进行数值最大化，同时保证在存在可行条目时，这些条目永远不会被选中。\n- 使用指定的容差 $\\varepsilon = 10^{-5}$ 和迭代上限。对于方法 B，在每组 $m$ 次策略评估扫描后检验收敛性。\n- 对于每个测试案例，记录 $M_A$、$M_B$、布尔值 $(M_B < M_A)$ 以及两种方法所得最终值函数之间的上确界范数距离，并将结果格式化为单行的列表之列表。\n\n在所提供的测试案例中，预期的定性结果是：\n- 当 $m > 1$ 时，$M_B$ 通常严格小于 $M_A$，特别是当 $\\beta$ 接近 1 且 $N_k$ 较大时，这是因为策略评估扫描带来的好处更强。\n- 当 $m = 1$ 时，这两种方法变得非常相关，$M_B$ 预计将与 $M_A$ 相近。\n- 最终值函数之间的上确界范数距离预计会非常小，通常在 $\\varepsilon$ 的数量级，这证实了两种方法都在规定的容差内计算出了相同的不动点。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef steady_state_k(alpha, beta, delta):\n    # k* = (alpha / (1/beta - 1 + delta))^(1/(1-alpha))\n    num = alpha\n    den = (1.0 / beta) - 1.0 + delta\n    kstar = (num / den) ** (1.0 / (1.0 - alpha))\n    return kstar\n\ndef build_grid(kstar, a, b, Nk):\n    kmin = a * kstar\n    kmax = b * kstar\n    return np.linspace(kmin, kmax, Nk)\n\ndef precompute_utility_matrix(kgrid, alpha, delta):\n    # U[i, j] = log(c) if c > 0 else a large negative sentinel\n    # c = k_i^alpha + (1 - delta) * k_i - k'_j\n    k = kgrid\n    kp = kgrid\n    K = k[:, None]            # shape (N, 1)\n    KP = kp[None, :]          # shape (1, N)\n    output = (K ** alpha)\n    C = output + (1.0 - delta) * K - KP\n    # Use a large negative number as -inf sentinel\n    neg_inf = -1.0e12\n    with np.errstate(divide='ignore', invalid='ignore'):\n        U = np.where(C > 0.0, np.log(C), neg_inf)\n    return U  # shape (N, N)\n\ndef method_A_vfi(U, beta, tol, max_iter):\n    \"\"\"\n    Full maximization at every iteration:\n    V_{n+1}(i) = max_j { U[i,j] + beta * V_n[j] }\n    Returns: V, iterations (number of full maximization sweeps)\n    \"\"\"\n    N = U.shape[0]\n    V = np.zeros(N, dtype=float)\n    count = 0\n    for it in range(max_iter):\n        # Compute RHS matrix: U + beta * V_j (broadcast along rows)\n        RHS = U + beta * V[None, :]\n        V_new = RHS.max(axis=1)\n        count += 1\n        diff = np.max(np.abs(V_new - V))\n        V = V_new\n        if diff < tol:\n            break\n    return V, count\n\ndef method_B_policy_improvement(U, beta, tol, m_eval, max_improve):\n    \"\"\"\n    Alternating improvement and evaluation:\n    - Improvement: pi(i) = argmax_j { U[i,j] + beta * V[j] }\n    - Evaluation: V_{t+1}(i) = U[i, pi(i)] + beta * V_t[pi(i)]  (apply m_eval times)\n    Convergence test after each block of m_eval evaluation sweeps:\n    ||V_after - V_before||_inf < tol\n    Returns: V, improvements (number of improvement steps = full maximization sweeps)\n    \"\"\"\n    N = U.shape[0]\n    V = np.zeros(N, dtype=float)\n    improvements = 0\n    for imp in range(max_improve):\n        V_before = V.copy()\n        # Improvement step: one full maximization sweep\n        RHS = U + beta * V[None, :]\n        pi = np.argmax(RHS, axis=1)  # tie-break: first maximizer (np.argmax behavior)\n        # m_eval policy evaluation sweeps with fixed pi\n        idx = np.arange(N)\n        U_pi = U[idx, pi]  # immediate utility under policy\n        for _ in range(m_eval):\n            V = U_pi + beta * V[pi]\n        improvements += 1\n        # Convergence check after the block of evaluations\n        diff = np.max(np.abs(V - V_before))\n        if diff < tol:\n            break\n    return V, improvements\n\ndef solve():\n    # Tolerance and iteration caps\n    tol = 1e-5\n    max_iter_A = 10000\n    max_improve_B = 10000\n\n    # Define the test cases from the problem statement.\n    # Each tuple: (alpha, beta, delta, N_k, a, b, m_eval)\n    test_cases = [\n        (0.33, 0.96, 0.08, 200, 0.5, 1.5, 20),\n        (0.36, 0.99, 0.025, 300, 0.25, 1.75, 25),\n        (0.30, 0.95, 0.10, 80, 0.30, 1.70, 1),\n        (0.40, 0.97, 0.05, 250, 0.40, 1.60, 50),\n    ]\n\n    results = []\n    for (alpha, beta, delta, Nk, a, b, m_eval) in test_cases:\n        # Build grid and utilities\n        kstar = steady_state_k(alpha, beta, delta)\n        kgrid = build_grid(kstar, a, b, Nk)\n        U = precompute_utility_matrix(kgrid, alpha, delta)\n\n        # Method A\n        V_A, it_A = method_A_vfi(U, beta, tol, max_iter_A)\n\n        # Method B\n        V_B, imp_B = method_B_policy_improvement(U, beta, tol, m_eval, max_improve_B)\n\n        # Compare value functions\n        dist = float(np.max(np.abs(V_A - V_B)))\n\n        results.append([it_A, imp_B, imp_B < it_A, dist])\n\n    # Final print statement in the exact required format.\n    # Ensure booleans and floats/ints are properly represented.\n    # Convert nested lists to string representation as required.\n    def format_element(x):\n        if isinstance(x, bool):\n            return \"True\" if x else \"False\"\n        elif isinstance(x, float):\n            # Use repr to avoid excessive rounding; ensure it's a valid Python float literal\n            return repr(x)\n        else:\n            return str(x)\n\n    formatted_cases = []\n    for case in results:\n        formatted_cases.append(\"[\" + \",\".join(format_element(x) for x in case) + \"]\")\n    print(\"[\" + \",\".join(formatted_cases) + \"]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"}]}