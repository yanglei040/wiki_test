{"hands_on_practices": [{"introduction": "567我们从一个经典的微观经济学问题开始：一家公司在满足生产配额的同时，力求最小化生产成本。这个练习不仅仅是寻找最优的投入组合，它更挑战你去揭示生产约束本身所蕴含的隐含经济价值——这个概念被拉格朗日乘子（或称“影子价格”）优雅地捕捉到。通过解决这个问题，你将练习使用Karush-Kuhn-Tucker (KKT) 条件，并对对偶变量如何提供关键的商业洞察力获得深刻而实用的理解。[@problem_id:2384397]", "id": "2384397", "problem": "一家竞争性企业使用两种投入 $x_1$ 和 $x_2$（投入价格为 $w_1$ 和 $w_2$）来生产产出 $y$，其生产函数为 $f(x_1,x_2) = x_1^{1/2} x_2^{1/2}$。该企业必须满足至少 $q_0$ 单位的生产配额。企业选择投入以在满足配额的前提下最小化总成本。假设 $w_1 = \\$\\,4$ 每单位，$w_2 = \\$\\,1$ 每单位，以及产出配额 $q_0 = 10$ 单位。\n\n建立成本最小化问题及其带有单个不等式约束的拉格朗日函数，使用 Karush-Kuhn-Tucker (KKT) 条件推导一阶必要条件，并求解最优投入和相关的拉格朗日乘数。使用最优解处的对偶变量（拉格朗日乘数）来估计当生产配额从 $q_0$ 增加到 $q_0 + 1$（即增加一个单位）时，企业最小总成本的变化。用美元表示您的最终答案。提供一个单一的数值；无需四舍五入。", "solution": "对问题陈述进行验证。\n\n步骤1：提取已知条件。\n- 生产函数: $f(x_1,x_2) = x_1^{1/2} x_2^{1/2}$\n- 投入: $x_1$, $x_2$\n- 投入价格: $w_1 = 4$ 美元每单位, $w_2 = 1$ 美元每单位\n- 生产配额: 至少 $q_0 = 10$ 单位\n- 目标：最小化总成本, $C = w_1 x_1 + w_2 x_2$。\n- 任务：建立成本最小化问题及其拉格朗日函数，使用 Karush-Kuhn-Tucker (KKT) 条件推导一阶必要条件，求解最优投入和拉格朗日乘数，并使用该乘数估计配额增加一个单位时最小成本的变化。\n\n步骤2：使用提取的已知条件进行验证。\n- **科学依据：** 该问题是微观经济学理论中的一个标准练习，特别是企业理论部分。它使用了一个 Cobb-Douglas 生产函数，这是经济学中的一个典型模型。所有概念都已成熟。这在科学上是合理的。\n- **适定性：** 该问题是一个约束优化问题。目标函数是线性的（因此是凸的），而约束函数 $g(x_1, x_2) = x_1^{1/2} x_2^{1/2}$ 是拟凹函数，它定义了一个凸上水平集。这种结构确保了这是一个具有唯一解的适定最小化问题。\n- **客观性：** 该问题用精确、无歧义的数学和经济学术语表述。\n\n步骤3：结论与行动。\n该问题有效。它在科学上是合理的、适定的且客观的。我将继续进行求解。\n\n企业的问题是在其产出 $y = f(x_1, x_2)$ 至少为配额量 $q_0$ 的约束下，最小化其总成本 $C(x_1, x_2) = w_1 x_1 + w_2 x_2$。投入的非负性，$x_1 \\ge 0$ 和 $x_2 \\ge 0$，是生产函数的形式所隐含要求的。\n\n代入给定值，优化问题为：\n$$\n\\text{Minimize } C(x_1, x_2) = 4x_1 + x_2\n$$\n$$\n\\text{Subject to } x_1^{1/2} x_2^{1/2} \\ge 10\n$$\n\n为了应用 Karush-Kuhn-Tucker (KKT) 框架，我们将约束写成标准形式 $g(x) \\le 0$。\n约束条件 $x_1^{1/2} x_2^{1/2} \\ge 10$ 等价于 $10 - x_1^{1/2} x_2^{1/2} \\le 0$。\n\n拉格朗日函数 $\\mathcal{L}$ 的构造为：目标函数加上拉格朗日乘数 $\\lambda$ 与约束函数的乘积：\n$$\n\\mathcal{L}(x_1, x_2, \\lambda) = 4x_1 + x_2 + \\lambda(10 - x_1^{1/2} x_2^{1/2})\n$$\n\n最优解 $(x_1^*, x_2^*, \\lambda^*)$ 的 KKT 条件如下：\n1.  一阶条件（平稳性）：拉格朗日函数对选择变量的偏导数必须为零。\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial x_1} = 4 - \\lambda \\left( \\frac{1}{2} x_1^{-1/2} x_2^{1/2} \\right) = 0\n    $$\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial x_2} = 1 - \\lambda \\left( \\frac{1}{2} x_1^{1/2} x_2^{-1/2} \\right) = 0\n    $$\n2.  原始可行性：解必须满足原始约束。\n    $$\n    10 - x_1^{1/2} x_2^{1/2} \\le 0 \\quad \\text{or} \\quad x_1^{1/2} x_2^{1/2} \\ge 10\n    $$\n3.  对偶可行性：在最小化问题中，与“小于等于”不等式约束相关联的拉格朗日乘数必须为非负。\n    $$\n    \\lambda \\ge 0\n    $$\n4.  互补松弛性：在最优解处，要么约束是紧的，要么乘数为零（或两者兼有）。\n    $$\n    \\lambda (10 - x_1^{1/2} x_2^{1/2}) = 0\n    $$\n\n我们现在求解这个条件系统。\n从一阶条件出发，假设 $x_1 > 0$ 和 $x_2 > 0$：\n$$\n4 = \\frac{\\lambda}{2} \\sqrt{\\frac{x_2}{x_1}} \\quad (1)\n$$\n$$\n1 = \\frac{\\lambda}{2} \\sqrt{\\frac{x_1}{x_2}} \\quad (2)\n$$\n等式右侧为正，这意味着 $\\lambda$ 必须严格为正 ($\\lambda > 0$)。如果 $\\lambda$ 为零，一阶条件将要求 $4=0$ 和 $1=0$，这是荒谬的。\n由于 $\\lambda > 0$，互补松弛性条件 $\\lambda (10 - x_1^{1/2} x_2^{1/2}) = 0$ 意味着约束必须是紧的：\n$$\n10 - x_1^{1/2} x_2^{1/2} = 0 \\implies x_1^{1/2} x_2^{1/2} = 10 \\quad (3)\n$$\n现在，我们可以通过将方程 $(1)$ 除以方程 $(2)$ 来求解 $x_1$ 和 $x_2$ 之间的关系：\n$$\n\\frac{4}{1} = \\frac{\\frac{\\lambda}{2} \\sqrt{\\frac{x_2}{x_1}}}{\\frac{\\lambda}{2} \\sqrt{\\frac{x_1}{x_2}}} = \\frac{\\sqrt{x_2}}{\\sqrt{x_1}} \\cdot \\frac{\\sqrt{x_2}}{\\sqrt{x_1}} = \\frac{x_2}{x_1}\n$$\n这给出了企业的最优投入扩展路径：$x_2 = 4x_1$。\n\n将此关系代入紧约束，即方程 $(3)$：\n$$\n\\sqrt{x_1 (4x_1)} = 10\n$$\n$$\n\\sqrt{4x_1^2} = 10\n$$\n$$\n2x_1 = 10\n$$\n求解 $x_1$ 得到第一种投入的最优数量：\n$$\nx_1^* = 5\n$$\n现在，我们求第二种投入的最优数量：\n$$\nx_2^* = 4x_1^* = 4(5) = 20\n$$\n最优投入组合是 $(x_1^*, x_2^*) = (5, 20)$。\n\n最后，我们使用方程 $(2)$ 和最优投入来求解拉格朗日乘数 $\\lambda^*$：\n$$\n1 = \\frac{\\lambda^*}{2} \\sqrt{\\frac{x_1^*}{x_2^*}} = \\frac{\\lambda^*}{2} \\sqrt{\\frac{5}{20}} = \\frac{\\lambda^*}{2} \\sqrt{\\frac{1}{4}} = \\frac{\\lambda^*}{2} \\cdot \\frac{1}{2} = \\frac{\\lambda^*}{4}\n$$\n得出：\n$$\n\\lambda^* = 4\n$$\n最优拉格朗日乘数为 $\\lambda^*=4$。我们确认 $\\lambda^* \\ge 0$，满足对偶可行性条件。\n\n在成本最小化背景下，拉格朗日乘数 $\\lambda^*$ 的经济解释是约束的边际成本。它代表了企业最小成本相对于生产配额 $q_0$ 放宽的变化率。\n也就是说，$\\lambda^* = \\frac{dC^*(q_0)}{dq_0}$，其中 $C^*$ 是最小化后的成本。\n\n问题要求估计当生产配额增加一个单位，即从 $q_0 = 10$ 增加到 $q_0' = 11$ 时，最小总成本的变化。这个变化量 $\\Delta q_0$ 是 $11 - 10 = 1$。\n使用一阶线性近似，最小成本的变化 $\\Delta C^*$ 为：\n$$\n\\Delta C^* \\approx \\lambda^* \\cdot \\Delta q_0\n$$\n代入计算出的值：\n$$\n\\Delta C^* \\approx 4 \\cdot 1 = 4\n$$\n企业最小总成本的估计增加值为 $4$ 美元。\n初始最小成本为 $C^* = 4x_1^* + x_2^* = 4(5) + 20 = 20 + 20 = 40$。新的估计最小成本将是 $40+4=44$。", "answer": "$$\\boxed{4}$$"}, {"introduction": "接下来，我们将优化原理应用于金融世界，解决构建最低风险投资组合这一基本任务。这个实践将指导你最小化两种资产的投资组合方差，这是现代投资组合理论的基石。请密切关注结果，因为它揭示了一种关于风险分散和相关性关键作用的反直觉策略，证明了定量方法可以得出超越简单启发式思维的结论。[@problem_id:2384386]", "id": "2384386", "problem": "考虑两种风险资产，分别标记为资产 $A$ 和资产 $B$。它们的年化标准差分别为 $\\sigma_A = 0.20$ 和 $\\sigma_B = 0.50$。它们收益率之间的相关系数为 $\\rho = 0.95$。投资者可以采用任何满足 $w_A + w_B = 1$ 的实值投资组合权重 $(w_A, w_B)$；允许卖空。投资组合的方差为 $w^{\\top} \\Sigma w$，其中 $\\Sigma$ 是协方差矩阵，其元素为 $\\Sigma_{11} = \\sigma_A^2$，$\\Sigma_{22} = \\sigma_B^2$ 和 $\\Sigma_{12} = \\Sigma_{21} = \\rho \\sigma_A \\sigma_B$。\n\n将全局最小方差投资组合定义为满足 $w_A + w_B = 1$ 约束条件下，使得 $w^{\\top} \\Sigma w$ 最小化的全投资组合 $(w_A, w_B)$。\n\n在全局最小方差投资组合中，资产 $B$（方差较高的资产）的权重是多少？将您的答案四舍五入到四位有效数字。请以纯数字形式（无单位）表示您的答案。", "solution": "问题陈述已经过验证，并被认定为有效。它提出了一个源于现代投资组合理论领域的标准、适定的约束优化问题，该理论是计算金融学的核心组成部分。所有数据和条件均已提供，并且在科学上是合理的、一致的。\n\n目标是找到使投资组合方差 $V$ 最小化的投资组合权重 $(w_A, w_B)$，并满足投资组合被完全投资的约束条件。对于资产 $A$ 和 $B$，其投资组合方差由以下二次型给出：\n$$V(w_A, w_B) = w_A^2 \\sigma_A^2 + w_B^2 \\sigma_B^2 + 2 w_A w_B \\rho \\sigma_A \\sigma_B$$\n其中 $w_A$ 和 $w_B$ 分别是资产 $A$ 和资产 $B$ 的权重。这些权重受到以下线性约束：\n$$w_A + w_B = 1$$\n此约束使我们能够将问题简化为单变量的无约束优化问题。通过将 $w_A = 1 - w_B$ 代入方差方程，我们将方差 $V$ 表示为仅关于 $w_B$ 的函数：\n$$V(w_B) = (1 - w_B)^2 \\sigma_A^2 + w_B^2 \\sigma_B^2 + 2 (1 - w_B) w_B \\rho \\sigma_A \\sigma_B$$\n为找到使该方差最小的 $w_B$ 值，我们必须求出 $V(w_B)$ 对 $w_B$ 的一阶导数，并将其设为零，以找到临界点。这是微分学的一个直接应用。\n$$\\frac{d V}{d w_B} = \\frac{d}{d w_B} \\left[ (1 - 2w_B + w_B^2)\\sigma_A^2 + w_B^2 \\sigma_B^2 + (2w_B - 2w_B^2)\\rho \\sigma_A \\sigma_B \\right]$$\n逐项应用幂次法则进行微分可得：\n$$\\frac{d V}{d w_B} = (-2 + 2w_B)\\sigma_A^2 + 2w_B \\sigma_B^2 + (2 - 4w_B)\\rho \\sigma_A \\sigma_B$$\n将导数设为零以找到极值点：\n$$(-2 + 2w_B)\\sigma_A^2 + 2w_B \\sigma_B^2 + (2 - 4w_B)\\rho \\sigma_A \\sigma_B = 0$$\n现在我们通过组合含有 $w_B$ 的项来求解 $w_B$：\n$$2w_B\\sigma_A^2 - 2\\sigma_A^2 + 2w_B \\sigma_B^2 + 2\\rho \\sigma_A \\sigma_B - 4w_B\\rho \\sigma_A \\sigma_B = 0$$\n$$w_B(2\\sigma_A^2 + 2\\sigma_B^2 - 4\\rho \\sigma_A \\sigma_B) = 2\\sigma_A^2 - 2\\rho \\sigma_A \\sigma_B$$\n两边同除以 $2$ 以简化表达式：\n$$w_B(\\sigma_A^2 + \\sigma_B^2 - 2\\rho \\sigma_A \\sigma_B) = \\sigma_A^2 - \\rho \\sigma_A \\sigma_B$$\n分离出 $w_B$ 可得全局最小方差投资组合中资产 $B$ 权重的通用公式：\n$$w_B = \\frac{\\sigma_A^2 - \\rho \\sigma_A \\sigma_B}{\\sigma_A^2 + \\sigma_B^2 - 2\\rho \\sigma_A \\sigma_B}$$\n二阶导数 $\\frac{d^2 V}{d w_B^2} = 2(\\sigma_A^2 + \\sigma_B^2 - 2\\rho \\sigma_A \\sigma_B)$，由于 $|\\rho| < 1$，该值为正，这确认了所找到的临界点是一个全局最小值点。\n\n现在，我们代入给定的数值：$\\sigma_A = 0.20$，$\\sigma_B = 0.50$ 和 $\\rho = 0.95$。\n首先，我们计算方差和协方差：\n$$\\sigma_A^2 = (0.20)^2 = 0.04$$\n$$\\sigma_B^2 = (0.50)^2 = 0.25$$\n$$\\text{Cov}(A,B) = \\rho \\sigma_A \\sigma_B = (0.95)(0.20)(0.50) = 0.095$$\n将这些值代入推导出的 $w_B$ 公式中：\n$$w_B = \\frac{0.04 - 0.095}{0.04 + 0.25 - 2(0.095)}$$\n$$w_B = \\frac{-0.055}{0.29 - 0.19}$$\n$$w_B = \\frac{-0.055}{0.10} = -0.55$$\n题目要求答案四舍五入到四位有效数字。因此，资产 $B$ 的权重为 $-0.5500$。负值表示对资产 $B$ 持有空头头寸，这是问题陈述所允许的。因此，资产 A 的权重为 $w_A = 1 - w_B = 1 - (-0.55) = 1.55$。", "answer": "$$\n\\boxed{-0.5500}\n$$"}, {"introduction": "在探讨了解析解之后，我们现在转向优化的计算核心：寻找答案的算法。这个练习要求你实现并比较两种最重要的迭代方法——一阶的梯度下降法和二阶的牛顿法。通过观察它们在解决一个病态问题时的表现，你将亲身体会到为什么利用问题曲率信息（Hessian矩阵）的牛顿法，其收敛速度通常远快于仅使用斜率信息（梯度）的方法。[@problem_id:2384404]", "id": "2384404", "problem": "考虑在有限维欧几里得空间中最大化一个凹二次效用函数。设效用由 $u(\\mathbf{x}) = \\mathbf{b}^{\\top}\\mathbf{x} - \\tfrac{1}{2}\\mathbf{x}^{\\top}\\mathbf{Q}\\mathbf{x}$ 给出，其中 $\\mathbf{x} \\in \\mathbb{R}^{n}$，$\\mathbf{b} \\in \\mathbb{R}^{n}$，且 $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵。该效用函数是严格凹的，其负数定义了一个凸最小化问题。定义凸目标函数 $f(\\mathbf{x}) = -u(\\mathbf{x}) = \\tfrac{1}{2}\\mathbf{x}^{\\top}\\mathbf{Q}\\mathbf{x} - \\mathbf{b}^{\\top}\\mathbf{x}$。$f$ 的海森矩阵等于 $\\mathbf{Q}$，海森矩阵关于欧几里得范数的条件数为 $\\kappa(\\mathbf{Q}) = \\lambda_{\\max}(\\mathbf{Q})/\\lambda_{\\min}(\\mathbf{Q})$，其中 $\\lambda_{\\max}$ 和 $\\lambda_{\\min}$ 分别表示最大和最小特征值。您将比较求解 $\\min_{\\mathbf{x}} f(\\mathbf{x})$ 的两种算法的收敛行为：带有回溯线搜索的梯度下降法和带有回溯线搜索的牛顿法。\n\n仅从多元微积分和凸分析的核心定义（梯度作为偏导数向量，海森矩阵作为梯度的雅可比矩阵，无约束凸最小化的一阶最优性条件，以及 Armijo 充分下降条件）出发，推导为实现上述二次函数 $f(\\mathbf{x})$ 的两种算法所需的解析表达式。按如下方式实现这两种算法：\n\n- 带有回溯的梯度下降法：在迭代点 $\\mathbf{x}_{k}$ 处，使用负梯度计算下降方向，并执行满足 Armijo 充分下降条件的回溯线搜索来选择步长。更新迭代点并重复，直至收敛。\n- 带有回溯的牛顿法：在迭代点 $\\mathbf{x}_{k}$ 处，通过求解由海森矩阵和梯度定义的线性系统来计算牛顿方向，并执行满足 Armijo 充分下降条件的回溯线搜索来选择步长。更新迭代点并重复，直至收敛。\n\n两种方法使用以下共享参数：\n- Armijo 参数 $c_{1} = 10^{-4}$，\n- 回溯收缩因子 $\\tau = \\tfrac{1}{2}$，\n- 初始试探步长 $\\alpha_{0} = 1$，\n- 基于梯度欧几里得范数的停止容差：当 $\\lVert \\nabla f(\\mathbf{x}_{k}) \\rVert_{2} \\le \\varepsilon$ 且 $\\varepsilon = 10^{-6}$ 时停止，\n- 梯度下降法的最大迭代次数为 $200{,}000$ 次，牛顿法的最大迭代次数为 $1{,}000$ 次。\n\n测试套件。在以下四个测试用例上运行这两种方法，每个用例由 $(\\mathbf{Q}, \\mathbf{b}, \\mathbf{x}_{0})$ 指定，其中 $\\mathbf{x}_{0}$ 是起始点。\n\n- 用例 1（边界最优性，病态但平凡）：\n  $$\\mathbf{Q} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 100 \\end{bmatrix},\\quad \\mathbf{b} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad \\mathbf{x}_{0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}.$$\n- 用例 2（病态，中等程度）：\n  $$\\mathbf{Q} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 100 \\end{bmatrix},\\quad \\mathbf{b} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix},\\quad \\mathbf{x}_{0} = \\begin{bmatrix} 10 \\\\ -10 \\end{bmatrix}.$$\n- 用例 3（病态，程度更强）：\n  $$\\mathbf{Q} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 500 \\end{bmatrix},\\quad \\mathbf{b} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix},\\quad \\mathbf{x}_{0} = \\begin{bmatrix} 10 \\\\ -10 \\end{bmatrix}.$$\n- 用例 4（病态，程度进一步增强）：\n  $$\\mathbf{Q} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 2000 \\end{bmatrix},\\quad \\mathbf{b} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix},\\quad \\mathbf{x}_{0} = \\begin{bmatrix} 10 \\\\ -10 \\end{bmatrix}.$$\n\n对于每个用例，运行两种算法，直到满足停止准则或达到迭代次数上限。对于每个用例，返回迭代次数的有序对 $[\\text{iters\\_GD}, \\text{iters\\_Newton}]$，其中 $\\text{iters\\_GD}$ 是梯度下降法使用的迭代次数，$\\text{iters\\_Newton}$ 是牛顿法使用的迭代次数。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，每个测试用例对应一个子列表，顺序与上文相同。例如，输出格式必须为\n$$[\\,[\\text{iters\\_GD}^{(1)},\\text{iters\\_Newton}^{(1)}],\\,[\\text{iters\\_GD}^{(2)},\\text{iters\\_Newton}^{(2)}],\\,[\\text{iters\\_GD}^{(3)},\\text{iters\\_Newton}^{(3)}],\\,[\\text{iters\\_GD}^{(4)},\\text{iters\\_Newton}^{(4)}]\\,].$$\n输出中的所有数字必须是整数，且不应打印任何额外文本。", "solution": "对所给问题陈述进行验证。\n\n**步骤 1：提取已知条件**\n\n- **目标函数：** 最小化 $f(\\mathbf{x}) = \\tfrac{1}{2}\\mathbf{x}^{\\top}\\mathbf{Q}\\mathbf{x} - \\mathbf{b}^{\\top}\\mathbf{x}$，其中 $\\mathbf{x}, \\mathbf{b} \\in \\mathbb{R}^{n}$ 且 $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵。\n- **算法：** 梯度下降法和牛顿法，均采用回溯线搜索。\n- **线搜索参数：**\n    - Armijo 充分下降参数：$c_{1} = 10^{-4}$。\n    - 收缩因子：$\\tau = \\tfrac{1}{2}$。\n    - 初始步长：$\\alpha_{0} = 1$。\n- **终止条件：**\n    - 梯度范数停止容差：$\\lVert \\nabla f(\\mathbf{x}_{k}) \\rVert_{2} \\le \\varepsilon = 10^{-6}$。\n    - 梯度下降法最大迭代次数：$200,000$。\n    - 牛顿法最大迭代次数：$1,000$。\n- **测试用例：** 提供了四个用例，每个用例由元组 $(\\mathbf{Q}, \\mathbf{b}, \\mathbf{x}_{0})$ 定义。\n    1.  $\\mathbf{Q} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 100 \\end{bmatrix}, \\mathbf{b} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf{x}_{0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n    2.  $\\mathbf{Q} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 100 \\end{bmatrix}, \\mathbf{b} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\mathbf{x}_{0} = \\begin{bmatrix} 10 \\\\ -10 \\end{bmatrix}$。\n    3.  $\\mathbf{Q} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 500 \\end{bmatrix}, \\mathbf{b} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\mathbf{x}_{0} = \\begin{bmatrix} 10 \\\\ -10 \\end{bmatrix}$。\n    4.  $\\mathbf{Q} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 2000 \\end{bmatrix}, \\mathbf{b} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\mathbf{x}_{0} = \\begin{bmatrix} 10 \\\\ -10 \\end{bmatrix}$。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n该问题具有科学依据，是适定且客观的。它是数值凸优化中的一个标准问题，比较了一阶和二阶方法在凸二次函数上的性能。所有参数、初始条件和矩阵都已明确定义。矩阵 $\\mathbf{Q}$ 是对称的，并且其对角线元素为正，因此是正定的，这确保了 $f(\\mathbf{x})$ 的严格凸性以及唯一最小化子（解）的存在。用例 1 是一个平凡的边界情况，其起始点就是解，这对任何正确的实现都是一个有效的测试。该问题是完整的、一致的且科学合理的。\n\n**步骤 3：结论与行动**\n\n该问题是**有效的**。将提供一个解决方案。\n\n**推导与算法设计**\n\n目标是找到 $\\mathbf{x}^* = \\arg\\min_{\\mathbf{x} \\in \\mathbb{R}^n} f(\\mathbf{x})$，其中 $f(\\mathbf{x}) = \\tfrac{1}{2}\\mathbf{x}^{\\top}\\mathbf{Q}\\mathbf{x} - \\mathbf{b}^{\\top}\\mathbf{x}$。\n\n**一阶和二阶导数**\n为应用基于梯度的优化方法，我们首先推导 $f(\\mathbf{x})$ 的梯度和海森矩阵。使用矩阵微积分的基本法则：\n$f(\\mathbf{x})$ 的梯度是其偏导数的向量，$\\nabla f(\\mathbf{x}) = \\left[ \\frac{\\partial f}{\\partial x_i} \\right]_{i=1}^n$。\n$$\n\\nabla f(\\mathbf{x}) = \\nabla \\left( \\tfrac{1}{2}\\mathbf{x}^{\\top}\\mathbf{Q}\\mathbf{x} - \\mathbf{b}^{\\top}\\mathbf{x} \\right) = \\tfrac{1}{2}(\\mathbf{Q} + \\mathbf{Q}^{\\top})\\mathbf{x} - \\mathbf{b}\n$$\n因为 $\\mathbf{Q}$ 是对称的，$\\mathbf{Q} = \\mathbf{Q}^{\\top}$，梯度可简化为：\n$$\n\\nabla f(\\mathbf{x}) = \\mathbf{Q}\\mathbf{x} - \\mathbf{b}\n$$\n$f(\\mathbf{x})$ 的海森矩阵是二阶偏导数矩阵，$\\nabla^2 f(\\mathbf{x}) = \\left[ \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} \\right]_{i,j=1}^n$。它是梯度的雅可比矩阵：\n$$\n\\nabla^2 f(\\mathbf{x}) = \\nabla (\\mathbf{Q}\\mathbf{x} - \\mathbf{b}) = \\mathbf{Q}\n$$\n海森矩阵是常数，等于矩阵 $\\mathbf{Q}$。由于 $\\mathbf{Q}$ 是正定的，$f(\\mathbf{x})$ 是一个严格凸函数。\n\n**最优性条件**\n对于一个无约束凸优化问题，点 $\\mathbf{x}^*$ 成为全局最小化子（解的）一阶充要条件是梯度为零：\n$$\n\\nabla f(\\mathbf{x}^*) = \\mathbf{Q}\\mathbf{x}^* - \\mathbf{b} = \\mathbf{0}\n$$\n由于 $\\mathbf{Q}$ 是可逆的，唯一解由 $\\mathbf{x}^* = \\mathbf{Q}^{-1}\\mathbf{b}$ 给出。我们的任务是迭代地找到这个解。\n\n**迭代算法**\n两种算法都遵循更新规则 $\\mathbf{x}_{k+1} = \\mathbf{x}_{k} + \\alpha_k \\mathbf{p}_k$，其中 $\\mathbf{p}_k$ 是一个搜索方向，$\\alpha_k > 0$ 是由线搜索确定的步长。\n\n**1. 带有回溯线搜索的梯度下降法**\n- **搜索方向：** 最速下降方向是负梯度：\n  $$\n  \\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k) = -(\\mathbf{Q}\\mathbf{x}_k - \\mathbf{b}) = \\mathbf{b} - \\mathbf{Q}\\mathbf{x}_k\n  $$\n- **线搜索：** 步长 $\\alpha_k$ 通过回溯法找到。从初始试探步长 $\\alpha = \\alpha_0$ 开始，我们以因子 $\\tau$ 迭代地减小它，直到满足 Armijo 充分下降条件：\n  $$\n  f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha \\nabla f(\\mathbf{x}_k)^{\\top} \\mathbf{p}_k\n  $$\n  代入 $\\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$，该条件变为：\n  $$\n  f(\\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k)) \\le f(\\mathbf{x}_k) - c_1 \\alpha \\lVert \\nabla f(\\mathbf{x}_k) \\rVert_2^2\n  $$\n算法通过生成一系列迭代点 $\\{\\mathbf{x}_k\\}$ 来进行，直到 $\\lVert \\nabla f(\\mathbf{x}_k) \\rVert_2 \\le \\varepsilon$。已知梯度下降法的收敛速率会随着条件数 $\\kappa(\\mathbf{Q})$ 的增大而降低。\n\n**2. 带有回溯线搜索的牛顿法**\n- **搜索方向：** 牛顿法使用函数的二阶近似。牛顿方向 $\\mathbf{p}_k^{\\text{N}}$ 通过求解以下线性系统找到：\n  $$\n  \\nabla^2 f(\\mathbf{x}_k) \\mathbf{p}_k^{\\text{N}} = -\\nabla f(\\mathbf{x}_k)\n  $$\n  对于我们的二次目标函数，这变为：\n  $$\n  \\mathbf{Q} \\mathbf{p}_k^{\\text{N}} = -(\\mathbf{Q}\\mathbf{x}_k - \\mathbf{b})\n  $$\n  解出 $\\mathbf{p}_k^{\\text{N}}$ 可得：\n  $$\n  \\mathbf{p}_k^{\\text{N}} = -\\mathbf{Q}^{-1}(\\mathbf{Q}\\mathbf{x}_k - \\mathbf{b}) = \\mathbf{Q}^{-1}\\mathbf{b} - \\mathbf{x}_k = \\mathbf{x}^* - \\mathbf{x}_k\n  $$\n  牛顿方向直接从当前迭代点指向精确的最小化子（解）。\n- **线搜索：** 线搜索的执行方式与梯度下降法中相同。让我们分析初始试探步长 $\\alpha=1$ 时的 Armijo 条件：\n  $$\n  f(\\mathbf{x}_k + \\mathbf{p}_k^{\\text{N}}) \\le f(\\mathbf{x}_k) + c_1 \\nabla f(\\mathbf{x}_k)^{\\top} \\mathbf{p}_k^{\\text{N}}\n  $$\n  $f(\\mathbf{x}_k + \\alpha\\mathbf{p}_k^{\\text{N}})$ 在 $\\mathbf{x}_k$ 周围的泰勒展开对于二次函数是精确到二阶的：\n  $$\n  f(\\mathbf{x}_k + \\alpha\\mathbf{p}_k^{\\text{N}}) = f(\\mathbf{x}_k) + \\alpha \\nabla f(\\mathbf{x}_k)^{\\top}\\mathbf{p}_k^{\\text{N}} + \\frac{1}{2}\\alpha^2 (\\mathbf{p}_k^{\\text{N}})^{\\top}\\mathbf{Q}\\mathbf{p}_k^{\\text{N}}\n  $$\n  代入 $\\mathbf{Q}\\mathbf{p}_k^{\\text{N}} = -\\nabla f(\\mathbf{x}_k)$：\n  $$\n  f(\\mathbf{x}_k + \\alpha\\mathbf{p}_k^{\\text{N}}) = f(\\mathbf{x}_k) + \\alpha \\nabla f(\\mathbf{x}_k)^{\\top}\\mathbf{p}_k^{\\text{N}} - \\frac{1}{2}\\alpha^2 \\nabla f(\\mathbf{x}_k)^{\\top}\\mathbf{p}_k^{\\text{N}}\n  $$\n  Armijo 条件变为：\n  $$\n  f(\\mathbf{x}_k) + \\alpha \\nabla f(\\mathbf{x}_k)^{\\top}\\mathbf{p}_k^{\\text{N}} - \\frac{1}{2}\\alpha^2 \\nabla f(\\mathbf{x}_k)^{\\top}\\mathbf{p}_k^{\\text{N}} \\le f(\\mathbf{x}_k) + c_1 \\alpha \\nabla f(\\mathbf{x}_k)^{\\top} \\mathbf{p}_k^{\\text{N}}\n  $$\n  由于 $\\mathbf{p}_k^{\\text{N}}$ 是一个下降方向，$\\nabla f(\\mathbf{x}_k)^{\\top}\\mathbf{p}_k^{\\text{N}} = -(\\mathbf{p}_k^{\\text{N}})^{\\top}\\mathbf{Q}\\mathbf{p}_k^{\\text{N}} < 0$。我们可以用这个负项相除，并反转不等式：\n  $$\n  1 - \\frac{1}{2}\\alpha \\ge c_1 \\implies \\alpha \\le 2(1-c_1)\n  $$\n  当 $c_1=10^{-4}$ 时，条件是 $\\alpha \\le 2(1 - 10^{-4}) = 1.9998$。由于初始试探步长是 $\\alpha_0 = 1$，小于 $1.9998$，所以步长 $\\alpha_k=1$ 将总是被接受。\n  更新变为 $\\mathbf{x}_{k+1} = \\mathbf{x}_k + 1 \\cdot (\\mathbf{x}^* - \\mathbf{x}_k) = \\mathbf{x}^*$。\n  因此，对于任何非最优点的起始点，使用指定回溯参数的牛顿法将在单次迭代中收敛到精确解。\n\n**实现总结**\n该实现将包含两个函数，每种算法各一个。每个函数将根据其特定规则迭代更新解向量 $\\mathbf{x}$，并执行回溯线搜索以确定步长。当梯度的欧几里得范数低于容差 $\\varepsilon$ 或达到最大迭代次数时，循环终止。\n对于用例 1，由于 $\\mathbf{x}_0 = \\mathbf{0}$ 和 $\\mathbf{b} = \\mathbf{0}$，初始梯度 $\\nabla f(\\mathbf{x}_0) = \\mathbf{Q}\\mathbf{0} - \\mathbf{0} = \\mathbf{0}$。停止条件在开始时即已满足，因此两种算法都将报告 0 次迭代。\n对于用例 2、3 和 4，牛顿法预计需要 1 次迭代，而梯度下降法预计需要大量迭代，且迭代次数随 $\\mathbf{Q}$ 的条件数增加而增加。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the number of iterations required by Gradient Descent and Newton's Method\n    for a series of test cases on a convex quadratic objective function.\n    \"\"\"\n\n    def objective_function(x, Q, b):\n        \"\"\"Computes the value of f(x) = 0.5*x.T*Q*x - b.T*x.\"\"\"\n        return 0.5 * x.T @ Q @ x - b.T @ x\n\n    def gradient(x, Q, b):\n        \"\"\"Computes the gradient of f(x).\"\"\"\n        return Q @ x - b\n\n    def gradient_descent(Q, b, x0, c1, tau, alpha0, tol, max_iter):\n        \"\"\"\n        Performs gradient descent with backtracking line search.\n        \n        Returns the number of iterations.\n        \"\"\"\n        x = np.copy(x0).astype(float)\n        iters = 0\n        \n        while iters < max_iter:\n            grad = gradient(x, Q, b)\n            \n            if np.linalg.norm(grad) <= tol:\n                return iters\n            \n            p = -grad  # Descent direction\n            \n            # Backtracking line search\n            alpha = alpha0\n            f_x = objective_function(x, Q, b)\n            grad_p_dot = grad.T @ p\n            \n            while True:\n                x_new = x + alpha * p\n                f_x_new = objective_function(x_new, Q, b)\n                if f_x_new <= f_x + c1 * alpha * grad_p_dot:\n                    break\n                alpha *= tau\n            \n            x = x + alpha * p\n            iters += 1\n            \n        return iters\n\n    def newton_method(Q, b, x0, c1, tau, alpha0, tol, max_iter):\n        \"\"\"\n        Performs Newton's method with backtracking line search.\n\n        Returns the number of iterations.\n        \"\"\"\n        x = np.copy(x0).astype(float)\n        iters = 0\n        \n        while iters < max_iter:\n            grad = gradient(x, Q, b)\n\n            if np.linalg.norm(grad) <= tol:\n                return iters\n            \n            # Newton direction: solve H*p = -g, where H = Q\n            p = np.linalg.solve(Q, -grad)\n            \n            # Backtracking line search\n            alpha = alpha0\n            f_x = objective_function(x, Q, b)\n            grad_p_dot = grad.T @ p\n            \n            while True:\n                x_new = x + alpha * p\n                f_x_new = objective_function(x_new, Q, b)\n                if f_x_new <= f_x + c1 * alpha * grad_p_dot:\n                    break\n                alpha *= tau\n            \n            x = x + alpha * p\n            iters += 1\n            \n        return iters\n\n    # Shared parameters\n    C1 = 1e-4\n    TAU = 0.5\n    ALPHA0 = 1.0\n    TOL = 1e-6\n    MAX_ITER_GD = 200000\n    MAX_ITER_NEWTON = 1000\n\n    # Test suite\n    test_cases = [\n        # Case 1\n        (np.array([[1, 0], [0, 100]]), np.array([0, 0]), np.array([0, 0])),\n        # Case 2\n        (np.array([[1, 0], [0, 100]]), np.array([1, 1]), np.array([10, -10])),\n        # Case 3\n        (np.array([[1, 0], [0, 500]]), np.array([1, 1]), np.array([10, -10])),\n        # Case 4\n        (np.array([[1, 0], [0, 2000]]), np.array([1, 1]), np.array([10, -10])),\n    ]\n\n    results = []\n    for Q, b, x0 in test_cases:\n        iters_gd = gradient_descent(Q, b, x0, C1, TAU, ALPHA0, TOL, MAX_ITER_GD)\n        iters_newton = newton_method(Q, b, x0, C1, TAU, ALPHA0, TOL, MAX_ITER_NEWTON)\n        results.append([iters_gd, iters_newton])\n\n    # Format the output string as required\n    formatted_results = [f\"[{gd},{nm}]\" for gd, nm in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"}]}
