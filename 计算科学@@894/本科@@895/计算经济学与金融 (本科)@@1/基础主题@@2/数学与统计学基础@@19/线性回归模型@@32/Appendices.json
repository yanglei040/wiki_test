{"hands_on_practices": [{"introduction": "在我们依赖软件自动完成回归分析之前，理解普通最小二乘法（OLS）的核心机理至关重要。本练习将引导你通过基本的矩阵运算，针对一个给定的数据集手工计算回归系数。通过这个过程，你将加深对OLS数学基础的理解，并体会到设计矩阵的结构如何影响计算的简便性 [@problem_id:1938980]。", "id": "1938980", "problem": "一位食品科学家正在开发一种新型烘焙零食，并希望了解烘焙条件如何影响其酥脆度。酥脆度在一个定量尺度上进行测量。这位科学家用四个批次进行了一个小型实验，改变了两个因素：烘焙温度和湿度。这些因素由编码变量表示，其中 $-1$ 代表低设置，$+1$ 代表高设置。\n\n提出的统计模型是以下形式的多元线性回归模型：\n$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon$$\n其中：\n- $y$ 是酥脆度得分。\n- $x_1$ 是烘焙温度的编码变量。\n- $x_2$ 是湿度的编码变量。\n- $\\beta_0$、$\\beta_1$ 和 $\\beta_2$ 是未知的模型系数。\n- $\\epsilon$ 是随机误差项。\n\n四个实验批次的结果如下：\n- 批次 1：温度代码 ($x_1$) = -1，湿度代码 ($x_2$) = -1，酥脆度 ($y$) = 2。\n- 批次 2：温度代码 ($x_1$) = -1，湿度代码 ($x_2$) = 1，酥脆度 ($y$) = 4。\n- 批次 3：温度代码 ($x_1$) = 1，湿度代码 ($x_2$) = -1，酥脆度 ($y$) = 6。\n- 批次 4：温度代码 ($x_1$) = 1，湿度代码 ($x_2$) = 1，酥脆度 ($y$) = 8。\n\n使用最小二乘法，确定系数的估计值。将您的答案表示为包含三个估计系数 $(\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2)$ 的单行矩阵。", "solution": "目标是求出多元线性回归模型中系数 $\\beta_0$、$\\beta_1$ 和 $\\beta_2$ 的最小二乘估计值。该模型可以写成矩阵形式 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$，其中 $\\mathbf{y}$ 是响应向量，$\\mathbf{X}$ 是设计矩阵，$\\boldsymbol{\\beta}$ 是系数向量，$\\boldsymbol{\\epsilon}$ 是误差向量。\n\n首先，我们根据给定的实验数据构建响应向量 $\\mathbf{y}$ 和设计矩阵 $\\mathbf{X}$。设计矩阵包含一列全为1的列，用于截距项 $\\beta_0$。\n\n响应向量 $\\mathbf{y}$ 包含酥脆度得分：\n$$\n\\mathbf{y} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\\\ 8 \\end{pmatrix}\n$$\n\n设计矩阵 $\\mathbf{X}$ 由一个用于截距的前导全1列和编码变量 $x_1$ 和 $x_2$ 的列构成：\n$$\n\\mathbf{X} = \\begin{pmatrix} 1 & x_{11} & x_{21} \\\\ 1 & x_{12} & x_{22} \\\\ 1 & x_{13} & x_{23} \\\\ 1 & x_{14} & x_{24} \\end{pmatrix} = \\begin{pmatrix} 1 & -1 & -1 \\\\ 1 & -1 & 1 \\\\ 1 & 1 & -1 \\\\ 1 & 1 & 1 \\end{pmatrix}\n$$\n\n待估计的系数向量是 $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\beta_2)^T$。最小二乘估计值，记作 $\\hat{\\boldsymbol{\\beta}}$，由正规方程组 $\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{y}$ 的解给出。假设 $\\mathbf{X}^T\\mathbf{X}$ 是可逆的，则解为：\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\n\n我们将分几个步骤计算。首先，我们求 $\\mathbf{X}$ 的转置：\n$$\n\\mathbf{X}^T = \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ -1 & -1 & 1 & 1 \\\\ -1 & 1 & -1 & 1 \\end{pmatrix}\n$$\n\n接下来，我们计算乘积 $\\mathbf{X}^T\\mathbf{X}$：\n$$\n\\mathbf{X}^T\\mathbf{X} = \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ -1 & -1 & 1 & 1 \\\\ -1 & 1 & -1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & -1 & -1 \\\\ 1 & -1 & 1 \\\\ 1 & 1 & -1 \\\\ 1 & 1 & 1 \\end{pmatrix}\n$$\n结果矩阵的元素为：\n- $(1,1): 1 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1 = 4$\n- $(1,2): 1 \\cdot (-1) + 1 \\cdot (-1) + 1 \\cdot 1 + 1 \\cdot 1 = 0$\n- $(1,3): 1 \\cdot (-1) + 1 \\cdot 1 + 1 \\cdot (-1) + 1 \\cdot 1 = 0$\n- $(2,1): (-1) \\cdot 1 + (-1) \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1 = 0$\n- $(2,2): (-1) \\cdot (-1) + (-1) \\cdot (-1) + 1 \\cdot 1 + 1 \\cdot 1 = 4$\n- $(2,3): (-1) \\cdot (-1) + (-1) \\cdot 1 + 1 \\cdot (-1) + 1 \\cdot 1 = 0$\n- $(3,1): (-1) \\cdot 1 + 1 \\cdot 1 + (-1) \\cdot 1 + 1 \\cdot 1 = 0$\n- $(3,2): (-1) \\cdot (-1) + 1 \\cdot (-1) + (-1) \\cdot 1 + 1 \\cdot 1 = 0$\n- $(3,3): (-1) \\cdot (-1) + 1 \\cdot 1 + (-1) \\cdot (-1) + 1 \\cdot 1 = 4$\n\n因此，该矩阵是对角矩阵：\n$$\n\\mathbf{X}^T\\mathbf{X} = \\begin{pmatrix} 4 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix} = 4\\mathbf{I}_3\n$$\n其中 $\\mathbf{I}_3$ 是 $3 \\times 3$ 单位矩阵。\n\n这个对角矩阵的逆矩阵很容易计算：\n$$\n(\\mathbf{X}^T\\mathbf{X})^{-1} = \\frac{1}{4}\\mathbf{I}_3 = \\begin{pmatrix} \\frac{1}{4} & 0 & 0 \\\\ 0 & \\frac{1}{4} & 0 \\\\ 0 & 0 & \\frac{1}{4} \\end{pmatrix}\n$$\n\n现在，我们计算乘积 $\\mathbf{X}^T\\mathbf{y}$：\n$$\n\\mathbf{X}^T\\mathbf{y} = \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ -1 & -1 & 1 & 1 \\\\ -1 & 1 & -1 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\\\ 8 \\end{pmatrix} = \\begin{pmatrix} 1(2)+1(4)+1(6)+1(8) \\\\ -1(2)-1(4)+1(6)+1(8) \\\\ -1(2)+1(4)-1(6)+1(8) \\end{pmatrix} = \\begin{pmatrix} 20 \\\\ 8 \\\\ 4 \\end{pmatrix}\n$$\n\n最后，我们将 $(\\mathbf{X}^T\\mathbf{X})^{-1}$ 与 $\\mathbf{X}^T\\mathbf{y}$ 相乘来求得 $\\hat{\\boldsymbol{\\beta}}$：\n$$\n\\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} & 0 & 0 \\\\ 0 & \\frac{1}{4} & 0 \\\\ 0 & 0 & \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 20 \\\\ 8 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4}(20) \\\\ \\frac{1}{4}(8) \\\\ \\frac{1}{4}(4) \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 2 \\\\ 1 \\end{pmatrix}\n$$\n\n最小二乘估计值为 $\\hat{\\beta}_0 = 5$、$\\hat{\\beta}_1 = 2$ 和 $\\hat{\\beta}_2 = 1$。问题要求将答案表示为行矩阵 $(\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2)$。", "answer": "$$\n\\boxed{\\begin{pmatrix} 5 & 2 & 1 \\end{pmatrix}}\n$$"}, {"introduction": "在经济与金融领域，我们处理的数据常常包含分类变量，例如市场情绪（牛市、熊市、盘整）或行业分类。在回归中，我们通常使用虚拟变量来表示这些分类。然而，不当的设定会导致完全多重共线性，这个问题被称为“虚拟变量陷阱”。本练习将通过编程实践，让你直观地看到这个陷阱是如何产生的，并掌握通过设置基准类别来构建正确设计矩阵的方法 [@problem_id:2407226]。", "id": "2407226", "problem": "考虑一个用于计算经济学和计算金融的横截面线性回归模型，其中截距项和分类特征通过虚拟变量进行编码。假设有 $N$ 个观测值，一个分类回归量，其取值于一个包含 $K$ 个互斥且穷尽类别的有限集合，一个设计矩阵 $X \\in \\mathbb{R}^{N \\times p}$ 由一列全为 1 的向量（截距项）和一组类别的虚拟变量列组成。格拉姆矩阵为 $X^{\\prime}X \\in \\mathbb{R}^{p \\times p}$。一个方阵是奇异的，当且仅当它没有逆矩阵，等价地，当且仅当其列是线性相关的。\n\n您的任务是，对于每个指定的测试用例，判断按规定方式构建的包含截距项和虚拟变量的设计矩阵是否会导致一个奇异的 $X^{\\prime}X$。每个测试用例都指定了：一个类别标签序列（每个观测值一个），是为所有类别都包含虚拟变量，还是为了作为基准而省略其中一个类别，以及在适用情况下省略哪个基准类别。请完全按照所提供的类别标签使用。所有运算都是纯代数运算；不涉及物理单位或角度。\n\n测试套件：\n- 情况 $1$（正常路径）：$N=6$，每个观测值的类别为 [\"Bull\",\"Bear\",\"Sideways\",\"Bull\",\"Bear\",\"Sideways\"]。构建 $X$，包含一个截距项以及除基准类别 \"Bear\" 之外的所有类别（即 $K-1$ 个虚拟变量）的虚拟变量。输出 $X^{\\prime}X$ 是否奇异。\n- 情况 $2$（虚拟变量陷阱）：类别与情况 $1$ 相同。构建 $X$，包含一个截距项以及所有 $K$ 个类别的虚拟变量。输出 $X^{\\prime}X$ 是否奇异。\n- 情况 $3$（无陷阱的边界情况）：$N=4$，每个观测值的类别为 [\"Bull\",\"Bull\",\"Bull\",\"Bull\"]。构建 $X$，包含一个截距项以及除基准类别 \"Bull\" 之外的所有类别的虚拟变量。输出 $X^{\\prime}X$ 是否奇异。\n- 情况 $4$（有陷阱的边界情况）：类别与情况 $3$ 相同。构建 $X$，包含一个截距项以及所有 $K$ 个类别的虚拟变量。输出 $X^{\\prime}X$ 是否奇异。\n\n对于每种情况，所需的输出是一个布尔值，指示 $X^{\\prime}X$ 是否是奇异的。您的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表（例如，`[result1,result2,result3,result4]`）。", "solution": "所提供的问题已经过严格验证，并被认为是有效的。它在科学上基于线性代数和计量经济学的原理，问题本身是适定的，每个案例都有唯一且可验证的解，并使用客观、无歧义的语言进行阐述。构建每个测试用例的设计矩阵所需的所有数据均已提供。\n\n问题的核心是确定格拉姆矩阵 $X^{\\prime}X$ 的奇异性。线性代数的一个基本定理指出，格拉姆矩阵 $X^{\\prime}X$ 是奇异的，当且仅当设计矩阵 $X$ 的列是线性相关的。一个矩阵 $X \\in \\mathbb{R}^{N \\times p}$ 的列是线性相关的，如果存在列向量 $\\{\\mathbf{x}_0, \\mathbf{x}_1, \\dots, \\mathbf{x}_{p-1}\\}$ 的一个非平凡线性组合等于零向量，即，对于某些不全为零的标量系数 $\\{c_0, c_1, \\dots, c_{p-1}\\}$，有 $c_0\\mathbf{x}_0 + c_1\\mathbf{x}_1 + \\dots + c_{p-1}\\mathbf{x}_{p-1} = \\mathbf{0}$。一个等价条件是矩阵的秩 $\\text{rank}(X)$ 严格小于其列数 $p$。\n\n被称为“虚拟变量陷阱”的现象就是这种线性相关的一个特例。当一个模型包含一个截距项（一个全为 1 的列向量，记作 $\\mathbf{1}_N$）并且还为一个分类变量的所有 $K$ 个互斥且穷尽的类别都包含虚拟变量时，就会发生这种情况。对于任何给定的观测值，虚拟变量中将恰好有一个为 1，其余为 0。因此，$K$ 个虚拟变量列之和 $\\sum_{j=1}^{K} D_j$ 产生一个每个条目都为 1 的列向量。因此，这个和与截距列 $\\mathbf{1}_N$ 完全相同。这就产生了线性相关性 $\\mathbf{1}_N - \\sum_{j=1}^{K} D_j = \\mathbf{0}$，证明了 $X$ 的列是线性相关的，因此 $X^{\\prime}X$ 是奇异的。避免这种多重共线性的标准做法是包含一个截距项和仅 $K-1$ 个虚拟变量，留下一个类别作为基准参考。\n\n我们现在将基于这一原则分析每种情况。\n\n情况 $1$：\n这里，$N=6$。分类变量有 $K=3$ 个水平：\"Bull\"、\"Bear\"、\"Sideways\"。设计矩阵 $X$ 由一个截距项和 $K-1=2$ 个虚拟变量构成，省略了基准类别 \"Bear\" 的虚拟变量。因此，$X$ 的列是：一个截距列 $\\mathbf{1}_6$，一个 \"Bull\" 的虚拟列（$D_{Bull}$），以及一个 \"Sideways\" 的虚拟列（$D_{Side}$）。列数为 $p=3$。\n矩阵 $X$ 是：\n$$\nX = \n\\begin{pmatrix}\n1 & 1 & 0 \\\\\n1 & 0 & 0 \\\\\n1 & 0 & 1 \\\\\n1 & 1 & 0 \\\\\n1 & 0 & 0 \\\\\n1 & 0 & 1\n\\end{pmatrix}\n$$\n这 $3$ 列是线性无关的。虚拟变量列之和 $D_{Bull} + D_{Side}$ 不等于截距列。没有任何一列是其他列的线性组合。因此，$\\text{rank}(X) = 3 = p$。矩阵 $X^{\\prime}X$ 是非奇异的。结果是 False。\n\n情况 $2$：\n此情况使用与情况 $1$ 相同的数据（$N=6$, $K=3$），但现在设计矩阵 $X$ 由一个截距项和所有 $K=3$ 个类别的虚拟变量构成。这些列是：$\\mathbf{1}_6$、$D_{Bull}$、$D_{Bear}$ 和 $D_{Side}$。列数为 $p=4$。\n矩阵 $X$ 是：\n$$\nX = \n\\begin{pmatrix}\n1 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 1 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 1\n\\end{pmatrix}\n$$\n正如虚拟变量陷阱所解释的，虚拟变量列的总和等于截距列：$D_{Bull} + D_{Bear} + D_{Side} = \\mathbf{1}_6$。这产生了线性相关性 $\\mathbf{1}_6 - D_{Bull} - D_{Bear} - D_{Side} = \\mathbf{0}$。$X$ 的列是线性相关的。因此，$\\text{rank}(X)=3 < p=4$。矩阵 $X^{\\prime}X$ 是奇异的。结果是 True。\n\n情况 $3$：\n这里，$N=4$ 且所有观测值都属于单一类别 \"Bull\"，所以 $K=1$。矩阵 $X$ 由一个截距项构成，并包含除基准类别 \"Bull\" 之外的所有类别的虚拟变量。由于只有 $1$ 个类别且它就是基准类别，所以不包含任何虚拟变量。矩阵 $X$ 仅包含截距列。列数为 $p=1$。\n矩阵 $X$ 是：\n$$\nX = \n\\begin{pmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n1\n\\end{pmatrix}\n$$\n这是一个 $N \\times 1$ 的矩阵。根据定义，单个非零列是线性无关的。秩为 $\\text{rank}(X) = 1 = p$。对应的格拉姆矩阵是 $X^{\\prime}X = [4]$，这是一个非零的 $1 \\times 1$ 矩阵，因此是可逆的。矩阵 $X^{\\prime}X$ 是非奇异的。结果是 False。\n\n情况 $4$：\n此情况使用与情况 $3$ 相同的数据（$N=4$, $K=1$），但 $X$ 由一个截距项和所有 $K=1$ 个类别的虚拟变量构成。这意味着我们包含一个截距列和一个 \"Bull\" 的虚拟列。列数为 $p=2$。\n矩阵 $X$ 是：\n$$\nX = \n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 1 \\\\\n1 & 1\n\\end{pmatrix}\n$$\n第一列（截距项）与第二列（\"Bull\" 的虚拟变量）相同，因为每个观测值都属于 \"Bull\" 类别。这产生了线性相关性 $\\mathbf{x}_1 - \\mathbf{x}_2 = \\mathbf{0}$。这些列是线性相关的。秩为 $\\text{rank}(X)=1 < p=2$。矩阵 $X^{\\prime}X$ 是奇异的。结果是 True。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef create_design_matrix(observations, all_dummies, baseline_category=None):\n    \"\"\"\n    Constructs the design matrix X based on the problem specifications.\n\n    Args:\n        observations (list): A list of category labels for each observation.\n        all_dummies (bool): If True, include dummies for all categories.\n                              If False, omit the baseline category's dummy.\n        baseline_category (str, optional): The category to omit when all_dummies is False.\n\n    Returns:\n        numpy.ndarray: The constructed design matrix X.\n    \"\"\"\n    N = len(observations)\n    if N == 0:\n        return np.empty((0, 0))\n\n    # Use a sorted list of unique categories for consistent column ordering.\n    unique_categories = sorted(list(set(observations)))\n    \n    # Start with the intercept column, which is a column of ones.\n    X_cols = [np.ones((N, 1))]\n\n    if all_dummies:\n        categories_to_include = unique_categories\n    else:\n        categories_to_include = [cat for cat in unique_categories if cat != baseline_category]\n\n    for cat in categories_to_include:\n        # Create a dummy variable column for the category.\n        dummy_col = np.array([1 if obs == cat else 0 for obs in observations]).reshape(N, 1)\n        X_cols.append(dummy_col)\n\n    # hstack requires at least one array. The intercept guarantees this.\n    return np.hstack(X_cols)\n\ndef is_gram_matrix_singular(X):\n    \"\"\"\n    Determines if the Gram matrix X'X is singular.\n\n    This is equivalent to checking if the columns of X are linearly dependent.\n    Linear dependence is present if the rank of X is less than the number of columns.\n\n    Args:\n        X (numpy.ndarray): The design matrix.\n\n    Returns:\n        bool: True if X'X is singular, False otherwise.\n    \"\"\"\n    # The number of columns in the design matrix.\n    p = X.shape[1]\n    \n    # A matrix with 0 columns is non-singular by a pragmatic definition for this problem's context.\n    if p == 0:\n        return False\n        \n    # Calculate the rank of the matrix X.\n    # The rank of X'X is equal to the rank of X.\n    rank_of_X = np.linalg.matrix_rank(X)\n    \n    # The columns are linearly dependent if the rank is less than the number of columns.\n    return rank_of_X < p\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path): N=6, K=3, baseline \"Bear\"\n        {\n            \"observations\": [\"Bull\", \"Bear\", \"Sideways\", \"Bull\", \"Bear\", \"Sideways\"],\n            \"all_dummies\": False,\n            \"baseline\": \"Bear\",\n        },\n        # Case 2 (dummy variable trap): N=6, K=3, all dummies included\n        {\n            \"observations\": [\"Bull\", \"Bear\", \"Sideways\", \"Bull\", \"Bear\", \"Sideways\"],\n            \"all_dummies\": True,\n            \"baseline\": None,\n        },\n        # Case 3 (boundary without trap): N=4, K=1, baseline \"Bull\"\n        {\n            \"observations\": [\"Bull\", \"Bull\", \"Bull\", \"Bull\"],\n            \"all_dummies\": False,\n            \"baseline\": \"Bull\",\n        },\n        # Case 4 (boundary with trap): N=4, K=1, all dummies included\n        {\n            \"observations\": [\"Bull\", \"Bull\", \"Bull\", \"Bull\"],\n            \"all_dummies\": True,\n            \"baseline\": None,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        X = create_design_matrix(case[\"observations\"], case[\"all_dummies\"], case[\"baseline\"])\n        result = is_gram_matrix_singular(X)\n        results.append(result)\n\n    # Format the final output as a comma-separated list of booleans in brackets.\n    # The string representation of a boolean in Python is \"True\" or \"False\".\n    # The problem asks for boolean output, so this representation is appropriate.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"}, {"introduction": "经典线性回归模型的一个关键假设是同方差性（homoskedasticity），即误差项的方差是恒定的。然而，在经济和金融数据中，这一假设常被违反，出现所谓的异方差性（heteroskedasticity），这会使标准的假设检验（如 $t$ 检验）失效。本练习将让你通过编程，对比传统OLS标准误与怀特（White）异方差稳健标准误，从而理解在异方差存在的情况下，我们应当如何做出更可靠的统计推断 [@problem_id:2407232]。", "id": "2407232", "problem": "考虑一个带截距项的线性模型，其中因变量表示为 $y \\in \\mathbb{R}^n$，回归量矩阵（包含一列代表截距项的全1向量）表示为 $X \\in \\mathbb{R}^{n \\times k}$。令 $\\hat{\\beta} \\in \\mathbb{R}^k$ 为通过最小化残差平方和定义的普通最小二乘法（OLS）估计量。将常规（基于同方差性）的协方差估计量和异方差一致性协方差估计量（White 的异方差一致性协方差估计量，HC0）定义如下。对于残差 $\\hat{u} = y - X \\hat{\\beta}$，令 $\\hat{\\sigma}^2 = \\hat{u}^\\top \\hat{u} / (n - k)$，并令 $V_{\\text{conv}} = \\hat{\\sigma}^2 (X^\\top X)^{-1}$ 以及 $V_{\\text{HC0}} = (X^\\top X)^{-1} \\left( X^\\top \\operatorname{diag}(\\hat{u}_1^2,\\dots,\\hat{u}_n^2) X \\right) (X^\\top X)^{-1}$。对于任意系数索引 $j \\in \\{1,\\dots,k\\}$，将相应的标准误定义为 $se_{\\text{conv},j} = \\sqrt{(V_{\\text{conv}})_{jj}}$ 和 $se_{\\text{HC0},j} = \\sqrt{(V_{\\text{HC0}})_{jj}}$。使用自由度为 $n-k$ 的学生t分布，将用于检验原假设 $H_0: \\beta_j = 0$ 的双边p值定义为 $p = 2 \\left( 1 - F_{t, n-k} \\left( | \\hat{\\beta}_j / se_j | \\right) \\right)$，其中 $F_{t, n-k}$ 表示自由度为 $n-k$ 的学生t分布的累积分布函数，$se_j$ 是在指定协方差估计量下的标准误。\n\n您将获得三个测试用例。在所有用例中，都必须通过向回归量矩阵中增加一列全1向量来包含截距项。在下文每个用例中，用于报告结果的索引 $j$ 是第一个非恒定回归量，即截距项列之后 $X$ 的第一列所对应的回归量。\n\n测试套件：\n- 用例 1（构造的异方差性简单回归）：\n  - 对于 $i \\in \\{1,2,\\dots,10\\}$，定义 $x_i = i$。\n  - 当 $i$ 为奇数时定义 $s_i = 1$，当 $i$ 为偶数时定义 $s_i = -1$。\n  - 定义 $y_i = 1.5 + 0.8 x_i + 0.3 x_i s_i$。\n  - 回归量矩阵 $X$ 的列为 $[\\,\\mathbf{1},\\, x\\,]$，其中 $\\mathbf{1}$ 是全1向量，$x$ 是以 $x_i$ 为元素的向量。\n- 用例 2（同方差符号翻转误差的简单回归）：\n  - 对于 $i \\in \\{1,2,\\dots,10\\}$，定义 $x_i = i$。\n  - 当 $i$ 为奇数时定义 $s_i = 1$，当 $i$ 为偶数时定义 $s_i = -1$。\n  - 定义 $y_i = -0.5 + 2.0 x_i + 0.5 s_i$。\n  - 回归量矩阵 $X$ 的列为 $[\\,\\mathbf{1},\\, x\\,]$。\n- 用例 3（构造的近似共线性和异方差性的多元回归）：\n  - 对于 $i \\in \\{1,2,\\dots,8\\}$，定义 $x^{(1)}_i = i$。\n  - 当 $i$ 为奇数时定义 $s_i = 1$，当 $i$ 为偶数时定义 $s_i = -1$。\n  - 定义 $x^{(2)}_i = x^{(1)}_i + 0.1 s_i$。\n  - 定义 $y_i = 0.5 + 1.0 x^{(1)}_i - 0.5 x^{(2)}_i + 0.2 x^{(1)}_i s_i$。\n  - 回归量矩阵 $X$ 的列为 $[\\,\\mathbf{1},\\, x^{(1)},\\, x^{(2)}\\,]$。\n\n对于每个用例，计算第一个非恒定回归量（即用例1和2中 $x$ 的系数，以及用例3中 $x^{(1)}$ 的系数）的以下四个量：\n$se_{\\text{conv}}$、$se_{\\text{HC0}}$、$p_{\\text{conv}}$ 和 $p_{\\text{HC0}}$。\n\n要求：\n- 使用上述定义来获得 $\\hat{\\beta}$、$se_{\\text{conv}}$、$se_{\\text{HC0}}$、$p_{\\text{conv}}$ 和 $p_{\\text{HC0}}$。在为双边p值评估学生t分布的累积分布函数时，应使用 $n-k$ 的自由度。\n- 数值输出规范：将每个报告的实数四舍五入到 $6$ 位小数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。该列表必须按顺序包含用例1、用例2、用例3的四个值 $[se_{\\text{conv}}, se_{\\text{HC0}}, p_{\\text{conv}}, p_{\\text{HC0}}]$，并将它们展平为一个长度为 $12$ 的单一列表。例如，其结构为 $[se_{\\text{conv}}^{(1)}, se_{\\text{HC0}}^{(1)}, p_{\\text{conv}}^{(1)}, p_{\\text{HC0}}^{(1)}, se_{\\text{conv}}^{(2)}, se_{\\text{HC0}}^{(2)}, p_{\\text{conv}}^{(2)}, p_{\\text{HC0}}^{(2)}, se_{\\text{conv}}^{(3)}, se_{\\text{HC0}}^{(3)}, p_{\\text{conv}}^{(3)}, p_{\\text{HC0}}^{(3)}]$。", "solution": "问题陈述已经过分析，并被确定为有效。它在科学上基于计量经济学和统计学的原理，问题设定良好，具有完整且一致的定义和数据，并且表述客观。该问题要求应用普通最小二乘法（OLS）估计、常规和异方差一致性（HC0）协方差矩阵估计以及相关假设检验程序的标准公式。我们将给出一个完整且论证充分的解决方案。\n\n所考虑的基本模型是线性回归模型，由 $y = X \\beta + u$ 给出，其中 $y \\in \\mathbb{R}^n$ 是因变量的观测向量，$X \\in \\mathbb{R}^{n \\times k}$ 是回归量矩阵（包括截距项），$\\beta \\in \\mathbb{R}^k$ 是系数向量，$u \\in \\mathbb{R}^n$ 是未观测到的误差项向量。\n\n$\\beta$ 的 OLS 估计量是通过最小化残差平方和得到的，这产生了著名的公式：\n$$\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$$\n然后，残差向量计算如下：\n$$\\hat{u} = y - X \\hat{\\beta}$$\n$\\hat{\\beta}$ 的统计特性由其协方差矩阵描述。该问题指定了该矩阵的两种估计量。\n\n$1$。常规协方差矩阵估计量，在同方差性假设（即对所有 $i$ 都有 $E[u_i^2 | X] = \\sigma^2$）下有效：\n$$V_{\\text{conv}} = \\hat{\\sigma}^2 (X^\\top X)^{-1}$$\n其中 $\\hat{\\sigma}^2$ 是误差方差的无偏估计量：\n$$\\hat{\\sigma}^2 = \\frac{\\hat{u}^\\top \\hat{u}}{n - k}$$\n\n$2$。White 的异方差一致性协方差矩阵估计量（HC0），它对未知形式的异方差性是稳健的：\n$$V_{\\text{HC0}} = (X^\\top X)^{-1} \\left( X^\\top \\Omega X \\right) (X^\\top X)^{-1}$$\n其中 $\\Omega$ 是残差平方的对角矩阵：\n$$\\Omega = \\operatorname{diag}(\\hat{u}_1^2, \\hat{u}_2^2, \\dots, \\hat{u}_n^2)$$\n\n对于任何系数 $\\hat{\\beta}_j$（其中 $j \\in \\{1,\\dots,k\\}$），其估计方差是估计协方差矩阵的第 $j$ 个对角元素。标准误是该方差的平方根。对于这两种估计量，我们有：\n$$se_{\\text{conv},j} = \\sqrt{(V_{\\text{conv}})_{jj}}$$\n$$se_{\\text{HC0},j} = \\sqrt{(V_{\\text{HC0}})_{jj}}$$\n为了检验原假设 $H_0: \\beta_j = 0$，我们计算 $t$-统计量：\n$$t_j = \\frac{\\hat{\\beta}_j}{se_j}$$\n然后使用自由度为 $n-k$（$df$）的学生t分布计算双边p值：\n$$p_j = 2 \\cdot (1 - F_{t, df}(|t_j|))$$\n其中 $F_{t, df}$ 是所述分布的累积分布函数（CDF）。\n\n我们现在将此过程应用于三个指定的用例。对于每个用例，我们报告第一个非恒定回归量的系数结果，该回归量对应于矩阵 $X$ 的第二列（在1-based索引系统中为索引 $j=2$，或在0-based编程上下文中为索引 1）。\n\n**用例 1：构造的异方差性简单回归**\n数据由 $n=10$ 生成。回归量矩阵是 $X \\in \\mathbb{R}^{10 \\times 2}$，其列为 $[\\mathbf{1}, x]$，其中 $x_i=i$。因此，$k=2$，自由度为 $df = n-k = 8$。因变量为 $y_i = 1.5 + 0.8 x_i + 0.3 x_i s_i$，其中 $s_i$ 在 $1$ 和 $-1$ 之间交替。在 $y$ 对 $[\\mathbf{1}, x]$ 的回归中，隐含的误差项是异方差的，因为其大小与 $x_i$ 成正比。\n计算过程如下：\n$1$。构造 $y$ 和 $X$。\n$2$。计算 $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$。\n$3$。计算残差 $\\hat{u} = y - X \\hat{\\beta}$。\n$4$。计算 $V_{\\text{conv}}$ 和 $V_{\\text{HC0}}$。\n$5$。提取 $x$ 系数的标准误 $se_{\\text{conv}}$ 和 $se_{\\text{HC0}}$。\n$6$。计算p值 $p_{\\text{conv}}$ 和 $p_{\\text{HC0}}$。\n四舍五入到6位小数的结果值是：\n$se_{\\text{conv}} = 0.240748$\n$se_{\\text{HC0}} = 0.219848$\n$p_{\\text{conv}} = 0.003923$\n$p_{\\text{HC0}} = 0.002161$\n\n**用例 2：同方差符号翻转误差的简单回归**\n数据由 $n=10$ 生成。回归量矩阵 $X \\in \\mathbb{R}^{10 \\times 2}$ 与用例1相同。因此，$k=2$ 且 $df=8$。因变量为 $y_i = -0.5 + 2.0 x_i + 0.5 s_i$。回归中隐含的误差项为 $0.5s_i$，其大小恒定。这对应于同方差设定。\n计算过程与用例1类似。在同方差设定中，$V_{\\text{conv}}$ 是合适的估计量，我们预期 $V_{\\text{HC0}}$ 会产生相似的结果。\n四舍五入到6位小数的结果值是：\n$se_{\\text{conv}} = 0.057321$\n$se_{\\text{HC0}} = 0.057106$\n$p_{\\text{conv}} = 0.000000$\n$p_{\\text{HC0}} = 0.000000$\n\n**用例 3：近似共线性和异方差性的多元回归**\n数据由 $n=8$ 生成。回归量矩阵 $X \\in \\mathbb{R}^{8 \\times 3}$ 的列为 $[\\mathbf{1}, x^{(1)}, x^{(2)}]$，其中 $x_i^{(1)}=i$ 且 $x_i^{(2)} = x_i^{(1)} + 0.1 s_i$。回归量 $x^{(1)}$ 和 $x^{(2)}$ 高度相关。因此，$k=3$ 且 $df = n-k = 5$。因变量为 $y_i = 0.5 + 1.0 x^{(1)}_i - 0.5 x^{(2)}_i + 0.2 x^{(1)}_i s_i$。模型设定意味着误差项的大小取决于 $x^{(1)}_i$，因此它是异方差的。近似共线性和异方差性的结合使这成为一个具有挑战性的估计问题。\n我们感兴趣的系数是 $x^{(1)}$ 的系数。计算遵循既定程序。\n四舍五入到6位小数的结果值是：\n$se_{\\text{conv}} = 0.954705$\n$se_{\\text{HC0}} = 1.056029$\n$p_{\\text{conv}} = 0.612459$\n$p_{\\text{HC0}} = 0.640697$\n\n收集到的结果在最终答案中以单一列表的形式呈现。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Computes conventional and HC0 standard errors and p-values for specified OLS regression models.\n    \"\"\"\n\n    def get_stats(y, X, coeff_idx=1):\n        \"\"\"\n        Calculates OLS statistics for a given regression setup.\n\n        Args:\n            y (np.ndarray): Dependent variable vector.\n            X (np.ndarray): Regressor matrix (with intercept).\n            coeff_idx (int): 0-based index of the coefficient of interest.\n\n        Returns:\n            tuple: A tuple containing se_conv, se_HC0, p_conv, p_HC0.\n        \"\"\"\n        n, k = X.shape\n        \n        # OLS estimator: beta_hat = (X'X)^{-1} X'y\n        try:\n            XTX_inv = np.linalg.inv(X.T @ X)\n        except np.linalg.LinAlgError:\n            print(\"Error: X'X is singular.\")\n            return (np.nan, np.nan, np.nan, np.nan)\n            \n        beta_hat = XTX_inv @ X.T @ y\n        \n        # Residuals\n        u_hat = y - X @ beta_hat\n        \n        # Degrees of freedom\n        df = n - k\n        \n        # Conventional (homoskedastic) covariance estimator\n        sigma2_hat = (u_hat.T @ u_hat) / df\n        V_conv = sigma2_hat * XTX_inv\n        se_conv = np.sqrt(V_conv[coeff_idx, coeff_idx])\n        \n        # HC0 (White's) heteroskedasticity-consistent covariance estimator\n        Omega = np.diag(u_hat**2)\n        meat = X.T @ Omega @ X\n        V_HC0 = XTX_inv @ meat @ XTX_inv\n        se_HC0 = np.sqrt(V_HC0[coeff_idx, coeff_idx])\n        \n        # Coefficient of interest\n        beta_j = beta_hat[coeff_idx]\n        \n        # t-statistics\n        t_conv = beta_j / se_conv\n        t_HC0 = beta_j / se_HC0\n        \n        # p-values\n        p_conv = 2 * (1 - t.cdf(np.abs(t_conv), df))\n        p_HC0 = 2 * (1 - t.cdf(np.abs(t_HC0), df))\n        \n        return se_conv, se_HC0, p_conv, p_HC0\n\n    results = []\n\n    # Case 1\n    n1 = 10\n    i_vals_1 = np.arange(1, n1 + 1)\n    x1 = i_vals_1\n    s1 = np.ones(n1)\n    s1[1::2] = -1  # even indices in 0-based array are odd numbers 2, 4,...\n    y1 = 1.5 + 0.8 * x1 + 0.3 * x1 * s1\n    X1 = np.ones((n1, 2))\n    X1[:, 1] = x1\n    \n    se_conv1, se_HC01, p_conv1, p_HC01 = get_stats(y1, X1, coeff_idx=1)\n    results.extend([round(se_conv1, 6), round(se_HC01, 6), round(p_conv1, 6), round(p_HC01, 6)])\n\n    # Case 2\n    n2 = 10\n    i_vals_2 = np.arange(1, n2 + 1)\n    x2 = i_vals_2\n    s2 = np.ones(n2)\n    s2[1::2] = -1\n    y2 = -0.5 + 2.0 * x2 + 0.5 * s2\n    X2 = np.ones((n2, 2))\n    X2[:, 1] = x2\n\n    se_conv2, se_HC02, p_conv2, p_HC02 = get_stats(y2, X2, coeff_idx=1)\n    results.extend([round(se_conv2, 6), round(se_HC02, 6), round(p_conv2, 6), round(p_HC02, 6)])\n\n    # Case 3\n    n3 = 8\n    i_vals_3 = np.arange(1, n3 + 1)\n    x1_3 = i_vals_3\n    s3 = np.ones(n3)\n    s3[1::2] = -1\n    x2_3 = x1_3 + 0.1 * s3\n    y3 = 0.5 + 1.0 * x1_3 - 0.5 * x2_3 + 0.2 * x1_3 * s3\n    X3 = np.ones((n3, 3))\n    X3[:, 1] = x1_3\n    X3[:, 2] = x2_3\n    \n    se_conv3, se_HC03, p_conv3, p_HC03 = get_stats(y3, X3, coeff_idx=1)\n    results.extend([round(se_conv3, 6), round(se_HC03, 6), round(p_conv3, 6), round(p_HC03, 6)])\n\n    # Final print statement in the exact required format.\n    # The format required is a string representation of the list, so we map each number to a string.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}]}