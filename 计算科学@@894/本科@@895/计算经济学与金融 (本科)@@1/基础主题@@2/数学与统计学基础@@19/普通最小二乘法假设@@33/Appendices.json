{"hands_on_practices": [{"introduction": "我们从检验普通最小二乘法（OLS）最基本的假设之一开始：变量之间的潜在关系是线性的。本练习提供了一个强大的视觉和计算课程，揭示了如果关系是非线性的，即使是完全确定的关系，OLS也可能无法检测到。通过对您从 $y_i = \\cos(x_i)$ 和 $y_i = x_i^2$ 等函数生成的数据运行线性回归，您将亲眼看到为什么低的 $R^2$ 不一定意味着变量之间没有关系。[@problem_id:2417149]", "id": "2417149", "problem": "构造一个程序，从第一性原理出发，演示线性普通最小二乘法 (OLS) 回归在何种情况下会无法检测出确定性的非线性关系。对于下述每个测试用例，您必须生成一个合成数据集，并计算带截距项的 OLS 拟合，定义为使残差平方和 $\\sum_{i=1}^{N}\\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2$ 最小化的实数对 $\\left(\\hat{\\beta}_0,\\hat{\\beta}_1\\right)$。然后，计算决定系数 $R^2$，其公式为 $1 - \\dfrac{\\sum_{i=1}^{N}\\hat{\\varepsilon}_i^2}{\\sum_{i=1}^{N}(y_i - \\bar{y})^2}$，其中 $\\hat{\\varepsilon}_i = y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i$ 且 $\\bar{y} = \\dfrac{1}{N}\\sum_{i=1}^{N} y_i$。所有计算都必须使用实数进行精确算术运算，并且所有生成的数据都是确定性的。\n\n生成以下三个数据集，每个数据集在指定区间上包含 $N=1001$ 个等距点 $x_i$，以及指定的相应 $y_i$。任何测试中都没有随机性：\n\n- 测试用例 A（非线性周期性，对称域）：$x_i$ 在 $[-\\pi,\\pi]$ 上， $y_i = \\cos(x_i)$。\n- 测试用例 B（非线性凸函数，对称域）：$x_i$ 在 $[-2,2]$ 上， $y_i = x_i^2$。\n- 测试用例 C（正确设定的线性基准）：$x_i$ 在 $[-5,5]$ 上， $y_i = 3 + 2 x_i$。\n\n您的程序必须为每个测试用例计算并按此顺序返回数对 $\\left(\\hat{\\beta}_1, R^2\\right)$。这三对数对必须按顺序汇总到单行输出中，形式为一个包含三个子列表的逗号分隔列表，不含空格，每个实数四舍五入到 $6$ 位小数。例如，所需格式为\n\"[[b1_A,R2_A],[b1_B,R2_B],[b1_C,R2_C]]\"\n所有数值显示到 $6$ 位小数。\n\n测试套件和答案规范：\n- 输入参数如上固定；没有用户输入。\n- 输出是三个有序实数对，每个测试用例一个：A、B 和 C 的 $\\left(\\hat{\\beta}_1, R^2\\right)$。\n- 最终输出必须打印为单行，包含一个由三个子列表组成的列表，每个子列表包含两个四舍五入到 6 位小数的浮点数，且不含空格。\n\n覆盖范围设计：\n- 测试用例 A 探讨了对称域上的确定性、非线性周期关系，在这种情况下，尽管存在完美的函数依赖关系，线性回归仍可能得出 $\\hat{\\beta}_1 \\approx 0$ 和 $R^2 \\approx 0$。\n- 测试用例 B 探讨了对称域上的确定性、非线性凸关系，在这种情况下，最佳线性拟合是一条常数，得出 $\\hat{\\beta}_1 = 0$ 和 $R^2 = 0$。\n- 测试用例 C 是一个正确设定的线性模型，它会得出 $\\hat{\\beta}_1 = 2$ 和 $R^2 = 1$，用作基准。", "solution": "对所述问题进行验证。\n\n**第 1 步：提取已知条件**\n- **目标**：演示线性普通最小二乘法 (OLS) 回归在检测确定性、非线性关系时的失败。\n- **模型**：带截距项的简单线性回归。\n- **最小化准则**：最小化残差平方和 $SSR = \\sum_{i=1}^{N}\\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2$，以找到估计量 $\\hat{\\beta}_0$ 和 $\\hat{\\beta}_1$。\n- **度量标准**：决定系数 $R^2 = 1 - \\dfrac{\\sum_{i=1}^{N}\\hat{\\varepsilon}_i^2}{\\sum_{i=1}^{N}(y_i - \\bar{y})^2}$，其中 $\\hat{\\varepsilon}_i = y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i$ 且 $\\bar{y} = \\frac{1}{N}\\sum_{i=1}^{N} y_i$。\n- **数据生成**：所有数据集都是确定性的，包含 $N=1001$ 个等距点。\n- **测试用例 A**：$x_i$ 在 $[-\\pi, \\pi]$ 上，$y_i = \\cos(x_i)$。\n- **测试用例 B**：$x_i$ 在 $[-2, 2]$ 上，$y_i = x_i^2$。\n- **测试用例 C**：$x_i$ 在 $[-5, 5]$ 上，$y_i = 3 + 2 x_i$。\n- **要求输出**：对于每个用例，输出数对 $(\\hat{\\beta}_1, R^2)$，汇总成一个列表的列表，数字四舍五入到 6 位小数。\n\n**第 2 步：使用提取的已知条件进行验证**\n根据既定标准评估该问题：\n1.  **科学性**：该问题基于普通最小二乘法回归的基本原理，这是统计学和计量经济学的基石。所提出的公式和概念都是标准的，且事实正确。\n2.  **适定性**：只要自变量存在方差，简单线性回归的 OLS 估计问题就存在唯一的解析解，数据集的构造满足这一条件。该问题清晰明确，并为求解提供了所有必要信息。\n3.  **客观性**：该问题以精确的数学术语陈述，没有主观性或模糊性。\n4.  **其他缺陷**：该问题不违反任何其他验证标准。它完整、一致、计算上可行，并与其陈述的 OLS 假设主题直接相关。\n\n**第 3 步：结论与行动**\n该问题**有效**。有必要提供完整解答。\n\n我们从第一性原理出发。目标是找到参数 $(\\hat{\\beta}_0, \\hat{\\beta}_1)$，以最小化残差平方和 $S(\\beta_0, \\beta_1) = \\sum_{i=1}^{N}(y_i - \\beta_0 - \\beta_1 x_i)^2$。为了找到最小值，我们对 $\\beta_0$ 和 $\\beta_1$ 求偏导数，并令其为零。这得到正规方程组：\n$$\n\\frac{\\partial S}{\\partial \\beta_0} = -2 \\sum_{i=1}^{N}(y_i - \\beta_0 - \\beta_1 x_i) = 0 \\implies \\sum y_i - N\\beta_0 - \\beta_1 \\sum x_i = 0\n$$\n$$\n\\frac{\\partial S}{\\partial \\beta_1} = -2 \\sum_{i=1}^{N}x_i(y_i - \\beta_0 - \\beta_1 x_i) = 0 \\implies \\sum x_i y_i - \\beta_0 \\sum x_i - \\beta_1 \\sum x_i^2 = 0\n$$\n求解这个线性方程组得到 $\\beta_0$ 和 $\\beta_1$，即 OLS 估计量，我们用帽子符号表示：\n$$\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{N}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{N}(x_i - \\bar{x})^2} = \\frac{\\text{Cov}(x, y)}{\\text{Var}(x)}\n$$\n$$\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n$$\n其中 $\\bar{x} = \\frac{1}{N}\\sum x_i$ 且 $\\bar{y} = \\frac{1}{N}\\sum y_i$。\n\n决定系数 $R^2$ 衡量因变量中可由自变量预测的方差比例。其定义为：\n$$\nR^2 = 1 - \\frac{\\text{SSR}}{\\text{TSS}} = 1 - \\frac{\\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{N}(y_i - \\bar{y})^2}\n$$\n其中 $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$ 是拟合值，SSR 是残差平方和 (Sum of Squared Residuals)，TSS 是总平方和 (Total Sum of Squares)。\n\n对于所有三个测试用例，一个关键点是自变量 $x$ 的生成。$N=1001$ 个点均匀分布在 $[-L, L]$ 形式的对称区间上。由于 $N$ 是奇数，中心点是 $x_{(N-1)/2} = 0$，对于数据集中的每个点 $x_i$，其相反数 $-x_i$ 也存在。这种对称性意味着 $x$ 的均值恰好为零：$\\bar{x} = 0$。\n这极大地简化了估计量：\n$$\n\\hat{\\beta}_1 = \\frac{\\sum x_i y_i}{\\sum x_i^2}\n$$\n$$\n\\hat{\\beta}_0 = \\bar{y}\n$$\n\n我们现在使用这些简化公式分析每个案例。\n\n**测试用例 A：非线性周期性，对称域 ($y_i = \\cos(x_i)$ on $[-\\pi, \\pi]$)**\n函数 $y = f(x) = \\cos(x)$ 是一个偶函数，意味着 $f(-x) = f(x)$。$\\hat{\\beta}_1$ 分子中的项是 $g(x_i) = x_i \\cos(x_i)$ 的和。函数 $g(x)$ 是一个奇函数 ($x$) 和一个偶函数 ($\\cos(x)$) 的乘积，结果是一个奇函数，$g(-x) = (-x)\\cos(-x) = -x\\cos(x) = -g(x)$。\n一个奇函数在以零为中心的对称点集上的和恰好为零。\n$$\n\\sum_{i=1}^{N} x_i y_i = \\sum_{i=1}^{N} x_i \\cos(x_i) = 0\n$$\n因此，斜率系数为零：\n$$\n\\hat{\\beta}_1 = \\frac{0}{\\sum x_i^2} = 0\n$$\n当 $\\hat{\\beta}_1 = 0$ 时，回归线是水平的，$\\hat{y}_i = \\hat{\\beta}_0 = \\bar{y}$。残差平方和 $SSR = \\sum(y_i - \\bar{y})^2$ 与总平方和 TSS 相同。\n$$\nR^2 = 1 - \\frac{SSR}{TSS} = 1 - \\frac{TSS}{TSS} = 0\n$$\n尽管 $x$ 和 $y$ 之间存在完美的确定性关系，但线性回归报告称其完全没有解释力。\n\n**测试用例 B：非线性凸函数，对称域 ($y_i = x_i^2$ on $[-2, 2]$)**\n函数 $y = f(x) = x^2$ 也是一个偶函数。分析与案例 A 相同。乘积 $g(x_i) = x_i y_i = x_i \\cdot x_i^2 = x_i^3$ 是一个奇函数。在对称点集上对该函数求和得到零。\n$$\n\\sum_{i=1}^{N} x_i y_i = \\sum_{i=1}^{N} x_i^3 = 0\n$$\n这同样导致 $\\hat{\\beta}_1 = 0$，因此 $R^2 = 0$。OLS 回归完全无法察觉这种完美的二次关系。这种失败是根本性的，它凸显了零协方差并不意味着独立，这是初学者常犯的错误。例如，在金融领域，简单的线性模型可能会忽略诸如波动率聚集之类的非线性依赖关系。\n\n**测试用例 C：正确设定的线性基准 ($y_i = 3 + 2x_i$ on $[-5, 5]$)**\n这个案例用作对照。数据是从一个完美的线性模型生成的，其真实参数为 $\\beta_0=3$ 和 $\\beta_1=2$。我们期望 OLS 能够精确地恢复这些参数。\n使用 $\\hat{\\beta}_1$ 的简化公式：\n$$\n\\hat{\\beta}_1 = \\frac{\\sum x_i y_i}{\\sum x_i^2} = \\frac{\\sum x_i (3 + 2x_i)}{\\sum x_i^2} = \\frac{3\\sum x_i + 2\\sum x_i^2}{\\sum x_i^2}\n$$\n由于 $\\sum x_i = N\\bar{x} = 0$，表达式简化为：\n$$\n\\hat{\\beta}_1 = \\frac{2\\sum x_i^2}{\\sum x_i^2} = 2\n$$\n截距是 $\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} = \\bar{y} - 2 \\cdot 0 = \\bar{y}$。$y$ 的均值是 $\\bar{y} = \\frac{1}{N}\\sum(3+2x_i) = \\frac{1}{N}(3N + 2\\sum x_i) = 3$。所以，$\\hat{\\beta}_0 = 3$。\n拟合模型是 $\\hat{y}_i = 3 + 2x_i$，与真实模型相同。对于所有 $i$，残差均为 $\\hat{\\varepsilon}_i = y_i - \\hat{y}_i = 0$。因此，$SSR = 0$。\n总平方和 $TSS = \\sum(y_i - \\bar{y})^2 = \\sum((3+2x_i)-3)^2 = \\sum(2x_i)^2 = 4\\sum x_i^2$。由于并非所有 $x_i$ 都为零，所以 $TSS > 0$。\n$$\nR^2 = 1 - \\frac{0}{TSS} = 1\n$$\n正如预期的那样，对于一个没有噪声的、正确设定的线性模型，OLS 提供了完美的拟合。\n\n实现过程将把这些解析公式转换为代码。它将为每个案例构建数据集，计算估计量和 $R^2$，并按规定格式化结果。由于浮点运算，可能会与理论精确值 0、1 和 2 出现微小偏差，但在要求的精度下，这些偏差是微不足道的。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes OLS coefficients for three deterministic datasets to demonstrate\n    the limitations of linear regression for non-linear relationships.\n    \"\"\"\n\n    # Define the problem parameters for each test case.\n    # Each tuple contains: (N, x_min, x_max, y_function, description)\n    test_cases = [\n        (1001, -np.pi, np.pi, lambda x: np.cos(x), \"A\"),\n        (1001, -2.0, 2.0, lambda x: x**2, \"B\"),\n        (1001, -5.0, 5.0, lambda x: 3.0 + 2.0 * x, \"C\"),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, x_min, x_max, y_func, _ = case\n\n        # Generate the deterministic dataset\n        x = np.linspace(x_min, x_max, N)\n        y = y_func(x)\n\n        # Compute means\n        x_mean = np.mean(x)\n        y_mean = np.mean(y)\n\n        # Compute OLS estimator for the slope (beta_1)\n        # beta_1 = Cov(x, y) / Var(x)\n        numerator = np.sum((x - x_mean) * (y - y_mean))\n        denominator = np.sum((x - x_mean)**2)\n        \n        # Avoid division by zero, though not expected here\n        if denominator == 0:\n            beta_1 = 0.0\n        else:\n            beta_1 = numerator / denominator\n\n        # Compute OLS estimator for the intercept (beta_0)\n        beta_0 = y_mean - beta_1 * x_mean\n\n        # Compute the fitted values\n        y_pred = beta_0 + beta_1 * x\n\n        # Compute Sum of Squared Residuals (SSR) and Total Sum of Squares (TSS)\n        ssr = np.sum((y - y_pred)**2)\n        tss = np.sum((y - y_mean)**2)\n\n        # Compute the coefficient of determination (R^2)\n        # If TSS is zero, all y values are the same.\n        # R^2 is undefined, but can be treated as 1 if the model is perfect (ssr=0)\n        # or 0 if it's not (ssr>0). Here, TSS will not be zero.\n        if tss == 0:\n            r_squared = 1.0 if ssr == 0 else 0.0\n        else:\n            r_squared = 1.0 - (ssr / tss)\n\n        results.append([beta_1, r_squared])\n\n    # Format the final output string as per problem specification.\n    # Example: [[b1_A,R2_A],[b1_B,R2_B],[b1_C,R2_C]]\n    # Numbers are rounded to 6 decimal places.\n    sublist_strs = []\n    for res_pair in results:\n        # Using f-string for precise decimal formatting\n        s = f\"[{res_pair[0]:.6f},{res_pair[1]:.6f}]\"\n        sublist_strs.append(s)\n    \n    final_output = f\"[{','.join(sublist_strs)}]\"\n\n    print(final_output)\n\nsolve()\n\n```"}, {"introduction": "在考虑了模型的函数形式之后，我们转向回归变量的属性。计量经济学中一个常见的陷阱是“虚拟变量陷阱”，这是在使用分类变量时出现的一种完全多重共线性的特殊情况。本练习要求您构建一个设计矩阵，并通过检查矩阵 $\\mathbf{X}^{\\top}\\mathbf{X}$ 是否为奇异矩阵，以编程方式识别何时因为这个陷阱而导致在数学上无法计算 OLS 估计值。[@problem_id:2417199]", "id": "2417199", "problem": "考虑一个通过普通最小二乘法 (OLS) 估计的线性回归模型，其设计矩阵由一个常数项（截距项）和单个分类回归变量的虚拟变量构成。设有 $n$ 个观测值，索引为 $i \\in \\{1,\\dots,n\\}$，并令 $\\ell_i$ 表示观测值 $i$ 的分类标签。令 $(\\ell_1,\\dots,\\ell_n)$ 中不同标签的集合为 $\\{c_1,\\dots,c_K\\}$，其排序使得 $c_1 < \\cdots < c_K$，因此 $K \\ge 1$。对于每个 $j \\in \\{1,\\dots,K\\}$，定义虚拟列 $D_j \\in \\mathbb{R}^n$ 为：如果 $\\ell_i = c_j$，则 $(D_j)_i = 1$；否则 $(D_j)_i = 0$。令 $1_n \\in \\mathbb{R}^n$ 表示全1向量。设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 将根据两个二元标志来构建：\n- include\\_intercept $\\in \\{0,1\\}$，指示是否包含常数列 $1_n$。\n- drop\\_baseline $\\in \\{0,1\\}$，指示是否从 $X$ 中剔除对应于 $c_1$ 的虚拟列；此标志仅在 include\\_intercept $= 1$ 时相关。\n\n形式上，按如下方式构建 $X$：\n- 如果 include\\_intercept $= 1$ 且 drop\\_baseline $= 0$，则 $X = [\\,1_n \\;\\; D_1 \\;\\; D_2 \\;\\; \\cdots \\;\\; D_K\\,]$，因此 $p = K+1$。\n- 如果 include\\_intercept $= 1$ 且 drop\\_baseline $= 1$，则 $X = [\\,1_n \\;\\; D_2 \\;\\; \\cdots \\;\\; D_K\\,]$，因此 $p = K$。\n- 如果 include\\_intercept $= 0$（无论 drop\\_baseline 为何值），则 $X = [\\,D_1 \\;\\; D_2 \\;\\; \\cdots \\;\\; D_K\\,]$，因此 $p = K$。\n\n对于下面定义的每个测试用例，请根据提供的标签序列和标志构建 $X$，然后确定格拉姆矩阵 $X^{\\top} X \\in \\mathbb{R}^{p \\times p}$ 是否为奇异矩阵。一个方阵 $M$ 是奇异的，当且仅当它不可逆（等价地，$\\det(M) = 0$）。\n\n您必须编写一个程序，对于每个测试用例，如果 $X^{\\top} X$ 是奇异的，则输出整数 $1$，否则输出 $0$。请对构建的 $X$ 使用精确的逻辑；不要读取任何外部输入。\n\n测试套件（每个用例是一个三元组 $(\\ell, \\text{include\\_intercept}, \\text{drop\\_baseline})$）：\n- 用例1：$\\ell = (1,2,3,1,2,3)$，include\\_intercept $= 1$，drop\\_baseline $= 0$。\n- 用例2：$\\ell = (1,2,3,1,2,3)$，include\\_intercept $= 1$，drop\\_baseline $= 1$。\n- 用例3：$\\ell = (1,2,3,1,2,3)$，include\\_intercept $= 0$，drop\\_baseline $= 0$。\n- 用例4：$\\ell = (5,5,5,5)$，include\\_intercept $= 1$，drop\\_baseline $= 0$。\n- 用例5：$\\ell = (5,5,5,5)$，include\\_intercept $= 1$，drop\\_baseline $= 1$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔的结果列表（例如，“[result1,result2,result3,result4,result5]”）。每个结果必须是如上所述的整数 $1$ 或 $0$。", "solution": "该问题要求分析格拉姆矩阵 $X^{\\top} X$ 的奇异性，其中 $X$ 是由虚拟变量和一个可选的截距项构成的设计矩阵。一个方阵是奇异的，当且仅当其列向量是线性相关的。格拉姆矩阵 $X^{\\top} X$ 是奇异的，当且仅当矩阵 $X$ 的列向量是线性相关的。因此，我们的任务简化为研究在何种条件下 $X$ 的列向量会表现出线性相关性。\n\n令 $n$ 为观测值的数量，唯一的分类标签集合为 $\\{c_1, \\dots, c_K\\}$，其中 $K \\ge 1$。对于每个 $j \\in \\{1, \\dots, K\\}$，列向量 $D_j \\in \\mathbb{R}^n$ 是一个虚拟变量，其中第 $i$ 个元素 $(D_j)_i$ 为 $1$（如果观测值 $i$ 的标签是 $c_j$），否则为 $0$。这种构建方式的一个基本性质是，对于任何观测值，有且仅有一个虚拟变量是“活动的”（取值为1）。这意味着在完整的虚拟变量集和截距向量 $1_n \\in \\mathbb{R}^n$（一个全为1的列向量）之间存在一个线性关系：\n$$ \\sum_{j=1}^{K} D_j = 1_n $$\n这个方程是确定 $X^{\\top} X$ 奇异性的关键。我们按规定分析构建 $X$ 的三种情况。\n\n情况1：`include_intercept` $= 1$，`drop_baseline` $= 0$。\n设计矩阵为 $X = [\\,1_n \\;\\; D_1 \\;\\; D_2 \\;\\; \\cdots \\;\\; D_K\\,]$。$X$ 的列是截距向量 $1_n$ 和所有 $K$ 个虚拟向量 $D_1, \\dots, D_K$。这个基本关系可以改写为 $X$ 列向量的一个线性组合：\n$$ (1) \\cdot 1_n - (1) \\cdot D_1 - (1) \\cdot D_2 - \\cdots - (1) \\cdot D_K = \\mathbf{0}_n $$\n这是一个等于零向量 $\\mathbf{0}_n$ 的 $X$ 列向量的非平凡线性组合（系数向量为 $[1, -1, \\dots, -1]^{\\top}$）。因此，$X$ 的列向量是线性相关的。这种多重共线性通常被称为“虚拟变量陷阱”。因此，格拉姆矩阵 $X^{\\top} X$ 是奇异的。\n\n情况2：`include_intercept` $= 1$，`drop_baseline` $= 1$。\n设计矩阵为 $X = [\\,1_n \\;\\; D_2 \\;\\; \\cdots \\;\\; D_K\\,]$。基准类别 $c_1$ 对应的虚拟变量 $D_1$ 被省略了。如果 $K=1$，则包含的虚拟向量集为空，因此 $X = [\\,1_n\\,]$。单个非零列向量构成一个线性无关集。其格拉姆矩阵 $X^{\\top}X = [n]$，对于 $n \\ge 1$ 是非奇异的。\n如果 $K > 1$，我们通过将一个线性组合设为零来检验列 $\\{1_n, D_2, \\dots, D_K\\}$ 的线性无关性：\n$$ \\alpha_0 \\cdot 1_n + \\sum_{j=2}^{K} \\alpha_j D_j = \\mathbf{0}_n $$\n根据问题的定义，对于每个类别 $c_j$，至少存在一个具有该标签的观测值 $i$。对于基准类别 $c_1$ 中的一个观测值，该方程对应的行变为 $\\alpha_0 \\cdot 1 + \\sum_{j=2}^{K} \\alpha_j \\cdot 0 = 0$，这意味着 $\\alpha_0 = 0$。对于任何其他类别 $c_j$（其中 $j \\ge 2$）中的一个观测值，方程变为 $\\alpha_0 \\cdot 1 + \\alpha_j \\cdot 1 = 0$。因为我们已经得出 $\\alpha_0 = 0$，所以对于所有的 $j \\in \\{2, \\dots, K\\}$，都有 $\\alpha_j = 0$。唯一的解是所有系数都为零的平凡解。因此，$X$ 的列向量是线性无关的，并且 $X^{\\top} X$ 是非奇异的。\n\n情况3：`include_intercept` $= 0$。\n设计矩阵为 $X = [\\,D_1 \\;\\; D_2 \\;\\; \\cdots \\;\\; D_K\\,]$。我们检验列 $\\{D_1, \\dots, D_K\\}$ 的线性无关性：\n$$ \\sum_{j=1}^{K} \\alpha_j D_j = \\mathbf{0}_n $$\n对于类别 $c_j$ 中的一个观测值，此向量方程对应的行为 $\\alpha_j \\cdot 1 = 0$，这意味着 $\\alpha_j = 0$。因为这对每个类别 $j \\in \\{1, \\dots, K\\}$ 都成立，所以所有系数都必须为零。$X$ 的列向量是线性无关的，并且 $X^{\\top} X$ 是非奇异的。\n\n基于此分析的结果摘要：\n- 用例1：$\\ell = (1,2,3,1,2,3)$，`include_intercept` $= 1$，`drop_baseline` $= 0$。此配置与情况1的分析相匹配。结果：奇异 ($1$)。\n- 用例2：$\\ell = (1,2,3,1,2,3)$，`include_intercept` $= 1$，`drop_baseline` $= 1$。此配置与情况2的分析相匹配。结果：非奇异 ($0$)。\n- 用例3：$\\ell = (1,2,3,1,2,3)$，`include_intercept` $= 0$，`drop_baseline` $= 0$。此配置与情况3的分析相匹配。结果：非奇异 ($0$)。\n- 用例4：$\\ell = (5,5,5,5)$，`include_intercept` $= 1$，`drop_baseline` $= 0$。此处 $K=1$。矩阵是 $X = [\\,1_n \\;\\; D_1\\,]$。由于每个观测值都在同一类别中，因此 $D_1=1_n$，所以 $X=[\\,1_n \\;\\; 1_n\\,]$。两个列向量相同，因此是线性相关的。结果：奇异 ($1$)。\n- 用例5：$\\ell = (5,5,5,5)$，`include_intercept` $= 1$，`drop_baseline` $= 1$。此处 $K=1$。这是情况2的一个实例，其中 $X=[\\,1_n\\,]$。它有一个列，秩为 $1$。结果：非奇异 ($0$)。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite. For each case, it constructs\n    the design matrix X and determines if the Gram matrix X'X is singular\n    by checking if the rank of X is less than its number of columns.\n    \"\"\"\n    test_cases = [\n        # Case 1: (labels, include_intercept, drop_baseline)\n        ((1, 2, 3, 1, 2, 3), 1, 0),\n        # Case 2\n        ((1, 2, 3, 1, 2, 3), 1, 1),\n        # Case 3\n        ((1, 2, 3, 1, 2, 3), 0, 0),\n        # Case 4\n        ((5, 5, 5, 5), 1, 0),\n        # Case 5\n        ((5, 5, 5, 5), 1, 1),\n    ]\n\n    results = []\n    for l, include_intercept, drop_baseline in test_cases:\n        n = len(l)\n        # The problem statement implies n >= 1 and K >= 1, so no need for n=0 check.\n\n        # Identify unique categories and map them to indices 0, 1, ..., K-1\n        categories = sorted(list(set(l)))\n        K = len(categories)\n        cat_map = {cat: i for i, cat in enumerate(categories)}\n\n        # Construct the full set of K dummy variables efficiently\n        all_dummies = np.zeros((n, K))\n        rows = np.arange(n)\n        cols = [cat_map[label] for label in l]\n        all_dummies[rows, cols] = 1\n\n        # Build the list of columns for the design matrix X\n        X_cols = []\n        \n        if include_intercept == 1:\n            # Add intercept column\n            X_cols.append(np.ones((n, 1)))\n            \n            if drop_baseline == 0:\n                # Add all K dummy variables\n                if K > 0:\n                    X_cols.append(all_dummies)\n            else:  # drop_baseline == 1\n                # Add dummies D_2, ..., D_K\n                if K > 1:\n                    X_cols.append(all_dummies[:, 1:])\n        else:  # include_intercept == 0\n            # Add all K dummy variables\n            if K > 0:\n                X_cols.append(all_dummies)\n\n        # If no columns were selected, the matrix has 0 columns.\n        if not X_cols:\n            p = 0\n            rank = 0\n        else:\n            # Horizontally stack columns to form X\n            X = np.hstack(X_cols)\n            p = X.shape[1]\n            # Calculate the rank of the design matrix X\n            rank = np.linalg.matrix_rank(X)\n\n        # X'X is singular if and only if the columns of X are linearly dependent,\n        # which is true if and only if rank(X) < number of columns of X.\n        is_singular = 1 if rank < p else 0\n        results.append(is_singular)\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "最后，我们研究误差项 $\\mathbf{u_i}$ 的属性。经典的同方差性假设——即误差具有恒定方差——在经济和金融数据中很少得到满足。本练习将指导您模拟具有异方差性的数据，然后计算并比较系数标准误差的三种不同估计值：经典的 OLS 版本、稳健的 White 标准误和现代的自助法（bootstrap）估计。[@problem_id:2417150]", "id": "2417150", "problem": "考虑一个计算经济学和金融学背景下的简单线性回归模型，该模型包含一个回归量和一个截距。响应变量 $y_i$ 由以下数据生成过程生成\n$$\ny_i = \\beta_0 + \\beta_1 x_i + u_i,\n$$\n其中 $x_i \\sim \\mathcal{N}(0,1)$ 在观测值 $i$ 间独立同分布，且扰动项 $u_i$ 根据以下形式表现出异方差性\n$$\nu_i = \\sigma_0 \\sqrt{1 + \\gamma x_i^2}\\,\\varepsilon_i,\\quad \\varepsilon_i \\sim \\mathcal{N}(0,1)\\ \\text{独立于}\\ x_i.\n$$\n假设我们关心的系数是斜率 $\\beta_1$。对于下文定义的每个测试案例，使用指定的参数生成一个样本 $\\{(y_i,x_i)\\}_{i=1}^n$，并计算普通最小二乘法 (OLS) 估计量 $\\beta_1$ 的三种标准误估计：\n- 基于经典同方差假定的OLS标准误，\n- 异方差一致性 (HC$0$) 标准误（也称为White稳健标准误），\n- 基于对观测对 $(y_i,x_i)$ 进行 $B$ 次有放回非参数重抽样得到的自助法标准误。\n\n为确保可复现性，在每个测试案例中，对所有随机抽取使用相同的固定随机数生成器种子 $s$。在所有计算中，回归模型均包含截距。将每个报告的标准误表示为四舍五入到六位小数的实数。\n\n测试集。使用以下参数集；每个项目符号描述一个测试案例：\n- 案例1：$n=500$, $\\beta_0=0.7$, $\\beta_1=1.5$, $\\sigma_0=1.0$, $\\gamma=2.0$, $B=400$, $s=17$。\n- 案例2：$n=500$, $\\beta_0=0.7$, $\\beta_1=1.5$, $\\sigma_0=1.0$, $\\gamma=0.0$, $B=400$, $s=23$。\n- 案例3：$n=50$, $\\beta_0=0.7$, $\\beta_1=1.5$, $\\sigma_0=1.0$, $\\gamma=2.0$, $B=400$, $s=123$。\n- 案例4：$n=500$, $\\beta_0=0.7$, $\\beta_1=1.5$, $\\sigma_0=1.0$, $\\gamma=10.0$, $B=400$, $s=2023$。\n\n最终输出格式。您的程序应生成一行输出，其中包含一个以逗号分隔的列表的列表，并用方括号括起来。每个内部列表对应一个测试案例，顺序与上文相同，并按 [OLS标准误, White稳健标准误, 自助法标准误] 的顺序包含三个实数，每个实数都四舍五入到六位小数。例如，输出必须具有以下形式\n$[[a_{1,1},a_{1,2},a_{1,3}],[a_{2,1},a_{2,2},a_{2,3}],[a_{3,1},a_{3,2},a_{3,3}],[a_{4,1},a_{4,2},a_{4,3}]]$\n不带空格。", "solution": "所提出的问题是一个定义明确的计量经济学计算练习。它具有科学依据，逻辑一致，并提供了求解所需的所有必要信息。任务是计算在包含异方差性的特定数据生成过程下，普通最小二乘法 (OLS) 回归系数标准误的三种不同估计量。\n\n该模型是一个简单线性回归：\n$$\ny_i = \\beta_0 + \\beta_1 x_i + u_i\n$$\n对于 $i = 1, \\dots, n$。在矩阵表示法中，这写作 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{u}$，其中 $\\mathbf{y}$ 是响应变量观测值的 $n \\times 1$ 向量，$\\mathbf{X}$ 是 $n \\times 2$ 的设计矩阵，其第一列是全为1的向量，第二列是回归量 $x_i$ 的观测值，$\\boldsymbol{\\beta} = [\\beta_0, \\beta_1]^T$ 是 $2 \\times 1$ 的系数向量，$\\mathbf{u}$ 是 $n \\times 1$ 的扰动项向量。\n\n$\\boldsymbol{\\beta}$ 的OLS估计量由下式给出：\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\n在给定 $\\mathbf{X}$ 的条件下，该估计量的方差-协方差矩阵为：\n$$\n\\text{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\Omega} \\mathbf{X} (\\mathbf{X}^T\\mathbf{X})^{-1}\n$$\n其中 $\\boldsymbol{\\Omega} = \\text{E}[\\mathbf{u}\\mathbf{u}^T | \\mathbf{X}]$。在此问题中，扰动项 $u_i$ 在观测值之间是独立的，因此 $\\boldsymbol{\\Omega}$ 是一个对角矩阵。异方差结构 $u_i = \\sigma_0 \\sqrt{1 + \\gamma x_i^2}\\,\\varepsilon_i$ 其中 $\\varepsilon_i \\sim \\mathcal{N}(0,1)$ 意味着 $\\boldsymbol{\\Omega}$ 的对角元素为 $\\text{Var}(u_i|x_i) = \\sigma_i^2 = \\sigma_0^2(1 + \\gamma x_i^2)$。\n\n任务是估计斜率系数的OLS估计量 $\\hat{\\beta}_1$ 的标准误，即 $\\text{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X})$ 第二个对角元素的平方根。我们将计算该数量的三种不同估计。\n\n1.  **基于经典同方差假定的OLS标准误**\n\n该估计量错误地假设扰动项是同方差的，即对所有 $i$ 都有 $\\text{Var}(u_i|x_i) = \\sigma^2$。在此假设下，$\\boldsymbol{\\Omega} = \\sigma^2\\mathbf{I}_n$，方差-协方差矩阵简化为：\n$$\n\\text{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}\n$$\n未知的误差方差 $\\sigma^2$ 使用OLS残差 $\\hat{u}_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)$ 进行估计。$\\sigma^2$ 的无偏估计量为：\n$$\n\\hat{\\sigma}^2 = \\frac{1}{n-k} \\sum_{i=1}^{n} \\hat{u}_i^2\n$$\n其中 $k$ 是回归量的数量，本例中为 $2$（截距和斜率）。估计的经典方差-协方差矩阵为 $\\widehat{\\text{Var}}_{OLS}(\\hat{\\boldsymbol{\\beta}}) = \\hat{\\sigma}^2(\\mathbf{X}^T\\mathbf{X})^{-1}$。$\\hat{\\beta}_1$ 的标准误是该矩阵第二个对角元素的平方根：\n$$\n\\text{SE}_{OLS}(\\hat{\\beta}_1) = \\sqrt{[\\widehat{\\text{Var}}_{OLS}(\\hat{\\boldsymbol{\\beta}})]_{2,2}}\n$$\n\n2.  **异方差一致性 (HC0) 标准误**\n\n该估计量，也称为White稳健标准误，不假定同方差性。即使当 $\\boldsymbol{\\Omega}$ 不是单位矩阵的倍数时，它也能提供方差-协方差矩阵的一致估计。它基于方差的“三明治”公式。三明治的中间部分 $\\mathbf{S} = \\mathbf{X}^T \\boldsymbol{\\Omega} \\mathbf{X}$，通过用OLS残差的平方 $\\hat{u}_i^2$ 替换未知的方差 $\\sigma_i^2$ 来进行估计。这给出了 $\\mathbf{S}$ 的HC$0$估计量：\n$$\n\\hat{\\mathbf{S}}_0 = \\sum_{i=1}^{n} \\hat{u}_i^2 \\mathbf{x}_i \\mathbf{x}_i^T\n$$\n其中 $\\mathbf{x}_i = [1, x_i]^T$ 是设计矩阵 $\\mathbf{X}$ 的第 $i$ 行。HC$0$方差-协方差估计量则为：\n$$\n\\widehat{\\text{Var}}_{HC0}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^T\\mathbf{X})^{-1} \\hat{\\mathbf{S}}_0 (\\mathbf{X}^T\\mathbf{X})^{-1}\n$$\n$\\hat{\\beta}_1$ 的相应标准误为：\n$$\n\\text{SE}_{HC0}(\\hat{\\beta}_1) = \\sqrt{[\\widehat{\\text{Var}}_{HC0}(\\hat{\\boldsymbol{\\beta}})]_{2,2}}\n$$\n当 $\\gamma > 0$ 时，误差确实是异方差的，预期此估计量比经典估计量更准确，尤其是在大样本中。当 $\\gamma=0.0$ 时，误差是同方差的，两种估计量应相似。\n\n3.  **自助法标准误**\n\n自助法提供了一种用于估计标准误的非参数方法。我们使用“配对自助法”，这在存在异方差性的情况下是合适的，因为它将对 $(x_i, y_i)$ 一起进行重抽样，从而保留了回归量与误差项方差之间的未知关系。算法如下：\n    1.  从原始样本 $\\{(y_i, x_i)\\}_{i=1}^n$ 中，通过有放回地抽样对，抽取一个大小为 $n$ 的“自助样本”。\n    2.  使用此自助样本，计算斜率系数的OLS估计值，记为 $\\hat{\\beta}_{1,b}^*$。\n    3.  对大量的重复次数 $B$ 重复步骤1和2。这将产生一个自助法估计值的分布 $\\{\\hat{\\beta}_{1,1}^*, \\dots, \\hat{\\beta}_{1,B}^*\\}$。\n    4.  自助法标准误是这 $B$ 个估计值的样本标准差：\n        $$\n        \\text{SE}_{Boot}(\\hat{\\beta}_1) = \\sqrt{\\frac{1}{B-1} \\sum_{b=1}^{B} (\\hat{\\beta}_{1,b}^* - \\bar{\\beta}_1^*)^2}\n        $$\n        其中 $\\bar{\\beta}_1^* = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{\\beta}_{1,b}^*$。此过程计算量大，但为 $\\hat{\\beta}_1$ 的抽样变异性提供了可靠的估计。\n\n实现将首先根据指定的参数（$n, \\beta_0, \\beta_1, \\sigma_0, \\gamma$）和随机种子 $s$ 为每个测试案例生成数据。然后，对于每个生成的数据集，将计算OLS估计值，随后计算三种指定的标准误。结果将按要求进行四舍五入和格式化。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It generates data, computes OLS and three types of standard errors,\n    and prints the results in the required format.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: n=500, β0=0.7, β1=1.5, σ0=1.0, γ=2.0, B=400, s=17\n        (500, 0.7, 1.5, 1.0, 2.0, 400, 17),\n        # Case 2: n=500, β0=0.7, β1=1.5, σ0=1.0, γ=0.0, B=400, s=23\n        (500, 0.7, 1.5, 1.0, 0.0, 400, 23),\n        # Case 3: n=50, β0=0.7, β1=1.5, σ0=1.0, γ=2.0, B=400, s=123\n        (50, 0.7, 1.5, 1.0, 2.0, 400, 123),\n        # Case 4: n=500, β0=0.7, β1=1.5, σ0=1.0, γ=10.0, B=400, s=2023\n        (500, 0.7, 1.5, 1.0, 10.0, 400, 2023),\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        n, beta0, beta1, sigma0, gamma, B, s = case\n        \n        # Initialize random number generator for reproducibility\n        rng = np.random.default_rng(s)\n\n        # 1. Data Generation\n        x = rng.normal(0, 1, size=n)\n        epsilon = rng.normal(0, 1, size=n)\n        u = sigma0 * np.sqrt(1 + gamma * x**2) * epsilon\n        y = beta0 + beta1 * x + u\n\n        # 2. OLS Estimation\n        X = np.vstack((np.ones(n), x)).T  # Design matrix [1, x_i]\n        \n        try:\n            # Pre-compute (X'X)^-1\n            inv_XTX = np.linalg.inv(X.T @ X)\n            beta_hat = inv_XTX @ X.T @ y\n        except np.linalg.LinAlgError:\n            # Handle rare case of singular matrix\n            all_results.append([np.nan, np.nan, np.nan])\n            continue\n            \n        residuals = y - X @ beta_hat\n        k = X.shape[1] # Number of regressors (intercept + slope)\n\n        # 3. Compute Standard Errors\n\n        # 3.1. Classical (Homoskedastic) OLS Standard Error\n        sigma2_hat = np.sum(residuals**2) / (n - k)\n        var_cov_ols = sigma2_hat * inv_XTX\n        se_ols = np.sqrt(var_cov_ols[1, 1])\n\n        # 3.2. HC0 (White-Robust) Standard Error\n        # S_0 = sum(u_i^2 * x_i * x_i')\n        S0 = X.T @ np.diag(residuals**2) @ X\n        var_cov_hc0 = inv_XTX @ S0 @ inv_XTX\n        se_hc0 = np.sqrt(var_cov_hc0[1, 1])\n\n        # 3.3. Bootstrap Standard Error (Pairs Bootstrap)\n        bootstrap_betas = np.zeros(B)\n        for b in range(B):\n            # Resample pairs (y_i, x_i) with replacement\n            indices = rng.choice(n, size=n, replace=True)\n            y_star = y[indices]\n            X_star = X[indices]\n            \n            try:\n                # OLS on bootstrap sample\n                inv_XTX_star = np.linalg.inv(X_star.T @ X_star)\n                beta_hat_star = inv_XTX_star @ X_star.T @ y_star\n                bootstrap_betas[b] = beta_hat_star[1]\n            except np.linalg.LinAlgError:\n                bootstrap_betas[b] = np.nan\n        \n        # Standard deviation of bootstrap estimates\n        se_boot = np.nanstd(bootstrap_betas, ddof=1)\n\n        # 4. Store rounded results\n        case_results = [\n            round(se_ols, 6),\n            round(se_hc0, 6),\n            round(se_boot, 6)\n        ]\n        all_results.append(case_results)\n\n    # Format output string to be a list of lists with no spaces\n    formatted_results = [repr(r).replace(' ', '') for r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"}]}