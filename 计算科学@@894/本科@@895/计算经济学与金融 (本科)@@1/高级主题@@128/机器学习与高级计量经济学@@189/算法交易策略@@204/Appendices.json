{"hands_on_practices": [{"introduction": "本练习将指导你将一个经典的交易理念——市场情绪分析——转化为可执行的算法。我们将利用期权市场中的看跌-看涨期权成交量比率（put-call ratio）作为反向指标，构建一个完整的交易策略。通过这个实践，你将学习如何精确定义交易信号、考虑交易成本等现实摩擦，并量化评估策略表现 [@problem_id:2371402]。", "id": "2371402", "problem": "给定一个离散时间的金融市场，其中包含一种标的资产以及由每日看涨和看跌期权交易量汇总的期权市场活动。对于每个由整数时间 $t \\in \\{0,1,\\dots,T-1\\}$ 索引的交易日，令 $C_t \\ge 0$ 表示看涨期权交易量， $P_t \\ge 0$ 表示看跌期权交易量，并令 $r_t$ 表示第 $t$ 天资产的简单回报率，以小数形式表示（例如，$r_t = 0.01$ 表示 $1$ percent 的增长，必须将其视为小数 $0.01$ 而不是百分比）。一个逆向情绪交易规则定义如下。首先，定义看涨-看跌期权成交量比率 $q_t$ 为\n$$\nq_t =\n\\begin{cases}\n\\dfrac{C_t}{P_t}, & \\text{if } P_t > 0, \\\\\n+\\infty, & \\text{if } P_t = 0 \\text{ and } C_t > 0, \\\\\n0, & \\text{if } P_t = 0 \\text{ and } C_t = 0.\n\\end{cases}\n$$\n给定两个阈值 $t_\\ell$ 和 $t_h$，满足 $0 < t_\\ell < t_h$，定义交易信号 $s_t$ 为\n$$\ns_t =\n\\begin{cases}\n+1, & \\text{if } q_t < t_\\ell, \\\\\n-1, & \\text{if } q_t > t_h, \\\\\n0, & \\text{otherwise.}\n\\end{cases}\n$$\n将 $s_t \\in \\{-1,0,+1\\}$ 解释为在第 $t$ 天收盘时决定并应用于下一天的头寸：$+1$ 表示对标的资产完全投资的多头头寸，$-1$ 表示对标的资产完全投资的空头头寸，0 表示无头寸。设 $c \\ge 0$ 为一个比例交易成本（以小数表示），每当第 $t$ 天收盘时头寸发生变化时支付一次。假设初始头寸为 $s_{-1} = 0$。第 $t+1$ 天的单日利润为\n$$\np_{t+1} = s_t \\, r_{t+1} - c \\cdot \\mathbf{1}\\{s_t \\ne s_{t-1}\\},\n$$\n适用于 $t = 0,1,\\dots,T-2$，其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。从第 $1$ 天到第 $T-1$ 天整个期间的累计简单回报率为\n$$\nR = \\prod_{u=1}^{T-1} (1 + p_u) - 1.\n$$\n您的程序必须为下面提供的每个测试用例计算 $R$，并将每个 $R$ 四舍五入到 $6$ 位小数后输出。\n\n测试套件（每个用例提供 $C_t$、$P_t$、$r_t$、$t_\\ell$、$t_h$ 和 $c$）：\n- 用例 1（正常路径，信号和成本变化）：$T = 6$，$C_t$ 等于 $100, 150, 80, 50, 200, 90$；$P_t$ 等于 $200, 100, 120, 100, 50, 110$；$r_t$ 等于 $0.01, -0.005, 0.008, -0.02, 0.015, 0.005$；$t_\\ell = 0.8$；$t_h = 1.2$；$c = 0.001$。\n- 用例 2（边界条件，$q_t$ 等于阈值不产生交易）：$T = 5$，$C_t$ 等于 $50, 100, 200, 60, 40$；$P_t$ 等于 $100, 50, 100, 120, 80$；$r_t$ 等于 $0.01, 0.02, -0.01, 0.005, -0.005$；$t_\\ell = 0.5$；$t_h = 2.0$；$c = 0.002$。\n- 用例 3（看跌期权交易量为零和两者交易量均为零，严格的逆向行为）：$T = 6$，$C_t$ 等于 $0, 10, 20, 0, 5, 0$；$P_t$ 等于 $0, 0, 5, 0, 10, 0$；$r_t$ 等于 $0.01, -0.02, 0.03, -0.01, 0.005, 0.002$；$t_\\ell = 0.8$；$t_h = 3.0$；$c = 0.0015$。\n- 用例 4（持续的极端情绪，主要是单边头寸，换手率极低）：$T = 5$，$C_t$ 等于 $300, 320, 310, 305, 315$；$P_t$ 等于 $100, 90, 95, 100, 85$；$r_t$ 等于 $0.01, -0.005, 0.002, 0.004, -0.003$；$t_\\ell = 0.7$；$t_h = 2.5$；$c = 0.0005$。\n\n要求的最终输出格式：您的程序应生成单行输出，其中包含四个用例的结果，形式为方括号括起来的逗号分隔列表，每个值都四舍五入到 $6$ 位小数，例如 $\\texttt{[0.123456,-0.010000,0.000000,0.250000]}$。", "solution": "该问题要求计算一种基于对期权市場情绪进行逆向解读的量化交易策略的累计回报率。该策略在一个离散时间范围 $t \\in \\{0, 1, \\dots, T-1\\}$ 内实施。对问题陈述的验证确认了其科学和数学上的合理性、完整性和客观性。它构成了一个计算金融学中的适定问题。我们继续进行求解。\n\n解决方案是通过逐日算法模拟该策略而得出的。该过程包括三个主要阶段：信号生成、每日利润计算和累计回报率聚合。\n\n**1. 信号生成**\n\n对于从 $t=0$到 $t=T-1$ 的每个交易日 $t$，都会确定一个交易信号 $s_t \\in \\{-1, 0, +1\\}$。该信号决定了下一天（即 $t+1$）要持有的头寸。信号是根据看涨-看跌期权成交量比率 $q_t$ 得出的。\n\n首先，比率 $q_t$ 是根据每日看涨期权交易量 $C_t$ 和看跌期权交易量 $P_t$ 计算得出的：\n$$\nq_t =\n\\begin{cases}\n\\dfrac{C_t}{P_t}, & \\text{if } P_t > 0, \\\\\n+\\infty, & \\text{if } P_t = 0 \\text{ and } C_t > 0, \\\\\n0, & \\text{if } P_t = 0 \\text{ and } C_t = 0.\n\\end{cases}\n$$\n$q_t=+\\infty$ 的值在计算上使用浮点数无穷大来处理。\n\n接下来，通过将 $q_t$ 与两个预定义的阈值——一个下阈值 $t_\\ell$ 和一个上阈值 $t_h$（其中 $0 < t_\\ell < t_h$）——进行比较来确定信号 $s_t$：\n$$\ns_t =\n\\begin{cases}\n+1, & \\text{if } q_t < t_\\ell, \\\\\n-1, & \\text{if } q_t > t_h, \\\\\n0, & \\text{otherwise.}\n\\end{cases}\n$$\n信号 $s_t=+1$ 代表多头头寸，$s_t=-1$ 代表空头头寸，$s_t=0$ 代表中性（持平）头寸。对每个交易日 $t$ 重复此过程，以生成信号的时间序列 $s_0, s_1, \\dots, s_{T-1}$。\n\n**2. 每日盈亏（P&L）计算**\n\n第 $u$ 天的利润，记为 $p_u$，是为 $u \\in \\{1, \\dots, T-1\\}$ 计算的。利润 $p_{t+1}$ 取决于信号 $s_t$（在第 $t$ 天收盘时确定）、第 $t+1$ 天资产的简单回报率 $r_{t+1}$，以及比例交易成本 $c$。第 $t+1$ 天的利润公式由下式给出：\n$$\np_{t+1} = s_t \\cdot r_{t+1} - c \\cdot \\mathbf{1}\\{s_t \\ne s_{t-1}\\}\n$$\n适用于 $t = 0, 1, \\dots, T-2$。初始头寸给定为 $s_{-1} = 0$。\n\n项 $s_t \\cdot r_{t+1}$ 表示投資回报。多头头寸 ($s_t=+1$) 从正回报 ($r_{t+1}>0$) 中获利，在负回报中亏损。空头头寸 ($s_t=-1$) 从负回报 ($r_{t+1}<0$) 中获利，在正回报中虧損。\n\n项 $c \\cdot \\mathbf{1}\\{s_t \\ne s_{t-1}\\}$ 表示交易成本。指示函数 $\\mathbf{1}\\{\\cdot\\}$ 在其参数为真时为 1，否则为 0。只有当第 $t$ 天结束时的头寸 ($s_t$) 与第 $t$ 天期间持有的頭寸（由 $s_{t-1}$ 决定）不同时，才会产生 $c$ 的成本。每日利润序列 $p_1, p_2, \\dots, p_{T-1}$ 是通过从 $t=0$ 迭代到 $t=T-2$ 来计算的。\n\n**3. 累计回报率计算**\n\n该策略从第 1 天到第 $T-1$ 天整个期间的总表现由累计简单回报率 $R$ 衡量。这是通过对每日回报率 $(1+p_u)$ 进行几何复合计算得出的：\n$$\nR = \\prod_{u=1}^{T-1} (1 + p_u) - 1.\n$$\n这个最终值 $R$ 代表初始资本的总增长，扣除所有交易成本。\n\n实现将通过顺序应用这些步骤来处理每个测试用例。$C_t$、$P_t$ 和 $r_t$ 的时间序列存储在数组中。首先计算并存储所有 $t$ 的信号 $s_t$。然后，第二个循环使用信号和回报率计算每日利润 $p_{t+1}$。最后，$(1+p_u)$ 的累积乘积得出总回报。每个用例的结果都按规定四舍五入到 6 位小数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_cumulative_return(T, C, P, r, t_ell, t_h, c):\n    \"\"\"\n    Calculates the cumulative return of the contrarian trading strategy.\n\n    Args:\n        T (int): The total number of days in the data series.\n        C (np.ndarray): Array of call option volumes.\n        P (np.ndarray): Array of put option volumes.\n        r (np.ndarray): Array of asset's simple returns.\n        t_ell (float): The lower threshold for the call-put ratio.\n        t_h (float): The upper threshold for the call-put ratio.\n        c (float): The proportional transaction cost.\n\n    Returns:\n        float: The cumulative simple return R.\n    \"\"\"\n    \n    # Step 1: Generate trading signals s_t for t = 0, ..., T-1.\n    s = np.zeros(T)\n    for t in range(T):\n        C_t, P_t = C[t], P[t]\n        \n        q_t = 0.0\n        if P_t > 0:\n            q_t = C_t / P_t\n        elif P_t == 0 and C_t > 0:\n            q_t = np.inf\n        # If P_t == 0 and C_t == 0, q_t remains 0.0 as initialized.\n        \n        if q_t < t_ell:\n            s[t] = 1.0\n        elif q_t > t_h:\n            s[t] = -1.0\n        # Otherwise, s[t] remains 0.0 as initialized.\n\n    # Step 2: Calculate daily profits p_{t+1} for t = 0, ..., T-2.\n    # This yields profits p_1, ..., p_{T-1}.\n    profits = []\n    s_prev = 0.0 # Initial position s_{-1} = 0.\n    \n    # The loop runs for t from 0 to T-2.\n    for t in range(T - 1):\n        s_t = s[t]\n        r_t_plus_1 = r[t + 1]\n        \n        # Calculate transaction cost\n        transaction_cost = c if s_t != s_prev else 0.0\n        \n        # Calculate profit for day t+1\n        profit_t_plus_1 = s_t * r_t_plus_1 - transaction_cost\n        profits.append(profit_t_plus_1)\n        \n        # Update s_prev for the next iteration\n        s_prev = s_t\n\n    # Step 3: Calculate the cumulative simple return R.\n    # R = product(1 + p_u) - 1, for u = 1, ..., T-1.\n    if not profits:\n        return 0.0\n\n    compounded_factors = [1 + p for p in profits]\n    cumulative_return = np.prod(compounded_factors) - 1.0\n    \n    return cumulative_return\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"T\": 6,\n            \"C\": np.array([100, 150, 80, 50, 200, 90]),\n            \"P\": np.array([200, 100, 120, 100, 50, 110]),\n            \"r\": np.array([0.01, -0.005, 0.008, -0.02, 0.015, 0.005]),\n            \"t_ell\": 0.8, \"t_h\": 1.2, \"c\": 0.001\n        },\n        {\n            \"T\": 5,\n            \"C\": np.array([50, 100, 200, 60, 40]),\n            \"P\": np.array([100, 50, 100, 120, 80]),\n            \"r\": np.array([0.01, 0.02, -0.01, 0.005, -0.005]),\n            \"t_ell\": 0.5, \"t_h\": 2.0, \"c\": 0.002\n        },\n        {\n            \"T\": 6,\n            \"C\": np.array([0, 10, 20, 0, 5, 0]),\n            \"P\": np.array([0, 0, 5, 0, 10, 0]),\n            \"r\": np.array([0.01, -0.02, 0.03, -0.01, 0.005, 0.002]),\n            \"t_ell\": 0.8, \"t_h\": 3.0, \"c\": 0.0015\n        },\n        {\n            \"T\": 5,\n            \"C\": np.array([300, 320, 310, 305, 315]),\n            \"P\": np.array([100, 90, 95, 100, 85]),\n            \"r\": np.array([0.01, -0.005, 0.002, 0.004, -0.003]),\n            \"t_ell\": 0.7, \"t_h\": 2.5, \"c\": 0.0005\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        R = calculate_cumulative_return(\n            case[\"T\"], case[\"C\"], case[\"P\"], case[\"r\"],\n            case[\"t_ell\"], case[\"t_h\"], case[\"c\"]\n        )\n        # Format to 6 decimal places.\n        results.append(f\"{R:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"}, {"introduction": "在生成交易信号的基础上，本练习引入了一种先进的信号处理技术，旨在从充满噪声的金融价格数据中提取更可靠的信息。你将学习应用哈尔小波变换对价格序列进行去噪，分离出潜在趋势，然后再计算移动平均收敛散度（MACD）指标。这个过程展示了如何利用复杂的数学工具来构建更稳健的交易信号 [@problem_id:2371373]。", "id": "2371373", "problem": "您收到的任务是，根据一份规格说明，从基本原理出发，构建一个程序，用于评估一个从去噪后的价格序列中派生出的简单算法交易信号。该价格序列是通过几何布朗运动（GBM）生成的模拟资产路径。去噪必须使用基于Haar基的离散小波变换并采用硬阈值处理来执行，之后对去噪后的序列计算移动平均收敛散度（MACD）指标。最终输出是为提供的一组测试用例计算出的MACD柱状图的最后一个值。\n\n数学设置：\n\n1. 通过几何布朗运动生成价格。对于每个测试用例，使用以下公式生成一个价格序列 $\\{P_t\\}_{t=0}^{T-1}$，单位为任意货币（无需物理单位），初始价格为 $P_0 = S_0$，时间步长为 $\\Delta t = dt$：\n$$\nP_{t+1} = P_t \\exp\\left(\\left(\\mu - \\tfrac{1}{2}\\sigma^2\\right)\\Delta t + \\sigma \\sqrt{\\Delta t}\\, Z_t\\right),\n$$\n其中 $\\mu$ 是漂移率，$\\sigma$ 是波动率，而 $Z_t \\sim \\mathcal{N}(0,1)$ 是独立同分布的标准正态随机变量。为确保可复现性，每个测试用例中的随机性必须由一个固定的整数种子控制。\n\n2. 通过离散Haar小波变换和硬阈值处理进行去噪。\n   - 令 $\\{x_n\\}_{n=0}^{N-1}$ 表示待去噪的观测序列，此处 $x_n = P_n$。令 $M$ 为不小于 $N$ 的最小二次幂。对 $\\{x_n\\}$ 进行对称反射右填充至长度 $M$，得到 $\\{\\tilde{x}_n\\}_{n=0}^{M-1}$。反射的定义是：围绕右端点进行镜像扩展序列，而不复制端点本身。\n   - 定义单层标准正交Haar变换，它将一个偶数长度的序列 $\\{a_n\\}_{n=0}^{L-1}$ 映射为近似系数 $\\{s_k\\}_{k=0}^{L/2-1}$ 和细节系数 $\\{d_k\\}_{k=0}^{L/2-1}$，具体如下：\n     $$\n     s_k = \\frac{a_{2k} + a_{2k+1}}{\\sqrt{2}}, \\quad d_k = \\frac{a_{2k} - a_{2k+1}}{\\sqrt{2}}.\n     $$\n     深度为 $J$ 的多层变换是通过仅在近似系数上迭代此映射 $J$ 次形成的，产生一个顶层近似 $\\{s^{(J)}\\}$ 和各层细节集 $\\{d^{(j)}\\}$（其中 $j=1,\\dots,J$）。\n   - 使用第一层细节系数的中位数绝对偏差来估计噪声水平：\n     $$\n     \\hat{\\sigma} = \\frac{\\operatorname{median}\\left(\\left|d^{(1)}_k - \\operatorname{median}(d^{(1)})\\right|\\right)}{0.67448975}.\n     $$\n     定义通用硬阈值\n     $$\n     \\lambda = k \\, \\hat{\\sigma} \\sqrt{2 \\log M},\n     $$\n     其中 $k$ 是一个给定的标量。使用相同的 $\\lambda$ 对每一层的所有细节系数应用硬阈值处理：\n     $$\n     \\tilde{d}^{(j)}_k = \\begin{cases}\n     d^{(j)}_k & \\text{if } |d^{(j)}_k| \\ge \\lambda, \\\\\n     0 & \\text{otherwise}.\n     \\end{cases}\n     $$\n     保持顶层近似不变，即 $\\tilde{s}^{(J)} = s^{(J)}$。\n   - 通过逐层使用逆变换来重构去噪后的序列 $\\{\\hat{x}_n\\}_{n=0}^{M-1}$：\n     $$\n     a_{2k} = \\frac{s_k + d_k}{\\sqrt{2}}, \\quad a_{2k+1} = \\frac{s_k - d_k}{\\sqrt{2}}.\n     $$\n     经过 $J$ 层逆变换后，截断右侧反射的填充部分，使其恢复到原始长度 $N$，得到 $\\{\\hat{x}_n\\}_{n=0}^{N-1}$。\n\n3. 对去噪后的价格计算移动平均收敛散度（MACD）。令 $\\{\\hat{P}_t\\}_{t=0}^{T-1}$ 为去噪后的价格。对于整数 $n_{\\text{fast}}$、$n_{\\text{slow}}$ 和 $n_{\\text{sig}}$，使用以下递归关系定义指数移动平均线（EMA）：\n   $$\n   \\operatorname{EMA}^{(n)}_t = \\alpha_n \\hat{P}_t + (1 - \\alpha_n)\\operatorname{EMA}^{(n)}_{t-1}, \\quad \\alpha_n = \\frac{2}{n+1}, \\quad \\operatorname{EMA}^{(n)}_0 = \\hat{P}_0.\n   $$\n   MACD线是\n   $$\n   \\operatorname{MACD}_t = \\operatorname{EMA}^{(n_{\\text{fast}})}_t - \\operatorname{EMA}^{(n_{\\text{slow}})}_t.\n   $$\n   信号线是MACD线的指数移动平均，\n   $$\n   \\operatorname{SIG}_t = \\alpha_{n_{\\text{sig}}} \\operatorname{MACD}_t + (1 - \\alpha_{n_{\\text{sig}}}) \\operatorname{SIG}_{t-1}, \\quad \\operatorname{SIG}_0 = \\operatorname{MACD}_0.\n   $$\n   柱状图是\n   $$\n   H_t = \\operatorname{MACD}_t - \\operatorname{SIG}_t.\n   $$\n   每个测试用例要求的输出是最终的柱状图值 $H_{T-1}$。\n\n测试套件：\n\n为以下四个测试用例实现上述过程。每个测试用例都指定了GBM参数 $(T, S_0, \\mu, \\sigma, dt, \\text{seed})$、小波去噪参数 $(J, k)$ 以及MACD参数 $(n_{\\text{fast}}, n_{\\text{slow}}, n_{\\text{sig}})$。所有整数和实数都已明确给出。\n\n- 用例A（一般情况）：\n  - $T = 256$, $S_0 = 100$, $\\mu = 0.06$, $\\sigma = 0.2$, $dt = \\frac{1}{252}$, $\\text{seed} = 12345$。\n  - $J = 3$, $k = 1.0$。\n  - $n_{\\text{fast}} = 12$, $n_{\\text{slow}} = 26$, $n_{\\text{sig}} = 9$。\n\n- 用例B（低噪声边界）：\n  - $T = 256$, $S_0 = 100$, $\\mu = 0.0$, $\\sigma = 0.0001$, $dt = \\frac{1}{252}$, $\\text{seed} = 2024$。\n  - $J = 3$, $k = 1.0$。\n  - $n_{\\text{fast}} = 12$, $n_{\\text{slow}} = 26$, $n_{\\text{sig}} = 9$。\n\n- 用例C（非二次幂长度和更宽的慢速窗口）：\n  - $T = 90$, $S_0 = 50$, $\\mu = 0.03$, $\\sigma = 0.3$, $dt = \\frac{1}{252}$, $\\text{seed} = 99$。\n  - $J = 4$, $k = 0.5$。\n  - $n_{\\text{fast}} = 5$, $n_{\\text{slow}} = 35$, $n_{\\text{sig}} = 5$。\n\n- 用例D（高阈值压力测试）：\n  - $T = 128$, $S_0 = 200$, $\\mu = -0.02$, $\\sigma = 0.25$, $dt = \\frac{1}{252}$, $\\text{seed} = 7$。\n  - $J = 3$, $k = 3.0$。\n  - $n_{\\text{fast}} = 12$, $n_{\\text{slow}} = 26$, $n_{\\text{sig}} = 9$。\n\n答案和输出格式：\n\n- 对于每个用例，计算最终的柱状图值 $H_{T-1}$，结果为实数。\n- 将每个结果表示为精确到 $6$ 位小数的十进制数。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[r_1,r_2,r_3,r_4]$），其中 $r_i$ 是按A、B、C、D顺序排列的用例 $i$ 的四舍五入值。", "solution": "所提出的问题是在计算信号处理应用于金融时间序列分析领域中一个完整且严格定义的练习。它具有科学依据，定义明确且客观。我们将着手解决此问题。\n\n任务是构建一个包含三个主要阶段的计算过程：\n1.  使用几何布朗运动（GBM）模型生成模拟资产价格序列。\n2.  使用离散Haar小波变换和特定的硬阈值策略对该价格序列进行去噪。\n3.  对去噪后的序列计算移动平均收敛散度（MACD）技术指标。\n\n最终输出是针对几个指定参数集的MACD柱状图的最后一个值。我们将按照规定，从基本原理出发，有条不紊地构建解决方案，处理每个阶段。\n\n**1. 通过几何布朗运动生成价格序列**\n\n资产在时间 $t$ 的价格 $P_t$ 由GBM的随机微分方程建模。为时间步长 $\\Delta t$ 提供的离散时间解为：\n$$\nP_{t+1} = P_t \\exp\\left(\\left(\\mu - \\tfrac{1}{2}\\sigma^2\\right)\\Delta t + \\sigma \\sqrt{\\Delta t}\\, Z_t\\right)\n$$\n此处，$\\mu$ 是漂移率，$\\sigma$ 是波动率，而 $Z_t$ 是来自标准正态分布 $Z_t \\sim \\mathcal{N}(0,1)$ 的独立同分布随机变量。问题要求从初始价格 $P_0 = S_0$ 开始，进行 $T$ 个时间步的模拟。为确保可复现性，每个测试用例的伪随机数生成器必须使用一个固定的种子。\n\n对于每个用例，我们将通过对 $t$ 从 $0$ 到 $T-2$ 迭代上述方程，来生成一个长度为 $T$ 的序列 $\\{P_t\\}_{t=0}^{T-1}$。\n\n**2. 小波去噪**\n\n该阶段涉及多层离散Haar小波变换（DWT）、细节系数的阈值处理以及逆变换。\n\n**2.1. 填充**\n对于指定的DWT算法，输入序列 $\\{P_n\\}_{n=0}^{N-1}$ (其中 $N=T$) 的长度 $M$ 必须是2的幂。如果 $N$ 不是2的幂，我们确定最小的整数 $M$ 使得 $M=2^k$ 且 $M \\ge N$。然后通过在右边界进行对称反射将序列扩展到长度 $M$。描述“围绕右端点进行镜像而不复制端点本身”意味着一种“反射”填充模式。对于一个序列 $\\{x_n\\}_{n=0}^{N-1}$，填充后的序列 $\\{\\tilde{x}_n\\}_{n=0}^{M-1}$ 是通过附加元素 $\\{x_{N-2}, x_{N-3}, \\dots \\}$ 构造的。令 $K=M-N$ 为要附加的元素数量。附加部分将是 $\\{x_{N-2}, x_{N-3}, ..., x_{N-1-K}\\}$。此过程仅在 $N$ 不是2的幂时才需要，这只在用例C中发生。对于所有其他用例， $N$ 都是2的幂，因此 $M=N$，不执行填充。\n\n**2.2. 正向Haar DWT**\n标准正交Haar DWT将长度为 $L$（其中 $L$ 为偶数）的序列映射为长度均为 $L/2$ 的近似系数序列 $\\{s_k\\}$ 和细节系数序列 $\\{d_k\\}$。变换如下：\n$$\ns_k = \\frac{a_{2k} + a_{2k+1}}{\\sqrt{2}}, \\quad d_k = \\frac{a_{2k} - a_{2k+1}}{\\sqrt{2}}\n$$\n通过对近似系数递归应用此分解来执行 $J$ 层变换。从 $\\{\\tilde{x}_n\\}_{n=0}^{M-1}$ 开始，我们计算第一层的细节 $\\{d^{(1)}_k\\}$ 和近似 $\\{s^{(1)}_k\\}$。然后，我们分解 $\\{s^{(1)}_k\\}$ 以获得 $\\{d^{(2)}_k\\}$ 和 $\\{s^{(2)}_k\\}$，以此类推，共进行 $J$ 层。这将产生一组系数数组：最终的近似 $\\{s^{(J)}\\}$ 和每一层的细节系数 $\\{d^{(j)}\\}$，其中 $j=1, \\dots, J$。\n\n**2.3. 阈值处理**\n通过对细节系数进行阈值处理来实现去噪。首先，从捕获最精细尺度变化的第一层细节系数 $d^{(1)}$ 中估计噪声水平 $\\hat{\\sigma}$。使用稳健的中位数绝对偏差（MAD）估计器：\n$$\n\\hat{\\sigma} = \\frac{\\operatorname{median}\\left(\\left|d^{(1)}_k - \\operatorname{median}(d^{(1)})\\right|\\right)}{C}\n$$\n其中 $C = 0.67448975 \\approx 1/\\Phi^{-1}(3/4)$ 是用于渐近正态数据的归一化常数。\n\n然后计算通用硬阈值 $\\lambda$：\n$$\n\\lambda = k \\, \\hat{\\sigma} \\sqrt{2 \\log M}\n$$\n其中 $k$ 是一个指定的缩放因子。此阈值应用于所有层级 $j=1, \\dots, J$ 的所有细节系数：\n$$\n\\tilde{d}^{(j)}_k = d^{(j)}_k \\cdot \\mathbf{1}_{|d^{(j)}_k| \\ge \\lambda} = \\begin{cases}\nd^{(j)}_k & \\text{if } |d^{(j)}_k| \\ge \\lambda, \\\\\n0 & \\text{otherwise}.\n\\end{cases}\n$$\n顶层近似系数 $\\{s^{(J)}\\}$ 不被修改，即 $\\tilde{s}^{(J)} = s^{(J)}$。这保留了信号的粗略结构。\n\n**2.4. 逆向Haar DWT与截断**\n去噪后的信号从修改后的系数 $\\{\\tilde{s}^{(J)}\\}$ 和 $\\{\\tilde{d}^{(j)}\\}$ 重构。单层逆Haar变换为：\n$$\na_{2k} = \\frac{s_k + d_k}{\\sqrt{2}}, \\quad a_{2k+1} = \\frac{s_k - d_k}{\\sqrt{2}}\n$$\n从 $\\{\\tilde{s}^{(J)}\\}$ 和 $\\{\\tilde{d}^{(J)}\\}$ 开始，我们重构出 $\\{\\tilde{s}^{(J-1)}\\}$。此过程迭代 $J$ 次，每次使用相应层级的细节系数，直到恢复出全长信号 $\\{\\hat{x}_n\\}_{n=0}^{M-1}$。\n\n最后，如果信号被填充过，我们通过取前 $N$ 个元素将重构的信号截断至其原始长度 $N$。这便得到了去噪后的价格序列 $\\{\\hat{P}_t\\}_{t=0}^{T-1}$。\n\n**3. MACD 计算**\n\nMACD指标是在去噪后的价格序列 $\\{\\hat{P}_t\\}$ 上计算的。该过程需要计算几个指数移动平均线（EMA）。周期为 $n$ 的EMA由以下递归关系定义：\n$$\n\\operatorname{EMA}^{(n)}_t = \\alpha_n \\hat{P}_t + (1 - \\alpha_n)\\operatorname{EMA}^{(n)}_{t-1}\n$$\n平滑因子为 $\\alpha_n = 2/(n+1)$，初始条件为 $\\operatorname{EMA}^{(n)}_0 = \\hat{P}_0$。\n\nMACD指标的组成部分是：\n1.  **MACD线**：‘快速’EMA（短周期 $n_{\\text{fast}}$）与‘慢速’EMA（长周期 $n_{\\text{slow}}$）之间的差值。\n    $$\n    \\operatorname{MACD}_t = \\operatorname{EMA}^{(n_{\\text{fast}})}_t - \\operatorname{EMA}^{(n_{\\text{slow}})}_t\n    $$\n2.  **信号线**：MACD线本身的EMA，周期为 $n_{\\text{sig}}$。\n    $$\n    \\operatorname{SIG}_t = \\alpha_{n_{\\text{sig}}} \\operatorname{MACD}_t + (1 - \\alpha_{n_{\\text{sig}}}) \\operatorname{SIG}_{t-1}\n    $$\n    初始条件指定为 $\\operatorname{SIG}_0 = \\operatorname{MACD}_0$。\n3.  **柱状图**：MACD线与信号线之间的差值。\n    $$\n    H_t = \\operatorname{MACD}_t - \\operatorname{SIG}_t\n    $$\n每个测试用例最终需要计算的值是 $H_{T-1}$。\n\n对于每个用例，给定其参数和随机种子，整个过程是确定性的。实现必须精确遵循这些步骤，才能得出正确的数值答案。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It orchestrates the GBM simulation, wavelet denoising, and MACD calculation.\n    \"\"\"\n\n    test_cases = [\n        # Case A\n        {'T': 256, 'S0': 100.0, 'mu': 0.06, 'sigma': 0.2, 'dt': 1/252, 'seed': 12345,\n         'J': 3, 'k': 1.0, 'n_fast': 12, 'n_slow': 26, 'n_sig': 9},\n        # Case B\n        {'T': 256, 'S0': 100.0, 'mu': 0.0, 'sigma': 0.0001, 'dt': 1/252, 'seed': 2024,\n         'J': 3, 'k': 1.0, 'n_fast': 12, 'n_slow': 26, 'n_sig': 9},\n        # Case C\n        {'T': 90, 'S0': 50.0, 'mu': 0.03, 'sigma': 0.3, 'dt': 1/252, 'seed': 99,\n         'J': 4, 'k': 0.5, 'n_fast': 5, 'n_slow': 35, 'n_sig': 5},\n        # Case D\n        {'T': 128, 'S0': 200.0, 'mu': -0.02, 'sigma': 0.25, 'dt': 1/252, 'seed': 7,\n         'J': 3, 'k': 3.0, 'n_fast': 12, 'n_slow': 26, 'n_sig': 9},\n    ]\n\n    results = []\n    \n    # Pre-calculate sqrt(2) as it is used repeatedly in wavelet transforms.\n    SQRT2 = np.sqrt(2)\n    MAD_NORMALIZATION_CONSTANT = 0.67448975\n\n    def generate_gbm(T, S0, mu, sigma, dt, seed):\n        \"\"\"Generates a price series using Geometric Brownian Motion.\"\"\"\n        rng = np.random.default_rng(seed)\n        prices = np.zeros(T)\n        prices[0] = S0\n        drift = (mu - 0.5 * sigma**2) * dt\n        diffusion = sigma * np.sqrt(dt)\n        \n        for t in range(T - 1):\n            Z = rng.standard_normal()\n            prices[t+1] = prices[t] * np.exp(drift + diffusion * Z)\n        return prices\n\n    def fwt_haar(data, level):\n        \"\"\"Performs a J-level forward discrete Haar wavelet transform.\"\"\"\n        coeffs = []\n        a = data.copy()\n        for _ in range(level):\n            s = (a[0::2] + a[1::2]) / SQRT2\n            d = (a[0::2] - a[1::2]) / SQRT2\n            coeffs.insert(0, d)\n            a = s\n        coeffs.insert(0, a)\n        return coeffs\n\n    def iwt_haar(coeffs):\n        \"\"\"Performs a J-level inverse discrete Haar wavelet transform.\"\"\"\n        a = coeffs[0]\n        for d in coeffs[1:]:\n            s_plus_d = a + d\n            s_minus_d = a - d\n            a_new = np.zeros(2 * len(a))\n            a_new[0::2] = s_plus_d / SQRT2\n            a_new[1::2] = s_minus_d / SQRT2\n            a = a_new\n        return a\n\n    def denoise_wavelet(prices, J, k):\n        \"\"\"Denoises a time series using Haar wavelet transform and hard thresholding.\"\"\"\n        N = len(prices)\n        \n        # Calculate M = smallest power of 2 >= N\n        if (N & (N - 1) == 0) and N > 0: # N is a power of two\n            M = N\n        else:\n            M = 1 << N.bit_length()\n\n        # Pad the signal if necessary\n        if N < M:\n            padded_prices = np.zeros(M)\n            padded_prices[:N] = prices\n            pad_len = M - N\n            # 'reflect' padding: mirror around the endpoint without duplication\n            pad_values = np.flip(prices[N-pad_len-1:N-1]) # numpy flip is more robust than [::-1]\n            padded_prices[N:] = pad_values\n        else:\n            padded_prices = prices\n\n        # Forward Wavelet Transform\n        coeffs = fwt_haar(padded_prices, J)\n        \n        # Noise estimation and thresholding\n        d1 = coeffs[-1]\n        med_d1 = np.median(d1)\n        mad = np.median(np.abs(d1 - med_d1))\n        sigma_hat = mad / MAD_NORMALIZATION_CONSTANT\n        \n        if sigma_hat == 0: # Prevent division by zero or invalid log\n            lambda_thresh = 0\n        else:\n            lambda_thresh = k * sigma_hat * np.sqrt(2 * np.log(M))\n\n        # Apply hard thresholding to detail coefficients\n        thresholded_coeffs = [coeffs[0]] # Keep approximation coeffs\n        for d in coeffs[1:]:\n            d_thresh = np.where(np.abs(d) >= lambda_thresh, d, 0)\n            thresholded_coeffs.append(d_thresh)\n            \n        # Inverse Wavelet Transform\n        reconstructed_signal = iwt_haar(thresholded_coeffs)\n        \n        # Truncate to original length\n        return reconstructed_signal[:N]\n\n    def calculate_ema(data, n):\n        \"\"\"Calculates Exponential Moving Average.\"\"\"\n        T = len(data)\n        ema = np.zeros(T)\n        alpha = 2 / (n + 1)\n        ema[0] = data[0]\n        for t in range(1, T):\n            ema[t] = alpha * data[t] + (1 - alpha) * ema[t-1]\n        return ema\n        \n    for case in test_cases:\n        # 1. Generate price series\n        prices = generate_gbm(case['T'], case['S0'], case['mu'], case['sigma'], case['dt'], case['seed'])\n        \n        # 2. Denoise the price series\n        denoised_prices = denoise_wavelet(prices, case['J'], case['k'])\n        \n        # 3. Calculate MACD\n        ema_fast = calculate_ema(denoised_prices, case['n_fast'])\n        ema_slow = calculate_ema(denoised_prices, case['n_slow'])\n        \n        macd_line = ema_fast - ema_slow\n        \n        # Calculate Signal line\n        sig_line = np.zeros_like(macd_line)\n        alpha_sig = 2 / (case['n_sig'] + 1)\n        sig_line[0] = macd_line[0]\n        for t in range(1, len(macd_line)):\n            sig_line[t] = alpha_sig * macd_line[t] + (1 - alpha_sig) * sig_line[t-1]\n        \n        histogram = macd_line - sig_line\n        \n        final_hist_val = histogram[-1]\n        results.append(round(final_hist_val, 6))\n\n    # Format and print the final output\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"}, {"introduction": "最后一个动手实践将带领我们从固定的、基于规则的策略，迈向能够自我优化的自适应学习系统。你将亲手实现一个$Q$学习（$Q$-learning）智能体，它能根据不同的市场状态（如牛市、熊市或震荡市）学习选择最优的交易策略（如动量策略或均值回归策略）。这个练习让你在金融场景下获得强化学习的实践经验，展示了构建动态和智能化交易模型的强大方法 [@problem_id:2371418]。", "id": "2371418", "problem": "要求您实现一个完整、可运行的程序，在一个代表市场状况的有限状态马尔可夫决策过程（MDP）中，使用 Q-learning 算法训练一个离策略（off-policy）控制智能体。智能体的状态是市场状况，其动作是算法交易策略。环境是完全指定且稳态的。您的任务是，在训练结束后，针对一小组测试用例，计算由学习到的状态-动作价值函数所导出的贪心策略。您的程序必须按照文末指定的格式，产出恰好一行的输出。\n\n这个问题的基础是：具有稳态转移概率和稳态奖励函数的有限状态马尔可夫决策过程（MDP）的定义、使用折扣因子的折扣回报的定义，以及 MDP 中的最优控制由 Bellman 最优性条件刻画这一原理。您必须在这些基础之上，实现使用 Q-learning 的离策略时间差分（TD）控制，且不得依赖任何允许范围之外的外部库。\n\n状态空间和动作空间：\n- 共有 $3$ 个状态（市场状况），按固定顺序 $[0,1,2]$ 索引，分别对应 $[\\,\\text{牛市},\\,\\text{熊市},\\,\\text{震荡市}\\,]$。\n- 共有 $3$ 个动作（交易策略），按固定顺序 $[0,1,2]$ 索引，分别对应 $[\\,\\text{动量策略},\\,\\text{均值回归策略},\\,\\text{现金/空仓}\\,]$。\n\n回合制训练与参数表：\n- 每次训练运行包含 $E$ 个回合，每个回合有固定的时域长度 $H$ 步。\n- 每个回合的初始状态从 $\\{0,1,2\\}$ 中独立且均匀地抽取。\n- 探索采用 $\\varepsilon$-贪心策略，其探索率在总训练步数 $T = E \\times H$ 内线性衰减。具体而言，如果 $t \\in \\{0,1,\\dots,T-1\\}$ 是整个训练运行的全局步数索引，则\n$$\n\\varepsilon(t) \\;=\\; \\max\\!\\left\\{\\varepsilon_{\\min},\\; \\varepsilon_{\\text{start}} \\;+\\; \\frac{t}{T-1}\\,\\big(\\varepsilon_{\\min} - \\varepsilon_{\\text{start}}\\big)\\right\\}.\n$$\n- 每个状态-动作对的学习率是一个基于访问次数 $N(s,a)$ 的递减序列，由下式给出：\n$$\n\\alpha(s,a) \\;=\\; \\frac{\\alpha_0}{1 + N(s,a)}.\n$$\n- 折扣因子为 $\\gamma \\in [0,1)$。\n- 所有的随机抽样（初始状态、$\\varepsilon$-贪心策略下的动作采样，以及下一状态的转移）都必须使用固定的伪随机数生成器种子 $7$，以保证结果的可复现性。\n- 奖励是确定性的，由奖励矩阵给出；不存在外生奖励噪声。\n- 平局打破规则：当需要对动作求最大值时（无论是在利用阶段选择动作，还是在训练后计算贪心策略），如果多个动作达到相同的最大值，则选择索引最小的动作。\n\n环境设定：\n- 奖励函数由一个矩阵 $R \\in \\mathbb{R}^{3 \\times 3}$ 给出，其中 $R[s,a]$ 是在状态 $s$ 下采取动作 $a$ 时的确定性即时奖励。\n- 转移动态由一个与动作无关的转移矩阵 $P \\in [0,1]^{3 \\times 3}$（其行和为 1，且对所有动作都相同），或者由一个与动作相关的核 $\\{P^{(a)}\\}_{a=0}^2$ 给出，其中 $P^{(a)} \\in [0,1]^{3 \\times 3}$ 且每行之和为 1。\n\n您必须实现与上述描述一致的 Q-learning（离策略 TD 控制），在训练期间使用 $\\varepsilon$-贪心行为，并仅在最后报告策略时使用贪心动作选择。\n\n测试用例集：\n请实现您的解决方案以处理以下四个测试用例。对于每个用例，请从头训练一个全新的智能体，然后按照规定的状态顺序，以包含三个整数的列表 $[\\,\\pi(0),\\,\\pi(1),\\,\\pi(2)\\,]$ 的形式报告最终的贪心策略。\n\n- 用例 A（理想路径；动作无关的动态；非零折扣）：\n    - 折扣：$\\gamma = 0.95$。\n    - 回合数与时域长度：$E = 10000$，$H = 40$。\n    - 探索：$\\varepsilon_{\\text{start}} = 0.8$，$\\varepsilon_{\\min} = 0.05$。\n    - 基础学习率：$\\alpha_0 = 0.5$。\n    - 奖励：\n    $$\n    R \\;=\\; \\begin{bmatrix}\n    0.8 & -0.3 & 0.1 \\\\\n    -0.6 & 0.5 & 0.05 \\\\\n    0.2 & 0.3 & 0.02\n    \\end{bmatrix}.\n    $$\n    - 转移（动作无关）：\n    $$\n    P \\;=\\; \\begin{bmatrix}\n    0.85 & 0.05 & 0.10 \\\\\n    0.05 & 0.80 & 0.15 \\\\\n    0.20 & 0.20 & 0.60\n    \\end{bmatrix}.\n    $$\n\n- 用例 B（边界条件；零折扣的短视控制）：\n    - 折扣：$\\gamma = 0$。\n    - 回合数与时域长度：$E = 2000$，$H = 30$。\n    - 探索：$\\varepsilon_{\\text{start}} = 0.8$，$\\varepsilon_{\\min} = 0.1$。\n    - 基础学习率：$\\alpha_0 = 0.5$。\n    - 奖励：与用例 A 中的 R 相同。\n    - 转移：与用例 A 中的 P 相同（动作无关）。\n\n- 用例 C（边缘情况；某一状态的即时奖励存在平局；动作无关的动态）：\n    - 折扣：$\\gamma = 0.95$。\n    - 回合数与时域长度：$E = 10000$，$H = 40$。\n    - 探索：$\\varepsilon_{\\text{start}} = 0.8$，$\\varepsilon_{\\min} = 0.05$。\n    - 基础学习率：$\\alpha_0 = 0.5$。\n    - 奖励：\n    $$\n    R \\;=\\; \\begin{bmatrix}\n    0.8 & -0.3 & 0.1 \\\\\n    -0.6 & 0.5 & 0.05 \\\\\n    0.3 & 0.3 & 0.02\n    \\end{bmatrix}.\n    $$\n    - 转移（动作无关）：与用例 A 中的 P 相同。\n    - 注意：在状态 2（震荡市）中，动作 0 和 1 的即时奖励相等。您的平局打破规则必须在最大化项中选择最小的索引。\n\n- 用例 D（动作相关的动态；长期效应很重要）：\n    - 折扣：$\\gamma = 0.99$。\n    - 回合数与时域长度：$E = 20000$，$H = 50$。\n    - 探索：$\\varepsilon_{\\text{start}} = 0.9$，$\\varepsilon_{\\min} = 0.05$。\n    - 基础学习率：$\\alpha_0 = 0.5$。\n    - 奖励：\n    $$\n    R \\;=\\; \\begin{bmatrix}\n    0.7 & 0.2 & 0.1 \\\\\n    -0.1 & 0.05 & 0.02 \\\\\n    0.25 & 0.35 & 0.02\n    \\end{bmatrix}.\n    $$\n    - 动作相关的转移：\n    $$\n    P^{(0)} \\;=\\; \\begin{bmatrix}\n    0.92 & 0.03 & 0.05 \\\\\n    0.99 & 0.005 & 0.005 \\\\\n    0.50 & 0.10 & 0.40\n    \\end{bmatrix},\\quad\n    P^{(1)} \\;=\\; \\begin{bmatrix}\n    0.80 & 0.05 & 0.15 \\\\\n    0.05 & 0.94 & 0.01 \\\\\n    0.40 & 0.20 & 0.40\n    \\end{bmatrix},\\quad\n    P^{(2)} \\;=\\; \\begin{bmatrix}\n    0.70 & 0.10 & 0.20 \\\\\n    0.01 & 0.98 & 0.01 \\\\\n    0.10 & 0.30 & 0.60\n    \\end{bmatrix}.\n    $$\n\n输出格式说明：\n- 在每个用例的训练完成后，使用上文描述的平局打破规则计算贪心策略 $\\pi(s) = \\arg\\max_{a \\in \\{0,1,2\\}} Q(s,a)$。\n- 您的程序应产出一行输出，其中包含所有四个用例的结果，格式为由方括号括起、无空格、由逗号分隔的列表的列表。例如，打印的行必须形如 \"[[x0,x1,x2],[y0,y1,y2],[z0,z1,z2],[w0,w1,w2]]\"，其中每个符号都是一个整数。请按顺序将这些占位符分别替换为用例 A、B、C 和 D 学习到的实际策略。", "solution": "所提出的问题是强化学习领域中一个良构（well-posed）且可通过计算验证的练习，具体而言，是将 Q-learning 算法应用于一个有限状态马尔可夫决策过程（MDP）。其任务是为一个简化的市场状况模型确定最优控制策略。问题陈述在科学上是合理的，并为获得一个唯一的、可复现的解提供了所有必要的参数和规范。我们将着手进行推导和实现。\n\n问题的核心在于马尔可夫决策过程的框架，它由一个元组 $(S, A, P, R, \\gamma)$ 定义。\n状态空间 $S$ 包含 3 种市场状况：$s \\in \\{0, 1, 2\\}$，分别对应 $[\\text{牛市}, \\text{熊市}, \\text{震荡市}]$。\n动作空间 $A$ 包含 3 种交易策略：$a \\in \\{0, 1, 2\\}$，分别对应 $[\\text{动量策略}, \\text{均值回归策略}, \\text{现金/空仓}]$。\n奖励函数 $R(s, a)$ 提供了在状态 $s$ 下采取动作 $a$ 所获得的即时、确定性奖励。\n转移概率函数 $P(s'|s, a)$ 给出了在状态 $s$ 采取动作 $a$ 后转移到状态 $s'$ 的概率。\n折扣因子 $\\gamma \\in [0, 1)$ 决定了未来奖励的现值。\n\n强化学习智能体的目标是找到一个最优策略 $\\pi^*(s)$，该策略将状态映射到动作，以最大化期望折扣回报。在策略 $\\pi$ 下，一个状态-动作对 $(s, a)$ 的价值由状态-动作价值函数 $Q^{\\pi}(s, a)$ 给出，它表示从状态 $s$ 开始、采取动作 $a$，然后遵循策略 $\\pi$ 所获得的期望回报。\n\n最优状态-动作价值函数 $Q^*(s, a) = \\max_{\\pi} Q^{\\pi}(s, a)$ 必须满足 Bellman 最优性方程：\n$$\nQ^*(s, a) = \\mathbb{E}\\left[ R_{t+1} + \\gamma \\max_{a'} Q^*(S_{t+1}, a') \\mid S_t=s, A_t=a \\right]\n$$\n对于一个确定性的奖励函数 $R(s,a)$，该方程可简化为：\n$$\nQ^*(s, a) = R(s, a) + \\gamma \\sum_{s' \\in S} P(s'|s, a) \\max_{a'} Q^*(s', a')\n$$\n\n由于转移模型 $P$ 和奖励函数 $R$ 是已知的，这个系统可以使用像值迭代这样的动态规划方法来求解。然而，该问题指定使用 Q-learning，一种无模型的时间差分（TD）学习算法。Q-learning 直接估计最优 $Q^*$ 值，而不需要关于 $P$ 和 $R$ 的显式知识（尽管在我们的案例中，它们是已知的，并用于模拟环境）。\n\nQ-learning 的工作方式是基于经验元组 $(S_t, A_t, R_{t+1}, S_{t+1})$，迭代地更新 Q 值的估计，记为 $Q(s, a)$。在训练过程的每个全局时间步 $t$，智能体处于状态 $S_t$ 并采取动作 $A_t$。它观察到奖励 $R_{t+1}$ 和下一个状态 $S_{t+1}$。更新规则是：\n$$\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha_t(S_t, A_t) \\left[ \\underbrace{R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, a)}_{\\text{TD 目标}} - Q(S_t, A_t) \\right]\n$$\n这里，$\\alpha_t(S_t, A_t)$ 是在第 $t$ 步时状态-动作对 $(S_t, A_t)$ 的学习率。为使算法收敛，学习率必须满足 Robbins-Monro 条件。在本问题中，这一点通过一个基于访问次数 $N(s, a)$ 的递减方案来近似：\n$$\n\\alpha(s, a) = \\frac{\\alpha_0}{1 + N(s, a)}\n$$\n其中给定了基础学习率 $\\alpha_0$。\n\nQ-learning 是一种离策略（off-policy）算法。这意味着它可以在遵循一个不同的、探索性策略的同时学习最优策略。该问题指定了一个 $\\varepsilon$-贪心行为策略。在每一步 $t$，智能体以概率 $\\varepsilon(t)$ 选择一个随机动作，以概率 $1 - \\varepsilon(t)$ 选择贪心动作 $a = \\arg\\max_{a'} Q(S_t, a')$。探索率 $\\varepsilon(t)$ 在总训练步数 $T = E \\times H$ 内线性衰减，从初始值 $\\varepsilon_{\\text{start}}$ 衰减到最小值 $\\varepsilon_{\\min}$：\n$$\n\\varepsilon(t) = \\max\\left\\{\\varepsilon_{\\min}, \\varepsilon_{\\text{start}} + \\frac{t}{T-1}\\left(\\varepsilon_{\\min} - \\varepsilon_{\\text{start}}\\right)\\right\\}\n$$\n这确保了在训练初期有充分的探索，并随着 Q 值估计变得更加可靠而逐渐转向利用。\n\n实现将首先把 Q 表 $Q(s, a) \\in \\mathbb{R}^{3 \\times 3}$ 初始化为全零。访问次数矩阵 $N(s, a) \\in \\mathbb{Z}^{3 \\times 3}$ 也初始化为全零。训练将运行 $E$ 个回合，每个回合持续 $H$ 步。一个种子固定为 7 的伪随机数生成器将用于所有随机元素：初始状态采样、$\\varepsilon$-贪心动作选择，以及从转移概率分布 $P(\\cdot|s, a)$ 中采样下一个状态。\n\n在每个测试用例的训练完成后，使用最终学习到的 Q 表来推导出纯贪心策略 $\\pi(s)$：\n$$\n\\pi(s) = \\arg\\max_{a \\in A} Q(s, a)\n$$\n问题指定了一个平局打破规则：如果对于给定的状态，有多个动作产生相同的最大 Q 值，则选择索引最小的动作。在 $\\varepsilon$-贪心策略的利用阶段和最终策略的推导过程中，都将严格遵守此规则。该算法将为指定的四个测试用例分别实现，并报告所得的策略。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Q-learning problem for four test cases and prints the results.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {\n            \"name\": \"A\",\n            \"gamma\": 0.95,\n            \"E\": 10000,\n            \"H\": 40,\n            \"eps_start\": 0.8,\n            \"eps_min\": 0.05,\n            \"alpha_0\": 0.5,\n            \"R\": np.array([\n                [0.8, -0.3, 0.1],\n                [-0.6, 0.5, 0.05],\n                [0.2, 0.3, 0.02]\n            ]),\n            \"P\": np.array([\n                [0.85, 0.05, 0.10],\n                [0.05, 0.80, 0.15],\n                [0.20, 0.20, 0.60]\n            ]),\n            \"action_dependent_P\": False,\n        },\n        # Case B\n        {\n            \"name\": \"B\",\n            \"gamma\": 0.0,\n            \"E\": 2000,\n            \"H\": 30,\n            \"eps_start\": 0.8,\n            \"eps_min\": 0.1,\n            \"alpha_0\": 0.5,\n            \"R\": np.array([\n                [0.8, -0.3, 0.1],\n                [-0.6, 0.5, 0.05],\n                [0.2, 0.3, 0.02]\n            ]),\n            \"P\": np.array([\n                [0.85, 0.05, 0.10],\n                [0.05, 0.80, 0.15],\n                [0.20, 0.20, 0.60]\n            ]),\n            \"action_dependent_P\": False,\n        },\n        # Case C\n        {\n            \"name\": \"C\",\n            \"gamma\": 0.95,\n            \"E\": 10000,\n            \"H\": 40,\n            \"eps_start\": 0.8,\n            \"eps_min\": 0.05,\n            \"alpha_0\": 0.5,\n            \"R\": np.array([\n                [0.8, -0.3, 0.1],\n                [-0.6, 0.5, 0.05],\n                [0.3, 0.3, 0.02]\n            ]),\n            \"P\": np.array([\n                [0.85, 0.05, 0.10],\n                [0.05, 0.80, 0.15],\n                [0.20, 0.20, 0.60]\n            ]),\n            \"action_dependent_P\": False,\n        },\n        # Case D\n        {\n            \"name\": \"D\",\n            \"gamma\": 0.99,\n            \"E\": 20000,\n            \"H\": 50,\n            \"eps_start\": 0.9,\n            \"eps_min\": 0.05,\n            \"alpha_0\": 0.5,\n            \"R\": np.array([\n                [0.7, 0.2, 0.1],\n                [-0.1, 0.05, 0.02],\n                [0.25, 0.35, 0.02]\n            ]),\n            \"P\": np.array([\n                # P for action 0\n                [[0.92, 0.03, 0.05], [0.99, 0.005, 0.005], [0.50, 0.10, 0.40]],\n                # P for action 1\n                [[0.80, 0.05, 0.15], [0.05, 0.94, 0.01], [0.40, 0.20, 0.40]],\n                # P for action 2\n                [[0.70, 0.10, 0.20], [0.01, 0.98, 0.01], [0.10, 0.30, 0.60]],\n            ]),\n            \"action_dependent_P\": True,\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        # Initialize RNG with fixed seed for reproducibility for each case\n        rng = np.random.default_rng(seed=7)\n        \n        # Extract parameters\n        gamma = case[\"gamma\"]\n        E = case[\"E\"]\n        H = case[\"H\"]\n        eps_start = case[\"eps_start\"]\n        eps_min = case[\"eps_min\"]\n        alpha_0 = case[\"alpha_0\"]\n        R = case[\"R\"]\n        P = case[\"P\"]\n        action_dependent_P = case[\"action_dependent_P\"]\n\n        num_states = 3\n        num_actions = 3\n        \n        # Initialize Q-table and visit counts\n        q_table = np.zeros((num_states, num_actions))\n        visit_counts = np.zeros((num_states, num_actions))\n        \n        total_steps = E * H\n        global_step_count = 0\n\n        # Training loop\n        for episode in range(E):\n            current_state = rng.choice(num_states)\n            \n            for step in range(H):\n                # Epsilon calculation\n                if total_steps > 1:\n                    epsilon = eps_start + (global_step_count / (total_steps - 1)) * (eps_min - eps_start)\n                    epsilon = max(eps_min, epsilon)\n                else:\n                    epsilon = eps_start\n\n                # Action selection (epsilon-greedy)\n                if rng.random() < epsilon:\n                    action = rng.choice(num_actions)\n                else:\n                    # Exploit: choose best action, np.argmax breaks ties by taking the first one\n                    action = np.argmax(q_table[current_state, :])\n                \n                # Get reward\n                reward = R[current_state, action]\n                \n                # Get next state from transition dynamics\n                if action_dependent_P:\n                    transition_probs = P[action, current_state, :]\n                else:\n                    transition_probs = P[current_state, :]\n                next_state = rng.choice(num_states, p=transition_probs)\n                \n                # Q-learning update\n                visit_counts[current_state, action] += 1\n                alpha = alpha_0 / (1 + visit_counts[current_state, action])\n                \n                max_next_q = np.max(q_table[next_state, :])\n                td_target = reward + gamma * max_next_q\n                td_error = td_target - q_table[current_state, action]\n                \n                q_table[current_state, action] += alpha * td_error\n                \n                # Move to next state\n                current_state = next_state\n                global_step_count += 1\n\n        # Derive final greedy policy\n        final_policy = []\n        for s in range(num_states):\n            # np.argmax handles the tie-breaking rule (select smallest index)\n            best_action = np.argmax(q_table[s, :])\n            final_policy.append(int(best_action))\n            \n        all_results.append(final_policy)\n\n    # Final print statement in the exact required format.\n    # The str representation of a list of lists is already in the right format.\n    # We just need to remove spaces.\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```"}]}