{"hands_on_practices": [{"introduction": "我们的第一个练习旨在揭开循环神经网络（RNN）的神秘面纱。我们将不再把 RNN 视为一个“黑箱”，而是通过一个具体的计算任务 [@problem_id:2387285]，亲手追踪信息在网络中的逐步流动，从而理解它是如何处理时间序列并生成预测的。这个过程将帮助你牢固掌握隐藏状态和循环机制这些核心概念。", "id": "2387285", "problem": "考虑一个金融背景下的二元预测任务。对于每只股票，您在一个固定的时间范围内观察到一个离散时间序列的参与度特征。在每个时间步 $t \\in \\{1,\\dots,T\\}$，特征向量为 $x_t \\in \\mathbb{R}^2$，其分量为 $x_t = (v_t, s_t)$，其中 $v_t$ 是标准化的评论速度度量，$s_t$ 是标准化的情绪得分。目标是为每个序列生成一个在 $(0,1)$ 区间内的单一概率，该概率表示该股票在下一个时间步将进入高关注度状态，此处解读为一次潜在的“网红股”事件。\n\n给定一个预测规则，定义如下。设隐藏状态维度为 $d = 3$，参数如下：\n$$\nW_{xh} = \\begin{bmatrix}\n0.8 & -0.4 \\\\\n0.3 & 0.5 \\\\\n-0.2 & 0.7\n\\end{bmatrix},\\quad\nW_{hh} = \\begin{bmatrix}\n0.1 & 0.0 & -0.3 \\\\\n0.2 & 0.4 & 0.0 \\\\\n-0.5 & 0.1 & 0.2\n\\end{bmatrix},\\quad\nb_h = \\begin{bmatrix}\n0.05 \\\\ -0.1 \\\\ 0.0\n\\end{bmatrix},\n$$\n$$\nW_{hy} = \\begin{bmatrix}\n0.6 & -0.2 & 0.4\n\\end{bmatrix},\\quad\nb_y = -0.05.\n$$\n定义隐藏状态递归，初始条件为 $h_0 = \\mathbf{0} \\in \\mathbb{R}^3$，并且对于 $t=1,\\dots,T$：\n$$\nh_t = \\tanh\\!\\left(W_{xh}\\,x_t + W_{hh}\\,h_{t-1} + b_h\\right),\n$$\n其中 $\\tanh(\\cdot)$ 按元素方式作用。在处理完整个序列后，构建 logit 值：\n$$\nz = W_{hy}\\,h_T + b_y,\n$$\n以及预测概率\n$$\np = \\sigma(z) = \\frac{1}{1 + e^{-z}}.\n$$\n\n测试套件。对于以下每个序列，您必须根据上述规则计算出相应的概率 $p$。\n\n- 情况 A（典型的参与度上升和积极情绪；$T=4$）：\n  $$\n  \\left[(0.1, 0.4),\\ (0.2, 0.5),\\ (0.3, 0.6),\\ (0.4, 0.7)\\right].\n  $$\n\n- 情况 B（零附近的近中性动态；$T=4$）：\n  $$\n  \\left[(0.0, 0.0),\\ (0.0, 0.1),\\ (0.0, -0.1),\\ (0.0, 0.0)\\right].\n  $$\n\n- 情况 C（高速度伴随负面情绪；$T=4$）：\n  $$\n  \\left[(0.5, -0.6),\\ (0.6, -0.5),\\ (0.7, -0.4),\\ (0.8, -0.3)\\right].\n  $$\n\n- 情况 D（振荡特征，包括负速度；$T=4$）：\n  $$\n  \\left[(-0.2, 0.9),\\ (0.2, -0.9),\\ (-0.1, 0.8),\\ (0.1, -0.8)\\right].\n  $$\n\n您的程序必须输出一行，其中包含按此顺序排列的对应于情况 A、B、C 和 D 的四个概率的列表。每个概率必须四舍五入到恰好 $6$ 位小数。输出格式必须是方括号内用逗号分隔的列表，例如：\n$$\n[\\text{p\\_A},\\text{p\\_B},\\text{p\\_C},\\text{p\\_D}]\n$$\n其中 $\\text{p\\_A}$、$\\text{p\\_B}$、$\\text{p\\_C}$ 和 $\\text{p\\_D}$ 分别是情况 A 到 D 的概率 $p$ 的四舍五入后的小数表示。", "solution": "问题陈述已经过验证，并被认定为有效。它具有科学依据，定义明确，客观且自洽。它描述了一个标准的离散时间循环神经网络（RNN），这是计算科学中的一个基本模型，并为确定性计算提供了所有必要的参数、初始条件和输入数据。其中没有矛盾、歧义或违反科学原则之处。因此，我们可以着手求解。\n\n该问题要求基于一个特征向量序列来计算预测概率 $p$。该模型是一个隐藏状态维度为 $d=3$ 的简单 RNN。对于从 $1$ 到固定范围 $T$ 的每个离散时间步 $t$，模型会处理一个输入特征向量 $x_t \\in \\mathbb{R}^2$。该特征向量由两个分量组成，$x_t = (v_t, s_t)$，分别代表评论速度和情绪得分。\n\n模型的核心是隐藏状态递归。隐藏状态向量 $h_t \\in \\mathbb{R}^3$ 根据以下方程随时间演化：\n$$\nh_t = \\tanh\\!\\left(W_{xh}\\,x_t + W_{hh}\\,h_{t-1} + b_h\\right)\n$$\n这个计算对 $t = 1, \\dots, T$ 执行。初始隐藏状态给定为零向量 $h_0 = \\mathbf{0} \\in \\mathbb{R}^3$。函数 $\\tanh(\\cdot)$ 是双曲正切函数，逐元素地应用于向量参数。参数是权重矩阵 $W_{xh} \\in \\mathbb{R}^{3 \\times 2}$ 和 $W_{hh} \\in \\mathbb{R}^{3 \\times 3}$，以及一个偏置向量 $b_h \\in \\mathbb{R}^{3}$。它们的值由下式给出：\n$$\nW_{xh} = \\begin{bmatrix}\n0.8 & -0.4 \\\\\n0.3 & 0.5 \\\\\n-0.2 & 0.7\n\\end{bmatrix},\\quad\nW_{hh} = \\begin{bmatrix}\n0.1 & 0.0 & -0.3 \\\\\n0.2 & 0.4 & 0.0 \\\\\n-0.5 & 0.1 & 0.2\n\\end{bmatrix},\\quad\nb_h = \\begin{bmatrix}\n0.05 \\\\ -0.1 \\\\ 0.0\n\\end{bmatrix}\n$$\n\n在处理完长度为 $T$ 的整个输入序列后，最终的隐藏状态 $h_T$ 被用来计算一个标量 logit 值 $z$。它代表了最终激活函数的输入。logit 的计算如下：\n$$\nz = W_{hy}\\,h_T + b_y\n$$\n其中 $W_{hy} \\in \\mathbb{R}^{1 \\times 3}$ 是一个权重矩阵（或向量），$b_y \\in \\mathbb{R}$ 是一个标量偏置。所提供的值为：\n$$\nW_{hy} = \\begin{bmatrix}\n0.6 & -0.2 & 0.4\n\\end{bmatrix},\\quad\nb_y = -0.05\n$$\n\n最后，通过将 sigmoid 函数 $\\sigma(\\cdot)$ 应用于 logit 值 $z$，得到预测概率 $p$。 sigmoid 函数将 logit 值压缩到区间 $(0, 1)$ 内，这是概率值所必需的。\n$$\np = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n$$\n\n每个测试用例的计算过程如下：\n$1$. 初始化隐藏状态为零向量：$h \\leftarrow \\begin{bmatrix} 0 & 0 & 0 \\end{bmatrix}^T$。\n$2$. 对于从 $1$ 到 $T=4$ 的每个时间步 $t$：\n    a. 从给定序列中取出输入向量 $x_t$。\n    b. 计算激活函数的参数：$u_t = W_{xh}\\,x_t + W_{hh}\\,h + b_h$。\n    c. 更新隐藏状态：$h \\leftarrow \\tanh(u_t)$。\n$3$. 循环结束后，$h$ 现在的值为 $h_T$。\n$4$. 计算 logit 值：$z = W_{hy}\\,h + b_y$。\n$5$. 计算最终概率：$p = 1 / (1 + \\exp(-z))$。\n\n将此确定性过程应用于四个指定的输入序列（情况 A、B、C 和 D），每个序列的长度为 $T=4$。然后收集并按要求格式化所得的四个概率。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes forecast probabilities for four equity engagement sequences using a\n    specified Recurrent Neural Network (RNN) model.\n    \"\"\"\n    \n    # Define the parameters of the RNN model.\n    # All vectors are defined as column vectors for correct matrix algebra.\n    W_xh = np.array([\n        [0.8, -0.4],\n        [0.3, 0.5],\n        [-0.2, 0.7]\n    ])\n    W_hh = np.array([\n        [0.1, 0.0, -0.3],\n        [0.2, 0.4, 0.0],\n        [-0.5, 0.1, 0.2]\n    ])\n    b_h = np.array([[0.05], [-0.1], [0.0]])\n    W_hy = np.array([[0.6, -0.2, 0.4]])\n    b_y = -0.05\n\n    # Define the test cases from the problem statement.\n    # The order of cases is fixed to A, B, C, D.\n    test_cases = [\n        # Case A: Typical rising engagement and positive sentiment\n        [(0.1, 0.4), (0.2, 0.5), (0.3, 0.6), (0.4, 0.7)],\n        # Case B: Near-neutral dynamics around zero\n        [(0.0, 0.0), (0.0, 0.1), (0.0, -0.1), (0.0, 0.0)],\n        # Case C: High velocity with negative sentiment\n        [(0.5, -0.6), (0.6, -0.5), (0.7, -0.4), (0.8, -0.3)],\n        # Case D: Oscillatory features\n        [(-0.2, 0.9), (0.2, -0.9), (-0.1, 0.8), (0.1, -0.8)]\n    ]\n\n    results = []\n    \n    # Sigmoid function for final probability calculation.\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n\n    for sequence in test_cases:\n        # Initialize hidden state h_0 = [0, 0, 0]^T.\n        # Dimension is (3, 1) to represent a column vector.\n        h = np.zeros((3, 1))\n\n        # Iterate through the time steps of the sequence.\n        for x_tuple in sequence:\n            # Reshape input tuple into a (2, 1) column vector.\n            x_t = np.array(x_tuple).reshape(2, 1)\n            \n            # Apply the RNN recurrence relation.\n            # h_t = tanh(W_xh * x_t + W_hh * h_{t-1} + b_h)\n            h = np.tanh(W_xh @ x_t + W_hh @ h + b_h)\n        \n        # After the loop, h is the final hidden state h_T.\n        # Compute the logit z = W_hy * h_T + b_y.\n        # The result of matrix multiplication is a 1x1 array; .item() extracts the scalar.\n        z = (W_hy @ h + b_y).item()\n        \n        # Compute the final probability p = sigma(z).\n        p = sigmoid(z)\n        \n        results.append(p)\n\n    # Format the results to exactly 6 decimal places and create the output string.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"}, {"introduction": "一个标准的损失函数，如均方误差，对所有类型的预测误差都一视同仁。然而，在金融预测中，错误地预测资产回报的方向（例如，预测上涨而实际下跌）通常比仅仅错估幅度要严重得多。这个练习将指导你设计并实现一个自定义损失函数 [@problem_id:2414391]，它能对方向性错误施加额外的惩罚，从而使你的模型训练更贴近实际的投资目标。", "id": "2414391", "problem": "您正在为训练一个用于预测计算经济学和金融学中资产回报的神经网络设计一个损失函数。目标是严厉惩罚那些回报方向预测错误的情况，而不仅仅是误差的大小。工作在由 $t \\in \\{1,\\dots,T\\}$ 索引的离散时间中进行。设已实现的简单回报为 $r_t \\in \\mathbb{R}$，模型的预测值为 $\\hat{r}_t \\in \\mathbb{R}$。标准的基准是均方误差，即经验风险 $(1/T)\\sum_{t=1}^T (r_t - \\hat{r}_t)^2$。\n\n从统计学习中使用的基本原理出发：\n- 样本上的经验风险最小化是基于对每个样本的损失进行平均。\n- 平方损失鼓励精确的量值匹配。\n- 为了编码方向准确性，基于间隔(margin)的惩罚使用乘积 $r_t \\hat{r}_t$ 作为带符号的一致性度量，其中 $r_t \\hat{r}_t &gt; 0$ 表示符号一致，而 $r_t \\hat{r}_t &lt; 0$ 表示不一致。\n- 当满足间隔阈值时为 $0$，而在违反间隔时线性增长的最小凸非负惩罚是仿射函数的铰链形式(hinge form)。\n\n您的任务：\n1. 根据上述原理，推导出一个凸的单样本损失 $\\ell_t$。该损失在平方误差 $(r_t - \\hat{r}_t)^2$ 的基础上增加了一个方向违规惩罚项。此惩罚项仅依赖于间隔 $r_t \\hat{r}_t$ 和两个非负超参数：惩罚权重 $\\lambda \\ge 0$ 和非负间隔阈值 $m \\ge 0$。该惩罚必须满足：\n   - 每当满足间隔要求时（即 $r_t \\hat{r}_t \\ge m$），该惩罚恒为零。\n   - 每当 $r_t \\hat{r}_t &lt; m$ 时，该惩罚是关于相对于阈值的差额的线性函数，斜率为 $\\lambda$。\n   - 对于所有的 $(r_t,\\hat{r}_t)$，该惩罚都是非负的。\n   - 使得总的单样本损失 $\\ell_t$ 对于 $\\hat{r}_t$ 是凸的。\n2. 将总体损失定义为样本平均值 $L = \\frac{1}{T}\\sum_{t=1}^T \\ell_t$。\n3. 实现一个程序，为以下 $(r,\\hat{r},\\lambda,m)$ 元组的测试套件计算 $L$。对于每种情况，$r$ 和 $\\hat{r}$ 是长度为 $T$ 的数组，$\\lambda$ 和 $m$ 是标量：\n   - 情况 A (理想情况，方向和大小均完美)：$r = [0.01,-0.02]$, $\\hat{r} = [0.01,-0.02]$, $\\lambda = 100$, $m = 0$。\n   - 情况 B (方向总是错误，零间隔)：$r = [0.01,-0.02]$, $\\hat{r} = [-0.01,0.02]$, $\\lambda = 100$, $m = 0$。\n   - 情况 C (边界/间隔测试，使用小回报和正间隔)：$r = [0.005,-0.003,0.004]$, $\\hat{r} = [0.0,-0.001,0.002]$, $\\lambda = 50$, $m = 0.001$。\n   - 情况 D (正确性混合，零间隔)：$r = [0.02,-0.01,-0.015]$, $\\hat{r} = [0.03,0.005,0.010]$, $\\lambda = 200$, $m = 0$。\n   - 情况 E (退化为纯平方误差)：$r = [0.01,-0.02,0.03]$, $\\hat{r} = [0.009,-0.018,0.029]$, $\\lambda = 0$, $m = 0.005$。\n4. 程序必须为每种情况计算标量损失 $L$，并生成单行输出，其中包含五个结果，格式为逗号分隔的列表，并用方括号括起来，每个数字四舍五入到 $6$ 位小数，例如 $[x_1,x_2,x_3,x_4,x_5]$。\n\n您的程序必须是一个完整的、可运行的实现，无需用户输入即可执行这些计算。不涉及物理单位。不使用角度。任何地方都不得使用百分比；当出现可能被非正式描述为百分比的量时，必须用小数表示。通过将回报视为小的实数来确保科学真实性，认识到方向准确性是通过 $r_t \\hat{r}_t$ 的符号来编码的，并且附加的惩罚项会强烈抑制符号错误。", "solution": "所提出的问题是在金融资产回报预测背景下，为训练神经网络设计一种专门的损失函数的任务。该问题具有科学依据，提法明确，且其所有组成部分都得到了清楚的定义。这是一个有效的数学工程问题。我将进行推导和求解。\n\n目标是为单个时间步长 $t$ 推导出一个单样本损失函数，记为 $\\ell_t$。该损失必须在标准平方误差的基础上增加一个针对方向不准确的惩罚。对于大小为 $T$ 的样本，总损失是经验风险，即这些单样本损失的样本平均值。\n\n设 $r_t$ 为时间 $t$ 的已实现回报，$\\hat{r}_t$ 为预测回报。单样本损失 $\\ell_t$ 由两部分组成：用于量值准确性的平方误差项和用于方向准确性的惩罚项，我们将其表示为 $P_t$。\n$$\n\\ell_t(\\hat{r}_t) = (r_t - \\hat{r}_t)^2 + P_t(\\hat{r}_t)\n$$\n\n惩罚项 $P_t$ 必须遵循问题陈述中概述的几项原则。\n1.  它是一个关于间隔（由乘积 $r_t \\hat{r}_t$ 定义）、惩罚权重 $\\lambda \\ge 0$ 和间隔阈值 $m \\ge 0$ 的函数。\n2.  如果满足间隔条件，即 $r_t \\hat{r}_t \\ge m$，则惩罚必须为零。\n3.  如果违反了间隔条件，即 $r_t \\hat{r}_t < m$，则惩罚必须是关于差额 $m - r_t \\hat{r}_t$ 的线性函数，斜率为 $\\lambda$。\n4.  惩罚必须是非负的，即 $P_t \\ge 0$。\n\n这些条件唯一地确定了 $P_t$ 的形式。只有当 $m - r_t \\hat{r}_t > 0$ 时，惩罚才非零，在这种情况下，惩罚等于 $\\lambda (m - r_t \\hat{r}_t)$。在所有其他情况下，它为零。这正是铰链损失(hinge loss)的定义，可以使用正部函数 $\\max(0, x)$ 紧凑地写出。\n因此，惩罚项为：\n$$\nP_t(\\hat{r}_t) = \\lambda \\cdot \\max(0, m - r_t \\hat{r}_t)\n$$\n这种形式满足所有要求。由于 $\\lambda \\ge 0$ 且 $\\max$ 函数总是非负的，所以 $P_t \\ge 0$。其他条件通过构造得以满足。\n\n完整的单样本损失函数因此为：\n$$\n\\ell_t(\\hat{r}_t) = (r_t - \\hat{r}_t)^2 + \\lambda \\cdot \\max(0, m - r_t \\hat{r}_t)\n$$\n一个关键要求是 $\\ell_t$ 必须是关于预测值 $\\hat{r}_t$ 的凸函数。由于凸函数的和也是凸函数，我们可以分别分析每一项。\n\n第一项，$L_{SE}(\\hat{r}_t) = (r_t - \\hat{r}_t)^2$，是 $\\hat{r}_t$ 的二次函数。它关于 $\\hat{r}_t$ 的二阶导数是 $\\frac{\\partial^2}{\\partial \\hat{r}_t^2} (r_t - \\hat{r}_t)^2 = 2$，这是严格为正的。因此，平方误差项是严格凸的。\n\n第二项，$P_t(\\hat{r}_t) = \\lambda \\cdot \\max(0, m - r_t \\hat{r}_t)$，也是凸的。这一点可以通过认识到它是一个凸函数 $f(x) = \\max(0,x)$ 与一个关于 $\\hat{r}_t$ 的仿射函数 $g(\\hat{r}_t) = m - r_t \\hat{r}_t$ 的复合来确定。凸函数与仿射映射的复合是凸的。由于非负标量 $\\lambda$ 保持凸性，因此 $P_t(\\hat{r}_t)$ 关于 $\\hat{r}_t$ 是凸的。\n\n由于 $\\ell_t$ 的两项都关于 $\\hat{r}_t$ 是凸的，它们的和 $\\ell_t(\\hat{r}_t)$ 也是一个凸函数。此属性对于确保最小化损失函数的优化问题是良态的(well-behaved)，以及基于梯度的方法将收敛到全局最小值至关重要。\n\n整个样本的总体损失函数 $L$ 是经验风险，定义为 $T$ 个观测值的单样本损失的平均值：\n$$\nL = \\frac{1}{T} \\sum_{t=1}^T \\ell_t = \\frac{1}{T} \\sum_{t=1}^T \\left[ (r_t - \\hat{r}_t)^2 + \\lambda \\cdot \\max(0, m - r_t \\hat{r}_t) \\right]\n$$\n这就是要实现的损失函数的最终形式。\n\n对于问题的计算部分，我们将此公式应用于所提供的五个测试案例。对于每个元组 $(r, \\hat{r}, \\lambda, m)$，其中 $r$ 和 $\\hat{r}$ 是长度为 $T$ 的向量，计算过程如下：\n1.  对每个 $t \\in \\{1,\\dots,T\\}$，计算逐元素的平方误差 $(r_t - \\hat{r}_t)^2$。\n2.  计算逐元素的间隔乘积 $r_t \\hat{r}_t$。\n3.  计算逐元素的惩罚项 $\\lambda \\cdot \\max(0, m - r_t \\hat{r}_t)$。\n4.  将每个样本点的平方误差和惩罚相加得到 $\\ell_t$。\n5.  计算这些单样本损失的平均值以求得 $L$。\n\n五种情况的计算值如下：\n- 情况 A: $L_A = 0.000000$\n- 情况 B: $L_B = 0.026000$\n- 情况 C: $L_C = 0.049828$\n- 情况 D: $L_D = 0.013650$\n- 情况 E: $L_E = 0.000002$\n\n实现将遵循此逻辑以生成所需的输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the custom loss function for several test cases in financial forecasting.\n\n    The loss function L is defined as the average of per-sample losses l_t:\n    L = (1/T) * sum_{t=1 to T} l_t\n    where l_t = (r_t - r_hat_t)^2 + lambda * max(0, m - r_t * r_hat_t)\n    - r_t: realized return\n    - r_hat_t: predicted return\n    - lambda: penalty weight\n    - m: margin threshold\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: happy path, perfect direction and magnitude\n        {'r': np.array([0.01, -0.02]), 'r_hat': np.array([0.01, -0.02]), 'lam': 100.0, 'm': 0.0},\n        # Case B: direction always wrong, zero margin\n        {'r': np.array([0.01, -0.02]), 'r_hat': np.array([-0.01, 0.02]), 'lam': 100.0, 'm': 0.0},\n        # Case C: boundary/margin test with small returns and positive margin\n        {'r': np.array([0.005, -0.003, 0.004]), 'r_hat': np.array([0.0, -0.001, 0.002]), 'lam': 50.0, 'm': 0.001},\n        # Case D: mixed correctness, zero margin\n        {'r': np.array([0.02, -0.01, -0.015]), 'r_hat': np.array([0.03, 0.005, 0.010]), 'lam': 200.0, 'm': 0.0},\n        # Case E: fallback to pure squared error\n        {'r': np.array([0.01, -0.02, 0.03]), 'r_hat': np.array([0.009, -0.018, 0.029]), 'lam': 0.0, 'm': 0.005}\n    ]\n\n    results = []\n    for case in test_cases:\n        r = case['r']\n        r_hat = case['r_hat']\n        lam = case['lam']\n        m = case['m']\n\n        # Ensure inputs are NumPy arrays for vectorized operations\n        if not isinstance(r, np.ndarray):\n            r = np.array(r)\n        if not isinstance(r_hat, np.ndarray):\n            r_hat = np.array(r_hat)\n\n        # 1. Squared error term\n        squared_error = (r - r_hat)**2\n\n        # 2. Directional penalty term\n        # The margin is the product of the realized and predicted returns\n        margin = r * r_hat\n        # The penalty is applied when the margin is below the threshold m\n        penalty = lam * np.maximum(0, m - margin)\n\n        # 3. Per-sample loss is the sum of the two terms\n        per_sample_loss = squared_error + penalty\n\n        # 4. The overall loss is the mean of the per-sample losses\n        total_loss = np.mean(per_sample_loss)\n\n        # Append the formatted result\n        results.append(f\"{total_loss:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"}, {"introduction": "在处理包含大量相关预测变量的经济数据时，模型很容易变得过于复杂并产生过拟合。本练习将引导你探索正则化（regularization）这一关键技术，以构建更简洁、更可靠的模型 [@problem_id:2414325]。你将亲手实现并对比 $L_1$ (Lasso) 和 $L_2$ (Ridge) 正则化，直观地理解为何 $L_1$ 正则化能有效进行特征选择并产生稀疏解。", "id": "2414325", "problem": "本题要求您形式化并实现一个受控比较，旨在探究 $L_1$ 与 $L_2$ 正则化在一个受神经网络启发的、用于企业盈利预测的线性模型中对特征稀疏性的影响。该模型是一个单层线性网络（即一个线性神经元），它是神经网络的一个特例，此处用于预测一个代表归一化企业盈利的标准化目标。此设计的基本原理是在凸正则化下使用均方误差进行经验风险最小化。请从以下基础开始。\n\n1. 使用均方误差的经验风险最小化：给定输入矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 和目标向量 $y \\in \\mathbb{R}^n$，选择权重 $w \\in \\mathbb{R}^d$ 以最小化\n$$\n\\frac{1}{2n} \\lVert y - X w \\rVert_2^2 + \\lambda \\, \\mathcal{R}(w),\n$$\n其中 $n$ 是样本数量，$d$ 是特征数量，$\\lambda \\ge 0$ 是正则化强度，$\\mathcal{R}$ 是一个惩罚泛函。\n\n2. $L_1$ 和 $L_2$ 正则化的定义：对于 $L_1$，定义 $\\mathcal{R}(w) = \\lVert w \\rVert_1 = \\sum_{j=1}^d \\lvert w_j \\rvert$；对于 $L_2$，定义 $\\mathcal{R}(w) = \\frac{1}{2} \\lVert w \\rVert_2^2 = \\frac{1}{2}\\sum_{j=1}^d w_j^2$。\n\n3. 特征稀疏性由支撑集大小来量化，其定义为满足 $\\lvert w_j \\rvert$ 超过一个很小的数值阈值 $\\tau > 0$ 的索引 $j \\in \\{1,\\dots,d\\}$ 的数量。\n\n构建一个确定性设计，通过使用平滑变化且相关的基函数来模拟盈利预测中常用的金融预测因子。\n\nA. 数据构建（确定性，无随机性）：\n- 设 $n = 64$ 且 $t = 0,1,2,\\dots,n-1$。\n- 使用三角函数定义四个基础预测因子：\n  - $b_1(t) = \\cos\\!\\left(2\\pi \\cdot 1 \\cdot t / n\\right)$,\n  - $b_2(t) = \\sin\\!\\left(2\\pi \\cdot 1 \\cdot t / n\\right)$,\n  - $b_3(t) = \\cos\\!\\left(2\\pi \\cdot 2 \\cdot t / n\\right)$,\n  - $b_4(t) = \\sin\\!\\left(2\\pi \\cdot 2 \\cdot t / n\\right)$。\n- 将每个 $b_j$ 标准化为零均值和单位方差，以得到 $s_j$，$j \\in \\{1,2,3,4\\}$。\n- 将目标（归一化企业盈利）定义为一个线性组合：\n  $$\n  y = 0.8\\, s_1 + 1.2\\, s_3 - 1.5\\, s_4.\n  $$\n- 为引入真实的共线性，将六个额外的预测因子定义为标准化基的混合，并加入一个小的正交扰动。设 $q(t) = \\cos\\!\\left(2\\pi \\cdot 5 \\cdot t / n\\right)$ 并将其标准化为 $s_q$。然后定义：\n  - $f_1 = s_1$,\n  - $f_2 = s_2$,\n  - $f_3 = s_3$,\n  - $f_4 = s_4$,\n  - $f_5 = 0.8\\, s_1 + 0.2\\, s_2 + 0.01\\, s_q$,\n  - $f_6 = 0.5\\, s_2 - 0.3\\, s_3 + 0.01\\, s_q$,\n  - $f_7 = 0.1\\, s_1 + 0.9\\, s_4 + 0.01\\, s_q$,\n  - $f_8 = 0.3\\, s_3 + 0.4\\, s_4 + 0.3\\, s_2 + 0.01\\, s_q$,\n  - $f_9 = 0.6\\, s_1 - 0.1\\, s_3 + 0.01\\, s_q$,\n  - $f_{10} = 0.2\\, s_2 + 0.2\\, s_3 + 0.6\\, s_4 + 0.01\\, s_q$。\n- 将每个 $f_j$ 标准化为零均值和单位方差，以构成最终的设计矩阵列 $X_{\\cdot j}$，$j \\in \\{1,\\dots,10\\}$。将 $y$ 标准化为零均值（它已经是了，但需在数值上强制执行）。这样得到 $X \\in \\mathbb{R}^{64 \\times 10}$ 和 $y \\in \\mathbb{R}^{64}$。\n\nB. 需要实现的求解器：\n- 对于 $L_2$ 正则化（岭回归），通过求解以下线性系统来获得一阶最优性条件的闭式解：\n$$\n\\left(\\frac{1}{n} X^\\top X + \\lambda I_d \\right) w = \\frac{1}{n} X^\\top y,\n$$\n其中 $I_d$ 是 $d \\times d$ 的单位矩阵。\n- 对于 $L_1$ 正则化（lasso），实现带软阈值处理的循环坐标下降法。对每个坐标 $j \\in \\{1,\\dots,d\\}$，定义\n$$\nH_j = \\frac{1}{n}\\sum_{i=1}^n X_{ij}^2 \\quad \\text{和} \\quad \\rho_j = \\frac{1}{n} \\sum_{i=1}^n X_{ij} \\left( y_i - \\sum_{k \\ne j} X_{ik} w_k \\right),\n$$\n并更新\n$$\nw_j \\leftarrow \\frac{\\operatorname{soft}(\\rho_j,\\lambda)}{H_j}, \\quad \\text{其中} \\quad \\operatorname{soft}(a,\\lambda) = \\operatorname{sign}(a)\\,\\max\\{ \\lvert a \\rvert - \\lambda, \\, 0 \\}.\n$$\n循环迭代坐标，直到经过一轮完整遍历后 $w$ 的 $\\ell_\\infty$ 变化量低于某个容差 $\\varepsilon > 0$。\n\nC. 特征稀疏性度量：\n- 将支撑集阈值定义为 $\\tau = 10^{-6}$。\n- 支撑集大小 $s(w)$ 是满足 $\\lvert w_j \\rvert > \\tau$ 的索引 $j$ 的数量。\n\n您必须：\n1. 按规定构建 $X$ 和 $y$。\n2. 实现上述的 $L_2$（岭回归）求解器和 $L_1$（lasso）求解器。\n3. 对于下方的每个测试案例，计算学得的权重 $w$ 并报告 $s(w)$。\n\n测试套件（五个案例）：\n- 案例 1：$L_2$ 正则化，$\\lambda = 0.0$。\n- 案例 2：$L_2$ 正则化，$\\lambda = 10.0$。\n- 案例 3：$L_1$ 正则化，$\\lambda = 0.0$。\n- 案例 4：$L_1$ 正则化，$\\lambda = 0.5$。\n- 案例 5：$L_1$ 正则化，$\\lambda = 2.2$。\n\n最终输出格式要求：\n- 您的程序应生成单行输出，其中包含一个方括号内的逗号分隔列表，结果顺序与测试案例相同。具体输出格式为：\n$$\n[ s(w^{(1)}), \\, s(w^{(2)}), \\, s(w^{(3)}), \\, s(w^{(4)}), \\, s(w^{(5)}) ].\n$$\n只打印此列表，不含任何额外文本。角度单位为弧度；不涉及物理单位；所有数值答案必须为整数。程序必须是完全确定性的，且无需用户输入。", "solution": "所给出的问题陈述是在计算统计学及其在金融建模中应用方面一个提法恰当且科学严谨的练习。其内容完整，定义了所有必要的参数和步骤，且没有矛盾或模糊之处。因此，该问题被认为是**有效的**，我将继续提供完整解答。\n\n任务是比较 $L_1$ 与 $L_2$ 正则化在线性预测模型中诱导稀疏性的效应。为实现此目的，我们将构建一个具有受控共线性的合成数据集，求解正则化回归问题，并量化所得权重向量的稀疏性。\n\n**A 部分：数据构建**\n\n第一步是确定性地生成设计矩阵 $X \\in \\mathbb{R}^{64 \\times 10}$ 和目标向量 $y \\in \\mathbb{R}^{64}$。样本数量为 $n=64$，特征数量为 $d=10$。\n\n1.  定义时间索引向量为 $t = [0, 1, \\dots, n-1]$。\n2.  使用三角函数在离散时间索引 $t$ 上生成四个正交基预测因子：\n    $$ b_1(t) = \\cos(2\\pi t / n), \\quad b_2(t) = \\sin(2\\pi t / n), \\quad b_3(t) = \\cos(4\\pi t / n), \\quad b_4(t) = \\sin(4\\pi t / n) $$\n3.  将每个基预测因子向量 $b_j$ 标准化，使其均值为零、方差为一。向量 $v$ 的标准化公式为 $(v - \\mu_v) / \\sigma_v$，其中 $\\mu_v$ 是均值，$\\sigma_v$是总体标准差。将标准化后的向量记为 $s_1, s_2, s_3, s_4$。\n4.  目标向量 $y$（代表归一化企业盈利）被构建为其中三个标准化基的已知线性组合：\n    $$ y = 0.8 s_1 + 1.2 s_3 - 1.5 s_4 $$\n    根据构造，$y$ 的均值为零，因此无需进一步的均值中心化处理。\n5.  为模拟经济数据中常见的多重共线性，构建了十个特征 $f_1, \\dots, f_{10}$。前四个是基预测因子本身。后六个是基的线性组合，并加入了一个小的正交扰动以避免完全线性相关。该扰动源自 $q(t) = \\cos(10\\pi t / n)$，并将其标准化为向量 $s_q$。特征定义如下：\n    \\begin{align*}\n    f_1 &= s_1 \\\\\n    f_2 &= s_2 \\\\\n    f_3 &= s_3 \\\\\n    f_4 &= s_4 \\\\\n    f_5 &= 0.8 s_1 + 0.2 s_2 + 0.01 s_q \\\\\n    f_6 &= 0.5 s_2 - 0.3 s_3 + 0.01 s_q \\\\\n    f_7 &= 0.1 s_1 + 0.9 s_4 + 0.01 s_q \\\\\n    f_8 &= 0.3 s_3 + 0.4 s_4 + 0.3 s_2 + 0.01 s_q \\\\\n    f_9 &= 0.6 s_1 - 0.1 s_3 + 0.01 s_q \\\\\n    f_{10} &= 0.2 s_2 + 0.2 s_3 + 0.6 s_4 + 0.01 s_q\n    \\end{align*}\n6.  最后，将每个特征向量 $f_j$ 标准化为零均值和单位方差。这十个标准化向量构成了最终设计矩阵 $X$ 的列。\n\n**B 部分：求解器**\n\n本题要求为一般性的经验风险最小化问题实现两个求解器：\n$$ \\min_{w \\in \\mathbb{R}^d} \\left\\{ \\frac{1}{2n} \\lVert y - X w \\rVert_2^2 + \\lambda \\mathcal{R}(w) \\right\\} $$\n\n**$L_2$ 正则化（岭回归）**\n\n对于 $L_2$ 正则化，惩罚项为 $\\mathcal{R}(w) = \\frac{1}{2} \\lVert w \\rVert_2^2$。对于任何 $\\lambda > 0$，目标函数都是严格凸的，其梯度为：\n$$ \\nabla_w J(w) = \\frac{1}{n} X^\\top(Xw - y) + \\lambda w $$\n将梯度设为零，即 $\\nabla_w J(w) = 0$，得到一阶最优性条件：\n$$ \\frac{1}{n} X^\\top X w - \\frac{1}{n} X^\\top y + \\lambda w = 0 $$\n$$ \\left( \\frac{1}{n} X^\\top X + \\lambda I_d \\right) w = \\frac{1}{n} X^\\top y $$\n这是问题陈述中指定的线性系统，其中 $I_d$ 是 $d \\times d$ 的单位矩阵。该系统可以使用标准线性代数程序直接求解。对于 $\\lambda = 0$ 的情况，这简化为普通最小二乘法 (OLS) 的正规方程。\n\n**$L_1$ 正则化 (Lasso)**\n\n对于 $L_1$ 正则化，惩罚项为 $\\mathcal{R}(w) = \\lVert w \\rVert_1 = \\sum_{j=1}^d |w_j|$。目标函数是凸的，但在任何 $w_j = 0$ 的点上不可微。我们使用迭代法求解，具体来说是循环坐标下降法。\n\n该算法一次更新一个权重坐标 $w_j$，同时保持所有其他权重 $w_k$ ($k \\neq j$) 固定。坐标 $j$ 的次梯度最优性条件导出了更新规则：\n$$ w_j \\leftarrow \\frac{\\operatorname{soft}(\\rho_j, \\lambda)}{H_j} $$\n其中软阈值算子为 $\\operatorname{soft}(a, \\lambda) = \\operatorname{sign}(a) \\max(|a| - \\lambda, 0)$，且\n$$ \\rho_j = \\frac{1}{n} \\sum_{i=1}^n X_{ij} \\left( y_i - \\sum_{k \\ne j} X_{ik} w_k \\right), \\quad H_j = \\frac{1}{n}\\sum_{i=1}^n X_{ij}^2 $$\n由于设计矩阵 $X$ 的每一列都已标准化为单位方差，因此对所有 $j \\in \\{1, \\dots, d\\}$ 都有 $H_j = 1$。更新规则简化为：\n$$ w_j \\leftarrow \\operatorname{soft}(\\rho_j, \\lambda) $$\n算法重复循环遍历所有坐标 $j=1, \\dots, d$，直到连续两次完整遍历之间权重向量 $w$ 的变化量低于一个小容差 $\\varepsilon$。在本次实现中，对 $w$ 变化的 $\\ell_\\infty$ 范数使用 $10^{-9}$ 的容差。对于 $\\lambda=0$ 的情况，软阈值算子变为恒等函数，$\\operatorname{soft}(\\rho_j, 0) = \\rho_j$，算法变为求解 OLS 正规方程的 Gauss-Seidel 方法。\n\n**C 部分：特征稀疏性**\n\n稀疏性通过权重向量 $w$ 的支撑集大小来衡量。支撑集大小 $s(w)$ 指的是绝对值大于数值阈值 $\\tau = 10^{-6}$ 的权重数量：\n$$ s(w) = |\\{j \\in \\{1, \\dots, d\\} : |w_j| > \\tau \\}| $$\n我们将为五个指定测试案例中得到的每个解向量 $w$ 计算此度量。\n\n实现将首先生成数据，然后对每个测试案例应用指定的求解器，最后为每个得到的权重向量计算支撑集大小。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Formalizes and implements a controlled comparison of L1 and L2 regularization\n    on feature sparsity in a linear forecasting model.\n    \"\"\"\n\n    # ------------------\n    # --- Parameters ---\n    # ------------------\n    N_SAMPLES = 64\n    N_FEATURES = 10\n    SPARSITY_THRESHOLD = 1e-6\n    LASSO_TOLERANCE = 1e-9\n    LASSO_MAX_ITER = 5000\n\n    # ----------------------------\n    # --- Part A: Data Construction ---\n    # ----------------------------\n\n    def standardize(v):\n        \"\"\"Standardizes a vector to have zero mean and unit variance.\"\"\"\n        # Using ddof=0 for population standard deviation, as is standard in ML.\n        return (v - v.mean()) / v.std(ddof=0)\n\n    t = np.arange(N_SAMPLES)\n\n    # Base predictors\n    b1 = np.cos(2 * np.pi * 1 * t / N_SAMPLES)\n    b2 = np.sin(2 * np.pi * 1 * t / N_SAMPLES)\n    b3 = np.cos(2 * np.pi * 2 * t / N_SAMPLES)\n    b4 = np.sin(2 * np.pi * 2 * t / N_SAMPLES)\n\n    # Standardized bases\n    s1 = standardize(b1)\n    s2 = standardize(b2)\n    s3 = standardize(b3)\n    s4 = standardize(b4)\n\n    # Target vector\n    y = 0.8 * s1 + 1.2 * s3 - 1.5 * s4\n    # Enforce zero mean numerically, as specified.\n    y = y - y.mean()\n\n    # Perturbation vector\n    q = np.cos(2 * np.pi * 5 * t / N_SAMPLES)\n    s_q = standardize(q)\n\n    # Feature construction\n    f_vectors = [\n        s1,\n        s2,\n        s3,\n        s4,\n        0.8 * s1 + 0.2 * s2 + 0.01 * s_q,\n        0.5 * s2 - 0.3 * s3 + 0.01 * s_q,\n        0.1 * s1 + 0.9 * s4 + 0.01 * s_q,\n        0.3 * s3 + 0.4 * s4 + 0.3 * s2 + 0.01 * s_q,\n        0.6 * s1 - 0.1 * s3 + 0.01 * s_q,\n        0.2 * s2 + 0.2 * s3 + 0.6 * s4 + 0.01 * s_q,\n    ]\n\n    # Final design matrix X\n    X = np.zeros((N_SAMPLES, N_FEATURES))\n    for j, f_vec in enumerate(f_vectors):\n        X[:, j] = standardize(f_vec)\n\n    # -----------------------\n    # --- Part B: Solvers ---\n    # -----------------------\n\n    def ridge_solver(X_mat, y_vec, lam):\n        \"\"\"Solves Ridge regression using the closed-form solution.\"\"\"\n        n, d = X_mat.shape\n        A = (1 / n) * (X_mat.T @ X_mat) + lam * np.identity(d)\n        b = (1 / n) * (X_mat.T @ y_vec)\n        w = np.linalg.solve(A, b)\n        return w\n\n    def soft_threshold(a, lam):\n        \"\"\"Soft-thresholding operator.\"\"\"\n        return np.sign(a) * np.maximum(np.abs(a) - lam, 0)\n    \n    def lasso_solver(X_mat, y_vec, lam):\n        \"\"\"Solves Lasso regression using cyclic coordinate descent.\"\"\"\n        n, d = X_mat.shape\n        w = np.zeros(d)\n        \n        # H_j = (1/n) * sum(X_ij^2). Since columns are standardized to unit variance, H_j = 1.\n        # We can verify this for safety.\n        H_j = np.sum(X_mat**2, axis=0) / n\n\n        for _ in range(LASSO_MAX_ITER):\n            w_old = np.copy(w)\n            for j in range(d):\n                # Calculate rho_j according to the definition\n                # rho_j = (1/n) * X_j^T * (y - sum_{k!=j} X_k w_k)\n                y_pred = X_mat @ w\n                residual_without_j = y_vec - y_pred + X_mat[:, j] * w[j]\n                rho_j = (1 / n) * (X_mat[:, j].T @ residual_without_j)\n                \n                # Update w_j\n                w[j] = soft_threshold(rho_j, lam) / H_j[j]\n\n            if np.linalg.norm(w - w_old, ord=np.inf) < LASSO_TOLERANCE:\n                break\n        return w\n\n    # --------------------------------\n    # --- Part C: Sparsity Metric & Execution ---\n    # --------------------------------\n\n    def calculate_support_size(w_vec):\n        \"\"\"Calculates the support size of a weight vector.\"\"\"\n        return np.sum(np.abs(w_vec) > SPARSITY_THRESHOLD)\n\n    test_cases = [\n        {'type': 'l2', 'lambda': 0.0},\n        {'type': 'l2', 'lambda': 10.0},\n        {'type': 'l1', 'lambda': 0.0},\n        {'type': 'l1', 'lambda': 0.5},\n        {'type': 'l1', 'lambda': 2.2},\n    ]\n\n    results = []\n    for case in test_cases:\n        lam = case['lambda']\n        if case['type'] == 'l2':\n            w_solution = ridge_solver(X, y, lam)\n        else: # type == 'l1'\n            w_solution = lasso_solver(X, y, lam)\n        \n        sparsity = calculate_support_size(w_solution)\n        results.append(sparsity)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}]}