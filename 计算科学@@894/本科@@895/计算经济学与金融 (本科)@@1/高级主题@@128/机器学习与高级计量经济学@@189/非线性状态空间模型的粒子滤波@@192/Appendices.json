{"hands_on_practices": [{"introduction": "这项初步实践要求你实现一个完整的自举粒子滤波器 (bootstrap particle filter)，这是现代状态估计的基石，并将其应用于一个经典的金融随机波动率模型。目标不仅仅是实现算法，更在于探索：通过比较一个分散的先验和一个有偏的精确先验，你将直接观察到滤波器的初始信念如何影响其早期性能。这个练习 [@problem_id:2418266] 对构建编程技能和深入、实践地理解粒子滤波器如何在不确定性面前工作至关重要。", "id": "2418266", "problem": "实现并分析一个序贯蒙特卡洛（SMC）粒子滤波器，该滤波器用于一个与计算金融学相关的非线性状态空间模型，并量化估计性能对初始粒子分布 $p(x_0)$ 的敏感性。您必须比较一个扩散先验与一个尖锐但错误的先验。您的程序必须是一个完整的、可运行的脚本，能够模拟数据、运行滤波器，并为一个小型测试套件输出量化指标。\n\n潜在状态模型是一个常用于金融回报率的随机波动率模型：\n- 状态转移（潜在对数波动率）：$x_t = \\mu + \\phi \\left(x_{t-1} - \\mu\\right) + \\sigma_v \\,\\eta_t$，其中 $\\eta_t \\sim \\mathcal{N}(0,1)$ 在 $t$ 上独立。\n- 观测（回报率）：$y_t \\mid x_t \\sim \\mathcal{N}\\left(0, \\exp(x_t)\\right)$ 在给定 $x_t$ 的情况下，在 $t$ 上独立。\n\n假设以下基础：\n- 如上所述的潜在马尔可夫过程以及给定状态下观测值的条件独立性。\n- 目标是使用粒子滤波器来近似滤波分布序列 $p(x_t \\mid y_{1:t})$。\n- 待实现的粒子滤波器是自助粒子滤波器，它使用转移密度作为提议分布。\n\n需要执行的任务：\n- 为每个测试案例从模型中模拟一个数据集 $\\{(x_t, y_t)\\}_{t=1}^T$。从转移动态所蕴含的平稳分布中初始化 $x_0$，即 $x_0 \\sim \\mathcal{N}\\!\\left(\\mu, \\frac{\\sigma_v^2}{1-\\phi^2}\\right)$，然后对 $t \\in \\{1,\\dots,T\\}$ 向前生成 $(x_t, y_t)$。\n- 实现具有 $N$ 个粒子、系统重采样以及有效样本量阈值为 $N/2$ 的自助粒子滤波器。\n- 使用对数权重以避免数值下溢，并计算每个时间点 $t$ 的滤波均值 $\\hat{m}_t = \\sum_{i=1}^N w_t^{(i)} x_t^{(i)}$，其中 $w_t^{(i)}$ 是时间 $t$ 的归一化权重。\n- 对于每个测试案例，在相同的模拟数据上运行两次粒子滤波器：\n  1. 扩散先验：$x_0^{(i)} \\sim \\mathcal{N}(m_0, s_0^2)$，其中 $m_0 = \\mu$ 且 $s_0 = 3.0$。\n  2. 尖锐但错误的先验：$x_0^{(i)} \\sim \\mathcal{N}(m_b, s_b^2)$，其中 $m_b = \\mu + 2\\,\\sigma_x$ 且 $s_b = 0.05$，其中 $\\sigma_x = \\sigma_v / \\sqrt{1-\\phi^2}$ 是 $x_t$ 的平稳标准差。\n- 对于每次运行，计算前 $H$ 步的均方根误差：\n$$\\mathrm{RMSE}_{1:H} = \\sqrt{\\frac{1}{H} \\sum_{t=1}^H \\left(\\hat{m}_t - x_t\\right)^2}.$$\n- 对于每个测试案例，计算比率\n$$R = \\frac{\\mathrm{RMSE}^{\\text{peaked}}_{1:H}}{\\mathrm{RMSE}^{\\text{diffuse}}_{1:H}},$$\n这样 $R > 1$ 表明扩散先验比尖锐但错误的先验产生更低的早期阶段误差。\n\n算法要求：\n- 使用系统重采样。\n- 使用有效样本量 $ESS = \\left(\\sum_{i=1}^N (w_t^{(i)})^2 \\right)^{-1}$ 并在 $ESS < N/2$ 时触发重采样。\n- 内部使用对数权重以保证数值稳定性。\n- 所有模拟和滤波器必须由具有如下指定的固定种子的伪随机数生成器驱动，以确保确定性。\n\n测试套件：\n- 对于下面的每个参数集，使用给定的种子模拟一次数据，然后使用不同的固定种子运行两个滤波器（扩散先验和尖锐先验）。为每个案例报告比率 $R$。\n- 案例 1（基线持续性和大量粒子）：\n  - $\\mu = -0.7$, $\\phi = 0.95$, $\\sigma_v = 0.15$, $T = 200$, $H = 25$, $N = 1000$。\n  - 数据模拟种子：$2023001$。\n  - 粒子滤波器种子：扩散 = $9020001$，尖锐 = $9020002$。\n- 案例 2（少量粒子，权重退化风险更高）：\n  - $\\mu = -0.7$, $\\phi = 0.95$, $\\sigma_v = 0.15$, $T = 200$, $H = 25$, $N = 150$。\n  - 数据模拟种子：$2023002$。\n  - 粒子滤波器种子：扩散 = $9020011$，尖锐 = $9020012$。\n- 案例 3（高持续性，对 $p(x_0)$ 更强的敏感性）：\n  - $\\mu = -0.7$, $\\phi = 0.985$, $\\sigma_v = 0.15$, $T = 200$, $H = 25$, $N = 1000$。\n  - 数据模拟种子：$2023003$。\n  - 粒子滤波器种子：扩散 = $9020021$，尖锐 = $9020022$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，顺序与测试套件案例的顺序完全一致：$[R_1,R_2,R_3]$。\n- 每个 $R_j$ 必须是精确到六位小数的浮点数。\n- 不应打印任何其他文本。\n\n此问题不涉及物理单位。在随机数生成中可能隐含出现的任何角度都是不相关的；不需要角度单位。\n\n您的程序必须是自包含的，不需要任何输入，并且在指定的种子下是可复现的。它必须实现所有必需的步骤，不依赖任何外部数据。输出必须完全由上述规范和您的确定性伪随机数使用方式决定。", "solution": "问题陈述已被解析和验证。经判定，该问题具有科学依据、提法恰当、客观且完整。它描述了一项标准的计算统计任务：将序贯蒙特卡洛（SMC）方法应用于金融计量经济学中常见的状态空间模型。所有参数、算法和评估指标都已明确指定，没有歧义。因此，该问题被视为有效，并将构建解决方案。\n\n该问题要求实现一个自助粒子滤波器，以估计随机波动率模型中的潜在对数波动率 $x_t$。该模型由两个方程定义：\n\n1. 潜在对数波动率过程 $x_t$ 的状态转移方程。这是一个一阶自回归过程，AR(1)：\n$$x_t = \\mu + \\phi \\left(x_{t-1} - \\mu\\right) + \\sigma_v \\,\\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0,1)$$\n这里，$\\mu$ 是过程的长期均值，$\\phi$ 是持续性参数，$\\sigma_v$ 是对数波动率的波动率。为使过程平稳，要求 $|\\phi| < 1$。该过程的平稳分布是高斯分布：\n$$x_t \\sim \\mathcal{N}\\!\\left(\\mu, \\frac{\\sigma_v^2}{1-\\phi^2}\\right)$$\n\n2. 金融回报率 $y_t$ 的观测方程。给定当前对数波动率，回报率是条件独立的，并服从均值为 $0$、方差为 $\\exp(x_t)$ 的正态分布：\n$$y_t \\mid x_t \\sim \\mathcal{N}\\left(0, \\exp(x_t)\\right)$$\n这种表述捕捉了众所周知的波动率聚集效应，即大的（小的）回报率很可能跟随有大的（小的）回报率。\n\n目标是近似 $t = 1, \\dots, T$ 的滤波分布 $p(x_t \\mid y_{1:t})$。自助粒子滤波器通过顺序更新一组 $N$ 个加权随机样本（或称“粒子”）$\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$ 来实现这一目标。这些粒子代表了滤波分布的离散近似。该算法对每个时间步 $t=1, \\dots, T$ 进行迭代。\n\n本文实现的算法遵循以下标准步骤，从近似 $p(x_{t-1} \\mid y_{1:t-1})$ 的一组粒子 $\\{x_{t-1}^{(i)}\\}_{i=1}^N$ 及其相关的归一化权重 $\\{w_{t-1}^{(i)}\\}_{i=1}^N$ 开始：\n\n**步骤 1：重采样。** 权重退化是粒子滤波器中的一个常见问题，即经过几次迭代后，一个粒子的权重会接近于1，而所有其他粒子的权重则接近于0。为了缓解这个问题，我们计算有效样本量 $ESS = \\left(\\sum_{i=1}^N (w_{t-1}^{(i)})^2 \\right)^{-1}$。如果 $ESS$ 低于指定的阈值 $N/2$，则执行重采样步骤。使用系统重采样，这是一种高效的方差缩减技术。它涉及从现有集合 $\\{x_{t-1}^{(i)}\\}_{i=1}^N$ 中有放回地选择 $N$ 个新粒子，其中选择粒子 $i$ 的概率是其权重 $w_{t-1}^{(i)}$。重采样后，新的粒子集是未加权的，因此所有权重都重置为 $w_{t-1}^{(i)} = 1/N$。\n\n**步骤 2：传播。** 每个粒子根据状态转移动态随时间向前演化。由于使用自助滤波器，提议分布就是状态转移密度本身。对于每个粒子 $i$，抽取一个新状态：\n$$x_t^{(i)} \\sim p(x_t \\mid x_{t-1}^{(i)}) = \\mathcal{N}\\left(\\mu + \\phi(x_{t-1}^{(i)} - \\mu), \\sigma_v^2\\right)$$\n这会产生一组新的粒子 $\\{x_t^{(i)}\\}_{i=1}^N$，它代表了在时间 $t$ 的先验分布 $p(x_t \\mid y_{1:t-1})$。\n\n**步骤 3：加权。** 更新重要性权重以包含新的观测值 $y_t$。每个粒子的更新后（未归一化）权重是其先前权重与给定新粒子状态 $x_t^{(i)}$ 下观测值 $y_t$ 的似然的乘积：\n$$\\tilde{w}_t^{(i)} = w_{t-1}^{(i)} \\cdot p(y_t \\mid x_t^{(i)})$$\n似然 $p(y_t \\mid x_t^{(i)})$ 由在 $y_t$ 处求值的 $\\mathcal{N}(0, \\exp(x_t^{(i)}))$ 的概率密度函数给出。为了数值稳定性，计算在对数域中执行：\n$$\\log \\tilde{w}_t^{(i)} = \\log w_{t-1}^{(i)} + \\log p(y_t \\mid x_t^{(i)})$$\n其中 $\\log p(y_t \\mid x_t^{(i)}) = -\\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}x_t^{(i)} - \\frac{y_t^2}{2\\exp(x_t^{(i)})}$。\n\n**步骤 4：归一化和估计。** 对未归一化的对数权重进行归一化，以确保它们的和为 1。首先，为防止数值溢出，在进行指数运算前减去最大对数权重。归一化后的权重是：\n$$w_t^{(i)} = \\frac{\\exp(\\log \\tilde{w}_t^{(i)})}{\\sum_{j=1}^N \\exp(\\log \\tilde{w}_t^{(j)})}$$\n然后，时间 $t$ 的状态均值的滤波估计计算为粒子的加权平均值：\n$$\\hat{m}_t = \\sum_{i=1}^N w_t^{(i)} x_t^{(i)}$$\n\n该分析要求比较滤波器在两种不同初始粒子分布 $p(x_0)$ 下的性能。一个“扩散”先验，$\\mathcal{N}(\\mu, 3.0^2)$，以真实平稳均值为中心但具有高方差。这反映了较弱的先验知识。一个“尖锐但错误”的先验，$\\mathcal{N}(\\mu + 2\\sigma_x, 0.05^2)$（其中 $\\sigma_x$ 是平稳标准差），反映了强烈但错误的置信度。通过在初始区段 $H$ 上的均方根误差（$\\mathrm{RMSE}$）之比 $R$ 来量化这种比较：\n$$R = \\frac{\\mathrm{RMSE}^{\\text{peaked}}_{1:H}}{\\mathrm{RMSE}^{\\text{diffuse}}_{1:H}}, \\quad \\mathrm{其中} \\quad \\mathrm{RMSE}_{1:H} = \\sqrt{\\frac{1}{H} \\sum_{t=1}^H \\left(\\hat{m}_t - x_t\\right)^2}$$\n比率 $R > 1$ 表明扩散先验在滤波的初始阶段表现更优。这是预料之中的，因为一个尖锐但错误的先验可能会误导滤波器，尤其是在状态演化缓慢的高度持续性过程中，需要更多时间让数据来纠正初始误差。实现将遵循这些原则，使用指定的参数和随机种子以确保可复现性。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef systematic_resample(weights, rng):\n    \"\"\"\n    Performs systematic resampling.\n\n    Args:\n        weights (np.ndarray): Array of normalized particle weights.\n        rng (np.random.Generator): A numpy random number generator.\n\n    Returns:\n        np.ndarray: Indices of resampled particles.\n    \"\"\"\n    N = len(weights)\n    # Generate N ordered pointers from a single random draw\n    u = rng.uniform(0.0, 1.0 / N)\n    positions = u + np.arange(N) / N\n    \n    # Calculate cumulative sum of weights\n    cumulative_weights = np.cumsum(weights)\n    \n    # Find indices of particles to keep\n    indices = np.searchsorted(cumulative_weights, positions)\n    return indices\n\ndef simulate_data(mu, phi, sigma_v, T, seed):\n    \"\"\"\n    Simulates data from the stochastic volatility model.\n\n    Args:\n        mu (float): Long-run mean of log-volatility.\n        phi (float): Persistence parameter.\n        sigma_v (float): Volatility of log-volatility.\n        T (int): Number of time steps.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: True states x (T+1) and observations y (T).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    x = np.zeros(T + 1)\n    y = np.zeros(T)\n    \n    # Stationary distribution for x_0\n    if abs(phi) < 1:\n        sigma_x_sq = sigma_v**2 / (1 - phi**2)\n        sigma_x = np.sqrt(sigma_x_sq)\n        x[0] = mu + sigma_x * rng.standard_normal()\n    else: # Handle non-stationary case if necessary, here we assume it's stationary.\n        x[0] = mu\n        \n    for t in range(1, T + 1):\n        # State transition\n        x[t] = mu + phi * (x[t-1] - mu) + sigma_v * rng.standard_normal()\n        \n        # Observation\n        obs_std_dev = np.exp(x[t] / 2.0)\n        y[t-1] = obs_std_dev * rng.standard_normal()\n        \n    return x, y\n\ndef run_particle_filter(y_data, mu, phi, sigma_v, N, m_prior, s_prior, seed, H):\n    \"\"\"\n    Runs the bootstrap particle filter for the stochastic volatility model.\n\n    Args:\n        y_data (np.ndarray): Array of observations.\n        mu, phi, sigma_v (float): Model parameters.\n        N (int): Number of particles.\n        m_prior, s_prior (float): Mean and std dev for the initial particle distribution.\n        seed (int): Seed for the filter's random number generator.\n        H (int): Horizon for RMSE calculation (unused in function, for context).\n\n    Returns:\n        np.ndarray: Array of filtered state mean estimates.\n    \"\"\"\n    T = len(y_data)\n    rng = np.random.default_rng(seed)\n    \n    # Initialization (t=0)\n    # This represents the particle approximation of p(x_0)\n    particles_tm1 = rng.normal(loc=m_prior, scale=s_prior, size=N)\n    weights_tm1 = np.full(N, 1.0 / N)\n    \n    estimates = np.zeros(T)\n    \n    # Use precomputed constant for log-likelihood\n    LOG_2PI_HALF = 0.5 * np.log(2 * np.pi)\n\n    # Main loop for t=1,...,T (Python index 0 to T-1)\n    for t in range(T):\n        y_obs = y_data[t]\n        \n        # --- Step 1: Resampling (based on weights from t-1) ---\n        ess = 1.0 / np.sum(weights_tm1**2)\n        if ess < N / 2.0:\n            indices = systematic_resample(weights_tm1, rng)\n            particles_tm1 = particles_tm1[indices]\n            # After resampling, weights are implicitly reset\n            log_weights_tm1 = np.full(N, -np.log(N))\n        else:\n            log_weights_tm1 = np.log(weights_tm1)\n\n        # --- Step 2: Propagation (from t-1 to t) ---\n        noise = rng.standard_normal(N)\n        particles_t = mu + phi * (particles_tm1 - mu) + sigma_v * noise\n\n        # --- Step 3: Weighting (using observation y_t) ---\n        log_var = particles_t\n        log_likelihoods = -LOG_2PI_HALF - 0.5 * log_var - (y_obs**2) / (2.0 * np.exp(log_var))\n        \n        # Update log-weights\n        unnorm_log_weights_t = log_weights_tm1 + log_likelihoods\n        \n        # Normalize weights for stability\n        max_log_weight = np.max(unnorm_log_weights_t)\n        temp_weights = np.exp(unnorm_log_weights_t - max_log_weight)\n        weights_t = temp_weights / np.sum(temp_weights)\n\n        # --- Step 4: Estimation ---\n        estimates[t] = np.sum(weights_t * particles_t)\n\n        # --- Step 5: Prepare for next iteration ---\n        particles_tm1 = particles_t\n        weights_tm1 = weights_t\n        \n    return estimates\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {'mu': -0.7, 'phi': 0.95, 'sigma_v': 0.15, 'T': 200, 'H': 25, 'N': 1000,\n         'data_seed': 2023001, 'pf_seeds': {'diffuse': 9020001, 'peaked': 9020002}},\n        # Case 2\n        {'mu': -0.7, 'phi': 0.95, 'sigma_v': 0.15, 'T': 200, 'H': 25, 'N': 150,\n         'data_seed': 2023002, 'pf_seeds': {'diffuse': 9020011, 'peaked': 9020012}},\n        # Case 3\n        {'mu': -0.7, 'phi': 0.985, 'sigma_v': 0.15, 'T': 200, 'H': 25, 'N': 1000,\n         'data_seed': 2023003, 'pf_seeds': {'diffuse': 9020021, 'peaked': 9020022}},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        mu, phi, sigma_v, T, H, N = case['mu'], case['phi'], case['sigma_v'], case['T'], case['H'], case['N']\n        \n        # Simulate data once for the case\n        x_true, y_data = simulate_data(mu, phi, sigma_v, T, case['data_seed'])\n\n        # Define prior parameters\n        # Diffuse prior\n        m0_diffuse = mu\n        s0_diffuse = 3.0\n        \n        # Peaked, incorrect prior\n        sigma_x = sigma_v / np.sqrt(1 - phi**2)\n        m0_peaked = mu + 2.0 * sigma_x\n        s0_peaked = 0.05\n        \n        # Run filters\n        estimates_diffuse = run_particle_filter(\n            y_data, mu, phi, sigma_v, N, m0_diffuse, s0_diffuse, case['pf_seeds']['diffuse'], H\n        )\n        estimates_peaked = run_particle_filter(\n            y_data, mu, phi, sigma_v, N, m0_peaked, s0_peaked, case['pf_seeds']['peaked'], H\n        )\n        \n        # Compute RMSE over the first H steps\n        # x_true[0] is x_0, x_true[1:H+1] corresponds to y_data[0:H]\n        errors_diffuse = estimates_diffuse[:H] - x_true[1:H+1]\n        rmse_diffuse = np.sqrt(np.mean(errors_diffuse**2))\n        \n        errors_peaked = estimates_peaked[:H] - x_true[1:H+1]\n        rmse_peaked = np.sqrt(np.mean(errors_peaked**2))\n        \n        # Compute the ratio R\n        ratio_R = rmse_peaked / rmse_diffuse\n        results.append(f\"{ratio_R:.6f}\")\n\n    # Print final output in the required format\n    print(f\"[{','.join(results)}]\")\n\n# Execute the solution\nsolve()\n```"}, {"introduction": "掌握了状态估计之后，我们现在转向估计模型的结构参数，例如体系变化 (regime change) 发生的时间。这个练习 [@problem_id:2418273] 提出了一个结构突变检测问题。由于一个简化的假设，即过程噪声为零 ($ \\sigma_{\\eta} = 0 $)，对于任何假设的突变时间，状态路径都变得确定。这个特例让你能够纯粹地关注最大似然估计的原理，而不涉及滤波的复杂性，为参数推断的工作方式提供了概念基础。请将此视为一个关键的跳板：如果过程噪声不为零，你就需要运行一个完整的粒子滤波器 (如我们的第一个练习) 来为每个潜在的突变时间评估其似然性。", "id": "2418273", "problem": "考虑一个由非线性状态空间模型生成的单变量时间序列，该模型在控制潜变量动态的一个时不变参数上存在单个结构性断点。令潜变量状态记为 $x_t$，观测序列记为 $y_t$，其中 $t \\in \\{1,\\dots,T\\}$。模型设定如下：\n- 状态转移方程：$x_t = \\theta_t + \\rho \\, x_{t-1} + \\sigma_{\\eta} \\, \\eta_t$，其中 $\\eta_t \\sim \\mathcal{N}(0,1)$。\n- 观测方程：$y_t = \\frac{1}{2} x_t^2 + \\sigma_{\\varepsilon} \\, \\varepsilon_t$，其中 $\\varepsilon_t \\sim \\mathcal{N}(0,1)$。\n- 参数 $\\theta_t$ 的断点结构：存在一个未知断点时间 $\\tau \\in \\{1,\\dots,T\\}$，使得当 $t \\le \\tau$ 时，$\\theta_t = \\theta_1$；当 $t > \\tau$ 时，$\\theta_t = \\theta_2$。$\\tau$ 的先验分布是在 $\\{1,\\dots,T\\}$ 上的均匀分布。\n- 初始条件 $x_0$ 为已知。\n\n您的任务是构建一个程序，对于下方的每个测试用例，该程序接收模型和由所提供的数据生成过程所隐含的观测时间序列，并返回结构性断点时间的最大后验估计 $\\hat{\\tau} \\in \\{1,\\dots,T\\}$，该估计以整数形式报告。\n\n对于所有测试用例的数据生成，均应用以下条件：\n- 状态创新项的标准差为零，即 $\\sigma_{\\eta} = 0$, 因此给定参数和 $\\tau$，$x_t$ 的演化是确定性的。\n- 观测创新项的标准差为 $\\sigma_{\\varepsilon} = 0.1$。\n- 自回归系数为 $\\rho = 0.7$。\n- 初始状态为 $x_0 = 0$。\n- 时间范围为 $T = 30$。\n- 观测噪声的抽样 $\\{\\varepsilon_t\\}_{t=1}^{T}$ 是固定的，并由以下列表给出\n$\\big[\\, 0.03,\\,-0.02,\\,0.01,\\,0.00,\\,-0.01,\\,0.02,\\,-0.03,\\,0.04,\\,-0.02,\\,0.01,\\,0.00,\\,-0.04,\\,0.05,\\,-0.01,\\,0.02,\\,-0.02,\\,0.03,\\,-0.03,\\,0.01,\\,0.00,\\,0.02,\\,-0.01,\\,0.04,\\,-0.02,\\,0.01,\\,-0.03,\\,0.02,\\,0.00,\\,-0.01,\\,0.03\\,\\big]$。\n- 对于每个具有指定 $(\\theta_1,\\theta_2,\\tau_{\\text{true}})$ 的测试用例，通过递归 $x_t = \\theta_t + \\rho \\, x_{t-1}$（其中 $\\theta_t$ 由 $\\tau_{\\text{true}}$ 定义）确定性地构建潜变量路径 $\\{x_t\\}$，然后使用上面给定的固定 $\\{\\varepsilon_t\\}$，通过 $y_t = \\frac{1}{2} x_t^2 + \\sigma_{\\varepsilon} \\, \\varepsilon_t$ 定义观测序列。所得到的 $\\{y_t\\}$ 构成您的估计过程应使用的数据；您在估计中不得使用 $\\{\\varepsilon_t\\}$ 或 $\\tau_{\\text{true}}$。\n\n测试套件：\n- 用例A（样本内无断点）：$\\theta_1 = 0.5$, $\\theta_2 = 1.5$, $\\tau_{\\text{true}} = 30$。\n- 用例B（早期断点）：$\\theta_1 = 0.5$, $\\theta_2 = 1.5$, $\\tau_{\\text{true}} = 8$。\n- 用例C（晚期断点）：$\\theta_1 = 0.5$, $\\theta_2 = 1.5$, $\\tau_{\\text{true}} = 22$。\n\n对于每个用例，请仅根据模型所隐含的似然和 $\\tau$ 上的均匀先验来计算断点时间的最大后验估计 $\\hat{\\tau}$。您的程序应生成单行输出，其中包含按用例 A、B、C 顺序排列的三个估计断点时间，形式为方括号括起来的逗号分隔列表，例如 $[\\hat{\\tau}_A,\\hat{\\tau}_B,\\hat{\\tau}_C]$。输出必须是整数。本问题不涉及物理单位，也不适用角度。解决方案不得读取任何外部输入；所有需要的值均在此处提供，或必须完全按照规定进行硬编码。", "solution": "此问题是有效的。这是一个关于带有结构性斷點的非线性状态空间模型参数估计的良构练习。我们着手进行解答。\n\n目标是找到未知结构性斷點时间 $\\tau$ 的最大后验 (MAP) 估计。参数 $\\tau$ 可以取集合 $\\{1, \\dots, T\\}$ 中的任何整数值，其中时间范围 $T=30$。MAP 估计记为 $\\hat{\\tau}$，是在给定观测数据 $y_{1:T} = \\{y_1, \\dots, y_T\\}$ 的条件下，使后验概率最大化的 $\\tau$ 值。根据贝叶斯定理，后验概率为：\n$$\nP(\\tau | y_{1:T}) = \\frac{P(y_{1:T} | \\tau) P(\\tau)}{P(y_{1:T})}\n$$\n我们希望找到 $\\hat{\\tau} = \\arg\\max_{\\tau \\in \\{1, \\dots, T\\}} P(\\tau | y_{1:T})$。分母 $P(y_{1:T})$ 是一个与 $\\tau$ 无关的归一化常数，可以在优化过程中忽略。问题指定了 $\\tau$ 的均匀先验分布，使得对所有 $\\tau \\in \\{1, \\dots, T\\}$ 都有 $P(\\tau) = 1/T$。由于这个先验分布对于 $\\tau$ 也是常数，它不影响最大值的位置。因此，MAP 估计问题简化为最大似然 (ML) 估计问题：\n$$\n\\hat{\\tau} = \\arg\\max_{\\tau \\in \\{1, \\dots, T\\}} P(y_{1:T} | \\tau)\n$$\n其中 $P(y_{1:T} | \\tau)$ 是观测数据的似然，以一个假设的斷點时间 $\\tau$ 为条件。处理对数似然 $\\mathcal{L}(\\tau) = \\log P(y_{1:T} | \\tau)$ 通常更为方便。使似然最大化的 $\\tau$ 值同样也会使对数似然最大化。\n$$\n\\hat{\\tau} = \\arg\\max_{\\tau \\in \\{1, \\dots, T\\}} \\mathcal{L}(\\tau)\n$$\n现在我们来定义似然函数。问题提供了一个关键的简化：状态创新项的标准差为零，即 $\\sigma_{\\eta} = 0$。这使得对于给定的一组参数，状态转移方程是确定性的。具体而言，对于任何一个假设的斷點时间 $k \\in \\{1, \\dots, T\\}$，整个潜变量状态路径 $\\{x_t(k)\\}_{t=1}^T$ 都由以下递归式唯一确定：\n$$\nx_t(k) = \\theta_t(k) + \\rho x_{t-1}(k)\n$$\n初始条件为 $x_0(k) = x_0 = 0$。参数 $\\theta_t(k)$ 定义为：当 $t \\le k$ 时，$\\theta_t(k) = \\theta_1$；当 $t > k$ 时，$\\theta_t(k) = \\theta_2$。因为对于给定的 $k$，潜变量路径是确定性的，所以无需通过滤波技术（例如，如果 $\\sigma_{\\eta} > 0$ 则必需的粒子滤波器）进行状态估计。\n\n观测方程为 $y_t = \\frac{1}{2} x_t^2 + \\sigma_{\\varepsilon} \\varepsilon_t$，其中 $\\varepsilon_t \\sim \\mathcal{N}(0,1)$。这意味着给定状态 $x_t$，每个观测值 $y_t$ 都服从条件正态分布：\n$$\ny_t | x_t \\sim \\mathcal{N}\\left(\\mu_t, \\sigma_{\\varepsilon}^2\\right) \\quad \\text{where} \\quad \\mu_t = \\frac{1}{2} x_t^2\n$$\n对于由假设 $\\tau=k$ 确定的状态 $x_t(k)$，单个观测值 $y_t$ 的概率密度函数为：\n$$\np(y_t | x_t(k)) = \\frac{1}{\\sqrt{2\\pi\\sigma_{\\varepsilon}^2}} \\exp\\left( -\\frac{\\left(y_t - \\frac{1}{2} x_t(k)^2\\right)^2}{2\\sigma_{\\varepsilon}^2} \\right)\n$$\n观测误差 $\\varepsilon_t$ 在时间上是独立的。因此，序列 $y_{1:T}$ 的总似然是各个概率密度的乘积：\n$$\nP(y_{1:T} | \\tau=k) = \\prod_{t=1}^T p(y_t | x_t(k))\n$$\n相应的对数似然为：\n$$\n\\mathcal{L}(k) = \\log P(y_{1:T} | \\tau=k) = \\sum_{t=1}^T \\log p(y_t | x_t(k)) = \\sum_{t=1}^T \\left[ -\\frac{1}{2}\\log(2\\pi\\sigma_{\\varepsilon}^2) - \\frac{\\left(y_t - \\frac{1}{2} x_t(k)^2\\right)^2}{2\\sigma_{\\varepsilon}^2} \\right]\n$$\n为了最大化关于 $k$ 的 $\\mathcal{L}(k)$，我们可以忽略常数项 $-\\frac{1}{2}\\log(2\\pi\\sigma_{\\varepsilon}^2)$ 和正的缩放因子 $1/(2\\sigma_{\\varepsilon}^2)$。因此，最大化对数似然等价于最小化观测数据与模型对 $y_t$ 均值的预测之间的残差平方和 (SSE)：\n$$\n\\hat{\\tau} = \\arg\\min_{k \\in \\{1, \\dots, T\\}} \\sum_{t=1}^T \\left(y_t - \\frac{1}{2} x_t(k)^2\\right)^2\n$$\n估计过程如下。对于每个测试用例：\n1. 首先，根据问题说明，使用提供的真实参数 $(\\theta_1, \\theta_2, \\tau_{\\text{true}})$ 和固定的噪声序列 $\\{\\varepsilon_t\\}_{t=1}^T$ 生成观测时间序列 $\\{y_t\\}_{t=1}^T$。\n2. 然后，为了估计 $\\hat{\\tau}$，遍历从 $1$ 到 $T=30$ 的每一个可能的候选斷點时间 $k$。\n3. 对于每个候选 $k$，使用已知的 $\\theta_1$, $\\theta_2$, $\\rho$ 和 $x_0$ 的值计算假设的确定性状态路径 $\\{x_t(k)\\}_{t=1}^T$。\n4. 计算残差平方和，$SSE(k) = \\sum_{t=1}^T (y_t - \\frac{1}{2}x_t(k)^2)^2$。\n5. 估计值 $\\hat{\\tau}$ 是产生最小 SSE 的 $k$ 值。如果有多个 $k$ 值产生相同的最小 SSE，则按惯例选择其中最小的 $k$ (例如，通过 `argmin`)。\n\n此过程针对每个测试用例实施，以找到相应的估计值 $\\hat{\\tau}$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the maximum a posteriori estimate of a structural break time\n    in a nonlinear state-space model for three test cases.\n    \"\"\"\n\n    # Global parameters and data generation settings from the problem statement\n    sigma_eps = 0.1\n    rho = 0.7\n    x0 = 0.0\n    T = 30\n    epsilon_t_draws = np.array([\n        0.03, -0.02, 0.01, 0.00, -0.01, 0.02, -0.03, 0.04, -0.02, 0.01,\n        0.00, -0.04, 0.05, -0.01, 0.02, -0.02, 0.03, -0.03, 0.01, 0.00,\n        0.02, -0.01, 0.04, -0.02, 0.01, -0.03, 0.02, 0.00, -0.01, 0.03\n    ])\n\n    test_cases = [\n        {'theta1': 0.5, 'theta2': 1.5, 'tau_true': 30},  # Case A\n        {'theta1': 0.5, 'theta2': 1.5, 'tau_true': 8},   # Case B\n        {'theta1': 0.5, 'theta2': 1.5, 'tau_true': 22}   # Case C\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        theta1 = case['theta1']\n        theta2 = case['theta2']\n        tau_true = case['tau_true']\n\n        #\n        # Step 1: Generate the observed time series y_t for the current case.\n        # This part simulates the data that the estimation procedure will use.\n        #\n        x_true = np.zeros(T + 1)\n        x_true[0] = x0\n        y_obs = np.zeros(T)\n        \n        for t in range(1, T + 1):\n            theta_t = theta1 if t <= tau_true else theta2\n            x_true[t] = theta_t + rho * x_true[t-1]\n            # y_obs is 0-indexed, so y_t corresponds to y_obs[t-1]\n            y_obs[t-1] = 0.5 * x_true[t]**2 + sigma_eps * epsilon_t_draws[t-1]\n            \n        #\n        # Step 2: Estimate the break time tau by maximizing the likelihood.\n        # This is equivalent to minimizing the sum of squared errors (SSE).\n        # The estimator only has access to y_obs and the model structure/parameters\n        # (theta1, theta2, rho, sigma_eps), not tau_true or epsilon_t_draws.\n        #\n        \n        sse_values = []\n        possible_taus = range(1, T + 1)\n        \n        for k in possible_taus:\n            # For each candidate break time k, compute the hypothetical state path\n            # and the corresponding SSE.\n            x_hypothetical = np.zeros(T + 1)\n            x_hypothetical[0] = x0\n            current_sse = 0.0\n            \n            for t in range(1, T + 1):\n                theta_t_hyp = theta1 if t <= k else theta2\n                x_hypothetical[t] = theta_t_hyp + rho * x_hypothetical[t-1]\n                \n                y_pred = 0.5 * x_hypothetical[t]**2\n                current_sse += (y_obs[t-1] - y_pred)**2\n                \n            sse_values.append(current_sse)\n            \n        # The MAP/ML estimate for tau is the one that minimizes the SSE.\n        # np.argmin returns the 0-based index of the minimum SSE.\n        # Since possible_taus starts from 1, the estimated tau is index + 1.\n        min_sse_index = np.argmin(sse_values)\n        tau_hat = possible_taus[min_sse_index]\n        \n        results.append(tau_hat)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "这个综合性练习将所有知识点融会贯通，要求你在实际条件下解决一个非线性宏观经济模型的参数估计问题。与前一个简化案例不同，这里的潜在状态 (非加速通货膨胀失业率，NAIRU) 是真正随机的，这意味着它的似然性无法直接计算。你将在此应用你的粒子滤波技能，为不同的参数值近似计算模型的对数似然，展示一种在复杂系统中进行推断的强大技术。成功完成此项实践 [@problem_id:2418262] 标志着你已熟练掌握应用粒子滤波器进行实际的计量经济学分析。", "id": "2418262", "problem": "考虑一个带有随时间变化的非加速通胀失业率 (NAIRU) 的非线性菲利普斯曲线。设不可观测状态为 NAIRU，记为 $n_t$，观测到的失业率为 $u_t$，观测到的通货膨胀率为 $\\pi_t$。该模型是一个非线性状态空间系统，定义如下：\n$$\n\\pi_t = \\alpha + \\beta \\left(u_t - n_t\\right) + \\gamma \\left(u_t - n_t\\right)^3 + \\varepsilon_t,\n$$\n$$\nn_t - \\mu = \\rho \\left(n_{t-1} - \\mu\\right) + \\eta_t,\n$$\n其中 $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma_\\varepsilon^2)$，$\\eta_t \\sim \\mathcal{N}(0,\\sigma_\\eta^2)$，且 $\\alpha = 0$。初始状态 $n_0$ 服从状态方程所蕴含的平稳分布，即：\n$$\nn_0 \\sim \\mathcal{N}\\!\\left(\\mu,\\;\\frac{\\sigma_\\eta^2}{1-\\rho^2}\\right).\n$$\n您的任务是为三种不同的参数配置（测试用例）模拟数据，然后，对每个测试用例，在指定的有限网格上计算参数向量 $\\theta = (\\beta,\\gamma,\\sigma_\\varepsilon,\\sigma_\\eta)$ 的最大似然估计，同时将 $\\alpha=0$、$\\mu$ 和 $\\rho$ 固定在每个测试用例指定的值。\n\n数据生成必须按以下方式进行。\n\n- 共同的观测失业过程 $u_t$：\n  - 时间长度 $T = 120$。\n  - 自回归过程 $u_t = \\mu_u + \\phi\\,(u_{t-1}-\\mu_u) + \\nu_t$，其中 $\\nu_t \\sim \\mathcal{N}(0,\\sigma_u^2)$。\n  - 参数：$\\mu_u = 5.5$，$\\phi = 0.8$，$\\sigma_u = 0.3$，以及初始值 $u_0 = \\mu_u$。\n  - 使用伪随机数种子 $100$ 来生成 $\\{u_t\\}_{t=1}^T$。\n- 对于每个测试用例 $i \\in \\{1,2,3\\}$，使用上述模型和特定于该测试用例的真实参数来模拟潜在状态 $\\{n_t\\}_{t=1}^T$ 和通货膨胀率 $\\{\\pi_t\\}_{t=1}^T$。对于每个测试用例 $i$，使用伪随机数种子 $200+i$ 来生成状态方程和测量方程中的新息。\n\n该测试套件由以下三个案例定义，每个案例都提供了用于数据模拟的固定参数 $(\\mu,\\rho)$ 和真实参数 $(\\beta^\\star,\\gamma^\\star,\\sigma_\\varepsilon^\\star,\\sigma_\\eta^\\star)$，以及您必须在其上计算并最大化似然函数的有限网格。\n\n- 案例 1 (一般情况):\n  - 固定参数: $\\mu = 5.5$, $\\rho = 0.95$。\n  - 真实模拟参数: $\\beta^\\star = -0.5$, $\\gamma^\\star = 0.06$, $\\sigma_\\varepsilon^\\star = 0.2$, $\\sigma_\\eta^\\star = 0.1$。\n  - 估计网格:\n    - $\\beta \\in \\{-0.6,\\,-0.5\\}$,\n    - $\\gamma \\in \\{0.04,\\,0.06\\}$,\n    - $\\sigma_\\varepsilon \\in \\{0.18,\\,0.22\\}$,\n    - $\\sigma_\\eta \\in \\{0.09,\\,0.11\\}$。\n- 案例 2 (近线性菲利普斯曲线，小状态噪声):\n  - 固定参数: $\\mu = 5.5$, $\\rho = 0.9$。\n  - 真实模拟参数: $\\beta^\\star = -0.4$, $\\gamma^\\star = 0.0$, $\\sigma_\\varepsilon^\\star = 0.15$, $\\sigma_\\eta^\\star = 0.02$。\n  - 估计网格:\n    - $\\beta \\in \\{-0.5,\\,-0.4\\}$,\n    - $\\gamma \\in \\{0.0,\\,0.02\\}$,\n    - $\\sigma_\\varepsilon \\in \\{0.12,\\,0.18\\}$,\n    - $\\sigma_\\eta \\in \\{0.01,\\,0.03\\}$。\n- 案例 3 (高度持续的 NAIRU 和更强的非线性):\n  - 固定参数: $\\mu = 5.5$, $\\rho = 0.98$。\n  - 真实模拟参数: $\\beta^\\star = -0.6$, $\\gamma^\\star = 0.2$, $\\sigma_\\varepsilon^\\star = 0.25$, $\\sigma_\\eta^\\star = 0.15$。\n  - 估计网格:\n    - $\\beta \\in \\{-0.7,\\,-0.6\\}$,\n    - $\\gamma \\in \\{0.18,\\,0.2\\}$,\n    - $\\sigma_\\varepsilon \\in \\{0.22,\\,0.28\\}$,\n    - $\\sigma_\\eta \\in \\{0.13,\\,0.17\\}$。\n\n对于每个测试用例，给定观测对 $\\{(u_t,\\pi_t)\\}_{t=1}^T$ 和固定的 $(\\mu,\\rho)$，计算指定网格中每个参数元组的对数似然值，并选择使该对数似然最大化的参数元组。您的程序必须对所有测试用例使用相同的观测失业序列 $\\{u_t\\}_{t=1}^T$。为了保证可复现性，您似然计算中使用的任何额外伪随机数都必须对测试用例 $i$ 使用伪随机数种子 $300+i$ 来生成，并且在同一测试用例的所有参数元组中必须被相同地重用。\n\n您的程序必须按测试用例 1、2、3 的顺序输出最大化参数元组的拼接列表，其中每个元组的顺序为 $(\\beta,\\gamma,\\sigma_\\varepsilon,\\sigma_\\eta)$。报告的每个数字都必须四舍五入到恰好六位小数。最终输出必须是单行文本，其中包含用方括号括起来的逗号分隔列表形式的结果，例如 $[\\beta_1,\\gamma_1,\\sigma_{\\varepsilon,1},\\sigma_{\\eta,1},\\beta_2,\\gamma_2,\\sigma_{\\varepsilon,2},\\sigma_{\\eta,2},\\beta_3,\\gamma_3,\\sigma_{\\varepsilon,3},\\sigma_{\\eta,3}]$。\n\n所有测试用例的答案必须是浮点数。不涉及物理单位；请勿包含任何单位符号。不涉及角度。所有类似分数的值都必须以小数形式提供。输出必须严格遵循上述单行格式。", "solution": "该问题陈述构成了计算计量经济学领域一个适定的练习，特别是在非线性状态空间模型的参数估计方面。它具有科学依据，内部逻辑一致，并为获得唯一、可验证的解提供了所有必要信息。因此，该问题被认为是有效的。\n\n核心任务是为一个非线性菲利普斯曲线模型找到参数向量 $\\theta = (\\beta, \\gamma, \\sigma_\\varepsilon, \\sigma_\\eta)$ 的最大似然估计，其搜索空间被限制在一个有限网格上。该模型由通货膨胀 $\\pi_t$ 的测量方程和不可观测的 NAIRU $n_t$ 的状态方程定义：\n$$\n\\pi_t = \\beta \\left(u_t - n_t\\right) + \\gamma \\left(u_t - n_t\\right)^3 + \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)\n$$\n$$\nn_t = \\mu + \\rho \\left(n_{t-1} - \\mu\\right) + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, \\sigma_\\eta^2)\n$$\n\n测量方程中由三次项 $(u_t - n_t)^3$ 引入的非线性使得似然函数在解析上难以处理。对于线性高斯状态空间模型，卡尔曼滤波器可以精确计算似然。然而，对于非线性或非高斯系统，则需要使用数值近似方法。解决此类问题的标准且有原则的方法是使用序贯蒙特卡罗 (SMC) 方法，通常称为粒子滤波器。\n\n所采用的方法是，对指定网格上的每个参数向量 $\\theta$，近似其对数似然函数 $\\log \\mathcal{L}(\\theta | Y_{1:T})$。该对数似然可以分解为一系列条件对数似然之和：\n$$\n\\log \\mathcal{L}(\\theta | Y_{1:T}) = \\sum_{t=1}^T \\log p(y_t | Y_{1:t-1}; \\theta)\n$$\n其中 $Y_{1:t} = \\{(\\pi_1, u_1), \\dots, (\\pi_t, u_t)\\}$ 代表截至时间 $t$ 的观测历史。粒子滤波器为每个预测密度项 $p(\\pi_t | Y_{1:t-1}; \\theta)$ 提供一个数值估计。\n\n我们将实现一种称为自举滤波器 (Bootstrap Filter) 的特定粒子滤波器变体。对于给定的参数向量 $\\theta$ 和一组 $N$ 个粒子，该算法流程如下：\n\n1.  **初始化 (t=0)**：从初始状态分布（即状态过程的平稳分布：$n_0 \\sim \\mathcal{N}(\\mu, \\sigma_\\eta^2/(1-\\rho^2))$）中抽取一组 $N$ 个粒子 $\\{n_0^{(j)}\\}_{j=1}^N$。总对数似然初始化为 $0$。\n\n2.  **序贯更新 (t = 1 到 T)**：\n    a.  **预测 (传播)**：根据状态转移方程将每个粒子向前传播。为每个粒子抽取一个随机新息：\n        $$\n        n_t^{(j)} = \\mu + \\rho(n_{t-1}^{(j)} - \\mu) + \\eta_t^{(j)}, \\quad \\text{where } \\eta_t^{(j)} \\sim \\mathcal{N}(0, \\sigma_\\eta^2)\n        $$\n    b.  **加权**：根据每个传播后的粒子 $n_t^{(j)}$ 对当前观测值 $\\pi_t$ 的解释程度，为其分配一个权重 $w_t^{(j)}$。该权重是在该观测值处计算的测量误差的概率密度函数 (PDF) 的值：\n        $$\n        \\hat{\\pi}_t^{(j)} = \\beta(u_t - n_t^{(j)}) + \\gamma(u_t - n_t^{(j)})^3\n        $$\n        $$\n        w_t^{(j)} = p(\\pi_t | n_t^{(j)}, u_t; \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma_\\varepsilon^2}} \\exp\\left(-\\frac{(\\pi_t - \\hat{\\pi}_t^{(j)})^2}{2\\sigma_\\varepsilon^2}\\right)\n        $$\n    c.  **似然近似**：条件似然 $p(\\pi_t | Y_{1:t-1}; \\theta)$ 通过未归一化权重的平均值来近似：\n        $$\n        \\hat{p}(\\pi_t | Y_{1:t-1}; \\theta) \\approx \\frac{1}{N} \\sum_{j=1}^N w_t^{(j)}\n        $$\n        总对数似然通过加上该平均权重的对数来更新。如果平均权重为零，则对数似然为负无穷，表明该参数向量与观测值不相容。\n    d.  **重采样**：为解决粒子退化问题（即单个粒子获得几乎所有权重），从当前粒子集 $\\{n_t^{(j)}\\}$ 中有放回地抽样生成一组新粒子，抽样概率由归一化权重 $W_t^{(j)} = w_t^{(j)} / \\sum_k w_t^{(k)}$ 给出。我们采用系统重采样 (systematic resampling)，这是一种高效且低方差的技术。重采样后的粒子随后用于时间 $t+1$ 的预测步骤。\n\n总体计算流程如下：\n首先，模拟共同的观测失业序列 $\\{u_t\\}_{t=1}^{T=120}$。然后，对三个测试用例中的每一个：\n1.  使用该测试用例的真实参数和指定的随机种子模拟通货膨胀序列 $\\{\\pi_t\\}_{t=1}^{T=120}$。\n2.  使用为估计步骤指定的种子，预先生成粒子滤波器所需的随机变量集（用于初始抽取、传播和重采样）。这确保了对于参数网格上的所有点，滤波器本身的随机性是相同的，从而保证了对它们似然值的公平比较。\n3.  使用粒子滤波器为测试用例网格中的每个参数元组计算近似对数似然。\n4.  产生最高对数似然值的参数元组被确定为该案例的最大似然估计。\n\n最后，将所有三个案例的估计参数进行拼接，并按照问题规范进行格式化。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nimport itertools\n\ndef solve():\n    \"\"\"\n    Main solver function to run simulations and estimations for all test cases.\n    \"\"\"\n    # Global parameters\n    T = 120\n    U_MU = 5.5\n    U_PHI = 0.8\n    U_SIGMA = 0.3\n    U_INITIAL = U_MU\n    U_SEED = 100\n    N_PARTICLES = 2000\n\n    # Test case definitions\n    test_cases = [\n        {\n            \"case_id\": 1,\n            \"fixed_params\": {\"mu\": 5.5, \"rho\": 0.95},\n            \"true_params\": {\"beta\": -0.5, \"gamma\": 0.06, \"sigma_e\": 0.2, \"sigma_n\": 0.1},\n            \"grid\": {\n                \"beta\": [-0.6, -0.5],\n                \"gamma\": [0.04, 0.06],\n                \"sigma_e\": [0.18, 0.22],\n                \"sigma_n\": [0.09, 0.11],\n            },\n            \"sim_seed\": 201,\n            \"est_seed\": 301,\n        },\n        {\n            \"case_id\": 2,\n            \"fixed_params\": {\"mu\": 5.5, \"rho\": 0.9},\n            \"true_params\": {\"beta\": -0.4, \"gamma\": 0.0, \"sigma_e\": 0.15, \"sigma_n\": 0.02},\n            \"grid\": {\n                \"beta\": [-0.5, -0.4],\n                \"gamma\": [0.0, 0.02],\n                \"sigma_e\": [0.12, 0.18],\n                \"sigma_n\": [0.01, 0.03],\n            },\n            \"sim_seed\": 202,\n            \"est_seed\": 302,\n        },\n        {\n            \"case_id\": 3,\n            \"fixed_params\": {\"mu\": 5.5, \"rho\": 0.98},\n            \"true_params\": {\"beta\": -0.6, \"gamma\": 0.2, \"sigma_e\": 0.25, \"sigma_n\": 0.15},\n            \"grid\": {\n                \"beta\": [-0.7, -0.6],\n                \"gamma\": [0.18, 0.2],\n                \"sigma_e\": [0.22, 0.28],\n                \"sigma_n\": [0.13, 0.17],\n            },\n            \"sim_seed\": 203,\n            \"est_seed\": 303,\n        }\n    ]\n\n    def generate_unemployment_data(T, mu_u, phi, sigma_u, u0, seed):\n        rng = np.random.default_rng(seed)\n        u = np.zeros(T)\n        u_prev = u0\n        nu = rng.normal(0, sigma_u, T)\n        for t in range(T):\n            u_t = mu_u + phi * (u_prev - mu_u) + nu[t]\n            u[t] = u_t\n            u_prev = u_t\n        return u\n\n    def simulate_model_data(T, u_series, true_params, fixed_params, seed):\n        rng = np.random.default_rng(seed)\n        mu, rho = fixed_params[\"mu\"], fixed_params[\"rho\"]\n        beta, gamma, sigma_e, sigma_n = true_params.values()\n        \n        n_series = np.zeros(T)\n        pi_series = np.zeros(T)\n        \n        # Initial state n_0 from stationary distribution\n        sigma_n0_sq = sigma_n**2 / (1 - rho**2)\n        n_prev = rng.normal(mu, np.sqrt(sigma_n0_sq))\n\n        innov_eta = rng.normal(0, sigma_n, T)\n        innov_eps = rng.normal(0, sigma_e, T)\n\n        for t in range(T):\n            # State equation\n            n_t = mu + rho * (n_prev - mu) + innov_eta[t]\n            n_series[t] = n_t\n            \n            # Measurement equation\n            u_gap = u_series[t] - n_t\n            pi_t = beta * u_gap + gamma * (u_gap**3) + innov_eps[t]\n            pi_series[t] = pi_t\n            \n            n_prev = n_t\n            \n        return pi_series\n\n    def particle_filter_log_likelihood(pi_series, u_series, params, fixed_params, N, rand_draws):\n        beta, gamma, sigma_e, sigma_n = params\n        mu, rho = fixed_params[\"mu\"], fixed_params[\"rho\"]\n        T = len(pi_series)\n        z_init, z_prop, u_resample = rand_draws\n\n        # Initial state distribution\n        if rho**2 >= 1 or sigma_n <= 0 or sigma_e <= 0:\n            return -np.inf\n        sigma_n0_sq = sigma_n**2 / (1 - rho**2)\n        sigma_n0 = np.sqrt(sigma_n0_sq)\n\n        # Initialization (t=0)\n        particles = mu + sigma_n0 * z_init\n        log_likelihood = 0.0\n\n        for t in range(T):\n            # Prediction/Propagation\n            particles = mu + rho * (particles - mu) + sigma_n * z_prop[t]\n\n            # Weighting\n            u_gap = u_series[t] - particles\n            pi_expected = beta * u_gap + gamma * (u_gap**3)\n            \n            # Using scipy.stats.norm.logpdf for numerical stability with small weights\n            log_weights = norm.logpdf(pi_series[t], loc=pi_expected, scale=sigma_e)\n            \n            # Log-Likelihood update using LogSumExp trick\n            if np.all(np.isneginf(log_weights)):\n                return -np.inf\n                \n            max_log_weight = np.max(log_weights)\n            weights_stable = np.exp(log_weights - max_log_weight)\n            mean_weight = np.mean(weights_stable)\n            \n            log_likelihood += max_log_weight + np.log(mean_weight)\n            \n            # Normalization\n            normalized_weights = weights_stable / np.sum(weights_stable)\n            \n            # Resampling (Systematic)\n            if np.sum(normalized_weights) > 0:\n                positions = (np.arange(N) + u_resample[t]) / N\n                cum_weights = np.cumsum(normalized_weights)\n                indices = np.searchsorted(cum_weights, positions)\n                particles = particles[indices]\n            else: # All weights were zero.\n                return -np.inf\n\n        return log_likelihood\n\n\n    # --- Main Execution ---\n    # Step 1: Generate common unemployment data\n    u_data = generate_unemployment_data(T, U_MU, U_PHI, U_SIGMA, U_INITIAL, U_SEED)\n\n    final_results = []\n    \n    for case in test_cases:\n        # Step 2: Simulate case-specific inflation data\n        pi_data = simulate_model_data(T, u_data, case[\"true_params\"], case[\"fixed_params\"], case[\"sim_seed\"])\n\n        # Step 3: Pre-generate random numbers for the particle filter\n        rng_est = np.random.default_rng(case[\"est_seed\"])\n        rand_draws_pf = (\n            rng_est.standard_normal(N_PARTICLES),  # For initialization\n            rng_est.standard_normal((T, N_PARTICLES)),  # For propagation\n            rng_est.random(T),  # For resampling\n        )\n\n        # Step 4: Grid search for MLE\n        max_log_lik = -np.inf\n        best_params = None\n\n        param_grid = list(itertools.product(\n            case[\"grid\"][\"beta\"],\n            case[\"grid\"][\"gamma\"],\n            case[\"grid\"][\"sigma_e\"],\n            case[\"grid\"][\"sigma_n\"]\n        ))\n        \n        for params_tuple in param_grid:\n            log_lik = particle_filter_log_likelihood(\n                pi_data, u_data, params_tuple, case[\"fixed_params\"], N_PARTICLES, rand_draws_pf\n            )\n            \n            if log_lik > max_log_lik:\n                max_log_lik = log_lik\n                best_params = params_tuple\n        \n        final_results.extend(best_params)\n\n    # Format and print the final output\n    print(f\"[{','.join([f'{val:.6f}' for val in final_results])}]\")\n\nsolve()\n```"}]}