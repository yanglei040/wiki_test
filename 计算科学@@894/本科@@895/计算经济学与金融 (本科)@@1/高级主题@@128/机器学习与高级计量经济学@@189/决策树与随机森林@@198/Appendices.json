{"hands_on_practices": [{"introduction": "决策树的核心在于其递归划分过程，而理解这一过程的关键是掌握单个“分裂”的机制。这个练习将揭示一个深刻的联系：在回归任务中，决策树寻找最优分裂点以最小化平方误差和 ($SSE$) 的过程，与金融时间序列分析中经典的“结构突变点”检测问题在数学上是等价的。通过解决这个问题，你不仅能加深对决策树构建模块的理解，还能体会到机器学习与传统统计学方法之间的共通之处。[@problem_id:2386904]", "id": "2386904", "problem": "给定一个十进制对数回报率的单变量金融时间序列 $\\{y_t\\}_{t=0}^{T-1}$，其中 $T \\in \\mathbb{N}$ 且每个 $y_t \\in \\mathbb{R}$。考虑一个分段常数双区制模型，其在整数分割位置 $\\tau$ 处存在一个结构性断点，其中左侧分段的索引为 $\\{0,1,\\dots,\\tau-1\\}$，右侧分段的索引为 $\\{\\tau,\\dots,T-1\\}$。对于任意容许分割 $\\tau$，将左侧分段均值 $\\mu_L(\\tau)$ 和右侧分段均值 $\\mu_R(\\tau)$ 定义为\n$$\n\\mu_L(\\tau) \\equiv \\frac{1}{\\tau}\\sum_{t=0}^{\\tau-1} y_t,\\qquad\n\\mu_R(\\tau) \\equiv \\frac{1}{T-\\tau}\\sum_{t=\\tau}^{T-1} y_t,\n$$\n以及总平方误差\n$$\n\\mathrm{SSE}(\\tau) \\equiv \\sum_{t=0}^{\\tau-1}\\bigl(y_t - \\mu_L(\\tau)\\bigr)^2 \\;+\\; \\sum_{t=\\tau}^{T-1}\\bigl(y_t - \\mu_R(\\tau)\\bigr)^2.\n$$\n给定一个最小叶子大小 $m \\in \\mathbb{N}$，如果分割 $\\tau$ 满足 $m \\le \\tau \\le T - m$，则该分割是容许的。您的任务是，对每个测试用例，计算一个整数 $\\tau^\\star$，该值能在所有容许分割上最小化 $\\mathrm{SSE}(\\tau)$。如果存在多个最小值点（包括数值上的相等情况），选择其中最小的 $\\tau^\\star$。如果没有容许分割存在（即 $T < 2m$），则该测试用例的输出定义为 $-1$。\n\n所有 $y_t$ 值均为十进制对数回报率，必须作为无量纲实数处理（无单位）。不要将任何结果表示为百分比；所有数值量必须是小数或整数。\n\n测试套件。对每个测试用例，给定一对 $(\\{y_t\\}_{t=0}^{T-1}, m)$：\n\n- 测试 1 (均值明显偏移): $T = 10$, $y = [\\,\\$0.000\\$, \\$0.000\\$, \\$0.000\\$, \\$0.000\\$, \\$0.000\\$, \\$0.020\\$, \\$0.020\\$, \\$0.020\\$, \\$0.020\\$, \\$0.020\\$\\,]$, $m = \\$1\\,$.\n- 测试 2 (常数序列，通过最小分割点进行决胜): $T = 6$, $y = [\\,\\$0.010\\$, \\$0.010\\$, \\$0.010\\$, \\$0.010\\$, \\$0.010\\$, \\$0.010\\$\\,]$, $m = \\$2\\,$.\n- 测试 3 (接近末尾的单个极端观测值，带有最小叶子大小约束): $T = 8$, $y = [\\,\\$0.000\\$, \\$0.000\\$, \\$0.000\\$, \\$0.000\\$, \\$0.000\\$, \\$0.000\\$, \\$0.000\\$, \\$0.100\\$\\,]$, $m = \\$2\\,$.\n- 测试 4 (边界情况，恰好只有一个容许分割): $T = 4$, $y = [\\,\\$0.010\\$, \\$0.020\\$, \\$0.030\\$, \\$0.040\\$\\,]$, $m = \\$2\\,$.\n- 测试 5 (漂移后接一个更高的恒定均值): $T = 10$, $y = [\\,\\$0.000\\$, \\$0.010\\$, \\$0.020\\$, \\$0.030\\$, \\$0.040\\$, \\$0.060\\$, \\$0.060\\$, \\$0.060\\$, \\$0.060\\$, \\$0.060\\$\\,]$, $m = \\$1\\,$.\n\n要求的最终输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表，顺序与上述测试用例一致。例如，如果五个答案是 $\\tau_1^\\star,\\dots,\\tau_5^\\star$，请打印\n[${\\tau_1^\\star}$,${\\tau_2^\\star}$,${\\tau_3^\\star}$,${\\tau_4^\\star}$,${\\tau_5^\\star}$]。", "solution": "在尝试求解之前，需对提出的问题进行验证。\n\n### 第 1 步：提取已知条件\n- **时间序列**：一个单变量序列 $\\{y_t\\}_{t=0}^{T-1}$，其中 $T \\in \\mathbb{N}$ 且 $y_t \\in \\mathbb{R}$。\n- **模型**：一个分段常数双区制模型，在整数分割位置 $\\tau$ 处存在一个结构性断点。\n- **分段**：\n    - 左侧分段索引：$\\{0, 1, \\dots, \\tau-1\\}$。\n    - 右侧分段索引：$\\{\\tau, \\dots, T-1\\}$。\n- **分段均值**：\n    - 左侧分段均值：$\\mu_L(\\tau) \\equiv \\frac{1}{\\tau}\\sum_{t=0}^{\\tau-1} y_t$。\n    - 右侧分段均值：$\\mu_R(\\tau) \\equiv \\frac{1}{T-\\tau}\\sum_{t=\\tau}^{T-1} y_t$。\n- **目标函数**：总平方误差，$\\mathrm{SSE}(\\tau) \\equiv \\sum_{t=0}^{\\tau-1}\\bigl(y_t - \\mu_L(\\tau)\\bigr)^2 \\;+\\; \\sum_{t=\\tau}^{T-1}\\bigl(y_t - \\mu_R(\\tau)\\bigr)^2$。\n- **约束条件**：\n    - 最小叶子大小：$m \\in \\mathbb{N}$。\n    - 容许分割：如果分割 $\\tau$ 满足 $m \\le \\tau \\le T - m$，则该分割是容许的。\n- **任务**：\n    - 计算一个整数 $\\tau^\\star$，该值能在所有容许分割上最小化 $\\mathrm{SSE}(\\tau)$。\n    - 决胜规则：如果存在多个最小值点，选择其中最小的 $\\tau^\\star$。\n    - 无容许分割：如果 $T < 2m$，则输出为 -1。\n- **测试套件**：提供了五个测试用例，每个测试用例包含一对 $(\\{y_t\\}_{t=0}^{T-1}, m)$。数据中的美元符号被视为装饰性的，其值被当作十进制数处理。\n\n### 第 2 步：使用提取的已知条件进行验证\n评估问题的有效性。\n- **科学依据**：该问题是变点检测的经典案例，这是统计学、信号处理和计量经济学中的一个基本课题。对于一系列随机变量，如果假设它们在其他方面是独立的、服从具有恒定方差的高斯分布，那么最小化总平方误差（SSE）等价于对均值断点进行最大似然估计。这是一个成熟且有科学依据的模型。\n- **适定性**：该问题是适定的。$\\tau$ 的优化域是整数的有限集 $\\{m, m+1, \\dots, T-m\\}$。对于该域中的每个 $\\tau$，目标函数 $\\mathrm{SSE}(\\tau)$ 都有唯一定义。由于我们是在一个有限集上最小化一个实值函数，因此最小值必然存在。决胜规则确保了选择唯一的解。对于不存在容许分割的情况（$T<2m$），其条件也已明确定义。\n- **目标**：问题陈述具有精确的数学定义，没有歧义。目标清晰，解的标准也已明确给出。\n\n### 第 3 步：结论与行动\n该问题有效。它是一个定义明确的数学优化问题，以标准的统计理论为基础。将提供解决方案。\n\n### 求解推导\n目标是找到 $\\tau^\\star = \\arg\\min_{m \\le \\tau \\le T-m} \\mathrm{SSE}(\\tau)$。\n\n一种朴素的方法是遍历每个容许分割 $\\tau$，并对每个 $\\tau$ 通过对数据点求和来计算分段均值和 SSE。对于给定的 $\\tau$，计算 $\\mu_L(\\tau)$ 和 $\\mu_R(\\tau)$ 分别需要 $O(\\tau)$ 和 $O(T-\\tau)$ 次操作。类似地，计算两个平方和也需要 $O(\\tau)$ 和 $O(T-\\tau)$ 次操作。因此，单个 $\\tau$ 的总复杂度为 $O(T)$。由于 $\\tau$ 有 $O(T)$ 个可能的值，这种朴素方法的总复杂度为 $O(T^2)$。这对于较大的 $T$ 来说效率低下。\n\n可以通过认识到 SSE 可以使用一个著名的方差恒等式来计算，从而构建一个更高效的算法：对于一组数 $\\{x_i\\}_{i=1}^n$，其与均值的平方偏差之和为 $\\sum_{i=1}^n (x_i - \\bar{x})^2 = \\sum_{i=1}^n x_i^2 - n\\bar{x}^2 = \\sum_{i=1}^n x_i^2 - \\frac{1}{n}(\\sum_{i=1}^n x_i)^2$。\n\n将此应用于我们的两个分段：\n左侧分段的 SSE 为：\n$$ \\mathrm{SSE}_L(\\tau) = \\sum_{t=0}^{\\tau-1} (y_t - \\mu_L(\\tau))^2 = \\left(\\sum_{t=0}^{\\tau-1} y_t^2\\right) - \\frac{1}{\\tau}\\left(\\sum_{t=0}^{\\tau-1} y_t\\right)^2 $$\n右侧分段的 SSE 为：\n$$ \\mathrm{SSE}_R(\\tau) = \\sum_{t=\\tau}^{T-1} (y_t - \\mu_R(\\tau))^2 = \\left(\\sum_{t=\\tau}^{T-1} y_t^2\\right) - \\frac{1}{T-\\tau}\\left(\\sum_{t=\\tau}^{T-1} y_t\\right)^2 $$\n总 SSE 为 $\\mathrm{SSE}(\\tau) = \\mathrm{SSE}_L(\\tau) + \\mathrm{SSE}_R(\\tau)$。\n\n为了对所有$\\tau$高效地计算这些和，我们可以预先计算序列 $y_t$ 及其平方 $y_t^2$ 的前缀和（累积和）。让我们定义两个大小为 $T+1$ 的辅助数组 $C_1$ 和 $C_2$：\n$$ C_1[k] = \\sum_{t=0}^{k-1} y_t \\quad \\text{for } k > 0, \\text{ with } C_1[0] = 0 $$\n$$ C_2[k] = \\sum_{t=0}^{k-1} y_t^2 \\quad \\text{for } k > 0, \\text{ with } C_2[0] = 0 $$\n这些前缀和数组可以在 $O(T)$ 时间内计算出来。\n\n使用这些数组，任何分段上的求和都可以在 $O(1)$ 时间内计算出来。对于在 $\\tau$ 处的分割：\n- 在左侧分段 $[0, \\dots, \\tau-1]$ 上的和：\n  - $\\sum_{t=0}^{\\tau-1} y_t = C_1[\\tau]$\n  - $\\sum_{t=0}^{\\tau-1} y_t^2 = C_2[\\tau]$\n- 在右侧分段 $[\\tau, \\dots, T-1]$ 上的和：\n  - $\\sum_{t=\\tau}^{T-1} y_t = \\sum_{t=0}^{T-1} y_t - \\sum_{t=0}^{\\tau-1} y_t = C_1[T] - C_1[\\tau]$\n  - $\\sum_{t=\\tau}^{T-1} y_t^2 = \\sum_{t=0}^{T-1} y_t^2 - \\sum_{t=0}^{\\tau-1} y_t^2 = C_2[T] - C_2[\\tau]$\n\n将这些代入 SSE 公式：\n$$ \\mathrm{SSE}_L(\\tau) = C_2[\\tau] - \\frac{(C_1[\\tau])^2}{\\tau} $$\n$$ \\mathrm{SSE}_R(\\tau) = (C_2[T] - C_2[\\tau]) - \\frac{(C_1[T] - C_1[\\tau])^2}{T-\\tau} $$\n因此，在初始的 $O(T)$ 预计算之后，给定 $\\tau$ 的总 SSE 可以在 $O(1)$ 时间内计算出来。\n\n总体算法如下：\n1.  首先，检查是否存在任何容许分割。如果 $T < 2m$，则没有分割 $\\tau$ 能同时满足 $\\tau \\ge m$ 和 $T-\\tau \\ge m$。在这种情况下，程序终止并返回 -1。\n2.  为给定的时间序列 $\\{y_t\\}$ 和 $\\{y_t^2\\}$ 计算前缀和数组 $C_1$ 和 $C_2$。这需要 $O(T)$ 的时间。\n3.  初始化一个变量用于存储已找到的最小 SSE，$\\mathrm{SSE}_{\\min}$，设为一个非常大的值（或无穷大）；以及最佳分割点 $\\tau^\\star$，设为一个默认值（例如 -1 或第一个容许分割点）。\n4.  遍历所有容许分割 $\\tau$，从 $m$ 到 $T-m$。\n5.  对于每个 $\\tau$，使用预计算的前缀和与推导出的公式，在 $O(1)$ 时间内计算 $\\mathrm{SSE}(\\tau)$。\n6.  如果计算出的 $\\mathrm{SSE}(\\tau)$ 严格小于当前的 $\\mathrm{SSE}_{\\min}$，则更新 $\\mathrm{SSE}_{\\min} = \\mathrm{SSE}(\\tau)$ 和 $\\tau^\\star = \\tau$。使用严格不等式 (`<`) 确保在 SSE 相等的情况下，保留遇到的第一个（因此是最小的） $\\tau$ 值，这满足了问题的决胜规则。\n7.  循环完成后，$\\tau^\\star$ 将持有最优分割点。返回 $\\tau^\\star$。\n\n该算法的总时间复杂度为 $O(T)$，主要由前缀和计算主导，这比朴素的 $O(T^2)$ 方法有了显著的改进。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the optimal structural break point in a time series.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test 1: clear mean shift\n        (np.array([0.000, 0.000, 0.000, 0.000, 0.000, 0.020, 0.020, 0.020, 0.020, 0.020]), 1),\n        # Test 2: constant series, tie-breaking by smallest split\n        (np.array([0.010, 0.010, 0.010, 0.010, 0.010, 0.010]), 2),\n        # Test 3: single extreme observation near the end with minimum leaf size\n        (np.array([0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.100]), 2),\n        # Test 4: boundary case with exactly one admissible split\n        (np.array([0.010, 0.020, 0.030, 0.040]), 2),\n        # Test 5: drift followed by a higher constant mean\n        (np.array([0.000, 0.010, 0.020, 0.030, 0.040, 0.060, 0.060, 0.060, 0.060, 0.060]), 1),\n    ]\n\n    results = []\n    \n    for y, m in test_cases:\n        T = len(y)\n        \n        # Step 1: Check for existence of an admissible split.\n        if T < 2 * m:\n            results.append(-1)\n            continue\n            \n        # Step 2: Compute prefix sums for y and y^2.\n        # We prepend a 0 to make indexing C[k] = sum_{i=0}^{k-1} y_i natural.\n        y_sq = y**2\n        \n        # C1[k] = sum_{t=0}^{k-1} y_t\n        C1 = np.zeros(T + 1)\n        np.cumsum(y, out=C1[1:])\n\n        # C2[k] = sum_{t=0}^{k-1} y_t^2\n        C2 = np.zeros(T + 1)\n        np.cumsum(y_sq, out=C2[1:])\n        \n        min_sse = np.inf\n        best_tau = -1\n        \n        # Step 3: Iterate through all admissible splits.\n        # The range is m <= tau <= T - m.\n        for tau in range(m, T - m + 1):\n            # Calculate sums for the left segment [0, ..., tau-1]\n            # The length of the left segment is tau.\n            sum_L = C1[tau]\n            sum_sq_L = C2[tau]\n            \n            # Calculate sums for the right segment [tau, ..., T-1]\n            # The length of the right segment is T - tau.\n            sum_R = C1[T] - C1[tau]\n            sum_sq_R = C2[T] - C2[tau]\n            \n            # Calculate SSE for left and right segments\n            # SSE = sum(y_i^2) - (sum(y_i))^2 / n\n            sse_L = sum_sq_L - (sum_L**2) / tau\n            sse_R = sum_sq_R - (sum_R**2) / (T - tau)\n            \n            current_sse = sse_L + sse_R\n            \n            # Step 4: Update minimum SSE and best tau.\n            # Strict inequality handles the tie-breaking rule (smallest tau).\n            if current_sse < min_sse:\n                min_sse = current_sse\n                best_tau = tau\n        \n        results.append(best_tau)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "随机森林等集成模型虽然强大，但其输出（如特征重要性）的可靠性完全取决于输入数据的质量。“目标泄漏”（Target Leakage）是金融建模中最隐蔽也最危险的陷阱之一，它指的是特征中包含了未来才能知道的信息。这个动手实践将引导你构建一个包含微妙目标泄漏的合成数据集，并观察一个随机森林模型如何错误地将这个泄漏变量识别为最重要的预测因子，从而让你深刻认识到数据审查和模型解释中保持警惕的重要性。[@problem_id:2386893]", "id": "2386893", "problem": "您正在对一个信贷投资组合中的二元贷款违约结果进行建模。对于每个观测值 $i \\in \\{1,\\dots,n\\}$，设其二元目标为 $y_i \\in \\{0,1\\}$，其中 $y_i = 1$ 表示违约。您将生成具有经济可解释性的合成协变量，然后添加一个会泄露关于 $y_i$ 信息的微观事后协变量。然后，您必须量化决策桩（一种单次分裂的决策森林）集成模型如何按重要性对协变量进行排序，并为每个测试用例报告最重要协变量的零基索引。索引必须以整数形式报告。\n\n数据生成过程：\n- 设基础协变量的数量为 $p_b = 5$。对于每个观测值 $i$，从一个均值为零、协方差矩阵为 $\\Sigma(\\rho) \\in \\mathbb{R}^{p_b \\times p_b}$ 的多元正态分布中抽取一个基础特征向量 $\\mathbf{x}_i \\in \\mathbb{R}^{p_b}$，该协方差矩阵定义为\n$$\n\\Sigma(\\rho) = (1-\\rho) I_{p_b} + \\rho \\mathbf{1}\\mathbf{1}^\\top,\n$$\n其中 $I_{p_b}$是大小为 $p_b$ 的单位矩阵，$\\mathbf{1}$ 是一个 $p_b$ 维的全一向量。标量 $\\rho \\in (-\\frac{1}{p_b-1},1)$ 控制基础特征间的共同相关性。\n- 独立于 $\\mathbf{x}_i$ 抽取一个特异性宏观因子 $m_i \\sim \\mathcal{N}(0,1)$。\n- 通过一个逻辑指数定义一个潜得分 $s_i$：\n$$\ns_i = \\beta_0 + \\sum_{j=1}^{p_b} \\beta_j x_{i,j} + \\gamma m_i,\n$$\n系数固定为 $\\beta_0 = -0.5$，$\\beta_1 = 0.8$，$\\beta_2 = -1.0$，$\\beta_3 = 0.6$，$\\beta_4 = 0.0$，$\\beta_5 = 0.5$ 以及 $\\gamma = 0.7$。\n- 通过逻辑函数定义违约概率 $p_i$：\n$$\np_i = \\frac{1}{1 + e^{-s_i}}.\n$$\n- 独立地对每个 $i$ 从伯努利分布 $y_i \\sim \\text{Bernoulli}(p_i)$ 中抽取二元结果。\n- 定义一个泄露目标信息的事后协变量 $z_i$为\n$$\nz_i = \\lambda \\, y_i + \\delta_i,\n$$\n其中 $\\delta_i \\sim \\mathcal{N}(0,\\sigma^2)$ 独立于其他所有变量。参数 $\\lambda \\in \\mathbb{R}$ 控制泄露的强度，$\\sigma \\ge 0$ 控制掩盖泄露的噪声量。\n- 在建模时，您将按如下方式形成特征向量 $\\tilde{\\mathbf{x}}_i$。如果测试用例标志 $\\text{include\\_leak} = 1$，则设置\n$$\n\\tilde{\\mathbf{x}}_i = \\big[x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}, z_i\\big] \\in \\mathbb{R}^{p},\n$$\n其中 $p = p_b + 1$，并且泄露协变量位于零基索引 $p_b$ 处。如果 $\\text{include\\_leak} = 0$，则设置\n$$\n\\tilde{\\mathbf{x}}_i = \\big[x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}\\big] \\in \\mathbb{R}^{p},\n$$\n其中 $p = p_b$。\n\n模型与重要性：\n- 考虑一个由 $T=200$ 个决策桩（单次分裂决策树）组成的集成模型。对于每棵树 $t \\in \\{1,\\dots,T\\}$：\n  - 通过从 $\\{1,\\dots,n\\}$ 中有放回地抽样索引，抽取一个大小为 $n$ 的自助样本（bootstrap sample）。\n  - 令 $m_{\\text{try}} = \\lceil \\sqrt{p} \\rceil$ 并从 $\\{0,\\dots,p-1\\}$ 中随机均匀地选择 $m_{\\text{try}}$ 个不同的特征作为分裂候选。\n  - 对于每个选定的特征 $j$，考虑形式为 $x_{j} \\le \\tau$ 的分裂，其中阈值 $\\tau$ 是该特征在自助样本上排序后的观测值中连续值的中点，并排除会导致空子节点的阈值。令 $G(S)$ 表示二元标签集合 $S$ 的基尼不纯度（Gini impurity）：\n  $$\n  G(S) = 1 - \\sum_{c \\in \\{0,1\\}} \\left(\\frac{n_c}{|S|}\\right)^2,\n  $$\n  其中 $n_c$ 是集合 $S$ 中类别 $c$ 的计数。对于一个标签多重集为 $S$ 的父节点，分裂为左子节点 $S_L$ 和右子节点 $S_R$ 时，不纯度减少量定义为\n  $$\n  \\Delta G = G(S) - \\left(\\frac{|S_L|}{|S|}G(S_L) + \\frac{|S_R|}{|S|}G(S_R)\\right).\n  $$\n  - 在所有考虑的特征和阈值中，选择能够最大化 $\\Delta G$ 的特征 $j^\\star$ 和阈值 $\\tau^\\star$。通过在 $(j^\\star,\\tau^\\star)$ 处进行分裂来生成一个决策桩。\n  - 将所选分裂实现的不纯度减少量 $\\Delta G^\\star$ 归因于特征 $j^\\star$。\n- 将特征 $j$ 的重要性定义为在 $T$ 棵树上归因于它的不纯度减少量之和：\n$$\nI_j = \\sum_{t=1}^{T} \\Delta G_t \\cdot \\mathbf{1}\\{j_t^\\star = j\\}.\n$$\n- 对于每个测试用例，计算使 $I_j$ 最大化的索引 $j_{\\max} \\in \\{0,\\dots,p-1\\}$。如果出现平局，则取达到最大值的最小索引。\n\n测试套件：\n为保证可复现性，每个测试用例的数据生成和集成模型构建过程均使用一个独立的随机种子 $s$。使用以下四个测试用例，每个用例由元组 $(n,\\sigma,\\lambda,\\rho,\\text{include\\_leak}, s)$ 指定：\n- 用例 A: $(3000, 0.1, 0.9, 0.2, 1, 11)$。\n- 用例 B: $(3000, 0.3, 0.6, 0.2, 1, 12)$。\n- 用例 C: $(1200, 0.1, 0.9, 0.2, 0, 13)$。\n- 用例 D: $(3000, 0.6, 0.4, 0.2, 1, 14)$。\n\n程序行为和输出要求：\n- 对于每个测试用例，根据上述过程生成数据，训练上述集成模型，计算特征重要性 $\\{I_j\\}_{j=0}^{p-1}$，并返回最重要特征的零基索引 $j_{\\max}$。\n- 您的程序必须生成单行输出，其中包含四个索引，格式为逗号分隔的列表并用方括号括起来，顺序与测试用例相同，例如 [$i_1$,$i_2$,$i_3$,$i_4$]。不应打印任何额外文本。", "solution": "所提出的问题是计算统计学和机器学习领域一个有效且定义明确的练习，具体涉及基于树的集成模型中特征重要性的评估。数据生成过程定义严谨，其科学基础是金融计量经济学的标准模型。任务是使用决策桩集成模型中的基尼不纯度减少量来量化特征重要性，并识别出最具影响力的特征，尤其是在存在“泄露”协变量的情况下。该问题是客观、自包含且算法明确的，因此可以得到唯一、可复现的解。\n\n对于每个测试用例，求解方法分为两个主要阶段：数据生成和包含重要性计算的模型训练。所有数学实体，包括变量、参数和数值，都按要求用 LaTeX 表示。\n\n**1. 数据生成过程**\n\n对于每个指定的测试用例，根据以下随机过程生成一个大小为 $n$ 的合成数据集。使用随机种子 $s$ 来确保可复现性。\n\n- **基础协变量**：对于每个观测值 $i \\in \\{1, \\dots, n\\}$，从多元正态分布 $\\mathcal{N}(\\mathbf{0}, \\Sigma(\\rho))$ 中抽取一组 $p_b = 5$ 个基础协变量，记为向量 $\\mathbf{x}_i \\in \\mathbb{R}^{p_b}$。协方差矩阵 $\\Sigma(\\rho)$ 定义为\n$$\n\\Sigma(\\rho) = (1-\\rho) I_{p_b} + \\rho \\mathbf{1}\\mathbf{1}^\\top\n$$\n其中 $I_{p_b}$ 是 $p_b \\times p_b$ 的单位矩阵，$\\mathbf{1}$ 是一个 $p_b$ 维的全一向量。参数 $\\rho$ 控制这些基础特征间的等相关性。\n\n- **潜得分与违约概率**：独立抽取一个特异性因子 $m_i \\sim \\mathcal{N}(0,1)$。潜得分 $s_i$ 构建为基础协变量和宏观因子的线性组合：\n$$\ns_i = \\beta_0 + \\sum_{j=1}^{p_b} \\beta_j x_{i,j} + \\gamma m_i\n$$\n系数固定为 $\\beta_0 = -0.5$，$\\boldsymbol{\\beta}_{1..p_b} = [0.8, -1.0, 0.6, 0.0, 0.5]$，以及 $\\gamma = 0.7$。请注意，$x_{i,4}$ 的系数为 $\\beta_4 = 0.0$，这使得该特征在构造上相对于潜得分是不提供信息的。然后使用标准逻辑函数将潜得分转换为违约概率 $p_i$：\n$$\np_i = \\frac{1}{1 + e^{-s_i}}\n$$\n\n- **二元结果**：二元目标变量 $y_i \\in \\{0, 1\\}$ 表示未违约（$0$）或违约（$1$），它是从以生成概率为参数的伯努利分布中抽取的，即 $y_i \\sim \\text{Bernoulli}(p_i)$。\n\n- **泄露协变量**：生成一个事后协变量 $z_i$ 以模拟来自目标变量的信息泄露。其定义为：\n$$\nz_i = \\lambda \\, y_i + \\delta_i\n$$\n其中 $\\delta_i$ 是从正态分布 $\\mathcal{N}(0, \\sigma^2)$ 中抽取的噪声项。参数 $\\lambda$ 控制泄露的强度，$\\sigma$ 控制噪声水平。$|\\lambda|$ 与 $\\sigma$ 的高比率意味着 $z_i$ 和 $y_i$ 之间存在强且易于检测的联系。\n\n- **最终特征矩阵**：组装完整的特征矩阵 $\\tilde{\\mathbf{X}}$。如果 `include_leak` 标志为 1，则特征集为 $\\tilde{\\mathbf{x}}_i = [x_{i,1}, \\dots, x_{i,5}, z_i] \\in \\mathbb{R}^{6}$。泄露特征 $z_i$ 位于最后一个位置（零基索引为 $5$）。如果标志为 0，则仅使用基础协变量，$\\tilde{\\mathbf{x}}_i = [x_{i,1}, \\dots, x_{i,5}] \\in \\mathbb{R}^{5}$。特征数量由 $p$ 表示。\n\n**2. 特征重要性量化**\n\n每个特征的重要性通过训练一个由 $T = 200$ 个决策桩组成的集成模型来确定。决策桩是仅有一次分裂的决策树。\n\n- **集成构建**：对于集成模型中的 $T$ 个决策桩中的每一个：\n    1. 通过从完整数据集 $(\\tilde{\\mathbf{X}}, \\mathbf{y})$ 中有放回地抽样，创建一个大小为 $n$ 的自助样本。\n    2. 随机选择一个由 $m_{\\text{try}} = \\lceil \\sqrt{p} \\rceil$ 个不同特征组成的子集。\n    3. 对于每个选定的特征，找到最优分裂。一次分裂由一个特征 $j$ 和一个阈值 $\\tau$ 定义。分裂的质量由基尼不纯度减少量 $\\Delta G$ 来衡量。一组标签 $S$ 的基尼不纯度由下式给出：\n    $$\n    G(S) = 1 - \\sum_{c \\in \\{0,1\\}} \\left(\\frac{n_c}{|S|}\\right)^2\n    $$\n    其中 $n_c$ 是集合 $S$ 中类别 $c$ 的计数。不纯度减少量是父节点的不纯度与两个子节点不纯度的加权平均值之间的差。\n    4. 选择产生最大不纯度减少量 $\\Delta G^\\star$ 的特征 $j^\\star$ 和阈值 $\\tau^\\star$ 用于决策桩的分裂。潜在的阈值是自助样本中特征的连续唯一排序值的中点。\n\n- **重要性聚合**：特征 $j$ 的重要性，记为 $I_j$，计算为它在集成模型中所有树上所贡献的不纯度减少量之和：\n$$\nI_j = \\sum_{t=1}^{T} \\Delta G_t \\cdot \\mathbf{1}\\{j_t^\\star = j\\}\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n\n- **结果**：最后，对于每个测试用例，识别出具有最高重要性分数的特征的零基索引，$j_{\\max} = \\arg\\max_j I_j$。平局通过选择最小的索引来解决。该索引是此用例的输出。该过程使用 Python 实现，并遵循指定的库和随机种子以确保结果可验证。", "answer": "```python\nimport numpy as np\nfrom math import ceil, sqrt\n\ndef _gini_impurity(y):\n    \"\"\"Calculates the Gini impurity of a set of binary labels.\"\"\"\n    n_samples = len(y)\n    if n_samples == 0:\n        return 0.0\n    n1_count = np.sum(y)\n    p1 = n1_count / n_samples\n    p0 = 1.0 - p1\n    return 1.0 - (p0**2 + p1**2)\n\ndef _find_best_split(X_boot, y_boot, feature_indices):\n    \"\"\"Finds the best split for a single decision stump.\"\"\"\n    n_samples = len(y_boot)\n    if n_samples <= 1:\n        return -1, -1.0\n\n    best_gain = -1.0\n    best_feature = -1\n\n    gini_parent = _gini_impurity(y_boot)\n    n1_total = np.sum(y_boot)\n    \n    for j in feature_indices:\n        feature_values = X_boot[:, j]\n        \n        unique_vals = np.unique(feature_values)\n        if len(unique_vals) < 2:\n            continue\n            \n        # Efficiently find the best split for feature j\n        sorted_indices = np.argsort(feature_values)\n        y_sorted = y_boot[sorted_indices]\n        x_sorted = feature_values[sorted_indices]\n        \n        y_cumsum = np.cumsum(y_sorted)\n\n        split_points = np.where(x_sorted[:-1] != x_sorted[1:])[0]\n\n        for i in split_points:\n            n_left = i + 1\n            n_right = n_samples - n_left\n\n            n1_left = y_cumsum[i]\n            n1_right = n1_total - n1_left\n\n            gini_left = 1.0 - ((n1_left / n_left)**2 + ((n_left - n1_left) / n_left)**2)\n            gini_right = 1.0 - ((n1_right / n_right)**2 + ((n_right - n1_right) / n_right)**2)\n            \n            gain = gini_parent - (n_left / n_samples * gini_left + n_right / n_samples * gini_right)\n\n            if gain > best_gain:\n                best_gain = gain\n                best_feature = j\n                \n    return best_feature, best_gain\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (n, sigma, lambda, rho, include_leak, seed)\n        (3000, 0.1, 0.9, 0.2, 1, 11),\n        (3000, 0.3, 0.6, 0.2, 1, 12),\n        (1200, 0.1, 0.9, 0.2, 0, 13),\n        (3000, 0.6, 0.4, 0.2, 1, 14),\n    ]\n\n    results = []\n\n    for n, sigma, lam, rho, include_leak, seed in test_cases:\n        # Set seed for reproducibility for the entire test case\n        np.random.seed(seed)\n\n        # 1. Data Generation\n        p_b = 5\n        betas = np.array([0.8, -1.0, 0.6, 0.0, 0.5])\n        beta_0 = -0.5\n        gamma = 0.7\n        \n        # Covariance matrix\n        cov_matrix = (1 - rho) * np.identity(p_b) + rho * np.ones((p_b, p_b))\n        \n        # Base features\n        X_base = np.random.multivariate_normal(np.zeros(p_b), cov_matrix, n)\n        \n        # Macro factor\n        m = np.random.randn(n)\n        \n        # Latent score\n        s = beta_0 + X_base @ betas + gamma * m\n        \n        # Default probability\n        p_default = 1.0 / (1.0 + np.exp(-s))\n        \n        # Binary outcome\n        y = np.random.binomial(1, p_default, n)\n        \n        # Assemble final feature matrix\n        if include_leak == 1:\n            delta = np.random.normal(0, sigma, n)\n            z = lam * y + delta\n            X = np.c_[X_base, z]\n        else:\n            X = X_base\n        \n        n_samples, p = X.shape\n\n        # 2. Ensemble Training and Importance Calculation\n        T = 200\n        m_try = ceil(sqrt(p))\n        importances = np.zeros(p)\n        \n        all_indices = np.arange(n_samples)\n\n        for _ in range(T):\n            # Bootstrap sample\n            boot_indices = np.random.choice(all_indices, size=n_samples, replace=True)\n            X_boot, y_boot = X[boot_indices], y[boot_indices]\n            \n            # Feature subsampling\n            feature_indices = np.random.choice(p, size=m_try, replace=False)\n            \n            # Find best split for this stump\n            best_feature, best_gain = _find_best_split(X_boot, y_boot, feature_indices)\n            \n            if best_feature != -1:\n                importances[best_feature] += best_gain\n\n        # 3. Find the most important feature index\n        j_max = np.argmax(importances)\n        results.append(j_max)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "标准的决策树算法，如使用基尼不纯度（Gini Impurity）或信息增益，通常平等地对待所有错分情况。然而，在经济和金融应用中，不同错误的代价往往是不对称的——例如，错失一个高价值客户的损失远大于误判一个低价值客户。本练习将带你深入算法的核心，通过用一个自定义的、反映不对称经济成本的损失函数来替换标准的分裂准则，让你学会如何定制算法以使其与真实的商业目标保持一致。[@problem_id:2386905]", "id": "2386905", "problem": "您面临一个基于计算经济学和金融学的二元分类任务，其中每个观测值代表一个客户，每个标签指示该客户是否为高价值客户，并且每个观测值都带有一个收入权重。二元决策树一个节点上的分裂是通过在单个实值特征上选择一个阈值，并在每个由此产生的叶节点中分配一个恒定的类别预测，以最小化一个自定义的经济损失。对于每个观测值索引 $i$（其中 $i \\in \\{1,\\dots,n\\}$），设标量特征为 $x_i \\in \\mathbb{R}$，二元标签为 $y_i \\in \\{0,1\\}$，非负收入权重为 $r_i \\in \\mathbb{R}_{\\ge 0}$。设 $\\alpha \\in \\mathbb{R}_{>0}$ 和 $\\beta \\in \\mathbb{R}_{>0}$ 为给定的错分成本参数。对于任意阈值 $\\tau \\in \\mathbb{R}$，定义左区域 $L(\\tau) = \\{i : x_i < \\tau\\}$ 和右区域 $R(\\tau) = \\{i : x_i \\ge \\tau\\}$。在一个区域 $S \\in \\{L(\\tau), R(\\tau)\\}$ 中，如果恒定预测为 $c \\in \\{0,1\\}$，则由观测值 $i \\in S$ 贡献的错分损失为：如果 $c = y_i$，则为 $0$；如果 $y_i = 1$ 且 $c = 0$，则为 $\\alpha \\cdot r_i$；如果 $y_i = 0$ 且 $c = 1$，则为 $\\beta$。叶节点预测 $c_S(\\tau)$ 必须最小化 $S$ 区域内的总损失。如果在叶节点中预测 $0$ 和 $1$ 的损失出现平局，则选择 $c_S(\\tau) = 0$。总分裂损失是来自 $L(\\tau)$ 和 $R(\\tau)$ 的两个叶节点损失之和。在有限的候选阈值集合 $\\mathcal{T} \\subset \\mathbb{R}$ 中，选择使总分裂损失最小化的 $\\tau^\\star \\in \\mathcal{T}$。如果多个阈值达到相同的最小总损失，则选择数值上最小的 $\\tau^\\star$。对于下方的每个测试用例，您必须确定 $\\tau^\\star$、相应的最小化总损失以及两个叶节点预测 $c_{L}(\\tau^\\star)$ 和 $c_{R}(\\tau^\\star)$。\n\n测试套件。对于每个用例，给定数组 $x$、$y$、$r$，参数 $\\alpha$、$\\beta$ 和候选阈值 $\\mathcal{T}$。请严格应用上述定义。\n\n- A用例（一般情况）：\n  - $x = [\\,1.0,\\,2.0,\\,2.5,\\,3.5,\\,4.0,\\,5.0\\,]$\n  - $y = [\\,0,\\,1,\\,0,\\,1,\\,1,\\,0\\,]$\n  - $r = [\\,10.0,\\,200.0,\\,5.0,\\,150.0,\\,300.0,\\,2.0\\,]$\n  - $\\alpha = 5.0$, $\\beta = 1.0$\n  - $\\mathcal{T} = \\{\\,1.5,\\,2.25,\\,3.0,\\,3.75,\\,4.5\\,\\}$\n- B用例（边界与平局；包含一个空叶节点和一个叶节点平局）：\n  - $x = [\\,0.0,\\,0.1,\\,0.2,\\,0.3\\,]$\n  - $y = [\\,1,\\,1,\\,0,\\,0\\,]$\n  - $r = [\\,1.0,\\,1.0,\\,1.0,\\,1.0\\,]$\n  - $\\alpha = 1.0$, $\\beta = 1.0$\n  - $\\mathcal{T} = \\{\\,-1.0,\\,0.15,\\,1.0\\,\\}$\n- C用例（边界情况，含单个极高收入的正例）：\n  - $x = [\\,10.0,\\,20.0,\\,30.0,\\,40.0,\\,50.0\\,]$\n  - $y = [\\,0,\\,0,\\,1,\\,0,\\,0\\,]$\n  - $r = [\\,1.0,\\,1.0,\\,1000.0,\\,1.0,\\,1.0\\,]$\n  - $\\alpha = 10.0$, $\\beta = 1.0$\n  - $\\mathcal{T} = \\{\\,15.0,\\,25.0,\\,35.0,\\,45.0\\,\\}$\n\n最终输出格式。您的程序必须生成单行输出，其中包含一个结果列表，每个测试用例一个结果，顺序和结构如下：对于每个测试用例，输出列表 $[\\,\\tau^\\star,\\,\\text{total\\_loss}(\\tau^\\star),\\,c_{L}(\\tau^\\star),\\,c_{R}(\\tau^\\star)\\,]$。将这三个用例的列表聚合为单个列表，并在一行中打印该列表。每个用例中 $\\tau^\\star$ 和 $\\text{total\\_loss}(\\tau^\\star)$ 的条目必须是实数；$c_{L}(\\tau^\\star)$ 和 $c_{R}(\\tau^\\star)$ 的条目必须是 $\\{0,1\\}$ 中的整数。", "solution": "所呈现的问题是一个在决策树构建背景下的良定优化问题，专为计算经济学应用而定制。问题陈述具有科学依据、内容完整且逻辑一致，不存在任何歧义或矛盾。任务是从给定的有限集合 $\\mathcal{T}$ 中找到一个最优分裂阈值 $\\tau^\\star$，该阈值可以最小化一个自定义的、非对称的、加权的经济损失函数。\n\n其过程如下：\n对于集合 $\\mathcal{T}$ 中的每个候选阈值 $\\tau$，我们必须计算总分裂损失。这包括划分数据，确定每个划分（叶节点）的最优预测，计算每个叶节点的损失，然后将这些损失相加。\n\n首先，我们来形式化单个区域 $S$ 的计算过程，该区域可以是左区域 $L(\\tau) = \\{i : x_i < \\tau\\}$ 或右区域 $R(\\tau) = \\{i : x_i \\ge \\tau\\}$。一个恒定的预测 $c_S \\in \\{0, 1\\}$ 将被分配给该区域内的所有观测值。区域 $S$ 中的总损失取决于此预测。\n\n设 $S_0 = \\{i \\in S : y_i = 0\\}$ 为 $S$ 中负类观测值的索引集合， $S_1 = \\{i \\in S : y_i = 1\\}$ 为正类观测值的索引集合。\n\n如果我们预测 $c_S = 0$，我们就会错分所有正类观测值。对于每个这样的观测值 $i \\in S_1$，损失为 $\\alpha \\cdot r_i$。预测为 $0$ 的总损失是：\n$$L_0(S) = \\sum_{i \\in S_1} \\alpha \\cdot r_i$$\n\n如果我们预测 $c_S = 1$，我们就会错分所有负类观测值。对于每个这样的观测值 $i \\in S_0$，损失为 $\\beta$。预测为 $1$ 的总损失是：\n$$L_1(S) = \\sum_{i \\in S_0} \\beta = \\beta \\cdot |S_0|$$\n其中 $|S_0|$ 是 $S$ 中负类观测值的数量。\n\n区域的最优预测 $c_S(\\tau)$ 是使该损失最小化的预测。根据问题的平局打破规则，如果 $L_0(S) = L_1(S)$，我们必须选择预测 $c_S(\\tau) = 0$。因此，区域 $S$ 中预测的决策规则是：\n$$\nc_S(\\tau) =\n\\begin{cases}\n1 & \\text{if } L_1(S) < L_0(S) \\\\\n0 & \\text{if } L_0(S) \\le L_1(S)\n\\end{cases}\n$$\n如果决策规则被正确应用，区域 $S$ 对应的最小损失是 $\\text{Loss}(S) = \\min(L_0(S), L_1(S))$。具体来说，如果 $c_S(\\tau) = 1$，则 $\\text{Loss}(S)$ 为 $L_1(S)$；如果 $c_S(\\tau) = 0$，则 $\\text{Loss}(S)$ 为 $L_0(S)$。\n对于一个空区域 $S = \\emptyset$，$S_0$ 和 $S_1$ 都是空的。因此，$L_0(\\emptyset) = 0$ 且 $L_1(\\emptyset) = 0$。根据预测的平局打破规则，规定 $c_\\emptyset(\\tau)=0$，且损失为 $0$。\n\n给定阈值 $\\tau$ 的总分裂损失是左右两个区域的最小损失之和：\n$$\\text{TotalLoss}(\\tau) = \\text{Loss}(L(\\tau)) + \\text{Loss}(R(\\tau))$$\n\n总任务是从候选集合 $\\mathcal{T}$ 中找到最小化此总损失的最优阈值 $\\tau^\\star$：\n$$\\tau^\\star = \\arg\\min_{\\tau \\in \\mathcal{T}} \\text{TotalLoss}(\\tau)$$\n如果多个阈值产生相同的最小损失，问题规定选择数值上最小的那个。\n\n我们现在将此过程应用于每个测试用例。\n\n**A用例**\n- 数据: $x = [1.0, 2.0, 2.5, 3.5, 4.0, 5.0]$, $y = [0, 1, 0, 1, 1, 0]$, $r = [10.0, 200.0, 5.0, 150.0, 300.0, 2.0]$\n- 参数: $\\alpha = 5.0$, $\\beta = 1.0$\n- 候选阈值: $\\mathcal{T} = \\{1.5, 2.25, 3.0, 3.75, 4.5\\}$\n\n1.  **$\\tau = 1.5$**:\n    - $L(1.5)$: 索引 $\\{1\\}$ ($x_1=1.0$)。$S_0=\\{1\\}$, $S_1=\\emptyset$。\n      $L_0(L) = 0$, $L_1(L) = \\beta \\cdot 1 = 1.0$。$L_0 \\le L_1 \\implies c_L=0$, $\\text{Loss}_L = 0.0$。\n    - $R(1.5)$: 索引 $\\{2,3,4,5,6\\}$。$S_0=\\{3,6\\}$, $S_1=\\{2,4,5\\}$。\n      $L_0(R) = 5.0 \\cdot (200.0 + 150.0 + 300.0) = 3250.0$。$L_1(R) = 1.0 \\cdot 2 = 2.0$。\n      $L_1 < L_0 \\implies c_R=1$, $\\text{Loss}_R = 2.0$。\n    - 总损失 = $0.0 + 2.0 = 2.0$。\n\n2.  **$\\tau = 2.25$**:\n    - $L(2.25)$: 索引 $\\{1,2\\}$。$S_0=\\{1\\}$, $S_1=\\{2\\}$。\n      $L_0(L) = 5.0 \\cdot 200.0 = 1000.0$。$L_1(L) = 1.0 \\cdot 1 = 1.0$。$c_L=1$, $\\text{Loss}_L = 1.0$。\n    - $R(2.25)$: 索引 $\\{3,4,5,6\\}$。$S_0=\\{3,6\\}$, $S_1=\\{4,5\\}$。\n      $L_0(R) = 5.0 \\cdot (150.0 + 300.0) = 2250.0$。$L_1(R) = 1.0 \\cdot 2 = 2.0$。$c_R=1$, $\\text{Loss}_R = 2.0$。\n    - 总损失 = $1.0 + 2.0 = 3.0$。\n\n3.  **$\\tau = 3.75$**:\n    - $L(3.75)$: 索引 $\\{1,2,3,4\\}$。$S_0=\\{1,3\\}$, $S_1=\\{2,4\\}$。\n      $L_0(L) = 5.0 \\cdot (200.0 + 150.0) = 1750.0$。$L_1(L) = 1.0 \\cdot 2 = 2.0$。$c_L=1$, $\\text{Loss}_L=2.0$。\n    - $R(3.75)$: 索引 $\\{5,6\\}$。$S_0=\\{6\\}$, $S_1=\\{5\\}$。\n      $L_0(R) = 5.0 \\cdot 300.0 = 1500.0$。$L_1(R) = 1.0 \\cdot 1 = 1.0$。$c_R=1$, $\\text{Loss}_R=1.0$。\n    - 总损失 = $2.0 + 1.0 = 3.0$。\n\n4.  **$\\tau = 4.5$**:\n    - $L(4.5)$: 索引 $\\{1,2,3,4,5\\}$。$S_0=\\{1,3\\}$, $S_1=\\{2,4,5\\}$。\n      $L_0(L) = 5.0 \\cdot (200.0+150.0+300.0) = 3250.0$。$L_1(L) = 1.0 \\cdot 2 = 2.0$。$c_L=1$, $\\text{Loss}_L=2.0$。\n    - $R(4.5)$: 索引 $\\{6\\}$。$S_0=\\{6\\}$, $S_1=\\emptyset$。\n      $L_0(R) = 0$。$L_1(R) = 1.0 \\cdot 1 = 1.0$。$c_R=0$, $\\text{Loss}_R=0.0$。\n    - 总损失 = $2.0 + 0.0 = 2.0$。\n\n找到的最小损失为 $2.0$，在 $\\tau=1.5$ 和 $\\tau=4.5$ 时出现。根据平局打破规则，我们选择最小的阈值。\n结果：$\\tau^\\star = 1.5$，总损失 = $2.0$，$c_L(\\tau^\\star)=0$，$c_R(\\tau^\\star)=1$。\n\n**B用例**\n- 数据: $x = [0.0, 0.1, 0.2, 0.3]$, $y = [1, 1, 0, 0]$, $r = [1.0, 1.0, 1.0, 1.0]$\n- 参数: $\\alpha = 1.0$, $\\beta = 1.0$\n- 候选阈值: $\\mathcal{T} = \\{-1.0, 0.15, 1.0\\}$\n\n1.  **$\\tau = -1.0$**:\n    - $L(-1.0)$: $\\emptyset$。$\\text{Loss}_L = 0.0$, $c_L=0$（根据平局打破规则）。\n    - $R(-1.0)$: 索引 $\\{1,2,3,4\\}$。$S_0=\\{3,4\\}$, $S_1=\\{1,2\\}$。\n      $L_0(R) = 1.0 \\cdot (1.0+1.0) = 2.0$。$L_1(R) = 1.0 \\cdot 2 = 2.0$。\n      $L_0 \\le L_1 \\implies c_R=0$, $\\text{Loss}_R = 2.0$。\n    - 总损失 = $0.0 + 2.0 = 2.0$。\n\n2.  **$\\tau = 0.15$**:\n    - $L(0.15)$: 索引 $\\{1,2\\}$。$S_0=\\emptyset$, $S_1=\\{1,2\\}$。\n      $L_0(L) = 1.0 \\cdot (1.0+1.0) = 2.0$。$L_1(L) = 1.0 \\cdot 0 = 0$。$c_L=1$, $\\text{Loss}_L=0.0$。\n    - $R(0.15)$: 索引 $\\{3,4\\}$。$S_0=\\{3,4\\}$, $S_1=\\emptyset$。\n      $L_0(R) = 0$。$L_1(R) = 1.0 \\cdot 2 = 2.0$。$c_R=0$, $\\text{Loss}_R=0.0$。\n    - 总损失 = $0.0 + 0.0 = 0.0$。\n\n3.  **$\\tau = 1.0$**:\n    - $L(1.0)$: 索引 $\\{1,2,3,4\\}$。与 $R(-1.0)$ 相同。$c_L=0$, $\\text{Loss}_L=2.0$。\n    - $R(1.0)$: $\\emptyset$。$\\text{Loss}_R = 0.0$, $c_R=0$。\n    - 总损失 = $2.0 + 0.0 = 2.0$。\n\n找到的最小损失为 $0.0$，唯一地在 $\\tau=0.15$ 时出现。\n结果：$\\tau^\\star = 0.15$，总损失 = $0.0$，$c_L(\\tau^\\star)=1$，$c_R(\\tau^\\star)=0$。\n\n**C用例**\n- 数据: $x = [10.0, 20.0, 30.0, 40.0, 50.0]$, $y = [0, 0, 1, 0, 0]$, $r = [1.0, 1.0, 1000.0, 1.0, 1.0]$\n- 参数: $\\alpha = 10.0$, $\\beta = 1.0$\n- 候选阈值: $\\mathcal{T} = \\{15.0, 25.0, 35.0, 45.0\\}$\n\n1.  **$\\tau = 15.0$**:\n    - $L(15.0)$: 索引 $\\{1\\}$。$S_0=\\{1\\}$, $S_1=\\emptyset$。$c_L=0$, $\\text{Loss}_L=0.0$。\n    - $R(15.0)$: 索引 $\\{2,3,4,5\\}$。$S_0=\\{2,4,5\\}$, $S_1=\\{3\\}$。\n      $L_0(R) = 10.0 \\cdot 1000.0 = 10000.0$。$L_1(R) = 1.0 \\cdot 3 = 3.0$。$c_R=1$, $\\text{Loss}_R=3.0$。\n    - 总损失 = $0.0 + 3.0 = 3.0$。\n\n2.  **$\\tau = 25.0$**:\n    - $L(25.0)$: 索引 $\\{1,2\\}$。$S_0=\\{1,2\\}$, $S_1=\\emptyset$。$c_L=0$, $\\text{Loss}_L=0.0$。\n    - $R(25.0)$: 索引 $\\{3,4,5\\}$。$S_0=\\{4,5\\}$, $S_1=\\{3\\}$。\n      $L_0(R) = 10.0 \\cdot 1000.0 = 10000.0$。$L_1(R) = 1.0 \\cdot 2 = 2.0$。$c_R=1$, $\\text{Loss}_R=2.0$。\n    - 总损失 = $0.0 + 2.0 = 2.0$。\n\n3.  **$\\tau = 35.0$**:\n    - $L(35.0)$: 索引 $\\{1,2,3\\}$。$S_0=\\{1,2\\}$, $S_1=\\{3\\}$。\n      $L_0(L) = 10.0 \\cdot 1000.0 = 10000.0$。$L_1(L) = 1.0 \\cdot 2 = 2.0$。$c_L=1$, $\\text{Loss}_L=2.0$。\n    - $R(35.0)$: 索引 $\\{4,5\\}$。$S_0=\\{4,5\\}$, $S_1=\\emptyset$。$c_R=0$, $\\text{Loss}_R=0.0$。\n    - 总损失 = $2.0 + 0.0 = 2.0$。\n\n4.  **$\\tau = 45.0$**:\n    - $L(45.0)$: 索引 $\\{1,2,3,4\\}$。$S_0=\\{1,2,4\\}$, $S_1=\\{3\\}$。\n      $L_0(L) = 10.0 \\cdot 1000.0 = 10000.0$。$L_1(L) = 1.0 \\cdot 3 = 3.0$。$c_L=1$, $\\text{Loss}_L=3.0$。\n    - $R(45.0)$: 索引 $\\{5\\}$。$S_0=\\{5\\}$, $S_1=\\emptyset$。$c_R=0$, $\\text{Loss}_R=0.0$。\n    - 总损失 = $3.0 + 0.0 = 3.0$。\n\n最小损失为 $2.0$，在 $\\tau=25.0$ 和 $\\tau=35.0$ 时出现。选择最小的阈值得到 $\\tau^\\star=25.0$。\n结果：$\\tau^\\star = 25.0$，总损失 = $2.0$，$c_L(\\tau^\\star)=0$，$c_R(\\tau^\\star)=1$。", "answer": "```python\nimport numpy as np\n\ndef calculate_leaf_stats(y_leaf, r_leaf, alpha, beta):\n    \"\"\"\n    Calculates the minimum loss and optimal prediction for a single leaf (region).\n\n    Args:\n        y_leaf (np.ndarray): Binary labels {0, 1} for the observations in the leaf.\n        r_leaf (np.ndarray): Revenue weights for the observations in the leaf.\n        alpha (float): Cost parameter for misclassifying a y=1 as 0.\n        beta (float): Cost parameter for misclassifying a y=0 as 1.\n\n    Returns:\n        tuple[float, int]: A tuple containing (minimum_loss, optimal_prediction).\n    \"\"\"\n    # For an empty leaf, loss is 0. Prediction is 0 by tie-breaking rule.\n    if len(y_leaf) == 0:\n        return 0.0, 0\n\n    # Calculate loss if we predict 0 for the entire leaf.\n    # This misclassifies all observations where y=1.\n    mask_y1 = (y_leaf == 1)\n    cost_if_0 = alpha * np.sum(r_leaf[mask_y1])\n\n    # Calculate loss if we predict 1 for the entire leaf.\n    # This misclassifies all observations where y=0.\n    mask_y0 = (y_leaf == 0)\n    cost_if_1 = beta * np.sum(mask_y0)\n\n    # Determine optimal prediction and corresponding loss.\n    # Tie-breaker: if costs are equal, predict 0.\n    if cost_if_1 < cost_if_0:\n        return float(cost_if_1), 1\n    else:\n        return float(cost_if_0), 0\n\ndef solve():\n    \"\"\"\n    Solves the decision tree split optimization problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"x\": np.array([1.0, 2.0, 2.5, 3.5, 4.0, 5.0]),\n            \"y\": np.array([0, 1, 0, 1, 1, 0]),\n            \"r\": np.array([10.0, 200.0, 5.0, 150.0, 300.0, 2.0]),\n            \"alpha\": 5.0, \"beta\": 1.0,\n            \"T_set\": [1.5, 2.25, 3.0, 3.75, 4.5]\n        },\n        {\n            \"x\": np.array([0.0, 0.1, 0.2, 0.3]),\n            \"y\": np.array([1, 1, 0, 0]),\n            \"r\": np.array([1.0, 1.0, 1.0, 1.0]),\n            \"alpha\": 1.0, \"beta\": 1.0,\n            \"T_set\": [-1.0, 0.15, 1.0]\n        },\n        {\n            \"x\": np.array([10.0, 20.0, 30.0, 40.0, 50.0]),\n            \"y\": np.array([0, 0, 1, 0, 0]),\n            \"r\": np.array([1.0, 1.0, 1000.0, 1.0, 1.0]),\n            \"alpha\": 10.0, \"beta\": 1.0,\n            \"T_set\": [15.0, 25.0, 35.0, 45.0]\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        x, y, r, alpha, beta, T_set = case[\"x\"], case[\"y\"], case[\"r\"], case[\"alpha\"], case[\"beta\"], case[\"T_set\"]\n        \n        best_tau = None\n        best_loss = float('inf')\n        best_c_L = -1\n        best_c_R = -1\n\n        for tau in T_set:\n            # Partition the data into left and right regions based on the threshold.\n            left_mask = x < tau\n            right_mask = x >= tau\n\n            # Calculate loss and prediction for the left leaf.\n            loss_L, c_L = calculate_leaf_stats(y[left_mask], r[left_mask], alpha, beta)\n            \n            # Calculate loss and prediction for the right leaf.\n            loss_R, c_R = calculate_leaf_stats(y[right_mask], r[right_mask], alpha, beta)\n\n            total_loss = loss_L + loss_R\n\n            # Check if this threshold provides a better or equal-but-smaller result.\n            if total_loss < best_loss:\n                best_loss = total_loss\n                best_tau = tau\n                best_c_L = c_L\n                best_c_R = c_R\n            elif total_loss == best_loss:\n                # Tie-breaker: choose the numerically smallest threshold.\n                if best_tau is None or tau < best_tau:\n                    best_tau = tau\n                    best_c_L = c_L\n                    best_c_R = c_R\n        \n        # Store the result for the current case.\n        case_result = [float(best_tau), float(best_loss), int(best_c_L), int(best_c_R)]\n        all_results.append(case_result)\n\n    # Print the aggregated results in the specified list-of-lists format.\n    print(all_results)\n\nsolve()\n```"}]}