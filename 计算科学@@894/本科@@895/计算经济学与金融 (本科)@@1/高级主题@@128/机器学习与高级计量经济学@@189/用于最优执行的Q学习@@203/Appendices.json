{"hands_on_practices": [{"introduction": "在智能体学习制定最优决策之前，我们必须首先为其构建一个它将要操作的世界的模拟环境。这个练习的核心是实现一个常见市场环境模型的核心机制，包括价格影响和未完成清算库存的终端惩罚。通过从头开始编写模拟代码，您将对状态转移和奖励计算有一个具体的理解，这是任何最优执行中强化学习任务的基础 [@problem_id:2423600]。", "id": "2423600", "problem": "您将得到一个离散时间执行环境，用于在有限的时间范围内出售固定库存。在每个时间步，交易员选择一个非负整数操作，指定要出售的数量。该交易以包含临时价格影响的执行价格产生即时现金流，并永久性地改变后续步骤中使用的中间价。如果在截止日期前全部库存尚未清算，则会施加终端惩罚。目标是为每个指定的参数集计算从给定的操作序列中获得的总片段奖励。\n\n形式上，令时间索引为 $t \\in \\{0,1,\\dots,T-1\\}$，截止日期为 $T$。状态包含当前中间价 $S_t \\in \\mathbb{R}$ 和剩余库存 $x_t \\in \\mathbb{N}_0$。交易员选择一个意向操作 $a_t \\in \\mathbb{N}_0$，代表要出售的单位数量。执行数量为\n$$\na'_t \\equiv \\min\\{a_t, x_t\\}。\n$$\n在时间 $t$ 的执行价格为\n$$\n\\tilde{S}_t \\equiv S_t - \\eta\\, a'_t,\n$$\n其中 $\\eta \\ge 0$ 是临时影响系数。在时间 $t$ 的即时奖励（现金流）为\n$$\nr_t \\equiv a'_t \\, \\tilde{S}_t = a'_t \\left(S_t - \\eta\\, a'_t\\right)。\n$$\n库存和价格按如下方式演变\n$$\nx_{t+1} = x_t - a'_t,\\qquad\nS_{t+1} = S_t - \\kappa\\, a'_t + \\epsilon_t,\n$$\n其中 $\\kappa \\ge 0$ 是永久影响系数，而 $\\{\\epsilon_t\\}_{t=0}^{T-1}$ 是一个指定的外生价格冲击的确定性序列。在终端时间 $T$，对任何剩余库存施加惩罚：\n$$\nR_T \\equiv -\\lambda\\, x_T^2,\n$$\n其中 $\\lambda \\ge 0$。总片段奖励为\n$$\nG \\equiv \\sum_{t=0}^{T-1} r_t + R_T。\n$$\n所有价格、成本和奖励均以任意货币单位计；不涉及物理单位。\n\n实现一个程序，对下方测试套件中的每个参数集，根据所提供的操作序列计算 $G$。对于每种情况，使用上述动态模型和给定参数。您必须将 $a_t$ 视为意向操作，并按规定执行 $a'_t = \\min\\{a_t, x_t\\}$。\n\n测试套件（每种情况指定 $(S_0, x_0, T, \\kappa, \\eta, \\lambda, \\{a_t\\}_{t=0}^{T-1}, \\{\\epsilon_t\\}_{t=0}^{T-1})$）：\n- 情况 1：$S_0 = 100.0$, $x_0 = 5$, $T = 3$, $\\kappa = 0.2$, $\\eta = 0.5$, $\\lambda = 10.0$, 操作 [2,2,1], 冲击 [0.0,0.0,0.0]。\n- 情况 2：$S_0 = 100.0$, $x_0 = 5$, $T = 3$, $\\kappa = 0.2$, $\\eta = 0.5$, $\\lambda = 10.0$, 操作 [1,1,1], 冲击 [0.0,0.0,0.0]。\n- 情况 3：$S_0 = 100.0$, $x_0 = 0$, $T = 3$, $\\kappa = 0.2$, $\\eta = 0.5$, $\\lambda = 10.0$, 操作 [2,2,2], 冲击 [0.0,0.0,0.0]。\n- 情况 4：$S_0 = 50.0$, $x_0 = 3$, $T = 2$, $\\kappa = 0.3$, $\\eta = 1.0$, $\\lambda = 5.0$, 操作 [2,2], 冲击 [1.5,0.0]。\n- 情况 5：$S_0 = 20.0$, $x_0 = 10$, $T = 2$, $\\kappa = 0.0$, $\\eta = 0.0$, $\\lambda = 100.0$, 操作 [0,0], 冲击 [0.0,0.0]。\n- 情况 6：$S_0 = 100.0$, $x_0 = 4$, $T = 2$, $\\kappa = 0.5$, $\\eta = 0.25$, $\\lambda = 1.0$, 操作 [3,1], 冲击 [-10.0,0.0]。\n\n您的程序应生成单行输出，其中包含六种情况的结果，格式为由方括号括起来的逗号分隔列表。每个结果必须四舍五入到六位小数。输出必须采用确切的格式：\n- 仅一行。\n- 用方括号括住列表。\n- 值之间用逗号分隔。\n- 每个值显示小数点后恰好六位数字。", "solution": "问题陈述是有效的。它在计算金融领域，特别是关于最优执行模型方面，提出了一个定义明确且具有科学依据的任务。该问题要求在一个离散的、有限的时间范围内模拟一个交易过程，其所有控制方程、参数和初始条件都已明确无误地给出。目标是为给定的操作序列计算总片段奖励，这是一个基于指定模型动态的确定性计算。\n\n该模型通过中间价 $S_t$ 和剩余库存 $x_t$ 来描述系统在时间 $t$ 的状态。该过程在时间步 $t \\in \\{0, 1, \\dots, T-1\\}$ 上进行模拟。\n\n为给定的参数集 $(S_0, x_0, T, \\kappa, \\eta, \\lambda, \\{a_t\\}_{t=0}^{T-1}, \\{\\epsilon_t\\}_{t=0}^{T-1})$ 计算总片段奖励 $G$ 的过程如下。\n\n首先，将总奖励累加器初始化为零，我们称之为 $G_{acc} = 0$。初始状态由 $(S_0, x_0)$ 给出。\n\n接下来，我们从 $0$ 到 $T-1$ 遍历每个时间步 $t$。在每一步中：\n1. 确定执行数量 $a'_t$。由于交易员不能出售比可用库存更多的库存，因此执行数量是意向操作 $a_t$ 和当前库存 $x_t$ 中的最小值。\n$$\na'_t = \\min\\{a_t, x_t\\}\n$$\n2. 计算在时间 $t$ 获得的即时奖励 $r_t$。这是销售产生的现金流，是执行数量 $a'_t$ 和执行价格 $\\tilde{S}_t$ 的乘积。执行价格是中间价 $S_t$ 根据与交易规模成比例的临时价格影响进行调整后的价格，系数为 $\\eta$。\n$$\nr_t = a'_t \\cdot \\tilde{S}_t = a'_t \\cdot (S_t - \\eta \\cdot a'_t)\n$$\n3. 将此即时奖励加到累积的总奖励中：\n$$\nG_{acc} \\leftarrow G_{acc} + r_t\n$$\n4. 为下一个时间步 $t+1$ 更新状态变量。库存因执行数量 $a'_t$ 而减少。中间价通过考虑与 $a'_t$ 成比例（系数为 $\\kappa$）的永久价格影响以及外生价格冲击 $\\epsilon_t$ 来更新。\n$$\nx_{t+1} = x_t - a'_t\n$$\n$$\nS_{t+1} = S_t - \\kappa \\cdot a'_t + \\epsilon_t\n$$\n此循环一直持续到 $t=T-1$。循环的最后一步之后，状态为 $(S_T, x_T)$。\n\n最后，我们对在截止日期 $T$ 时任何未售出的库存 $x_T$ 施加终端惩罚 $R_T$。该惩罚是剩余库存的二次函数，惩罚系数为 $\\lambda$。\n$$\nR_T = -\\lambda \\cdot x_T^2\n$$\n总片段奖励 $G$ 是在整个交易期限内累积的所有即时奖励与终端惩罚之和。\n$$\nG = G_{acc} + R_T = \\left(\\sum_{t=0}^{T-1} r_t\\right) + R_T\n$$\n对于每个给定的测试用例，此过程都会产生一个唯一的、确定性的 $G$ 值。该计算是所给公式的直接应用。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the total episodic reward for a series of optimal execution problems.\n    \"\"\"\n    # Test suite (S0, x0, T, kappa, eta, lambda, actions, shocks)\n    test_cases = [\n        # Case 1\n        (100.0, 5, 3, 0.2, 0.5, 10.0, [2, 2, 1], [0.0, 0.0, 0.0]),\n        # Case 2\n        (100.0, 5, 3, 0.2, 0.5, 10.0, [1, 1, 1], [0.0, 0.0, 0.0]),\n        # Case 3\n        (100.0, 0, 3, 0.2, 0.5, 10.0, [2, 2, 2], [0.0, 0.0, 0.0]),\n        # Case 4\n        (50.0, 3, 2, 0.3, 1.0, 5.0, [2, 2], [1.5, 0.0]),\n        # Case 5\n        (20.0, 10, 2, 0.0, 0.0, 100.0, [0, 0], [0.0, 0.0]),\n        # Case 6\n        (100.0, 4, 2, 0.5, 0.25, 1.0, [3, 1], [-10.0, 0.0]),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        s0, x0, T, kappa, eta, lambd, actions, shocks = case\n        \n        # Initialize state variables\n        s_t = s0\n        x_t = x0\n        total_reward = 0.0\n\n        # Simulate the execution process over the time horizon\n        for t in range(T):\n            action_t = actions[t]\n            shock_t = shocks[t]\n\n            # 1. Determine executed quantity\n            executed_quantity = min(action_t, x_t)\n\n            # 2. Calculate immediate reward\n            if executed_quantity > 0:\n                execution_price = s_t - eta * executed_quantity\n                immediate_reward = executed_quantity * execution_price\n                total_reward += immediate_reward\n            \n            # 3. Update state for the next time step\n            x_t_plus_1 = x_t - executed_quantity\n            s_t_plus_1 = s_t - kappa * executed_quantity + shock_t\n            \n            # Move to the next state\n            x_t = x_t_plus_1\n            s_t = s_t_plus_1\n\n        # 4. Calculate terminal penalty\n        terminal_penalty = -lambd * (x_t ** 2)\n        total_reward += terminal_penalty\n        \n        results.append(total_reward)\n\n    # Format the results as specified\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"}, {"introduction": "在建立了模拟环境之后，我们现在可以训练一个智能体来使用 Q-learning 发现最优的交易策略。这个实践将指导您实现表格型 Q-learning 算法来解决一个简化的最优执行问题，重点关注交易成本和库存风险之间的权衡 [@problem_id:2423640]。您将研究折扣因子 $\\gamma$ 如何影响智能体的行为，从而直观地展示这个参数如何控制即时奖励与未来奖励之间的平衡，并最终决定智能体的交易时间跨度。", "id": "2423640", "problem": "给定一个程式化的最优执行问题，该问题被建模为离散时间马尔可夫决策过程 (MDP)，其中一个强化学习 (RL) 智能体通过标准的 Q-learning 递推学习一个动作价值函数。目标是研究折扣因子 $\\,\\gamma\\,$ 如何影响智能体的有效交易期限，该期限定义为在学习到的动作价值函数所导出的贪心策略下，完全清算一个固定库存所需的时间步数。\n\n该 MDP 的具体定义如下。\n\n- 时间：$\\,t \\in \\{0,1,\\dots,T\\}\\,$，具有固定的期限 $\\,T\\,$。当 $\\,t=T\\,$ 或库存降为零时，回合终止，以先到者为准。\n- 状态：$\\,s_t=(t,x_t)\\,$，其中库存 $\\,x_t \\in \\{0,1,\\dots,X_0\\}\\,$ 是在时间 $\\,t\\,$ 剩余的离散单位数量。\n- 动作：在状态 $\\,s_t=(t,x_t)\\,$，智能体选择一个整数交易规模 $\\,a_t \\in \\{0,1,\\dots,\\min(s_{\\max},x_t)\\}\\,$，表示在时期 $\\,t\\,$ 内卖出的单位数量。\n- 转移：库存确定性地演变为 $\\,x_{t+1}=x_t - a_t\\,$，并且时间递增 $\\,t \\mapsto t+1\\,$。\n- 即时奖励：在状态 $\\,s_t=(t,x_t)\\,$ 下采取动作 $\\,a_t\\,$ 的奖励为\n$$\nr_t \\;=\\; -\\big(k\\,a_t^2 \\;+\\; \\lambda\\,x_t^2\\big),\n$$\n其中 $\\,k>0\\,$ 是交易成本曲率，$\\,\\lambda>0\\,$ 是库存持有风险惩罚。终止状态的持续价值为零。\n- 目标：对于一个固定的折扣因子 $\\,\\gamma \\in [0,1]\\,$，智能体寻求最大化期望折扣回报 $\\,\\sum_{t=0}^{T-1} \\gamma^t\\,r_t\\,$。\n\nQ-learning 智能体根据以下递推公式更新动作价值估计：\n$$\nQ(s_t,a_t) \\;\\leftarrow\\; (1-\\alpha)\\,Q(s_t,a_t) \\;+\\; \\alpha \\left[r_t \\;+\\; \\gamma \\,\\max_{a' \\in \\mathcal{A}(s_{t+1})} Q(s_{t+1},a')\\right],\n$$\n约定如果 $\\,s_{t+1}\\,$ 是终止状态（即 $\\,t+1=T\\,$ 或 $\\,x_{t+1}=0\\,$），则目标值简化为 $\\,r_t\\,$，因为没有后续价值。此处 $\\,\\alpha \\in (0,1]\\,$ 是学习率，$\\,\\mathcal{A}(s)\\,$ 表示在状态 $\\,s\\,$ 下的可行动作集合。在学习过程中，智能体遵循 $\\varepsilon$-贪心行为策略：以概率 $\\,\\varepsilon\\,$ 从可行动作中均匀随机选择一个动作，以概率 $\\,1-\\varepsilon\\,$ 选择一个最大化当前 $\\,Q\\,$ 估计值的贪心动作。$\\arg\\max$ 中的平局总是通过选择最大的动作来打破（即在所有最大化动作中选择最大的 $\\,a\\,$）。\n\n训练期间用于动作选择的所有随机性都必须由一个以种子 $\\,0\\,$ 初始化的伪随机数生成器生成，以确保可复现性。\n\n使用的参数值如下：\n- 期限 $\\,T=10\\,$。\n- 初始库存 $\\,X_0=10\\,$。\n- 每期最大卖出规模 $\\,s_{\\max}=3\\,$。\n- 交易成本曲率 $\\,k=0.05\\,$。\n- 库存持有风险惩罚 $\\,\\lambda=0.10\\,$。\n- 学习率 $\\,\\alpha=0.10\\,$。\n- 探索概率 $\\,\\varepsilon=0.20\\,$。\n- 训练回合数 $\\,N_{\\text{episodes}}=20000\\,$。\n- 每个训练回合都从固定的初始状态 $\\,s_0=(0,X_0)\\,$ 开始。\n\n对于下面测试集中的每个折扣因子 $\\,\\gamma\\,$，您必须：\n- 根据上述规格训练 Q-learning 智能体。\n- 训练后，导出由学习到的动作价值函数所引出的贪心策略。\n- 从固定的初始状态 $\\,s_0=(0,X_0)\\,$ 开始，模拟无探索的贪心策略，以获得清算时间 $\\,L(\\gamma)\\,$，该时间定义为满足 $\\,x_{\\ell}=0\\,$ 的最小整数 $\\,\\ell \\in \\{0,1,\\dots,T\\}\\,$。如果在 $\\,T\\,$ 时刻库存未能完全清算，则定义 $\\,L(\\gamma)=T\\,$。\n\n折扣因子测试集：\n- $\\,\\gamma \\in \\{0.0,\\,0.3,\\,0.6,\\,0.9,\\,0.99,\\,1.0\\}\\,$.\n\n您的程序必须将六个清算时间作为单行输出，该行包含一个用方括号括起来的逗号分隔列表，顺序与测试集中的顺序一致。要求的最终输出格式是：\n- 单行：例如，$\\,[1,2,3,4,5,6]\\,$，其中每个条目是对应于测试集中相同位置折扣因子的整数清算时间。输出行中不允许有空格。", "solution": "所提出的问题是计算金融领域内，特别是在最优交易执行方面，一个有效且适定的最优控制问题。它要求实施 Q-learning 算法来求解一个离散时间的马尔可夫决策过程 (MDP)。我将首先阐明其理论基础，然后描述获取解决方案的算法流程。\n\n该问题被建模为一个由元组 $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$ 定义的 MDP。\n- 状态空间 $\\mathcal{S}$ 由状态对 $s_t=(t,x_t)$ 组成，其中 $t \\in \\{0, 1, \\dots, T\\}$ 是时间步， $x_t \\in \\{0, 1, \\dots, X_0\\}$ 是剩余库存。当 $T=10$ 和 $X_0=10$ 时，状态数量为 $(T+1) \\times (X_0+1) = 11 \\times 11 = 121$。\n- 状态 $s_t=(t,x_t)$ 下的动作空间 $\\mathcal{A}(s_t)$ 由允许的交易规模集合 $a_t \\in \\{0, 1, \\dots, \\min(s_{\\max}, x_t)\\}$ 构成，其中 $s_{\\max}=3$。\n- 转移函数是确定性的。给定状态 $s_t=(t,x_t)$ 和动作 $a_t$，下一个状态是 $s_{t+1}=(t+1, x_t - a_t)$。\n- 即时奖励函数 $r(s_t, a_t)$ 由 $r_t = -\\big(k\\,a_t^2 + \\lambda\\,x_t^2\\big)$ 给出。该函数捕捉了最优执行中的基本权衡：卖出大量 $a_t$ 会产生高昂的二次交易成本 ($k\\,a_t^2$)，而持有大量库存 $x_t$ 则会产生高昂的二次持有风险成本 ($\\lambda\\,x_t^2$)。智能体的目标是最小化总累积成本，这等同于最大化总累积负奖励。\n- 折扣因子 $\\gamma \\in [0,1]$ 决定了未来奖励的现值。\n\n目标是找到一个最优策略 $\\pi^*: \\mathcal{S} \\to \\mathcal{A}$，对于任何起始状态，该策略都能最大化期望折扣奖励总和（即价值函数）。我们使用 Q-learning 这一无模型强化学习算法来寻找最优动作价值函数 $Q^*(s,a)$，它表示从状态 $s$ 开始，采取动作 $a$，然后遵循最优策略所能获得的最大期望回报。最优动作价值函数满足 Bellman 最优方程：\n$$\nQ^*(s,a) = \\mathbb{E}\\left[r + \\gamma \\max_{a' \\in \\mathcal{A}(s')} Q^*(s', a') \\mid s, a\\right]\n$$\n其中 $s'$ 是在状态 $s$ 采取动作 $a$ 后的状态。由于转移是确定性的，该方程简化为：\n$$\nQ^*(s_t, a_t) = r_t + \\gamma \\max_{a' \\in \\mathcal{A}(s_{t+1})} Q^*(s_{t+1}, a')\n$$\nQ-learning 通过使用与环境交互产生的样本来更新估计值 $Q(s,a)$，从而迭代地逼近 $Q^*$。其更新规则是：\n$$\nQ(s_t, a_t) \\leftarrow (1-\\alpha)Q(s_t, a_t) + \\alpha \\left[r_t + \\gamma \\max_{a' \\in \\mathcal{A}(s_{t+1})} Q(s_{t+1}, a')\\right]\n$$\n此处，$\\alpha=0.10$ 是学习率。训练期间使用 $\\varepsilon=0.20$ 的 $\\varepsilon$-贪心策略来确保对状态-动作空间的探索。为保证可复现性，随机种子固定为 0。\n\n折扣因子 $\\gamma$ 的作用是该问题的核心。它决定了智能体的时间偏好。\n- 当 $\\gamma \\approx 0$ 时，智能体是短视的，会严重贴现未来的奖励（成本）。它主要寻求最大化即时奖励 $r_t = -k\\,a_t^2 - \\lambda\\,x_t^2$。由于在状态 $s_t$ 时 $x_t$ 是固定的，这简化为最大化 $-k\\,a_t^2$，即通过最小化 $a_t$ 来实现。智能体学会了尽可能少地交易，导致清算过程非常漫长或无法完成。清算时间 $L(\\gamma)$ 应该很大，可能为 $T=10$。\n- 当 $\\gamma \\approx 1$ 时，智能体是有远见的。$\\gamma \\max_{a'} Q(s_{t+1}, a')$ 这一项变得非常重要。该项携带着所有未来成本的信息。智能体认识到持有库存（$x > 0$）将产生一连串的未来持有成本。为了最小化总成本，它有动机更快地清算库存，即使这意味着产生更高的即时交易成本。这将导致更短的清算时间。\n\n因此，我们预期清算时间 $L(\\gamma)$ 是折扣因子 $\\gamma$ 的一个非增函数。\n\n该解决方案的实现步骤如下：\n1. 对每个给定的 $\\gamma$ 值，将一个维度为 $(T+1, X_0+1, s_{\\max}+1)$ 的 Q-table 初始化为零。\n2. 智能体被训练 $N_{\\text{episodes}} = 20000$ 个回合。在每个回合中，从 $s_0=(0, 10)$ 开始，智能体与环境交互，直到达到终止状态（$t=T$ 或 $x=0$）。\n3. 在每一步，通过 $\\varepsilon$-贪心策略选择一个动作。平局打破规则（即在 Q 值相同的动作中选择最大的交易规模）被严格执行。\n4. 根据指定的递推公式更新 Q-table。\n5. 训练结束后，提取贪心策略 $\\pi_G(s) = \\arg\\max_a Q(s,a)$。同样的平局打破规则适用。\n6. 从 $s_0=(0, 10)$ 开始，遵循策略 $\\pi_G$ 运行一次确定性模拟。将库存 $x_t$ 降至 0 所需的时间步数被记录为清算时间 $L(\\gamma)$。如果在 $t=T$ 时库存不为零，则 $L(\\gamma)$ 设为 $T$。\n对所有指定的 $\\gamma$ 值重复此过程，并报告最终的清算时间。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimal execution problem using Q-learning for a suite of discount factors.\n    \"\"\"\n    \n    # Problem parameters\n    T = 10\n    X0 = 10\n    S_MAX = 3\n    K = 0.05\n    LAMBDA = 0.10\n\n    # Q-learning parameters\n    ALPHA = 0.10\n    EPSILON = 0.20\n    N_EPISODES = 20000\n    SEED = 0\n    \n    # Test suite for the discount factor\n    gammas = [0.0, 0.3, 0.6, 0.9, 0.99, 1.0]\n    \n    liquidation_times = []\n\n    for gamma in gammas:\n        # Initialize Q-table: Q(t, x, a)\n        # Dimensions: (time_steps, inventory_levels, action_choices)\n        q_table = np.zeros((T + 1, X0 + 1, S_MAX + 1))\n        \n        # Initialize pseudo-random number generator for reproducibility\n        rng = np.random.default_rng(SEED)\n\n        # Main training loop\n        for _ in range(N_EPISODES):\n            t = 0\n            x = X0\n            \n            # An episode runs until a terminal state is reached\n            while t < T and x > 0:\n                current_state_t = t\n                current_state_x = x\n\n                # Determine the set of feasible actions\n                feasible_actions = list(range(min(S_MAX, current_state_x) + 1))\n\n                # Epsilon-greedy action selection\n                if rng.random() < EPSILON:\n                    # Exploration: choose a random feasible action\n                    action = rng.choice(feasible_actions)\n                else:\n                    # Exploitation: choose the best action based on Q-values\n                    q_values_for_state = q_table[current_state_t, current_state_x, feasible_actions]\n                    max_q = np.max(q_values_for_state)\n                    \n                    # Tie-breaking: choose the largest action among those with max Q-value\n                    best_actions = [a for i, a in enumerate(feasible_actions) if q_values_for_state[i] == max_q]\n                    action = max(best_actions)\n\n                # Execute action: calculate reward and find next state\n                reward = -(K * action**2 + LAMBDA * current_state_x**2)\n                next_t = t + 1\n                next_x = x - action\n\n                # Q-table update\n                is_next_state_terminal = (next_t == T) or (next_x == 0)\n                \n                if is_next_state_terminal:\n                    q_max_next = 0.0\n                else:\n                    next_feasible_actions = list(range(min(S_MAX, next_x) + 1))\n                    q_max_next = np.max(q_table[next_t, next_x, next_feasible_actions])\n                \n                target = reward + gamma * q_max_next\n                \n                old_q_value = q_table[current_state_t, current_state_x, action]\n                q_table[current_state_t, current_state_x, action] = \\\n                    (1 - ALPHA) * old_q_value + ALPHA * target\n\n                # Transition to the next state\n                t = next_t\n                x = next_x\n        \n        # After training, simulate the greedy policy to find liquidation time\n        t_sim = 0\n        x_sim = X0\n        liquidation_time = T # Default if not liquidated by T\n\n        while t_sim < T and x_sim > 0:\n            current_state_t_sim = t_sim\n            current_state_x_sim = x_sim\n\n            # Select greedy action with the apecified tie-breaking rule\n            sim_feasible_actions = list(range(min(S_MAX, current_state_x_sim) + 1))\n            q_values_sim = q_table[current_state_t_sim, current_state_x_sim, sim_feasible_actions]\n            max_q_sim = np.max(q_values_sim)\n            best_actions_sim = [a for i, a in enumerate(sim_feasible_actions) if q_values_sim[i] == max_q_sim]\n            action_sim = max(best_actions_sim)\n\n            x_sim -= action_sim\n            t_sim += 1\n            \n            if x_sim == 0:\n                liquidation_time = t_sim\n                break\n        \n        liquidation_times.append(liquidation_time)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, liquidation_times))}]\")\n\nsolve()\n```"}]}