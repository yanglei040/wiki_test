{"hands_on_practices": [{"introduction": "我们将从最基础的任务开始：从零开始构建虚拟对局（fictitious play）算法。通过在一个比标准“石头-剪刀-布”更复杂的游戏中模拟这一过程，你将对该算法的核心机制——信念如何作为经验频率形成，以及参与者如何做出最优反应选择——获得深刻而实践性的理解。这个练习是进一步分析博弈中学习行为的基础。[@problem_id:2405836]", "id": "2405836", "problem": "要求您设计并实现一个完整的、可运行的程序，该程序在一个双人零和的“石头-剪刀-布-蜥蜴-斯波克”（Rock-Paper-Scissors-Lizard-Spock）游戏中模拟确定性虚拟博弈（deterministic fictitious play），记录一个参与者的五维信念轨迹，并在指定的时间快照下生成可供可视化的二维主成分坐标。最终输出必须是精确指定格式的单行文本。\n\n基本依据和定义：\n- 考虑一个双人范式零和博弈，其行动集 $\\mathcal{A}=\\{0,1,2,3,4\\}$ 分别对应石头、布、剪刀、蜥蜴和斯波克。\n- 设行参与者的收益矩阵为 $A\\in\\mathbb{R}^{5\\times 5}$，其条目 $A_{ij}\\in\\{-1,0,1\\}$ 满足反对称性 $A=-A^{\\top}$ 且对所有 $i$ 都有 $A_{ii}=0$。优势关系由以下有序对集合给出：\n$$\n\\mathcal{W}=\\{(0,2),(0,3),(1,0),(1,4),(2,1),(2,3),(3,4),(3,1),(4,2),(4,0)\\},\n$$\n这意味着如果 $(i,j)\\in\\mathcal{W}$，则行动 $i$ 击败行动 $j$，因此 $A_{ij}=1$ 且 $A_{ji}=-1$；平局产生 $A_{ii}=0$ 的收益。\n- 确定性虚拟博弈在离散时期 $t=1,2,\\dots,T$ 中进行。每个参与者根据对手过去行动的经验频率形成信念，然后选择对该信念的纯最佳响应。设行参与者关于列参与者在时期 $t$ 的混合策略的信念为经验频率向量 $q_t\\in\\Delta^4$，该向量根据列参与者在时期 $\\{1,\\dots,t-1\\}$ 的行动计算得出。类似地，设列参与者关于行参与者在时期 $t$ 的混合策略的信念为 $p_t\\in\\Delta^4$，该向量根据行参与者在时期 $\\{1,\\dots,t-1\\}$ 的行动计算得出。行参与者针对信念 $q_t$ 的期望收益向量为 $v_t=A\\,q_t$，纯最佳响应是任何 $i\\in\\arg\\max_{k\\in\\mathcal{A}} (v_t)_k$。列参与者寻求最小化行参与者的收益；其在给定 $p_t$ 下的期望行收益向量为 $w_t=p_t^{\\top}A\\in\\mathbb{R}^5$，纯最佳响应是任何 $j\\in\\arg\\min_{\\ell\\in\\mathcal{A}} (w_t)_{\\ell}$。您必须实现确定性平局打破规则，即在最佳响应的 argmax 或 argmin 集合中始终选择最小的索引。\n- 设行参与者的信念轨迹为序列 $(b_t)_{t=1}^T$，其中 $b_t\\in\\Delta^4$ 是列参与者在时期 $\\{1,\\dots,t\\}$ 内行动的经验频率。这是一个位于 $5$-单纯形中的五维时间序列。\n\n需要实现的计算任务：\n1. 使用优势关系集合 $\\mathcal{W}$ 构建收益矩阵 $A$。\n2. 从时期 $t=1$ 指定的行参与者和列参与者的初始纯行动开始，模拟确定性虚拟博弈共 $T$ 个时期。对于每个时期 $t\\ge 2$，两个参与者同时选择他们的行动，作为对从对手在时期 $\\{1,\\dots,t-1\\}$ 的行动形成的信念的纯最佳响应，并采用指定的确定性平局打破规则。记录行参与者信念的完整序列 $(b_t)_{t=1}^T$。\n3. 定义均匀混合策略 $u=\\left(\\frac{1}{5},\\frac{1}{5},\\frac{1}{5},\\frac{1}{5},\\frac{1}{5}\\right)$。计算最终的欧几里得距离 $\\|b_T-u\\|_2$。\n4. 为了“可视化”，计算信念轨迹的二维主成分投影。设 $X\\in\\mathbb{R}^{T\\times 5}$ 是一个矩阵，其第 $t$ 行是 $b_t^{\\top}$。通过减去列均值来中心化 $X$，得到 $X_c$。使用奇异值分解 (SVD) $X_c=U\\Sigma V^{\\top}$，其中奇异值位于 $\\Sigma$ 的对角线上，将前两个主成分解释的方差比例定义为\n$$\n\\rho_2=\\frac{\\sigma_1^2+\\sigma_2^2}{\\sum_{k}\\sigma_k^2},\n$$\n并约定如果 $\\sum_k \\sigma_k^2=0$，则 $\\rho_2=0$。将时间 $t$ 的二维坐标定义为 $U_{[:,1:2]}\\,\\mathrm{diag}(\\sigma_1,\\sigma_2)$ 的第 $t$ 行，即在前两个主成分上的得分。\n5. 在指定的采样时间点，返回二维坐标作为“可供可视化”的数据。\n\n测试套件：\n为以下三个参数集实现上述过程。每个案例都使用提供的时期数、初始行动和用于投影输出的采样时间。行动索引遵循石头 $=0$、布 $=1$、剪刀 $=2$、蜥蜴 $=3$、斯波克 $=4$。\n- 案例 $1$ (通用“顺利路径”)：$T=400$，初始行行动 $=0$，初始列行动 $=1$，采样时间 $=\\{1,2,50,200,400\\}$。\n- 案例 $2$ (小样本边界情况)：$T=5$，初始行行动 $=0$，初始列行动 $=0$，采样时间 $=\\{1,2,5\\}$。\n- 案例 $3$ (更长的时间范围，不同的起点)：$T=1000$，初始行行动 $=4$，初始列行动 $=2$，采样时间 $=\\{1,10,100,500,1000\\}$。\n\n每个测试用例的必需输出：\n- 设 $d=\\|b_T-u\\|_2$ 为与均匀策略的最终欧几里得距离。\n- 设 $\\rho_2$ 为前两个主成分解释的方差比例，如上所定义。\n- 设 $C$ 为在指定采样时间的二维坐标列表，其顺序与采样时间的顺序相同，其中每个坐标都是一对 $[c_{t,1},c_{t,2}]$。\n\n最终输出格式：\n- 您的程序必须生成单行文本，其中包含一个由方括号括起来的、逗号分隔的三个测试用例结果的列表。每个测试用例结果必须是 $[d,\\rho_2,C]$ 形式的列表，其中 $C$ 是一个包含两个元素列表的列表。例如，一个语法上有效的总体输出形式为\n$[[d_1,\\rho_{2,1},C_1],[d_2,\\rho_{2,2},C_2],[d_3,\\rho_{2,3},C_3]]$。\n- 程序没有输入；测试套件是硬编码的。\n- 角度和物理单位不适用；所有输出都是无单位的实数。", "solution": "问题陈述已经过严格验证，并被确定为有效。它在博弈论和统计学领域具有科学依据，问题设定良好，具有唯一的确定性解，其定义和要求都是客观的。所有必要的数据和条件均已提供，不存在内部矛盾。因此，我们可以着手解决问题。\n\n该问题要求为一个双人零和的“石头-剪刀-布-蜥蜴-斯波克”游戏实现一个确定性虚拟博弈的模拟。该过程涉及构建游戏的收益矩阵，在指定数量的时期内模拟学习动态，并使用主成分分析（PCA）来分析其中一个参与者的最终信念轨迹。\n\n首先，我们构建行参与者的收益矩阵 $A \\in \\mathbb{R}^{5 \\times 5}$。行动集为 $\\mathcal{A} = \\{0, 1, 2, 3, 4\\}$。问题提供了胜负对的集合 $\\mathcal{W}$。对于每个对 $(i, j) \\in \\mathcal{W}$，行动 $i$ 击败行动 $j$，产生 $A_{ij} = 1$ 的收益。该博弈是零和的，这意味着反对称性 $A = -A^\\top$，因此 $A_{ji} = -A_{ij} = -1$。平局导致零收益，即 $A_{ii} = 0$。这些规则唯一地定义了矩阵 $A$。\n\n其次，我们模拟 $T$ 个时期的虚拟博弈动态。模拟从时期 $t=1$ 给定的初始行动 $i_1$ 和 $j_1$ 开始。对于随后的每个时期 $t \\in \\{2, \\dots, T\\}$，参与者形成信念并选择一个最佳响应。行参与者在时间 $t$ 的信念，表示为 $q_t \\in \\Delta^4$，是列参与者在所有先前时期 $\\{1, \\dots, t-1\\}$ 中行动的经验频率。行参与者选择行动 $k \\in \\mathcal{A}$ 的期望收益是向量 $v_t = A q_t$ 的第 $k$ 个分量。然后，行参与者选择一个行动 $i_t$ 来最大化此收益：\n$$\ni_t \\in \\arg\\max_{k \\in \\mathcal{A}} (A q_t)_k\n$$\n同样，列参与者根据行参与者的过去行动形成信念 $p_t \\in \\Delta^4$。列参与者旨在最小化行参与者的收益。列参与者选择每个行动 $\\ell \\in \\mathcal{A}$ 的期望收益由向量 $w_t = p_t^\\top A$ 给出。列参与者选择一个行动 $j_t$ 来最小化此值：\n$$\nj_t \\in \\arg\\min_{\\ell \\in \\mathcal{A}} (p_t^\\top A)_\\ell\n$$\n实施了严格的平局打破规则：在有多个最佳响应的情况下，选择索引最小的行动。这使得模拟过程完全是确定性的。\n\n第三，我们分析行参与者的信念轨迹 $(b_t)_{t=1}^T$。请注意，用于分析的信念 $b_t$ 被定义为列参与者在时期 $\\{1, \\dots, t\\}$ 内行动的经验频率，这包括了时间 $t$ 的行动。这与用于在时间 $t$ 进行决策的信念 $q_t$ 是不同的。最终的信念偏差通过欧几里得距离 $d = \\|b_T - u\\|_2$ 来衡量，其中 $u = (\\frac{1}{5}, \\dots, \\frac{1}{5})^\\top$ 是均匀混合策略。\n\n第四，我们对信念轨迹进行主成分分析（PCA），以获得一个二维表示。该轨迹被组织成一个数据矩阵 $X \\in \\mathbb{R}^{T \\times 5}$，其中第 $t$ 行为 $b_t^\\top$。通过从每行中减去列均值向量来对数据进行中心化，得到 $X_c$。中心化矩阵的奇异值分解（SVD），$X_c = U \\Sigma V^\\top$，提供了主成分。$\\Sigma$ 对角线上的奇异值 $\\sigma_k$ 用于计算前两个主成分解释的方差比例：\n$$\n\\rho_2 = \\frac{\\sigma_1^2 + \\sigma_2^2}{\\sum_{k=1}^{\\text{rank}(X_c)} \\sigma_k^2}\n$$\n如果总方差 $\\sum_k \\sigma_k^2 = 0$，我们设定 $\\rho_2=0$。时间 $t$ 的二维坐标，也称为得分，由矩阵乘积 $U_{[:,1:2]} \\mathrm{diag}(\\sigma_1, \\sigma_2)$ 的第 $t$ 行给出。这表示中心化数据在前两个主成分轴上的投影。\n\n最后，对于每个测试用例，我们计算距离 $d$、方差比例 $\\rho_2$ 以及在指定采样时间的二维坐标列表 $C$。然后将这些结果格式化为所需的输出结构。", "answer": "```python\nimport numpy as np\n\ndef run_case(T, initial_row_action, initial_col_action, sample_times):\n    \"\"\"\n    Runs a single test case for the fictitious play simulation.\n    \"\"\"\n    # Task 1: Build the payoff matrix A\n    A = np.zeros((5, 5), dtype=np.float64)\n    dominance_pairs = [\n        (0, 2), (0, 3), (1, 0), (1, 4), (2, 1),\n        (2, 3), (3, 4), (3, 1), (4, 2), (4, 0)\n    ]\n    for i, j in dominance_pairs:\n        A[i, j] = 1.0\n        A[j, i] = -1.0\n\n    # Task 2: Simulate deterministic fictitious play\n    row_actions = [initial_row_action]\n    col_actions = [initial_col_action]\n\n    row_counts = np.zeros(5, dtype=np.float64)\n    row_counts[initial_row_action] = 1.0\n    col_counts = np.zeros(5, dtype=np.float64)\n    col_counts[initial_col_action] = 1.0\n\n    for t in range(2, T + 1):\n        # Beliefs for period t are based on actions from 1 to t-1\n        q_t = col_counts / (t - 1)\n        p_t = row_counts / (t - 1)\n\n        # Row player's best response (maximizes payoff)\n        row_expected_payoffs = A @ q_t\n        i_t = np.argmax(row_expected_payoffs)\n\n        # Column player's best response (minimizes row's payoff)\n        col_expected_payoffs = p_t @ A\n        j_t = np.argmin(col_expected_payoffs)\n\n        # Record actions and update counts\n        row_actions.append(i_t)\n        col_actions.append(j_t)\n        row_counts[i_t] += 1.0\n        col_counts[j_t] += 1.0\n\n    # Compute row player's belief trajectory (b_t)\n    # b_t is the empirical frequency of column actions up to time t.\n    X = np.zeros((T, 5), dtype=np.float64)\n    temp_col_counts = np.zeros(5, dtype=np.float64)\n    for t in range(T):\n        action_idx = col_actions[t]\n        temp_col_counts[action_idx] += 1.0\n        X[t, :] = temp_col_counts / (t + 1)\n    \n    # Task 3: Compute final Euclidean distance from uniform strategy\n    b_T = X[-1, :]\n    u = np.full(5, 1.0/5.0)\n    d = np.linalg.norm(b_T - u)\n\n    # Task 4: Perform Principal Component Analysis\n    X_c = X - X.mean(axis=0)\n\n    # Use SVD for PCA. full_matrices=False is efficient.\n    try:\n        U, S, Vh = np.linalg.svd(X_c, full_matrices=False)\n    except np.linalg.LinAlgError: # Safety for unusual matrices, though not expected here\n        U, S, Vh = np.zeros((T,1)), np.array([]), np.zeros((1,5))\n\n    # Calculate fraction of variance explained by first two PCs\n    s2 = S**2\n    total_variance = np.sum(s2)\n    rho_2 = 0.0\n    if total_variance > 1e-15:\n        rho_2 = np.sum(s2[:2]) / total_variance\n\n    # Calculate 2D coordinates (scores on first two PCs)\n    scores = np.zeros((T, 2))\n    num_components = min(2, S.shape[0])\n    if num_components > 0:\n        scores[:, :num_components] = U[:, :num_components] * S[:num_components]\n\n    # Task 5: Return coordinates at specified sample times\n    # Sample times are 1-indexed, so we access index t-1\n    C = [scores[t - 1].tolist() for t in sample_times]\n\n    return [d, rho_2, C]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and generate the final output.\n    \"\"\"\n    # Test suite from the problem statement\n    test_cases = [\n        {'T': 400, 'initial_row_action': 0, 'initial_col_action': 1, 'sample_times': [1, 2, 50, 200, 400]},\n        {'T': 5, 'initial_row_action': 0, 'initial_col_action': 0, 'sample_times': [1, 2, 5]},\n        {'T': 1000, 'initial_row_action': 4, 'initial_col_action': 2, 'sample_times': [1, 10, 100, 500, 1000]}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_case(\n            case['T'],\n            case['initial_row_action'],\n            case['initial_col_action'],\n            case['sample_times']\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The default str() representation includes spaces, which we remove.\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"}, {"introduction": "既然我们已经有了一个可运行的实现，那么虚拟对局算法的效果究竟如何？本练习将通过将其与一个强大的替代方案——Hedge算法——进行基准比较来回答这个问题。通过模拟这两种算法对抗一个非平稳的对手，你将探索“遗憾”（regret）这一关键概念，并发现这些学习规则提供的不同性能保证，从而更深入地理解在何种情境下以及为何一种算法可能优于另一种。[@problem_id:2405816]", "id": "2405816", "problem": "设有一个标准零和规范形式的石头–剪刀–布博弈，行参与者的支付矩阵为\n$$\nA \\;=\\;\n\\begin{pmatrix}\n0 & -1 & 1 \\\\\n1 & 0 & -1 \\\\\n-1 & 1 & 0\n\\end{pmatrix},\n$$\n其中行动索引 $1$、$2$ 和 $3$ 分别对应于石头、布和剪刀。行参与者（学习者）面对一个确定性的、非平稳的对手（列参与者），该对手在每一轮 $t\\in\\{1,\\dots,T\\}$ 选择一个纯行动 $j(t)\\in\\{1,2,3\\}$。交互持续 $T$ 轮。\n\n需要对学习者的两种学习规则进行评估：\n\n$1.$ 虚构博弈 (Fictitious play)。对于 $t=1$，选择行动 $i(1)=1$。对于 $t\\ge 2$，令 $\\hat{q}(t-1)\\in\\mathbb{R}^3$ 为截至第 $t-1$ 期对手过去行动的经验频率向量，即对每个 $k\\in\\{1,2,3\\}$，有 $\\hat{q}_k(t-1)=\\frac{1}{t-1}\\sum_{\\tau=1}^{t-1}\\mathbf{1}\\{j(\\tau)=k\\}$。选择一个纯最佳响应 $i(t)\\in\\arg\\max_{i\\in\\{1,2,3\\}} \\left(A\\,\\hat{q}(t-1)\\right)_i$，若有多个最优解，则选择最小的索引。在第 $t$ 轮的实现支付为 $r^{\\mathrm{FP}}(t)=A_{i(t),\\,j(t)}$。定义经验混合策略 $\\bar{p}^{\\mathrm{FP}}\\in\\mathbb{R}^3$ 为 $\\bar{p}^{\\mathrm{FP}}_i=\\frac{1}{T}\\sum_{t=1}^T \\mathbf{1}\\{i(t)=i\\}$。\n\n$2.$ Hedge（指数权重）算法，具有完全信息反馈。初始化权重 $w_i(1)=1$ (对每个 $i\\in\\{1,2,3\\}$)。在每一轮 $t\\in\\{1,\\dots,T\\}$，构建混合策略 $p_i(t)=\\frac{w_i(t)}{\\sum_{k=1}^3 w_k(t)}$。令 $g(t)\\in\\mathbb{R}^3$ 为针对对手已实现的纯行动 $j(t)$ 的反事实支付向量，即 $g_i(t)=A_{i,\\,j(t)}$。期望支付为 $r^{\\mathrm{H}}(t)=\\sum_{i=1}^3 p_i(t)\\,g_i(t)$。通过 $w_i(t+1)=w_i(t)\\,\\exp\\!\\left(\\eta\\,g_i(t)\\right)$ 更新权重，其中学习率 $\\eta=\\sqrt{\\frac{2\\ln n}{T}}$ 且 $n=3$。定义时间平均策略 $\\bar{p}^{\\mathrm{H}}\\in\\mathbb{R}^3$ 为 $\\bar{p}^{\\mathrm{H}}=\\frac{1}{T}\\sum_{t=1}^T p(t)$。\n\n对于两种学习规则，在期限 $T$ 计算两个汇总统计量：\n\n$1.$ 平均外部遗憾 (Average external regret)，\n$$\n\\frac{R_T}{T}\n\\;=\\;\n\\frac{1}{T}\\left(\n\\max_{i\\in\\{1,2,3\\}} \\sum_{t=1}^T g_i(t)\n\\;-\\;\n\\sum_{t=1}^T r(t)\n\\right),\n$$\n其中，对于虚构博弈，$r(t)=r^{\\mathrm{FP}}(t)$；对于 Hedge 算法，$r(t)=r^{\\mathrm{H}}(t)$。在两种情况下，$g_i(t)=A_{i,\\,j(t)}$。\n\n$2.$ 学习者的时间平均策略与唯一的混合策略纳什均衡 $u^\\star=(\\frac{1}{3},\\frac{1}{3},\\frac{1}{3})$ 之间的 $\\ell_1$ 距离，即\n$$\nD(\\bar{p})\n\\;=\\;\n\\sum_{i=1}^3 \\left| \\bar{p}_i - \\tfrac{1}{3} \\right|.\n$$\n\n测试套件。您的程序必须在以下四种对手行动序列上评估这两种学习规则，每种序列都由一个期限 $T$ 和一个分段常数纯行动序列指定：\n\n$1.$ 情况 A（理想路径）：$T=90$。对手先玩 30 轮石头，然后玩 30 轮布，再玩 30 轮剪刀，即当 $t\\in\\{1,\\dots,30\\}$ 时 $j(t)=1$，当 $t\\in\\{31,\\dots,60\\}$ 时 $j(t)=2$，当 $t\\in\\{61,\\dots,90\\}$ 时 $j(t)=3$。\n\n$2.$ 情况 B（边界情况）：$T=3$。对手在 $t=1$ 时玩石头，在 $t=2$ 时玩布，在 $t=3$ 时玩剪刀，即 $j(1)=1$, $j(2)=2$, $j(3)=3$。\n\n$3.$ 情况 C（非平稳边缘情况）：$T=60$。对手先玩 20 轮石头，然后玩 10 轮布，再玩 20 轮剪刀，最后玩 10 轮布，即当 $t\\in\\{1,\\dots,20\\}$ 时 $j(t)=1$，当 $t\\in\\{21,\\dots,30\\}$ 时 $j(t)=2$，当 $t\\in\\{31,\\dots,50\\}$ 时 $j(t)=3$，当 $t\\in\\{51,\\dots,60\\}$ 时 $j(t)=2$。\n\n$4.$ 情况 D（平稳边缘情况）：$T=60$。对手在所有轮次都玩石头，即对所有 $t\\in\\{1,\\dots,60\\}$，$j(t)=1$。\n\n所需输出。对于每种情况，生成一个包含四个实数的列表\n$$\n\\big[\\, \\tfrac{R_T^{\\mathrm{H}}}{T},\\;\\tfrac{R_T^{\\mathrm{FP}}}{T},\\; D(\\bar{p}^{\\mathrm{H}}),\\; D(\\bar{p}^{\\mathrm{FP}})\\,\\big],\n$$\n其中上标 $\\mathrm{H}$ 和 $\\mathrm{FP}$ 分别表示 Hedge 和虚构博弈。每个数字必须四舍五入到 6 位小数。您的程序应生成单行输出，其中包含这些四元素列表的逗号分隔列表形式的结果，并用方括号括起来，不含任何附加文本。例如，\n$$\n[\\,[x_1,x_2,x_3,x_4],[y_1,y_2,y_3,y_4],\\dots]\\,,\n$$\n其中每个 $x_k$ 和 $y_k$ 都是四舍五入到 6 位小数的小数。此问题不涉及物理单位，所有角度均无关紧要。不得有输入；仅需为指定的测试套件计算输出。", "solution": "该问题是有效的。它提出了一个在博弈论和在线学习领域（计算经济学的一个子领域）中定义明确的计算任务。所有参数、算法和评估指标都得到了足够精确的规定，从而可以得到一个唯一且可验证的解。该问题具有科学依据，使用了标准模型（石头–剪刀–布）、算法（虚构博弈、Hedge）和性能指标（遗憾、到纳什均衡的距离）。\n\n任务是为一个在石头–剪刀–布博弈中的行参与者模拟两种学习算法——虚构博弈（Fictitious Play）和 Hedge——对抗一个确定性的、非平稳的列参与者。我们已知行参与者的支付矩阵 $A$：\n$$\nA = \\begin{pmatrix} 0 & -1 & 1 \\\\ 1 & 0 & -1 \\\\ -1 & 1 & 0 \\end{pmatrix}\n$$\n其中行和列分别对应于行动石头（索引1）、布（索引2）和剪刀（索引3）。模拟需要针对四种不同的预定义对手行动序列 $j(t)$ 运行 $T$ 轮。\n\n解决方案涉及实现这两种算法和指定的汇总统计量。模拟以迭代方式进行，从 $t = 1$ 到 $T$。\n\n**1. 虚构博弈（Fictitious Play, FP）算法**\n\n虚构博弈是一种迭代学习规则，其中参与者假设其对手正在玩一个平稳的混合策略，该策略通过对手过去行动的经验频率来估计。然后，参与者选择对该估计策略的纯最佳响应。\n\n- **初始化 ($t=1$):** 学习者被规定玩石头，因此 $i(1)=1$。\n- **迭代 ($t \\ge 2$):**\n    1.  学习者计算截至第 $t-1$ 轮的对手行动的经验频率。这构成了信念向量 $\\hat{q}(t-1) \\in \\mathbb{R}^3$，其中每个分量 $\\hat{q}_k(t-1)$ 由 $\\frac{1}{t-1}\\sum_{\\tau=1}^{t-1}\\mathbf{1}\\{j(\\tau)=k\\}$ 给出。\n    2.  学习者计算其每个纯行动针对此信念的期望支付。行动 $i$ 的期望支付为 $(A\\hat{q}(t-1))_i$。\n    3.  学习者选择使该期望支付最大化的行动 $i(t)$：$i(t) \\in \\arg\\max_{i\\in\\{1,2,3\\}} (A\\hat{q}(t-1))_i$。若有多个最优解，则通过选择索引最小的行动来解决。\n- **支付：** 第 $t$ 轮的实现支付为 $r^{\\mathrm{FP}}(t) = A_{i(t), j(t)}$。\n\n**2. Hedge（指数权重）算法**\n\nHedge 是一种用于在线决策的算法，它在一组专家（在此情况下为纯行动）上维护一个分布。它根据每个专家的表现更新此分布。\n\n- **初始化 ($t=1$):** 学习者为每个行动赋予相等的初始权重：$w_i(1) = 1$ (对 $i \\in \\{1, 2, 3\\}$)。学习率 $\\eta$ 设为 $\\sqrt{\\frac{2\\ln n}{T}}$，其中 $n=3$。\n- **迭代 ($t = 1, \\dots, T$):**\n    1.  学习者通过归一化当前权重来形成混合策略 $p(t)$：$p_i(t) = \\frac{w_i(t)}{\\sum_{k=1}^3 w_k(t)}$。\n    2.  在对手玩出行动 $j(t)$ 后，学习者观察到自己所有行动的反事实支付。这由向量 $g(t) \\in \\mathbb{R}^3$ 给出，其中 $g_i(t) = A_{i, j(t)}$。该向量对应于矩阵 $A$ 的第 $j(t)$ 列。\n    3.  下一轮的权重进行乘法更新：$w_i(t+1) = w_i(t) \\exp(\\eta g_i(t))$。本可以产生更高支付的行动会获得更大的权重增长。\n- **支付：** 算法在第 $t$ 轮的表现由其在混合策略 $p(t)$下的期望支付来衡量，即 $r^{\\mathrm{H}}(t) = \\sum_{i=1}^3 p_i(t) g_i(t)$。\n\n**3. 汇总统计量**\n\n对于每种算法和每个对手行动序列，在期限 $T$ 计算两个指标：\n\n- **平均外部遗憾：** 该指标衡量算法的累积支付与事后最佳单一固定行动的累积支付之间的平均每轮差异。其计算公式为：\n$$\n\\frac{R_T}{T} = \\frac{1}{T}\\left( \\max_{i\\in\\{1,2,3\\}} \\sum_{t=1}^T g_i(t) - \\sum_{t=1}^T r(t) \\right)\n$$\n对于虚构博弈，$r(t) = r^{\\mathrm{FP}}(t)$；对于 Hedge 算法，$r(t) = r^{\\mathrm{H}}(t)$。项 $\\sum_{t=1}^T g_i(t)$ 表示通过持续玩行动 $i$ 本可以累积的总支付。\n\n- **到纳什均衡的 $\\ell_1$ 距离：** 该指标量化了学习者的时间平均策略与石头–剪刀–布博弈的唯一混合策略纳什均衡 $u^\\star = (\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$ 的差距。该距离为：\n$$\nD(\\bar{p}) = \\sum_{i=1}^3 \\left| \\bar{p}_i - \\frac{1}{3} \\right|\n$$\n对于虚构博弈，时间平均策略 $\\bar{p}^{\\mathrm{FP}}$ 是所玩行动的经验频率：$\\bar{p}^{\\mathrm{FP}}_i = \\frac{1}{T}\\sum_{t=1}^T \\mathbf{1}\\{i(t)=i\\}$。对于 Hedge 算法，它是每轮使用的混合策略的平均值：$\\bar{p}^{\\mathrm{H}} = \\frac{1}{T}\\sum_{t=1}^T p(t)$。\n\n实现将为四个指定的测试案例模拟这些过程，并为每个案例计算和收集所需的四个统计数据（$R_T^{\\mathrm{H}}/T$、$R_T^{\\mathrm{FP}}/T$、$D(\\bar{p}^{\\mathrm{H}})$、$D(\\bar{p}^{\\mathrm{FP}})$）。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run simulations for all test cases and print results.\n    \"\"\"\n    # Payoff matrix for the row player (learner).\n    # Actions: 0=Rock, 1=Paper, 2=Scissors\n    A = np.array([[0, -1, 1],\n                  [1, 0, -1],\n                  [-1, 1, 0]])\n    n_actions = 3\n    u_star = np.full(n_actions, 1.0 / n_actions)\n\n    def generate_schedule(case, T):\n        \"\"\"Generates the opponent's action schedule for a given case.\"\"\"\n        # Note: actions are 1-indexed in problem, 0-indexed in code.\n        # This function returns 0-indexed actions.\n        if case == 'A':  # T=90\n            return np.concatenate([np.full(30, 0), np.full(30, 1), np.full(30, 2)])\n        elif case == 'B':  # T=3\n            return np.array([0, 1, 2])\n        elif case == 'C':  # T=60\n            return np.concatenate([np.full(20, 0), np.full(10, 1), np.full(20, 2), np.full(10, 1)])\n        elif case == 'D':  # T=60\n            return np.full(60, 0)\n        return None\n\n    def run_fictitious_play(T, opponent_schedule, A):\n        \"\"\"Simulates the Fictitious Play algorithm.\"\"\"\n        opponent_action_counts = np.zeros(n_actions)\n        learner_action_counts = np.zeros(n_actions)\n        total_payoff_fp = 0.0\n\n        for t in range(T):\n            opponent_action_idx = opponent_schedule[t]\n\n            if t == 0:\n                learner_action_idx = 0  # Rule: i(1)=1 (Rock)\n            else:\n                # Belief is the empirical frequency of opponent's past actions\n                opponent_empirical_freq = opponent_action_counts / t\n                expected_payoffs = A @ opponent_empirical_freq\n                # Best response, ties broken by smallest index (np.argmax default)\n                learner_action_idx = np.argmax(expected_payoffs)\n\n            # Record actions and payoffs\n            learner_action_counts[learner_action_idx] += 1\n            total_payoff_fp += A[learner_action_idx, opponent_action_idx]\n            opponent_action_counts[opponent_action_idx] += 1\n\n        # Calculate summary statistics\n        p_bar_fp = learner_action_counts / T\n        d_fp = np.sum(np.abs(p_bar_fp - u_star))\n\n        return total_payoff_fp, d_fp\n\n    def run_hedge(T, opponent_schedule, A):\n        \"\"\"Simulates the Hedge (Exponential Weights) algorithm.\"\"\"\n        eta = np.sqrt(2 * np.log(n_actions) / T)\n        weights = np.ones(n_actions)\n        \n        sum_p_t = np.zeros(n_actions)\n        total_expected_payoff_h = 0.0\n\n        for t in range(T):\n            opponent_action_idx = opponent_schedule[t]\n            \n            # Form mixed strategy\n            p_t = weights / np.sum(weights)\n            sum_p_t += p_t\n            \n            # Counterfactual payoffs\n            g_t = A[:, opponent_action_idx]\n            \n            # Expected payoff for this round\n            total_expected_payoff_h += p_t @ g_t\n            \n            # Update weights\n            weights *= np.exp(eta * g_t)\n            \n        # Calculate summary statistics\n        p_bar_h = sum_p_t / T\n        d_h = np.sum(np.abs(p_bar_h - u_star))\n\n        return total_expected_payoff_h, d_h\n\n    def calculate_regret(T, total_payoff, opponent_schedule, A):\n        \"\"\"Calculates the average external regret.\"\"\"\n        # Payoff of best fixed action in hindsight\n        total_g = np.zeros(n_actions)\n        for t in range(T):\n            opponent_action_idx = opponent_schedule[t]\n            total_g += A[:, opponent_action_idx]\n        \n        max_fixed_payoff = np.max(total_g)\n        avg_regret = (max_fixed_payoff - total_payoff) / T\n        return avg_regret\n\n    test_cases = [\n        ('A', 90),\n        ('B', 3),\n        ('C', 60),\n        ('D', 60)\n    ]\n\n    all_results = []\n\n    for case_label, T in test_cases:\n        opponent_schedule = generate_schedule(case_label, T)\n\n        # Run simulations\n        total_payoff_fp, d_fp = run_fictitious_play(T, opponent_schedule, A)\n        total_payoff_h, d_h = run_hedge(T, opponent_schedule, A)\n        \n        # Calculate regrets\n        avg_regret_fp = calculate_regret(T, total_payoff_fp, opponent_schedule, A)\n        avg_regret_h = calculate_regret(T, total_payoff_h, opponent_schedule, A)\n        \n        # Store results in the specified order\n        results = [avg_regret_h, avg_regret_fp, d_h, d_fp]\n        all_results.append(results)\n    \n    # Format the output string as required\n    formatted_cases = []\n    for case_res in all_results:\n        formatted_nums = [f\"{num:.6f}\" for num in case_res]\n        formatted_cases.append(f\"[{','.join(formatted_nums)}]\")\n    \n    final_output = f\"[{','.join(formatted_cases)}]\"\n    print(final_output)\n\nsolve()\n\n```"}, {"introduction": "理解对手的学习过程为策略性操控创造了机会。在最后一个练习中，我们将转换视角：你不再是学习者，而是那个*知道*对手遵循虚拟对局规则的“高明”参与者。你的任务是设计一个最优策略来利用这一信息，这需要你向前看并解决一个动态规划问题，从而展示了在学习环境中战略思维的巅峰。[@problem_id:2405888]", "id": "2405888", "problem": "考虑一个重复的双参与者、双行动策略型博弈，该博弈在由 $t \\in \\{1,2,\\dots,T\\}$ 索引的 $T$ 个离散时期的有限期界内进行。参与者1（老练参与者）从 $\\{0,1\\}$ 中选择行动，并寻求最大化整个时期内的阶段支付总和。参与者2（对手）从 $\\{0,1\\}$ 中选择行动，并遵循经典的虚拟博弈规则：在每个时期 $t$ 开始前，参与者2根据一个初始先验和参与者1过去行动的经验频率，形成关于参与者1选择行动0的概率的信念，然后对该信念进行最佳响应，以最大化参与者2自身的期望阶段支付。参与者1了解参与者2的学习规则和支付矩阵，并通过预测当前行动对参与者2未来信念的影响来选择行动，从而对该学习过程做出最佳响应。\n\n单次阶段支付如下。设 $U_1$ 为参与者1的 $2 \\times 2$ 支付矩阵， $U_2$ 为参与者2的 $2 \\times 2$ 支付矩阵，其中行索引对应参与者1的行动，列索引对应参与者2的行动。如果在某个时期，参与者1选择行动 $a_1 \\in \\{0,1\\}$，参与者2选择行动 $a_2 \\in \\{0,1\\}$，则参与者1获得支付 $U_1[a_1,a_2]$，参与者2获得支付 $U_2[a_1,a_2]$。\n\n在时期 $t$ 开始时，参与者2的虚拟博弈信念由下式给出：\n$$\np_t \\equiv \\frac{\\alpha_0 + n_0(t-1)}{\\alpha_0 + \\alpha_1 + (t-1)},\n$$\n其中 $\\alpha_0,\\alpha_1 \\in \\mathbb{N}_0$ 分别是行动0和行动1的固定先验伪计数，$n_0(t-1)$ 是参与者1在时期1到 $t-1$ 期间选择行动0的总次数。假设 $\\alpha_0 + \\alpha_1 \\ge 1$，以确保在 $t=1$ 时分母为严格正数。在每个时期 $t$，参与者2选择一个最佳响应行动\n$$\na_2(t) \\in \\arg\\max_{b \\in \\{0,1\\}} \\left\\{ \\mathbb{E}[U_2[a_1,b] \\mid a_1 \\sim \\text{Bernoulli}(p_t)] \\right\\} \n= \\arg\\max_{b \\in \\{0,1\\}} \\left\\{ p_t \\cdot U_2[0,b] + (1-p_t)\\cdot U_2[1,b] \\right\\}.\n$$\n如果 argmax 不是唯一的，参与者2通过选择较小的行动索引来打破平局，即在平局情况下选择 $a_2(t)=0$。\n\n在每个时期 $t$，事件的顺序如下：\n- 根据先验和参与者1的过去行动形成信念 $p_t$。\n- 参与者2如上文定义，选择对 $p_t$ 的最佳响应 $a_2(t)$。\n- 参与者1选择一个纯行动 $a_1(t) \\in \\{0,1\\}$。\n- 实现支付 $U_1[a_1(t),a_2(t)]$ 和 $U_2[a_1(t),a_2(t)]$。\n- 计数 $n_0(t)$ 通过 $n_0(t)=n_0(t-1)+\\mathbf{1}\\{a_1(t)=0\\}$ 进行更新。\n\n参与者1观察整个历史并自适应地选择行动，以最大化未贴现的总支付\n$$\n\\sum_{t=1}^{T} U_1[a_1(t),a_2(t)],\n$$\n同时预测参与者2的信念更新和最佳响应。\n\n你的任务是编写一个程序，针对下方的每个测试用例，计算在上述动态下，参与者1在整个期界 $T$ 内可获得的最大总支付。假设参与者1在每个时期都可以承诺执行任一纯行动，同时完全预测到参与者2的虚拟博弈行为和打破平局的规则。\n\n问题不涉及物理量或角度，因此不需要单位。所有输出必须是精确的整数。\n\n输入不会提供给你的程序；相反，你的程序必须在内部使用以下测试套件，其中每个用例指定了 $U_1$、$U_2$、期界 $T$ 和先验 $(\\alpha_0,\\alpha_1)$：\n\n- 测试用例1（操纵有利）：\n  - $U_1 = \\begin{bmatrix} 2 & 0 \\\\ 1 & 1 \\end{bmatrix}$，\n  - $U_2 = \\begin{bmatrix} 3 & 0 \\\\ 0 & 2 \\end{bmatrix}$，\n  - $T = 5$，\n  - $(\\alpha_0,\\alpha_1) = (0,1)$。\n\n- 测试用例2（零和匹配硬币）：\n  - $U_1 = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}$，\n  - $U_2 = \\begin{bmatrix} -1 & 1 \\\\ 1 & -1 \\end{bmatrix}$，\n  - $T = 6$，\n  - $(\\alpha_0,\\alpha_1) = (1,1)$。\n\n- 测试用例3（开局时边界平局，协调博弈）：\n  - $U_1 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix}$，\n  - $U_2 = \\begin{bmatrix} 2 & 0 \\\\ 0 & 2 \\end{bmatrix}$，\n  - $T = 3$，\n  - $(\\alpha_0,\\alpha_1) = (1,1)$。\n\n- 测试用例4（阈值恰好在先验处）：\n  - $U_1 = \\begin{bmatrix} 2 & 0 \\\\ 1 & 1 \\end{bmatrix}$，\n  - $U_2 = \\begin{bmatrix} 3 & 0 \\\\ 0 & 2 \\end{bmatrix}$，\n  - $T = 2$，\n  - $(\\alpha_0,\\alpha_1) = (2,3)$。\n\n你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，结果顺序与测试用例的顺序相同，例如 $[r_1,r_2,r_3,r_4]$，其中每个 $r_i$ 是测试用例 $i$ 的最大总支付（一个整数）。", "solution": "该问题已经过验证，被认定为有效。这是一个计算博弈论中的适定问题，具体模拟了一个老练参与者针对使用虚拟博弈学习规则的对手所采取的最优策略。该问题的设定在数学上是一致的、自洽的，并且没有科学或逻辑上的缺陷。该问题可以通过动态规划算法求解。\n\n该问题要求为参与者1找到最优策略。参与者1是一位老练的代理人，在有限的 $T$ 个时期内面对一个行为可预测的对手（参与者2）。参与者1的行动会影响参与者2的未来信念，并因此影响其未来行动。这是一个确定性环境下的序贯决策问题，因为参与者2的行为是确定性的，这使其适合通过动态规划和逆向归纳法来求解。\n\n我们将系统在时期 $t \\in \\{1, 2, \\dots, T\\}$ 开始时的状态定义为参与者1行动的历史。历史中唯一影响未来动态的因素是参与者1选择行动0的次数。因此，一个充分的状态表示是数对 $(t, k)$，其中 $t$ 是当前时期编号，$k = n_0(t-1)$ 是参与者1在时期1到 $t-1$ 中选择行动0的次数。参与者1选择行动1的次数是 $(t-1)-k$。\n\n设 $V(t, k)$ 表示在系统处于状态 $(t, k)$ 的情况下，参与者1从时期 $t$ 到期界 $T$ 结束所能获得的最大总支付。我们的目标是计算 $V(1, 0)$，因为在博弈开始时（$t=1$），尚未采取任何行动（$k=0$）。\n\n动态规划的求解过程通过逆向归纳法进行，从最后一个时期开始。\n\n基准情形是期界结束后的时期。在 $t=T+1$ 时，博弈结束，不能再累积支付。\n$$V(T+1, k) = 0 \\quad \\text{for all valid } k \\in \\{0, 1, \\dots, T\\}.$$\n\n对于任何时期 $t \\in \\{T, T-1, \\dots, 1\\}$ 和任何有效状态 $(t, k)$（其中 $k \\in \\{0, 1, \\dots, t-1\\}$），我们可以写出 $V(t, k)$ 的贝尔曼方程。首先，我们确定参与者2的行动 $a_2(t)$，它是状态 $(t, k)$ 的一个确定性函数。参与者2关于参与者1选择行动0的信念为：\n$$p_t = \\frac{\\alpha_0 + k}{\\alpha_0 + \\alpha_1 + t - 1}.$$\n参与者2选择 $a_2(t) \\in \\{0,1\\}$ 来最大化其期望支付。如果选择0的期望支付大于或等于（由于打破平局规则）选择1的期望支付，他们将选择行动0：\n$$ p_t \\cdot U_2[0,0] + (1-p_t)\\cdot U_2[1,0] \\ge p_t \\cdot U_2[0,1] + (1-p_t)\\cdot U_2[1,1]. $$\n为了避免浮点数运算和潜在的精度误差，我们可以用整数来表示这个不等式。设 $N_t = \\alpha_0 + k$ 且 $D_t = \\alpha_0 + \\alpha_1 + t - 1$，因此 $p_t = N_t / D_t$。由于 $D_t > 0$，我们可以在不等式两边乘以 $D_t$ 并重新整理得到：\n$$ N_t \\cdot (U_2[0,0] - U_2[1,0] - U_2[0,1] + U_2[1,1]) \\ge D_t \\cdot (U_2[1,1] - U_2[1,0]). $$\n这个比较只涉及整数运算，并完全确定了任何状态 $(t, k)$ 下的 $a_2(t)$。\n\n给定 $a_2(t)$，参与者1选择 $a_1(t) \\in \\{0,1\\}$ 以最大化当前时期的支付与后续状态价值之和。贝尔曼方程为：\n$$ V(t, k) = \\max \\left\\{\n    \\underbrace{U_1[0, a_2(t)] + V(t+1, k+1)}_{\\text{若 } a_1(t)=0 \\text{ 的价值}},\n    \\underbrace{U_1[1, a_2(t)] + V(t+1, k)}_{\\text{若 } a_1(t)=1 \\text{ 的价值}}\n\\right\\}. $$\n这里，如果参与者1选择行动0，则下一时期 $t+1$ 的0计数变为 $k+1$。如果参与者1选择行动1，则计数保持为 $k$。\n\n该算法包括通过从 $T$ 向下迭代到1来计算所有相关状态的 $V(t, k)$，对于每个 $t$，从0迭代到 $t-1$来计算 $k$。可以使用一个二维数组来存储计算出的 $V(t, k)$ 值，这种技术被称为记忆化。最终答案是值 $V(1, 0)$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the repeated game problem for a sophisticated player against a \n    fictitious play opponent for a given set of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: manipulation beneficial\n        {\n            \"U1\": np.array([[2, 0], [1, 1]], dtype=int),\n            \"U2\": np.array([[3, 0], [0, 2]], dtype=int),\n            \"T\": 5,\n            \"prior\": (0, 1),\n        },\n        # Case 2: zero-sum matching pennies\n        {\n            \"U1\": np.array([[1, -1], [-1, 1]], dtype=int),\n            \"U2\": np.array([[-1, 1], [1, -1]], dtype=int),\n            \"T\": 6,\n            \"prior\": (1, 1),\n        },\n        # Case 3: boundary tie at the start, coordination\n        {\n            \"U1\": np.array([[1, 0], [0, 0]], dtype=int),\n            \"U2\": np.array([[2, 0], [0, 2]], dtype=int),\n            \"T\": 3,\n            \"prior\": (1, 1),\n        },\n        # Case 4: threshold exactly at the prior\n        {\n            \"U1\": np.array([[2, 0], [1, 1]], dtype=int),\n            \"U2\": np.array([[3, 0], [0, 2]], dtype=int),\n            \"T\": 2,\n            \"prior\": (2, 3),\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        U1 = case[\"U1\"]\n        U2 = case[\"U2\"]\n        T = case[\"T\"]\n        alpha0, alpha1 = case[\"prior\"]\n\n        # V[t][k] stores the max payoff from period t to T, given that Player 1\n        # has played action 0 a total of k times up to period t-1.\n        # Dimensions are (T+2) x (T+1) to handle t=T+1 and k=T.\n        V = np.zeros((T + 2, T + 1), dtype=int)\n\n        # Precompute constants for Player 2's best response to avoid redundant calculations.\n        # P2 plays 0 if: N * D_U2 >= D * C_U2\n        D_U2 = (U2[0, 0] - U2[1, 0]) - (U2[0, 1] - U2[1, 1])\n        C_U2 = U2[1, 1] - U2[1, 0]\n\n        # Backward induction from period T down to 1\n        for t in range(T, 0, -1):\n            # In period t, t-1 rounds have passed, so k (count of '0's) can be from 0 to t-1\n            for k in range(t):\n                # Belief p_t = N/D. We use integer arithmetic to determine P2's action.\n                N = alpha0 + k\n                D = alpha0 + alpha1 + t - 1\n                \n                # Determine Player 2's action a2_t\n                # Tie-breaking rule: P2 chooses action 0 in case of indifference.\n                # This is handled by '>=' in the main comparison.\n                \n                a2_t = 0 # Default action\n                if D_U2 > 0:\n                    if not (N * D_U2 >= D * C_U2):\n                        a2_t = 1\n                elif D_U2 < 0:\n                    if not (N * D_U2 <= D * C_U2):  # Inequality flips\n                        a2_t = 1\n                else: # D_U2 == 0, p_t does not affect choice\n                    if not (0 >= C_U2):\n                        a2_t = 1\n                \n                # Player 1's Bellman equation\n                # Value if P1 chooses action 0\n                val_if_0 = U1[0, a2_t] + V[t + 1, k + 1]\n                \n                # Value if P1 chooses action 1\n                val_if_1 = U1[1, a2_t] + V[t + 1, k]\n                \n                V[t, k] = max(val_if_0, val_if_1)\n        \n        # The result is the value at the beginning of the game (t=1, k=0)\n        results.append(V[1, 0])\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}]}