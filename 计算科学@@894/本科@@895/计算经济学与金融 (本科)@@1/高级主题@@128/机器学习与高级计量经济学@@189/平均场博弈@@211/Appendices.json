{"hands_on_practices": [{"introduction": "我们的动手实践始于一个高度简化的单周期模型，旨在帮助你建立对平均场均衡的核心直觉。该练习通过对财富低于某一阈值的智能体施加严厉惩罚来模拟潜在的市场崩盘 [@problem_id:2409409]。你将培养出的关键洞见是，均衡一致性条件——即个体最优行为必须与群体平均行为相一致——如何将问题转化为一个可解的约束优化问题。", "id": "2409409", "problem": "考虑一个金融市场中的单周期平均场博弈（Mean Field Game, MFG），其中有连续统的相同代理人。每个代理人拥有初始财富 $x_0 \\in \\mathbb{R}$，并选择一个控制 $\\,\\alpha \\in [\\underline{\\alpha}, \\overline{\\alpha}] \\subset \\mathbb{R}\\,$。群体中的总平均控制为 $m \\in \\mathbb{R}$。一个代理人的下一周期财富由以下确定性仿射定律给出\n$$\nx_1 \\;=\\; x_0 \\,+\\, \\alpha \\,+\\, \\gamma\\, m,\n$$\n其中 $\\gamma \\in \\mathbb{R}$ 是一个给定的耦合参数，并假设 $1+\\gamma>0$。运行成本是不连续的，定义为\n$$\nL(x_0,\\alpha;m)\\;=\\;\n\\begin{cases}\n\\dfrac{1}{2}\\,\\alpha^2, & \\text{若 } x_1 \\;\\ge\\; x_{\\min},\\\n$$6pt]\n+\\infty, & \\text{若 } x_1 \\;<\\; x_{\\min},\n\\end{cases}\n$$\n其中 $x_{\\min}\\in\\mathbb{R}$ 是一个给定的财富阈值，用于模拟市场崩溃：财富低于 $x_{\\min}$ 会产生无限成本。一个平均场博弈均衡是一个控制 $\\,\\alpha^\\star\\,$，它在满足 $\\alpha \\in [\\underline{\\alpha}, \\overline{\\alpha}]$ 和 $x_1 \\ge x_{\\min}$ 的条件下最小化 $L(x_0,\\alpha;m)$，同时满足一致性条件 $\\,m=\\alpha^\\star\\,$。\n\n对于每组参数 $(x_0, x_{\\min}, \\gamma, \\underline{\\alpha}, \\overline{\\alpha})$，将均衡值定义为\n$$\nV^\\star(x_0, x_{\\min}, \\gamma, \\underline{\\alpha}, \\overline{\\alpha}) \\;=\\;\n\\inf\\left\\{\\tfrac{1}{2}\\,\\alpha^2 \\;:\\; \\alpha \\in [\\underline{\\alpha}, \\overline{\\alpha}],\\; x_0 + \\alpha + \\gamma\\,\\alpha \\;\\ge\\; x_{\\min}\\right\\},\n$$\n约定如果可行集为空，则 $V^\\star=+\\infty$。\n\n您的任务是编写一个程序，为以下每个测试用例计算 $V^\\star$，结果为一个实数，如果不可行则为 $+\\infty$：\n\n- 测试用例 A（正常路径）：$(x_0, x_{\\min}, \\gamma, \\underline{\\alpha}, \\overline{\\alpha}) = (\\,10,\\,5,\\,0.5,\\,-2,\\,2\\,)$。\n- 测试用例 B（有约束力的防崩溃，可行）：$(x_0, x_{\\min}, \\gamma, \\underline{\\alpha}, \\overline{\\alpha}) = (\\,4,\\,5,\\,1,\\,-1,\\,3\\,)$。\n- 测试用例 C（不可行的防崩溃）：$(x_0, x_{\\min}, \\gamma, \\underline{\\alpha}, \\overline{\\alpha}) = (\\,0,\\,5,\\,0,\\,-1,\\,1\\,)$。\n- 测试用例 D（控制边界强制非零行动）：$(x_0, x_{\\min}, \\gamma, \\underline{\\alpha}, \\overline{\\alpha}) = (\\,10,\\,5,\\,0.2,\\,0.1,\\,0.5\\,)$。\n- 测试用例 E（恰好在边界上）：$(x_0, x_{\\min}, \\gamma, \\underline{\\alpha}, \\overline{\\alpha}) = (\\,2,\\,5,\\,0,\\,-10,\\,3\\,)$。\n\n最终输出格式：您的程序应生成单行输出，其中包含按 A、B、C、D、E 顺序排列的结果，形式为逗号分隔的列表，并用方括号括起（例如 $[v_A,v_B,v_C,v_D,v_E]$），其中每个 $v_\\cdot$ 是一个实数或表示为 IEEE 浮点无穷大的 $+\\infty$。", "solution": "所呈现的问题是一个源自单周期平均场博弈的约束优化问题。该问题定义明确，数学上一致，并以平均场博弈的标准理论为基础。因此，该问题是有效的，并将提供一个解决方案。\n\n目标是计算均衡值 $V^\\star$，定义如下：\n$$\nV^\\star(x_0, x_{\\min}, \\gamma, \\underline{\\alpha}, \\overline{\\alpha}) = \\inf\\left\\{\\tfrac{1}{2}\\,\\alpha^2 \\;:\\; \\alpha \\in [\\underline{\\alpha}, \\overline{\\alpha}],\\; x_0 + \\alpha + \\gamma\\,\\alpha \\;\\ge\\; x_{\\min}\\right\\}\n$$\n这是一个优化问题，旨在在控制 $\\alpha$ 的可行集上找到成本函数 $J(\\alpha) = \\frac{1}{2}\\alpha^2$ 的最小值。可行集（我们用 $\\mathcal{A}$ 表示）由两个约束条件决定：\n1. 控制边界：$\\alpha \\in [\\underline{\\alpha}, \\overline{\\alpha}]$。\n2. 防崩溃约束：$x_0 + (1+\\gamma)\\alpha \\ge x_{\\min}$。\n\n问题陈述中提供了一个关键假设，即 $1+\\gamma > 0$。这使得我们可以直接重排防崩溃约束以分离出 $\\alpha$：\n$$\n(1+\\gamma)\\alpha \\ge x_{\\min} - x_0\n$$\n$$\n\\alpha \\ge \\frac{x_{\\min} - x_0}{1+\\gamma}\n$$\n我们将此财富约束对控制施加的临界阈值定义为 $\\alpha_{\\text{crash}}$：\n$$\n\\alpha_{\\text{crash}} = \\frac{x_{\\min} - x_0}{1+\\gamma}\n$$\n可行集 $\\mathcal{A}$ 是 $\\alpha$ 的两个约束集的交集：\n$$\n\\mathcal{A} = \\{\\alpha \\in \\mathbb{R} \\mid \\underline{\\alpha} \\le \\alpha \\le \\overline{\\alpha} \\quad \\text{且} \\quad \\alpha \\ge \\alpha_{\\text{crash}}\\}\n$$\n这可以表示为单个紧区间。令 $\\alpha_L$ 为控制的有效下界，$\\alpha_U$ 为有效上界：\n$$\n\\alpha_L = \\max(\\underline{\\alpha}, \\alpha_{\\text{crash}})\n$$\n$$\n\\alpha_U = \\overline{\\alpha}\n$$\n因此，可行集是区间 $\\mathcal{A} = [\\alpha_L, \\alpha_U]$。\n\n当且仅当该区间非空时，问题是可行的，这要求 $\\alpha_L \\le \\alpha_U$。如果 $\\alpha_L > \\alpha_U$，则可行集为空，根据问题约定，值为 $V^\\star = +\\infty$。\n\n如果问题可行，我们必须在区间 $[\\alpha_L, \\alpha_U]$ 上找到最小化目标函数 $J(\\alpha) = \\frac{1}{2}\\alpha^2$ 的最优控制 $\\alpha^\\star$。函数 $J(\\alpha)$ 是凸函数，在 $\\alpha=0$ 处有全局最小值。可行区间 $[\\alpha_L, \\alpha_U]$ 内最接近 $0$ 的 $\\alpha$ 值将是最优控制 $\\alpha^\\star$。这等价于将点 $0$ 投影到区间 $[\\alpha_L, \\alpha_U]$ 上。我们根据区间相对于 $0$ 的位置，分三种情况分析这个投影：\n\n1.  如果区间 $[\\alpha_L, \\alpha_U]$ 包含 $0$（即 $\\alpha_L \\le 0 \\le \\alpha_U$），则无约束最小化子是可行的，最优控制为 $\\alpha^\\star = 0$。\n\n2.  如果区间是严格正的（即 $0 < \\alpha_L \\le \\alpha_U$），则函数 $J(\\alpha)$ 在该区间上单调递增。因此，在左端点达到最小值，$\\alpha^\\star = \\alpha_L$。\n\n3.  如果区间是严格负的（即 $\\alpha_L \\le \\alpha_U < 0$），则函数 $J(\\alpha)$ 在该区间上单调递减。因此，在右端点达到最小值，$\\alpha^\\star = \\alpha_U$。\n\n一旦确定了最优控制 $\\alpha^\\star$，均衡值就计算为 $V^\\star = \\frac{1}{2}(\\alpha^\\star)^2$。这样就完成了分析解。计算的算法如下：\n\n对于每组参数 $(x_0, x_{\\min}, \\gamma, \\underline{\\alpha}, \\overline{\\alpha})$：\n1.  计算 $\\alpha_{\\text{crash}} = (x_{\\min} - x_0) / (1 + \\gamma)$。\n2.  确定有效可行区间的边界：$\\alpha_L = \\max(\\underline{\\alpha}, \\alpha_{\\text{crash}})$ 和 $\\alpha_U = \\overline{\\alpha}$。\n3.  如果 $\\alpha_L > \\alpha_U$，结果为 $+\\infty$。\n4.  否则，确定 $\\alpha^\\star$：\n    - 如果 $\\alpha_L > 0$，则 $\\alpha^\\star = \\alpha_L$。\n    - 如果 $\\alpha_U < 0$，则 $\\alpha^\\star = \\alpha_U$。\n    - 否则，$\\alpha^\\star = 0$。\n5.  计算最终值 $V^\\star = \\frac{1}{2}(\\alpha^\\star)^2$。\n\n现在将此过程应用于具体的测试用例。\n- **测试用例 A**: $(10, 5, 0.5, -2, 2)$。$\\alpha_{\\text{crash}} = (5-10)/1.5 \\approx -3.33$。$\\alpha_L = \\max(-2, -3.33) = -2$。$\\alpha_U = 2$。可行区间为 $[-2, 2]$。因为 $0 \\in [-2, 2]$，所以 $\\alpha^\\star = 0$。$V^\\star = \\frac{1}{2}(0)^2 = 0$。\n- **测试用例 B**: $(4, 5, 1, -1, 3)$。$\\alpha_{\\text{crash}} = (5-4)/2 = 0.5$。$\\alpha_L = \\max(-1, 0.5) = 0.5$。$\\alpha_U = 3$。可行区间为 $[0.5, 3]$。因为 $0.5 > 0$，所以 $\\alpha^\\star = 0.5$。$V^\\star = \\frac{1}{2}(0.5)^2 = 0.125$。\n- **测试用例 C**: $(0, 5, 0, -1, 1)$。$\\alpha_{\\text{crash}} = (5-0)/1 = 5$。$\\alpha_L = \\max(-1, 5) = 5$。$\\alpha_U = 1$。因为 $5 > 1$，问题不可行。$V^\\star = +\\infty$。\n- **测试用例 D**: $(10, 5, 0.2, 0.1, 0.5)$。$\\alpha_{\\text{crash}} = (5-10)/1.2 \\approx -4.17$。$\\alpha_L = \\max(0.1, -4.17) = 0.1$。$\\alpha_U = 0.5$。可行区间为 $[0.1, 0.5]$。因为 $0.1 > 0$，所以 $\\alpha^\\star = 0.1$。$V^\\star = \\frac{1}{2}(0.1)^2 = 0.005$。\n- **测试用例 E**: $(2, 5, 0, -10, 3)$。$\\alpha_{\\text{crash}} = (5-2)/1 = 3$。$\\alpha_L = \\max(-10, 3) = 3$。$\\alpha_U = 3$。可行集是单点 $\\{3\\}$。$\\alpha^\\star = 3$。$V^\\star = \\frac{1}{2}(3)^2 = 4.5$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the equilibrium value V* for a single-period Mean Field Game.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (x0, x_min, gamma, alpha_underline, alpha_overline)\n    test_cases = [\n        (10.0, 5.0, 0.5, -2.0, 2.0),   # Test Case A\n        (4.0, 5.0, 1.0, -1.0, 3.0),    # Test Case B\n        (0.0, 5.0, 0.0, -1.0, 1.0),    # Test Case C\n        (10.0, 5.0, 0.2, 0.1, 0.5),    # Test Case D\n        (2.0, 5.0, 0.0, -10.0, 3.0),   # Test Case E\n    ]\n\n    def calculate_equilibrium_value(params):\n        \"\"\"\n        Calculates V* for a single parameter set.\n        \n        The problem is to minimize (1/2)*alpha^2 subject to:\n        1. alpha_underline <= alpha <= alpha_overline\n        2. x0 + (1 + gamma)*alpha >= x_min\n        \n        This leads to a feasible interval [alpha_L, alpha_U] for alpha.\n        \"\"\"\n        x0, x_min, gamma, alpha_underline, alpha_overline = params\n        \n        # The problem statement guarantees 1 + gamma > 0.\n        # Calculate the crash-avoidance threshold for alpha.\n        # This is derived from x0 + (1+gamma)*alpha >= x_min.\n        alpha_crash = (x_min - x0) / (1.0 + gamma)\n        \n        # Determine the effective feasible interval for alpha.\n        # The lower bound is the maximum of the given lower bound and the crash-avoidance threshold.\n        alpha_L = max(alpha_underline, alpha_crash)\n        alpha_U = alpha_overline\n        \n        # Check for feasibility. If the lower bound exceeds the upper bound, the feasible set is empty.\n        if alpha_L > alpha_U:\n            return np.inf\n\n        # The objective is to minimize (1/2)*alpha^2, which is equivalent to minimizing |alpha|.\n        # We need to find the point in the interval [alpha_L, alpha_U] that is closest to 0.\n        \n        alpha_star = 0.0\n        # If the interval is entirely to the right of 0, the closest point is the left endpoint.\n        if alpha_L > 0:\n            alpha_star = alpha_L\n        # If the interval is entirely to the left of 0, the closest point is the right endpoint.\n        elif alpha_U < 0:\n            alpha_star = alpha_U\n        # Otherwise, the interval contains 0, which is the unconstrained minimizer.\n        else: # alpha_L <= 0 <= alpha_U\n            alpha_star = 0.0\n            \n        # The equilibrium value is (1/2) * (alpha_star)^2.\n        v_star = 0.5 * alpha_star**2\n        return v_star\n\n    results = []\n    for case in test_cases:\n        result = calculate_equilibrium_value(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # map(str, ...) correctly handles floating point numbers and 'inf'.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "在掌握了静态均衡的概念之后，我们现在来解决一个包含离散行为（“买入”、“卖出”、“持有”）的动态、多周期交易模型。这个问题要求你实现“虚拟博弈”算法，这是数值求解平均场博弈的基石 [@problem_id:2409414]。你将编写基本的迭代循环：通过后向归纳法求解单个智能体的最优策略，然后前向模拟群体演化以更新平均场，直至找到一个一致的均衡。", "id": "2409414", "problem": "考虑一个有限时域 $T \\in \\mathbb{N}$ 的离散时间交易平均场博弈 (MFG)。时间由 $t \\in \\{0,1,\\dots,T-1\\}$ 索引。每个智能体在时间 $t$ 有一个库存状态 $i_t \\in \\{-1,0,1\\}$。在每个时间 $t$，智能体选择一个控制（行动）$a_t \\in \\{-1,0,1\\}$，分别解释为“卖出”（-1）、“持有”（0）或“买入”（+1），并受限于库存约束，即下一个库存为\n$$\ni_{t+1} = \\min\\{1,\\max\\{-1,\\, i_t + a_t\\}\\}.\n$$\n令 $p_t(i)$ 表示在时间 $t$ 时库存状态 $i \\in \\{-1,0,1\\}$ 上的群体分布，其中 $p_t(-1)+p_t(0)+p_t(1)=1$。在时间 $t$ 的群体平均行动是\n$$\nm_t \\equiv \\sum_{i \\in \\{-1,0,1\\}} p_t(i)\\, a_t(i),\n$$\n其中 $a_t(i)$ 是最优反馈控制在时间 $t$ 为状态 $i$ 指定的行动。市场漂移信号由下式给出\n$$\n\\mu_t = \\beta\\, m_t + \\xi_t,\n$$\n其中 $\\beta \\in \\mathbb{R}$ 是一个耦合参数，$\\xi_t \\in \\mathbb{R}$ 是一个外生漂移输入。一个智能体在时间 $t$、状态 $i_t$ 下采取行动 $a_t$ 的单步收益为\n$$\nr_t(i_t,a_t;\\mu_t) = \\mu_t\\, i_{t+1} - \\lambda\\, i_{t+1}^2 - c\\, |a_t|,\n$$\n其中库存惩罚为 $\\lambda \\ge 0$，交易成本为 $c \\ge 0$，且 $i_{t+1}$ 是由 $(i_t,a_t)$ 产生的交易后库存。智能体的目标是最大化时域内单步收益的折扣总和，折扣因子为 $\\gamma \\in (0,1]$，在时间 $T$ 的终端价值为 $0$。也就是说，给定一个序列 $(\\mu_t)_{t=0}^{T-1}$，最优控制是任何能最大化以下表达式的反馈行动序列 $a_t(i) \\in \\{-1,0,1\\}$\n$$\n\\mathbb{E}\\left[\\sum_{t=0}^{T-1} \\gamma^t\\, r_t(i_t,a_t;\\mu_t)\\right],\n$$\n并受限于库存动态和约束集。如果在给定的状态-时间对 $(i,t)$ 上，多个行动达到相同的最大值，则通过选择 $a_t(i)=0$ 来解决平局；如果 $a_t(i)=0$ 不在最大化行动之中，则选择 $a_t(i)=1$；否则选择 $a_t(i)=-1$。\n\n一个MFG均衡是一个序列 $(m_t)_{t=0}^{T-1}$，使得当每个智能体在给定 $\\mu_t=\\beta m_t + \\xi_t$ 的 $(\\mu_t)_{t=0}^{T-1}$ 下进行最优控制时，在每个时间点上所产生的平均行动等于 $m_t$。初始群体分布 $p_0$ 在 $\\{-1,0,1\\}$ 上是给定的，群体的运动定律由最优反馈控制通过确定性的库存转换导出。\n\n基于此模型，编写一个完整的程序，对以下测试套件中的每组参数，计算满足上述不动点定义的均衡平均行动序列 $(m_t)_{t=0}^{T-1}$。对于每个测试用例，输出四舍五入到6位小数的序列 $(m_0,\\dots,m_{T-1})$。\n\n使用以下参数集测试套件，每组由 $(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0)$ 指定，其中 $\\xi=(\\xi_0,\\xi_1,\\xi_2)$ 且 $p_0=(p_0(-1),p_0(0),p_0(1))$:\n- 测试 $1$: $(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0) = (\\,3,\\,0.2,\\,0.95,\\,0.02,\\,0.01,\\,(0.0,0.0,0.0),\\,(0.3,0.4,0.3)\\,)$.\n- 测试 $2$: $(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0) = (\\,3,\\,0.0,\\,0.95,\\,0.02,\\,0.01,\\,(0.0,0.0,0.0),\\,(0.3,0.4,0.3)\\,)$.\n- 测试 $3$: $(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0) = (\\,3,\\,0.3,\\,0.95,\\,1.0,\\,0.01,\\,(0.0,0.0,0.0),\\,(0.3,0.4,0.3)\\,)$.\n- 测试 $4$: $(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0) = (\\,3,\\,0.2,\\,0.95,\\,0.02,\\,0.01,\\;(-0.08,-0.02,0.0),\\,(0.2,0.5,0.3)\\,)$.\n- 测试 $5$: $(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0) = (\\,3,\\,0.02,\\,0.95,\\,0.02,\\,0.01,\\,(0.0,0.0,0.0),\\,(0.3,0.4,0.3)\\,)$.\n\n您的程序应生成单行输出，其中包含一个Python风格的列表的列表的结果，每个内部列表等于相应测试用例的 $[m_0,m_1,m_2]$，所有数字四舍五入到6位小数。格式必须严格为\n$$\n[[m_0^{(1)},m_1^{(1)},m_2^{(1)}],[m_0^{(2)},m_1^{(2)},m_2^{(2)}],\\dots,[m_0^{(5)},m_1^{(5)},m_2^{(5)}]]\n$$\n且逗号后没有空格。", "solution": "所提出的问题是一个定义明确的、离散时间、有限状态的最优交易平均场博弈（MFG）。它在科学上基于已建立的MFG理论，数学上是一致的，并为计算唯一解提供了所有必要的参数和条件。因此，该问题被认为是有效的。任务是找到MFG均衡，它被定义为平均行动轨迹空间上一个映射的不动点。我们将基于不动点迭代法（在此背景下也称为虚拟博弈）构建一个数值解。\n\n均衡是一个平均行动序列 $(m_t)_{t=0}^{T-1}$，使得如果所有智能体都预期这个序列并相应地优化其个体交易策略，那么他们行动的聚合结果将重现完全相同的序列。我们定义一个映射 $\\mathcal{F}$，它以一个推测的平均行动序列 $m = (m_t)_{t=0}^{T-1}$作为输入，并返回由智能体优化和群体动态产生的平均行动序列 $\\hat{m} = (\\hat{m}_t)_{t=0}^{T-1}$。均衡是该映射的一个不动点，即 $m = \\mathcal{F}(m)$。\n\n该算法通过以下步骤进行，迭代直至收敛：\n\n1.  **解决代表性智能体的最优控制问题**：对于一个给定的平均行动序列 $(m_t)_{t=0}^{T-1}$，代表性智能体的市场漂移信号被确定为 $\\mu_t = \\beta m_t + \\xi_t$，适用于每个时间 $t \\in \\{0, \\dots, T-1\\}$。智能体的问题是选择一个行动序列 $(a_t)_{t=0}^{T-1}$ 来最大化总折扣收益。这是一个标准的有限时域动态规划问题。令 $V_t(i)$ 为价值函数，表示从时间 $t$ 状态 $i \\in \\{-1, 0, 1\\}$ 开始可实现的最大收益。该问题通过使用Bellman方程进行反向归纳求解。终端价值指定为零，因此对所有 $i$ 都有 $V_T(i) = 0$。对于 $t = T-1, \\dots, 0$，价值函数计算如下：\n    $$ V_t(i) = \\max_{a \\in \\{-1, 0, 1\\}} \\left\\{ r_t(i, a; \\mu_t) + \\gamma V_{t+1}(i') \\right\\} $$\n    其中 $i' = \\min\\{1, \\max{-1, i+a}\\}$ 是后续的库存状态。单步收益由 $r_t(i, a; \\mu_t) = \\mu_t i' - \\lambda (i')^2 - c|a|$ 给出。\n    对于每个状态-时间对 $(i, t)$，我们首先为每个可能的行动 $a \\in \\{-1, 0, 1\\}$ 计算“Q值”：\n    $$ Q_t(i, a) = \\mu_t i' - \\lambda (i')^2 - c|a| + \\gamma V_{t+1}(i') $$\n    然后从 $Q_t(i, a)$ 的最大化行动集合中选择最优行动 $a_t(i)$。问题规定了一个严格的平局决胜规则以确保唯一的最优行动：首先，如果 $a=0$ 是最大化者，则选择它；如果不是，如果 $a=1$ 是最大化者，则选择它；否则，选择 $a=-1$。这个过程为所有 $i$ 和 $t$ 定义了一个唯一的最优策略（反馈控制）$a_t(i)$。\n\n2.  **模拟群体动态**：确定最优策略 $a_t(i)$ 后，我们模拟群体在库存状态上的分布演化。从 $t=0$ 时的给定初始分布 $p_0$ 开始，我们按时间向前演化系统。在每一步 $t$，新的平均行动 $\\hat{m}_t$ 通过在当前群体分布 $p_t$ 上对最优行动进行平均来计算：\n    $$ \\hat{m}_t = \\sum_{i \\in \\{-1, 0, 1\\}} p_t(i) a_t(i) $$\n    然后计算下一个时间步的群体分布 $p_{t+1}$。由于状态转移是确定性的，每个处于状态 $i$ 的子群体 $p_t(i)$ 完全转移到下一个状态 $i' = \\min\\{1, \\max\\{-1, i+a_t(i)\\}\\}$。因此，新的分布通过将所有转移到每个状态的群体的概率相加来找到：\n    $$ p_{t+1}(j) = \\sum_{i \\in \\{-1,0,1\\} \\text{ s.t. } \\min\\{1,\\max\\{-1,i+a_t(i)\\}\\}=j} p_t(i) $$\n    这个前向过程产生一个新的平均行动序列 $\\hat{m} = (\\hat{m}_t)_{t=0}^{T-1}$。\n\n3.  **迭代至收敛**：计算出的序列 $\\hat{m}$ 是映射 $\\mathcal{F}$ 的输出，因此我们设置 $m^{(k+1)} = \\hat{m}$，其中 $k$ 是迭代索引。然后我们重复这个过程，将 $m^{(k+1)}$ 反馈回步骤1。迭代以一个初始猜测开始，例如 $m^{(0)} = (0, \\dots, 0)$。当平均行动序列收敛时，即当连续迭代之间的最大绝对差小于一个小的容差 $\\epsilon$ 时，过程终止：$\\|_m^{(k+1)} - m^{(k)}\\|_{\\infty} < \\epsilon$。得到的序列就是所求的MFG均衡。\n\n为便于实现，库存状态 $\\{-1, 0, 1\\}$ 被方便地映射到数组索引 $\\{0, 1, 2\\}$。然后将这整个过程应用于测试套件中提供的每组参数。", "answer": "```python\nimport numpy as np\n\ndef solve_mfg_equilibrium(T, beta, gamma, c, lambda_val, xi, p0):\n    \"\"\"\n    Computes the Mean Field Game equilibrium for a discrete-time trading model.\n\n    Args:\n        T (int): Time horizon.\n        beta (float): Coupling parameter for market drift.\n        gamma (float): Discount factor.\n        c (float): Transaction cost.\n        lambda_val (float): Inventory holding penalty.\n        xi (tuple): Exogenous drift sequence.\n        p0 (tuple): Initial population distribution over states {-1, 0, 1}.\n\n    Returns:\n        list: The equilibrium mean-action sequence [m_0, m_1, ..., m_{T-1}].\n    \"\"\"\n    states = np.array([-1, 0, 1])\n    actions = np.array([-1, 0, 1])\n\n    # Fixed-point iteration\n    m = np.zeros(T)\n    max_iter = 1000\n    tolerance = 1e-12\n\n    for _ in range(max_iter):\n        m_old = m.copy()\n\n        # 1. Calculate market drift given the mean-field m\n        mu = beta * m + np.array(xi)\n\n        # 2. Solve agent's optimal control problem (backward induction)\n        V = np.zeros(3)  # V_T(i) = 0 for states {-1, 0, 1}\n        policy = np.zeros((T, 3), dtype=int)  # policy[t, s] where s is state index\n\n        for t in range(T - 1, -1, -1):\n            V_next = V.copy()\n            V_current = np.zeros(3)\n            for s, i in enumerate(states):  # s is state index {0,1,2}, i is state value {-1,0,1}\n                q_values = np.zeros(3)\n                for a_idx, a in enumerate(actions):\n                    i_next = min(1, max(-1, i + a))\n                    s_next = i_next + 1\n                    \n                    reward = mu[t] * i_next - lambda_val * (i_next**2) - c * abs(a)\n                    q = reward + gamma * V_next[s_next]\n                    q_values[a_idx] = q\n\n                # Find optimal action with the specified tie-breaking rule\n                max_q = np.max(q_values)\n                optimal_actions_mask = np.isclose(q_values, max_q, atol=1e-12)\n                \n                if optimal_actions_mask[1]:  # Action a=0 is optimal\n                    best_action = 0\n                elif optimal_actions_mask[2]:  # Action a=1 is optimal\n                    best_action = 1\n                else:  # Action a=-1 is optimal\n                    best_action = -1\n                \n                policy[t, s] = best_action\n                # Index in q_values for action `a` is `a+1`\n                V_current[s] = q_values[best_action + 1]\n            V = V_current\n\n        # 3. Simulate population dynamics (forward pass)\n        p = np.array(p0)\n        m_new = np.zeros(T)\n        for t in range(T):\n            # Calculate mean action at time t\n            # policy[t, s] gives action for state_index s\n            actions_t = policy[t, :]\n            m_new[t] = np.dot(p, actions_t)\n\n            # Evolve population distribution\n            p_next = np.zeros(3)\n            for s, i in enumerate(states): # s is current state index\n                action = policy[t, s]\n                i_next = min(1, max(-1, i + action))\n                s_next = i_next + 1 # next state index\n                p_next[s_next] += p[s]\n            p = p_next\n\n        m = m_new\n\n        # 4. Check for convergence\n        if np.max(np.abs(m - m_old)) < tolerance:\n            break\n            \n    return m.tolist()\n\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results in the required format.\n    \"\"\"\n    test_cases = [\n        # (T, beta, gamma, c, lambda_val, xi, p0)\n        (3, 0.2, 0.95, 0.02, 0.01, (0.0, 0.0, 0.0), (0.3, 0.4, 0.3)),\n        (3, 0.0, 0.95, 0.02, 0.01, (0.0, 0.0, 0.0), (0.3, 0.4, 0.3)),\n        (3, 0.3, 0.95, 1.0, 0.01, (0.0, 0.0, 0.0), (0.3, 0.4, 0.3)),\n        (3, 0.2, 0.95, 0.02, 0.01, (-0.08, -0.02, 0.0), (0.2, 0.5, 0.3)),\n        (3, 0.02, 0.95, 0.02, 0.01, (0.0, 0.0, 0.0), (0.3, 0.4, 0.3)),\n    ]\n    \n    results_str_list = []\n    for params in test_cases:\n        m_equilibrium = solve_mfg_equilibrium(*params)\n        m_rounded = [round(val, 6) for val in m_equilibrium]\n        \n        # Format list to string '[v1,v2,...]' without spaces\n        case_result_str = \"[\" + \",\".join(map(str, m_rounded)) + \"]\"\n        results_str_list.append(case_result_str)\n\n    # Format final output string '[[...],[...],...]'\n    final_output = \"[\" + \",\".join(results_str_list) + \"]\"\n    print(final_output)\n\nsolve()\n```"}, {"introduction": "尽管迭代法具有普适性，但某些重要的平均场博弈类别允许更直接、更优雅的解法。本练习将探讨一个关于城市增长的线性二次（$LQ$）模型，这是经济学和金融学中一种常见的结构 [@problem_id:2409391]。你将应用一种源自控制理论的强大技术，通过向后求解Riccati方程来找到最优反馈策略，然后向前追踪横截面方差的演变。", "id": "2409391", "problem": "考虑一个由索引 $i$ 标记的城市连续体，时间为离散的 $t \\in \\{0,1,\\dots,T\\}$。每个城市 $i$ 都有一个对数规模 $x_t^i \\in \\mathbb{R}$。令 $m_t \\in \\mathbb{R}$ 表示在时间 $t$ 时 $\\{x_t^i\\}_{i}$ 的横截面均值。城市 $i$ 通过选择一个控制量 $u_t^i \\in \\mathbb{R}$ 来控制其增长，该控制量通过以下确定性运动定律影响其下一周期的对数规模：\n$$\nx_{t+1}^i \\;=\\; (1-\\beta)\\,x_t^i \\;+\\; \\beta\\,m_t \\;+\\; u_t^i,\n$$\n其中 $\\beta \\in [0,1]$ 是一个给定参数。对数规模的初始横截面分布使得 $x_0^i$ 的均值为 $m_0 \\in \\mathbb{R}$，方差为 $s_0^2 \\ge 0$。对于一个给定的确定性平均场路径 $\\{m_t\\}_{t=0}^T$，每个城市 $i$ 最小化二次目标函数\n$$\nJ^i \\;=\\; \\sum_{t=0}^{T-1} \\left(\\tfrac{1}{2}\\,q\\,\\big(x_t^i - m_t\\big)^2 \\;+\\; \\tfrac{1}{2}\\,r\\,\\big(u_t^i\\big)^2\\right) \\;+\\; \\tfrac{1}{2}\\,q_T\\,\\big(x_T^i - m_T\\big)^2,\n$$\n其中 $q \\ge 0$，$r > 0$ 和 $q_T \\ge 0$ 是给定的常数。一个平均场均衡是一个序列 $\\{m_t\\}_{t=0}^T$ 和一个可测的反馈定律 $u_t^i = \\pi_t(x_t^i,m_t)$，使得对于每个城市 $i$，在给定 $\\{m_t\\}_{t=0}^T$ 的情况下，控制量 $u_t^i$ 最小化 $J^i$，并且当所有城市都使用最优策略且 $x_0^i$ 的均值为 $m_0$、方差为 $s_0^2$ 时，所引出的横截面均值满足对所有 $t \\in \\{0,1,\\dots,T\\}$ 的不动点条件 $m_t = \\mathbb{E}[x_t^i]$。\n\n您的任务是，对于下面测试套件中的每一组参数，计算表征上述模型下唯一平均场均衡的以下两个量：\n- 在均衡平均场路径下，由时间 $t=0$ 时的线性关系 $u_0^i = -K_0\\,(x_0^i - m_0)$ 定义的最优初始反馈系数 $K_0 \\in \\mathbb{R}$。\n- 终端时刻对数规模的横截面方差 $\\mathrm{Var}(x_T^i)$。\n\n假设除了上述给定的确定性动态之外，没有其他外生冲击。将报告的每个数字表示为四舍五入到六位小数的实数。所有角度都是无量纲的。不涉及物理单位。\n\n待评估的参数值测试套件：\n- 情况 $1$：$T = 10$，$\\beta = 0.3$， $q = 1.0$， $r = 0.5$，$q_T = 1.0$，$m_0 = 2.0$，$s_0^2 = 1.0$。\n- 情况 $2$：$T = 5$，$\\beta = 0.0$， $q = 1.0$， $r = 1.0$，$q_T = 1.0$，$m_0 = 0.0$，$s_0^2 = 0.25$。\n- 情况 $3$：$T = 20$，$\\beta = 0.9$， $q = 0.5$， $r = 1.0$，$q_T = 0.5$，$m_0 = -1.0$，$s_0^2 = 2.0$。\n- 情况 $4$：$T = 8$，$\\beta = 0.2$， $q = 1.0$， $r = 0.01$，$q_T = 1.0$，$m_0 = 1.0$，$s_0^2 = 3.0$。\n- 情况 $5$：$T = 12$，$\\beta = 0.4$， $q = 0.01$， $r = 1.0$，$q_T = 0.01$，$m_0 = 0.5$，$s_0^2 = 1.5$。\n- 情况 $6$：$T = 7$，$\\beta = 0.5$， $q = 1.0$， $r = 1.0$，$q_T = 1.0$，$m_0 = 3.0$，$s_0^2 = 0.0$。\n\n最终输出格式：\n- 对于每种情况，输出一个双元素列表 $[K_0,\\mathrm{Var}(x_T^i)]$，两个条目都四舍五入到六位小数。\n- 将所有情况的结果汇总到一行，形式为用方括号括起来的逗号分隔列表，例如 $[[a_1,b_1],[a_2,b_2],\\dots]$，不含任何附加文本。", "solution": "对所提供的问题进行验证。\n\n逐字提取给定条件：\n- 一个由索引 $i$ 标记的城市连续体。\n- 离散时间 $t \\in \\{0, 1, \\dots, T\\}$。\n- 城市 $i$ 的对数规模：$x_t^i \\in \\mathbb{R}$。\n- 横截面均值对数规模：$m_t = \\mathbb{E}[x_t^i]$。\n- 城市 $i$ 的控制量：$u_t^i \\in \\mathbb{R}$。\n- 运动定律：$x_{t+1}^i = (1-\\beta)x_t^i + \\beta m_t + u_t^i$，其中 $\\beta \\in [0,1]$。\n- $x_0^i$ 的初始分布：均值为 $m_0 \\in \\mathbb{R}$，方差为 $s_0^2 \\ge 0$。\n- 城市 $i$ 的目标函数：$J^i = \\sum_{t=0}^{T-1} (\\frac{1}{2}q(x_t^i - m_t)^2 + \\frac{1}{2}r(u_t^i)^2) + \\frac{1}{2}q_T(x_T^i - m_T)^2$，其中 $q \\ge 0$，$r > 0$，$q_T \\ge 0$。\n- 平均场均衡定义：一个路径 $\\{m_t\\}_{t=0}^T$ 和一个反馈定律 $u_t^i = \\pi_t(x_t^i, m_t)$，使得 $u_t^i$ 对每个城市 $i$ 在给定 $\\{m_t\\}$ 的情况下是最优的，并且一致性条件 $m_t = \\mathbb{E}[x_t^i]$ 对所有 $t$ 都成立。\n- 任务：计算初始反馈系数 $K_0$（来自 $u_0^i = -K_0(x_0^i - m_0)$）和终端方差 $\\mathrm{Var}(x_T^i)$。\n\n该问题是平均场博弈论中一个定义明确的标准问题，具体来说是一个离散时间的线性二次模型。它具有科学依据，是适定的，并以客观的数学语言表述。求解所需的所有参数都已提供。因此，该问题被认为是有效的。\n\n求解过程如下。首先，我们刻画平均场均衡。个体城市 $i$ 求解一个线性二次最优控制问题，将平均场路径 $\\{m_t\\}_{t=0}^T$ 视为给定。均衡要求遵循其最优策略的城市总体行为能够再现给定的平均场路径。\n\n令 $\\tilde{x}_t^i = x_t^i - m_t$ 为城市 $i$ 的对数规模与均值的偏差。该偏差的运动定律是：\n$$\n\\tilde{x}_{t+1}^i = x_{t+1}^i - m_{t+1} = \\big((1-\\beta)x_t^i + \\beta m_t + u_t^i\\big) - m_{t+1}\n$$\n$$\n\\tilde{x}_{t+1}^i = (1-\\beta)(\\tilde{x}_t^i + m_t) + \\beta m_t + u_t^i - m_{t+1} = (1-\\beta)\\tilde{x}_t^i + u_t^i + m_t - m_{t+1}\n$$\n成本函数变为：\n$$\nJ^i = \\sum_{t=0}^{T-1} \\left(\\tfrac{1}{2}q(\\tilde{x}_t^i)^2 + \\tfrac{1}{2}r(u_t^i)^2\\right) + \\tfrac{1}{2}q_T(\\tilde{x}_T^i)^2\n$$\n这是一个标准的线性二次调节器问题，其中 $(m_t - m_{t+1})$ 充当外生扰动。值函数是二次的，$V_t(\\tilde{x}_t^i) = \\frac{1}{2}P_t(\\tilde{x}_t^i)^2 + \\dots$，其中省略了与 $\\tilde{x}_t^i$ 无关的项。时间 $t$ 的 Hamilton-Jacobi-Bellman 方程为：\n$$\nV_t(\\tilde{x}_t^i) = \\min_{u_t^i} \\left\\{ \\tfrac{1}{2}q(\\tilde{x}_t^i)^2 + \\tfrac{1}{2}r(u_t^i)^2 + V_{t+1}(\\tilde{x}_{t+1}^i) \\right\\}\n$$\n关于 $u_t^i$ 最小化括号内的项，得到一阶条件：\n$$\nr u_t^i + \\frac{\\partial V_{t+1}}{\\partial \\tilde{x}_{t+1}^i} \\frac{\\partial \\tilde{x}_{t+1}^i}{\\partial u_t^i} = r u_t^i + P_{t+1}\\tilde{x}_{t+1}^i(1) = r u_t^i + P_{t+1}((1-\\beta)\\tilde{x}_t^i + u_t^i + m_t - m_{t+1}) = 0\n$$\n求解最优控制 $u_t^i$：\n$$\n(r+P_{t+1})u_t^i = -P_{t+1}((1-\\beta)\\tilde{x}_t^i + m_t - m_{t+1})\n$$\n$$\nu_t^i = -\\frac{P_{t+1}}{r+P_{t+1}}((1-\\beta)\\tilde{x}_t^i + m_t - m_{t+1})\n$$\n平均场一致性条件是 $\\mathbb{E}[x_t^i] = m_t$，这意味着对所有 $t$，$\\mathbb{E}[\\tilde{x}_t^i] = 0$。对原始状态动态取期望，$m_{t+1} = (1-\\beta)m_t + \\beta m_t + \\mathbb{E}[u_t^i] = m_t + \\mathbb{E}[u_t^i]$，这意味着 $\\mathbb{E}[u_t^i] = m_{t+1} - m_t$。\n对最优控制定律取期望：\n$$\n\\mathbb{E}[u_t^i] = -\\frac{P_{t+1}}{r+P_{t+1}}((1-\\beta)\\mathbb{E}[\\tilde{x}_t^i] + m_t - m_{t+1}) = -\\frac{P_{t+1}}{r+P_{t+1}}(m_t - m_{t+1})\n$$\n将 $\\mathbb{E}[u_t^i]$ 的两个表达式相等：\n$$\nm_{t+1} - m_t = -\\frac{P_{t+1}}{r+P_{t+1}}(m_t - m_{t+1}) \\implies \\left(1 + \\frac{P_{t+1}}{r+P_{t+1}}\\right)(m_{t+1} - m_t) = 0\n$$\n因为 $r > 0$ 且 $P_{t+1} \\ge 0$ (因为它代表成本)，所以括号中的项是严格为正的。因此，必须有 $m_{t+1} - m_t = 0$ 对所有 $t$ 成立。这证明了均衡平均场路径必须是常数：对所有 $t = 0, \\dots, T$，有 $m_t = m_0$。\n\n在 $m_t$ 为常数的情况下，问题显著简化。偏差的动态变为 $\\tilde{x}_{t+1}^i = (1-\\beta)\\tilde{x}_t^i + u_t^i$。现在问题是关于偏差状态 $\\tilde{x}_t^i$ 的一个标准确定性 LQR 问题。最优控制简化为线性反馈定律：\n$$\nu_t^i = -\\frac{P_{t+1}(1-\\beta)}{r+P_{t+1}}\\tilde{x}_t^i = -K_t \\tilde{x}_t^i\n$$\n其中 $K_t = \\frac{P_{t+1}(1-\\beta)}{r+P_{t+1}}$ 是反馈增益。系数 $P_t$ 由离散时间 Riccati 方程确定，从 $t=T-1$ 向后求解到 $t=0$：\n$$\nP_t = q + (1-\\beta)^2 P_{t+1} - \\frac{((1-\\beta)P_{t+1})^2}{r+P_{t+1}} = q + \\frac{r P_{t+1}(1-\\beta)^2}{r+P_{t+1}}\n$$\n该递归从终端条件 $P_T = q_T$ 开始，该条件源自终端成本。\n我们感兴趣的量 $K_0$ 随后计算为 $K_0 = \\frac{P_1(1-\\beta)}{r+P_1}$，这需要计算序列 $\\{P_t\\}_{t=1}^T$。\n\n接下来，我们推导横截面方差的演化， $s_t^2 = \\mathrm{Var}(x_t^i) = \\mathrm{Var}(\\tilde{x}_t^i)$。将最优控制代入偏差的动态中：\n$$\n\\tilde{x}_{t+1}^i = (1-\\beta)\\tilde{x}_t^i - K_t \\tilde{x}_t^i = \\left((1-\\beta) - \\frac{P_{t+1}(1-\\beta)}{r+P_{t+1}}\\right)\\tilde{x}_t^i\n$$\n$$\n\\tilde{x}_{t+1}^i = (1-\\beta)\\left(1 - \\frac{P_{t+1}}{r+P_{t+1}}\\right)\\tilde{x}_t^i = \\left(\\frac{r(1-\\beta)}{r+P_{t+1}}\\right)\\tilde{x}_t^i\n$$\n方差根据一个前向递归演化。令 $v_t = s_t^2$：\n$$\nv_{t+1} = \\mathrm{Var}(\\tilde{x}_{t+1}^i) = \\mathbb{E}\\left[\\left(\\frac{r(1-\\beta)}{r+P_{t+1}}\\tilde{x}_t^i\\right)^2\\right] = \\left(\\frac{r(1-\\beta)}{r+P_{t+1}}\\right)^2 \\mathbb{E}[(\\tilde{x}_t^i)^2] = \\left(\\frac{r(1-\\beta)}{r+P_{t+1}}\\right)^2 v_t\n$$\n从给定的初始方差 $v_0 = s_0^2$ 开始，我们可以通过前向传递，使用先前计算的 $\\{P_t\\}$ 序列来计算序列 $\\{v_t\\}_{t=1}^T$。终端方差为 $\\mathrm{Var}(x_T^i) = v_T$。\n\n算法如下：\n1. 用 $P_T = q_T$ 初始化一个大小为 $T+1$ 的数组 $P$。\n2. 通过从 $t = T-1$ 向下到 $t=0$ 的后向迭代来求解 Riccati 方程，以填充数组 $P$。\n3. 计算初始反馈增益 $K_0 = \\frac{P_1(1-\\beta)}{r+P_1}$。对于 $T=0$ 的情况，这没有定义，但所有测试用例都有 $T \\ge 1$。\n4. 初始化方差 $v_0 = s_0^2$。\n5. 通过从 $t=0$ 到 $t=T-1$ 的前向迭代，使用存储的 $P_{t+1}$ 值，计算方差的演化。\n6. 每种情况的最终结果是 $[K_0, v_T]$。\n\n下面对提供的测试用例实施此程序。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a discrete-time linear-quadratic mean-field game for a series of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (T, beta, q, r, q_T, m_0, s0_sq)\n        (10, 0.3, 1.0, 0.5, 1.0, 2.0, 1.0),\n        (5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.25),\n        (20, 0.9, 0.5, 1.0, 0.5, -1.0, 2.0),\n        (8, 0.2, 1.0, 0.01, 1.0, 1.0, 3.0),\n        (12, 0.4, 0.01, 1.0, 0.01, 0.5, 1.5),\n        (7, 0.5, 1.0, 1.0, 1.0, 3.0, 0.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        T, beta, q, r, q_T, m_0, s0_sq = case\n        \n        # 1. Backward pass: Solve the Riccati equation for P_t\n        # P is an array of size T+1 for P_0, ..., P_T\n        P = np.zeros(T + 1)\n        P[T] = q_T\n        \n        # The equation for P_t depends on P_{t+1}. We iterate t from T-1 down to 0.\n        for t in range(T - 1, -1, -1):\n            P_next = P[t + 1]\n            term = (r * P_next * (1 - beta)**2) / (r + P_next)\n            P[t] = q + term\n\n        # 2. Calculate the initial feedback coefficient K_0\n        # K_0 depends on P_1. If T=0, there is no P_1 and no control u_0.\n        # Problem statement implies T >= 1 for control to be meaningful.\n        if T > 0:\n            K0 = (P[1] * (1 - beta)) / (r + P[1])\n        else:\n            # If T=0, there is no control phase, so K_0 is not applicable.\n            # Based on the problem structure, K_0=0 is a reasonable assignment.\n            K0 = 0.0\n\n        # 3. Forward pass: Evolve the cross-sectional variance\n        var = s0_sq\n        \n        # The evolution of var_t to var_{t+1} depends on P_{t+1}\n        for t in range(T):\n            P_next = P[t + 1]\n            factor = (r * (1 - beta)) / (r + P_next)\n            var = (factor**2) * var\n            \n        var_T = var\n\n        # Append rounded results for the current case\n        results.append([round(K0, 6), round(var_T, 6)])\n\n    # Format the final output string as specified.\n    # e.g., [[val1, val2], [val3, val4], ...]\n    final_output = \"[\" + \",\".join([f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in results]) + \"]\"\n    print(final_output)\n\nsolve()\n```"}]}