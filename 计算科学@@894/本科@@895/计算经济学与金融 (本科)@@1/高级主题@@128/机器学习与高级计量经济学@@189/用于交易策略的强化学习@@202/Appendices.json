{"hands_on_practices": [{"introduction": "强化学习智能体的核心是其奖励函数，它定义了“好”的行为。这个练习将引导你聚焦于一个关键的权衡：预期利润与交易成本之间的平衡。通过解决这个单步优化问题，你将亲身体验智能体如何评估单个行为的质量，并理解滑点（一种重要的交易成本）如何影响最优决策 [@problem_id:2426687]。", "id": "2426687", "problem": "给定一个单步交易决策问题，其中一个交易代理选择一个带符号的交易规模 $v$（$v$ 为正表示买入，$v$ 为负表示卖出），以最大化一个带惩罚的期望回报。每单位的即时期望价格变化为 $\\mu$，市场波动率参数为 $\\sigma$。瞬时滑点惩罚被建模为一种每单位效应，该效应随波动率和交易规模大小的平方根而增加，遵循经验平方根冲击：每单位滑点为 $s_{\\text{per}}(v,\\sigma) = k\\,\\sigma\\,|v|^{1/2}$，其中 $k$ 是一个正常数。以货币单位计，总滑点成本为 $C(v,\\sigma) = s_{\\text{per}}(v,\\sigma)\\,|v| = k\\,\\sigma\\,|v|^{3/2}$。因此，采取行动 $v$ 的带惩罚期望回报为\n$$\nR(v;\\mu,\\sigma,k) = \\mu\\,v \\;-\\; k\\,\\sigma\\,|v|^{3/2}.\n$$\n选择变量 $v$ 受到硬性交易限制 $|v| \\le V_{\\max}$ 的约束。\n\n任务：对于下面测试套件中的每一组参数，计算唯一的最优交易规模 $v^\\star$，该值在所有满足 $|v|\\le V_{\\max}$ 的实数 $v$ 中最大化 $R(v;\\mu,\\sigma,k)$。当因退化情况存在多个最大化器时，选择 $v^\\star = 0$ 作为决胜约定。您的程序必须根据 $R(v;\\mu,\\sigma,k)$ 的定义来实现此计算，并为每个测试用例输出最优的 $v^\\star$。\n\n测试套件（每一项是一个元组 $(\\mu,\\sigma,k,V_{\\max})$）：\n- 情况1：$(0.01, 0.02, 0.5, 10)$\n- 情况2：$(-0.015, 0.03, 0.7, 20)$\n- 情况3：$(0.02, 0, 1, 5)$\n- 情况4：$(0, 0.05, 0.5, 100)$\n- 情况5：$(0.05, 0.02, 0.2, 1)$\n- 情况6：$(0.01, 0.5, 2, 100)$\n\n最终输出格式：您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表，顺序与上述情况相同。将每个结果四舍五入到 $6$ 位小数。例如，一个包含三个假设结果的有效输出将如下所示：$[0.123456,-0.500000,1.000000]$。", "solution": "问题陈述已提交以供验证。\n\n### 步骤1：提取已知信息\n- **目标函数**：带惩罚的期望回报是 $R(v;\\mu,\\sigma,k) = \\mu\\,v \\;-\\; k\\,\\sigma\\,|v|^{3/2}$。\n- **选择变量**：$v$，一个表示带符号交易规模的实数。\n- **参数**：\n    - $\\mu$：每单位的期望价格变化。\n    - $\\sigma$：市场波动率参数。\n    - $k$：一个正常数，$k>0$。\n- **约束**：交易规模受限于 $|v| \\le V_{\\max}$。\n- **决胜规则**：如果因退化情况存在多个最大化器，则选择 $v^\\star = 0$。\n- **任务**：计算在约束条件下最大化 $R(v;\\mu,\\sigma,k)$ 的最优交易规模 $v^\\star$。\n- **测试套件**：\n    - 情况1：$(\\mu,\\sigma,k,V_{\\max}) = (0.01, 0.02, 0.5, 10)$\n    - 情况2：$(\\mu,\\sigma,k,V_{\\max}) = (-0.015, 0.03, 0.7, 20)$\n    - 情况3：$(\\mu,\\sigma,k,V_{\\max}) = (0.02, 0, 1, 5)$\n    - 情况4：$(\\mu,\\sigma,k,V_{\\max}) = (0, 0.05, 0.5, 100)$\n    - 情况5：$(\\mu,\\sigma,k,V_{\\max}) = (0.05, 0.02, 0.2, 1)$\n    - 情况6：$(\\mu,\\sigma,k,V_{\\max}) = (0.01, 0.5, 2, 100)$\n\n### 步骤2：使用提取的已知信息进行验证\n1.  **科学性**：该问题描述了金融执行中的一个经典权衡：在最小化交易成本的同时，从可预测的价格变动（$\\mu v$）中最大化期望利润。成本函数 $C(v,\\sigma) = k\\,\\sigma\\,|v|^{3/2}$ 是“平方根市场冲击”定律的一种表示，这是金融市场中一个根据经验观察到的现象。该模型是计算金融学中使用的标准简化表示。它具有科学依据。\n2.  **良定性**：该任务是在紧集 $[-V_{\\max}, V_{\\max}]$ 上最大化一个连续函数 $R(v)$。根据极值定理，最大值保证存在。解的唯一性必须得到验证。该问题是良定的。\n3.  **客观性**：该问题以精确的数学术语陈述，使用了无歧义的定义和客观标准。\n4.  **完整性**：为每个情况提供了所有必要的参数（$\\mu, \\sigma, k, V_{\\max}$）。目标函数和约束条件都已完全指定。该设置是完整的。\n5.  **缺陷**：该问题不存在任何不合格的缺陷。在金融背景下，这些参数在物理上是合理的。其中一个情况包含 $\\sigma=0$，这是一个有效且重要的极限情况，代表一个没有波动性的市场，必须正确处理。\n\n### 步骤3：结论与行动\n该问题有效。将提供一个严谨的解。\n\n目标是在区间 $v \\in [-V_{\\max}, V_{\\max}]$ 上找到使函数 $R(v) = \\mu v - k \\sigma |v|^{3/2}$ 最大化的 $v^\\star$。函数 $R(v)$ 是连续的，但其在 $v=0$ 处的导数未定义。因此，我们通过分别考虑 $v>0$、$v<0$ 和 $v=0$ 的情况来分析此问题。在 $v=0$ 处的值为 $R(0)=0$。\n\n情况1：$v > 0$。\n函数为 $R(v) = \\mu v - k \\sigma v^{3/2}$。我们寻求其在 $(0, V_{\\max}]$ 上的最大值。\n关于 $v$ 的一阶和二阶导数是：\n$$\nR'(v) = \\mu - \\frac{3}{2} k \\sigma v^{1/2}\n$$\n$$\nR''(v) = -\\frac{3}{4} k \\sigma v^{-1/2}\n$$\n鉴于 $k>0$ 并且我们暂时假设 $\\sigma>0$，对于所有 $v>0$，都有 $R''(v) < 0$。这表明 $R(v)$ 在 $v>0$ 时是严格凹函数，因此它在该域内至多有一个局部最大值。\n\n为了找到无约束最大化器，我们令 $R'(v) = 0$：\n$$\n\\mu - \\frac{3}{2} k \\sigma v^{1/2} = 0 \\implies v^{1/2} = \\frac{2\\mu}{3k\\sigma}\n$$\n只有当 $\\mu > 0$ 时，$v^{1/2}$ 才存在实数正解。在这种情况下，无约束的最优交易为 $v_{\\text{unc}} = \\left(\\frac{2\\mu}{3k\\sigma}\\right)^2$。\n如果 $\\mu \\le 0$，则对于所有 $v>0$，R'(v) \\le 0，意味着 $R(v)$ 在 $(0, \\infty)$ 上是非增的。因此，在 $[0, V_{\\max}]$ 上的最大值在 $v=0$ 处取得。\n如果 $\\mu > 0$，$R(v)$的凹性意味着 $[0, V_{\\max}]$ 上的最大值在 $v^\\star_+ = \\min(v_{\\text{unc}}, V_{\\max})$ 处取得。由此产生的回报 $R(v^\\star_+)$ 将为非负。\n\n情况2：$v < 0$。\n令 $u = -v > 0$。以 $u$ 表示的函数为 $R(u) = \\mu(-u) - k\\sigma u^{3/2} = -\\mu u - k\\sigma u^{3/2}$。我们在 $u \\in (0, V_{\\max}]$ 上最大化此函数。\n关于 $u$ 的一阶导数是：\n$$\nR'(u) = -\\mu - \\frac{3}{2} k \\sigma u^{1/2}\n$$\n令 $R'(u) = 0$ 得到 $u^{1/2} = \\frac{-2\\mu}{3k\\sigma}$。只有当 $\\mu < 0$ 时，$u^{1/2}$ 才存在实数正解。无约束的最优规模为 $u_{\\text{unc}} = \\left(\\frac{-2\\mu}{3k\\sigma}\\right)^2 = \\left(\\frac{2|\\mu|}{3k\\sigma}\\right)^2$。\n如果 $\\mu \\ge 0$，则对于所有 $u>0$，$R'(u) < 0$，意味着该函数是递减的。对于负 $v$（正 $u$）值，最大值在 $v \\to 0^-$ 时取得，得到 $R(0)=0$。\n如果 $\\mu < 0$，最优的正规模 $u$ 为 $u^\\star = \\min(u_{\\text{unc}}, V_{\\max})$，这对应于一个负交易 $v^\\star_- = -u^\\star$。回报将为非负。\n\n整体解综合：\n- 如果 $\\mu > 0$，任何交易 $v < 0$ 都会产生 $R(v) < 0$，而最优交易 $v^\\star_+ > 0$ 产生 $R(v^\\star_+) \\ge 0$。因此，全局最优解是 $v^\\star = v^\\star_+$。\n- 如果 $\\mu < 0$，任何交易 $v > 0$ 都会产生 $R(v) < 0$，而最优交易 $v^\\star_- < 0$ 产生 $R(v^\\star_-) \\ge 0$。因此，全局最优解是 $v^\\star = v^\\star_-$。\n- 如果 $\\mu = 0$（且 $\\sigma > 0$），函数为 $R(v) = -k\\sigma|v|^{3/2}$，其在 $v^\\star = 0$ 处最大化。\n\n综合这些对于 $\\sigma > 0$ 的结果：\n最优交易规模的大小为 $v_{\\text{mag}} = \\min\\left( \\left(\\frac{2|\\mu|}{3k\\sigma}\\right)^2, V_{\\max} \\right)$。交易的符号必须与 $\\mu$ 的符号相匹配。如果 $\\mu=0$，则规模大小为 0。\n所以，$v^\\star = \\text{sign}(\\mu) \\cdot \\min\\left( \\left(\\frac{2|\\mu|}{3k\\sigma}\\right)^2, V_{\\max} \\right)$。\n\n特殊情况：$\\sigma = 0$。\n目标函数简化为 $R(v) = \\mu v$。我们必须在 $v \\in [-V_{\\max}, V_{\\max}]$ 上最大化这个线性函数。\n- 如果 $\\mu > 0$，R(v) 在上界处取得最大值，$v^\\star = V_{\\max}$。\n- 如果 $\\mu < 0$，R(v) 在下界处取得最大值，$v^\\star = -V_{\\max}$。\n- 如果 $\\mu = 0$，对于所有 $v$，R(v)=0。这是一种退化情况。所有 $v \\in [-V_{\\max}, V_{\\max}]$ 都是最大化器。根据规定的决胜规则，我们必须选择 $v^\\star = 0$。\n这可以紧凑地写为 $v^\\star = V_{\\max} \\cdot \\text{sign}(\\mu)$。\n\n最终算法：\n1. 对于给定的一组参数 $(\\mu, \\sigma, k, V_{\\max})$：\n2. 如果 $\\sigma = 0$，则最优交易为 $v^\\star = V_{\\max} \\cdot \\text{sign}(\\mu)$。\n3. 如果 $\\sigma > 0$：\n   - 如果 $\\mu = 0$，则最优交易为 $v^\\star = 0$。\n   - 如果 $\\mu \\ne 0$，计算无约束的最优规模大小 $v_{\\text{unc\\_mag}} = \\left(\\frac{2|\\mu|}{3k\\sigma}\\right)^2$。受约束的规模大小为 $v_{\\text{mag}} = \\min(v_{\\text{unc\\_mag}}, V_{\\max})$。最优交易为 $v^\\star = \\text{sign}(\\mu) \\cdot v_{\\text{mag}}$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal trade size v* for a series of one-step trading problems.\n    The problem is to maximize R(v) = mu*v - k*sigma*|v|^(3/2) subject to |v| <= V_max.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (mu, sigma, k, V_max).\n    test_cases = [\n        (0.01, 0.02, 0.5, 10),\n        (-0.015, 0.03, 0.7, 20),\n        (0.02, 0, 1, 5),\n        (0, 0.05, 0.5, 100),\n        (0.05, 0.02, 0.2, 1),\n        (0.01, 0.5, 2, 100),\n    ]\n\n    results = []\n    for case in test_cases:\n        mu, sigma, k, V_max = case\n        \n        v_star = 0.0\n\n        if sigma == 0:\n            # Special case: no volatility implies no slippage penalty.\n            # Maximize R(v) = mu*v.\n            # Optimal trade is at the boundary, with sign matching mu.\n            # np.sign(0) is 0, correctly handling the mu=0 degenerate case.\n            v_star = V_max * np.sign(mu)\n        else:\n            # Standard case with volatility and slippage.\n            if mu == 0:\n                # If mu is 0, R(v) = -k*sigma*|v|^(3/2), which is maximized at v=0.\n                v_star = 0.0\n            else:\n                # Unconstrained optimal trade magnitude from setting R'(v)=0.\n                v_unc_mag = (2 * abs(mu) / (3 * k * sigma))**2\n                \n                # The optimal magnitude is constrained by V_max.\n                v_mag = min(v_unc_mag, V_max)\n                \n                # The sign of the trade should match the sign of the expected return.\n                v_star = np.sign(mu) * v_mag\n        \n        results.append(v_star)\n    \n    # Format the results to 6 decimal places and join into a single string.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"}, {"introduction": "在掌握了单个决策的评估后，我们现在将问题扩展到一系列决策中。本练习介绍了一个有限时间范围内的最优库存管理问题，这是一个经典马尔可夫决策过程（MDP）的实例。你将使用动态规划（一种强大的向后归纳技术）来精确计算最优策略，从而理解像 Q-learning 这类基于价值的强化学习算法的内在逻辑 [@problem_id:2426659]。", "id": "2426659", "problem": "要求您将隔夜风险形式化为期末库存惩罚，并为一个离散时间的单一资产交易日计算最优期望回报。在该交易日中，一个代理人学习在收盘前平仓。该交易日包含有限数量的日内决策时间点。在每个决策时间点，代理人每个周期最多可以调整一个单位的库存，并受到严格的库存边界限制。\n\n环境定义如下。存在一个有限的时间范围，其日内决策时间点由 $t \\in \\{0,1,\\dots,T-1\\}$ 索引，并在 $t = T$ 时收盘。代理人在时间 $t$ 的库存为 $p_t \\in \\{-P_{\\max},-P_{\\max}+1,\\dots,P_{\\max}-1,P_{\\max}\\}$，且 $p_0 = 0$。在每个 $t \\in \\{0,1,\\dots,T-1\\}$，代理人选择一个行动 $a_t \\in \\{-1,0,1\\}$。库存转移遵循\n$$\np_{t+1} = \\mathrm{clip}(p_t + a_t, -P_{\\max}, P_{\\max}),\n$$\n其中 $\\mathrm{clip}(x,\\ell,u) = \\min\\{u,\\max\\{\\ell,x\\}\\}$。\n\n在周期 $[t,t+1]$ 内的期望价格变动是给定的，并记为 $\\mu_t$。在时间 $t$ 的任何行动的每笔交易成本为 $c \\cdot |a_t|$。在时间 $t$ 每个周期的持续风险惩罚为 $k \\cdot p_{t+1}^2$。在收盘时，会施加一笔隔夜风险惩罚：$-\\beta \\cdot p_T^2$。\n\n在时间 $t$ 做出决策的每个周期的期望回报定义为，在 $[t,t+1]$ 期间持有 $p_{t+1}$ 的期望交易利润减去交易成本和持续库存惩罚：\n$$\nr_t(p_t,a_t) = p_{t+1} \\cdot \\mu_t - c\\cdot|a_t| - k\\cdot p_{t+1}^2.\n$$\n期末贡献为\n$$\nr_T(p_T) = -\\beta \\cdot p_T^2.\n$$\n目标是计算从 $p_0=0$ 开始的最优期望总回报，\n$$\nR^\\star = \\max_{\\{a_t\\}_{t=0}^{T-1}} \\sum_{t=0}^{T-1} r_t(p_t,a_t) + r_T(p_T),\n$$\n受制于 $p_{t+1}$ 的转移动态和行动约束。\n\n必须使用确定性的平局打破规则，以确保在任何决策时间 $t$，当多个行动产生相同的最大期望值时，存在唯一的 dla 行动。平局打破规则如下：\n- 在最大化期望值的行动中，优先选择那些导致最小 $|p_{t+1}|$ 的行动。\n- 如果仍然平局，则在这些行动中，优先选择 $|a_t|$ 最小的行动。\n- 如果仍然平局，则按照常规顺序 $-1 < 0 < 1$ 选择最小的 $a_t$。\n\n对于每个测试用例，从 $p_0=0$ 开始，计算：\n- 最优期望总回报 $R^\\star$。\n- 在指定的平局打破规则下，遵循最优行动所导致的期末库存 $p_T$。\n\n您的程序应为每个测试用例返回数对 $\\left(R^\\star, p_T\\right)$，其中 $R^\\star$ 使用标准四舍五入到6位小数，$p_T$ 是一个整数。将所有测试用例的结果按下面给出的顺序汇总到一个列表中。\n\n测试套件：\n1. 用例 A (混合漂移的通用多周期): $T=4$, $\\mu = [0.02, 0.01, 0.00, -0.01]$, $c=0.003$, $k=0.0005$, $\\beta=0.05$, $P_{\\max}=3$。\n2. 用例 B (零漂移基线): $T=3$, $\\mu = [0.00, 0.00, 0.00]$, $c=0.001$, $k=0.00$, $\\beta=0.10$, $P_{\\max}=2$。\n3. 用例 C (立即收盘边界): $T=1$, $\\mu = [0.05]$, $c=0.002$, $k=0.00$, $\\beta=0.03$, $P_{\\max}=1$。\n4. 用例 D (临近收盘的平仓激励): $T=2$, $\\mu = [0.03, 0.03]$, $c=0.002$, $k=0.00$, $\\beta=0.20$, $P_{\\max}=2$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，顺序为 $[\\text{用例 A } R^\\star,\\text{用例 A } p_T,\\text{用例 B } R^\\star,\\text{用例 B } p_T,\\text{用例 C } R^\\star,\\text{用例 C } p_T,\\text{用例 D } R^\\star,\\text{用例 D } p_T]$。\n- $R^\\star$ 条目必须打印为四舍五入到6位小数的小数，$p_T$ 条目必须打印为整数。", "solution": "所提出的问题是一个离散时间、有限范围的最优控制问题，这是计算金融学中为最优执行策略建模的标准方法。该问题是适定的且在科学上是合理的，允许通过动态规划求解。目标是找到一个从初始库存 $p_0=0$ 开始的行动序列 $\\{a_t\\}_{t=0}^{T-1}$，以最大化总期望回报。\n\n解决方案是通过定义一个价值函数 $V_t(p)$ 来进行的，它表示在时间 $t$ 的库存为 $p_t = p$ 的情况下，从时间 $t$ 到达时间 $T$ 收盘前的最大可能回报总和。该问题通过反向归纳法求解，从终点时间 $T$ 开始，逆向推导至初始时间 $t=0$。\n\n在任何时间 $t \\in \\{0, 1, \\dots, T\\}$，系统的状态是代理人的库存 $p_t$。状态空间是离散且有限的，由 $p_t \\in \\{-P_{\\max}, \\dots, P_{\\max}\\}$ 给出。在任何决策时间 $t \\in \\{0, 1, \\dots, T-1\\}$，行动空间也是离散且有限的：$a_t \\in \\{-1, 0, 1\\}$。\n\nBellman 最优性原理为价值函数提供了递归关系。在终点时间 $t=T$，价值函数仅由隔夜风险惩罚决定：\n$$\nV_T(p_T) = r_T(p_T) = -\\beta \\cdot p_T^2\n$$\n对于任何之前的时间 $t \\in \\{T-1, T-2, \\dots, 0\\}$，价值函数 $V_t(p_t)$ 是在所有可能的行动 $a_t$ 上可实现的最大值。它等于即时回报 $r_t(p_t, a_t)$ 与后续状态的价值 $V_{t+1}(p_{t+1})$ 之和：\n$$\nV_t(p_t) = \\max_{a_t \\in \\{-1, 0, 1\\}} \\left\\{ r_t(p_t, a_t) + V_{t+1}(p_{t+1}) \\right\\}\n$$\n其中状态转移由 $p_{t+1} = \\mathrm{clip}(p_t + a_t, -P_{\\max}, P_{\\max})$ 给出，即时回报为 $r_t(p_t, a_t) = p_{t+1} \\cdot \\mu_t - c \\cdot |a_t| - k \\cdot p_{t+1}^2$。\n\n让我们定义品质函数，或称 Q 值，即 $Q_t(p_t, a_t)$，作为在时间 $t$ 处于状态 $p_t$ 时采取行动 $a_t$ 的价值：\n$$\nQ_t(p_t, a_t) = p_{t+1} \\cdot \\mu_t - c \\cdot |a_t| - k \\cdot p_{t+1}^2 + V_{t+1}(p_{t+1})\n$$\n于是 Bellman 方程可以写为 $V_t(p_t) = \\max_{a_t} Q_t(p_t, a_t)$。\n\n计算流程如下：\n1.  初始化一张表，用于存储所有 $t \\in \\{0, \\dots, T\\}$ 和 $p \\in \\{-P_{\\max}, \\dots, P_{\\max}\\}$ 的值 $V_t(p)$。\n2.  在 $t=T$ 时，用所有可能的期末库存 $p_T$ 的期末值 $V_T(p_T) = -\\beta \\cdot p_T^2$ 填充该表。\n3.  算法从 $t=T-1$ 开始沿时间反向迭代至 $t=0$。在每一步 $t$ 中，对于每个可能的库存状态 $p_t$，使用已知的 $V_{t+1}$ 值计算所有三种行动 $a_t \\in \\{-1, 0, 1\\}$ 的值 $Q_t(p_t, a_t)$。\n4.  为确保存在唯一的 dla 行动 $\\pi_t(p_t)$，必须严格执行指定的平局打破规则。当多个行动产生相同的最大 Q 值时，通过字典序比较来精化选择。被选择的行动 $a^\\star$ 必须是按顺序最小化以下标准的行动：\n    a. 最大化 Q 值，$Q_t(p_t, a_t)$。\n    b. 最小化下一库存的绝对值，$|p_{t+1}|$。\n    c. 最小化行动的绝对值，$|a_t|$。\n    d. 最小化行动本身的值，按自然顺序 $-1 < 0 < 1$。\n5.  存储最优值 $V_t(p_t) = Q_t(p_t, \\pi_t(p_t))$。\n6.  从 $p_0=0$ 开始的整个交易日的最优总期望回报为 $R^\\star = V_0(0)$。\n7.  为找到对应的期末库存 $p_T$，可以从初始状态 $(t,p) = (0,0)$ 开始，使用计算出的最优策略 $\\pi_t(p_t)$ 向前模拟轨迹。算法中采用了一种更直接的方法，即在反向过程中维护另一张表，该表将每个状态-时间对 $(t, p_t)$ 映射到如果从该点开始遵循最优策略所导致的期末库存 $p_T$。设此为 $P_T(t, p_t)$。那么 $P_T(T, p_T) = p_T$，对于 $t < T$，则有 $P_T(t, p_t) = P_T(t+1, \\mathrm{clip}(p_t + \\pi_t(p_t), -P_{\\max}, P_{\\max}))$。那么期末库存的最终答案就是 $p_T = P_T(0, 0)$。\n\n这种动态规划方法保证了在模型假设下能够找到全局最优解。该实现为每个测试用例计算这些值，以确定指定的输出。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the given test cases for the optimal trading problem.\n    \"\"\"\n\n    def solve_case(T, mu, c, k, beta, Pmax):\n        \"\"\"\n        Solves a single instance of the optimal trading problem using dynamic programming.\n\n        Args:\n            T (int): Number of time steps.\n            mu (list): List of expected price changes.\n            c (float): Transaction cost per share.\n            k (float): Running inventory risk penalty coefficient.\n            beta (float): Terminal inventory risk penalty coefficient.\n            Pmax (int): Maximum absolute inventory.\n\n        Returns:\n            tuple: A tuple containing the optimal total reward (R_star) and the\n                   resulting terminal inventory (p_T).\n        \"\"\"\n        # State space for inventory p\n        num_p_states = 2 * Pmax + 1\n        p_states = np.arange(-Pmax, Pmax + 1)\n        \n        # DP tables\n        # V_table stores the value function V_t(p)\n        V_table = np.zeros((T + 1, num_p_states))\n        # pT_table stores the resulting terminal inventory p_T if starting from state p at time t\n        pT_table = np.zeros((T + 1, num_p_states), dtype=np.int32)\n\n        # Helper function to map inventory p to an array index\n        def p_to_idx(p):\n            return int(p + Pmax)\n\n        # --- Backward Pass ---\n\n        # Time t = T: Terminal condition\n        for p_idx, p_T in enumerate(p_states):\n            V_table[T, p_idx] = -beta * p_T**2\n            pT_table[T, p_idx] = p_T\n\n        # Recursion from t = T-1 down to 0\n        for t in range(T - 1, -1, -1):\n            for p_idx, p_t in enumerate(p_states):\n                \n                candidates = []\n                \n                # Evaluate all possible actions a_t in {-1, 0, 1}\n                for a_t in [-1, 0, 1]:\n                    # State transition\n                    p_t_plus_1 = int(np.clip(p_t + a_t, -Pmax, Pmax))\n                    p_t_plus_1_idx = p_to_idx(p_t_plus_1)\n                    \n                    # Immediate reward\n                    reward = p_t_plus_1 * mu[t] - c * abs(a_t) - k * p_t_plus_1**2\n                    \n                    # Q-value (sum of immediate reward and future value)\n                    q_value = reward + V_table[t + 1, p_t_plus_1_idx]\n                    \n                    # Store candidate with its tie-breaking criteria as a sortable tuple:\n                    # 1. Maximize Q-value (equivalent to minimizing -q_value)\n                    # 2. Minimize |p_{t+1}|\n                    # 3. Minimize |a_t|\n                    # 4. Minimize a_t\n                    sort_key = (-q_value, abs(p_t_plus_1), abs(a_t), a_t)\n                    candidates.append((sort_key, p_t_plus_1, q_value))\n                \n                # Sort to find the best action according to the tie-breaking rules\n                candidates.sort(key=lambda x: x[0])\n                \n                # The best choice is the first element after sorting\n                best_choice = candidates[0]\n                best_p_next = best_choice[1]\n                best_q = best_choice[2]\n                \n                # Store results in the DP tables\n                V_table[t, p_idx] = best_q\n                \n                # Propagate the terminal position for this state-time pair\n                best_p_next_idx = p_to_idx(best_p_next)\n                pT_table[t, p_idx] = pT_table[t + 1, best_p_next_idx]\n\n        # --- Extract Final Answer ---\n        \n        # The solution corresponds to the initial state p_0 = 0 at t = 0\n        p0_idx = p_to_idx(0)\n        \n        # Optimal total expected reward\n        R_star = V_table[0, p0_idx]\n        \n        # Resulting terminal inventory\n        p_T_final = pT_table[0, p0_idx]\n        \n        return R_star, p_T_final\n\n    test_cases = [\n        # Case A: general multi-period with mixed drifts\n        {'T': 4, 'mu': [0.02, 0.01, 0.00, -0.01], 'c': 0.003, 'k': 0.0005, 'beta': 0.05, 'Pmax': 3},\n        # Case B: zero drift baseline\n        {'T': 3, 'mu': [0.00, 0.00, 0.00], 'c': 0.001, 'k': 0.00, 'beta': 0.10, 'Pmax': 2},\n        # Case C: immediate close boundary\n        {'T': 1, 'mu': [0.05], 'c': 0.002, 'k': 0.00, 'beta': 0.03, 'Pmax': 1},\n        # Case D: flattening incentive near close\n        {'T': 2, 'mu': [0.03, 0.03], 'c': 0.002, 'k': 0.00, 'beta': 0.20, 'Pmax': 2},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        R_star, p_T = solve_case(\n            case['T'], case['mu'], case['c'], case['k'], case['beta'], case['Pmax']\n        )\n        all_results.append(format(R_star, '.6f'))\n        all_results.append(p_T)\n\n    # Print the final output in the exact required format\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"}, {"introduction": "动态规划虽然强大，但当状态空间变得巨大或连续时，其应用会受到限制。本练习将介绍一种关键技术——函数近似——来解决这个问题。你将学习如何使用简单的线性模型来近似价值函数，并将强化学习问题转化为一个回归任务，从而能够从数据中估计模型参数，并构建一个其决策逻辑可被解释的“白盒”智能体 [@problem_id:2426627]。", "id": "2426627", "problem": "您的任务是在一个风格化的交易环境中，通过对行动价值函数使用线性模型来构建一个可解释的强化学习（RL）交易代理。交易环境被建模为一个离散时间马尔可夫决策过程（MDP），其时间索引为 $t \\in \\{0,1,\\dots,T-1\\}$，行动集为 $\\mathcal{A}=\\{-1,0,1\\}$（分别代表空头、平仓和多头头寸），以及单个风险资产的随机收益过程。\n\n环境动态与特征：\n- 资产的简单收益率 $r_t$ 服从一阶自回归过程：\n$$r_t = \\mu + \\varphi \\, r_{t-1} + \\sigma \\, \\varepsilon_t,$$\n其中 $\\varepsilon_t \\sim \\mathcal{N}(0,1)$ 是独立同分布的，$r_{-1}=0$，且参数为 $\\mu \\in \\mathbb{R}$、$\\varphi \\in \\mathbb{R}$ 和 $\\sigma > 0$。\n- 将收益率的指数移动平均（EMA）$m_t$ 递归定义为\n$$m_t = (1-\\alpha) m_{t-1} + \\alpha r_t,$$\n其中 $m_{-1}=0$ 且平滑参数为 $\\alpha \\in (0,1]$。\n- 在时间 $t$ 的状态特征向量为\n$$\\phi(s_t) = \\begin{bmatrix} 1 \\\\ r_{t-1} \\\\ m_{t-1} \\end{bmatrix},$$\n约定 $r_{-1} = 0$ 且 $m_{-1} = 0$。\n\n行为策略与奖励：\n- 用于生成数据的行为策略与状态无关，并且从 $\\{-1,0,1\\}$ 中均匀随机地抽取 $a_t$。\n- 设 $a_{-1}=0$。时间 $t$ 的单步奖励为\n$$u_t = a_t \\, r_t \\;-\\; c \\, |a_t - a_{t-1}|,$$\n其中 $c \\ge 0$ 是单位交易成本率（以小数而非百分比形式表示）。\n\n折扣回报与线性行动价值模型：\n- 对于给定的折扣因子 $\\gamma \\in [0,1)$ 和视界 $H \\in \\mathbb{N}$，将从 $t$ 开始的截断折扣回报定义为\n$$G_t \\;=\\; \\sum_{k=0}^{K_t-1} \\gamma^k \\, u_{t+k}, \\quad \\text{where } K_t = \\min\\{H,\\, T-t\\}.$$\n- 考虑由 $\\theta \\in \\mathbb{R}^3$ 参数化的线性行动价值函数（线性 $Q$-函数）：\n$$Q_\\theta(s_t,a_t) \\;=\\; \\theta^\\top \\big(a_t \\, \\phi(s_t)\\big).$$\n\n估计目标：\n- 给定由上述行为策略和环境生成的轨迹 $\\{(s_t,a_t,u_t)\\}_{t=0}^{T-1}$，定义正则化经验风险\n$$J(\\theta) \\;=\\; \\sum_{t=0}^{T-1} \\Big(Q_\\theta(s_t,a_t) - G_t\\Big)^2 \\;+\\; \\lambda \\, \\|\\theta\\|_2^2,$$\n其中 $\\lambda \\ge 0$ 是一个正则化参数。假设所有参数都满足保证目标函数为严格凸函数的条件，从而存在唯一的最小值点。\n\n任务：\n- 对于每个测试用例（下面列出的参数集），使用指定的随机种子模拟环境和行为策略，以生成高斯冲击 $\\varepsilon_t$ 和随机行动 $a_t$。\n- 计算特征 $\\phi(s_t)$、奖励 $u_t$ 和折扣回报 $G_t$。\n- 计算该测试用例中 $J(\\theta)$ 的唯一最小值点 $\\hat{\\theta}$。\n- 对于每个测试用例，您的程序必须输出学习到的系数向量 $\\hat{\\theta}$，其形式为实数列表 $[\\hat{\\theta}_0,\\hat{\\theta}_1,\\hat{\\theta}_2]$。按相同顺序将所有测试用例的结果聚合到单个列表中。\n\n测试套件（四个用例以确保覆盖率）：\n- 用例 1（基线）：\n  - $T=200$, $\\mu=0$, $\\varphi=0.1$, $\\sigma=0.02$, $\\alpha=0.2$, $\\lambda=0.1$, $\\gamma=0.95$, $c=0.0005$, $H=50$, seed $=42$。\n- 用例 2（短视，无正则化）：\n  - $T=200$, $\\mu=0$, $\\varphi=0$, $\\sigma=0.02$, $\\alpha=0.2$, $\\lambda=0$, $\\gamma=0$, $c=0.0005$, $H=1$, seed $=7$。\n- 用例 3（高正则化）：\n  - $T=200$, $\\mu=0$, $\\varphi=0.3$, $\\sigma=0.02$, $\\alpha=0.2$, $\\lambda=10$, $\\gamma=0.9$, $c=0.001$, $H=50$, seed $=123$。\n- 用例 4（高交易成本）：\n  - $T=200$, $\\mu=0$, $\\varphi=0.1$, $\\sigma=0.02$, $\\alpha=0.2$, $\\lambda=0.1$, $\\gamma=0.95$, $c=0.01$, $H=50$, seed $=99$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，每个元素是对应一个测试用例的三维列表，顺序与上文相同。例如，一个可接受的格式为\n$$\\big[\\,[\\hat{\\theta}_{0}^{(1)},\\hat{\\theta}_{1}^{(1)},\\hat{\\theta}_{2}^{(1)}],\\;[\\hat{\\theta}_{0}^{(2)},\\hat{\\theta}_{1}^{(2)},\\hat{\\theta}_{2}^{(2)}],\\;[\\hat{\\theta}_{0}^{(3)},\\hat{\\theta}_{1}^{(3)},\\hat{\\theta}_{2}^{(3)}],\\;[\\hat{\\theta}_{0}^{(4)},\\hat{\\theta}_{1}^{(4)},\\hat{\\theta}_{2}^{(4)}]\\big].$$\n- 所有数值均需以实数形式打印。本问题不涉及任何单位或角度。作为输入参数出现的百分比，应如上文所述，以小数形式提供和处理。", "solution": "该问题要求我们找到最小化正则化经验风险函数 $J(\\theta)$ 的参数向量 $\\hat{\\theta}$。这是统计学习中的一个标准问题，具体来说，它采用了岭回归（Ridge Regression）的形式。\n\n需要最小化的目标函数由下式给出：\n$$J(\\theta) = \\sum_{t=0}^{T-1} \\Big(Q_\\theta(s_t,a_t) - G_t\\Big)^2 + \\lambda \\, \\|\\theta\\|_2^2$$\n其中线性行动价值函数为 $Q_\\theta(s_t,a_t) = \\theta^\\top \\big(a_t \\, \\phi(s_t)\\big)$。\n\n该目标函数可以更方便地用矩阵-向量符号表示。设 $d=3$ 为参数向量 $\\theta$ 的维度。我们定义一个大小为 $T \\times d$ 的设计矩阵 $X$ 和一个大小为 $T \\times 1$ 的目标向量 $y$。$X$ 的第 $t$ 行（记作 $X_t$）对应于时间 $t$ 预测的特征向量：\n$$X_t = \\big(a_t \\, \\phi(s_t)\\big)^\\top = a_t \\begin{bmatrix} 1 & r_{t-1} & m_{t-1} \\end{bmatrix}$$\n目标向量 $y$ 由采样的截断折扣回报组成：\n$$y = \\begin{bmatrix} G_0 \\\\ G_1 \\\\ \\vdots \\\\ G_{T-1} \\end{bmatrix}$$\n根据这些定义，目标函数 $J(\\theta)$ 可以重写为岭回归的标准形式：\n$$J(\\theta) = \\|X\\theta - y\\|_2^2 + \\lambda \\|\\theta\\|_2^2 = (X\\theta - y)^\\top(X\\theta - y) + \\lambda \\theta^\\top\\theta$$\n此函数 $J(\\theta)$ 是凸函数。问题陈述保证了满足存在唯一最小值点的条件。为找到这个最小值点 $\\hat{\\theta}$，我们计算 $J(\\theta)$ 相对于 $\\theta$ 的梯度，并将其设为零：\n$$\\nabla_\\theta J(\\theta) = \\frac{\\partial}{\\partial \\theta} \\left( \\theta^\\top X^\\top X \\theta - 2y^\\top X \\theta + y^\\top y + \\lambda \\theta^\\top \\theta \\right)$$\n$$\\nabla_\\theta J(\\theta) = 2 X^\\top X \\theta - 2 X^\\top y + 2 \\lambda I \\theta = 0$$\n其中 $I$ 是 $d \\times d$ 的单位矩阵。\n重新整理各项，我们得到正规方程组：\n$$(X^\\top X + \\lambda I) \\theta = X^\\top y$$\n唯一的最小值点 $\\hat{\\theta}$ 是这个线性方程组的解。\n\n为每个测试用例找到 $\\hat{\\theta}$ 的计算过程如下：\n1.  **数据生成**：我们首先根据为 $t \\in \\{0, 1, \\dots, T-1\\}$ 指定的动态来模拟时间序列数据。\n    - 使用给定的种子初始化随机数生成器，以确保可复现性。\n    - 生成两个随机序列：用于收益过程的高斯冲击 $\\{\\varepsilon_t\\}_{t=0}^{T-1}$（来自 $\\mathcal{N}(0,1)$）和行动序列 $\\{a_t\\}_{t=0}^{T-1}$（来自 $\\{-1, 0, 1\\}$ 上的均匀分布）。\n    - 使用 AR(1) 过程迭代生成资产收益率 $r_t$：$r_t = \\mu + \\varphi r_{t-1} + \\sigma \\varepsilon_t$，初始条件为 $r_{-1}=0$。\n    - 迭代生成收益率的指数移动平均 (EMA) $m_t$：$m_t = (1-\\alpha) m_{t-1} + \\alpha r_t$，初始条件为 $m_{-1}=0$。\n    - 计算单步奖励 $u_t$ 为 $u_t = a_t r_t - c|a_t - a_{t-1}|$，初始条件为 $a_{-1}=0$。\n\n2.  **目标与特征构建**：利用模拟出的轨迹，我们构建回归问题的各个组成部分。\n    - 对于每个时间步 $t \\in \\{0, \\dots, T-1\\}$，使用公式 $G_t = \\sum_{k=0}^{K_t-1} \\gamma^k u_{t+k}$（其中 $K_t = \\min(H, T-t)$）计算目标值 $G_t$（截断折扣回报）。\n    - 构建 $T \\times 3$ 的设计矩阵 $X$，其第 $t$ 行由 $a_t [1, r_{t-1}, m_{t-1}]$ 给出。\n    - 形成 $T \\times 1$ 的目标向量 $y$，其第 $t$ 个元素为 $G_t$。\n\n3.  **求解 $\\hat{\\theta}$**：通过求解线性方程组 $(X^\\top X + \\lambda I) \\hat{\\theta} = X^\\top y$ 来找到最优参数向量 $\\hat{\\theta}$。这一步通过使用标准的线性代数求解器以数值方式完成，该方法通常比直接计算矩阵逆更稳定。结果向量 $\\hat{\\theta} = [\\hat{\\theta}_0, \\hat{\\theta}_1, \\hat{\\theta}_2]^\\top$构成了线性行动价值函数逼近的学习系数。\n\n对四个指定的测试用例中的每一個都执行这整个过程，从而产生四个不同的系数向量。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n\n    def compute_theta(T, mu, phi_r, sigma, alpha, lam, gam, c, H, seed):\n        \"\"\"\n        Computes the optimal theta_hat for a given set of parameters.\n\n        This function implements the three-step procedure:\n        1. Data Generation: Simulates returns, EMAs, actions, and rewards.\n        2. Target and Feature Construction: Computes discounted returns (G_t) and\n           builds the design matrix (X) and target vector (y).\n        3. Solving for theta_hat: Solves the Ridge Regression normal equations.\n        \"\"\"\n        # 1. Data Generation\n        rng = np.random.default_rng(seed)\n        \n        # Generate random shocks and actions for the entire trajectory\n        epsilons = rng.standard_normal(T)\n        actions_random = rng.choice([-1, 0, 1], size=T)\n        \n        # History arrays: index t corresponds to time t-1.\n        # Size T+1 to hold values from t=-1 to t=T-1.\n        r_hist = np.zeros(T + 1)  # r_hist[t] = r_{t-1}\n        m_hist = np.zeros(T + 1)  # m_hist[t] = m_{t-1}\n        a_hist = np.zeros(T + 1)  # a_hist[t] = a_{t-1}\n\n        # Reward vector: index t corresponds to time t.\n        u_vec = np.zeros(T)\n        \n        # Time-stepping simulation from t=0 to T-1\n        for t in range(T):\n            r_prev = r_hist[t]\n            m_prev = m_hist[t]\n            a_prev = a_hist[t]\n\n            # Calculate r_t, m_t, a_t at time t\n            r_t = mu + phi_r * r_prev + sigma * epsilons[t]\n            m_t = (1 - alpha) * m_prev + alpha * r_t\n            a_t = actions_random[t]\n            \n            # Calculate reward u_t\n            u_t = a_t * r_t - c * np.abs(a_t - a_prev)\n            \n            # Store new values in history arrays\n            r_hist[t + 1] = r_t\n            m_hist[t + 1] = m_t\n            a_hist[t + 1] = a_t\n            u_vec[t] = u_t\n            \n        # 2. Target and Feature Construction\n        \n        # Compute truncated discounted returns G_t\n        G_vec = np.zeros(T)\n        gam_powers = np.power(gam, np.arange(H))\n        \n        for t in range(T):\n            K_t = min(H, T - t)\n            # Sum of discounted future rewards\n            G_vec[t] = np.sum(gam_powers[:K_t] * u_vec[t : t + K_t])\n            \n        # Construct design matrix X and target vector y\n        d = 3  # Dimension of theta\n        X = np.zeros((T, d))\n        y = G_vec\n        \n        for t in range(T):\n            # State features are from time t-1\n            r_prev = r_hist[t]\n            m_prev = m_hist[t]\n            phi_st = np.array([1.0, r_prev, m_prev])\n            \n            # Action is from time t\n            a_t = a_hist[t + 1]\n            \n            # The t-th row of the design matrix\n            X[t, :] = a_t * phi_st\n            \n        # 3. Solving for theta_hat\n        \n        # Formulate the normal equations: A * theta = b\n        A = X.T @ X + lam * np.identity(d)\n        b = X.T @ y\n        \n        # Solve the linear system for theta\n        theta_hat = np.linalg.solve(A, b)\n        \n        return theta_hat.tolist()\n\n    # Test suite (four cases to ensure coverage):\n    # T, mu, phi, sigma, alpha, lambda, gamma, c, H, seed\n    test_cases = [\n        # Case 1 (baseline):\n        (200, 0.0, 0.1, 0.02, 0.2, 0.1, 0.95, 0.0005, 50, 42),\n        # Case 2 (myopic, no regularization):\n        (200, 0.0, 0.0, 0.02, 0.2, 0.0, 0.0, 0.0005, 1, 7),\n        # Case 3 (high regularization):\n        (200, 0.0, 0.3, 0.02, 0.2, 10.0, 0.9, 0.001, 50, 123),\n        # Case 4 (high transaction costs):\n        (200, 0.0, 0.1, 0.02, 0.2, 0.1, 0.95, 0.01, 50, 99),\n    ]\n\n    all_results = []\n    for params in test_cases:\n        # Unpack parameters and call the computation function\n        T, mu, phi_r, sigma, alpha, lam, gam, c, H, seed = params\n        theta_hat = compute_theta(T, mu, phi_r, sigma, alpha, lam, gam, c, H, seed)\n        all_results.append(theta_hat)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"}]}