{"hands_on_practices": [{"introduction": "主成分分析（PCA）通过寻找最大化数据方差的方向来工作。这意味着，如果变量的单位或尺度相差悬殊（例如，以美元计的股价与以百万股计的交易量），方差较大的变量将不成比例地主导第一个主成分，但这可能只是度量单位造成的假象。这个练习 [@problem_id:2421735] 将通过一个具体的计算任务，让你亲手验证不进行标准化如何扭曲PCA的结果，并理解为什么在实践中对数据进行标准化是至关重要的一步。", "id": "2421735", "problem": "您需要使用主成分分析（PCA）的基本原理，论证在变量以不同单位度量时，若未能进行标准化，会如何扭曲估计出的主方向和解释方差。您将在一个纯数学框架下，通过一个模拟典型金融变量（如价格和交易量）的合成数据生成过程来完成此项工作。您需要实现完整的处理流程，并报告量化诊断指标，以比较对原始数据与对标准化数据执行PCA的结果。\n\n基本原理：\n- PCA旨在寻找使样本方差最大化的标准正交方向。对于一个中心化的数据矩阵 $X \\in \\mathbb{R}^{T \\times n}$，样本协方差矩阵为 $\\Sigma = \\frac{1}{T-1} X^\\top X$。主成分是 $\\Sigma$ 的特征向量，按其对应特征值从大到小排序。\n- 标准化将每个变量 $x_j$ 变换为 $\\tilde{x}_j = \\frac{x_j - \\bar{x}_j}{\\hat{\\sigma}_j}$，其中 $\\bar{x}_j$ 是样本均值，$\\hat{\\sigma}_j$ 是样本标准差，从而使得每个标准化后的变量都具有单位样本方差。对标准化数据进行PCA等同于对样本相关系数矩阵进行PCA。\n- 将对角缩放 $D = \\operatorname{diag}(s_1,\\dots,s_n)$ 应用于变量，$X \\mapsto X D$，会将协方差矩阵的元素乘以 $s_i s_j$，从而改变特征向量，除非所有 $s_j$ 都相等。\n\n数据生成过程：\n- 对于每个测试用例 $k$，固定样本量 $T_k \\in \\mathbb{N}$、变量数 $n_k \\in \\mathbb{N}$、因子载荷 $b^{(k)} \\in \\mathbb{R}^{n_k}$、异质性标准差 $u^{(k)} \\in \\mathbb{R}^{n_k}$ 以及单位尺度 $s^{(k)} \\in \\mathbb{R}^{n_k}$。\n- 对于 $t = 1,\\dots,T_k$，生成单一共同因子 $f_t \\sim \\mathcal{N}(0,1)$，并生成异质性噪声 $e_{t,j} \\sim \\mathcal{N}(0,(u^{(k)}_j)^2)$，所有因子和噪声在 $t$ 和 $j$ 上相互独立。\n- 对于 $t=1,\\dots,T_k$ 和 $j=1,\\dots,n_k$，构建原始观测值 $x_{t,j} = s^{(k)}_j \\left( b^{(k)}_j f_t + e_{t,j} \\right)$。\n- 在计算任何协方差之前，通过减去其样本均值来中心化 $X$ 的每一列。\n\n各测试用例的计算任务：\n- 从中心化的原始数据 $X$ 计算样本协方差矩阵 $\\Sigma_{\\text{raw}}$，并获取第一个主成分的单位范数特征向量 $v_{\\text{raw}}$ 及其特征值 $\\lambda_{\\text{raw}}$。\n- 将 $X$ 的每一列标准化为单位样本方差以得到 $Z$，计算 $\\Sigma_{\\text{std}} = \\frac{1}{T_k-1} Z^\\top Z$（即样本相关系数矩阵），并获取第一个主成分的单位范数特征向量 $v_{\\text{std}}$ 及其特征值 $\\lambda_{\\text{std}}$。\n- 计算角度 $\\theta = \\arccos\\!\\left( \\left| v_{\\text{raw}}^\\top v_{\\text{std}} \\right| \\right)$；以弧度为单位报告 $\\theta$。\n- 计算解释方差贡献率的差异 $\\Delta = \\left| \\frac{\\lambda_{\\text{raw}}}{\\operatorname{tr}(\\Sigma_{\\text{raw}})} - \\frac{\\lambda_{\\text{std}}}{\\operatorname{tr}(\\Sigma_{\\text{std}})} \\right|$，该值必须以小数形式报告（而非百分比）。\n\n随机性与可复现性：\n- 在整个实验中使用固定的伪随机数生成器种子 $314159$，以确保结果可复现。\n\n测试套件：\n- 共有 $3$ 个测试用例。对每个测试用例 $k$，使用以下参数 $(T_k, n_k, b^{(k)}, u^{(k)}, s^{(k)})$：\n  - 用例 $1$（单位相似，两个变量）：\n    - $T_1 = 500$, $n_1 = 2$,\n    - $b^{(1)} = [1.0, 0.9]$,\n    - $u^{(1)} = [0.1, 0.1]$,\n    - $s^{(1)} = [1.0, 1.2]$.\n  - 用例 $2$（单位不匹配，两个变量：一个因尺度而占主导）：\n    - $T_2 = 500$, $n_2 = 2$,\n    - $b^{(2)} = [1.0, 0.9]$,\n    - $u^{(2)} = [0.1, 0.1]$,\n    - $s^{(2)} = [1000.0, 1.0]$.\n  - 用例 $3$（单位不匹配，三个变量：一个巨大尺度，一个中等尺度，一个微小尺度）：\n    - $T_3 = 800$, $n_3 = 3$,\n    - $b^{(3)} = [0.2, 1.0, 1.0]$,\n    - $u^{(3)} = [0.3, 0.2, 0.2]$,\n    - $s^{(3)} = [1000.0, 1.0, 0.01]$.\n\n各测试用例的必需输出：\n- 一个包含两个浮点数的列表 $[\\theta, \\Delta]$，其中 $\\theta$ 是以弧度为单位的角度，$\\Delta$ 是解释方差贡献率的绝对差。两个值都必须精确到 $6$ 位小数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个由各用例列表组成的逗号分隔列表，并用方括号括起来，例如 $[[\\theta_1,\\Delta_1],[\\theta_2,\\Delta_2],[\\theta_3,\\Delta_3]]$，每个浮点数都精确到 $6$ 位小数，角度以弧度为单位。", "solution": "所提出的问题是计算统计学中一个有效且定义明确的练习，旨在具体论证主成分分析（PCA）对变量尺度的敏感性。它在科学上是合理的，基于线性代数和统计学的基础原则，并且所有参数和步骤都得到了足够清晰的说明，从而可以得出一个唯一的、可验证的解。我们将继续进行分析。\n\n其核心论点是，PCA作为一种方差最大化技术，不具备尺度不变性。当变量以迥异的单位度量时（例如，以美元计的股价与以百万股计的交易量），方差最大的变量将在机制上主导第一个主成分。这通常是所选单位造成的人为结果，而非真实潜在重要性的指标。标准化是标准的修正方法，它将所有变量转换到同一尺度（单位方差），从而使分析聚焦于数据的相关性结构，而非任意的度量尺度。\n\n我们首先将数据生成和分析流程形式化。\n\n**1. 数据生成过程**\n\n对于每个测试用例 $k$，我们给定样本量 $T_k$、变量数 $n_k$、因子载荷 $b^{(k)} \\in \\mathbb{R}^{n_k}$、异质性标准差 $u^{(k)} \\in \\mathbb{R}^{n_k}$ 以及单位尺度 $s^{(k)} \\in \\mathbb{R}^{n_k}$。\n\n数据由单因子模型生成。对于每个时间点 $t=1, \\dots, T_k$，从标准正态分布中抽取一个共同的潜在因子 $f_t \\sim \\mathcal{N}(0, 1)$。对于每个变量 $j=1, \\dots, n_k$，从 $\\mathcal{N}(0, (u^{(k)}_j)^2)$ 中抽取一个异质性噪声项 $e_{t,j}$。所有 $f_t$ 和 $e_{t,j}$ 相互独立。\n\n变量 $j$ 在时间 $t$ 的观测值构造如下：\n$$\nx_{t,j} = s^{(k)}_j \\left( b^{(k)}_j f_t + e_{t,j} \\right)\n$$\n这就构成了一个数据矩阵 $X \\in \\mathbb{R}^{T_k \\times n_k}$，其列代表不同的变量。尺度因子 $s^{(k)}_j$ 代表变量 $j$ 的任意度量单位。\n\n**2. 对原始数据进行PCA（基于协方差的PCA）**\n\nPCA的第一步是通过减去列向样本均值来中心化数据。令 $\\bar{x}_j = \\frac{1}{T_k} \\sum_{t=1}^{T_k} x_{t,j}$ 为第 $j$ 个变量的样本均值。中心化后的数据矩阵记为 $X_c$，其元素为 $(X_c)_{t,j} = x_{t,j} - \\bar{x}_j$。\n\n然后计算样本协方差矩阵 $\\Sigma_{\\text{raw}}$：\n$$\n\\Sigma_{\\text{raw}} = \\frac{1}{T_k-1} X_c^\\top X_c\n$$\n主成分是 $\\Sigma_{\\text{raw}}$ 的特征向量。我们对该矩阵进行特征分解：\n$$\n\\Sigma_{\\text{raw}} V = V \\Lambda\n$$\n其中 $V$ 是标准正交特征向量矩阵，$\\Lambda$ 是相应特征值的对角矩阵。特征值按降序排列，$\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_{n_k}$。第一个主成分是与最大特征值 $\\lambda_1$ 相关联的特征向量 $v_1$。对于本问题，我们将此特征向量记为 $v_{\\text{raw}}$，特征值记为 $\\lambda_{\\text{raw}}$。\n\n**3. 对标准化数据进行PCA（基于相关系数的PCA）**\n\n为消除任意尺度的影响，我们对数据进行标准化。对于原始数据矩阵 $X$ 的每一列 $j$，我们计算其样本标准差，$\\hat{\\sigma}_j = \\sqrt{\\frac{1}{T_k-1} \\sum_{t=1}^{T_k} (x_{t,j} - \\bar{x}_j)^2}$。\n\n标准化的数据矩阵 $Z$ 的元素构造如下：\n$$\nz_{t,j} = \\frac{x_{t,j} - \\bar{x}_j}{\\hat{\\sigma}_j}\n$$\n根据构造，$Z$ 的每一列的样本均值为 $0$，样本方差为 $1$。\n\n然后对这个标准化的数据 $Z$ 进行PCA。相关矩阵是 $Z$ 的样本协方差矩阵，我们记为 $\\Sigma_{\\text{std}}$：\n$$\n\\Sigma_{\\text{std}} = \\frac{1}{T_k-1} Z^\\top Z\n$$\n由于 $Z$ 的每一列都具有单位方差，$\\Sigma_{\\text{std}}$ 的对角元素全为 $1$，而非对角元素 $(i, j)$ 是原始变量 $x_i$ 和 $x_j$ 之间的样本相关系数。因此，$\\Sigma_{\\text{std}}$ 是 $X$ 的样本相关系数矩阵。\n\n我们对 $\\Sigma_{\\text{std}}$ 进行特征分解，以找到其最大特征值 $\\lambda_{\\text{std}}$ 和相应的特征向量 $v_{\\text{std}}$。\n\n**4. 诊断指标**\n\n为量化因未标准化而造成的扭曲，我们计算两个指标：\n\n- **主成分之间的夹角**：主成分方向 $v_{\\text{raw}}$ 和 $v_{\\text{std}}$ 是 $\\mathbb{R}^{n_k}$ 中的单位向量。它们之间的夹角衡量了最大方差方向的偏移程度。由于特征向量的定义仅精确到符号（即，若 $v$ 是一个特征向量，则 $-v$ 也是），我们计算它们所张成直线之间的锐角：\n  $$\n  \\theta = \\arccos\\left( \\left| v_{\\text{raw}}^\\top v_{\\text{std}} \\right| \\right)\n  $$\n  $\\theta=0$ 的值表示完全对齐，而一个大的角度（接近 $\\pi/2$）则表示严重错位。\n\n- **解释方差贡献率的差异**：第一个主成分所解释的总方差比例由其特征值除以所有特征值之和得出。特征值之和等于矩阵的迹，$\\operatorname{tr}(\\Sigma) = \\sum_{j=1}^{n_k} \\Sigma_{jj}$，它代表数据中的总方差。我们计算解释方差贡献率的绝对差异：\n  $$\n  \\Delta = \\left| \\frac{\\lambda_{\\text{raw}}}{\\operatorname{tr}(\\Sigma_{\\text{raw}})} - \\frac{\\lambda_{\\text{std}}}{\\operatorname{tr}(\\Sigma_{\\text{std}})} \\right|\n  $$\n  请注意，对于标准化数据，$\\operatorname{tr}(\\Sigma_{\\text{std}}) = n_k$，即变量的数量。一个大的 $\\Delta$ 值表明两种方法对第一个主成分重要性的评估大相径庭。\n\n该过程将对每个测试用例执行，使用指定的参数和固定的随机种子以保证可复现性。预计结果将显示，用例1（尺度相似）的扭曲最小，而用例2和用例3（尺度迥异）的扭曲显著，从而验证了在实践中进行标准化的必要性。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PCA problem by comparing results from raw and standardized data.\n\n    This function iterates through a set of predefined test cases. For each case, it:\n    1. Generates synthetic financial data based on a single-factor model with specified scales.\n    2. Performs PCA on the raw, centered data.\n    3. Performs PCA on the standardized data (equivalent to using the correlation matrix).\n    4. Computes two diagnostic metrics:\n        - The angle between the first principal components from the raw and standardized analyses.\n        - The absolute difference in the fraction of variance explained by the first component.\n    5. Collects and formats the results according to the problem specification.\n    \"\"\"\n    # Use a fixed pseudorandom number generator seed for reproducibility.\n    seed = 314159\n    rng = np.random.default_rng(seed)\n\n    # Test cases defined as (T_k, n_k, b^(k), u^(k), s^(k)))\n    test_cases = [\n        (500, 2, np.array([1.0, 0.9]), np.array([0.1, 0.1]), np.array([1.0, 1.2])),\n        (500, 2, np.array([1.0, 0.9]), np.array([0.1, 0.1]), np.array([1000.0, 1.0])),\n        (800, 3, np.array([0.2, 1.0, 1.0]), np.array([0.3, 0.2, 0.2]), np.array([1000.0, 1.0, 0.01]))\n    ]\n\n    all_results = []\n\n    for T, n, b, u, s in test_cases:\n        # 1. Data Generation\n        # Generate common factor f_t ~ N(0,1)\n        f = rng.normal(loc=0.0, scale=1.0, size=T)\n        \n        # Generate idiosyncratic noises e_{t,j} ~ N(0, u_j^2)\n        # E is a T x n matrix\n        E = rng.normal(loc=0.0, scale=u, size=(T, n))\n        \n        # Construct raw observations x_{t,j} = s_j * (b_j * f_t + e_{t,j})\n        X = s * (np.outer(f, b) + E)\n\n        # 2. PCA on Raw Data\n        # Center the data matrix X\n        X_centered = X - np.mean(X, axis=0)\n        \n        # Compute the sample covariance matrix (ddof=1 for unbiased estimator)\n        Sigma_raw = np.cov(X_centered, rowvar=False, ddof=1)\n        \n        # Eigendecomposition of the symmetric covariance matrix\n        # eigh returns eigenvalues in ascending order and corresponding eigenvectors in columns\n        eigvals_raw, eigvecs_raw = np.linalg.eigh(Sigma_raw)\n        \n        # The first principal component corresponds to the largest eigenvalue\n        lambda_raw = eigvals_raw[-1]\n        v_raw = eigvecs_raw[:, -1]\n\n        # 3. PCA on Standardized Data\n        # Standardize the data matrix X\n        stds = np.std(X, axis=0, ddof=1)\n        Z = X_centered / stds\n        \n        # Compute the sample correlation matrix (covariance of standardized data)\n        Sigma_std = np.cov(Z, rowvar=False, ddof=1)\n        \n        # Eigendecomposition of the correlation matrix\n        eigvals_std, eigvecs_std = np.linalg.eigh(Sigma_std)\n        \n        # The first principal component corresponds to the largest eigenvalue\n        lambda_std = eigvals_std[-1]\n        v_std = eigvecs_std[:, -1]\n\n        # 4. Diagnostic Computations\n        # Angle theta between the first principal components\n        # Take absolute value of dot product to handle sign ambiguity of eigenvectors\n        cos_theta = np.abs(np.dot(v_raw, v_std))\n        # Clip to prevent domain errors with arccos due to potential floating point inaccuracies\n        theta = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n\n        # Difference in explained variance shares\n        total_var_raw = np.trace(Sigma_raw)\n        total_var_std = np.trace(Sigma_std) # This is always n for a correlation matrix\n        \n        share_raw = lambda_raw / total_var_raw\n        share_std = lambda_std / total_var_std\n        \n        delta = np.abs(share_raw - share_std)\n\n        # Append results, rounded to 6 decimal places\n        all_results.append([round(theta, 6), round(delta, 6)])\n\n    # Format the final output string as specified\n    formatted_results = [f\"[{theta:.6f},{delta:.6f}]\" for theta, delta in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"}, {"introduction": "除了对变量尺度敏感之外，经典的PCA对异常值也同样非常敏感。由于其最大化方差的目标，一个极端的数据点就可能“绑架”第一个主成分，使其不再反映数据的整体结构，而仅仅指向这个异常点。在这个练习 [@problem_id:2421778] 中，你将构建一个合成数据集，亲眼见证一个精心设计的异常值如何完全主导主成分分析的结果，从而深刻理解在应用PCA之前进行数据清洗或采用稳健方法的重要性。", "id": "2421778", "problem": "您的任务是，基于第一性原理，构建合成的横截面资产回报面板，其中主成分分析（PCA）的第一主成分（PC）由单个极端观测值驱动。考虑一个具有 $T$ 个时间点和 $N$ 个资产的面板。令 $X \\in \\mathbb{R}^{T \\times N}$ 表示数据矩阵，其行是时间点，列是资产。通过从 $X$ 的每一列中减去其样本均值，定义中心化矩阵 $X_c$。定义第一主成分（PC1）为任意单位向量 $v_1 \\in \\mathbb{R}^N$，该向量能最大化投影数据的样本方差，即 $v_1 \\in \\arg\\max_{\\|v\\|=1} \\mathrm{Var}(X_c v)$，其中 $\\mathrm{Var}(X_c v)$ 表示标量时间序列 $X_c v$ 的样本方差。定义 PC1 的方差解释比为 $\\rho_1 = \\dfrac{\\mathrm{Var}(X_c v_1)}{\\sum_{j=1}^N \\mathrm{Var}(X_c e_j)}$，其中 $\\{e_j\\}$ 是 $\\mathbb{R}^N$ 中的标准基向量。\n\n$X$ 的构建过程如下。对于给定的整数 $N \\ge 2$、$T \\ge 3$、非负振幅 $A \\ge 0$、时间索引 $t^\\star \\in \\{0,1,\\dots,T-1\\}$ 以及等于第一个标准基向量 $e_1$ 的单位方向 $u \\in \\mathbb{R}^N$，令 $Z \\in \\mathbb{R}^{T \\times N}$ 的条目是独立的、服从均值为零、方差为一的高斯分布的变量。通过设置 $X = Z$ 来定义 $X$，然后仅通过 $X_{t^\\star,\\cdot} \\leftarrow X_{t^\\star,\\cdot} + A u^\\top$ 修改由 $t^\\star$ 索引的单行。行索引 $t^\\star$ 从零开始计数。\n\n对于每个指定的参数元组，您的程序必须：\n- 使用给定的 $(N,T,A,t^\\star)$ 按上述方式构建 $X$。\n- 计算包含离群值的数据集的 PC1 及其方差解释比 $\\rho_1$。\n- 移除离群观测值（从 $X$ 中删除索引为 $t^\\star$ 的行），通过列均值重新中心化剩余数据，重新计算 PC1，并计算缩减后数据集的方差解释比 $\\tilde{\\rho}_1$。\n- 计算包含离群值数据集的 PC1 载荷向量与方向 $u$ 之间的绝对对齐度，即 $\\alpha = |\\langle v_1, u \\rangle|$。\n- 返回一个布尔值，指示以下所有优势条件是否同时成立：$\\rho_1 > \\tau_{\\mathrm{high}}$，$\\tilde{\\rho}_1 < \\tau_{\\mathrm{low}}$ 和 $\\alpha > \\tau_{\\mathrm{align}}$，其中 $(\\tau_{\\mathrm{high}}, \\tau_{\\mathrm{low}}, \\tau_{\\mathrm{align}})$ 是下面测试套件中提供的阈值。\n\n使用以下参数值测试套件，其中每个案例都是一个元组 $(N,T,A,t^\\star,\\tau_{\\mathrm{high}},\\tau_{\\mathrm{low}},\\tau_{\\mathrm{align}},\\text{seed})$：\n- 案例 A（理想路径，强离群值）：$(5, 200, 50, 100, 0.6, 0.35, 0.95, 20231102)$。\n- 案例 B（边界幅度离群值）：$(5, 200, 12, 150, 0.28, 0.27, 0.90, 20231103)$。\n- 案例 C（边缘案例，无离群值）：$(8, 400, 0, 50, 0.25, 0.20, 0.90, 20231104)$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。对于上述三个案例，输出格式必须与“[bA,bB,bC]”完全一致，其中 $bA$、$bB$ 和 $bC$ 分别是对应案例 A、B 和 C 的布尔值。", "solution": "该问题要求分析第一主成分对合成横截面数据集中单个显著离群值的敏感性。我们必须首先验证问题的完整性。该问题定义明确，其科学基础在于线性代数和统计学原理，并且所有参数都已指定。它提出了一个稳健统计学中的标准计算任务。因此，该问题是有效的，我们可以继续进行求解。\n\n问题的核心在于应用主成分分析（PCA），这是一种降维技术，用于识别数据集中方差最大的方向。设 $X \\in \\mathbb{R}^{T \\times N}$ 为数据矩阵，其中 $T$ 代表时间点，$N$ 代表资产。PCA 的第一步是通过减去每列（资产）的均值来中心化数据。这就得到了中心化矩阵 $X_c$。\n\n第一主成分（PC1）被定义为载荷向量 $v_1 \\in \\mathbb{R}^N$，它是一个单位向量，能最大化数据在其上投影的方差。在数学上，这表示为：\n$$\nv_1 = \\arg\\max_{\\|v\\|=1} \\mathrm{Var}(X_c v)\n$$\n长度为 $T$ 的投影时间序列 $y = X_c v$ 的样本方差由 $\\mathrm{Var}(y) = \\frac{1}{T-1} \\sum_{i=1}^T (y_i - \\bar{y})^2$ 给出。由于 $X_c$ 是中心化的，任何投影 $X_c v$ 的均值都为零。因此，方差表达式得以简化，从而引出以下优化问题：\n$$\nv_1 = \\arg\\max_{\\|v\\|=1} \\frac{1}{T-1} (X_c v)^\\top (X_c v) = \\arg\\max_{\\|v\\|=1} v^\\top \\left(\\frac{1}{T-1} X_c^\\top X_c\\right) v = \\arg\\max_{\\|v\\|=1} v^\\top S v\n$$\n其中 $S = \\frac{1}{T-1} X_c^\\top X_c$ 是资产的样本协方差矩阵。根据瑞利商定理，最大化 $v^\\top S v$ 的向量 $v_1$ 是 $S$ 对应其最大特征值 $\\lambda_1$ 的特征向量。该特征向量构成了第一主成分的载荷向量。\n\nPC1 的方差解释比（记为 $\\rho_1$）是第一主成分所捕获的方差占总方差的比例。总方差是所有资产方差的总和，等于协方差矩阵的迹，即 $\\mathrm{Tr}(S)$。PC1 捕获的方差是 $\\lambda_1$。因此，\n$$\n\\rho_1 = \\frac{\\lambda_1}{\\mathrm{Tr}(S)}\n$$\n在计算上，对协方差矩阵 $S$ 进行特征分解可能存在数值不稳定的问题，尤其是在处理病态数据时。一种更稳健的方法是使用中心化数据矩阵 $X_c$ 的奇异值分解（SVD）。设 $X_c$ 的 SVD 为：\n$$\nX_c = U \\Sigma V^\\top\n$$\n其中 $U \\in \\mathbb{R}^{T \\times T}$ 和 $V \\in \\mathbb{R}^{N \\times N}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{T \\times N}$ 的対角线上包含奇异值 $s_k$。$V$ 的列是主成分的载荷向量，因此 $v_1$ 是 $V$ 的第一列。 $S$ 的特征值与 $X_c$ 的奇异值之间的关系为 $\\lambda_k = \\frac{s_k^2}{T-1}$。方差解释比 $\\rho_1$ 可以用奇异值来表示，这样就巧妙地避免了对样本大小项的依赖：\n$$\n\\rho_1 = \\frac{\\lambda_1}{\\sum_{k=1}^N \\lambda_k} = \\frac{s_1^2 / (T-1)}{\\sum_{k=1}^N s_k^2 / (T-1)} = \\frac{s_1^2}{\\sum_{k=1}^N s_k^2}\n$$\n所有计算都将使用这种基于 SVD 的方法。\n\n对于每个给定的案例 $(N, T, A, t^\\star, \\tau_{\\mathrm{high}}, \\tau_{\\mathrm{low}}, \\tau_{\\mathrm{align}}, \\text{seed})$，流程如下：\n\n1.  **数据生成：** 使用指定的随机种子生成一个基准矩阵 $Z \\in \\mathbb{R}^{T \\times N}$，其条目独立同分布于标准正态分布 $\\mathcal{N}(0, 1)$，以确保可复现性。通过在元素 $(t^\\star, 0)$ 处引入一个幅度为 $A$ 的点离群值来形成数据矩阵 $X$，该位置对应于方向 $u = e_1$：\n    $$\n    X = Z, \\quad X_{t^\\star, 0} \\leftarrow Z_{t^\\star, 0} + A\n    $$\n\n2.  **完整数据集分析：**\n    - 通过减去逐列样本均值来中心化矩阵 $X$，得到 $X_c$。\n    - 计算 $X_c$ 的 SVD：$X_c = U \\Sigma V^\\top$。\n    - PC1 载荷向量是 $V$ 的第一列，对应于 $V^\\top$ 的第一行。设其为 $v_1$。\n    - 方差解释比计算为 $\\rho_1 = s_1^2 / \\sum_k s_k^2$。\n    - 与离群值方向 $u=e_1$ 的对齐度计算为 $\\alpha = |\\langle v_1, e_1 \\rangle| = |(v_1)_1|$，即 $v_1$ 第一个元素的绝对值。\n\n3.  **缩减数据集分析：**\n    - 从 $X$ 中删除索引为 $t^\\star$ 的整行离群观测值，形成一个新矩阵 $X' \\in \\mathbb{R}^{(T-1) \\times N}$。\n    - 使用其自身的逐列样本均值对这个缩减后的矩阵 $X'$ 进行重新中心化，得到 $X'_c$。此时观测数量为 $T-1$。\n    - 计算 $X'_c$ 的 SVD。设新的奇异值为 $s'_k$。\n    - 缩减数据集的 PC1 的方差解释比计算为 $\\tilde{\\rho}_1 = (s'_1)^2 / \\sum_k (s'_k)^2$。\n\n4.  **优势条件验证：** 最后一步是评估所有三个指定条件是否同时满足：\n    $$\n    (\\rho_1 > \\tau_{\\mathrm{high}}) \\land (\\tilde{\\rho}_1 < \\tau_{\\mathrm{low}}) \\land (\\alpha > \\tau_{\\mathrm{align}})\n    $$\n    该案例的结果是代表此逻辑表达式结果的布尔值。对所提供的每个测试案例重复这整个过程。实现将严格遵循这些步骤。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases.\n    For each case, it constructs a synthetic dataset with an outlier,\n    analyzes its principal components, re-analyzes the dataset after\n    removing the outlier, and checks if a set of dominance conditions are met.\n    \"\"\"\n    # Test suite format: (N, T, A, t_star, tau_high, tau_low, tau_align, seed)\n    test_cases = [\n        (5, 200, 50, 100, 0.6, 0.35, 0.95, 20231102),  # Case A\n        (5, 200, 12, 150, 0.28, 0.27, 0.90, 20231103),  # Case B\n        (8, 400, 0, 50, 0.25, 0.20, 0.90, 20231104),   # Case C\n    ]\n\n    results = []\n    for case in test_cases:\n        N, T, A, t_star, tau_high, tau_low, tau_align, seed = case\n        \n        # --- Analysis with Outlier ---\n\n        # 1. Generate Data\n        rng = np.random.default_rng(seed)\n        Z = rng.standard_normal(size=(T, N))\n        X = Z.copy()\n        # Add the outlier. u is e_1, so we modify the first column (index 0).\n        if A > 0:\n            X[t_star, 0] += A\n\n        # 2. Center Data\n        X_c = X - X.mean(axis=0)\n\n        # 3. Perform PCA (via SVD) and Calculate Metrics\n        # We use full_matrices=False for efficiency\n        _, s, Vh = np.linalg.svd(X_c, full_matrices=False)\n        \n        # Explained variance ratio rho_1\n        rho_1 = (s[0]**2) / np.sum(s**2)\n        \n        # PC1 loading vector v_1 is the first row of Vh (V transpose)\n        v_1 = Vh[0, :]\n        \n        # Alignment alpha with u = e_1\n        # <v_1, e_1> is just the first element of v_1\n        alpha = np.abs(v_1[0])\n\n        # --- Analysis without Outlier ---\n        \n        # 1. Remove Outlier Row\n        X_prime = np.delete(X, t_star, axis=0)\n        \n        # 2. Re-center Reduced Data\n        X_prime_c = X_prime - X_prime.mean(axis=0)\n        \n        # 3. Perform PCA on Reduced Data\n        _, s_prime, _ = np.linalg.svd(X_prime_c, full_matrices=False)\n        \n        # Explained variance ratio tilde_rho_1\n        tilde_rho_1 = (s_prime[0]**2) / np.sum(s_prime**2)\n\n        # --- Final Check ---\n        \n        # 4. Evaluate Dominance Conditions\n        holds = (rho_1 > tau_high) and (tilde_rho_1 < tau_low) and (alpha > tau_align)\n        results.append(holds)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "在处理高维或大规模金融数据集时，直接计算和存储样本协方差矩阵 $S$ 可能会因为计算成本过高或内存不足而变得不可行。幸运的是，我们有不需要显式构造 $S$ 就能找到主要主成分的算法。这个练习 [@problem_id:2421742] 将指导你使用幂迭代法（power iteration method），这是一种高效的迭代算法，能够以流式处理的方式处理数据，从而在内存受限的情况下估算出最重要的主成分。", "id": "2421742", "problem": "题目给出一个资产收益的数据生成模型，该模型由计算经济学和金融学中的单一潜在市场因子驱动。设 $R \\in \\mathbb{R}^{n \\times d}$ 表示资产收益矩阵，其中 $n$ 是时间周期数，$d$ 是资产数量。行对应时间，列对应资产。数据按如下方式生成：\n$$\nR_{t,\\cdot} \\;=\\; z_t \\, a^\\top \\;+\\; \\varepsilon_{t,\\cdot}, \\quad \\text{for } t \\in \\{1,\\dots,n\\},\n$$\n其中 $z_t \\sim \\mathcal{N}(0,1)$ 是独立同分布的标量因子实现，$a \\in \\mathbb{R}^d$ 是一个固定的载荷向量，且 $\\varepsilon_{t,\\cdot} \\in \\mathbb{R}^d$ 的各分量独立同分布于 $\\mathcal{N}(0,\\sigma^2)$，并独立于 $\\{z_t\\}_{t=1}^n$。列中心化后数据的样本协方差矩阵为：\n$$\nS \\;=\\; \\frac{1}{n-1} \\, X^\\top X,\n$$\n其中 $X$ 是通过使用每列的经验均值对 $R$ 进行列中心化得到的。\n\n目标是仅计算第一主成分载荷向量，即求解一个满足以下条件的单位向量 $v^\\ast \\in \\mathbb{R}^d$：\n$$\nS v^\\ast \\;=\\; \\lambda_1 \\, v^\\ast, \\quad \\|v^\\ast\\|_2 \\;=\\; 1,\n$$\n其中 $\\lambda_1$ 是 $S$ 的最大特征值。您必须假定显式构造 $S$ 是不可行的。您只能通过以数据块（连续的行块）的形式流式访问 $R$ 的行，并且可以对数据进行多轮遍历。您必须使用从数据中计算出的经验均值对列进行中心化。您的程序不得构造 $X^\\top X$，并且必须使用在 $d$ 上线性扩展的内存。\n\n为进行测试，您的程序必须使用上述模型为三个独立的案例在内部生成 $R$，并为每个案例计算两个量：\n1. 计算出的单位向量 $\\hat v$ 与真实载荷向量的单位范数版本 $\\bar a = a / \\|a\\|_2$ 之间的绝对余弦相似度，由下式给出：\n$$\nc \\;=\\; \\left| \\hat v^\\top \\bar a \\right|.\n$$\n2. 第一个主成分的方差解释率，定义为：\n$$\n\\rho \\;=\\; \\frac{\\hat \\lambda_1}{\\operatorname{tr}(S)},\n$$\n其中 $\\hat \\lambda_1$ 通过瑞利商 (Rayleigh quotient) 估计：\n$$\n\\hat \\lambda_1 \\;=\\; \\hat v^\\top S \\hat v \\;=\\; \\frac{1}{n-1} \\, \\| X \\hat v \\|_2^2,\n$$\n$\\operatorname{tr}(S)$ 是 $S$ 的迹，等价于：\n$$\n\\operatorname{tr}(S) \\;=\\; \\frac{1}{n-1} \\, \\| X \\|_F^2.\n$$\n所有范数均为所标示的标准欧几里得范数或弗罗贝尼乌斯范数。\n\n您的程序必须使用下面确切的参数集，以流式方式（不存储完整矩阵）模拟数据。在每个案例中，向量 $a$ 均按指定方式确定性地定义，并且在计算 $c$ 之前必须将其归一化为单位范数。\n\n案例 A（一般良态）：\n- $n = 5000$，$d = 50$，$\\sigma = 0.5$，随机种子 $s = 42$，数据块大小 $m = 250$。\n- 载荷 $a \\in \\mathbb{R}^{50}$ 定义为 $a_i = i$，$i \\in \\{1,\\dots,50\\}$。\n\n案例 B（高维情况，$d \\gt n$）：\n- $n = 120$，$d = 600$，$\\sigma = 0.6$，随机种子 $s = 7$，数据块大小 $m = 64$。\n- 载荷 $a \\in \\mathbb{R}^{600}$ 定义为 $a_i = (-1)^i \\left(0.5 + \\frac{i}{d}\\right)$，$i \\in \\{1,\\dots,600\\}$。\n\n案例 C（小特征值间隙，收敛较慢）：\n- $n = 4000$，$d = 100$，$\\sigma = 0.97$，随机种子 $s = 123$，数据块大小 $m = 200$。\n- 载荷 $a \\in \\mathbb{R}^{100}$ 定义为 $a_i = \\sin\\!\\left(\\frac{2\\pi i}{d}\\right) + 0.1 \\frac{i}{d}$，$i \\in \\{1,\\dots,100\\}$，其中 $\\pi$ 是圆周率。\n\n在所有案例中，随机种子 $s$ 控制 $\\{z_t\\}$ 和噪声 $\\{\\varepsilon_{t,\\cdot}\\}$ 的生成，并且必须确保每次遍历数据时都能复现相同的序列。您最多可以使用固定次数的迭代（请选择合适的值），并使用一个数值公差，在方向稳定时提前终止迭代。如果内部使用了角度，则无需报告。不涉及物理单位。\n\n您的程序应生成单行输出，其中包含一个由逗号分隔的列表组成的列表，每个内部列表按顺序包含案例 A、B 和 C 的两个浮点数 $[c,\\rho]$，并四舍五入到六位小数。例如，输出格式必须严格如下：\n$$\n\\big[ [c_A,\\rho_A], [c_B,\\rho_B], [c_C,\\rho_C] \\big],\n$$\n以单行打印，例如：\n$$\n\\texttt{[[0.999000,0.850000],[0.990000,0.300000],[0.950000,0.600000]]}.\n$$", "solution": "所提出的问题是计算金融领域中一个定义明确且具有科学依据的任务。它要求在内存和数据访问限制下计算样本协方差矩阵的第一主成分，这是大规模统计分析中的一个标准问题。所有参数和条件都以数学精度进行了规定。因此，该问题是有效的，我将着手解决它。\n\n问题的核心是找到样本协方差矩阵 $S = \\frac{1}{n-1} X^\\top X$ 最大特征值 $\\lambda_1$ 对应的单位范数特征向量 $v^\\ast$。此处，$X \\in \\mathbb{R}^{n \\times d}$ 是资产收益的数据矩阵，通过减去列向经验均值进行中心化。关键的约束是，完整的资产收益矩阵 $R \\in \\mathbb{R}^{n \\times d}$ 和矩阵 $X^\\top X \\in \\mathbb{R}^{d \\times d}$ 都不能显式构造或存储在内存中。数据只能通过流式传输连续的行数据块来访问。\n\n这种设定需要使用迭代算法。幂迭代法是寻找矩阵最大模特征值对应特征向量的经典算法。该方法从一个初始随机向量 $v_0$ 开始，通过迭代应用矩阵 $S$ 生成一个向量序列，该序列收敛于主特征向量：\n$$\nv_{k+1} = \\frac{S v_k}{\\|S v_k\\|_2}\n$$\n在迭代过程中，$S$ 定义中的常数因子 $\\frac{1}{n-1}$ 可以省略，因为向量在每一步都会被重新归一化。因此，核心计算是矩阵-向量乘积 $w_k = X^\\top X v_k$。\n\n为了在流式约束下实现这一点，我们必须在不显式构造 $X$ 或 $X^\\top X$ 的情况下执行计算。\n首先，数据必须被中心化。中心化后的数据矩阵是 $X = R - \\mathbf{1}\\mu^\\top$，其中 $\\mathbf{1}$ 是一个 $n \\times 1$ 的全一向量，$\\mu \\in \\mathbb{R}^d$ 是列均值向量。均值计算公式为 $\\mu = \\frac{1}{n} \\sum_{t=1}^n R_{t,\\cdot}$，这需要对数据矩阵 $R$ 进行一次完整的遍历。我们可以计算并存储向量 $\\mu$，因为其内存占用 $O(d)$ 是允许的。\n\n在已知 $\\mu$ 的情况下，乘积 $w_k = X^\\top (X v_k)$ 可以在每次迭代 $k$ 中通过对数据进行单次遍历来计算。该乘积可以表示为对 $X$ 的行 $x_{t,\\cdot}$ 的求和：\n$$\nw_k = X^\\top (X v_k) = \\sum_{t=1}^{n} x_{t,\\cdot}^\\top (x_{t,\\cdot} v_k)\n$$\n在遍历过程中，对于数据流中的每一行 $R_{t,\\cdot}$，我们首先计算中心化后的行 $x_{t,\\cdot} = R_{t,\\cdot} - \\mu$。然后，我们计算标量投影 $p_t = x_{t,\\cdot} v_k$。这个标量用于缩放向量 $x_{t,\\cdot}^\\top$，结果被添加到一个累加器向量中，在遍历结束时，该累加器向量即为所求的向量 $w_k$。特征向量的新估计值为 $v_{k+1} = w_k / \\|w_k\\|_2$。这个迭代过程会重复进行，直到向量 $v_k$ 的变化变得可以忽略不计，例如，当 $1 - |v_k^\\top v_{k-1}| < \\epsilon$（对于某个小的公差 $\\epsilon > 0$）时。\n\n$R$ 的数据生成过程必须在每次遍历时都可复现。这可以通过实现一个生成器函数来达到，该函数在每次遍历开始时使用特定的种子 $s$ 重新初始化其随机数生成器。\n\n在幂迭代收敛到一个稳定的特征向量 $\\hat{v}$ 后，问题要求计算两个指标：绝对余弦相似度 $c = |\\hat{v}^\\top \\bar{a}|$ 和方差解释率 $\\rho = \\hat{\\lambda}_1 / \\operatorname{tr}(S)$。\n真实载荷向量 $a$ 是给定的，其单位范数版本 $\\bar{a}$ 计算为 $\\bar{a} = a / \\|a\\|_2$。值 $c$ 可直接得出。\n\n方差解释率 $\\rho$ 需要计算 $\\hat{\\lambda}_1$ 和 $\\operatorname{tr}(S)$。它们由以下公式给出：\n$$\n\\hat{\\lambda}_1 = \\hat{v}^\\top S \\hat{v} = \\frac{1}{n-1} \\|X\\hat{v}\\|_2^2\n$$\n$$\n\\operatorname{tr}(S) = \\frac{1}{n-1} \\operatorname{tr}(X^\\top X) = \\frac{1}{n-1} \\|X\\|_F^2\n$$\n该比率简化为 $\\rho = \\frac{\\|X\\hat{v}\\|_2^2}{\\|X\\|_F^2}$。$\\|X\\hat{v}\\|_2^2 = \\sum_{t=1}^n (x_{t,\\cdot}\\hat{v})^2$ 和弗罗贝尼乌斯范数的平方 $\\|X\\|_F^2 = \\sum_{t=1}^n \\sum_{j=1}^d X_{t,j}^2 = \\sum_{t=1}^n \\|x_{t,\\cdot}\\|_2^2$ 都可以在最后一次数据遍历中高效计算。在这次遍历中，我们初始化两个标量累加器为零，对于每个中心化后的行 $x_{t,\\cdot}$，我们将 $(x_{t,\\cdot}\\hat{v})^2$ 加到第一个累加器，将 $\\|x_{t,\\cdot}\\|_2^2$ 加到第二个累加器。\n\n完整的算法如下：\n1.  **均值计算（第一轮遍历）**：对数据流进行一次遍历以计算均值向量 $\\mu$。\n2.  **幂迭代（多轮遍历）**：初始化一个随机单位向量 $v$。进行固定次数的迭代或直到收敛为止，每次迭代都对数据进行一次遍历以计算 $w = X^\\top(Xv)$ 并更新 $v \\leftarrow w/\\|w\\|_2$。\n3.  **指标计算（最后一轮遍历）**：收敛到 $\\hat{v}$ 后，进行最后一轮遍历以计算 $\\|X\\hat{v}\\|_2^2$ 和 $\\|X\\|_F^2$。\n4.  **最终计算**：根据收集到的统计数据和真实载荷向量 $a$ 计算 $c$ 和 $\\rho$。\n\n这种多轮遍历的流式方法遵守了所有问题约束，其内存使用以 $O(d)$ 规模扩展，并以可管理的数据块处理数据。", "answer": "```python\nimport numpy as np\n\ndef generate_R_chunks(n, d, a, sigma, seed, chunk_size):\n    \"\"\"\n    Generator for streaming the data matrix R in chunks.\n    A new np.random.RandomState is created to ensure reproducibility for each pass.\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    current_pos = 0\n    while current_pos < n:\n        size = min(chunk_size, n - current_pos)\n        \n        # Generate z_t and epsilon_t for the whole chunk for efficiency\n        z = rng.normal(0, 1, size=(size, 1))\n        epsilon = rng.normal(0, sigma, size=(size, d))\n        \n        # R_chunk = z * a + epsilon\n        chunk = z * a.reshape(1, d) + epsilon\n        \n        yield chunk\n        current_pos += size\n\ndef solve_pca_streaming(n, d, a, sigma, seed, chunk_size, max_iter=100, tol=1e-9):\n    \"\"\"\n    Computes the first PC and related metrics using a streaming algorithm.\n    \"\"\"\n    # Pass 1: Compute column means\n    mean_vec = np.zeros(d)\n    data_stream_mean = generate_R_chunks(n, d, a, sigma, seed, chunk_size)\n    for chunk in data_stream_mean:\n        mean_vec += np.sum(chunk, axis=0)\n    mu = mean_vec / n\n\n    # Power Iteration to find the first principal component vector\n    v = np.random.rand(d)\n    v /= np.linalg.norm(v)\n\n    for _ in range(max_iter):\n        v_old = v\n        w = np.zeros(d)\n\n        # New pass for this iteration to compute w = X^T @ (X @ v)\n        data_stream_iter = generate_R_chunks(n, d, a, sigma, seed, chunk_size)\n        for chunk in data_stream_iter:\n            X_chunk = chunk - mu\n            p_chunk = X_chunk @ v\n            w += X_chunk.T @ p_chunk\n\n        norm_w = np.linalg.norm(w)\n        if norm_w == 0:\n            # Should not happen in this problem\n            break\n        \n        v = w / norm_w\n\n        # Check for convergence (sign-invariant)\n        if 1 - abs(v @ v_old) < tol:\n            break\n    \n    v_hat = v\n\n    # Final pass: Compute metrics (explained variance ratio)\n    sq_norm_Xv = 0.0\n    sq_frob_norm_X = 0.0\n    data_stream_metrics = generate_R_chunks(n, d, a, sigma, seed, chunk_size)\n    for chunk in data_stream_metrics:\n        X_chunk = chunk - mu\n        sq_norm_Xv += np.sum((X_chunk @ v_hat)**2)\n        sq_frob_norm_X += np.sum(X_chunk**2) \n\n    # Explained variance ratio\n    rho = sq_norm_Xv / sq_frob_norm_X if sq_frob_norm_X > 0 else 0.0\n\n    # Absolute cosine similarity\n    a_norm = np.linalg.norm(a)\n    if a_norm == 0:\n        a_bar = a # Or handle as an error\n    else:\n        a_bar = a / a_norm\n    c = abs(v_hat @ a_bar)\n\n    return c, rho\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    # Define test cases as per the problem statement\n    test_cases = [\n        # Case A: general well-conditioned\n        {'n': 5000, 'd': 50, 'sigma': 0.5, 's': 42, 'm': 250, \n         'a_def': lambda i, d_val: i},\n        # Case B: high-dimensional with d > n\n        {'n': 120, 'd': 600, 'sigma': 0.6, 's': 7, 'm': 64, \n         'a_def': lambda i, d_val: ((-1)**i) * (0.5 + i / d_val)},\n        # Case C: small eigengap, slower convergence\n        {'n': 4000, 'd': 100, 'sigma': 0.97, 's': 123, 'm': 200, \n         'a_def': lambda i, d_val: np.sin(2 * np.pi * i / d_val) + 0.1 * (i / d_val)}\n    ]\n    \n    results = []\n    # Use a fixed seed for initializing the random vector v in power iteration\n    # to ensure the entire program's output is deterministic.\n    np.random.seed(0)\n\n    for case in test_cases:\n        d = case['d']\n        indices = np.arange(1, d + 1)\n        a = case['a_def'](indices, d)\n        \n        c, rho = solve_pca_streaming(\n            n=case['n'], d=d, a=a, \n            sigma=case['sigma'], seed=case['s'], chunk_size=case['m']\n        )\n        results.append((c, rho))\n\n    # Format the output exactly as required\n    formatted_results = [f\"[{c:.6f},{rho:.6f}]\" for c, rho in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"}]}