{"hands_on_practices": [{"introduction": "在深入研究逻辑回归的细节之前，理解我们为什么需要它至关重要。本练习通过一个简单的计算，展示了为何将线性回归（如普通最小二乘法 OLS）用于分类任务是有问题的 [@problem_id:2407549]。你将亲自看到 OLS 如何产生无意义的概率预测（例如大于 $1$ 或小于 $0$），从而凸显出逻辑回归这类专为概率建模而设计的工具的必要性。", "id": "2407549", "problem": "在一个计算金融领域的信用风险分类任务中，考虑一个二元响应变量 $y \\in \\{0,1\\}$，它表示一家公司是否在一年内违约，以及一个单一预测变量 $x$，它是一个标准化的杠杆指数（无单位）。你观察到三家公司的样本，其 $(x,y)$ 对分别为：$(0,0)$、$(1,0)$ 和 $(2,1)$。\n\n使用通过普通最小二乘法（OLS）估计的线性概率模型，将模型 $y = \\beta_{0} + \\beta_{1} x$ 拟合到此样本，并计算在 $x = 3$ 处的 OLS 预测值。另外，回想一下，逻辑斯谛回归模型将预测变量值为 $x$ 时的违约概率指定为 $p(x) = \\frac{1}{1 + \\exp\\!\\big(-(\\gamma_{0} + \\gamma_{1} x)\\big)}$，对于任何实数参数 $\\gamma_{0}$ 和 $\\gamma_{1}$，该概率值严格位于开区间 $(0,1)$ 内。在给定参数 $\\gamma_{0} = -\\frac{1}{6}$ 和 $\\gamma_{1} = \\frac{1}{2}$ 的情况下，计算在 $x = 3$ 处的逻辑斯谛概率，并用此来对比这两种建模方法。\n\n你的最终答案只需提供在 $x = 3$ 处的 OLS 预测值，并四舍五入到四位有效数字。", "solution": "问题陈述具有科学依据、提法恰当且客观。它展示了计量经济学和金融学中用于二元分类的两种标准统计模型之间的比较：线性概率模型（LPM）和逻辑斯谛回归模型。所提供的数据和参数是完整和一致的，从而可以得出一个唯一且有意义的解。因此，该问题是有效的。\n\n我们首先处理使用普通最小二乘法（OLS）对所提供的三个观测值样本 $(x_{1}, y_{1}) = (0, 0)$、$(x_{2}, y_{2}) = (1, 0)$ 和 $(x_{3}, y_{3}) = (2, 1)$ 拟合线性概率模型 $y = \\beta_{0} + \\beta_{1} x$ 的任务。\n\n简单线性回归系数的 OLS 估计量 $\\hat{\\beta}_{0}$ 和 $\\hat{\\beta}_{1}$ 是通过最小化残差平方和来计算的。这些估计量的公式是：\n$$\n\\hat{\\beta}_{1} = \\frac{\\sum_{i=1}^{n} (x_{i} - \\bar{x})(y_{i} - \\bar{y})}{\\sum_{i=1}^{n} (x_{i} - \\bar{x})^2}\n$$\n$$\n\\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1} \\bar{x}\n$$\n其中 $n$ 是样本量，$\\bar{x}$ 是预测变量的样本均值，$\\bar{y}$ 是响应变量的样本均值。\n\n对于给定的数据，$n=3$：\n$x$ 的样本均值为：\n$$\n\\bar{x} = \\frac{0 + 1 + 2}{3} = \\frac{3}{3} = 1\n$$\n$y$ 的样本均值为：\n$$\n\\bar{y} = \\frac{0 + 0 + 1}{3} = \\frac{1}{3}\n$$\n\n接下来，我们计算必要的总和。\n$x$ 的离差平方和为：\n$$\n\\sum_{i=1}^{3} (x_{i} - \\bar{x})^2 = (0 - 1)^2 + (1 - 1)^2 + (2 - 1)^2 = (-1)^2 + 0^2 + 1^2 = 1 + 0 + 1 = 2\n$$\n离差乘积和为：\n$$\n\\sum_{i=1}^{3} (x_{i} - \\bar{x})(y_{i} - \\bar{y}) = (0 - 1)(0 - \\frac{1}{3}) + (1 - 1)(0 - \\frac{1}{3}) + (2 - 1)(1 - \\frac{1}{3})\n$$\n$$\n= (-1)(-\\frac{1}{3}) + (0)(-\\frac{1}{3}) + (1)(\\frac{2}{3}) = \\frac{1}{3} + 0 + \\frac{2}{3} = 1\n$$\n\n现在我们可以计算 OLS 估计量：\n$$\n\\hat{\\beta}_{1} = \\frac{1}{2}\n$$\n$$\n\\hat{\\beta}_{0} = \\frac{1}{3} - \\hat{\\beta}_{1} \\bar{x} = \\frac{1}{3} - (\\frac{1}{2})(1) = \\frac{2 - 3}{6} = -\\frac{1}{6}\n$$\n\n因此，拟合的线性概率模型为：\n$$\n\\hat{y} = -\\frac{1}{6} + \\frac{1}{2} x\n$$\n题目要求我们计算在 $x = 3$ 处的预测值。将 $x=3$ 代入拟合模型得到：\n$$\n\\hat{y}|_{x=3} = -\\frac{1}{6} + \\frac{1}{2}(3) = -\\frac{1}{6} + \\frac{3}{2} = -\\frac{1}{6} + \\frac{9}{6} = \\frac{8}{6} = \\frac{4}{3}\n$$\n\n为了按要求进行对比，我们现在计算在 $x = 3$ 处的逻辑斯谛概率。逻辑斯谛模型给出的概率 $p(x)$ 为：\n$$\np(x) = \\frac{1}{1 + \\exp(-(\\gamma_{0} + \\gamma_{1} x))}\n$$\n使用给定的参数 $\\gamma_{0} = -\\frac{1}{6}$ 和 $\\gamma_{1} = \\frac{1}{2}$，在 $x=3$ 处的概率为：\n$$\np(3) = \\frac{1}{1 + \\exp(-(-\\frac{1}{6} + \\frac{1}{2}(3)))} = \\frac{1}{1 + \\exp(-(-\\frac{1}{6} + \\frac{3}{2}))} = \\frac{1}{1 + \\exp(-(\\frac{8}{6}))} = \\frac{1}{1 + \\exp(-\\frac{4}{3})}\n$$\n数值上，$p(3) \\approx 0.7914$。\n\n两种模型之间的对比是鲜明的。LPM 的 OLS 预测值为 $\\hat{y}|_{x=3} = \\frac{4}{3} \\approx 1.3333$，该值大于 $1$。将此值解释为违约概率是毫无意义的，因为概率必须位于区间 $[0, 1]$ 内。这是线性概率模型一个众所周知的严重缺陷。相比之下，逻辑斯谛回归模型根据其数学定义，产生的概率估计值严格界于 $0$ 和 $1$ 之间。值 $p(3) \\approx 0.7914$ 是一个有效的概率。这说明了逻辑斯谛回归在为二元结果建模方面的理论优越性。\n\n题目要求的是在 $x=3$ 处的 OLS 预测值，四舍五入到四位有效数字。\n$$\n\\frac{4}{3} = 1.3333...\n$$\n四舍五入到四位有效数字得到 $1.333$。", "answer": "$$\n\\boxed{1.333}\n$$"}, {"introduction": "现在我们理解了逻辑回归的理论优势，接下来将进入一个真实的金融应用场景。本练习将让你扮演一名金融分析师，使用逻辑回归模型来预测企业并购是创造还是摧毁股东价值 [@problem_id:2407571]。你将处理一个受现实世界启发的数据集，并探索 $\\ell_2$ 正则化（岭回归）这一关键概念，观察不同的惩罚强度 $\\lambda$ 如何影响模型的预测能力。", "id": "2407571", "problem": "考虑一个二元分类问题，其中每个观测值代表一次已完成的公司收购。对于观测值 $i \\in \\{1,\\dots,n\\}$，令 $\\mathbf{z}_i \\in \\mathbb{R}^d$ 表示在收购公告时测量的 $d$ 个可观测特征组成的向量，令 $y_i \\in \\{0,1\\}$ 为收购方在收购完成后一年内获得正超常回报的指示符。数据中所有的分数均已表示为 $[0,1]$ 区间内的小数。给定 $n = 14$ 个观测值，每个观测值有 $d = 4$ 个特征，这些数据被收集到矩阵 $Z \\in \\mathbb{R}^{n \\times d}$ 和标签向量 $\\mathbf{y} \\in \\{0,1\\}^n$ 中。这些特征按列分别为：相对于目标公司公告前价格的交易溢价、相对规模（目标公司资本总额除以收购方资本总额）、收购方净杠杆率以及一个行业重叠指数。训练数据如下\n$$\nZ=\\begin{bmatrix}\n0.20 & 0.30 & 0.25 & 0.80\\\\\n0.35 & 0.60 & 0.50 & 0.20\\\\\n0.10 & 0.20 & 0.15 & 0.90\\\\\n0.45 & 0.70 & 0.60 & 0.10\\\\\n0.25 & 0.40 & 0.30 & 0.70\\\\\n0.30 & 0.50 & 0.55 & 0.30\\\\\n0.15 & 0.25 & 0.20 & 0.85\\\\\n0.50 & 0.80 & 0.65 & 0.05\\\\\n0.18 & 0.35 & 0.22 & 0.75\\\\\n0.40 & 0.55 & 0.45 & 0.40\\\\\n0.22 & 0.32 & 0.28 & 0.60\\\\\n0.28 & 0.45 & 0.35 & 0.50\\\\\n0.38 & 0.65 & 0.58 & 0.25\\\\\n0.12 & 0.18 & 0.12 & 0.95\n\\end{bmatrix},\\quad\n\\mathbf{y}=\\begin{bmatrix}\n1\\\\\n0\\\\\n1\\\\\n0\\\\\n1\\\\\n0\\\\\n1\\\\\n0\\\\\n1\\\\\n0\\\\\n1\\\\\n1\\\\\n0\\\\\n1\n\\end{bmatrix}.\n$$\n假设采用以下概率模型。对于每个观测值 $i$，定义增广特征向量 $\\mathbf{x}_i = \\begin{bmatrix}1 & \\mathbf{z}_i^\\top\\end{bmatrix}^\\top \\in \\mathbb{R}^{d+1}$，并令 $\\boldsymbol{\\theta}=\\begin{bmatrix}\\beta_0 & \\boldsymbol{\\beta}^\\top\\end{bmatrix}^\\top \\in \\mathbb{R}^{d+1}$。在给定 $\\mathbf{z}_i$ 的条件下，$y_i=1$ 的条件概率为\n$$\n\\mathbb{P}\\!\\left(y_i=1 \\mid \\mathbf{z}_i\\right)=\\sigma\\!\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\theta}\\right),\\quad \\sigma(t)=\\frac{1}{1+e^{-t}}.\n$$\n参数 $\\boldsymbol{\\theta}$ 由惩罚对数似然函数的唯一最大化器（当其存在时）确定\n$$\n\\ell_\\lambda(\\boldsymbol{\\theta})=\\sum_{i=1}^{n}\\left[y_i\\log\\!\\left(\\sigma\\!\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\theta}\\right)\\right)+(1-y_i)\\log\\!\\left(1-\\sigma\\!\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\theta}\\right)\\right)\\right]-\\frac{\\lambda}{2}\\left\\|\\boldsymbol{\\beta}\\right\\|_2^2,\n$$\n其中 $\\lambda \\ge 0$ 是给定的惩罚参数，截距项 $\\beta_0$ 不受惩罚。\n\n您的任务是，对于下方的每个测试用例，计算一个具有特征 $\\mathbf{z}^\\star \\in \\mathbb{R}^d$ 的新收购创造价值的预测概率，该概率定义为\n$$\np^\\star=\\sigma\\!\\left(\\begin{bmatrix}1 & (\\mathbf{z}^\\star)^\\top\\end{bmatrix}\\boldsymbol{\\theta}_\\lambda^\\star\\right),\n$$\n其中 $\\boldsymbol{\\theta}_\\lambda^\\star$ 是对于指定的 $\\lambda$，使用上述训练数据 $(Z,\\mathbf{y})$ 时 $\\ell_\\lambda(\\boldsymbol{\\theta})$ 的任何一个最大化器。\n\n测试套件（每个用例指定 $(\\lambda,\\mathbf{z}^\\star)$）：\n- 用例 1：$\\lambda=0.1$, $\\mathbf{z}^\\star=\\begin{bmatrix}0.27\\\\0.40\\\\0.33\\\\0.55\\end{bmatrix}$。\n- 用例 2：$\\lambda=10.0$, $\\mathbf{z}^\\star=\\begin{bmatrix}0.27\\\\0.40\\\\0.33\\\\0.55\\end{bmatrix}$。\n- 用例 3：$\\lambda=0.0$, $\\mathbf{z}^\\star=\\begin{bmatrix}0.55\\\\0.90\\\\0.70\\\\0.05\\end{bmatrix}$。\n- 用例 4：$\\lambda=0.01$, $\\mathbf{z}^\\star=\\begin{bmatrix}0.08\\\\0.15\\\\0.10\\\\0.98\\end{bmatrix}$。\n\n要求：\n- 使用上述指定的模型和训练数据，为每个用例计算 $p^\\star$。\n- 将所有输出表示为在 $[0,1]$ 区间内、精确到 $6$ 位小数的实数。\n- 您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表，格式如 $\\texttt{[0.123456,0.234567,0.345678,0.456789]}$。", "solution": "所给问题是L2正则化逻辑回归在二元分类中的一个明确应用，这是计算金融和计量经济学中的一种标准技术。任务是在给定一组训练数据和特定的正则化参数的情况下，为几个测试用例计算出现正向结果的预测概率。该问题具有科学依据，在数学上是一致的，并包含了所有必要信息，以便在 $\\lambda > 0$ 的情况下确定唯一解，以及在 $\\lambda=0$ 时确定一个数值可计算的解。\n\n模型由具有特征 $\\mathbf{z}_i \\in \\mathbb{R}^d$ 的观测值出现正向结果（$y_i=1$）的条件概率定义：\n$$ \\mathbb{P}(y_i=1 \\mid \\mathbf{z}_i) = \\sigma(\\mathbf{x}_i^\\top \\boldsymbol{\\theta}) $$\n其中 $\\mathbf{x}_i = \\begin{bmatrix}1 & \\mathbf{z}_i^\\top\\end{bmatrix}^\\top$ 是维度为 $d+1$ 的增广特征向量，$\\boldsymbol{\\theta} \\in \\mathbb{R}^{d+1}$ 是参数向量，$\\sigma(t) = (1+e^{-t})^{-1}$ 是 logistic S型函数。\n\n参数向量 $\\boldsymbol{\\theta}_\\lambda^\\star = \\begin{bmatrix}\\beta_{0,\\lambda}^\\star & (\\boldsymbol{\\beta}_\\lambda^\\star)^\\top\\end{bmatrix}^\\top$ 通过最大化惩罚对数似然函数求得：\n$$ \\ell_\\lambda(\\boldsymbol{\\theta})=\\sum_{i=1}^{n}\\left[y_i\\log(p_i)+(1-y_i)\\log(1-p_i)\\right]-\\frac{\\lambda}{2}\\left\\|\\boldsymbol{\\beta}\\right\\|_2^2 $$\n其中 $p_i = \\sigma(\\mathbf{x}_i^\\top \\boldsymbol{\\theta})$，$n=14$ 是观测值的数量，$\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_1 & \\dots & \\beta_d\\end{bmatrix}^\\top$ 是 $d=4$ 个特征的系数，不包括截距项 $\\beta_0$。\n\n为解决这个最大化问题，我们等价地最小化负的惩罚对数似然。设该目标函数为 $f(\\boldsymbol{\\theta}) = -\\ell_\\lambda(\\boldsymbol{\\theta})$。这是一个凸优化问题，其解可以使用数值方法找到。目标函数为：\n$$ f(\\boldsymbol{\\theta}) = -\\left( \\sum_{i=1}^{n}\\left[y_i\\log(p_i)+(1-y_i)\\log(1-p_i)\\right] \\right) + \\frac{\\lambda}{2}\\left\\|\\boldsymbol{\\beta}\\right\\|_2^2 $$\n为了稳健的数值计算，交叉熵损失项可以表示为一种更稳定的形式。要最小化的总损失的正确表达式为：\n$$ f(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\left[ (1-y_i)\\log(1+e^{\\mathbf{x}_i^\\top\\boldsymbol{\\theta}}) + y_i\\log(1+e^{-\\mathbf{x}_i^\\top\\boldsymbol{\\theta}}) \\right] + \\frac{\\lambda}{2} \\sum_{j=1}^d \\beta_j^2 $$\n我们将使用一个拟牛顿优化算法，具体是 L-BFGS-B，该算法需要目标函数的梯度。梯度 $\\nabla f(\\boldsymbol{\\theta})$ 推导如下：\n$$ \\nabla f(\\boldsymbol{\\theta}) = X^\\top(\\mathbf{p} - \\mathbf{y}) + \\lambda \\boldsymbol{\\theta}_{\\text{pen}} $$\n此处，$X \\in \\mathbb{R}^{n \\times (d+1)}$ 是设计矩阵，通过在特征矩阵 $Z$ 前添加一列全为1的列构成。向量 $\\mathbf{y} \\in \\{0,1\\}^n$ 包含标签，$\\mathbf{p} = \\sigma(X\\boldsymbol{\\theta})$ 是预测概率的向量，而 $\\boldsymbol{\\theta}_{\\text{pen}} = \\begin{bmatrix}0 & \\beta_1 & \\dots & \\beta_d\\end{bmatrix}^\\top$ 是将截距项分量置零的参数向量，以反映 $\\beta_0$ 不受惩罚的事实。\n\n对于由一个数对 $(\\lambda, \\mathbf{z}^\\star)$ 定义的每个测试用例，执行以下步骤：\n$1$. 为给定的 $\\lambda$ 定义目标函数 $f(\\boldsymbol{\\theta})$ 及其梯度 $\\nabla f(\\boldsymbol{\\theta})$。\n$2$. 一个数值优化器从一个初始猜测（例如 $\\boldsymbol{\\theta}^{(0)} = \\mathbf{0}$）开始最小化 $f(\\boldsymbol{\\theta})$，以找到最优参数向量 $\\boldsymbol{\\theta}_\\lambda^\\star$。\n$3$. 新的特征向量 $\\mathbf{z}^\\star$被增广为 $\\mathbf{x}^\\star = \\begin{bmatrix}1 & (\\mathbf{z}^\\star)^\\top\\end{bmatrix}^\\top$。\n$4$. 预测概率 $p^\\star$ 按 $p^\\star = \\sigma((\\mathbf{x}^\\star)^\\top \\boldsymbol{\\theta}_\\lambda^\\star)$ 计算。\n\n对所有四个测试用例重复此过程，并将所得概率四舍五入到指定精度。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Solves the L2-regularized logistic regression problem for the given test cases.\n    \"\"\"\n    # Training data provided in the problem statement.\n    Z = np.array([\n        [0.20, 0.30, 0.25, 0.80],\n        [0.35, 0.60, 0.50, 0.20],\n        [0.10, 0.20, 0.15, 0.90],\n        [0.45, 0.70, 0.60, 0.10],\n        [0.25, 0.40, 0.30, 0.70],\n        [0.30, 0.50, 0.55, 0.30],\n        [0.15, 0.25, 0.20, 0.85],\n        [0.50, 0.80, 0.65, 0.05],\n        [0.18, 0.35, 0.22, 0.75],\n        [0.40, 0.55, 0.45, 0.40],\n        [0.22, 0.32, 0.28, 0.60],\n        [0.28, 0.45, 0.35, 0.50],\n        [0.38, 0.65, 0.58, 0.25],\n        [0.12, 0.18, 0.12, 0.95]\n    ])\n    \n    y = np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1])\n\n    # Test suite from the problem statement.\n    test_cases = [\n        (0.1, np.array([0.27, 0.40, 0.33, 0.55])),\n        (10.0, np.array([0.27, 0.40, 0.33, 0.55])),\n        (0.0, np.array([0.55, 0.90, 0.70, 0.05])),\n        (0.01, np.array([0.08, 0.15, 0.10, 0.98]))\n    ]\n    \n    # Augment the feature matrix Z with an intercept column.\n    n, d = Z.shape\n    X = np.hstack([np.ones((n, 1)), Z])\n\n    def objective_function(theta, X_mat, y_vec, lambda_val):\n        \"\"\"\n        Computes the negative penalized log-likelihood (objective to minimize).\n        A small epsilon is used for numerical stability of the logarithm,\n        although L-BFGS-B with a good gradient is generally robust.\n        \"\"\"\n        m = X_mat.shape[0]\n        u = X_mat @ theta\n        p = expit(u)\n        \n        # To prevent log(0), clip probabilities to be within a safe range.\n        eps = 1e-15\n        p = np.clip(p, eps, 1 - eps)\n        \n        log_likelihood = np.sum(y_vec * np.log(p) + (1 - y_vec) * np.log(1 - p))\n        \n        # L2 penalty term (Ridge), excluding the intercept.\n        beta = theta[1:]\n        penalty = (lambda_val / 2) * np.sum(beta**2)\n        \n        return -(log_likelihood - penalty)\n\n    def gradient_function(theta, X_mat, y_vec, lambda_val):\n        \"\"\"\n        Computes the gradient of the negative penalized log-likelihood.\n        \"\"\"\n        m = X_mat.shape[0]\n        u = X_mat @ theta\n        p = expit(u)\n        \n        error = p - y_vec\n        grad_log_likelihood = X_mat.T @ error\n        \n        # Gradient of the L2 penalty term.\n        grad_penalty = lambda_val * theta\n        grad_penalty[0] = 0  # No penalty on the intercept.\n        \n        return grad_log_likelihood + grad_penalty\n\n    results = []\n    for lambda_val, z_star in test_cases:\n        # Initial guess for the parameters.\n        initial_theta = np.zeros(d + 1)\n\n        # Perform the optimization to find the best parameters.\n        opt_result = minimize(\n            fun=objective_function,\n            x0=initial_theta,\n            args=(X, y, lambda_val),\n            method='L-BFGS-B',\n            jac=gradient_function\n        )\n        theta_star = opt_result.x\n\n        # Augment the test vector and compute the prediction.\n        x_star = np.hstack([1, z_star])\n        p_star = expit(x_star @ theta_star)\n        \n        results.append(p_star)\n\n    # Format output according to the problem requirements.\n    formatted_results = [\"{:.6f}\".format(res) for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"}, {"introduction": "本章的最终挑战将带你深入到构建和评估稳健分类模型的实践核心。你不仅要从零开始实现一个带有 $\\ell_2$ 正则化的逻辑回归求解器，还要应对现实世界中常见的难题：类别不平衡 [@problem_id:2407556]。通过预测体育冠军这一罕见事件，你将学习到为何准确率等传统指标会产生误导，并掌握使用如 $F_1$ 分数、平衡准确率和 AUC-PR 等更合适的度量标准来公正地评估模型性能。", "id": "2407556", "problem": "要求您编写一个完整的、可运行的程序，用于训练和评估一个逻辑回归分类器来解决一个稀有事件预测问题，该问题框架如下。一个球队赛季由一个标准化赛季统计数据的向量表示：胜率、场均净胜分、薪资与联盟中位数的比率，以及一个季后赛经验指数。目标是预测该球队在该赛季是否赢得总冠军。赢得总冠军是一个稀有事件。模型为带有截距项的逻辑回归，并可选择为正类设置类别权重以解决类别不平衡问题。评估过程强调非平衡类别的评估指标以及阈值效应。\n\n从以下基本原理开始：\n- 每个观测值的输出是一个伯努利随机变量，其成功概率由一个逻辑连接函数参数化。设 $y_i \\in \\{0,1\\}$ 为二元标签，$x_i \\in \\mathbb{R}^d$ 为特征向量。\n- 逻辑连接函数设定 $p_i = \\Pr(y_i = 1 \\mid x_i) = \\sigma(z_i)$，其中 $z_i = \\beta_0 + x_i^\\top \\beta$，且 $\\sigma(u) = \\dfrac{1}{1 + e^{-u}}$。\n- 需要最小化的目标是带有 $\\ell_2$ 正则化（岭回归）的（可选）加权负对数似然。如果 $w_i > 0$ 是观测权重，$\\lambda \\ge 0$ 是正则化强度，则目标函数为\n$$\n\\mathcal{L}(\\beta_0, \\beta) = -\\sum_{i=1}^{n} w_i \\left[ y_i \\log p_i + (1 - y_i) \\log(1 - p_i) \\right] + \\frac{\\lambda}{2} \\lVert \\beta \\rVert_2^2,\n$$\n其中截距项 $\\beta_0$ 不参与正则化。\n\n您的程序必须：\n- 通过在完整数据集上（不使用小批量）使用牛顿-拉夫逊（Newton–Raphson）方法最小化上述目标函数来现实逻辑回归训练。使用精确的梯度和Hessian矩阵，为保证稳定性应采用回溯线搜索，并且不对截距项进行惩罚。允许设置一个标量正类权重 $w_+ > 1$，即当 $y_i = 1$ 时设 $w_i = w_+$，当 $y_i = 0$ 时设 $w_i = 1$；非加权训练则使用 $w_+ = 1$。\n- 训练后，在测试集上计算预测概率 $\\hat{p}_i = \\sigma(\\beta_0 + x_i^\\top \\beta)$，并使用一个固定的阈值 $\\tau \\in (0,1)$ 将其转换为类别预测：如果 $\\hat{p}_i \\ge \\tau$，则预测 $\\hat{y}_i = 1$，否则预测 $\\hat{y}_i = 0$。\n- 在测试集上计算以下评估指标，其中 $\\mathrm{TP}$、$\\mathrm{FP}$、$\\mathrm{TN}$、$\\mathrm{FN}$ 分别是真正例（true positives）、假正例（false positives）、真负例（true negatives）和假负例（false negatives）：\n    - 精确率（Precision）：$\\mathrm{Prec} = \\dfrac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}}$，分母为零时定义为 $0$。\n    - 召回率（Recall，即真正例率）：$\\mathrm{Rec} = \\dfrac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}}$，分母为零时定义为 $0$。\n    - $F_1$ 分数：$F_1 = \\dfrac{2 \\cdot \\mathrm{Prec} \\cdot \\mathrm{Rec}}{\\mathrm{Prec} + \\mathrm{Rec}}$，分母为零时定义为 $0$。\n    - 平衡准确率（Balanced accuracy）：$\\mathrm{BAcc} = \\dfrac{1}{2}\\left( \\dfrac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} + \\dfrac{\\mathrm{TN}}{\\mathrm{TN} + \\mathrm{FP}} \\right)$，其中每个分数在分母为零时定义为 $0$。\n    - 马修斯相关系数（Matthews Correlation Coefficient, MCC）：$\\mathrm{MCC} = \\dfrac{\\mathrm{TP}\\cdot \\mathrm{TN} - \\mathrm{FP}\\cdot \\mathrm{FN}}{\\sqrt{(\\mathrm{TP} + \\mathrm{FP})(\\mathrm{TP} + \\mathrm{FN})(\\mathrm{TN} + \\mathrm{FP})(\\mathrm{TN} + \\mathrm{FN})}}$，分母为零时定义为 $0$。\n    - 精确率-召回率曲线下面积（Area under the Precision–Recall curve, AUC-PR）：通过将测试实例按 $\\hat{p}_i$ 降序排序，并在累积阈值处评估精确率和召回率来构建精确率-召回率曲线；使用梯形法则在召回率上计算面积。\n\n训练和测试数据在下方以显式数值数组的形式提供。每个样本是一个包含四个特征的向量 $\\left[x_1, x_2, x_3, x_4\\right]$，分别对应胜率、场均净胜分、薪资与联盟中位数的比率以及季后赛经验指数。标签 $y$ 表示是否赢得总冠军。\n\n请完全按照给定的特征和标签数组使用。\n\n- 训练集 $\\left(X_{\\text{train}}, y_{\\text{train}}\\right)$，$n = 16$：\n    - $[0.55, -1.0, 0.90, 0.20] \\rightarrow 0$\n    - $[0.60, 1.50, 1.10, 0.30] \\rightarrow 0$\n    - $[0.48, -3.50, 0.85, 0.10] \\rightarrow 0$\n    - $[0.70, 5.00, 1.20, 0.50] \\rightarrow 0$\n    - $[0.75, 6.00, 1.40, 0.60] \\rightarrow 1$\n    - $[0.65, 3.00, 1.00, 0.40] \\rightarrow 0$\n    - $[0.80, 8.50, 1.50, 0.70] \\rightarrow 1$\n    - $[0.42, -6.00, 0.75, 0.10] \\rightarrow 0$\n    - $[0.58, 0.50, 0.95, 0.20] \\rightarrow 0$\n    - $[0.62, 2.00, 1.05, 0.35] \\rightarrow 0$\n    - $[0.68, 4.00, 1.10, 0.45] \\rightarrow 0$\n    - $[0.85, 10.00, 1.60, 0.80] \\rightarrow 1$\n    - $[0.50, -2.00, 0.90, 0.15] \\rightarrow 0$\n    - $[0.73, 5.50, 1.30, 0.55] \\rightarrow 0$\n    - $[0.66, 3.50, 1.15, 0.45] \\rightarrow 0$\n    - $[0.77, 7.00, 1.35, 0.60] \\rightarrow 0$\n\n- 测试集 A $\\left(X_{\\text{A}}, y_{\\text{A}}\\right)$，$m = 6$：\n    - $[0.74, 6.00, 1.25, 0.50] \\rightarrow 1$\n    - $[0.57, 0.00, 1.00, 0.20] \\rightarrow 0$\n    - $[0.82, 9.00, 1.55, 0.75] \\rightarrow 1$\n    - $[0.45, -4.50, 0.80, 0.05] \\rightarrow 0$\n    - $[0.63, 2.50, 1.10, 0.40] \\rightarrow 0$\n    - $[0.79, 7.50, 1.45, 0.65] \\rightarrow 1$\n\n- 测试集 B $\\left(X_{\\text{B}}, y_{\\text{B}}\\right)$，$m = 8$：\n    - $[0.61, 1.50, 1.05, 0.30] \\rightarrow 0$\n    - $[0.52, -1.50, 0.90, 0.12] \\rightarrow 0$\n    - $[0.76, 6.50, 1.40, 0.60] \\rightarrow 1$\n    - $[0.59, 0.50, 0.95, 0.25] \\rightarrow 0$\n    - $[0.47, -4.00, 0.82, 0.08] \\rightarrow 0$\n    - $[0.66, 3.00, 1.12, 0.40] \\rightarrow 0$\n    - $[0.64, 2.00, 1.08, 0.38] \\rightarrow 0$\n    - $[0.54, -0.50, 0.97, 0.20] \\rightarrow 0$\n\n测试套件。所有案例均在 $\\left(X_{\\text{train}}, y_{\\text{train}}\\right)$ 上进行训练，但改变阈值和正类权重以测试不同方面：\n- 案例 1（理想路径）：非加权训练，$\\lambda = 1.0$，$\\tau = 0.5$，最大迭代次数 $= 100$，容差 $= 10^{-9}$；在测试集 A 上评估。\n- 案例 2（阈值边界）：非加权训练，$\\lambda = 1.0$，$\\tau = 0.9$，最大迭代次数 $= 100$，容差 $= 10^{-9}$；在测试集 A 上评估。\n- 案例 3（类别不平衡缓解）：加权训练，正类权重 $w_+ = 3.0$，$\\lambda = 1.0$，$\\tau = 0.5$，最大迭代次数 $= 100$，容差 $= 10^{-9}$；在测试集 B 上评估。\n\n最终输出要求：\n- 对于每个案例，按 $[\\mathrm{Prec}, \\mathrm{Rec}, F_1, \\mathrm{BAcc}, \\mathrm{MCC}, \\mathrm{AUC\\mbox{-}PR}]$ 这一确切顺序将指标报告为列表，每个值四舍五入到 $6$ 位小数。\n- 您的程序应生成单行输出，其中包含由三个案例级别列表组成的逗号分隔列表，并用方括号括起来，例如：$[[a_1,\\dots,a_6],[b_1,\\dots,b_6],[c_1,\\dots,c_6]]$，其中每个 $a_j$、$b_j$、$c_j$ 都是四舍五入到 $6$ 位小数的浮点数。\n\n不允许用户输入或使用外部文件；程序必须完全自包含且具有确定性。本问题不涉及角度。不涉及物理单位。所有答案均为浮点数，且必须按规定四舍五入。", "solution": "所提出的问题是计算统计学中一个明确定义的任务：为二元分类问题，特别是一个稀有事件场景，实现、训练和评估一个逻辑回归分类器。该问题具有科学依据，数学上完备，且算法上明确。因此，该问题是有效的，我们将从第一性原理出发，构建一个严谨的解决方案。\n\n问题的核心是找到逻辑回归模型的参数，以最小化一个指定的目标函数。该模型通过逻辑函数 $p_i = \\sigma(\\beta_0 + x_i^\\top \\beta)$ 来预测特征向量为 $x_i \\in \\mathbb{R}^d$ 的观测值出现正向结果（$y_i=1$）的概率，其中 $\\sigma(u) = (1 + e^{-u})^{-1}$。为方便表示，我们将特征向量增广为 $\\tilde{x}_i = [1, x_i^\\top]^\\top$，参数向量增广为 $\\tilde{\\beta} = [\\beta_0, \\beta^\\top]^\\top$，从而线性部分为 $z_i = \\tilde{x}_i^\\top \\tilde{\\beta}$。\n\n需要最小化的目标函数是带 $\\ell_2$ 惩罚项的加权负对数似然，该惩罚项作用于特征系数 $\\beta$（但不包括截距 $\\beta_0$）：\n$$\n\\mathcal{L}(\\tilde{\\beta}) = -\\sum_{i=1}^{n} w_i \\left[ y_i \\log p_i + (1 - y_i) \\log(1 - p_i) \\right] + \\frac{\\lambda}{2} \\beta^\\top \\beta\n$$\n此处，$n$ 是训练样本数，$w_i$ 是样本权重，$\\lambda$ 是正则化强度。如果 $y_i=1$，权重 $w_i$ 被设为一个指定的正类权重 $w_+$；如果 $y_i=0$，则设为 $1$。\n\n优化过程使用牛顿-拉夫逊（Newton-Raphson）方法，这是一种二阶迭代算法。每次迭代根据以下规则更新参数向量 $\\tilde{\\beta}$：\n$$\n\\tilde{\\beta}^{(k+1)} = \\tilde{\\beta}^{(k)} + \\alpha \\Delta \\tilde{\\beta}^{(k)}\n$$\n其中 $\\alpha$ 是步长，$\\Delta \\tilde{\\beta}^{(k)}$ 是牛顿步，通过求解以下线性系统得到：\n$$\n\\mathbf{H}(\\tilde{\\beta}^{(k)}) \\Delta \\tilde{\\beta}^{(k)} = -\\nabla \\mathcal{L}(\\tilde{\\beta}^{(k)})\n$$\n在此方程中，$\\nabla \\mathcal{L}$ 是目标函数 $\\mathcal{L}$ 的梯度（一阶偏导数向量），$\\mathbf{H}$ 是其Hessian矩阵（二阶偏导数矩阵）。\n\n梯度向量 $\\nabla \\mathcal{L}(\\tilde{\\beta})$ 推导如下：\n$$\n\\nabla \\mathcal{L}(\\tilde{\\beta}) = \\tilde{X}^\\top W (p - y) + \\lambda \\Lambda \\tilde{\\beta}\n$$\n此处，$\\tilde{X}$ 是 $n \\times (d+1)$ 的增广设计矩阵，$W$ 是由样本权重 $w_i$ 组成的 $n \\times n$ 对角矩阵，$p$ 是预测概率向量，$y$ 是真实标签向量，$\\Lambda$ 是一个 $(d+1) \\times (d+1)$ 的对角矩阵，其对角元 $\\Lambda_{00}=0$ 且当 $j>0$ 时 $\\Lambda_{jj}=1$，这确保了截距项不被惩罚。\n\nHessian矩阵 $\\mathbf{H}(\\tilde{\\beta})$ 推导如下：\n$$\n\\mathbf{H}(\\tilde{\\beta}) = \\tilde{X}^\\top S \\tilde{X} + \\lambda \\Lambda\n$$\n其中 $S$ 是一个 $n \\times n$ 的对角矩阵，其元素为 $S_{ii} = w_i p_i (1 - p_i)$。由于此Hessian矩阵是对称的，并且当 $\\lambda > 0$ 时是正定的，因此牛顿步的线性系统有唯一解。\n\n为确保稳健收敛，步长 $\\alpha$ 通过回溯线搜索确定。从 $\\alpha=1$ 开始，步长通过一个因子 $\\rho$（例如 $0.5$）迭代减小，直到满足Armijo-Goldstein条件：\n$$\n\\mathcal{L}(\\tilde{\\beta}^{(k)} + \\alpha \\Delta \\tilde{\\beta}^{(k)}) \\le \\mathcal{L}(\\tilde{\\beta}^{(k)}) + c_1 \\alpha (\\nabla \\mathcal{L}(\\tilde{\\beta}^{(k)}))^\\top \\Delta \\tilde{\\beta}^{(k)}\n$$\n其中 $c_1$ 是一个很小的常数（例如 $10^{-4}$）。\n\n训练结束后，在测试集上评估模型性能。计算预测概率 $\\hat{p}_i$，并使用一个指定的阈值 $\\tau$ 将其转换为类别标签 $\\hat{y}_i \\in \\{0,1\\}$。评估依赖于几个指标，这些指标根据真正例（$\\mathrm{TP}$）、假正例（$\\mathrm{FP}$）、真负例（$\\mathrm{TN}$）和假负例（$\\mathrm{FN}$）定义：\n- **精确率（Precision）**：$\\mathrm{Prec} = \\mathrm{TP} / (\\mathrm{TP} + \\mathrm{FP})$\n- **召回率（Recall）**：$\\mathrm{Rec} = \\mathrm{TP} / (\\mathrm{TP} + \\mathrm{FN})$\n- **$F_1$ 分数**：$F_1 = 2 \\cdot (\\mathrm{Prec} \\cdot \\mathrm{Rec}) / (\\mathrm{Prec} + \\mathrm{Rec})$\n- **平衡准确率（Balanced Accuracy）**：$\\mathrm{BAcc} = 0.5 \\cdot (\\mathrm{Rec} + \\mathrm{TN} / (\\mathrm{TN} + \\mathrm{FP}))$\n- **马修斯相关系数 (MCC)**：$\\mathrm{MCC} = (\\mathrm{TP}\\cdot \\mathrm{TN} - \\mathrm{FP}\\cdot \\mathrm{FN}) / \\sqrt{(\\mathrm{TP} + \\mathrm{FP})(\\mathrm{TP} + \\mathrm{FN})(\\mathrm{TN} + \\mathrm{FP})(\\mathrm{TN} + \\mathrm{FN})}$\n如果分母为零，每个指标都定义为 $0$。\n\n精确率-召回率曲线下面积（AUC-PR）通过数值积分计算。测试实例按预测概率降序排序。通过依次考虑已排序实例的更大子集，生成一系列精确率和召回率值。然后使用梯形法则计算所得曲线下的面积。召回率值是非递减的，作为积分点。曲线锚定在起始点 $(\\text{recall}=0, \\text{precision}=1)$，以正确计算召回率范围起始部分的面积。\n\n所提供的实现封装了这整个流程。它定义了必要的数据结构，实现了带有回溯线搜索的牛顿-拉夫逊训练器，并为问题陈述中指定的三个测试案例计算了全套评估指标。", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve as solve_linear_system\n\ndef solve():\n    \"\"\"\n    Main function to run the logistic regression experiments as specified.\n    \"\"\"\n    \n    # --- Data Definition ---\n    X_train = np.array([\n        [0.55, -1.0, 0.90, 0.20], [0.60, 1.50, 1.10, 0.30], [0.48, -3.50, 0.85, 0.10],\n        [0.70, 5.00, 1.20, 0.50], [0.75, 6.00, 1.40, 0.60], [0.65, 3.00, 1.00, 0.40],\n        [0.80, 8.50, 1.50, 0.70], [0.42, -6.00, 0.75, 0.10], [0.58, 0.50, 0.95, 0.20],\n        [0.62, 2.00, 1.05, 0.35], [0.68, 4.00, 1.10, 0.45], [0.85, 10.00, 1.60, 0.80],\n        [0.50, -2.00, 0.90, 0.15], [0.73, 5.50, 1.30, 0.55], [0.66, 3.50, 1.15, 0.45],\n        [0.77, 7.00, 1.35, 0.60]\n    ])\n    y_train = np.array([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n\n    X_A = np.array([\n        [0.74, 6.00, 1.25, 0.50], [0.57, 0.00, 1.00, 0.20], [0.82, 9.00, 1.55, 0.75],\n        [0.45, -4.50, 0.80, 0.05], [0.63, 2.50, 1.10, 0.40], [0.79, 7.50, 1.45, 0.65]\n    ])\n    y_A = np.array([1, 0, 1, 0, 0, 1])\n\n    X_B = np.array([\n        [0.61, 1.50, 1.05, 0.30], [0.52, -1.50, 0.90, 0.12], [0.76, 6.50, 1.40, 0.60],\n        [0.59, 0.50, 0.95, 0.25], [0.47, -4.00, 0.82, 0.08], [0.66, 3.00, 1.12, 0.40],\n        [0.64, 2.00, 1.08, 0.38], [0.54, -0.50, 0.97, 0.20]\n    ])\n    y_B = np.array([0, 0, 1, 0, 0, 0, 0, 0])\n    \n    test_cases = [\n        # (lambda_reg, w_plus, tau, X_test, y_test, max_iter, tol)\n        (1.0, 1.0, 0.5, X_A, y_A, 100, 1e-9),\n        (1.0, 1.0, 0.9, X_A, y_A, 100, 1e-9),\n        (1.0, 3.0, 0.5, X_B, y_B, 100, 1e-9)\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        lambda_reg, w_plus, tau, X_test, y_test, max_iter, tol = case\n        \n        # Train model\n        beta = _logistic_regression_train(X_train, y_train, lambda_reg, w_plus, max_iter, tol)\n        \n        # Get predictions\n        y_proba = _predict_proba(X_test, beta)\n        y_pred = (y_proba >= tau).astype(int)\n        \n        # Evaluate metrics\n        metrics = _evaluate_metrics(y_test, y_pred, y_proba)\n        results.append(metrics)\n        \n    # Format and print the final output\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n\ndef _sigmoid(z):\n    # Clip to avoid overflow/underflow in exp\n    z_clipped = np.clip(z, -500, 500)\n    p = 1 / (1 + np.exp(-z_clipped))\n    # Clip probabilities to avoid log(0)\n    return np.clip(p, 1e-15, 1 - 1e-15)\n\ndef _calculate_cost(X_aug, y, beta, weights, lambda_reg):\n    p = _sigmoid(X_aug @ beta)\n    log_likelihood = -np.sum(weights * (y * np.log(p) + (1 - y) * np.log(1 - p)))\n    reg_term = (lambda_reg / 2) * np.dot(beta[1:], beta[1:])\n    return log_likelihood + reg_term\n\ndef _logistic_regression_train(X, y, lambda_reg, w_plus, max_iter, tol):\n    n_samples, n_features = X.shape\n    X_aug = np.c_[np.ones(n_samples), X]\n    beta = np.zeros(n_features + 1)\n    \n    weights = np.ones(n_samples)\n    weights[y == 1] = w_plus\n\n    lambda_vec = np.full(n_features + 1, lambda_reg)\n    lambda_vec[0] = 0.0\n\n    # Newton-Raphson iterations\n    for _ in range(max_iter):\n        z = X_aug @ beta\n        p = _sigmoid(z)\n        \n        error = p - y\n        grad = X_aug.T @ (weights * error) + lambda_vec * beta\n        \n        S_diag = weights * p * (1 - p)\n        H = (X_aug.T * S_diag) @ X_aug + np.diag(lambda_vec)\n        \n        # Solve H * delta_beta = -grad\n        # Use scipy.linalg.solve for stability and performance\n        search_dir = solve_linear_system(H, -grad, assume_a='sym')\n        \n        # Backtracking line search\n        alpha = 1.0\n        c1 = 1e-4\n        rho = 0.5\n        cost_current = _calculate_cost(X_aug, y, beta, weights, lambda_reg)\n        grad_dot_dir = np.dot(grad, search_dir)\n        \n        while True:\n            beta_new = beta + alpha * search_dir\n            cost_new = _calculate_cost(X_aug, y, beta_new, weights, lambda_reg)\n            if cost_new <= cost_current + c1 * alpha * grad_dot_dir or alpha < 1e-9:\n                break\n            alpha *= rho\n        \n        step = alpha * search_dir\n        beta += step\n        \n        if np.linalg.norm(step) < tol:\n            break\n            \n    return beta\n\ndef _predict_proba(X, beta):\n    X_aug = np.c_[np.ones(X.shape[0]), X]\n    return _sigmoid(X_aug @ beta)\n\ndef _evaluate_metrics(y_true, y_pred, y_proba):\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    tn = np.sum((y_true == 0) & (y_pred == 0))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    \n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n    \n    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n    \n    tpr = recall\n    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n    balanced_accuracy = 0.5 * (tpr + tnr)\n\n    mcc_denom_sq = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n    mcc = (tp * tn - fp * fn) / np.sqrt(mcc_denom_sq) if mcc_denom_sq > 0 else 0.0\n        \n    # AUC-PR calculation\n    sort_indices = np.argsort(y_proba)[::-1]\n    y_true_sorted = y_true[sort_indices]\n    \n    tps = np.cumsum(y_true_sorted)\n    fps = np.cumsum(1 - y_true_sorted)\n    \n    total_positives = np.sum(y_true)\n    if total_positives == 0:\n        auc_pr = 0.0\n    else:\n        recalls = tps / total_positives\n        precisions = tps / (tps + fps)\n        \n        # Keep only points where recall changes (at positive instances)\n        is_positive = (y_true_sorted == 1)\n        recall_points = recalls[is_positive]\n        precision_points = precisions[is_positive]\n\n        # Prepend starting point for trapezoidal rule\n        recall_aug = np.concatenate([[0.], recall_points])\n        precision_aug = np.concatenate([[1.], precision_points])\n        \n        auc_pr = np.trapz(precision_aug, recall_aug)\n\n    metrics = [precision, recall, f1_score, balanced_accuracy, mcc, auc_pr]\n    return [round(m, 6) for m in metrics]\n\nif __name__ == '__main__':\n    solve()\n```"}]}