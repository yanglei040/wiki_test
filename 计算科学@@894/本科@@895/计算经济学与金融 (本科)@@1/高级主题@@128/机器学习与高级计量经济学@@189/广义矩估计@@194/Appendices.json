{"hands_on_practices": [{"introduction": "广义矩估计 (Generalized Method of Moments, GMM) 是一种强大而灵活的估计原理。第一个练习将 GMM 剥离至其核心：最小化一个基于样本矩的准则函数。通过解决一个涉及循环数据的、可在分析上处理的问题，你将亲手推导出一个 GMM 估计量，从而为支撑整个框架的“矩匹配”原理建立起坚实的直觉 [@problem_id:2397110]。", "id": "2397110", "problem": "考虑一个计算金融中的场景，其中周期性行为在单位圆上建模。您观察到 $T$ 个独立同分布的角度 $X_1, X_2, \\ldots, X_T \\in (-\\pi, \\pi]$，它们表示日内市场活动中一个潜在周期性因子的相位。真实的相位参数为 $\\theta \\in (-\\pi, \\pi]$。模型所蕴含的结构性矩条件是\n$$\n\\mathbb{E}\\!\\left[\\sin(X_t - \\theta)\\right] = 0.\n$$\n使用广义矩估计法（GMM），并采用单位权重矩阵（一个等于 $1$ 的标量），推导出估计量 $\\hat{\\theta}_T$ 关于样本 $\\{X_t\\}_{t=1}^T$ 的闭式解析表达式。假设 $\\sum_{t=1}^{T} \\cos(X_t) > 0$，以使估计量在主值范围 $(-\\pi, \\pi]$ 内被唯一确定。请以单一的解析表达式形式提供您的最终答案。无需四舍五入，且无适用单位。", "solution": "本题要求推导一个广义矩估计法（GMM）的估计量。严格验证问题陈述是强制性的第一步。\n\n### 问题验证\n\n**第一步：提取已知条件**\n\n*   **数据**：$T$ 个独立同分布 (i.i.d.) 的角度观测值 $\\{X_t\\}_{t=1}^T$，其中每个 $X_t \\in (-\\pi, \\pi]$。\n*   **参数**：真实参数为 $\\theta \\in (-\\pi, \\pi]$。\n*   **矩条件**：总体矩条件为 $\\mathbb{E}\\!\\left[\\sin(X_t - \\theta)\\right] = 0$。\n*   **GMM 设置**：权重矩阵是单位矩阵，在这个标量情况下即为 $W=1$。\n*   **假设**：给定 $\\sum_{t=1}^{T} \\cos(X_t) > 0$。\n*   **目标**：推导估计量 $\\hat{\\theta}_T$ 的闭式解析表达式。\n\n**第二步：使用提取的已知条件进行验证**\n\n*   **科学基础**：该问题是在计量经济学估计（GMM）的标准框架内提出的。矩条件 $\\mathbb{E}[\\sin(X_t - \\theta)] = 0$ 是为循环数据定义中心参数的一种常用且有效的方法，例如在 von Mises 分布中，$\\theta$ 代表平均方向。该问题具有科学合理性。\n*   **适定性**：该问题旨在寻求一个唯一的估计量。所给定的假设 $\\sum_{t=1}^{T} \\cos(X_t) > 0$ 被明确地引入，以解决反正切函数所固有的模糊性，从而确保在指定的主值范围内存在唯一解。该问题是适定的。\n*   **客观性**：该问题使用精确的数学语言和定义进行陈述，完全没有主观或含糊的术语。\n*   **完整性与一致性**：推导估计量所需的所有信息都已提供：矩条件、样本数据结构、估计框架（GMM 及指定的权重矩阵），以及确保唯一性的关键假设。整个设置是自洽且无矛盾的。\n\n**第三步：结论与行动**\n\n该问题是有效的。这是一个定义明确的统计估计任务。我将继续进行推导。\n\n### 估计量的推导\n\n广义矩估计法（GMM）的估计量 $\\hat{\\theta}_T$ 是使 GMM 目标函数 $J(\\theta)$ 最小化的参数 $\\theta$ 的值。该函数是样本矩条件的二次型。\n\n总体矩条件由 $g(\\theta) = \\mathbb{E}[\\sin(X_t - \\theta)] = 0$ 给出。\n相应的样本矩条件是其经验模拟：\n$$\ng_T(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\sin(X_t - \\theta)\n$$\nGMM 目标函数为 $J(\\theta) = T \\cdot g_T(\\theta)' W g_T(\\theta)$。在本题中，我们只有一个矩条件，因此 $g_T(\\theta)$ 是一个标量。权重矩阵被指定为单位矩阵，即 $W=1$。目标函数简化为：\n$$\nJ(\\theta) = T \\left( \\frac{1}{T} \\sum_{t=1}^{T} \\sin(X_t - \\theta) \\right)^2 = \\frac{1}{T} \\left( \\sum_{t=1}^{T} \\sin(X_t - \\theta) \\right)^2\n$$\nGMM 估计量 $\\hat{\\theta}_T$ 通过最小化 $J(\\theta)$ 关于 $\\theta$ 来找到：\n$$\n\\hat{\\theta}_T = \\arg\\min_{\\theta \\in (-\\pi, \\pi]} J(\\theta)\n$$\n由于 $J(\\theta)$ 是一个由正常数 $1/T$ 缩放的平方量，其最小值为 $0$。当且仅当平方内的项为零时，才能达到此最小值。因此，最小化问题等价于求解以下关于 $\\theta$ 的方程：\n$$\n\\sum_{t=1}^{T} \\sin(X_t - \\hat{\\theta}_T) = 0\n$$\n为了求解 $\\hat{\\theta}_T$，我们应用差角正弦的三角恒等式 $\\sin(A - B) = \\sin(A)\\cos(B) - \\cos(A)\\sin(B)$：\n$$\n\\sum_{t=1}^{T} \\left[ \\sin(X_t)\\cos(\\hat{\\theta}_T) - \\cos(X_t)\\sin(\\hat{\\theta}_T) \\right] = 0\n$$\n由于 $\\cos(\\hat{\\theta}_T)$ 和 $\\sin(\\hat{\\theta}_T)$ 相对于求和索引 $t$ 是常数，我们可以将它们提出：\n$$\n\\cos(\\hat{\\theta}_T) \\sum_{t=1}^{T} \\sin(X_t) - \\sin(\\hat{\\theta}_T) \\sum_{t=1}^{T} \\cos(X_t) = 0\n$$\n整理各项，我们得到：\n$$\n\\sin(\\hat{\\theta}_T) \\sum_{t=1}^{T} \\cos(X_t) = \\cos(\\hat{\\theta}_T) \\sum_{t=1}^{T} \\sin(X_t)\n$$\n我们定义 $S_T = \\sum_{t=1}^{T} \\sin(X_t)$ 和 $C_T = \\sum_{t=1}^{T} \\cos(X_t)$。方程变为 $\\sin(\\hat{\\theta}_T) C_T = \\cos(\\hat{\\theta}_T) S_T$。\n\n我们必须确保可以进行除法以分离出 $\\hat{\\theta}_T$。问题陈述了假设 $C_T = \\sum_{t=1}^{T} \\cos(X_t) > 0$。这意味着 $C_T \\neq 0$。此外，如果 $\\cos(\\hat{\\theta}_T) = 0$，则方程将变为 $\\sin(\\hat{\\theta}_T) C_T = 0$。由于 $C_T \\neq 0$，这将要求 $\\sin(\\hat{\\theta}_T) = 0$，这是不可能的，因为 $\\sin(\\theta)$ 和 $\\cos(\\theta)$ 永远不会同时为零。因此，$\\cos(\\hat{\\theta}_T) \\neq 0$。\n\n我们可以安全地将方程两边同时除以 $\\cos(\\hat{\\theta}_T)$ 和 $C_T$：\n$$\n\\frac{\\sin(\\hat{\\theta}_T)}{\\cos(\\hat{\\theta}_T)} = \\tan(\\hat{\\theta}_T) = \\frac{\\sum_{t=1}^{T} \\sin(X_t)}{\\sum_{t=1}^{T} \\cos(X_t)}\n$$\n这个方程定义了估计量 $\\hat{\\theta}_T$。为了获得唯一解，我们必须正确地对正切函数求逆。方程 $\\sin(\\hat{\\theta}_T) C_T = \\cos(\\hat{\\theta}_T) S_T$ 意味着向量 $(\\cos(\\hat{\\theta}_T), \\sin(\\hat{\\theta}_T))$ 与向量 $(C_T, S_T)$ 平行。由于 $(\\cos(\\theta), \\sin(\\theta))$ 的模为 $1$，这意味着 $\\cos(\\hat{\\theta}_T)$ 必须与 $C_T$ 具有相同的符号。\n\n鉴于明确的假设 $C_T = \\sum_{t=1}^{T} \\cos(X_t) > 0$，我们必须有 $\\cos(\\hat{\\theta}_T) > 0$。这个条件将 $\\hat{\\theta}_T$ 的解限制在区间 $(-\\frac{\\pi}{2}, \\frac{\\pi}{2})$ 内。标准反正切函数 $\\arctan(\\cdot)$ 的主值范围恰好是 $(-\\frac{\\pi}{2}, \\frac{\\pi}{2})$。因此，模糊性得以解决，唯一的 GMM 估计量由下式给出：\n$$\n\\hat{\\theta}_T = \\arctan\\left(\\frac{\\sum_{t=1}^{T} \\sin(X_t)}{\\sum_{t=1}^{T} \\cos(X_t)}\\right)\n$$\n此表达式按要求提供了估计量 $\\hat{\\theta}_T$ 关于样本数据的闭式解析解。", "answer": "$$\n\\boxed{\\arctan\\left(\\frac{\\sum_{t=1}^{T} \\sin(X_t)}{\\sum_{t=1}^{T} \\cos(X_t)}\\right)}\n$$"}, {"introduction": "在掌握了基本原理之后，我们现在将 GMM 应用于一个强大且实用的计量经济学技术：分位数回归。这个练习将指导你实现一个两步 GMM 估计量——这是实证研究中的主力方法——来估计一个超越简单均值的模型。你将学习如何将分位数回归的理论最优性条件转化为一组适用于 GMM 估计的矩条件 [@problem_id:2397079]。", "id": "2397079", "problem": "您的任务是，使用由检验损失（check loss）推导出的矩条件，为一个线性分位数回归模型构建并计算一个广义矩估计 (GMM) 估计量。考虑观测值 $\\{(y_i, x_i, z_i)\\}_{i=1}^n$，其中 $y_i \\in \\mathbb{R}$ 是因变量，$x_i \\in \\mathbb{R}^p$ 是一个回归量行向量，$z_i \\in \\mathbb{R}^{p_z}$ 是一个工具变量行向量。线性分位数回归模型将给定 $x_i$ 时 $y_i$ 的第 $\\tau$ 个条件分位数指定为 $Q_{\\tau}(y_i \\mid x_i) = x_i^{\\prime}\\beta$，其中 $\\beta \\in \\mathbb{R}^p$ 是一个参数向量，$\\tau \\in (0,1)$ 是一个分位数指数。检验函数定义为 $\\rho_{\\tau}(u) = u(\\tau - I(u<0))$，其中 $I(\\cdot)$ 是指示函数。\n\n从检验函数的定义以及分位数损失最小化的次梯度最优性出发，推导一个适用于基于工具变量的 GMM 的有效无条件矩条件，然后定义用于估计 $\\beta$ 的 GMM 样本目标函数。您的推导必须基于基本原理，依据检验函数、凸函数的次梯度以及真实参数处总体矩为零的矩估计原理。请勿为目标矩条件假设或使用任何现成的快捷公式。\n\n您的程序必须使用以下设计实现一个两步 GMM 估计量：\n\n- 使用样本矩函数 $g_n(\\beta) = \\frac{1}{n}\\sum_{i=1}^n z_i \\cdot \\psi_{\\tau}(y_i - x_i^{\\prime}\\beta)$，其中 $\\psi_{\\tau}(u)$ 是 $\\rho_{\\tau}(u)$ 的一个次梯度，以及 GMM 目标函数 $J_n(\\beta; W) = g_n(\\beta)^{\\prime} W g_n(\\beta)$，其中 $W \\in \\mathbb{R}^{p_z \\times p_z}$ 是一个正定权重矩阵。\n- 第 1 步：使用单位权重矩阵 $W = I_{p_z}$，通过在 $\\beta \\in \\mathbb{R}^p$ 上最小化 $J_n(\\beta; I_{p_z})$ 来获得初步估计值 $\\hat{\\beta}_1$。\n- 第 2 步：使用矩协方差的独立同分布 (i.i.d.) 近似 $S \\approx \\tau(1-\\tau)\\,\\mathbb{E}[z_i z_i^{\\prime}]$ 计算一个可行的权重矩阵，并通过其样本模拟 $\\hat{S} = \\tau(1-\\tau)\\,\\frac{1}{n}\\sum_{i=1}^n z_i z_i^{\\prime}$ 对其进行估计。使用 $W = \\hat{S}^{-1}$ (如有必要，使用 Moore–Penrose 伪逆)，并最小化 $J_n(\\beta; \\hat{S}^{-1})$ 以获得两步 GMM 估计值 $\\hat{\\beta}_{\\text{GMM}}$。\n\n您的程序必须使用一个确定性的、无导数的全局-局部搜索过程来解决最小化问题：\n\n- 按如下方式构建一个合理参数值的超矩形。如果 $X$ 的第一列是截距项（全为1），则使用 $y$ 的稳健位置和尺度来定义截距的边界：设 $q_{0.1}$ 和 $q_{0.9}$ 分别为 $y$ 的 $0.1$ 和 $0.9$ 样本分位数，并设 $\\text{IQR} = q_{0.75} - q_{0.25}$。截距被限制在 $[q_{0.1} - 2\\,\\text{IQR},\\; q_{0.9} + 2\\,\\text{IQR}]$ 内。对于每个非截距项回归量 $x_{\\cdot j}$，定义 $R_j = 10 \\cdot \\frac{\\text{sd}(y)}{\\max(\\text{sd}(x_{\\cdot j}), 10^{-8})}$，将边界的中心设在该回归量的普通最小二乘系数处（在可行时计算），并将该系数限制在 $[\\hat{\\beta}^{\\text{OLS}}_j - R_j,\\; \\hat{\\beta}^{\\text{OLS}}_j + R_j]$ 内。如果不存在截距项，则对所有系数应用此规则，在可行时使用普通最小二乘中心；否则以零为中心。\n- 在此超矩形上，对每个维度使用恰好 81 个等距点进行粗略网格搜索，以最小化 $J_n(\\beta; W)$ 并选择最佳网格点，若出现平局，则选择 $\\beta$ 的欧几里得范数最小的候选点。\n- 构建一个以最佳粗略解为中心的精细化超矩形，其在每个维度上的半宽等于 $\\max\\{0.25 \\times \\text{原始半宽}, 10^{-3}\\}$。在此精细化区域上，再次对每个维度使用恰好 81 个点进行第二次网格搜索，并采用相同的平局打破规则。\n- 将精细网格解作为第 1 步中的 $\\hat{\\beta}_1$。然后计算 $\\hat{S}$ 和 $W = \\hat{S}^{-1}$，并重复两阶段网格搜索（先粗略后精细），以生成最终的两步 GMM 估计值 $\\hat{\\beta}_{\\text{GMM}}$。\n\n对于以下所有测试用例，使用与回归量相等的工具变量，即 $z_i = x_i$ 且 $Z = X$。\n\n请实现您的程序以解决以下三个测试用例。每个测试用例提供 $(y, X, \\tau)$，其中 $Z=X$。\n\n- 测试 1（理想路径，带截距和斜率的中位数回归）：\n  - $n=12$, $x = (-2.0, -1.5, -1.0, -0.3, 0.0, 0.2, 0.5, 1.0, 1.2, 1.8, 2.2, 2.5)$ 且 $X = [\\mathbf{1}, x]$。\n  - $y$ 由 $y_i = 1.0 + 0.8\\,x_i + \\varepsilon_i$ 确定性地生成，其中 $\\varepsilon = (0.1, -0.2, 0.05, 0.1, -0.05, 0.0, 0.2, -0.1, 0.0, 0.15, -0.15, 0.05)$。\n  - $\\tau = 0.5$。\n- 测试 2（边界结构，针对较低分位数的仅截距模型）：\n  - $n=11$, $X = \\mathbf{1}$, $y = (2.2, 1.0, 1.5, 0.8, 3.2, 2.0, 1.8, 2.5, 2.3, 0.9, 1.7)$。\n  - $\\tau = 0.25$。\n- 测试 3（对高分位数处异常值的稳健性）：\n  - $n=15$, $x = (-1.0, -0.5, 0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4)$ 且 $X = [\\mathbf{1}, x]$。\n  - $y_i = 0.5 + 0.6\\,x_i + \\eta_i$ 其中 $\\eta = (0.0, 0.1, -0.05, 0.0, 0.02, -0.03, 0.04, -0.02, 0.01, 0.0, -0.05, 0.03, 0.02, 8.0, -0.04)$。\n  - $\\tau = 0.9$。\n\n您的程序应为每个测试用例输出两步 GMM 估计值 $\\hat{\\beta}_{\\text{GMM}}$，其形式为列表的列表，其中每个内部列表是对应于该测试用例的参数向量，所有条目均四舍五入到六位小数。最终输出必须是包含这三个列表的单行文本，列表为逗号分隔的序列，并用方括号括起来，例如 $[\\,[\\cdot,\\cdot],\\,[\\cdot],\\,[\\cdot,\\cdot]\\,]$。\n\n所有量都是无量纲的；不涉及物理单位。不使用角度。输出必须仅为浮点数列表。程序不得读取任何输入，也不得访问任何外部文件或网络。它必须使用如上所述的确定性过程，以确保相同的输入产生相同的输出。允许使用的数值库包括 Python 标准库、NumPy 和 SciPy。", "solution": "该问题要求为一个线性分位数回归模型构建和实现一个两步广义矩估计 (GMM) 估计量。该过程首先对基础矩条件进行形式化推导，然后解释指定的数值估计算法。\n\n**1. GMM 矩条件的推导**\n\n线性分位数回归模型假定，在给定回归量向量 $x_i \\in \\mathbb{R}^p$ 的条件下，因变量 $y_i$ 的第 $\\tau$ 个条件分位数是 $x_i$ 的线性函数：\n$$\nQ_{\\tau}(y_i \\mid x_i) = x_i^{\\prime}\\beta_0\n$$\n其中 $\\beta_0 \\in \\mathbb{R}^p$ 是真实参数向量，而 $x_i^{\\prime}$ 是一个行向量。$\\beta_0$ 的标准估计量通过最小化检验损失之和来找到：\n$$\n\\hat{\\beta} = \\arg\\min_{\\beta \\in \\mathbb{R}^p} \\sum_{i=1}^n \\rho_{\\tau}(y_i - x_i^{\\prime}\\beta)\n$$\n检验函数 $\\rho_{\\tau}(u): \\mathbb{R} \\to \\mathbb{R}$ 定义为：\n$$\n\\rho_{\\tau}(u) = u(\\tau - I(u<0))\n$$\n其中 $I(\\cdot)$ 是指示函数，当其参数为真时等于 1，否则为 0。该函数是凸函数，因此，达到最小值的充分必要条件是，在解 $\\hat{\\beta}$ 处，零向量必须是目标函数次梯度的一个元素。\n\n目标函数关于 $\\beta$ 的次梯度是使用次梯度的链式法则获得的。设 $u_i(\\beta) = y_i - x_i^{\\prime}\\beta$。关于 $\\beta$ 的次梯度为：\n$$\n\\partial_{\\beta} \\left( \\sum_{i=1}^n \\rho_{\\tau}(u_i(\\beta)) \\right) = \\sum_{i=1}^n \\partial_{\\beta} \\rho_{\\tau}(u_i(\\beta)) = \\sum_{i=1}^n (-x_i) \\cdot \\partial_u \\rho_{\\tau}(u_i(\\beta))\n$$\n检验函数 $\\rho_{\\tau}(u)$ 关于其参数 $u$ 的次梯度由下式给出：\n$$\n\\partial_u \\rho_{\\tau}(u) =\n\\begin{cases}\n    \\{\\tau - 1\\} & \\text{if } u < 0 \\\\\n    [\\tau - 1, \\tau] & \\text{if } u = 0 \\\\\n    \\{\\tau\\} & \\text{if } u > 0\n\\end{cases}\n$$\n问题指定使用函数 $\\psi_{\\tau}(u) = \\tau - I(u<0)$ 作为该次梯度集中的一个选择。注意，对于 $u=0$，$\\psi_{\\tau}(0) = \\tau$，这是区间 $[\\tau - 1, \\tau]$ 中的一个有效选择。\n\n最小化的一阶条件是零必须在次梯度集中，可以表述为：\n$$\n\\frac{1}{n} \\sum_{i=1}^n x_i \\psi_{\\tau}(y_i - x_i^{\\prime}\\hat{\\beta}) \\approx \\mathbf{0}\n$$\n通过类比，在真实参数 $\\beta_0$ 处精确成立的总体矩条件是 $\\mathbb{E}[x_i \\psi_{\\tau}(y_i - x_i^{\\prime}\\beta_0)] = \\mathbf{0}$。对于 GMM 估计，我们通过使用一个工具变量向量 $z_i \\in \\mathbb{R}^{p_z}$ 来推广此条件，该工具变量被假定与分位数误差项不相关。总体矩条件变为：\n$$\n\\mathbb{E}[z_i^{\\prime} \\psi_{\\tau}(y_i - x_i^{\\prime}\\beta_0)] = \\mathbf{0}\n$$\n在这里，我们将 $z_i^{\\prime}$ 解释为一个 $p_z \\times 1$ 的列向量。相应的样本矩向量是候选参数向量 $\\beta$ 的函数，是其样本模拟：\n$$\ng_n(\\beta) = \\frac{1}{n} \\sum_{i=1}^n z_i^{\\prime} \\psi_{\\tau}(y_i - x_i^{\\prime}\\beta)\n$$\n\n**2. 两步 GMM 估计**\n\nGMM 估计量 $\\hat{\\beta}_{\\text{GMM}}$ 最小化样本矩向量的一个二次型：\n$$\nJ_n(\\beta; W) = g_n(\\beta)^{\\prime} W g_n(\\beta)\n$$\n其中 $W$ 是一个 $p_z \\times p_z$ 的正定权重矩阵。估计过程分两步进行。\n\n**第 1 步：** 使用一个次优但简单的权重矩阵，获得 $\\beta$ 的一个初步的、一致的估计。按照规定，使用单位矩阵 $W_1 = I_{p_z}$。第一步估计量是：\n$$\n\\hat{\\beta}_1 = \\arg\\min_{\\beta} J_n(\\beta; I_{p_z}) = \\arg\\min_{\\beta} g_n(\\beta)^{\\prime} g_n(\\beta)\n$$\n\n**第 2 步：** 通过使用一个权重矩阵来获得一个渐近有效估计量，该权重矩阵是样本矩的渐近协方差矩阵的逆的一致估计。问题为该协方差矩阵指定了一个特定的近似 $\\hat{S}$，由下式给出：\n$$\n\\hat{S} = \\tau(1-\\tau) \\left( \\frac{1}{n} \\sum_{i=1}^n z_i^{\\prime} z_i \\right) = \\frac{\\tau(1-\\tau)}{n} Z^{\\prime}Z\n$$\n其中 $Z$ 是 $n \\times p_z$ 的工具变量矩阵。第二步的权重矩阵是 $W_2 = \\hat{S}^{-1}$。为保证数值稳定性，使用 Moore-Penrose 伪逆。然后，两步 GMM 估计量是：\n$$\n\\hat{\\beta}_{\\text{GMM}} = \\arg\\min_{\\beta} J_n(\\beta; \\hat{S}^{-1})\n$$\n\n**3. 通过网格搜索进行数值最小化**\n\n两个最小化问题都使用一个确定性的两阶段网格搜索过程来解决。\n\n**搜索空间定义：** 基于样本数据构建一个用于 $\\beta$ 的超矩形搜索空间。\n- 对于截距参数，搜索区间为 $[q_{0.1} - 2\\,\\text{IQR}, q_{0.9} + 2\\,\\text{IQR}]$，其中 $q$ 表示 $y$ 的样本分位数，$\\text{IQR} = q_{0.75} - q_{0.25}$。\n- 对于非截距系数 $\\beta_j$，区间以普通最小二乘 (OLS) 估计值 $\\hat{\\beta}^{\\text{OLS}}_j$ 为中心，其范围由数据变异性决定：$[\\hat{\\beta}^{\\text{OLS}}_j - R_j, \\hat{\\beta}^{\\text{OLS}}_j + R_j]$，其中 $R_j = 10 \\cdot \\frac{\\text{sd}(y)}{\\max(\\text{sd}(x_{\\cdot j}), 10^{-8})}$。如果 OLS 不可行或没有截距项，则区间以 0 为中心。\n\n**网格搜索过程：**\n1.  **粗略网格搜索：** 在初始超矩形的每个维度上，对 81 个等距点评估目标函数 $J_n(\\beta; W)$。选择产生最小目标值的参数向量 $\\beta$。若出现平局，则选择欧几里得范数最小的候选向量。\n2.  **精细网格搜索：** 定义一个新的、更小的超矩形。它以粗略搜索的解为中心。其在每个维度上的半宽为 $\\max\\{0.25 \\times \\text{原始半宽}, 10^{-3}\\}$。在此精细区域上，对每个维度使用 81 个点进行第二次网格搜索。其结果即为该 GMM 步骤的解。\n\n为找到 $\\hat{\\beta}_1$，对第 1 步执行这整个两阶段搜索过程，然后为第 2 步重复此过程（包括重新定义初始搜索空间）以找到最终的估计量 $\\hat{\\beta}_{\\text{GMM}}$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the GMM estimation for all specified test cases.\n    \"\"\"\n\n    def gmm_objective(beta, Y, X, Z, tau, W):\n        \"\"\"\n        Computes the GMM objective function J(beta; W) = g_n(beta)' * W * g_n(beta).\n        \"\"\"\n        n_obs, p_dim = X.shape\n        beta = np.asarray(beta).reshape(p_dim, 1)\n\n        residuals = Y.reshape(n_obs, 1) - X @ beta\n        psi_vals = tau - (residuals < 0).astype(float)\n        \n        # moment_vals has shape (n_obs, p_z)\n        moment_vals = Z * psi_vals\n        \n        # g_n is the mean of moment_vals, shape (p_z,)\n        g_n = np.mean(moment_vals, axis=0)\n        \n        # J = g_n' W g_n\n        objective_value = g_n.T @ W @ g_n\n        return objective_value\n\n    def build_search_bounds(Y, X):\n        \"\"\"\n        Constructs the hyper-rectangle for the grid search based on problem specification.\n        \"\"\"\n        n_obs, p_dim = X.shape\n        bounds = np.zeros((p_dim, 2))\n        \n        has_intercept = np.all(X[:, 0] == 1)\n        \n        try:\n            # Use lstsq for robust OLS estimation\n            beta_ols, _, _, _ = np.linalg.lstsq(X, Y, rcond=None)\n            ols_feasible = True\n        except np.linalg.LinAlgError:\n            beta_ols = np.zeros(p_dim)\n            ols_feasible = False\n            \n        sd_y = np.std(Y)\n\n        for j in range(p_dim):\n            if j == 0 and has_intercept:\n                q = np.quantile(Y, [0.1, 0.9, 0.25, 0.75])\n                q01, q09, q025, q075 = q[0], q[1], q[2], q[3]\n                iqr = q075 - q025\n                bounds[j, :] = [q01 - 2 * iqr, q09 + 2 * iqr]\n            else:\n                center = beta_ols[j] if ols_feasible else 0.0\n                sd_xj = np.std(X[:, j])\n                R_j = 10.0 * sd_y / max(sd_xj, 1e-8)\n                bounds[j, :] = [center - R_j, center + R_j]\n                \n        return bounds\n\n    def grid_search_minimizer(obj_func, bounds):\n        \"\"\"\n        Performs the two-stage (coarse + refined) grid search.\n        \"\"\"\n        p_dim = bounds.shape[0]\n        n_points = 81\n\n        # --- Coarse Grid Search ---\n        grid_axes = [np.linspace(b[0], b[1], n_points) for b in bounds]\n        grid_mesh = np.meshgrid(*grid_axes)\n        grid_points = np.vstack([m.ravel() for m in grid_mesh]).T\n\n        min_obj_val = np.inf\n        best_beta_coarse = None\n\n        for beta_candidate in grid_points:\n            val = obj_func(beta_candidate)\n            if val < min_obj_val:\n                min_obj_val = val\n                best_beta_coarse = beta_candidate\n            elif np.isclose(val, min_obj_val):\n                if np.linalg.norm(beta_candidate) < np.linalg.norm(best_beta_coarse):\n                    best_beta_coarse = beta_candidate\n        \n        # --- Refined Grid Search ---\n        refined_bounds = np.zeros_like(bounds)\n        for j in range(p_dim):\n            original_half_width = (bounds[j, 1] - bounds[j, 0]) / 2.0\n            new_half_width = max(0.25 * original_half_width, 1e-3)\n            refined_bounds[j, :] = [best_beta_coarse[j] - new_half_width, best_beta_coarse[j] + new_half_width]\n\n        refined_grid_axes = [np.linspace(b[0], b[1], n_points) for b in refined_bounds]\n        refined_grid_mesh = np.meshgrid(*refined_grid_axes)\n        refined_grid_points = np.vstack([m.ravel() for m in refined_grid_mesh]).T\n\n        min_obj_val_refined = np.inf\n        best_beta_refined = None\n\n        for beta_candidate in refined_grid_points:\n            val = obj_func(beta_candidate)\n            if val < min_obj_val_refined:\n                min_obj_val_refined = val\n                best_beta_refined = beta_candidate\n            elif np.isclose(val, min_obj_val_refined):\n                if np.linalg.norm(beta_candidate) < np.linalg.norm(best_beta_refined):\n                    best_beta_refined = beta_candidate\n        \n        return best_beta_refined\n\n    # --- Test Case Definitions ---\n    test_cases = [\n        {\n            \"y\": np.array([1.0 + 0.8 * x_i + eps_i for x_i, eps_i in zip(\n                [-2.0, -1.5, -1.0, -0.3, 0.0, 0.2, 0.5, 1.0, 1.2, 1.8, 2.2, 2.5],\n                [0.1, -0.2, 0.05, 0.1, -0.05, 0.0, 0.2, -0.1, 0.0, 0.15, -0.15, 0.05]\n            )]),\n            \"X\": np.array([\n                [1.0, -2.0], [1.0, -1.5], [1.0, -1.0], [1.0, -0.3], [1.0, 0.0], [1.0, 0.2],\n                [1.0, 0.5], [1.0, 1.0], [1.0, 1.2], [1.0, 1.8], [1.0, 2.2], [1.0, 2.5]\n            ]),\n            \"tau\": 0.5\n        },\n        {\n            \"y\": np.array([2.2, 1.0, 1.5, 0.8, 3.2, 2.0, 1.8, 2.5, 2.3, 0.9, 1.7]),\n            \"X\": np.ones((11, 1)),\n            \"tau\": 0.25\n        },\n        {\n            \"y\": np.array([0.5 + 0.6 * x_i + eta_i for x_i, eta_i in zip(\n                [-1.0, -0.5, 0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4],\n                [0.0, 0.1, -0.05, 0.0, 0.02, -0.03, 0.04, -0.02, 0.01, 0.0, -0.05, 0.03, 0.02, 8.0, -0.04]\n            )]),\n            \"X\": np.array([\n                [1.0, -1.0], [1.0, -0.5], [1.0, 0.0], [1.0, 0.2], [1.0, 0.4], [1.0, 0.6],\n                [1.0, 0.8], [1.0, 1.0], [1.0, 1.2], [1.0, 1.4], [1.0, 1.6], [1.0, 1.8],\n                [1.0, 2.0], [1.0, 2.2], [1.0, 2.4]\n            ]),\n            \"tau\": 0.9\n        }\n    ]\n\n    final_results = []\n\n    for case in test_cases:\n        Y, X, tau = case[\"y\"], case[\"X\"], case[\"tau\"]\n        Z = X  # Instruments are regressors\n        n, pz = Z.shape\n        \n        # Define search space once for both steps\n        bounds = build_search_bounds(Y, X)\n\n        # --- Step 1 GMM ---\n        W1 = np.identity(pz)\n        obj_func_1 = lambda beta: gmm_objective(beta, Y, X, Z, tau, W1)\n        # We don't need beta_1, but we run the procedure as specified.\n        # The problem statement says to repeat the two-stage grid search for Step 2.\n        # This implies running the full procedure, though beta_1 is not used further.\n        _ = grid_search_minimizer(obj_func_1, bounds)\n        \n        # --- Step 2 GMM ---\n        S_hat = tau * (1.0 - tau) * (Z.T @ Z) / n\n        W2 = np.linalg.pinv(S_hat)\n        obj_func_2 = lambda beta: gmm_objective(beta, Y, X, Z, tau, W2)\n        beta_gmm_2_step = grid_search_minimizer(obj_func_2, bounds)\n        \n        # Round and format results\n        result = [round(b, 6) for b in beta_gmm_2_step]\n        final_results.append(result)\n\n    # Format final output string\n    output_str = \"[\"\n    for i, res in enumerate(final_results):\n        res_str = f\"[{','.join(map(str, res))}]\"\n        output_str += res_str\n        if i < len(final_results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    # Print in the exact required format\n    # The f-string representation of lists might have spaces, so we build it manually\n    print(output_str)\n\nsolve()\n```"}]}