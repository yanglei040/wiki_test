{"hands_on_practices": [{"introduction": "理论学习之后，最好的检验方法就是付诸实践。这个练习将带你进入一个真实的计算金融场景：利用支持向量机（SVM）预测抵押贷款是否违约。你将通过实践学习如何使用k折交叉验证来比较线性核与高斯径向基函数（RBF）核的性能，并根据模型表现推断信用风险数据的内在结构。[@problem_id:2435431] 这个练习不仅能锻炼你的编程和模型评估技能，还能让你深入理解核函数的选择对于解决实际问题的重要性。", "id": "2435431", "problem": "给定一个计算经济学和金融学中的二元分类任务：使用支持向量机 (SVM) 预测抵押贷款违约。标签为 $y \\in \\{-1, +1\\}$，其中 $+1$ 表示违约，$-1$ 表示无违约。每个观测值有 $3$ 个特征：贷款与价值比率 $(x_1)$（小数），债务与收入比率 $(x_2)$（小数），以及 FICO 分数 $(x_3)$（整数）。数据集包含由 $i \\in \\{1,2,\\ldots,20\\}$ 索引的 $20$ 个观测值，表示为 $(x_{i1}, x_{i2}, x_{i3}; y_i)$：\n- $1$: $(0.65, 0.18, 780; -1)$\n- $2$: $(0.70, 0.20, 760; -1)$\n- $3$: $(0.75, 0.25, 740; -1)$\n- $4$: $(0.80, 0.22, 770; -1)$\n- $5$: $(0.68, 0.30, 720; -1)$\n- $6$: $(0.72, 0.28, 730; -1)$\n- $7$: $(0.85, 0.20, 750; -1)$\n- $8$: $(0.90, 0.18, 760; -1)$\n- $9$: $(0.78, 0.26, 740; -1)$\n- $10$: $(0.82, 0.27, 735; -1)$\n- $11$: $(0.95, 0.45, 660; +1)$\n- $12$: $(1.02, 0.40, 680; +1)$\n- $13$: $(0.88, 0.55, 620; +1)$\n- $14$: $(0.92, 0.50, 600; +1)$\n- $15$: $(1.05, 0.35, 650; +1)$\n- $16$: $(0.90, 0.60, 590; +1)$\n- $17$: $(0.98, 0.48, 630; +1)$\n- $18$: $(1.10, 0.30, 610; +1)$\n- $19$: $(0.84, 0.58, 605; +1)$\n- $20$: $(0.70, 0.40, 580; +1)$\n\n构建一个软间隔二元支持向量机 (SVM) 分类器，并使用 $k=5$ 的 k 折交叉验证来评估其样本外分类准确率。使用以下三个参数集作为测试套件：\n- 测试 $A$：线性核，软间隔参数 $C = 10$。\n- 测试 $B$：高斯径向基函数 (RBF) 核，参数 $C = 10$ 且 $\\gamma = 0.5$。\n- 测试 $C$：高斯径向基函数 (RBF) 核，参数 $C = 10$ 且 $\\gamma = 2.0$。\n\n对于测试 $A$、 $B$ 和 $C$ 中的每一个，计算平均 $5$ 折样本外分类准确率，结果为 $[0,1]$ 内的实数。将每个准确率四舍五入到三位小数。\n\n最后，推断核函数的选择对从特征到违约的信用风险映射的性质意味着什么。输出一个按如下方式定义的整数指示符：如果 RBF 准确率中的最佳值（来自测试 $B$ 或 $C$）超过线性核准确率（来自测试 $A$）至少 $0.03$，则输出 $1$；否则输出 $0$。\n\n您的程序应生成单行输出，其中包含以逗号分隔的列表形式的结果，并用方括号括起，顺序为 $[\\text{acc}_A,\\text{acc}_B,\\text{acc}_C,\\text{indicator}]$，其中 $\\text{acc}_A$ 是测试 $A$ 的平均准确率，$\\text{acc}_B$ 是测试 $B$ 的，$\\text{acc}_C$ 是测试 $C$ 的，而 $\\text{indicator}$ 是上述整数。例如，格式应为 $[0.842,0.902,0.881,1]$。", "solution": "所呈现的问题是机器学习中一个标准的二元分类任务，应用于计算金融领域。目标是构建并评估一个用于预测抵押贷款违约的软间隔支持向量机 (SVM) 分类器。该问题具有科学依据，提法明确，并提供了所有必要的数据和参数。因此，该问题被认为是有效的，并将构建一个正式的解决方案。\n\n解决方案的核心涉及求解软间隔 SVM 的对偶优化问题。对于一个包含 $N$ 个数据点 $(\\mathbf{x}_i, y_i)$ 的训练集，其中 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 是特征向量，而 $y_i \\in \\{-1, +1\\}$ 是标签，其对偶问题是一个二次规划 (QP) 问题，表述如下：\n$$\n\\max_{\\boldsymbol{\\alpha}} \\mathcal{L}_D(\\boldsymbol{\\alpha}) = \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_i \\alpha_j y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j)\n$$\n受限于以下约束条件：\n$$\n\\sum_{i=1}^{N} \\alpha_i y_i = 0\n$$\n$$\n0 \\le \\alpha_i \\le C, \\quad \\text{for } i=1, \\ldots, N\n$$\n此处，$\\boldsymbol{\\alpha} = (\\alpha_1, \\ldots, \\alpha_N)$ 是拉格朗日乘子向量，$C$ 是控制错分类惩罚的正则化参数，$K(\\mathbf{x}_i, \\mathbf{x}_j)$ 是核函数。该问题通过最小化对偶拉格朗日函数的负数 $-\\mathcal{L}_D(\\boldsymbol{\\alpha})$ 来求解，这等价于最小化 $L(\\boldsymbol{\\alpha}) = \\frac{1}{2} \\boldsymbol{\\alpha}^T P \\boldsymbol{\\alpha} - \\mathbf{1}^T \\boldsymbol{\\alpha}$，其中矩阵 $P$ 的元素为 $P_{ij} = y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j)$。\n\n问题指定了两种类型的核函数：\n$1$. **线性核**：此核函数对应于原始特征空间中的线性决策边界。其定义为：\n$$\nK(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^T \\mathbf{x}_j\n$$\n$2$. **高斯径向基函数 (RBF) 核**：此核函数通过将数据映射到无限维特征空间来允许非线性决策边界。其定义为：\n$$\nK(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2)\n$$\n其中 $\\gamma > 0$ 是一个控制高斯函数宽度的参数。\n\n该解决方案通过使用 `scipy.optimize.minimize` 中可用的序列最小二乘规划 (SLSQP) 算法来数值求解此 QP 问题来实现。在找到最优向量 $\\boldsymbol{\\alpha}^*$ 后，计算偏置项 $b$。一个稳健的方法是对所有满足 $0 < \\alpha_i^* < C$ 的支持向量进行平均，因为对于这些点，Karush-Kuhn-Tucker (KKT) 条件意味着间隔被精确满足：\n$$\nb = y_s - \\sum_{i=1}^{N} \\alpha_i^* y_i K(\\mathbf{x}_i, \\mathbf{x}_s)\n$$\n如果不存在这样的非边界支持向量，$b$ 则通过取由有界支持向量（$\\alpha_i^* = C$）的间隔约束定义的可行区间的中点来确定。\n\n那么，对于一个新数据点 $\\mathbf{x}$ 的决策函数由下式给出：\n$$\nf(\\mathbf{x}) = \\text{sign}\\left(\\sum_{i=1}^{N} \\alpha_i^* y_i K(\\mathbf{x}_i, \\mathbf{x}) + b\\right)\n$$\n\n该方法中的一个关键步骤是**特征缩放**。输入特征（$x_1, x_2, x_3$）的尺度差异巨大。否则，FICO 分数 $x_3$ 将在任何基于距离的计算中（例如在 RBF 核中）占主导地位。为解决此问题，我们对特征应用标准化（Z-score 归一化）。在交叉验证的每一折中，从训练数据计算均值和标准差，然后用它们来缩放该折的训练数据和测试数据。这可以防止测试集的数据泄露到训练过程中。\n\n模型的性能使用 $k=5$ 的 k 折交叉验证进行评估。包含 $20$ 个观测值的数据集被划分为 $5$ 个不相交的折，每折包含 $4$ 个观测值。在 $5$ 次迭代的每一次中，使用一折作为测试集，其余 $4$ 折用于训练。对于每一折，计算其分类准确率，定义为测试集上被正确预测的标签的比例。给定参数集的最终样本外准确率是这 $5$ 个准确率的平均值。\n\n对三个指定的测试用例（A、B、C）中的每一个都执行此过程。最后，计算一个指示变量。如果性能最佳的 RBF 核的准确率比线性核的准确率至少高出 $0.03$，则该变量为 $1$，否则为 $0$。这种比较可以推断类别分离的几何性质；显著更优的 RBF 性能意味着非线性决策边界更适合此信用风险分类问题。最终输出将计算出的准确率和指示符组合成所需的列表格式。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to solve the SVM cross-validation problem.\n    \"\"\"\n    # Dataset definition\n    data = [\n        (0.65, 0.18, 780, -1), (0.70, 0.20, 760, -1), (0.75, 0.25, 740, -1),\n        (0.80, 0.22, 770, -1), (0.68, 0.30, 720, -1), (0.72, 0.28, 730, -1),\n        (0.85, 0.20, 750, -1), (0.90, 0.18, 760, -1), (0.78, 0.26, 740, -1),\n        (0.82, 0.27, 735, -1), (0.95, 0.45, 660, 1), (1.02, 0.40, 680, 1),\n        (0.88, 0.55, 620, 1), (0.92, 0.50, 600, 1), (1.05, 0.35, 650, 1),\n        (0.90, 0.60, 590, 1), (0.98, 0.48, 630, 1), (1.10, 0.30, 610, 1),\n        (0.84, 0.58, 605, 1), (0.70, 0.40, 580, 1)\n    ]\n    X = np.array([d[:3] for d in data])\n    y = np.array([d[3] for d in data])\n\n    # Test cases\n    test_cases = [\n        {'C': 10.0, 'kernel_type': 'linear', 'gamma': None},\n        {'C': 10.0, 'kernel_type': 'rbf', 'gamma': 0.5},\n        {'C': 10.0, 'kernel_type': 'rbf', 'gamma': 2.0},\n    ]\n\n    # --- Kernel Functions ---\n    def linear_kernel(x1, x2):\n        return np.dot(x1, x2)\n\n    def make_rbf_kernel(gamma):\n        def rbf_kernel(x1, x2):\n            return np.exp(-gamma * np.linalg.norm(x1 - x2)**2)\n        return rbf_kernel\n\n    # --- SVM Solver ---\n    def train_svm(X_train, y_train, C, kernel_func):\n        n_samples = X_train.shape[0]\n        \n        # Build Kernel/Gram matrix\n        K = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(n_samples):\n                K[i, j] = kernel_func(X_train[i], X_train[j])\n        \n        # QP problem formulation for scipy.optimize.minimize\n        # Minimize: 0.5 * alpha.T * P * alpha - 1.T * alpha\n        # P_ij = y_i * y_j * K_ij\n        P = np.outer(y_train, y_train) * K\n        # Add a small regularization for numerical stability\n        P += 1e-8 * np.eye(n_samples)\n\n\n        def objective(alpha):\n            return 0.5 * alpha.T @ P @ alpha - np.sum(alpha)\n\n        # Constraints: sum(alpha_i * y_i) = 0\n        eq_cons = {'type': 'eq',\n                   'fun': lambda alpha: y_train.T @ alpha,\n                   'jac': lambda alpha: y_train}\n\n        # Bounds: 0 <= alpha_i <= C\n        bounds = [(0, C) for _ in range(n_samples)]\n\n        # Initial guess\n        alpha0 = np.zeros(n_samples)\n\n        # Solve QP problem\n        res = minimize(objective, alpha0, method='SLSQP', bounds=bounds, constraints=[eq_cons])\n        alpha = res.x\n\n        # Find support vectors\n        sv_mask = alpha > 1e-6\n        \n        # Compute bias term 'b'\n        non_bound_sv_mask = (alpha > 1e-6) & (alpha < C - 1e-6)\n        if np.any(non_bound_sv_mask):\n            non_bound_sv_indices = np.where(non_bound_sv_mask)[0]\n            b_values = [y_train[s] - np.sum(alpha[sv_mask] * y_train[sv_mask] * K[sv_mask, s]) for s in non_bound_sv_indices]\n            b = np.mean(b_values)\n        else: # Fallback if no non-bound SVs are found\n            sv_indices = np.where(sv_mask)[0]\n            f_vals_pos = [np.sum(alpha[sv_mask] * y_train[sv_mask] * K[sv_mask, i]) for i in sv_indices if y_train[i] == 1]\n            f_vals_neg = [np.sum(alpha[sv_mask] * y_train[sv_mask] * K[sv_mask, i]) for i in sv_indices if y_train[i] == -1]\n            \n            if not f_vals_pos or not f_vals_neg:\n                # Can happen if all SVs are from one class. Use a simple mean.\n                b_vals = [y_train[i] - np.sum(alpha * y_train * K[:, i]) for i in sv_indices]\n                b = np.mean(b_vals) if b_vals else 0\n            else:\n                b = -0.5 * (np.min(f_vals_pos) + np.max(f_vals_neg))\n\n        return alpha, b\n\n    # --- Prediction Function ---\n    def predict(X_test, X_train, y_train, alpha, b, kernel_func):\n        sv_mask = alpha > 1e-6\n        alpha_sv = alpha[sv_mask]\n        y_train_sv = y_train[sv_mask]\n        X_train_sv = X_train[sv_mask]\n        \n        y_pred = np.zeros(X_test.shape[0])\n        for i in range(X_test.shape[0]):\n            s = 0\n            for j in range(alpha_sv.shape[0]):\n                s += alpha_sv[j] * y_train_sv[j] * kernel_func(X_train_sv[j], X_test[i])\n            y_pred[i] = s + b\n\n        pred_labels = np.sign(y_pred)\n        pred_labels[pred_labels == 0] = 1 # Assign a class if decision value is exactly 0\n        return pred_labels\n\n    # --- Cross-Validation ---\n    def run_cross_validation(X, y, C, kernel_func, k=5):\n        n_samples = X.shape[0]\n        fold_size = n_samples // k\n        indices = np.arange(n_samples)\n        \n        accuracies = []\n        for i in range(k):\n            # Split data into train and test sets for the current fold\n            start, end = i * fold_size, (i + 1) * fold_size\n            test_indices = indices[start:end]\n            train_indices = np.delete(indices, test_indices)\n\n            X_train, y_train = X[train_indices], y[train_indices]\n            X_test, y_test = X[test_indices], y[test_indices]\n            \n            # --- Feature Scaling ---\n            mean = np.mean(X_train, axis=0)\n            std = np.std(X_train, axis=0)\n            std[std == 0] = 1.0 # Avoid division by zero\n            \n            X_train_scaled = (X_train - mean) / std\n            X_test_scaled = (X_test - mean) / std\n            \n            # Train SVM\n            alpha, b = train_svm(X_train_scaled, y_train, C, kernel_func)\n            \n            # Predict on test set\n            y_pred = predict(X_test_scaled, X_train_scaled, y_train, alpha, b, kernel_func)\n            \n            # Calculate accuracy\n            accuracy = np.mean(y_pred == y_test)\n            accuracies.append(accuracy)\n            \n        return np.mean(accuracies)\n\n    # --- Main Execution Logic ---\n    results = []\n    for case in test_cases:\n        C = case['C']\n        if case['kernel_type'] == 'linear':\n            kernel = linear_kernel\n        else: # rbf\n            kernel = make_rbf_kernel(case['gamma'])\n        \n        mean_accuracy = run_cross_validation(X, y, C, kernel)\n        results.append(round(mean_accuracy, 3))\n    \n    acc_A, acc_B, acc_C = results\n    \n    # Compute the final indicator\n    indicator = 1 if max(acc_B, acc_C) >= acc_A + 0.03 else 0\n    results.append(indicator)\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "在掌握了标准核函数后，让我们来挑战一个更具创造性的任务：设计一个自定义核函数。在金融时间序列分析中，最近的数据往往比陈旧的数据包含更多有效信息，因此赋予它们更高的权重是合乎逻辑的。[@problem_id:2435408] 这个练习将指导你如何通过构建一个包含时间加权因子的新核函数 $K_{\\alpha,\\gamma}$，将这种领域知识直接融入到SVM模型中，从而提高模型对时间动态的敏感度。", "id": "2435408", "problem": "您正在处理一个受金融时间序列预测启发的二元分类任务。每个样本都包含一个特征向量（表示近期回报率和波动率的摘要统计）、一个时间戳（指示其在时间序列中的位置）以及一个标签（指示次日回报率是上涨还是下跌）。考虑以下包含四个样本的训练数据，其中每个特征向量位于 $\\mathbb{R}^2$ 空间，每个时间戳为整数，每个标签属于 $\\{-1,+1\\}$：\n- 样本 $1$：$x_1 = (0.00, 0.00)$，$t_1 = 1$，$y_1 = -1$。\n- 样本 $2$：$x_2 = (1.00, 0.20)$，$t_2 = 2$，$y_2 = +1$。\n- 样本 $3$：$x_3 = (0.90, 1.00)$，$t_3 = 3$，$y_3 = +1$。\n- 样本 $4$：$x_4 = (-0.80, -0.50)$，$t_4 = 4$，$y_4 = -1$。\n\n您必须使用支持向量机（SVM），其核函数会增加近期观测值的影响。对于任意两个样本 $(x_i,t_i)$ 和 $(x_j,t_j)$，核函数定义如下：\n$$\nK_{\\alpha,\\gamma}\\big((x_i,t_i),(x_j,t_j)\\big) \\;=\\; \\exp\\!\\big(-\\gamma \\,\\|x_i - x_j\\|_2^2\\big)\\;\\cdot\\;\\exp\\!\\big(\\alpha\\,(t_i + t_j)\\big),\n$$\n其中 $\\alpha \\ge 0$ 控制时间衰减强度，$\\gamma > 0$ 控制径向基尺度。\n\n使用此核函数在对偶形式下训练一个软间隔支持向量机。令 $n$ 表示训练样本的数量。其对偶优化问题为：\n$$\n\\max_{\\alpha_1,\\dots,\\alpha_n}\\;\\; \\sum_{i=1}^n \\alpha_i \\;-\\;\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n \\alpha_i\\alpha_j\\,y_i y_j\\, K_{\\alpha,\\gamma}\\big((x_i,t_i),(x_j,t_j)\\big),\n$$\n约束条件为\n$$\n\\sum_{i=1}^n y_i \\alpha_i \\;=\\; 0,\\quad 0 \\le \\alpha_i \\le C \\;\\text{ for all } i.\n$$\n在获得最优解 $\\{\\alpha_i^\\star\\}_{i=1}^n$ 后，对于任何输入 $(z,t_z)$，决策函数定义为：\n$$\nf(z,t_z) \\;=\\; \\sum_{i=1}^n \\alpha_i^\\star y_i\\, K_{\\alpha,\\gamma}\\big((x_i,t_i),(z,t_z)\\big) \\;+\\; b,\n$$\n其中的偏置项 $b$ 的选择需满足 Karush-Kuhn-Tucker 条件。具体来说，如果存在至少一个索引 $i$ 满足 $0 < \\alpha_i^\\star < C$，则强制 $y_i\\,f(x_i,t_i)=1$，并将 $b$ 取为所有这些 $i$ 对应的 $y_i - \\sum_{j=1}^n \\alpha_j^\\star y_j K_{\\alpha,\\gamma}((x_j,t_j),(x_i,t_i))$ 的平均值。如果不存在这样的索引，则在与互补松弛不等式一致的数值区间内选择 $b$：\n$$\n\\begin{aligned}\n&\\text{for } \\alpha_i^\\star = 0:\\;\\; y_i\\,f(x_i,t_i) \\ge 1,\\\\\n&\\text{for } \\alpha_i^\\star = C:\\;\\; y_i\\,f(x_i,t_i) \\le 1,\n\\end{aligned}\n$$\n并且为了使输出唯一，将 $b$ 选为由这些不等式所蕴含的最紧可行区间的中点。\n\n请对以下三个测试输入进行分类，每个输入都在时间戳 $t_z = 5$ 时进行评估：\n- $z_1 = (0.95, 0.70)$，\n- $z_2 = (-0.70, -0.40)$，\n- $z_3 = (0.10, 0.05)$。\n使用 $f(z,t_z)$ 的符号来预测标签（属于 $\\{-1,+1\\}$），并约定非负值映射为 $+1$。\n\n您的程序必须按照所给出的形式，为下面每种参数配置精确求解支持向量机对偶问题，并为每种配置按规定顺序输出三个测试输入的预测标签。参数配置的测试套件如下：\n- 情况 $1$：$(\\alpha,\\gamma,C) = (0.0, 1.0, 10.0)$。\n- 情况 $2$：$(\\alpha,\\gamma,C) = (0.5, 1.0, 10.0)$。\n- 情况 $3$：$(\\alpha,\\gamma,C) = (1.0, 0.5, 10.0)$。\n- 情况 $4$：$(\\alpha,\\gamma,C) = (0.0, 5.0, 1.0)$。\n\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。列表中的每个元素本身是一个包含三个整数的列表，分别对应于每种情况下按顺序对 $(z_1,5)$、$(z_2,5)$ 和 $(z_3,5)$ 的预测标签。例如，输出格式必须为\n$$\n\\big[\\,[\\ell_{1,1},\\ell_{1,2},\\ell_{1,3}],\\;[\\ell_{2,1},\\ell_{2,2},\\ell_{2,3}],\\;[\\ell_{3,1},\\ell_{3,2},\\ell_{3,3}],\\;[\\ell_{4,1},\\ell_{4,2},\\ell_{4,3}]\\,\\big],\n$$\n其中每个 $\\ell_{k,j} \\in \\{-1,+1\\}$ 是一个整数。不应打印任何其他文本。", "solution": "该问题被视为有效。它提出了一个标准的软间隔支持向量机（SVM）分类任务，尽管使用了自定义的核函数。该问题在既定的机器学习理论中有科学依据，被明确地表述为一个凸优化问题，陈述客观，并为获得唯一解提供了所有必要信息。\n\n对于每个给定的参数配置 $(\\alpha, \\gamma, C)$，求解过程包括几个不同的步骤。\n\n首先，我们将支持向量机对偶问题形式化为一个标准的二次规划（QP）问题，以适用于数值求解器。目标是找到能最大化对偶目标函数的拉格朗日乘子 $\\boldsymbol{\\alpha} = [\\alpha_1, \\dots, \\alpha_n]^T$。这等同于最小化以下二次型：\n$$\n\\mathcal{L}(\\boldsymbol{\\alpha}) = \\frac{1}{2} \\boldsymbol{\\alpha}^T \\mathbf{H} \\boldsymbol{\\alpha} - \\mathbf{1}^T \\boldsymbol{\\alpha}\n$$\n约束条件为：\n$$\n\\mathbf{y}^T \\boldsymbol{\\alpha} = 0 \\quad \\text{and} \\quad \\mathbf{0} \\le \\boldsymbol{\\alpha} \\le C \\cdot \\mathbf{1}\n$$\n这里，$n=4$是训练样本的数量，$\\mathbf{y} = [-1, 1, 1, -1]^T$是标签向量，$\\mathbf{1}$是全一向量，$C$是正则化参数。矩阵 $\\mathbf{H}$ 是一个 $n \\times n$ 矩阵，其元素为 $H_{ij} = y_i y_j K_{ij}$，其中 $K_{ij}$ 是第 $i$ 个和第 $j$ 个训练样本的核函数值。\n\n核函数由以下公式给出：\n$$\nK_{\\alpha,\\gamma}\\big((x_i,t_i),(x_j,t_j)\\big) \\;=\\; \\exp\\!\\big(-\\gamma \\,d_{ij}^2\\big)\\;\\cdot\\;\\exp\\!\\big(\\alpha\\,(t_i + t_j)\\big)\n$$\n其中 $d_{ij}^2 = \\|x_i - x_j\\|_2^2$ 是特征向量 $x_i$ 和 $x_j$ 之间的欧几里得距离的平方。对于给定的训练数据：\n$x_1 = (0.00, 0.00)$, $t_1 = 1$, $y_1 = -1$\n$x_2 = (1.00, 0.20)$, $t_2 = 2$, $y_2 = +1$\n$x_3 = (0.90, 1.00)$, $t_3 = 3$, $y_3 = +1$\n$x_4 = (-0.80, -0.50)$, $t_4 = 4$, $y_4 = -1$\n我们首先预先计算 $n \\times n$ 的格拉姆矩阵（Gram matrix），记为 $\\mathbf{K}$，其条目为 $K_{ij} = K_{\\alpha,\\gamma}\\big((x_i,t_i),(x_j,t_j)\\big)$。然后，我们使用公式 $H_{ij} = y_i y_j K_{ij}$ 构建矩阵 $\\mathbf{H}$。\n\n这个约束优化问题使用数值 QP 求解器来解决。我们采用 `scipy.optimize.minimize` 库函数中的序列最小二乘规划（Sequential Least Squares Programming, SLSQP）算法。该函数接受目标函数 $\\mathcal{L}(\\boldsymbol{\\alpha})$、一个初始猜测（例如 $\\boldsymbol{\\alpha}_0 = \\mathbf{0}$）、每个 $i$ 的边界 $0 \\le \\alpha_i \\le C$ 以及线性等式约束 $\\sum_i y_i \\alpha_i = 0$。求解器返回最优向量 $\\boldsymbol{\\alpha}^\\star = [\\alpha_1^\\star, \\dots, \\alpha_n^\\star]^T$。由于数值精度的原因，将非常接近 $0$ 或 $C$ 的 $\\alpha_i^\\star$ 值钳位到这些精确值。\n\n获得 $\\boldsymbol{\\alpha}^\\star$ 后，我们计算偏置项 $b$。其计算方法取决于所得的 $\\alpha_i^\\star$ 值，并基于 Karush-Kuhn-Tucker (KKT) 条件。\n令 $S$ 为在间隔边界上的支持向量的索引集合，其中 $0 < \\alpha_i^\\star < C$。\n如果 $S$ 非空，计算偏置项 $b$ 以确保对于任意 $i \\in S$，决策函数满足 $y_i f(x_i, t_i) = 1$。这导出了以下表达式：\n$$\nb_i = y_i - \\sum_{j=1}^n \\alpha_j^\\star y_j K_{ij}\n$$\n为保证稳健性，我们对所有 $i \\in S$ 的这些值取平均：$b = \\frac{1}{|S|} \\sum_{i \\in S} b_i$。\n\n如果 $S$ 为空，则所有的 $\\alpha_i^\\star$ 都在边界上（$0$ 或 $C$）。在这种情况下，$b$ 不是由单个方程唯一确定的，而是必须位于由 KKT 互补松弛条件定义的区间内：\n- 对于 $\\alpha_i^\\star = 0$：$y_i \\big(\\sum_{j=1}^n \\alpha_j^\\star y_j K_{ij} + b\\big) \\ge 1$。\n- 对于 $\\alpha_i^\\star = C$：$y_i \\big(\\sum_{j=1}^n \\alpha_j^\\star y_j K_{ij} + b\\big) \\le 1$。\n这些不等式为 $b$ 定义了一个可行区间。令 $b_{lower}$ 为从这些条件导出的下界的最大值，而 $b_{upper}$ 为上界的最小值。问题要求将 $b$ 选为此最紧可行区间的中点：$b = (b_{lower} + b_{upper}) / 2$。\n\n最后，在确定了 $\\boldsymbol{\\alpha}^\\star$ 和 $b$ 之后，我们对时间戳 $t_z = 5$ 时的三个新测试输入 $z_1, z_2, z_3$ 进行分类。对于每个测试点 $(z, t_z)$，评估决策函数：\n$$\nf(z,t_z) \\;=\\; \\sum_{i=1}^n \\alpha_i^\\star y_i\\, K_{\\alpha,\\gamma}\\big((x_i,t_i),(z,t_z)\\big) \\;+\\; b\n$$\n预测标签 $\\ell \\in \\{-1, +1\\}$ 由 $f(z,t_z)$ 的符号给出，并约定非负值映射为 $+1$：\n$$\n\\ell = \\begin{cases} +1 & \\text{if } f(z,t_z) \\ge 0 \\\\ -1 & \\text{if } f(z,t_z) < 0 \\end{cases}\n$$\n对四个指定的参数配置中的每一个都执行这整个过程，并将所得的预测结果汇总成所要求的输出格式。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize, Bounds\n\ndef solve():\n    # Define the training data from the problem statement.\n    X_train = np.array([\n        [0.00, 0.00],\n        [1.00, 0.20],\n        [0.90, 1.00],\n        [-0.80, -0.50]\n    ])\n    T_train = np.array([1, 2, 3, 4])\n    Y_train = np.array([-1, 1, 1, -1])\n    n_samples = len(X_train)\n\n    # Define the test inputs.\n    X_test = np.array([\n        [0.95, 0.70],\n        [-0.70, -0.40],\n        [0.10, 0.05]\n    ])\n    T_test = 5\n\n    # Define the parameter configurations from the problem statement.\n    test_cases = [\n        (0.0, 1.0, 10.0),\n        (0.5, 1.0, 10.0),\n        (1.0, 0.5, 10.0),\n        (0.0, 5.0, 1.0),\n    ]\n\n    all_results = []\n    # Numerical tolerance for floating point comparisons\n    tol = 1e-9\n\n    for case in test_cases:\n        alpha_param, gamma, C = case\n        \n        # 1. Construct the Kernel Matrix K.\n        # Efficiently compute squared Euclidean distances between all pairs of training points.\n        sq_dists = np.sum(X_train**2, axis=1, keepdims=True) + np.sum(X_train**2, axis=1) - 2 * np.dot(X_train, X_train.T)\n        rbf_kernel = np.exp(-gamma * sq_dists)\n        time_component = np.exp(alpha_param * (T_train.reshape(-1, 1) + T_train))\n        K = rbf_kernel * time_component\n\n        # 2. Construct the matrix H for the QP problem.\n        H = np.outer(Y_train, Y_train) * K\n        \n        # 3. Define the QP objective function and constraints.\n        def objective_func(alphas):\n            return 0.5 * alphas.T @ H @ alphas - np.sum(alphas)\n        \n        constraints = ({'type': 'eq', 'fun': lambda alphas: np.dot(Y_train, alphas)})\n        bounds = Bounds([0] * n_samples, [C] * n_samples)\n        \n        # 4. Solve the QP problem for the optimal alphas.\n        result = minimize(objective_func, np.zeros(n_samples), method='SLSQP', bounds=bounds, constraints=constraints)\n        alpha_star = result.x\n        \n        # Clamp near-zero and near-C values for stable bias calculation.\n        alpha_star[alpha_star < tol] = 0\n        alpha_star[alpha_star > C - tol] = C\n        \n        # 5. Calculate the bias term b.\n        # Find support vectors on the margin (0 < alpha_i < C).\n        sv_margin_indices = np.where((alpha_star > tol) & (alpha_star < C - tol))[0]\n\n        if len(sv_margin_indices) > 0:\n            # Case 1: bias calculation from margin support vectors.\n            b_values = [Y_train[i] - np.sum(alpha_star * Y_train * K[:, i]) for i in sv_margin_indices]\n            b = np.mean(b_values)\n        else:\n            # Case 2: bias calculation from KKT conditions on non-margin SVs.\n            b_lowers, b_uppers = [], []\n            f_preds_no_b = K.T @ (alpha_star * Y_train)\n\n            for i in range(n_samples):\n                if not ((alpha_star[i] > tol) and (alpha_star[i] < C - tol)):\n                    if np.isclose(alpha_star[i], 0): # alpha_i = 0 implies y_i(f_i+b) >= 1\n                        if Y_train[i] == 1: b_lowers.append(1 - f_preds_no_b[i])\n                        else: b_uppers.append(-1 - f_preds_no_b[i])\n                    elif np.isclose(alpha_star[i], C): # alpha_i = C implies y_i(f_i+b) <= 1\n                        if Y_train[i] == 1: b_uppers.append(1 - f_preds_no_b[i])\n                        else: b_lowers.append(-1 - f_preds_no_b[i])\n            \n            max_lower = max(b_lowers) if b_lowers else -np.inf\n            min_upper = min(b_uppers) if b_uppers else np.inf\n            b = (max_lower + min_upper) / 2.0\n\n        # 6. Classify test inputs using the calculated model.\n        case_predictions = []\n        for z in X_test:\n            # Calculate kernel values between training points and the test point.\n            sq_dists_test = np.sum(X_train**2, axis=1) + np.sum(z**2) - 2 * np.dot(X_train, z)\n            rbf_kernel_test = np.exp(-gamma * sq_dists_test)\n            time_component_test = np.exp(alpha_param * (T_train + T_test))\n            K_test = rbf_kernel_test * time_component_test\n            \n            # Evaluate the decision function.\n            f_z = np.sum(alpha_star * Y_train * K_test) + b\n            \n            # Predict the label.\n            prediction = 1 if f_z >= 0 else -1\n            case_predictions.append(prediction)\n        \n        all_results.append(case_predictions)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"}, {"introduction": "静态的分类任务是学习的起点，但真实的金融市场是动态变化的，模型也需要随之调整。本次实践模拟了一个贴近现实的滚动窗口分析场景，用于评估一个股票分类SVM模型。这个练习的核心目标不再仅仅是追求单次预测的准确性，而是通过分析模型关键组成部分——即支持向量集合——随时间变化的稳定性，来深入理解模型的动态行为和市场的变迁。[@problem_id:2435480]", "id": "2435480", "problem": "您将要处理一个横截面股票分类任务，该任务的背景是计算经济学和金融学中的一项滚动窗口研究。其目的是要形式化并量化，当预测任务在滚动窗口上被重新评估时，“最重要”的股票集合（定义为软间隔支持向量机 (SVM) 中的支持向量）是否会随时间发生变化。\n\n从以下基本原理开始：\n- 软间隔支持向量机 (SVM) 解决以下凸优化问题\n  $$\n  \\min_{\\mathbf{w}, b, \\boldsymbol{\\xi}} \\;\\; \\frac{1}{2}\\lVert \\mathbf{w} \\rVert_2^2 + C \\sum_{i=1}^{m} \\xi_i\n  \\quad \\text{subject to} \\quad\n  y_i \\big(\\mathbf{w}^\\top \\mathbf{x}_i + b\\big) \\ge 1 - \\xi_i, \\;\\; \\xi_i \\ge 0,\n  $$\n  其中 $C \\ge 0$ 是正则化常数，$\\mathbf{x}_i \\in \\mathbb{R}^d$ 是特征，$y_i \\in \\{-1, +1\\}$ 是标签，$\\xi_i$ 是松弛变量。\n- 其线性核的拉格朗日对偶可以写成如下的二次规划问题：\n  $$\n  \\min_{\\boldsymbol{\\alpha} \\in \\mathbb{R}^m} \\;\\; \\frac{1}{2} \\boldsymbol{\\alpha}^\\top \\mathbf{Q} \\boldsymbol{\\alpha} - \\mathbf{1}^\\top \\boldsymbol{\\alpha}\n  \\quad \\text{subject to} \\quad\n  \\mathbf{y}^\\top \\boldsymbol{\\alpha} = 0, \\;\\; 0 \\le \\alpha_i \\le C,\n  $$\n  其中 $\\mathbf{Q}$ 是一个矩阵，其元素为 $Q_{ij} = y_i y_j \\mathbf{x}_i^\\top \\mathbf{x}_j$，$\\mathbf{1}$ 是全1向量，$\\mathbf{y} = (y_1,\\dots,y_m)^\\top$。\n- 根据 Karush–Kuhn–Tucker (KKT) 条件，对偶变量 $\\alpha_i &gt; 0$ 的数据点是支持向量。满足 $0 &lt; \\alpha_i &lt; C$ 的点位于间隔边界上；满足 $\\alpha_i = C$ 的点是违反间隔边界或被错误分类的点。原始问题的解满足 $\\mathbf{w} = \\sum_{i=1}^m \\alpha_i y_i \\mathbf{x}_i$。\n\n数据集构建和滚动窗口：\n- 共有 $N = 8$ 支股票，索引为 $i \\in \\{0,1,\\dots,7\\}$，每支股票都有一个固定的二维特征向量 $\\mathbf{x}_i \\in \\mathbb{R}^2$，代表简化的横截面特征（例如，动量和规模），具体如下：\n  $$\n  \\mathbf{x}_0 = (0.9, 0.2), \\;\\;\n  \\mathbf{x}_1 = (0.7, 0.4), \\;\\;\n  \\mathbf{x}_2 = (0.5, 0.1), \\;\\;\n  \\mathbf{x}_3 = (0.3, 0.3),\n  $$\n  $$\n  \\mathbf{x}_4 = (-0.2, -0.8), \\;\\;\n  \\mathbf{x}_5 = (-0.4, -0.5), \\;\\;\n  \\mathbf{x}_6 = (-0.6, -0.2), \\;\\;\n  \\mathbf{x}_7 = (-0.8, -0.4).\n  $$\n- 共有 $T = 6$ 个时间点，索引为 $t \\in \\{0,1,2,3,4,5\\}$。在每个时间点 $t$，一个真实但未知的线性评分规则会平滑地变化：\n  $$\n  \\mathbf{w}_t = (\\cos\\theta_t, \\sin\\theta_t), \\quad \\text{with} \\quad \\theta_t \\in \\{0^\\circ, 22.5^\\circ, 45^\\circ, 67.5^\\circ, 90^\\circ, 112.5^\\circ\\}.\n  $$\n  截距为 $b_t = 0$。标签通过 $y_{i,t} = \\mathrm{sign}(\\mathbf{w}_t^\\top \\mathbf{x}_i)$ 确定性地生成，因此对所有的 $i,t$，都有 $y_{i,t} \\in \\{-1, +1\\}$。\n- 对于一个宽度为 $W$ 的滚动窗口，窗口 $k$（从时间 $t = k$ 开始）中的训练样本聚合了所有满足 $i \\in \\{0,\\dots,7\\}$ 和 $t \\in \\{k, k+1, \\dots, k+W-1\\}$ 的数据对 $(\\mathbf{x}_i, y_{i,t})$，从而产生 $m = N \\cdot W$ 个样本。令 $\\mathcal{S}_k$ 表示在窗口 $k$ 中作为支持向量出现的股票索引集合，即 $\\mathcal{S}_k = \\{i \\in \\{0,\\dots,7\\} \\mid \\text{在窗口 } k \\text{ 中存在股票 } i \\text{ 的样本，其 } \\alpha > 0\\}$。\n\n稳定性度量：\n- 对于连续的窗口 $k$ 和 $k+1$，将集合相似性稳定性定义为 Jaccard 指数：\n  $$\n  J(\\mathcal{S}_k, \\mathcal{S}_{k+1}) = \\frac{|\\mathcal{S}_k \\cap \\mathcal{S}_{k+1}|}{|\\mathcal{S}_k \\cup \\mathcal{S}_{k+1}|}.\n  $$\n- 对于给定的正则化常数 $C$ 和窗口宽度 $W$，以及 $K = T - W + 1$ 个窗口，计算平均连续窗口稳定性：\n  $$\n  \\bar{J} = \\frac{1}{K-1}\\sum_{k=0}^{K-2} J(\\mathcal{S}_k, \\mathcal{S}_{k+1}).\n  $$\n\n任务：\n- 使用上述带有线性核的二次规划来实现 SVM 对偶优化，以获得对偶变量 $\\boldsymbol{\\alpha}$，识别每个窗口的支持向量，并计算稳定性度量 $\\bar{J}$。\n- 您的实现必须是一个完整、可运行的程序，能够无需任何用户输入地执行整个流程。\n\n测试套件和要求输出：\n- 使用以下 $(C, W)$ 参数对的测试套件：\n  1. $(C, W) = (1.0, 3)$\n  2. $(C, W) = (10.0, 3)$\n  3. $(C, W) = (0.3, 2)$\n- 对于每个参数对，计算平均连续窗口稳定性 $\\bar{J}$，结果为浮点数，并四舍五入到四位小数。\n- 最终输出格式：您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表，例如 $[a,b,c]$，其中 $a$、$b$ 和 $c$ 是按给定顺序排列的三个测试用例的 $\\bar{J}$ 值。\n- 角度以度为单位给出；没有需要报告的物理单位；所有输出都是无单位的浮点数。最终打印的值必须四舍五入到四位小数。", "solution": "问题陈述经评估有效。它在科学上基于支持向量机和凸优化的理论，其在计算金融中的应用是一个合理的研究领域。该问题是适定的，提供了所有必需的数据、定义和一个清晰、客观的计算目标。它不包含逻辑矛盾、歧义或事实不正确的前提。因此，我们将着手提供一个完整的解决方案。\n\n问题的核心是量化股票“重要性”随时间变化的稳定性。重要性是根据其在软间隔支持向量机 (SVM) 分类任务中是否为支持向量来定义的。该分析在滚动窗口的基础上进行。解决方案通过遵循一系列从第一性原理推导出的步骤来执行。\n\n首先，我们建立合成数据集。有 $N=8$ 支股票，每支股票都有一个固定的二维特征向量 $\\mathbf{x}_i \\in \\mathbb{R}^2$。在 $T=6$ 个时间周期内，底层的分类规则会平滑变化。这通过一个旋转的权重向量 $\\mathbf{w}_t = (\\cos\\theta_t, \\sin\\theta_t)^\\top$（对应于一系列角度 $\\theta_t$）来建模。股票 $i$ 在时间 $t$ 的类别标签 $y_{i,t} \\in \\{-1, +1\\}$ 由 $y_{i,t} = \\mathrm{sign}(\\mathbf{w}_t^\\top \\mathbf{x}_i)$ 确定性地决定。\n\n其次，我们实施滚动窗口分析。对于给定的窗口宽度 $W$ 和正则化参数 $C$，我们构建一系列训练数据集。第 $k$ 个窗口（对于 $k \\in \\{0, 1, \\dots, T-W\\}$）包含所有满足 $i \\in \\{0, \\dots, N-1\\}$ 和 $t \\in \\{k, \\dots, k+W-1\\}$ 的数据点 $(\\mathbf{x}_i, y_{i,t})$。这为每个窗口生成一个大小为 $m = N \\cdot W$ 的训练集。\n\n第三，对于每个窗口的训练数据，我们求解软间隔 SVM 的对偶问题。该问题是一个二次规划 (QP) 问题，定义如下：\n$$\n\\min_{\\boldsymbol{\\alpha} \\in \\mathbb{R}^m} \\;\\; \\frac{1}{2} \\boldsymbol{\\alpha}^\\top \\mathbf{Q} \\boldsymbol{\\alpha} - \\mathbf{1}^\\top \\boldsymbol{\\alpha}\n$$\n约束条件为：\n$$\n\\mathbf{y}^\\top \\boldsymbol{\\alpha} = 0\n$$\n$$\n0 \\le \\alpha_j \\le C \\quad \\text{for } j=1, \\dots, m\n$$\n在这里，$\\boldsymbol{\\alpha}$ 是拉格朗日乘子向量，$\\mathbf{y}$ 是窗口训练集中的标签向量，$\\mathbf{Q}$ 是一个 $m \\times m$ 的矩阵，其元素为 $Q_{jl} = y_j y_l (\\mathbf{x}_j^\\top \\mathbf{x}_l)$。这个约束优化问题是凸的，可以使用数值方法可靠地求解。我们采用序列最小二乘规划 (SLSQP) 算法，该算法在 `scipy.optimize.minimize` 函数中实现。我们提供了目标函数、其梯度（雅可比矩阵）$\\nabla_{\\boldsymbol{\\alpha}}f(\\boldsymbol{\\alpha}) = \\mathbf{Q}\\boldsymbol{\\alpha} - \\mathbf{1}$、线性等式约束以及对 $\\boldsymbol{\\alpha}$ 的箱式约束。\n\n第四，在求解出窗口 $k$ 的最优对偶变量 $\\boldsymbol{\\alpha}^{(k)}$ 后，我们识别出重要股票的集合。根据 SVM 的 Karush–Kuhn–Tucker (KKT) 条件，如果一个数据点 $j$ 对应的拉格朗日乘子 $\\alpha_j$ 严格为正，则该点为支持向量。由于浮点运算的限制，我们使用一个小的容差，将满足 $\\alpha_j > \\epsilon$ (对于某个小的 $\\epsilon > 0$) 的点识别为支持向量。窗口 $k$ 的重要股票集合 $\\mathcal{S}_k$ 是所有唯一股票索引 $i$ 的集合，其中至少有一个在窗口 $k$ 内的对应数据点是支持向量。\n\n第五，我们量化这些集合随时间变化的稳定性。两个连续窗口 $\\mathcal{S}_k$ 和 $\\mathcal{S}_{k+1}$ 的支持向量集合之间的相似性由 Jaccard 指数度量：\n$$\nJ(\\mathcal{S}_k, \\mathcal{S}_{k+1}) = \\frac{|\\mathcal{S}_k \\cap \\mathcal{S}_{k+1}|}{|\\mathcal{S}_k \\cup \\mathcal{S}_{k+1}|}\n$$\n对于给定的 $(C, W)$ 对，总体稳定性 $\\bar{J}$ 是从 $k=0$ 到 $k=K-2$ 的所有连续窗口对的 Jaccard 指数的算术平均值，其中 $K = T - W + 1$ 是窗口总数。\n\n最后，对测试套件中指定的每个 $(C, W)$ 对执行此完整流程。计算出的每个案例的平均稳定性 $\\bar{J}$ 被四舍五入到四位小数，并作为最终结果报告。该实现包含在一个独立的 Python 脚本中，利用 `numpy` 进行高效的数值计算，并使用 `scipy` 完成核心优化任务，严格遵守问题的要求。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the SVM stability analysis problem for the given test cases.\n    \"\"\"\n    \n    # Define fixed problem parameters and data\n    N = 8\n    T = 6\n    x_stocks = np.array([\n        [0.9, 0.2], [0.7, 0.4], [0.5, 0.1], [0.3, 0.3],\n        [-0.2, -0.8], [-0.4, -0.5], [-0.6, -0.2], [-0.8, -0.4]\n    ])\n    thetas_deg = np.array([0.0, 22.5, 45.0, 67.5, 90.0, 112.5])\n    \n    # Pre-calculate true weights and labels for all time points\n    thetas_rad = np.deg2rad(thetas_deg)\n    w_t_vectors = np.array([np.cos(thetas_rad), np.sin(thetas_rad)]).T\n    # y_labels[i, t] is the label for stock i at time t. Shape (8, 6)\n    y_labels = np.sign(x_stocks @ w_t_vectors.T)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 3),    # (C, W) pair 1\n        (10.0, 3),   # (C, W) pair 2\n        (0.3, 2),    # (C, W) pair 3\n    ]\n\n    results = []\n    \n    for C, W in test_cases:\n        K = T - W + 1  # Total number of rolling windows\n        all_S_sets = []\n\n        # Iterate over each rolling window\n        for k in range(K):\n            # 1. Construct the training dataset for the current window k\n            m = N * W\n            train_X = np.zeros((m, 2))\n            train_y = np.zeros(m)\n            train_stock_indices = np.zeros(m, dtype=int)\n            \n            idx = 0\n            for i in range(N):\n                for t_offset in range(W):\n                    t = k + t_offset\n                    train_X[idx] = x_stocks[i]\n                    train_y[idx] = y_labels[i, t]\n                    train_stock_indices[idx] = i\n                    idx += 1\n\n            # 2. Set up the dual SVM Quadratic Program\n            gram_matrix = train_X @ train_X.T\n            y_outer = np.outer(train_y, train_y)\n            Q = gram_matrix * y_outer\n\n            def objective_func(alpha):\n                return 0.5 * (alpha.T @ Q @ alpha) - np.sum(alpha)\n\n            def objective_jac(alpha):\n                return (alpha.T @ Q) - 1.0\n\n            # Equality constraint: y^T * alpha = 0\n            eq_cons = {\n                'type': 'eq',\n                'fun': lambda alpha: alpha @ train_y,\n                'jac': lambda alpha: train_y\n            }\n            \n            # Box constraints: 0 <= alpha_i <= C\n            bounds = [(0, C) for _ in range(m)]\n\n            # Initial guess for alpha\n            alpha_0 = np.zeros(m)\n            \n            # 3. Solve the QP using SLSQP\n            res = minimize(\n                fun=objective_func,\n                x0=alpha_0,\n                jac=objective_jac,\n                bounds=bounds,\n                constraints=[eq_cons],\n                method='SLSQP',\n                tol=1e-9\n            )\n            \n            alphas = res.x\n            \n            # 4. Identify support vector stock indices\n            # A stock is a support vector if any of its observations have alpha > 0\n            sv_mask = alphas > 1e-7  # Tolerance for floating point precision\n            sv_stock_indices = train_stock_indices[sv_mask]\n            S_k = set(sv_stock_indices)\n            all_S_sets.append(S_k)\n            \n        # 5. Compute the average consecutive-window stability (Jaccard index)\n        jaccard_scores = []\n        if K > 1:\n            for k_pair in range(K - 1):\n                S_k = all_S_sets[k_pair]\n                S_k_plus_1 = all_S_sets[k_pair + 1]\n                \n                intersection_len = len(S_k.intersection(S_k_plus_1))\n                union_len = len(S_k.union(S_k_plus_1))\n                \n                # J(A,B) = 1 if A and B are empty. Otherwise, |A intersect B| / |A union B|\n                if union_len == 0:\n                    jaccard_index = 1.0\n                else:\n                    jaccard_index = intersection_len / union_len\n                \n                jaccard_scores.append(jaccard_index)\n            \n            avg_jaccard = np.mean(jaccard_scores)\n        else: # Case with only one window, stability is not defined.\n            avg_jaccard = 0.0\n\n        results.append(round(avg_jaccard, 4))\n\n    # Print the final results in the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}]}