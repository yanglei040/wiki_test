{"hands_on_practices": [{"introduction": "信用风险建模的核心任务之一是构建能够预测违约或制裁等事件的分类器。本练习将指导您从第一性原理出发，使用最大似然估计和牛顿法来构建一个逻辑回归模型，而不是简单地调用现有库。通过这个实践[@problem_id:2407536]，您将深入理解分类模型内部的工作机制以及正则化在防止过拟合中的重要作用。", "id": "2407536", "problem": "您的任务是使用逻辑回归实现一个二元分类器，根据财务顾问的客户投诉历史和就业不稳定性，来预测其是否会在下一年受到监管机构的处罚。目标是在计算经济学和计算金融学的背景下，从第一性原理出发构建模型，使用适用于二元结果的最大似然框架以及用于正则化的惩罚概念。您的程序必须是一个完整、可运行的脚本，使用所提供的数据和测试套件执行以下步骤，并以指定格式输出汇总结果。\n\n您必须从基本原理开始构建分类器：将二元结果建模为一个伯努利随机变量，其概率取决于经逻辑斯蒂链接函数转换的线性指数，参数通过最大化观测数据的似然来估计。应用金融风险建模中典型的可解释性约束：包含一个截距项，仅使用训练集统计数据将特征标准化为零均值和单位方差，并对斜率系数使用各向同性平方范数惩罚（岭正则化）以防止过拟合。截距项不得被惩罚。不得使用任何捷径公式或现成的模型拟合程序；必须推导参数的更新方程，并使用牛顿法实现一个数值稳定的求解器。\n\n数据描述：\n- 每位顾问的特征：\n  - $c$：客户投诉数量（计数），\n  - $r$：年均投诉次数（非负实数），\n  - $s$：投诉平均严重程度（在 $[0,1]$ 上归一化），\n  - $f$：过去 $5$ 年内更换公司的次数（计数），\n  - $t$：在当前公司的任职年限（非负实数）。\n- 目标：$y \\in \\{0,1\\}$，表示该顾问是否在下一年内受到处罚。\n\n训练数据集（每个项目为 $(c,r,s,f,t; y)$）：\n- $(0,\\, 0.0,\\, 0.1,\\, 0,\\, 15;\\, 0)$\n- $(1,\\, 0.1,\\, 0.2,\\, 0,\\, 12;\\, 0)$\n- $(2,\\, 0.2,\\, 0.3,\\, 1,\\, 10;\\, 0)$\n- $(0,\\, 0.0,\\, 0.2,\\, 1,\\, 18;\\, 0)$\n- $(1,\\, 0.1,\\, 0.1,\\, 0,\\, 20;\\, 0)$\n- $(2,\\, 0.2,\\, 0.15,\\, 1,\\, 16;\\, 0)$\n- $(1,\\, 0.0,\\, 0.25,\\, 0,\\, 14;\\, 0)$\n- $(0,\\, 0.05,\\, 0.05,\\, 0,\\, 22;\\, 0)$\n- $(3,\\, 0.5,\\, 0.4,\\, 1,\\, 9;\\, 0)$\n- $(5,\\, 0.8,\\, 0.7,\\, 2,\\, 5;\\, 1)$\n- $(7,\\, 1.2,\\, 0.9,\\, 3,\\, 3;\\, 1)$\n- $(4,\\, 0.6,\\, 0.6,\\, 2,\\, 6;\\, 1)$\n- $(8,\\, 1.5,\\, 0.85,\\, 4,\\, 2;\\, 1)$\n- $(6,\\, 1.0,\\, 0.8,\\, 3,\\, 4;\\, 1)$\n- $(9,\\, 1.8,\\, 0.95,\\, 5,\\, 1;\\, 1)$\n- $(3,\\, 0.4,\\, 0.55,\\, 2,\\, 8;\\, 1)$\n\n建模与训练要求：\n- 使用训练集数据将每个特征 $c$、$r$、$s$、$f$ 和 $t$ 标准化为零均值和单位方差。对于每个特征 $x$，其标准化值的计算公式为 $(x - \\mu_x)/\\sigma_x$，其中 $\\mu_x$ 和 $\\sigma_x$ 是该特征的训练集均值和标准差。如果任何 $\\sigma_x$ 等于 $0$，则用 $1$ 替换，以避免除以零。\n- 为标准化后的特征矩阵增加一列全为1的截距项。\n- 使用牛顿法拟合逻辑回归参数，通过最小化带惩罚的负对数似然来实现。停止标准为参数更新的欧几里得范数小于 $10^{-8}$，或在 $50$ 次迭代之后，以先到者为准。\n- 使用强度为 $\\lambda \\ge 0$ 的岭惩罚，且仅应用于斜率系数（不包括截距项）。\n\n分类规则：\n- 对于具有特征 $(c,r,s,f,t)$ 的给定顾问，计算其预测受处罚概率，结果为 $[0,1]$ 区间内的小数。如果概率至少为 $0.5$，则分类为受处罚；否则分类为未受处罚。所有报告的概率必须是小数；请勿使用百分号。\n\n测试套件：\n您的程序必须按规定多次训练模型，并返回以下四种情况的分类结果。每种情况都提供一个惩罚水平 $\\lambda$ 和一个测试顾问的特征 $(c,r,s,f,t)$。对于每种情况，使用提供的 $\\lambda$ 在训练数据上拟合模型，然后使用 $0.5$ 的阈值对给定的测试顾问进行分类。\n\n- 情况 1：$\\lambda = 1.0$，$(c,r,s,f,t) = (6,\\, 1.0,\\, 0.8,\\, 3,\\, 3)$\n- 情况 2：$\\lambda = 0.0$，$(c,r,s,f,t) = (0,\\, 0.0,\\, 0.1,\\, 0,\\, 15)$\n- 情况 3：$\\lambda = 10.0$，$(c,r,s,f,t) = (10,\\, 1.5,\\, 0.9,\\, 5,\\, 2)$\n- 情况 4：$\\lambda = 0.5$，$(c,r,s,f,t) = (1,\\, 0.1,\\, 0.2,\\, 0,\\, 12)$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含分类结果，格式为方括号内由逗号分隔的整数列表，顺序与上述情况一致，例如，$\"[1,0,1,0]\"$。", "solution": "所述问题是有效的。它在科学上基于统计学习的原理，特别是带有岭正则化的逻辑回归。该设定是适定的，提供了所有必要的数据、一个明确的目标函数以及一个指定的优化数值方法。这是计算金融学中一个客观且可形式化的问题。我们将从第一性原理出发推导解决方案。\n\n设训练数据集为 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$，其中 $\\mathbf{x}_i \\in \\mathbb{R}^D$ 是第 $i$ 位顾问的特征向量，$y_i \\in \\{0, 1\\}$ 是对应的二元结果（受处罚或未受处罚）。这里，$N=16$ 是训练观测值的数量，$D=5$ 是特征的数量。\n\n逻辑回归模型假设正向结果（$y_i=1$）的概率由应用于特征线性组合的逻辑斯蒂（sigmoid）函数给出：\n$$\nP(y_i=1 | \\mathbf{x}_i^*; \\boldsymbol{\\beta}) = p_i = \\sigma(\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}) = \\frac{1}{1 + e^{-\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}}\n$$\n在这里，$\\mathbf{x}_i^*$ 是第 $i$ 个特征向量，其前端增加了一个 $1$ 以容纳截距项，因此 $\\mathbf{x}_i^* \\in \\mathbb{R}^{D+1}$。向量 $\\boldsymbol{\\beta} \\in \\mathbb{R}^{D+1}$ 包含模型参数，其中 $\\beta_0$ 是截距项，$\\beta_1, \\dots, \\beta_D$ 是斜率系数。\n\n参数 $\\boldsymbol{\\beta}$ 是通过最大化观测数据的对数似然来估计的，并附加一个惩罚项用于正则化。单个观测值的似然由伯努利概率质量函数给出：$L_i(\\boldsymbol{\\beta}) = p_i^{y_i} (1-p_i)^{1-y_i}$。$N$ 个独立观测值的总对数似然为：\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N \\ln(L_i(\\boldsymbol{\\beta})) = \\sum_{i=1}^N [y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i)]\n$$\n代入 $p_i$ 的表达式以及 $\\ln(1-p_i) = \\ln(1-\\sigma(z_i)) = -z_i - \\ln(1+e^{z_i})$ 和 $\\ln(p_i) = \\ln(\\sigma(z_i)) = -\\ln(1+e^{-z_i}) = z_i - \\ln(1+e^{z_i})$（其中 $z_i=\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}$），我们可以将对数似然简化为更方便的形式：\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N [y_i (\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}) - \\ln(1 + e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}})]\n$$\n我们的目标是最小化带惩罚的*负*对数似然。惩罚项是斜率系数的平方 $\\ell_2$-范数（岭正则化），它抑制过大的参数值，有助于防止过拟合。截距项 $\\beta_0$ 不被惩罚。要最小化的目标函数是：\n$$\nJ(\\boldsymbol{\\beta}) = -\\ell(\\boldsymbol{\\beta}) + \\frac{\\lambda}{2} \\sum_{j=1}^D \\beta_j^2 = \\sum_{i=1}^N [\\ln(1 + e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}) - y_i (\\mathbf{x}_i^{*T} \\boldsymbol{\\beta})] + \\frac{\\lambda}{2} \\boldsymbol{\\beta}^T \\mathbf{I}' \\boldsymbol{\\beta}\n$$\n其中 $\\lambda \\ge 0$ 是正则化强度，$\\mathbf{I}'$ 是一个 $(D+1) \\times (D+1)$ 的对角矩阵，其对角线元素为 $I'_{00} = 0$ 和 $I'_{jj} = 1$（对于 $j=1, \\dots, D$）。\n\n为了最小化 $J(\\boldsymbol{\\beta})$，我们使用牛顿法，这是一个迭代算法，其更新规则如下：\n$$\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - [\\mathbf{H}(\\boldsymbol{\\beta}^{(t)})]^{-1} \\nabla J(\\boldsymbol{\\beta}^{(t)})\n$$\n其中 $\\nabla J$ 是目标函数的梯度，$\\mathbf{H}$ 是其海森矩阵。\n\n首先，我们推导梯度 $\\nabla J(\\boldsymbol{\\beta})$。无惩罚部分梯度的第 $j$ 个分量是：\n$$\n\\frac{\\partial}{\\partial \\beta_j} \\left( \\sum_{i=1}^N [\\ln(1 + e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}) - y_i (\\mathbf{x}_i^{*T} \\boldsymbol{\\beta})] \\right) = \\sum_{i=1}^N \\left[ \\frac{e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}}{1 + e^{\\mathbf{x}_i^{*T} \\boldsymbol{\\beta}}} x_{ij}^* - y_i x_{ij}^* \\right] = \\sum_{i=1}^N (p_i - y_i) x_{ij}^*\n$$\n在矩阵表示法中，其中 $\\mathbf{X}^*$ 是 $N \\times (D+1)$ 的增广设计矩阵，$\\mathbf{p}$ 是概率向量，$\\mathbf{y}$ 是结果向量，这可以表示为 $\\mathbf{X}^{*T}(\\mathbf{p} - \\mathbf{y})$。\n惩罚项的梯度是 $\\lambda \\mathbf{I}' \\boldsymbol{\\beta}$。\n因此，完整的梯度是：\n$$\n\\nabla J(\\boldsymbol{\\beta}) = \\mathbf{X}^{*T}(\\mathbf{p} - \\mathbf{y}) + \\lambda \\mathbf{I}' \\boldsymbol{\\beta}\n$$\n接下来，我们推导海森矩阵 $\\mathbf{H}(\\boldsymbol{\\beta})$。其元素 $H_{jk}$ 是 $\\frac{\\partial^2 J(\\boldsymbol{\\beta})}{\\partial \\beta_j \\partial \\beta_k}$。对于无惩罚部分：\n$$\n\\frac{\\partial^2}{\\partial \\beta_j \\partial \\beta_k} \\left( \\sum_{i=1}^N \\dots \\right) = \\frac{\\partial}{\\partial \\beta_k} \\left( \\sum_{i=1}^N p_i x_{ij}^* \\right) = \\sum_{i=1}^N x_{ij}^* \\frac{\\partial p_i}{\\partial \\beta_k}\n$$\n由于 $\\frac{\\partial p_i}{\\partial \\beta_k} = \\sigma(\\mathbf{x}_i^{*T} \\boldsymbol{\\beta})(1 - \\sigma(\\mathbf{x}_i^{*T} \\boldsymbol{\\beta})) x_{ik}^* = p_i(1-p_i)x_{ik}^*$，无惩罚部分的海森矩阵是：\n$$\nH_{jk} = \\sum_{i=1}^N x_{ij}^* p_i(1-p_i) x_{ik}^*\n$$\n用矩阵形式表示，这是 $\\mathbf{X}^{*T} \\mathbf{W} \\mathbf{X}^*$，其中 $\\mathbf{W}$ 是一个 $N \\times N$ 的对角矩阵，其对角线元素为 $W_{ii} = p_i(1-p_i)$。\n惩罚项的海森矩阵就是 $\\lambda \\mathbf{I}'$。\n完整的海森矩阵是：\n$$\n\\mathbf{H}(\\boldsymbol{\\beta}) = \\mathbf{X}^{*T} \\mathbf{W} \\mathbf{X}^* + \\lambda \\mathbf{I}'\n$$\n当 $\\lambda > 0$ 时，该海森矩阵是正定的，这确保了牛顿法具有唯一的最小值和数值稳定性。\n\n实现将按以下步骤进行：\n1.  从训练数据中计算出 $D=5$ 个特征中每个特征的均值 $\\boldsymbol{\\mu}$ 和标准差 $\\boldsymbol{\\sigma}$。如果任何 $\\sigma_j = 0$，则将其设为 $1$。\n2.  将训练数据矩阵 $\\mathbf{X}$ 标准化得到 $\\mathbf{X}_{std}$，使其每一列都具有零均值和单位方差。\n3.  为 $\\mathbf{X}_{std}$ 增加一列全为1的截距项，以构成设计矩阵 $\\mathbf{X}^*$。\n4.  对于每个测试用例，初始化参数向量 $\\boldsymbol{\\beta}$（例如，初始化为零向量）。\n5.  迭代应用牛顿法更新规则：\n    a. 计算概率 $\\mathbf{p} = \\sigma(\\mathbf{X}^* \\boldsymbol{\\beta}^{(t)})$ 和权重矩阵 $\\mathbf{W}^{(t)}$。\n    b. 计算梯度 $\\nabla J(\\boldsymbol{\\beta}^{(t)})$ 和海森矩阵 $\\mathbf{H}(\\boldsymbol{\\beta}^{(t)})$。\n    c. 求解线性方程组 $\\mathbf{H}(\\boldsymbol{\\beta}^{(t)}) \\Delta \\boldsymbol{\\beta} = - \\nabla J(\\boldsymbol{\\beta}^{(t)})$ 以获得更新步长 $\\Delta \\boldsymbol{\\beta}$。\n    d. 更新参数：$\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + \\Delta \\boldsymbol{\\beta}$。\n    e. 如果更新的欧几里得范数 $\\|\\Delta \\boldsymbol{\\beta}\\|_2$ 小于 $10^{-8}$ 或达到 $50$ 次迭代，则终止。\n6.  对于每个测试向量 $\\mathbf{x}_{\\text{test}}$，使用训练集的 $\\boldsymbol{\\mu}$ 和 $\\boldsymbol{\\sigma}$ 对其进行标准化，增广为 $\\mathbf{x}_{\\text{test}}^*$，计算预测概率 $p_{\\text{test}} = \\sigma(\\mathbf{x}_{\\text{test}}^{*T} \\boldsymbol{\\beta}_{\\text{final}})$，如果 $p_{\\text{test}} \\ge 0.5$ 则分类为 $1$，否则分类为 $0$。\n最终输出将是这些分类结果的列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and solves the logistic regression problem as specified.\n    \"\"\"\n    # Training dataset from the problem statement\n    # (c, r, s, f, t; y)\n    training_data = np.array([\n        [0.0, 0.0, 0.1, 0.0, 15.0, 0.0],\n        [1.0, 0.1, 0.2, 0.0, 12.0, 0.0],\n        [2.0, 0.2, 0.3, 1.0, 10.0, 0.0],\n        [0.0, 0.0, 0.2, 1.0, 18.0, 0.0],\n        [1.0, 0.1, 0.1, 0.0, 20.0, 0.0],\n        [2.0, 0.2, 0.15, 1.0, 16.0, 0.0],\n        [1.0, 0.0, 0.25, 0.0, 14.0, 0.0],\n        [0.0, 0.05, 0.05, 0.0, 22.0, 0.0],\n        [3.0, 0.5, 0.4, 1.0, 9.0, 0.0],\n        [5.0, 0.8, 0.7, 2.0, 5.0, 1.0],\n        [7.0, 1.2, 0.9, 3.0, 3.0, 1.0],\n        [4.0, 0.6, 0.6, 2.0, 6.0, 1.0],\n        [8.0, 1.5, 0.85, 4.0, 2.0, 1.0],\n        [6.0, 1.0, 0.8, 3.0, 4.0, 1.0],\n        [9.0, 1.8, 0.95, 5.0, 1.0, 1.0],\n        [3.0, 0.4, 0.55, 2.0, 8.0, 1.0],\n    ])\n\n    X_train_raw = training_data[:, :-1]\n    y_train = training_data[:, -1].reshape(-1, 1)\n\n    # Test cases from the problem statement\n    # Each is (lambda, (c, r, s, f, t))\n    test_cases = [\n        (1.0, (6.0, 1.0, 0.8, 3.0, 3.0)),\n        (0.0, (0.0, 0.0, 0.1, 0.0, 15.0)),\n        (10.0, (10.0, 1.5, 0.9, 5.0, 2.0)),\n        (0.5, (1.0, 0.1, 0.2, 0.0, 12.0)),\n    ]\n\n    # Step 1: Standardize features using training data statistics\n    mean_X = np.mean(X_train_raw, axis=0)\n    std_X = np.std(X_train_raw, axis=0)\n    # As per instructions, replace std=0 with 1 to avoid division by zero\n    std_X[std_X == 0] = 1.0\n    \n    X_train_std = (X_train_raw - mean_X) / std_X\n\n    # Step 2: Augment the standardized feature matrix with an intercept column\n    intercept_col = np.ones((X_train_std.shape[0], 1))\n    X_train_aug = np.hstack((intercept_col, X_train_std))\n\n    def numerically_stable_sigmoid(z):\n        \"\"\"Computes the sigmoid function in a numerically stable way.\"\"\"\n        # Using a vectorized implementation for efficiency\n        # This prevents overflow with large positive z and underflow with large negative z\n        pos_mask = (z >= 0)\n        neg_mask = (z < 0)\n        p = np.zeros_like(z, dtype=float)\n        p[pos_mask] = 1.0 / (1.0 + np.exp(-z[pos_mask]))\n        p[neg_mask] = np.exp(z[neg_mask]) / (1.0 + np.exp(z[neg_mask]))\n        return p\n\n    results = []\n    \n    num_features = X_train_aug.shape[1]\n\n    for lambda_val, test_features_raw in test_cases:\n        # Step 3: Fit the logistic regression model using Newton's method\n        \n        # Initialize parameters (beta) to zeros\n        beta = np.zeros((num_features, 1))\n        \n        # Regularization matrix I'\n        # Diagonal matrix with 0 for intercept and lambda for slopes\n        penalty_matrix = lambda_val * np.eye(num_features)\n        penalty_matrix[0, 0] = 0.0\n        \n        max_iter = 50\n        tolerance = 1e-8\n        \n        for i in range(max_iter):\n            # Calculate linear predictors and probabilities\n            z = X_train_aug @ beta\n            p = numerically_stable_sigmoid(z)\n            \n            # W is a diagonal matrix of weights p_i * (1 - p_i)\n            # Implemented with a 1D array for efficiency\n            weights = p * (1 - p)\n            W = np.diag(weights.flatten())\n            \n            # Calculate gradient of the penalized negative log-likelihood\n            # grad = X^T * (p - y) + lambda * I' * beta\n            gradient = X_train_aug.T @ (p - y_train) + penalty_matrix @ beta\n            \n            # Calculate Hessian of the penalized negative log-likelihood\n            # H = X^T * W * X + lambda * I'\n            hessian = X_train_aug.T @ W @ X_train_aug + penalty_matrix\n            \n            # Solve the linear system H * delta_beta = -gradient\n            # This is the Newton-Raphson update step\n            try:\n                # np.linalg.solve is more stable and efficient than inverting the matrix\n                delta_beta = np.linalg.solve(hessian, -gradient)\n            except np.linalg.LinAlgError:\n                # Use pseudo-inverse if Hessian is singular, for robustness\n                delta_beta = np.linalg.pinv(hessian) @ -gradient\n            \n            # Update parameters\n            beta += delta_beta\n            \n            # Check for convergence\n            if np.linalg.norm(delta_beta) < tolerance:\n                break\n\n        # Step 4: Classify the test advisor\n        x_test_std = (np.array(test_features_raw) - mean_X) / std_X\n        x_test_aug = np.hstack(([1.0], x_test_std))\n        \n        # Calculate predicted probability for the test case\n        prob_test = numerically_stable_sigmoid(x_test_aug @ beta).item()\n        \n        # Classify based on the 0.5 threshold\n        classification = 1 if prob_test >= 0.5 else 0\n        results.append(classification)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "信用风险分析不仅限于预测违约，还包括理解违约后的动态。本练习引入了“治愈率”模型，这是一个用于评估已违约贷款恢复正常状态可能性的专门工具[@problem_id:2385819]。您将运用生存分析的思想，为离散时间下的治愈过程推导并最大化一个自定义的对数似然函数，从而实践如何为特定的金融问题量身定制统计模型。", "id": "2385819", "problem": "要求您为违约贷款建立一个简约的离散时间治愈模型，以便在月度监控框架内估计借款人从不良状态转换到正常履约状态的概率。在不使用任何针对目标参数的预封装公式的情况下，请基于生存分析和 Bernoulli 试验的基本原理进行分析。您將实现一个程序，通过最大似然估计（Maximum Likelihood Estimation, MLE）来估计模型参数，并为一组固定的测试套件生成特定的数值输出。\n\n假设一笔贷款在第 $t=1$ 個月的月初违约后，其向治愈状态的转变由每个月的独立 Bernoulli 试验决定，每个借款人具有特定的月度治愈概率 $h_i \\in (0,1)$。如果观察一个借款人 $n_i \\in \\mathbb{N}$ 个月，并在第 $n_i$ 个月末治愈，则事件指标为 $d_i=1$；如果借款人在观察期内未治愈，则观察值为右删失，此时 $d_i=0$。在月度试验独立的假设下，对于借款人 $i$，其似然贡献的计算方式如下：当 $d_i=1$ 时，将前 $(n_i-1)$ 个月未治愈的概率与第 $n_i$ 个月治愈的概率相乘；当 $d_i=0$ 时，则是 $n_i$ 个月均未治愈的概率。这样可以得到一个可用 $h_i$、$n_i$ 和 $d_i$ 表示的似然函数。\n\n使用一个逻辑链接函数、一个协变量 $x_i \\in \\mathbb{R}$ 和一个截距项来对月度治愈概率进行建模：\n$$\nh_i \\;=\\; \\sigma(\\eta_i) \\;=\\; \\frac{1}{1 + e^{-\\eta_i}}, \n\\quad \\text{其中} \\quad \\eta_i \\;=\\; \\alpha + \\beta x_i,\n$$\n其中参数为 $\\alpha \\in \\mathbb{R}$ 和 $\\beta \\in \\mathbb{R}$。利用借款人之间的独立性，將全样本似然写成各借款人似然的乘积，然后使用对数推导出对数似然。\n\n从以上定义和仅有的这些核心假设出发，完成以下任务：\n- 推导样本对数似然作为 $(\\alpha,\\beta)$ 的函数。\n- 推导得分函数（对数似然关于 $(\\alpha,\\beta)$ 的梯度）。\n-推导 Hessian 矩阵（二阶导数矩阵），并用它设计一个 Newton 型算法，沿着凹的对数似然函数攀升至其最大值。为您选择的任何用于确保数值稳定性的阻尼或线搜索方法提供理由。\n- 实现一个程序，通过在给定数据集上最大化对数似然来估计 $(\\alpha,\\beta)$。请使用双精度算术。所有概率必须表示为小数，而非百分比。\n\n使用以下借款人数据集，其中每个元组为 $(x_i, n_i, d_i)$，$x_i$ 无单位，$n_i$ 的单位是月，$d_i \\in \\{0,1\\}$。数据集如下：\n- $(-1.5, 6, 0)$\n- $(-1.2, 6, 0)$\n- $(-1.0, 6, 0)$\n- $(-0.8, 5, 1)$\n- $(-0.5, 6, 1)$\n- $(-0.2, 4, 1)$\n- $(0.0, 3, 1)$\n- $(0.2, 2, 1)$\n- $(0.5, 1, 1)$\n- $(0.8, 1, 1)$\n- $(1.0, 1, 1)$\n- $(1.2, 1, 1)$\n- $(1.5, 1, 1)$\n- $(1.8, 1, 1)$\n- $(-0.3, 6, 0)$\n- $(0.3, 2, 1)$\n\n在估计出 $(\\hat{\\alpha},\\hat{\\beta})$ 后，计算以下测试套件的输出：\n- 估计的参数 $\\hat{\\alpha}$ 和 $\\hat{\\beta}$。\n- 使用 $h(x) = \\sigma(\\hat{\\alpha} + \\hat{\\beta} x)$ 计算 $x \\in \\{-1.0, 0.0, 1.0\\}$ 时的预测月度治愈概率。\n- 使用 $1 - (1 - h(x))^{6}$ 计算 $x \\in \\{-1.0, 1.0\\}$ 时的预测六个月治愈概率。\n- 边界条件检查：使用相同公式计算 $x \\in \\{-3.0, 3.0\\}$ 时的预测六个月治愈概率。\n\n所有概率都必须以小数形式报告。沒有物理单位。不涉及角度。\n\n您的程序必须生成单行输出，其中包含 $9$ 个结果，顺序完全如下：\n$[\\hat{\\alpha}, \\hat{\\beta}, h(-1.0), h(0.0), h(1.0), \\text{cure6}(-1.0), \\text{cure6}(1.0), \\text{cure6}(-3.0), \\text{cure6}(3.0)]$，\n结果需四舍五入到六位小数，以逗号分隔，并用方括号括起来。例如，一个有效的格式是 $[\\dots]$，其中包含不多不少 $9$ 个小数。\n\n您的程序需要计算的测试套件摘要：\n- 在提供的数据集上进行参数估计。\n- 预测 $x=-1.0$、$x=0.0$、$x=1.0$ 时的月度治愈概率。\n- 预测 $x=-1.0$、$x=1.0$、$x=-3.0$、$x=3.0$ 时的六个月治愈概率。\n\n您的最终答案必须是一个完整的、可运行的程序，该程序执行所有步骤并完全按照规定格式打印所需的单行输出。", "solution": "我们从离散时间生存模型构造开始。借款人 $i$ 在 $t=0$ 时违约，并从 $t=1$ 开始按月监控。令 $h_i \\in (0,1)$ 表示借款人 $i$ 的恒定月度治愈概率。月度治愈试验的独立性意味着，到治愈为止的等待时间服从参数为 $h_i$ 的几何分布。如果借款人在第 $n_i$ 个月治愈（事件指标 $d_i=1$），此结果的概率为\n$$\n\\Pr(T_i = n_i \\mid h_i) \\;=\\; (1-h_i)^{n_i-1} h_i.\n$$\n如果借款人在 $n_i$ 个月后被右删失（事件指标 $d_i=0$），观察到截至第 $n_i$ 个月（含）仍未治愈的概率为\n$$\n\\Pr(T_i > n_i \\mid h_i) \\;=\\; (1-h_i)^{n_i}.\n$$\n结合兩種情况，借款人层面的似然可以紧凑地写为\n$$\nL_i(h_i; n_i, d_i) \\;=\\; \\left[ (1-h_i)^{n_i-1} h_i \\right]^{d_i} \\left[(1-h_i)^{n_i}\\right]^{1-d_i}\n\\;=\\; h_i^{d_i} (1-h_i)^{n_i-d_i}.\n$$\n假设各借款人之间相互独立，样本似然为 $L(\\{h_i\\}) = \\prod_{i=1}^N L_i(h_i;n_i,d_i)$，样本对数似然为\n$$\n\\ell(\\{h_i\\}) \\;=\\; \\sum_{i=1}^N \\left[ d_i \\log h_i + (n_i - d_i) \\log(1 - h_i) \\right].\n$$\n\n接下来，我们通过一个带有截距 $\\alpha$ 和斜率 $\\beta$ 的逻辑链接函数，将 $h_i$ 与单个协变量 $x_i$ 所体现的观察到的异质性联系起来：\n$$\n\\eta_i \\;=\\; \\alpha + \\beta x_i, \\qquad\nh_i \\;=\\; \\sigma(\\eta_i) \\;=\\; \\frac{1}{1 + e^{-\\eta_i}}.\n$$\n因此，作为参数 $(\\alpha,\\beta)$ 函数的对数似然变为\n$$\n\\ell(\\alpha,\\beta) \\;=\\; \\sum_{i=1}^N \\left[ d_i \\log \\sigma(\\alpha + \\beta x_i) + (n_i - d_i) \\log\\left(1 - \\sigma(\\alpha + \\beta x_i)\\right) \\right].\n$$\n\n为应用最大似然估计（MLE），我们推导得分（梯度）和 Hessian 矩阵。令 $h_i = \\sigma(\\eta_i)$，并回顾 $\\frac{d\\sigma}{d\\eta} = h_i(1 - h_i)$。根据链式法则，\n$$\n\\frac{\\partial \\ell}{\\partial \\eta_i}\n= \\frac{d_i}{h_i} \\frac{d h_i}{d \\eta_i} - \\frac{n_i - d_i}{1 - h_i} \\frac{d h_i}{d \\eta_i}\n= \\left( \\frac{d_i}{h_i} - \\frac{n_i - d_i}{1 - h_i} \\right) h_i(1 - h_i)\n= d_i - n_i h_i.\n$$\n使用 $\\eta_i = \\alpha + \\beta x_i$，得分向量为\n$$\n\\nabla \\ell(\\alpha,\\beta)\n= \\sum_{i=1}^N (d_i - n_i h_i)\n\\begin{bmatrix}\n\\frac{\\partial \\eta_i}{\\partial \\alpha} \\\\\n\\frac{\\partial \\eta_i}{\\partial \\beta}\n\\end{bmatrix}\n= \\sum_{i=1}^N (d_i - n_i h_i)\n\\begin{bmatrix}\n1 \\\\ x_i\n\\end{bmatrix}.\n$$\nHessian 矩阵通过对得分再次求导得出。由于 $\\frac{\\partial}{\\partial \\eta_i}(d_i - n_i h_i) = -n_i h_i (1 - h_i)$，我们得到\n$$\n\\nabla^2 \\ell(\\alpha,\\beta)\n= \\sum_{i=1}^N \\left( - n_i h_i (1 - h_i) \\right)\n\\begin{bmatrix}\n1 \\\\ x_i\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & x_i\n\\end{bmatrix}\n= - \\sum_{i=1}^N n_i h_i (1 - h_i)\n\\begin{bmatrix}\n1 & x_i\\\\\nx_i & x_i^2\n\\end{bmatrix}.\n$$\n此 Hessian 矩阵是负半定的，因为 $n_i h_i (1 - h_i) \\ge 0$，这确保了对数似然函数是凹函数，并且任何驻点都是全局最大值点。\n\n一个用于最大化 $\\ell(\\alpha,\\beta)$ 的 Newton 型算法在每次迭代 $k$ 中求解线性系统\n$$\n\\nabla^2 \\ell(\\theta^{(k)}) \\, s^{(k)} \\;=\\; - \\nabla \\ell(\\theta^{(k)}),\n\\quad \\text{其中} \\quad \\theta^{(k+1)} \\;=\\; \\theta^{(k)} + \\gamma^{(k)} s^{(k)},\n$$\n其中 $\\theta = \\begin{bmatrix}\\alpha \\\\ \\beta\\end{bmatrix}$ 且 $\\gamma^{(k)} \\in (0,1]$ 是通过回溯线搜索选择的步长，以确保单调递增，即 $\\ell(\\theta^{(k+1)}) > \\ell(\\theta^{(k)})$。为保持数值稳定性，在计算 $\\log h_i$ 和 $\\log(1-h_i)$ 时，使用一个很小的 $\\varepsilon$ 将 $h_i$ 的值限制在远离 0 和 1 的范围内，例如，在双精度下强制 $h_i \\in [\\varepsilon, 1-\\varepsilon]$。\n\n利用最大似然估计量 $\\hat{\\theta} = (\\hat{\\alpha}, \\hat{\\beta})$，对于协变量值 $x$ 的预测月度治愈概率为\n$$\nh(x) \\;=\\; \\sigma(\\hat{\\alpha} + \\hat{\\beta} x),\n$$\n并且，在 $K$ 个月内治愈的预测概率可从几何等待时间分布得到：\n$$\n\\Pr(T \\le K \\mid x) \\;=\\; 1 - (1 - h(x))^{K}.\n$$\n在测试套件中，我们取 $K=6$，因此 $\\text{cure6}(x) = 1 - (1 - h(x))^{6}$。\n\n算法设计摘要：\n- 将 $\\theta^{(0)}$ 初始化为一个有限值，例如 $\\theta^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 迭代：\n  - 计算 $h_i = \\sigma(\\alpha + \\beta x_i)$ 并为保证稳定性，将其值限制在 $[\\varepsilon, 1 - \\varepsilon]$ 内。\n  - 评估 $\\ell(\\theta)$、$\\nabla \\ell(\\theta)$ 和 $\\nabla^2 \\ell(\\theta)$。\n  - 求解 $\\nabla^2 \\ell(\\theta) s = - \\nabla \\ell(\\theta)$；如果系统是病态的，则向 $\\nabla^2 \\ell$ 添加一个小的岭项 $\\lambda I$。\n  - 对 $\\gamma \\in (0,1]$ 执行回溯线搜索以确保函数值递增。\n  - 当 $\\lVert \\nabla \\ell(\\theta) \\rVert_2$ 低于某个容差或 $\\ell(\\theta)$ 的变化可忽略不计时停止。\n- 报告 $\\hat{\\alpha}$ 和 $\\hat{\\beta}$。\n- 计算 $x \\in \\{-1.0, 0.0, 1.0\\}$ 时的 $h(x)$ 和 $x \\in \\{-1.0, 1.0, -3.0, 3.0\\}$ 时的 $\\text{cure6}(x)$。\n- 以指定顺序和格式输出所有结果，结果为小数形式，四舍五入至六位小数。\n\n此方法基于包含右删失的离散时间治愈的几何等待时间模型、用于概率建模的逻辑链接函数以及用于凹似然函数的标准 Newton 优化方法。最终的程序实现了这些步骤，在提供的数据集上拟合模型，并打印所要求的单行输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef sigmoid(z):\n    # Stable sigmoid\n    # For large negative z, exp(-z) overflows, so we use numerically stable form\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef loglik_grad_hess(theta, X, n, d, eps=1e-12):\n    # theta: [alpha, beta]\n    # X: array of shape (N, 2) with columns [1, x]\n    # n: durations (N,)\n    # d: event indicators (N,)\n    eta = X @ theta\n    h = sigmoid(eta)\n    # clamp h for numerical stability in logs\n    h_clamped = np.clip(h, eps, 1.0 - eps)\n    # log-likelihood\n    ll = np.sum(d * np.log(h_clamped) + (n - d) * np.log(1.0 - h_clamped))\n    # gradient: sum_i (d_i - n_i h_i) * [1, x_i]\n    residual = d - n * h\n    grad = X.T @ residual\n    # Hessian: - sum_i n_i h_i(1-h_i) * [[1, x_i],[x_i, x_i^2]]\n    w = n * h * (1.0 - h)  # (N,)\n    # Build Hessian as - X^T diag(w) X\n    # Compute X^T * (w * X) efficiently\n    WX = X * w[:, None]\n    hess = -(X.T @ WX)\n    return ll, grad, hess\n\ndef newton_maximize(initial_theta, X, n, d, tol=1e-9, max_iter=100, ls_beta=0.5, ls_c=1e-4):\n    theta = initial_theta.copy()\n    prev_ll = -np.inf\n    for it in range(max_iter):\n        ll, grad, hess = loglik_grad_hess(theta, X, n, d)\n        grad_norm = np.linalg.norm(grad, ord=2)\n        if grad_norm < tol:\n            break\n        # Ensure Hessian is not singular; add small ridge if needed\n        # Try Cholesky on -hess (which should be positive semi-definite)\n        # We'll solve hess * s = -grad; if fails, add ridge\n        ridge = 1e-8\n        for _ in range(8):\n            try:\n                # Solve linear system\n                step = np.linalg.solve(hess - ridge * np.eye(hess.shape[0]), -grad)\n                break\n            except np.linalg.LinAlgError:\n                ridge *= 10.0\n        else:\n            # Fallback to gradient ascent direction if Hessian is too ill-conditioned\n            step = grad / max(grad_norm, 1e-12)\n\n        # Backtracking line search to ensure ascent: ll(theta + t*step) >= ll + ls_c * t * grad^T * step\n        t = 1.0\n        current_ll = ll\n        directional_derivative = grad @ step\n        # If directional derivative is negative (should be ascent), flip step\n        if directional_derivative < 0:\n            step = -step\n            directional_derivative = -directional_derivative\n\n        while t > 1e-8:\n            candidate = theta + t * step\n            new_ll, _, _ = loglik_grad_hess(candidate, X, n, d)\n            if new_ll >= current_ll + ls_c * t * directional_derivative:\n                theta = candidate\n                prev_ll = new_ll\n                break\n            t *= ls_beta\n        else:\n            # If line search fails, stop to avoid infinite loop\n            theta = theta  # unchanged\n            break\n    return theta\n\ndef solve():\n    # Define the dataset: list of (x, n, d)\n    data = [\n        (-1.5, 6, 0),\n        (-1.2, 6, 0),\n        (-1.0, 6, 0),\n        (-0.8, 5, 1),\n        (-0.5, 6, 1),\n        (-0.2, 4, 1),\n        (0.0, 3, 1),\n        (0.2, 2, 1),\n        (0.5, 1, 1),\n        (0.8, 1, 1),\n        (1.0, 1, 1),\n        (1.2, 1, 1),\n        (1.5, 1, 1),\n        (1.8, 1, 1),\n        (-0.3, 6, 0),\n        (0.3, 2, 1),\n    ]\n    # Prepare arrays\n    x = np.array([row[0] for row in data], dtype=float)\n    n = np.array([row[1] for row in data], dtype=float)\n    d = np.array([row[2] for row in data], dtype=float)\n    N = x.shape[0]\n    X = np.column_stack([np.ones(N), x])\n\n    # Estimate parameters via Newton maximization\n    theta0 = np.array([0.0, 0.0], dtype=float)\n    theta_hat = newton_maximize(theta0, X, n, d, tol=1e-9, max_iter=100)\n\n    alpha_hat, beta_hat = theta_hat.tolist()\n\n    # Prediction functions\n    def h_of_x(xv):\n        return float(sigmoid(alpha_hat + beta_hat * xv))\n\n    def cure_k_of_x(xv, K=6):\n        h = h_of_x(xv)\n        return float(1.0 - (1.0 - h) ** K)\n\n    # Test suite predictions\n    xs_monthly = [-1.0, 0.0, 1.0]\n    xs_cure6_main = [-1.0, 1.0]\n    xs_cure6_edge = [-3.0, 3.0]\n\n    monthly_probs = [h_of_x(v) for v in xs_monthly]\n    cure6_main = [cure_k_of_x(v, K=6) for v in xs_cure6_main]\n    cure6_edge = [cure_k_of_x(v, K=6) for v in xs_cure6_edge]\n\n    # Aggregate results in required order:\n    # [alpha_hat, beta_hat, h(-1.0), h(0.0), h(1.0), cure6(-1.0), cure6(1.0), cure6(-3.0), cure6(3.0)]\n    results = [alpha_hat, beta_hat] + monthly_probs + cure6_main + cure6_edge\n\n    # Round to six decimals\n    results_str = [f\"{v:.6f}\" for v in results]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"}, {"introduction": "单个实体的信用风险可能会通过金融网络蔓延，形成系统性风险。这个实践练习[@problem_id:2385774]将带您进入宏观审慎的视角，通过构建一个网络传染模型来模拟初始违约事件如何在互联的债务人之间引发连锁反应。您将实施一个迭代的压力测试过程，以量化风险的级联效应并评估整个投资组合的最终损失。", "id": "2385774", "problem": "您的任务是为一个贷款组合形式化并实现一个压力测试程序，该程序需包含由外生交易对手违约产生的二阶传染效应。设计必须从核心信用风险定义出发，并遵循明确的逻辑规则进行。考虑一个包含 $N$ 个债务人的系统，其索引为 $i \\in \\{0,1,\\dots,N-1\\}$。相关的原始要素如下：\n- 银行对这些债务人的违约风险暴露向量 $b \\in \\mathbb{R}_{\\ge 0}^{N}$，其中 $b_i$ 是银行对债务人 $i$ 的风险暴露。\n- 初始权益缓冲向量 $K^{(0)} \\in \\mathbb{R}_{> 0}^{N}$，其中 $K_i^{(0)}$ 是债务人 $i$ 在传染发生前的权益。\n- 一个债务人间的风险暴露矩阵 $X \\in \\mathbb{R}_{\\ge 0}^{N \\times N}$，对所有 $i$ 都有 $X_{ii} = 0$，其中 $X_{i,j}$ 是债务人 $i$ 对债务人 $j$ 的风险暴露。\n- 银行贷款的违约损失率 $L^{\\text{bank}} \\in [0,1]$ 和债务人之间债权的违约损失率 $L^{\\text{inter}} \\in [0,1]$。\n- 一个系统性损失乘数 $m \\ge 1$，用于在压力期间放大债务人间的损失。\n- 一个初始违约集合 $D^{(0)} \\subseteq \\{0,1,\\dots,N-1\\}$，代表在时间 $t=0$ 时的外生违约。\n\n基本定义与规则：\n- 违约风险暴露（EAD）是指当交易对手违约时，面临损失的未偿风险暴露；在此，银行对债务人 $i$ 的 EAD 为 $b_i$，债务人 $i$ 对 $j$ 的 EAD 为 $X_{i,j}$。\n- 违约损失率（LGD）是指违约时未能收回的 EAD 比例；在此，银行贷款的 LGD 为 $L^{\\text{bank}}$，债务人间的 LGD 为 $L^{\\text{inter}}$。\n- 权益因已实现的信用损失而减少。在每个传染迭代 $t \\in \\{1,2,\\dots\\}$ 中，通过扣除仅由在迭代 $t-1$ 时新近违约的交易对手所造成的损失来定义 $K^{(t)}$。对于尚未违约的债务人 $i$，\n$$\nK_i^{(t)} \\;=\\; K_i^{(t-1)} \\;-\\; \\sum_{j \\in \\Delta D^{(t-1)}} X_{i,j} \\cdot \\big(m \\cdot L^{\\text{inter}}\\big),\n$$\n其中 $\\Delta D^{(t-1)} = D^{(t-1)} \\setminus D^{(t-2)}$ 是在迭代 $t-1$ 时发生违约的债务人集合（设 $D^{(-1)} := \\emptyset$）。违约规则是：债务人 $i$ 在迭代 $t$ 时违约，当且仅当 $K_i^{(t)} \\le 0$。使用数值容差 $\\varepsilon = 10^{-12}$，并将 $K_i^{(t)} \\le \\varepsilon$ 的情况视为违约，以避免浮点运算误差。\n- 当首次出现迭代 $T$ 使得 $\\Delta D^{(T)} = \\emptyset$ 时，传染过程终止。最终违约集合为 $D^{(\\infty)} = D^{(T)}$。\n- 银行的组合损失为\n$$\n\\text{Loss} \\;=\\; \\sum_{i \\in D^{(\\infty)}} b_i \\cdot L^{\\text{bank}}.\n$$\n\n您的任务是实现一个程序，为下述每个测试用例计算：\n- 银行的总损失，以实数形式表示（结果表示为小数，而非百分比），以及\n- 违约债务人的总数（一个整数），包括外生初始违约和任何传染引发的违约。\n\n所有输入中的索引均从零开始。不涉及物理单位。不使用角度。百分比必须表示为小数（例如，写成 $0.4$ 而非 $40\\%$）。\n\n测试套件：\n- 案例A（二阶传染，中度放大）：\n  - $N = 4$。\n  - $b = [10.0,\\, 5.0,\\, 8.0,\\, 6.0]$。\n  - $K^{(0)} = [2.0,\\, 2.0,\\, 1.0,\\, 1.1]$。\n  - $X$ 的非零项为 $X_{1,0} = 1.6$, $X_{2,0} = 1.8$, $X_{3,0} = 0.6$, $X_{1,2} = 0.9$, $X_{3,2} = 1.2$；所有其他项均为 $0$。\n  - $L^{\\text{bank}} = 0.45$, $L^{\\text{inter}} = 0.5$, $m = 1.2$。\n  - $D^{(0)} = \\{0\\}$。\n- 案例B（无传染网络）：\n  - $N = 3$。\n  - $b = [5.0,\\, 5.0,\\, 5.0]$。\n  - $K^{(0)} = [1.0,\\, 1.0,\\, 1.0]$。\n  - $X$ 是 $3 \\times 3$ 的零矩阵。\n  - $L^{\\text{bank}} = 0.4$, $L^{\\text{inter}} = 0.5$, $m = 1.2$。\n  - $D^{(0)} = \\{1\\}$。\n- 案例C（违约阈值的边界条件，完全级联）：\n  - $N = 3$。\n  - $b = [1.0,\\, 1.0,\\, 1.0]$。\n  - $K^{(0)} = [1.0,\\, 0.5,\\, 0.5]$。\n  - $X$ 的非零项为 $X_{1,0} = 1.0$, $X_{2,1} = 1.0$；所有其他项均为 $0$。\n  - $L^{\\text{bank}} = 0.5$, $L^{\\text{inter}} = 0.5$, $m = 1.0$。\n  - $D^{(0)} = \\{0\\}$。\n- 案例D（放大效应在 $m = 1$ 时不会发生的情况下引发了传染）：\n  - $N = 4$。\n  - $b = [2.0,\\, 2.0,\\, 2.0,\\, 2.0]$。\n  - $K^{(0)} = [1.2,\\, 0.9,\\, 0.15,\\, 0.5]$。\n  - $X$ 的非零项为 $X_{0,3} = 0.3$, $X_{1,3} = 0.3$, $X_{2,3} = 0.2$, $X_{0,2} = 0.7$, $X_{1,2} = 0.6$；所有其他项均为 $0$。\n  - $L^{\\text{bank}} = 0.5$, $L^{\\text{inter}} = 0.4$, $m = 2.0$。\n  - $D^{(0)} = \\{3\\}$。\n\n编程任务：\n- 精确地按照所述实现上述传染动态。\n- 对于每个案例，返回一个列表 $[\\text{Loss},\\, \\text{Count}]$，其中 $\\text{Loss}$ 是一个浮点数，$\\text{Count}$ 是一个整数。您可以根据需要对中间计算进行舍入，但最终的 $\\text{Loss}$ 应使用标准浮点运算计算，并且绝对误差预计至少精确到 $10^{-6}$。\n- 最终输出格式：您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，每个案例的结果本身就是一个双元素列表。例如，您的输出必须如下所示\n$[\\,[\\text{Loss}_A,\\text{Count}_A],[\\text{Loss}_B,\\text{Count}_B],[\\text{Loss}_C,\\text{Count}_C],[\\text{Loss}_D,\\text{Count}_D]\\,]$\n打印行中无空格。", "solution": "问题陈述需经过验证。\n\n步骤 1：提取已知条件。\n- 债务人数量为 $N$。\n- 银行的违约风险暴露向量为 $b \\in \\mathbb{R}_{\\ge 0}^{N}$。\n- 初始权益缓冲向量为 $K^{(0)} \\in \\mathbb{R}_{> 0}^{N}$。\n- 债务人间的风险暴露矩阵为 $X \\in \\mathbb{R}_{\\ge 0}^{N \\times N}$，其中 $X_{i,j}$ 是债务人 $i$ 对债务人 $j$ 的风险暴露，且 $X_{ii} = 0$。\n- 银行的违约损失率为 $L^{\\text{bank}} \\in [0,1]$。\n- 债务人间的违约损失率为 $L^{\\text{inter}} \\in [0,1]$。\n- 系统性损失乘数为 $m \\ge 1$。\n- 外生初始违约债务人集合为 $D^{(0)} \\subseteq \\{0,1,\\dots,N-1\\}$。\n- 违约的数值容差为 $\\varepsilon = 10^{-12}$。如果债务人 $i$ 的权益 $K_i^{(t)} \\le \\varepsilon$，则其违约。\n- 传染动态由权益更新的迭代规则控制：\n$$K_i^{(t)} = K_i^{(t-1)} - \\sum_{j \\in \\Delta D^{(t-1)}} X_{i,j} \\cdot \\big(m \\cdot L^{\\text{inter}}\\big)$$\n其中 $\\Delta D^{(t-1)} = D^{(t-1)} \\setminus D^{(t-2)}$ 且 $D^{(-1)} := \\emptyset$。\n- 当首次出现迭代 $T$ 使得 $\\Delta D^{(T)} = \\emptyset$ 时，传染终止。最终违约集合为 $D^{(\\infty)} = D^{(T)}$。\n- 银行的组合损失为 $\\text{Loss} = \\sum_{i \\in D^{(\\infty)}} b_i \\cdot L^{\\text{bank}}$。\n- 提供了四个具体的测试用例（A、B、C、D），包含了上述参数的所有必要数值。\n\n步骤 2：使用提取的已知条件进行验证。\n- **科学性依据**：该问题描述了一个金融传染的离散时间网络模型。风险暴露、权益、违约损失率和违约级联等概念是金融风险管理和计算经济学领域的标准概念。该模型虽然经过简化，但基于一个既定原则：资产（银行间风险暴露）的损失会消耗公司的权益，导致其自身倒闭。因此，该问题具有科学依据。\n- **适定性**：该过程是一个迭代算法。系统的状态是违约债务人的集合。该集合是单调不减的。由于债务人总数 $N$ 是有限的，传染过程最多在 $N$ 次迭代后必定终止。最终状态（所有违约债务人的集合）由初始条件和系统规则唯一确定。因此，总损失和违约数量存在唯一的稳定解。该问题是适定的。\n- **客观性**：该问题使用精确的数学符号和无歧义的规则进行定义。所有参数均已给出，并且要求的输出也已明确指定。没有主观性语言。\n\n步骤 3：结论与行动。\n该问题是有效的，因为它具有科学依据、适定性和客观性。将提供一个解。\n\n传染过程被建模为一系列离散的时间步，索引为 $t \\in \\{0, 1, 2, \\dots\\}$。我们必须追踪每个债务人的状态，包括其权益资本和违约状态。\n\n在迭代 $t$ 时的状态变量是：\n- $K^{(t)} \\in \\mathbb{R}^N$：所有债务人的权益向量。\n- $D^{(t)} \\subseteq \\{0, 1, \\dots, N-1\\}$：截至并包括迭代 $t$ 时所有已违约债务人的累积集合。\n- $\\Delta D^{(t)} = D^{(t)} \\setminus D^{(t-1)}$：在迭代 $t$ 时新发生违约的债务人集合。\n\n算法流程如下：\n1.  **初始化 ($t=0$)**：\n    - 权益向量为初始权益 $K^{(0)}$。\n    - 累积违约集合 $D^{(-1)}$ 定义为空集 $\\emptyset$。\n    - 初始违约集合 $D^{(0)}$ 是外生给定的。\n    - $t=0$ 时新发生违约的债务人集合为 $\\Delta D^{(0)} = D^{(0)} \\setminus D^{(-1)} = D^{(0)}$。\n\n2.  **迭代传染 ($t=1, 2, \\dots, T$)**：仿真的核心是一个循环，只要新一波违约发生，该循环就会继续。我们将迭代 $t$ 开始时的状态定义为 $(K^{(t-1)}, D^{(t-1)})$，其中 $\\Delta D^{(t-1)}$ 是刚刚违约的债务人集合。\n    - 如果 $\\Delta D^{(t-1)} = \\emptyset$，则传染已停止。过程终止。达到最终状态。\n    - 否则，我们必须计算对剩余有偿付能力债务人的影响。对于每个尚未违约的债务人 $i$（即 $i \\notin D^{(t-1)}$），我们计算其因 $\\Delta D^{(t-1)}$ 中的交易对手而招致的损失：\n    $$\n    L_{i, t} = \\sum_{j \\in \\Delta D^{(t-1)}} X_{i,j} \\cdot \\big(m \\cdot L^{\\text{inter}}\\big)\n    $$\n    - 然后更新这些有偿付能力债务人的权益：\n    $$\n    K_i^{(t)} = K_i^{(t-1)} - L_{i, t}\n    $$\n    对于已违约的债务人或在此步骤中权益未被更新的债务人，其权益值保持不变，即 $K_i^{(t)} = K_i^{(t-1)}$。\n    - 确定本轮迭代的新违约集合。一个先前有偿付能力的债务人 $i$ ($i \\notin D^{(t-1)}$)，如果其权益被耗尽，即 $K_i^{(t)} \\le \\varepsilon$，则现在违约。令此集合为 $\\Delta D_{\\text{new}}^{(t)}$。\n    - 更新累积违约集合：$D^{(t)} = D^{(t-1)} \\cup \\Delta D_{\\text{new}}^{(t)}$。\n    - 用于*下一次*迭代的新违约债务人集合是 $\\Delta D^{(t)} = \\Delta D_{\\text{new}}^{(t)}$。然后过程对 $t+1$ 重复。\n\n3.  **终止与最终计算**：当迭代 $T$ 出现 $\\Delta D^{(T-1)} = \\emptyset$ 时，循环终止。\n    - 所有违约债务人的最终集合为 $D^{(\\infty)} = D^{(T-1)}$。\n    - 违约总数为该集合的基数，即 $|D^{(\\infty)}|$。\n    - 银行的总损失根据其对最终违约集合中债务人的风险暴露计算得出：\n    $$\n    \\text{Loss} = \\sum_{i \\in D^{(\\infty)}} b_i \\cdot L^{\\text{bank}}\n    $$\n\n该算法针对所提供的四个测试用例中的每一个进行实现。每个案例的参数用于初始化仿真，然后运行仿真直至完成，以找出最终损失和违约债务人的数量。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print results.\n    \"\"\"\n\n    # Numerical tolerance for default check\n    EPSILON = 1e-12\n\n    test_cases = [\n        # Case A: second-order contagion, moderate amplification\n        {\n            \"N\": 4,\n            \"b\": np.array([10.0, 5.0, 8.0, 6.0]),\n            \"K0\": np.array([2.0, 2.0, 1.0, 1.1]),\n            \"X_sparse\": {(1, 0): 1.6, (2, 0): 1.8, (3, 0): 0.6, (1, 2): 0.9, (3, 2): 1.2},\n            \"L_bank\": 0.45,\n            \"L_inter\": 0.5,\n            \"m\": 1.2,\n            \"D0\": {0},\n        },\n        # Case B: no contagion network\n        {\n            \"N\": 3,\n            \"b\": np.array([5.0, 5.0, 5.0]),\n            \"K0\": np.array([1.0, 1.0, 1.0]),\n            \"X_sparse\": {},\n            \"L_bank\": 0.4,\n            \"L_inter\": 0.5,\n            \"m\": 1.2,\n            \"D0\": {1},\n        },\n        # Case C: boundary condition at default threshold, full cascade\n        {\n            \"N\": 3,\n            \"b\": np.array([1.0, 1.0, 1.0]),\n            \"K0\": np.array([1.0, 0.5, 0.5]),\n            \"X_sparse\": {(1, 0): 1.0, (2, 1): 1.0},\n            \"L_bank\": 0.5,\n            \"L_inter\": 0.5,\n            \"m\": 1.0,\n            \"D0\": {0},\n        },\n        # Case D: amplification creates contagion that would not occur at m=1\n        {\n            \"N\": 4,\n            \"b\": np.array([2.0, 2.0, 2.0, 2.0]),\n            \"K0\": np.array([1.2, 0.9, 0.15, 0.5]),\n            \"X_sparse\": {(0, 3): 0.3, (1, 3): 0.3, (2, 3): 0.2, (0, 2): 0.7, (1, 2): 0.6},\n            \"L_bank\": 0.5,\n            \"L_inter\": 0.4,\n            \"m\": 2.0,\n            \"D0\": {3},\n        },\n    ]\n\n    def run_simulation(case):\n        \"\"\"\n        Runs the contagion simulation for a single test case.\n        \"\"\"\n        N = case[\"N\"]\n        b = case[\"b\"]\n        K = case[\"K0\"].copy()\n        X_sparse = case[\"X_sparse\"]\n        L_bank = case[\"L_bank\"]\n        L_inter = case[\"L_inter\"]\n        m = case[\"m\"]\n        D0 = case[\"D0\"]\n\n        # Construct the dense exposure matrix X\n        X = np.zeros((N, N))\n        for (i, j), val in X_sparse.items():\n            X[i, j] = val\n\n        # State variables for the simulation\n        D_final = set(D0)\n        newly_defaulted = set(D0)\n        \n        effective_lgd = m * L_inter\n\n        while newly_defaulted:\n            last_wave_defaults = list(newly_defaulted)\n            newly_defaulted = set()\n            \n            solvent_obligors = [i for i in range(N) if i not in D_final]\n            \n            if not solvent_obligors:\n                break\n                \n            # Calculate losses for solvent obligors from the last wave of defaults\n            losses = X[solvent_obligors, :][:, last_wave_defaults].sum(axis=1) * effective_lgd\n\n            # Update equity and check for new defaults\n            for idx, obligor_idx in enumerate(solvent_obligors):\n                K[obligor_idx] -= losses[idx]\n                if K[obligor_idx] <= EPSILON:\n                    newly_defaulted.add(obligor_idx)\n\n            D_final.update(newly_defaulted)\n\n        # Calculate final results\n        final_loss = b[list(D_final)].sum() * L_bank\n        num_defaults = len(D_final)\n        \n        return [final_loss, num_defaults]\n\n    results = []\n    for case in test_cases:\n        result = run_simulation(case)\n        results.append(f\"[{result[0]},{result[1]}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"}]}