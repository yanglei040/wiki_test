{"hands_on_practices": [{"introduction": "“El Farol Bar”问题是基于主体的计算经济学中的一个基础模型，它生动地展示了简单的个体决策如何涌现出复杂的集体行为。在这个练习中，主体根据过去的经验独立决定是否去酒吧，而无法确切知道他人的选择。本实践在经典模型的基础上引入了联盟和通信机制，让您能够探索群体内部的协调如何影响整体的出席率和资源利用效率。通过实现这个仿真，您将获得强化学习模型和分析社会现象涌现的实践经验。[@problem_id:2370498]", "id": "2370498", "problem": "要求您实现并分析一个 El Farol 酒吧问题的基于主体的模型。在此模型中，主体可以组织成联盟，这些联盟能以给定的成功内部沟通概率来协调其行动。该模型必须在固定的离散时期内进行模拟，其中定义了明确的主体行为、联盟激活和基于收益的学习。所有随机性必须通过显式设定伪随机数生成器的种子来确保可复现。\n\n模型定义：\n- 有 $N$ 个主体，索引为 $i \\in \\{0,1,\\dots,N-1\\}$，以及一个容量为 $C \\in \\mathbb{N}$ 的酒吧。时间以离散时期 $t=0,1,\\dots,T-1$ 推进。\n- 每个主体 $i$ 在每个时期选择一个行动 $a_i(t) \\in \\{0,1\\}$，其中 $a_i(t)=1$ 表示去酒吧，$a_i(t)=0$ 表示待在家里。令总出席人数为 $A(t)=\\sum_{i=0}^{N-1} a_i(t)$。\n- 主体被外生地划分为不相交的联盟 $g \\in \\mathcal{G}$，这些联盟构成主体集合的一个完全划分。一个联盟 $g$ 的大小为 $|g| \\ge 1$。大小为 $|g|=1$ 的联盟是单体。对于每个大小为 $|g| \\ge 2$ 的联盟 $g$，定义其配额为 $q_g = \\left\\lfloor C \\cdot \\frac{|g|}{N} \\right\\rfloor$。对于大小为 $|g|=1$ 的单体，从不执行协调，也不应用任何配额。\n- 每个主体 $i$ 维持两种吸引力（倾向）：去酒吧的吸引力 $A_i^{\\text{att}}(t)$ 和待在家的吸引力 $A_i^{\\text{stay}}(t)$，初始值均为 $A_i^{\\text{att}}(0)=0$ 和 $A_i^{\\text{stay}}(0)=0$。定义参数 $\\rho \\in (0,1)$（新近度）和 $\\lambda > 0$（学习步长）。\n- 私人意图阶段：在每个时期 $t$ 开始时，每个主体根据吸引力形成一个私人意图 $b_i(t) \\in \\{0,1\\}$：如果 $A_i^{\\text{att}}(t) > A_i^{\\text{stay}}(t)$，则 $b_i(t)=1$；如果 $A_i^{\\text{att}}(t) &lt; A_i^{\\text{stay}}(t)$，则 $b_i(t)=0$；如果 $A_i^{\\text{att}}(t)=A_i^{\\text{stay}}(t)$，则通过一次公平的伯努利抛硬币来选择 $b_i(t)$。平局和所有抛硬币必须使用指定的伪随机数生成器和种子以保证可复现性。\n- 联盟激活与协调：对于每个大小为 $|g|\\ge 2$ 的联盟 $g$，通过一次成功概率为 $p_{\\text{comm}} \\in [0,1]$ 的独立伯努利试验来决定 $g$ 在时期 $t$ 是否激活。如果未激活，所有成员将其私人意图作为最终行动。如果在时期 $t$ 激活，联盟观察其成员的私人意图向量 $\\{b_i(t): i \\in g\\}$，并应用以下向下调整规则来产生最终行动 $\\{a_i(t): i \\in g\\}$：\n  1. 令 $S_g(t)=\\sum_{i \\in g} b_i(t)$ 为意图去酒吧的成员数量。\n  2. 如果 $S_g(t) \\le q_g$，则对所有 $i \\in g$，有 $a_i(t)=b_i(t)$。\n  3. 如果 $S_g(t) > q_g$，则在子集 $\\{i \\in g: b_i(t)=1\\}$ 中，精确选择 $q_g$ 名成员去酒吧，选择的标准是其 $\\Delta_i(t)=A_i^{\\text{att}}(t)-A_i^{\\text{stay}}(t)$ 值最大；$\\Delta_i(t)$ 的平局通过优先选择较小的主体索引来打破。为选定的 $q_g$ 名成员设置 $a_i(t)=1$，为该子集中的其余成员设置 $a_i(t)=0$。对于意图为 $b_i(t)=0$ 的主体，设置 $a_i(t)=0$。\n- 对于任何大小为 $|g|=1$ 的单体联盟 $g$，其唯一成员 $i$ 的最终行动为 $a_i(t)=b_i(t)$。\n- 收益与学习：在所有联盟都决策完毕后，总出席人数 $A(t)=\\sum_i a_i(t)$ 得以实现。每个主体的时期收益 $u_i(t)$ 为\n  $$u_i(t)=\\begin{cases}\n  1 & \\text{if } a_i(t)=1 \\text{ and } A(t) \\le C,\\\\\n  1 & \\text{if } a_i(t)=0 \\text{ and } A(t) > C,\\\\\n  0 & \\text{otherwise.}\n  \\end{cases}$$\n  吸引力按如下方式更新：\n  $$A_i^{\\text{att}}(t+1)=\\begin{cases}\n  (1-\\rho)\\,A_i^{\\text{att}}(t) + \\lambda\\,u_i(t) & \\text{if } a_i(t)=1,\\\\\n  (1-\\rho)\\,A_i^{\\text{att}}(t) & \\text{if } a_i(t)=0,\n  \\end{cases}$$\n  $$A_i^{\\text{stay}}(t+1)=\\begin{cases}\n  (1-\\rho)\\,A_i^{\\text{stay}}(t) + \\lambda\\,u_i(t) & \\text{if } a_i(t)=0,\\\\\n  (1-\\rho)\\,A_i^{\\text{stay}}(t) & \\text{if } a_i(t)=1.\n  \\end{cases}$$\n\n度量定义：\n- 对于给定的时间范围 $T$ 和尾部窗口长度 $H$（$1 \\le H \\le T$），定义在时期 $t \\in \\{T-H, T-H+1, \\dots, T-1\\}$ 上的尾部平均值：\n  1. 平均出席人数差距 $\\bar{G} = \\frac{1}{H}\\sum_{t=T-H}^{T-1} \\left( A(t) - C \\right)$。\n  2. 过度拥挤频率 $\\bar{O} = \\frac{1}{H}\\sum_{t=T-H}^{T-1} \\mathbf{1}\\{A(t) > C\\}$，以 $[0,1]$ 范围内的小数表示。\n  3. 平均每时期协调反转份额 $\\bar{F} = \\frac{1}{H}\\sum_{t=T-H}^{T-1} \\left( \\frac{1}{N} \\sum_{i=0}^{N-1} \\mathbf{1}\\{a_i(t) \\ne b_i(t)\\} \\right)$，即因联盟协调导致最终行动与其私人意图不同的主体的平均比例。\n\n随机性与可复现性：\n- 所有随机性，包括意图的平局打破和联盟激活，都必须由一个伪随机数生成器产生，该生成器须根据各测试用例的指定进行种子设定。请将给定的种子精确地用作该测试用例中随机数生成器的初始化种子。\n\n测试套件：\n请实现该模拟，并为以下四个参数集中的每一个计算 $(\\bar{G},\\bar{O},\\bar{F})$。在每种情况下，请按指定顺序报告在最后 $H$ 个时期内计算出的结果。\n\n- 用例 1：$N=51$，$C=25$，$T=2000$，$H=500$，$\\rho=0.01$，$\\lambda=0.2$，联盟大小由一个包含 $51$ 个均为 $1$ 的条目的列表给出（每个主体都是一个单体），$p_{\\text{comm}}=0.5$，种子 $=42$。\n- 用例 2：$N=51$，$C=25$，$T=2000$，$H=500$，$\\rho=0.01$，$\\lambda=0.2$，联盟大小由列表 $[3,3,3,3,3,3,3,30]$ 给出（按主体索引递增顺序），$p_{\\text{comm}}=1.0$，种子 $=43$。\n- 用例 3：$N=51$，$C=25$，$T=2000$，$H=500$，$\\rho=0.01$，$\\lambda=0.2$，联盟大小由列表 $[3,3,3,3,3,3,3,30]$ 给出，$p_{\\text{comm}}=0.0$，种子 $=44$。\n- 用例 4：$N=51$，$C=25$，$T=2000$，$H=500$，$\\rho=0.01$，$\\lambda=0.2$，联盟大小由一个包含十个等于 $5$ 的条目后跟一个等于 $1$ 的条目的列表给出（即, $[5,5,5,5,5,5,5,5,5,5,1]$），$p_{\\text{comm}}=0.8$，种子 $=45$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。每个测试用例的结果本身必须是包含三个浮点数 $[\\bar{G},\\bar{O},\\bar{F}]$ 的列表，顺序与上述测试用例相同。例如，整体输出格式必须是\n$[[g_1,o_1,f_1],[g_2,o_2,f_2],[g_3,o_3,f_3],[g_4,o_4,f_4]]$,\n其中每个 $g_k$、$o_k$和 $f_k$ 都以小数形式报告。", "solution": "问题陈述已经过严格验证，并被确定为在基于主体的计算经济学领域内是良定的、有科学依据且内部一致的。所有参数、行为规则以及客观评估标准都得到了明确的规定。因此，我将着手提供一个完整的解决方案。\n\n该问题要求实现一个离散时间的基于主体的模拟模型。系统状态由 $N$ 个主体的吸引力定义，这些吸引力根据它们的行动和由此产生的收益在 $T$ 个时期内演变。解决方案的核心是一个精确执行模型动态的计算算法。设计分为三个主要阶段：初始化、主模拟循环和最终度量。\n\n首先，初始化阶段建立模拟环境。这包括设置 $N$ 个主体，并根据指定的联盟大小列表将它们划分为不相交的联盟 $\\mathcal{G}$。对于每个大小为 $|g| \\ge 2$ 的联盟 $g \\in \\mathcal{G}$，预先计算一个协调配额 $q_g = \\lfloor C \\cdot \\frac{|g|}{N} \\rfloor$，其中 $C$ 是酒吧容量。每个主体 $i$ 的去酒吧和待在家的吸引力都初始化为零，即 $A_i^{\\text{att}}(0)=0$ 和 $A_i^{\\text{stay}}(0)=0$。为每个测试用例指定伪随机数生成器的种子，以确保所有随机事件的可复现性。\n\n其次，主模拟循环从时间 $t=0$ 迭代到 $t=T-1$。每个时期 $t$ 包含一系列直接源自问题定义的步骤：\n\n1.  **私人意图形成**：每个主体 $i$ 形成一个私人意图 $b_i(t) \\in \\{0,1\\}$，选择去酒吧（1）或待在家里（0）。这个决策基于对其当前吸引力的比较。如果 $A_i^{\\text{att}}(t) > A_i^{\\text{stay}}(t)$，意图为 $b_i(t)=1$。如果 $A_i^{\\text{att}}(t) &lt; A_i^{\\text{stay}}(t)$，意图为 $b_i(t)=0$。在平局的情况下，即 $A_i^{\\text{att}}(t) = A_i^{\\text{stay}}(t)$，意图 $b_i(t)$ 通过使用带种子的随机数生成器进行一次公平的抛硬币来确定。\n\n2.  **联盟协调**：确定最终行动 $a_i(t)$。对于单体联盟（$|g|=1$）中的主体，最终行动总是其私人意图，即 $a_i(t) = b_i(t)$。对于每个非单体联盟 $g$（$|g| \\ge 2$），通过一次成功概率为 $p_{\\text{comm}}$ 的伯努利试验来决定联盟是否激活以进行协调。如果未激活，所有成员 $i \\in g$ 设 $a_i(t) = b_i(t)$。如果激活，则将意图总和 $S_g(t) = \\sum_{i \\in g} b_i(t)$ 与联盟配额 $q_g$ 进行比较。如果 $S_g(t) \\le q_g$，则无需协调，对所有 $i \\in g$ 都有 $a_i(t)=b_i(t)$。然而，如果 $S_g(t) > q_g$，则应用向下调整规则。在 $S_g(t)$ 名意图去酒吧的成员中，精确选择 $q_g$ 名去酒吧。选择标准是首先按其吸引力差异 $\\Delta_i(t) = A_i^{\\text{att}}(t) - A_i^{\\text{stay}}(t)$ 的降序排序，然后按其主体索引 $i$ 的升序排序以打破平局。这 $q_g$ 名主体的最终行动被设为 $a_i(t)=1$。其余 $S_g(t) - q_g$ 名意图去酒吧的成员的行动被“反转”为最终行动 $a_i(t)=0$。最初意图待在家的主体（$b_i(t)=0$）维持其行动 $a_i(t)=0$。\n\n3.  **收益与学习**：一旦所有最终行动确定，计算总出席人数 $A(t) = \\sum_{i=0}^{N-1} a_i(t)$。每个主体 $i$ 如果做出“正确”决策（去一个未拥挤的酒吧，或者因酒吧拥挤而待在家里），将获得收益 $u_i(t)=1$，否则收益为 $u_i(t)=0$。随后，使用指定的 Roth-Erev 学习规则更新吸引力。对于一个选择了行动 $a_i(t) \\in \\{0, 1\\}$ 的主体 $i$，所选行动的吸引力通过 $A_i^{\\text{chosen}}(t+1) = (1-\\rho)A_i^{\\text{chosen}}(t) + \\lambda u_i(t)$ 更新，而未选行动的吸引力通过 $A_i^{\\text{unchosen}}(t+1) = (1-\\rho)A_i^{\\text{unchosen}}(t)$ 更新。此处，$\\rho$ 是新近度参数，$\\lambda$ 是学习步长。\n\n第三，模拟完成后，进行最终分析。所需的度量指标——平均出席人数差距 $\\bar{G}$、过度拥挤频率 $\\bar{O}$ 和平均协调反转份额 $\\bar{F}$——是通过在模拟的最后 $H$ 个时期（从 $t=T-H$ 到 $t=T-1$）对其各自的时间序列求平均值来计算的。该实现使用 NumPy 库提供的向量化操作来提高涉及主体状态数组的计算效率。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(N, C, T, H, rho, lambda_param, p_comm, coalition_sizes, seed):\n    \"\"\"\n    Implements and simulates the described agent-based El Farol Bar model with coalitions.\n    \"\"\"\n    # Initialize the pseudo-random number generator for reproducibility.\n    rng = np.random.default_rng(seed)\n\n    # 1. Initialization\n    # Construct coalition structure: a list of lists of agent indices.\n    coalitions = []\n    # Store quotas for non-singleton coalitions, indexed by coalition index.\n    quotas = {}\n    agent_idx_counter = 0\n    for g_idx, size in enumerate(coalition_sizes):\n        members = list(range(agent_idx_counter, agent_idx_counter + size))\n        coalitions.append(members)\n        if size >= 2:\n            # Per the problem, quota is floor(C * |g| / N).\n            quotas[g_idx] = int(C * size / N)\n        agent_idx_counter += size\n\n    # Agent state variables initialized to 0.\n    attractions = np.zeros(N, dtype=float)\n    stays = np.zeros(N, dtype=float)\n\n    # Data structures for storing tail-end history for final metrics.\n    attendance_history = []\n    flips_history = []\n\n    # 2. Main Simulation Loop\n    for t in range(T):\n        # 2a. Private Intention Phase\n        deltas = attractions - stays\n        intentions = np.zeros(N, dtype=int)\n        intentions[deltas > 0] = 1\n        \n        # Resolve ties with a fair coin flip.\n        tie_indices = np.where(deltas == 0)[0]\n        if len(tie_indices) > 0:\n            intentions[tie_indices] = rng.integers(0, 2, size=len(tie_indices))\n        \n        # Final actions start as intentions; may be modified by coordination.\n        actions = intentions.copy()\n\n        # 2b. Coalition Coordination Phase\n        for g_idx, g_members in enumerate(coalitions):\n            # Singleton coalitions (|g|=1) do not coordinate.\n            if len(g_members) < 2:\n                continue\n\n            # Bernoulli trial for coalition activation.\n            is_active = rng.random() < p_comm\n            if not is_active:\n                continue\n\n            # Active coalition coordination logic.\n            member_intentions = intentions[g_members]\n            S_g = np.sum(member_intentions)\n            q_g = quotas[g_idx]\n\n            if S_g > q_g:\n                # Identify members who intended to attend.\n                intending_member_local_indices = np.where(member_intentions == 1)[0]\n                global_intending_indices = [g_members[i] for i in intending_member_local_indices]\n                \n                # Create a list of candidates to be sorted.\n                candidates = []\n                for agent_idx in global_intending_indices:\n                    candidates.append((deltas[agent_idx], agent_idx))\n                \n                # Sort: 1st key descending delta, 2nd key ascending agent index.\n                candidates.sort(key=lambda x: (-x[0], x[1]))\n                \n                # The agents beyond the quota are 'flipped' to stay home.\n                for _, agent_idx in candidates[q_g:]:\n                    actions[agent_idx] = 0\n\n        # 2c. Payoff and Learning\n        A_t = np.sum(actions)\n        \n        payoffs = np.zeros(N, dtype=float)\n        actions_mask = (actions == 1) # Boolean mask for agents who attended.\n        \n        if A_t <= C:  # Bar not crowded: attenders get payoff 1.\n            payoffs[actions_mask] = 1.0\n        else:  # Bar crowded: stayers get payoff 1.\n            payoffs[~actions_mask] = 1.0\n\n        # Update attractions for t+1 using vectorized operations.\n        # First, apply the recency discount to all.\n        attractions = (1 - rho) * attractions\n        stays = (1 - rho) * stays\n\n        # Then, add the reinforcement term for the chosen action.\n        attractions[actions_mask] += lambda_param * payoffs[actions_mask]\n        stays[~actions_mask] += lambda_param * payoffs[~actions_mask]\n\n        # 2d. Data Recording for Tail Averages\n        if t >= T - H:\n            attendance_history.append(A_t)\n            flips_history.append(np.sum(actions != intentions))\n\n    # 3. Final Metrics Calculation\n    attendance_history = np.array(attendance_history, dtype=float)\n    flips_history = np.array(flips_history, dtype=float)\n\n    mean_gap = np.mean(attendance_history - C)\n    overcrowding_freq = np.mean(attendance_history > C)\n    mean_flip_share = np.mean(flips_history / N)\n\n    return [mean_gap, overcrowding_freq, mean_flip_share]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: All agents are singletons.\n        {'N': 51, 'C': 25, 'T': 2000, 'H': 500, 'rho': 0.01, 'lambda_param': 0.2, \n         'coalition_sizes': [1] * 51, 'p_comm': 0.5, 'seed': 42},\n        # Case 2: Coalitions, full communication.\n        {'N': 51, 'C': 25, 'T': 2000, 'H': 500, 'rho': 0.01, 'lambda_param': 0.2,\n         'coalition_sizes': [3,3,3,3,3,3,3,30], 'p_comm': 1.0, 'seed': 43},\n        # Case 3: Coalitions, no communication.\n        {'N': 51, 'C': 25, 'T': 2000, 'H': 500, 'rho': 0.01, 'lambda_param': 0.2,\n         'coalition_sizes': [3,3,3,3,3,3,3,30], 'p_comm': 0.0, 'seed': 44},\n        # Case 4: Another coalition structure, partial communication.\n        {'N': 51, 'C': 25, 'T': 2000, 'H': 500, 'rho': 0.01, 'lambda_param': 0.2,\n         'coalition_sizes': [5]*10 + [1], 'p_comm': 0.8, 'seed': 45},\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_simulation(**params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```"}, {"introduction": "本练习深入探讨“反公地悲剧”这一引人深思的概念，它揭示了市场机制如何导致效率低下的结果。与资源被过度使用的“公地悲剧”不同，“反公地悲剧”描述了当多个所有者各自拥有排他权时，资源反而因过度分割而被闲置的困境。您将对各个主体为最大化自身收益而独立设定准入价格的行为进行建模，并通过仿真观察这种理性的、但非合作的行为如何导致一个次优的集体均衡，即资源价格过高且利用率不足。这个实践能让您对战略互动和市场失灵获得深刻的见解。[@problem_id:2370580]", "id": "2370580", "problem": "考虑一个基于代理人的环境，该环境捕捉了“反公地悲剧”现象，即多个代理人对单一资源拥有排他权。环境中有 $N$ 个排他权持有者（代理人），索引为 $i \\in \\{1,\\dots,N\\}$。在每个名义周期中，一个代表性用户对该资源的估值为一个随机量 $V$，该值从 $[0,1]$ 上的均匀分布中独立抽取。只有当用户能够支付所有代理人的使用价格总和以及每位代理人的交易摩擦成本时，她才能使用该资源。摩擦成本为非负，记为 $\\kappa \\ge 0$；该成本由用户承担，不归代理人所有。\n\n基本设定：\n- 期望需求等于支付意愿超过总负担的概率。在 $V \\sim \\mathrm{Uniform}[0,1]$ 的假设下，当总负担 $P \\in [0,1]$ 时，需求函数为 $D(P) = \\mathbb{P}(V \\ge P) = \\max(1 - P, 0)$，而当 $P \\ge 1$ 时，$D(P)=0$。\n- 每个代理人设定一个非负的使用价格 $p_i \\ge 0$。设 $P_{\\text{prices}} \\equiv \\sum_{i=1}^N p_i$，总负担 $P_{\\text{total}} \\equiv P_{\\text{prices}} + N\\kappa$。\n\n基于代理人的价格更新规则：\n- 代理人是短视的收益最大化者，他们将其他代理人的当前价格视为给定，并通过松弛最优响应进行更新。在给定其他代理人价格的情况下，代理人 $i$ 选择 $p_i$ 以最大化其期望收益 $p_i \\cdot D\\big(P_{\\text{total}}\\big)$，其中 $P_{\\text{total}} = N\\kappa + p_i + \\sum_{j \\ne i} p_j$。给定松弛参数 $\\beta \\in (0,1]$，更新规则为 $p_i \\leftarrow (1-\\beta)\\,p_i + \\beta \\cdot \\arg\\max_{p \\ge 0}\\; p \\cdot D\\!\\left(N\\kappa + p + \\sum_{j \\ne i} p_j\\right)$。\n- 该过程在代理人之间顺序迭代，直至收敛。\n\n您的任务：\n1. 实现上述基于代理人的学习动态的模拟。所有代理人的初始价格 $p_i$ 均为 0。\n2. 在每次迭代中（对所有代理人的一次完整遍历），使用松弛最优响应规则更新每个代理人的价格一次。\n3. 当一次遍历中所有 $p_i$ 的最大绝对变化小于给定的容差 $\\varepsilon$，或达到最大迭代次数时，停止迭代。\n4. 停止后，计算：\n   - 长期使用率 $u \\equiv D(P_{\\text{total}}) = \\max(1 - P_{\\text{total}}, 0)$。以小数形式（而非百分比）报告 $u$。\n   - 无条件平均实现收益 $R \\equiv u \\cdot P_{\\text{prices}}$（这是代理人收到的总支付在所有周期内的平均值，当没有发生使用时计为零）。\n\n需遵守的数值与算法细节：\n- 使用 $D(P) = \\max(1 - P, 0)$。\n- 在更新规则中，对标量凹问题使用精确最优响应。不要离散化价格空间。\n- 使用收敛容差 $\\varepsilon$ 和最大迭代次数上限 $\\text{max\\_iter}$。\n- 在最终报告的输出中，将 $u$ 和 $R$ 四舍五入至 6 位小数。\n- 如果 $P_{\\text{total}} \\ge 1$，那么根据定义 $u = 0$ 且 $R = 0$。\n\n测试组：\n在以下参数集 $(N,\\kappa,\\beta,\\varepsilon,\\text{max\\_iter})$ 上运行您的程序：\n- 情况 1：$(N=\\;1,\\;\\kappa=\\;0.0,\\;\\beta=\\;1.0,\\;\\varepsilon=\\;10^{-12},\\;\\text{max\\_iter}=\\;10000)$。\n- 情况 2：$(N=\\;2,\\;\\kappa=\\;0.0,\\;\\beta=\\;1.0,\\;\\varepsilon=\\;10^{-12},\\;\\text{max\\_iter}=\\;10000)$。\n- 情况 3：$(N=\\;5,\\;\\kappa=\\;0.05,\\;\\beta=\\;1.0,\\;\\varepsilon=\\;10^{-12},\\;\\text{max\\_iter}=\\;10000)$。\n- 情况 4：$(N=\\;10,\\;\\kappa=\\;0.10,\\;\\beta=\\;1.0,\\;\\varepsilon=\\;10^{-12},\\;\\text{max\\_iter}=\\;10000)$。\n- 情况 5：$(N=\\;12,\\;\\kappa=\\;0.10,\\;\\beta=\\;1.0,\\;\\varepsilon=\\;10^{-12},\\;\\text{max\\_iter}=\\;10000)$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含五个测试情况的结果。结果为一个用方括号括起来的逗号分隔列表，每个情况的结果本身是一个两元素列表 $[u,R]$，两个条目都四舍五入到 6 位小数。例如，整体格式必须如下所示：\n“[[u_1,R_1],[u_2,R_2],[u_3,R_3],[u_4,R_4],[u_5,R_5]]”\n前后不含任何额外文本。", "solution": "所呈现的问题是基于代理人的计算经济学中一个定义明确的练习，具体模拟了“反公地悲剧”。该模型具有科学依据且内部一致，允许直接的计算求解。我们将首先推导代理人的最优行为，然后描述用于寻找系统均衡的模拟算法。\n\n问题的核心在于确定每个代理人的最优响应函数。代理人 $i$ 在给定所有其他代理人价格 $\\mathbf{p}_{-i} = \\{p_j\\}_{j \\ne i}$ 的情况下，寻求选择其价格 $p_i \\ge 0$ 以最大化其自身期望收益 $\\pi_i$。代理人是短视的，意味着它不考虑其他代理人对其当前价格选择的未来反应。\n\n代理人的收益是其价格与用户购买使用权的概率的乘积。用户的总成本或总负担是所有价格的总和加上总交易摩擦：$P_{\\text{total}} = \\sum_{j=1}^N p_j + N\\kappa$。设 $S_{-i} = \\sum_{j \\ne i} p_j$ 为所有其他代理人设定的价格总和。那么从代理人 $i$ 的角度来看，总负担可以写为 $P_{\\text{total}} = p_i + S_{-i} + N\\kappa$。\n\n需求函数由用户的估值 $V$（从 $\\mathrm{Uniform}[0,1]$ 分布中抽取）超过总负担的概率给出：$D(P_{\\text{total}}) = \\mathbb{P}(V \\ge P_{\\text{total}}) = \\max(0, 1 - P_{\\text{total}})$。\n\n因此，代理人 $i$ 的优化问题是：\n$$ \\max_{p_i \\ge 0} \\pi_i(p_i) = p_i \\cdot D(p_i + S_{-i} + N\\kappa) $$\n$$ \\max_{p_i \\ge 0} \\pi_i(p_i) = p_i \\cdot \\max(0, 1 - (p_i + S_{-i} + N\\kappa)) $$\n\n让我们来分析这个目标函数。如果成本的固定部分 $S_{-i} + N\\kappa$ 大于或等于 $1$，那么对于任何 $p_i > 0$，总负担 $P_{\\text{total}}$ 将严格大于 $1$。这导致需求为零，$D(P_{\\text{total}}) = 0$，从而收益也为零，$\\pi_i = 0$。如果 $p_i = 0$，收益也为 $0$。因此，如果 $S_{-i} + N\\kappa \\ge 1$，任何非负价格 $p_i$ 都产生零收益，最优响应就是 $p_i = 0$。\n\n现在，考虑 $S_{-i} + N\\kappa < 1$ 的情况。为了使需求为正，必须有 $p_i + S_{-i} + N\\kappa < 1$，这意味着 $p_i < 1 - S_{-i} - N\\kappa$。在此范围内，目标函数为：\n$$ \\pi_i(p_i) = p_i (1 - S_{-i} - N\\kappa - p_i) $$\n这是关于 $p_i$ 的二次函数，代表一个开口向下的抛物线：$\\pi_i(p_i) = -p_i^2 + (1 - S_{-i} - N\\kappa)p_i$。该函数的最大值点可以通过将其关于 $p_i$ 的导数设为零来找到：\n$$ \\frac{\\partial \\pi_i}{\\partial p_i} = 1 - S_{-i} - N\\kappa - 2p_i = 0 $$\n解出 $p_i$ 得到无约束最优价格：\n$$ p_i^* = \\frac{1 - S_{-i} - N\\kappa}{2} $$\n由于我们处于 $1 - S_{-i} - N\\kappa > 0$ 的情况，这个价格 $p_i^*$ 是正的。只要这个无约束最大值满足非负约束 $p_i \\ge 0$，它就是解，而它确实满足。\n\n综合两种情况，代理人 $i$ 在给定其他代理人价格时的最优价格（即最优响应函数 $\\mathrm{BR}_i(\\mathbf{p}_{-i})$）是：\n$$ \\mathrm{BR}_i(\\mathbf{p}_{-i}) = \\max\\left(0, \\frac{1 - N\\kappa - \\sum_{j \\ne i} p_j}{2}\\right) $$\n该表达式提供了问题陈述所要求的精确最优响应。\n\n该模拟实现了一个迭代过程，以找到这个定价博弈的纳什均衡。在任何时候，系统的状态是价格向量 $\\mathbf{p} = (p_1, \\dots, p_N)$。该过程从所有 $i \\in \\{1, \\dots, N\\}$ 的初始价格 $p_i = 0$ 开始。\n\n在每次迭代或遍历中，代理人顺序更新其价格。对于代理人 $i$，新价格 $p_i'$ 使用松弛最优响应规则计算：\n$$ p_i' = (1-\\beta)p_i + \\beta \\cdot \\mathrm{BR}_i(\\mathbf{p}_{-i}) $$\n这里，$p_i$ 是该代理人上一轮迭代的价格，而 $\\mathbf{p}_{-i}$ 包含代理人 $j<i$ 的最新更新价格和代理人 $j>i$ 的上一轮迭代价格。这是一种 Gauss-Seidel 类型的更新方案。松弛参数 $\\beta \\in (0,1]$ 控制朝向最优响应的步长。当 $\\beta = 1$ 时，代理人完全采用新的最优响应价格。\n\n模拟最多进行 $\\text{max\\_iter}$ 次迭代。如果价格收敛，则会提前终止。当一次完整遍历中任何代理人价格的最大绝对变化小于指定的容差 $\\varepsilon$ 时，即达到收敛：\n$$ \\max_{i \\in \\{1,\\dots,N\\}} |p_i' - p_i| < \\varepsilon $$\n\n终止时，设最终价格向量为 $\\mathbf{p}^{\\text{final}}$。聚合指标按以下方式计算：\n1.  最终价格总和为 $P_{\\text{prices}} = \\sum_{i=1}^N p_i^{\\text{final}}$。\n2.  最终总负担为 $P_{\\text{total}} = P_{\\text{prices}} + N\\kappa$。\n3.  长期使用率为 $u = D(P_{\\text{total}}) = \\max(0, 1 - P_{\\text{total}})$。\n4.  无条件平均实现收益为 $R = u \\cdot P_{\\text{prices}}$。\n\n然后，按要求将 $u$ 和 $R$ 的结果四舍五入到 6 位小数。所提供的代码为每个指定的测试用例实现了这一逻辑。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef simulate(N, kappa, beta, epsilon, max_iter):\n    \"\"\"\n    Simulates the agent-based price setting in an anti-commons environment.\n\n    Args:\n        N (int): Number of agents.\n        kappa (float): Per-agent transaction friction cost.\n        beta (float): Relaxation parameter for price updates.\n        epsilon (float): Convergence tolerance.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        tuple: A tuple containing the utilization rate (u) and total revenue (R).\n    \"\"\"\n    # Initialize prices p_i = 0 for all i.\n    prices = np.zeros(N, dtype=np.float64)\n    \n    for iteration in range(max_iter):\n        old_prices = prices.copy()\n        max_change = 0.0\n        \n        # Sequentially update each agent's price.\n        for i in range(N):\n            # Sum of other agents' prices. This uses updated prices for j < i\n            # and old prices for j > i, which is a Gauss-Seidel update.\n            S_minus_i = np.sum(prices) - prices[i]\n            \n            # Calculate the best response for agent i.\n            numerator = 1.0 - N * kappa - S_minus_i\n            best_response = max(0.0, numerator / 2.0)\n            \n            # Apply the relaxed best-response update rule.\n            new_price = (1.0 - beta) * prices[i] + beta * best_response\n            \n            # Update the agent's price in the vector for the current iteration.\n            prices[i] = new_price\n            \n        # Check for convergence after a full sweep over all agents.\n        # The problem asks for checking convergence based on changes within a sweep.\n        # old_prices store the state before the sweep, prices store state after.\n        max_change = np.max(np.abs(prices - old_prices))\n        \n        if max_change < epsilon:\n            break\n            \n    # Calculate final metrics after convergence or max iterations.\n    P_prices = np.sum(prices)\n    P_total = P_prices + N * kappa\n    \n    # Utilization rate u\n    u = max(0.0, 1.0 - P_total)\n    \n    # Mean realized revenue R\n    R = u * P_prices\n    \n    return u, R\n\ndef solve():\n    \"\"\"\n    Runs the simulation for all test cases and prints the results in the required format.\n    \"\"\"\n    # Test suite: (N, kappa, beta, epsilon, max_iter)\n    test_cases = [\n        (1, 0.0, 1.0, 1e-12, 10000),   # Case 1\n        (2, 0.0, 1.0, 1e-12, 10000),   # Case 2\n        (5, 0.05, 1.0, 1e-12, 10000),  # Case 3\n        (10, 0.10, 1.0, 1e-12, 10000), # Case 4\n        (12, 0.10, 1.0, 1e-12, 10000)  # Case 5\n    ]\n\n    all_results = []\n    for case in test_cases:\n        N, kappa, beta, epsilon, max_iter = case\n        u, R = simulate(N, kappa, beta, epsilon, max_iter)\n        \n        # Round final results to 6 decimal places.\n        u_rounded = round(u, 6)\n        R_rounded = round(R, 6)\n        \n        all_results.append(f\"[{u_rounded},{R_rounded}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n\n```"}, {"introduction": "本实践要求您构建一个复杂的预测市场模型，这是一种用于汇集分散信息的强大工具。您将使用对数市场评分规则（LMSR）——一种被广泛应用的自动化做市商——来搭建市场，其中贝叶斯主体根据其私有信号进行交易。这个练习的核心是通过引入蓄意传播虚假信息的主体，来探索市场的弹性和脆弱性，这是一个与当今世界高度相关的问题。这次仿真将加深您对贝叶斯学习、先进市场设计以及金融环境中信息流动动态的理解。[@problem_id:2370584]", "id": "2370584", "problem": "要求您设计并实现一个基于智能体的计算经济学 (Agent-Based Computational Economics, ACE) 模拟。该模拟针对一个采用对数市场评分规则 (Logarithmic Market Scoring Rule, LMSR) 做市商的二元结果预测市场。一个连续统的短视智能体依序到达。每个智能体观察一个关于事件的私有二元信号，并在单次交易预算约束下与做市商进行交易。每个智能体有固定的概率通过在交易前反转其私有信号来故意传播错误信息。\n\n基本原理：\n- 基于智能体的计算经济学 (ACE) 中的智能体和信号：事件结果是一个二元随机变量 $Y \\in \\{0,1\\}$，其真实概率为 $\\theta = \\mathbb{P}(Y=1)$，该概率对智能体是未知的。在给定 $Y$ 的条件下，每个智能体观察到一个独立的私有信号 $s \\in \\{0,1\\}$，满足 $\\mathbb{P}(s=1 \\mid Y=1) = \\alpha$ 和 $\\mathbb{P}(s=1 \\mid Y=0) = 1-\\alpha$，其中 $\\alpha \\in (0,1)$ 是一个已知的信号准确度参数。在给定 $Y$ 的条件下，各智能体的信号是条件独立的。\n- 贝叶斯更新：如果先验信念为 $p = \\mathbb{P}(Y=1)$，那么在观察到 $s \\in \\{0,1\\}$ 后，后验信念 $b$ 由 Bayes 法则给出，\n  - 如果 $s=1$：$b = \\dfrac{\\alpha p}{\\alpha p + (1-\\alpha)(1-p)}$。\n  - 如果 $s=0$：$b = \\dfrac{(1-\\alpha) p}{(1-\\alpha) p + \\alpha (1-p)}$。\n- 对数市场评分规则 (LMSR)：做市商维护一个份额向量 $\\mathbf{q} = (q_0,q_1)$ 和一个流动性参数 $b_L > 0$。成本函数为\n  $$C(\\mathbf{q}) = b_L \\log\\left(\\exp\\left(\\frac{q_0}{b_L}\\right) + \\exp\\left(\\frac{q_1}{b_L}\\right)\\right)。$$\n  结果 1 的瞬时价格是梯度分量，\n  $$p = \\frac{\\partial C}{\\partial q_1} = \\frac{\\exp\\left(\\frac{q_1}{b_L}\\right)}{\\exp\\left(\\frac{q_0}{b_L}\\right) + \\exp\\left(\\frac{q_1}{b_L}\\right)}。$$\n  在二元情况下，对数优势比满足\n  $$\\log\\left(\\frac{p}{1-p}\\right) = \\frac{q_1 - q_0}{b_L}。$$\n  通过只购买“是”（结果 1）的份额将价格从 $p_{\\text{old}}$ 移动到 $p_{\\text{new}}$，这对应于将 $q_1$ 增加 $\\Delta = b_L\\left(\\log\\left(\\frac{p_{\\text{new}}}{1-p_{\\text{new}}}\\right) - \\log\\left(\\frac{p_{\\text{old}}}{1-p_{\\text{old}}}\\right)\\right)$。当 $p_{\\text{new}} &lt; p_{\\text{old}}$ 时，只购买“否”（结果 0）的份额对应于将 $q_0$ 增加一个类似的正常数。任何此类交易的货币成本是成本函数的差值，$\\Delta C = C(\\mathbf{q}_{\\text{new}}) - C(\\mathbf{q}_{\\text{old}})$。\n\n模拟环境：\n- 初始化：LMSR 以价格 $p_0 = \\pi_0 \\in (0,1)$ 开始。这可以通过将初始份额向量设置为任何满足 $(q_{1,0}-q_{0,0})/b_L = \\log\\left(\\frac{\\pi_0}{1-\\pi_0}\\right)$ 的 $\\mathbf{q}_0$ 来实现。为具体起见，可以取 $q_{0,0} = 0$ 和 $q_{1,0} = b_L \\log\\left(\\frac{\\pi_0}{1-\\pi_0}\\right)$。\n- 到达与行为：一个包含 $T$ 个智能体的序列到达。在交易前，每个智能体观察当前市场价格 $p$，并将其视为先验概率 $\\mathbb{P}(Y=1)$。然后，智能体观察一个从上述信号过程中抽取的私有信号 $s \\in \\{0,1\\}$。智能体有 $p_{\\text{fake}} \\in [0,1]$ 的概率通过反转信号来故意传播错误信息，即使用 $s' = 1-s$ 进行贝叶斯更新。否则，他们使用真实信号 $s' = s$。智能体的后验信念 $b$ 通过 $p$ 和 $s'$ 使用 Bayes 法则计算得出。\n- 带预算的短视交易：每个智能体都是风险中性和短视的。他们试图通过与 LMSR 做市商交易，将市场价格 $p$ 推向其信念 $b$，但他们面临一个严格的单次交易预算 $w &gt; 0$。智能体选择向 $b$ 方向上最大的可行移动，使得支付的 LMSR 成本不超过 $w$。如果将价格完全移动到 $b$ 的成本小于或等于 $w$，智能体将价格精确移动到 $b$。否则，智能体部分地向 $b$ 移动，选择 $p$ 和 $b$ 之间的直线上唯一的价格，使得 LMSR 成本差等于 $w$。您必须通过求解一维路径（当 $b \\ge p$ 时只购买“是”，当 $b &lt; p$ 时只购买“否”）并（如果需要）对成本差使用数值求根方法，来精确计算此部分移动。\n- 实现与评估：真实结果 $Y$ 在开始时从 $\\text{Bernoulli}(\\theta)$ 分布中抽取一次，并只在最后揭晓。评估指标是最终市场概率的 Brier 分数，定义为 $(p_T - Y)^2$，以小数值表示。\n\n随机性与可复现性：\n- 所有随机性必须使用伪随机数生成器生成，每个测试用例使用固定的种子以确保可复现性。结果 $Y$ 和所有智能体信号都应使用该种子化的生成器抽取。没有其他随机性来源。\n\n数值细节与约束：\n- 对 LMSR 成本和价格使用稳定的数值评估方法（例如，使用稳定的 log-sum-exp 计算）。将任何概率参数限制在 $(\\varepsilon,1-\\varepsilon)$ 范围内，其中 $\\varepsilon = 10^{-12}$，以避免除以零或对零取对数。角度单位不适用。此问题中没有物理单位。所有概率和准确度参数必须以 $[0,1]$ 范围内的十进制数给出。\n- 输出格式：对于每个测试用例，在最终输出行上生成两个浮点数：最终价格 $p_T$ 和 Brier 分数 $(p_T - Y)^2$，每个都四舍五入到 $6$ 位小数。将所有测试用例的结果汇总到一个单行的扁平化 Python 风格列表中，例如 $[p_1,\\text{Brier}_1,p_2,\\text{Brier}_2,\\dots]$。\n\n测试套件：\n请实现您的程序，以完全按以下顺序运行四个测试用例。对于每个用例，请使用提供的种子初始化伪随机数生成器，并按规定格式打印结果。\n\n- 案例 A（正常路径，无错误信息）：\n  - 种子：$123$\n  - 智能体数量：$T = 200$\n  - 流动性：$b_L = 10.0$\n  - 单个智能体预算：$w = 1.0$\n  - 信号准确度：$\\alpha = 0.7$\n  - 错误信息概率：$p_{\\text{fake}} = 0.0$\n  - 真实事件概率：$\\theta = 0.6$\n  - 初始先验价格：$\\pi_0 = 0.5$\n\n- 案例 B（混合行为）：\n  - 种子：$456$\n  - 智能体数量：$T = 200$\n  - 流动性：$b_L = 10.0$\n  - 单个智能体预算：$w = 1.0$\n  - 信号准确度：$\\alpha = 0.7$\n  - 错误信息概率：$p_{\\text{fake}} = 0.5$\n  - 真实事件概率：$\\theta = 0.6$\n  - 初始先验价格：$\\pi_0 = 0.5$\n\n- 案例 C（所有智能体都传播错误信息）：\n  - 种子：$789$\n  - 智能体数量：$T = 200$\n  - 流动性：$b_L = 10.0$\n  - 单个智能体预算：$w = 1.0$\n  - 信号准确度：$\\alpha = 0.7$\n  - 错误信息概率：$p_{\\text{fake}} = 1.0$\n  - 真实事件概率：$\\theta = 0.6$\n  - 初始先验价格：$\\pi_0 = 0.5$\n\n- 案例 D（无信息信号边界情况）：\n  - 种子：$321$\n  - 智能体数量：$T = 200$\n  - 流动性：$b_L = 10.0$\n  - 单个智能体预算：$w = 1.0$\n  - 信号准确度：$\\alpha = 0.5$\n  - 错误信息概率：$p_{\\text{fake}} = 0.7$\n  - 真实事件概率：$\\theta = 0.6$\n  - 初始先验价格：$\\pi_0 = 0.5$\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果（例如 $[0.512345,0.237890,0.498765,0.251234,0.476543,0.226789,0.500000,0.250000]$）。将每个浮点数四舍五入到 $6$ 位小数。不应打印其他任何文本。", "solution": "该问题要求实现一个关于二元预测市场的基于智能体的计算经济学（ACE）模拟。该市场由一个对数市场评分规则（LMSR）自动化做市商运营。一系列智能体与该市场进行交互。该问题在科学上是适定 (well-posed) 的，所有参数、智能体行为和市场机制都有明确定义。它构成了计算金融和经济学中的一个标准模拟任务。我将首先详细阐述数学模型，然后概述模拟算法，以此来构建解决方案。\n\nLMSR 市场的状态由两种互斥结果 $Y=0$ 和 $Y=1$ 的已发行份额数量 $\\mathbf{q} = (q_0, q_1)$ 定义。结果 1 的市场价格（表示为 $p$）由 LMSR 成本函数 $C(\\mathbf{q})$ 的梯度导出。二元结果 LMSR 的一个关键特性是，价格仅是份额数量之差 $q_1 - q_0$ 和流动性参数 $b_L$ 的函数：\n$$ \\log\\left(\\frac{p}{1-p}\\right) = \\frac{q_1 - q_0}{b_L} $$\n这意味着就价格而言，市场的整个状态可以用单个变量，即价格 $p$ 本身来捕捉。我们不需要单独跟踪 $q_0$ 和 $q_1$。模拟从初始价格 $p_0 = \\pi_0$ 开始。\n\n一个智能体与市场的交互是一个两步过程：信念形成和交易。\n首先，一个在市场价格为 $p_k$ 时到达的智能体将此价格视为其先验概率，$\\mathbb{P}(Y=1) = p_k$。他们观察一个私有信号 $s \\in \\{0,1\\}$，该信号与真实（但未知）的结果 $Y$ 相关。信号的准确度由 $\\alpha = \\mathbb{P}(s=1 \\mid Y=1) = \\mathbb{P}(s=0 \\mid Y=0)$ 给出。以指定的概率 $p_{\\text{fake}}$，该智能体作为错误信息传播者，使用反转的信号 $s' = 1-s$ 进行其信念更新。否则，使用真实信号 $s' = s$。智能体使用 $s'$ 通过 Bayes 法则计算其后验信念 $b$：\n- 如果 $s'=1$：$b = \\dfrac{\\alpha p_k}{\\alpha p_k + (1-\\alpha)(1-p_k)}$\n- 如果 $s'=0$：$b = \\dfrac{(1-\\alpha) p_k}{(1-\\alpha) p_k + \\alpha(1-p_k)}$\n\n其次，短视、风险中性的智能体通过交易将市场价格从 $p_k$ 移动到其后验信念 $b$。该交易受到预算约束 $w$ 的限制。将价格从 $p_{\\text{old}}$ 改变为 $p_{\\text{new}}$ 的交易成本可以从 LMSR 成本函数 $C(\\mathbf{q}) = b_L \\log\\left(\\exp(q_0/b_L) + \\exp(q_1/b_L)\\right)$ 导出。\n一个增加价格的交易（即 $b > p_k$）涉及购买结果 1 的份额，这只改变 $q_1$。将价格从 $p_k$ 移动到 $p_{\\text{new}}$ 的成本是 $\\Delta C = b_L (\\log(1-p_k) - \\log(1-p_{\\text{new}}))$。\n一个降低价格的交易（即 $b < p_k$）涉及购买结果 0 的份额，这只改变 $q_0$。成本是 $\\Delta C = b_L (\\log(p_k) - \\log(p_{\\text{new}}))$。\n这些以价格表示的简化成本公式，避免了跟踪 $q_0$ 和 $q_1$ 的需要，并且在价格远离 0 和 1 的情况下是数值稳定的。\n\n智能体希望将价格移动到 $b$。\n如果 $b > p_k$，完全移动到 $b$ 的成本是 $\\Delta C_{\\text{full}} = b_L (\\log(1-p_k) - \\log(1-b))$。\n- 如果 $\\Delta C_{\\text{full}} \\le w$，智能体进行完整交易，新价格为 $p_{k+1} = b$。\n- 如果 $\\Delta C_{\\text{full}} > w$，智能体花费其全部预算 $w$。我们必须找到新价格 $p_{k+1}$ 使得 $w = b_L (\\log(1-p_k) - \\log(1-p_{k+1}))$ 成立。这个方程可以解析地解出 $p_{k+1}$：\n  $$ p_{k+1} = 1 - (1-p_k) \\exp\\left(-\\frac{w}{b_L}\\right) $$\n如果 $b < p_k$，完全移动到 $b$ 的成本是 $\\Delta C_{\\text{full}} = b_L (\\log(p_k) - \\log(b))$。\n- 如果 $\\Delta C_{\\text{full}} \\le w$，新价格为 $p_{k+1} = b$。\n- 如果 $\\Delta C_{\\text{full}} > w$，我们求解 $w = b_L (\\log(p_k) - \\log(p_{k+1}))$。解析解为：\n  $$ p_{k+1} = p_k \\exp\\left(-\\frac{w}{b_L}\\right) $$\n与问题描述的建议相反，由于存在针对预算约束交易的这些精确解析解，数值求根不是必需的。如果 $b=p_k$，则不发生交易，且 $p_{k+1} = p_k$。\n\n模拟按以下步骤进行：\n1. 使用给定的种子初始化一个伪随机数生成器 (RNG)。\n2. 通过从参数为 $\\theta$ 的伯努利分布中抽样，确定真实结果 $Y \\in \\{0, 1\\}$。\n3. 初始化市场价格 $p = \\pi_0$。\n4. 对于 $T$ 个智能体中的每一个：\n    a. 设当前价格为 $p_k = p$。为保证数值稳定性，将 $p_k$ 限制在 $[\\varepsilon, 1-\\varepsilon]$ 范围内，其中 $\\varepsilon=10^{-12}$。\n    b. 抽取一个随机数以决定智能体是否传播错误信息（基于 $p_{\\text{fake}}$）。\n    c. 根据给定的 $Y$ 和 $\\alpha$，从条件分布中抽取智能体的私有信号 $s \\in \\{0,1\\}$。\n    d. 根据是否传播错误信息的决定，确定要使用的信号 $s'$。\n    e. 使用先验 $p_k$ 和信号 $s'$，通过 Bayes 法则计算智能体的后验信念 $b$。将 $b$ 限制在 $[\\varepsilon, 1-\\varepsilon]$ 范围内。\n    f. 使用上面推导的带预算约束的交易逻辑计算新的市场价格 $p_{k+1}$。\n    g. 更新市场价格：$p = p_{k+1}$。\n5. 在所有 $T$ 个智能体交易之后，最终价格为 $p_T = p$。模拟的性能通过 Brier 分数 $(p_T - Y)^2$ 来衡量。\n\n对于给定的种子和参数集，此过程是确定性的，从而可以获得可复现的结果。实现将遵循此逻辑来处理每个指定的测试用例。一个值得注意的特殊情况是当 $\\alpha=0.5$ 时；此时信号不提供任何信息，因此智能体的后验信念 $b$ 将始终等于其先验 $p_k$，导致没有交易发生，市场价格保持不变。", "answer": "```python\nimport numpy as np\nfrom math import log, exp\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    test_cases = [\n        # Case A (happy path, no misinformation)\n        {\"seed\": 123, \"T\": 200, \"bL\": 10.0, \"w\": 1.0, \"alpha\": 0.7, \"p_fake\": 0.0, \"theta\": 0.6, \"pi_0\": 0.5},\n        # Case B (mixed behavior)\n        {\"seed\": 456, \"T\": 200, \"bL\": 10.0, \"w\": 1.0, \"alpha\": 0.7, \"p_fake\": 0.5, \"theta\": 0.6, \"pi_0\": 0.5},\n        # Case C (all agents misinform)\n        {\"seed\": 789, \"T\": 200, \"bL\": 10.0, \"w\": 1.0, \"alpha\": 0.7, \"p_fake\": 1.0, \"theta\": 0.6, \"pi_0\": 0.5},\n        # Case D (uninformative signals boundary)\n        {\"seed\": 321, \"T\": 200, \"bL\": 10.0, \"w\": 1.0, \"alpha\": 0.5, \"p_fake\": 0.7, \"theta\": 0.6, \"pi_0\": 0.5},\n    ]\n\n    results = []\n    for params in test_cases:\n        p_final, brier_score = run_simulation(**params)\n        results.append(f\"{p_final:.6f}\")\n        results.append(f\"{brier_score:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef run_simulation(seed, T, bL, w, alpha, p_fake, theta, pi_0):\n    \"\"\"\n    Runs a single ACE simulation for a given set of parameters.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    eps = 1e-12\n\n    # Step 1: Draw the true outcome Y for the event\n    Y = 1 if rng.random() < theta else 0\n\n    # Step 2: Initialize market price\n    p = pi_0\n\n    # Step 3: Sequentially process T agents\n    for _ in range(T):\n        p_current = np.clip(p, eps, 1 - eps)\n\n        # Agent behavior:\n        # 1. Observe signal s based on true outcome Y\n        if Y == 1:\n            signal_s = 1 if rng.random() < alpha else 0\n        else: # Y == 0\n            signal_s = 1 if rng.random() < (1 - alpha) else 0\n\n        # 2. Decide whether to misinform\n        is_misinforming = rng.random() < p_fake\n        signal_s_prime = 1 - signal_s if is_misinforming else signal_s\n\n        # 3. Form posterior belief 'b' using Bayes' rule\n        # The agent uses the current market price p_current as their prior\n        if signal_s_prime == 1:\n            numerator = alpha * p_current\n            denominator = alpha * p_current + (1 - alpha) * (1 - p_current)\n        else: # signal_s_prime == 0\n            numerator = (1 - alpha) * p_current\n            denominator = (1 - alpha) * p_current + alpha * (1 - p_current)\n        \n        # If denominator is zero (due to extreme alpha/p), belief doesn't change\n        b = numerator / denominator if denominator > 0 else p_current\n        b = np.clip(b, eps, 1 - eps)\n\n        # Myopic trade logic with budget constraint w:\n        p_next = p_current \n        if b > p_current:\n            # Agent buys \"YES\" shares, wants to move price up to b\n            cost_full_move = bL * (log(1 - p_current) - log(1 - b))\n            if cost_full_move <= w:\n                p_next = b\n            else:\n                # Spend full budget w\n                p_next = 1 - (1 - p_current) * exp(-w / bL)\n        elif b < p_current:\n            # Agent buys \"NO\" shares, wants to move price down to b\n            cost_full_move = bL * (log(p_current) - log(b))\n            if cost_full_move <= w:\n                p_next = b\n            else:\n                # Spend full budget w\n                p_next = p_current * exp(-w / bL)\n        \n        # Update market price\n        p = p_next\n\n    # Step 4: Calculate final price and Brier score\n    p_final = p\n    brier_score = (p_final - Y)**2\n    \n    return p_final, brier_score\n\nif __name__ == '__main__':\n    solve()\n```"}]}