{"hands_on_practices": [{"introduction": "本章的第一个练习将指导你完成稀疏网格插值的基础实现。你将近似一个光滑且具有强交互作用的五维函数，这是稀疏网格在克服“维度灾难”方面展现其强大能力的理想场景。通过从头开始实现该算法 [@problem_id:2399817]，你将对组合技术有更实际的理解，并亲眼见证随着网格层级的增加，近似误差是如何减小的。", "id": "2399817", "problem": "设 $d = 5$，并考虑紧致超立方体域 $[0,1]^5$。定义光滑、高度非可加的函数 $f : [0,1]^5 \\to \\mathbb{R}$ 为\n$$\nf(x_1,x_2,x_3,x_4,x_5) \\;=\\; \\exp\\!\\big(0.5\\,x_1 x_2 + 0.25\\,x_3\\big)\\,\\cos\\!\\Big(\\pi\\big(x_1 x_3 + 2.0\\,x_2 x_4\\big)\\Big)\\;+\\;\\sin\\!\\Big(2\\pi\\big(x_4 x_5 + 0.1\\,x_1\\big)\\Big)\\;+\\;0.05\\,x_1 x_2 x_3 x_4 x_5,\n$$\n其中所有三角函数参数均以弧度为单位。\n\n您的任务是使用 $[0,1]^5$ 上的五维稀疏网格插值，在三个近似级别 $\\ell \\in \\{1,2,3\\}$ 上逼近 $f$，并在一个固定的评估集上量化逼近质量。对于每个级别 $\\ell$，计算插值相对于 $f$ 在以下 15 个测试点上的最大绝对逐点误差：\n$$\n\\begin{aligned}\n&(\\,0.5,\\,0.5,\\,0.5,\\,0.5,\\,0.5\\,),\\quad (\\,0,\\,0,\\,0,\\,0,\\,0\\,),\\quad (\\,1,\\,1,\\,1,\\,1,\\,1\\,),\\quad (\\,0,\\,1,\\,0,\\,1,\\,0\\,),\\quad (\\,1,\\,0,\\,1,\\,0,\\,1\\,),\\\\\n&(\\,0.25,\\,0.75,\\,0.33,\\,0.67,\\,0.5\\,),\\quad (\\,0.9,\\,0.1,\\,0.2,\\,0.8,\\,0.3\\,),\\quad (\\,0.1,\\,0.9,\\,0.8,\\,0.2,\\,0.7\\,),\\\\\n&(\\,0.3,\\,0.3,\\,0.7,\\,0.7,\\,0.2\\,),\\quad (\\,0.6,\\,0.4,\\,0.2,\\,0.9,\\,0.1\\,),\\quad (\\,0.15,\\,0.85,\\,0.55,\\,0.45,\\,0.35\\,),\\\\\n&(\\,0.8,\\,0.2,\\,0.6,\\,0.4,\\,0.5\\,),\\quad (\\,0.2,\\,0.8,\\,0.4,\\,0.6,\\,0.9\\,),\\quad (\\,0.05,\\,0.95,\\,0.25,\\,0.75,\\,0.35\\,),\\quad (\\,0.4,\\,0.4,\\,0.4,\\,0.4,\\,0.4\\,).\n\\end{aligned}\n$$\n\n测试套件包含三个级别 $\\ell \\in \\{1,2,3\\}$。对于每个 $\\ell$，所需的答案是一个实数，等于上述测试集上的最大绝对误差。您的程序必须生成单行输出，其中包含这三个结果，形式为方括号内用逗号分隔的列表，顺序为 $\\ell = 1, \\ell = 2, \\ell = 3$，每个实数都四舍五入到 $6$ 位小数。例如，所需格式的输出为\n$$\n[\\;e_1,\\,e_2,\\,e_3\\;],\n$$\n其中 $e_\\ell$ 表示在级别 $\\ell$ 处四舍五入到 $6$ 位小数的最大绝对误差。\n\n程序没有外部输入，也不涉及物理单位。三角函数中的所有角度均以弧度为单位。最终输出必须是指定格式的单行文本。", "solution": "问题陈述经过验证，确认有效。它在数值分析这一成熟领域，特别是在高维函数逼近方面，具有科学依据。该问题是适定的、客观的，并提供了构建唯一可验证解所需的所有信息。任务是实现一种标准的数值算法——稀疏网格插值方法，并评估其性能。\n\n求解过程如下。任务的核心是使用稀疏网格插值（记为 $\\mathcal{A}_{\\ell,d}f$）在近似级别 $\\ell \\in \\{1, 2, 3\\}$ 上逼近给定的一个 $d=5$ 维函数 $f: [0,1]^5 \\to \\mathbb{R}$。然后，通过计算在预先指定的 15 个测试点集合上的最大绝对误差来评估该插值的精度。\n\n稀疏网格插值是使用组合技术构建的，该技术结合了多个在各向异性网格上建立的张量积插值。这种方法通过审慎选择对逼近精度贡献最大的网格配置，来缓解维度灾难。对于在 $d$ 维域上定义的函数 $f$，级别 $\\ell$ 的稀疏网格插值由以下公式给出：\n$$\n\\mathcal{A}_{\\ell,d}f(x) = \\sum_{q=0}^{\\min(\\ell, d-1)} (-1)^q \\binom{d-1}{q} \\sum_{|\\mathbf{i}|_1 = \\ell-q} \\mathcal{I}^{\\mathbf{i}}f(x)\n$$\n其中：\n- $\\mathbf{i} = (i_1, i_2, \\dots, i_d)$ 是一个非负整数的多重索引，表示每个维度中一维插值的级别。\n- $|\\mathbf{i}|_1 = i_1 + i_2 + \\dots + i_d$ 是级别之和。\n- $\\mathcal{I}^{\\mathbf{i}}f(x)$ 是由级别为 $i_1, \\dots, i_d$ 的一维插值构建的张量积插值。\n- $\\binom{n}{k}$ 是二项式系数。\n\n$\\mathcal{I}^{\\mathbf{i}}f(x)$ 的构建依赖于一维插值规则的选择。由于其卓越的稳定性和精度特性，我们使用基于 Clenshaw-Curtis 节点的 Lagrange 插值。在区间 $[0,1]$ 上，级别为 $i \\ge 0$ 的一维 Clenshaw-Curtis 网格定义如下：\n- 对于级别 $i=0$：$m_0=1$ 个点，位于 $x_0=0.5$。\n- 对于级别 $i > 0$：$m_i=2^i+1$ 个点，由 $x_k = \\frac{1}{2}\\left(1 - \\cos\\left(\\frac{k\\pi}{m_i-1}\\right)\\right)$ 给出，其中 $k=0, 1, \\dots, m_i-1$。\n\n对于点 $x=(x_1, \\dots, x_d)$，张量积插值 $\\mathcal{I}^{\\mathbf{i}}f(x)$ 计算如下：\n$$\n\\mathcal{I}^{\\mathbf{i}}f(x) = \\sum_{p \\in \\mathcal{G}_{\\mathbf{i}}} f(p) \\prod_{j=1}^{d} L_{i_j, p_j}(x_j)\n$$\n在此，$\\mathcal{G}_{\\mathbf{i}} = X_{i_1} \\times X_{i_2} \\times \\dots \\times X_{i_d}$ 是张量积网格，其中 $X_{i_j}$ 是级别为 $i_j$ 的一维节点集。项 $L_{i_j, p_j}(x_j)$ 表示维度 $j$ 中节点 $p_j$ 的一维 Lagrange 基多项式在坐标 $x_j$ 处的值。\n\n计算算法的结构如下：\n1.  对于每个近似级别 $\\ell \\in \\{1, 2, 3\\}$，遍历所提供的 15 个测试点。\n2.  对于每个测试点 $x_{test}$，使用组合公式计算稀疏网格插值 $\\mathcal{A}_{\\ell,d}f(x_{test})$ 的值。这包括：\n    a.  遍历 $q$，范围从 $0$ 到 $\\min(\\ell, d-1)$。\n    b.  生成所有满足 $|\\mathbf{i}|_1 = \\ell-q$ 的多重索引 $\\mathbf{i}$。\n    c.  对于每个多重索引 $\\mathbf{i}$，计算相应的张量积插值 $\\mathcal{I}^{\\mathbf{i}}f(x_{test})$ 的值。\n    d.  将这些值按组合系数 $(-1)^q \\binom{d-1}{q}$ 加权求和。\n3.  为优化性能，函数 $f$ 在网格点上的求值被缓存，因为许多张量积插值会使用相同的网格点。在适当的情况下，递归和中间计算也被记忆化。\n4.  对于每个测试点，计算绝对误差 $|\\,f(x_{test}) - \\mathcal{A}_{\\ell,d}f(x_{test})\\,|$。\n5.  在所有 15 个测试点上这些绝对误差的最大值被记录为级别 $\\ell$ 的结果。\n6.  最终输出是 $\\ell=1, 2, 3$ 的最大误差列表，按要求格式化为 6 位小数。", "answer": "```python\nimport numpy as np\nfrom scipy.special import comb\nfrom itertools import product\nfrom functools import lru_cache\n\n# The target function to be approximated, as defined in the problem.\ndef f_definition(x):\n    \"\"\"The function f, vectorized for efficient evaluation on NumPy arrays.\"\"\"\n    x1, x2, x3, x4, x5 = x\n    term1 = np.exp(0.5 * x1 * x2 + 0.25 * x3) * np.cos(np.pi * (x1 * x3 + 2.0 * x2 * x4))\n    term2 = np.sin(2 * np.pi * (x4 * x5 + 0.1 * x1))\n    term3 = 0.05 * x1 * x2 * x3 * x4 * x5\n    return term1 + term2 + term3\n\n# Caching for function evaluations. The key is a tuple representation of the point.\nfunc_eval_cache = {}\n\ndef f_cached(p_tuple):\n    \"\"\"A wrapper for f_definition that uses a cache to avoid re-computation.\"\"\"\n    if p_tuple not in func_eval_cache:\n        func_eval_cache[p_tuple] = f_definition(np.array(p_tuple))\n    return func_eval_cache[p_tuple]\n\n@lru_cache(maxsize=None)\ndef get_1d_nodes(level):\n    \"\"\"\n    Computes 1D Clenshaw-Curtis nodes on [0,1] for a given hierarchical level.\n    Level 0 corresponds to a single point at the domain center.\n    Level i > 0 corresponds to 2^i+1 points. The results are cached.\n    \"\"\"\n    if level == 0:\n        return (0.5,)  # Return as tuple for hashability\n    num_points = 2**level + 1\n    k = np.arange(num_points)\n    nodes = 0.5 * (1.0 - np.cos(k * np.pi / (num_points - 1)))\n    return tuple(nodes)\n\n@lru_cache(maxsize=None)\ndef generate_multi_indices(d, s):\n    \"\"\"\n    Generates all d-dimensional multi-indices i with |i|_1 = s.\n    The recursive generation is memoized for efficiency.\n    \"\"\"\n    if d == 1:\n        return [(s,)]\n    indices = []\n    for i in range(s + 1):\n        # Recursively find indices for the remaining dimensions and sum\n        sub_indices = generate_multi_indices(d - 1, s - i)\n        for sub in sub_indices:\n            indices.append((i,) + sub)\n    return indices\n\ndef get_1d_lagrange_basis_eval(t, nodes_tuple, k):\n    \"\"\"\n    Evaluates the k-th 1D Lagrange basis polynomial at point t.\n    The nodes are passed as a tuple to enable potential caching if this function were wrapped.\n    \"\"\"\n    nodes = np.array(nodes_tuple)\n    xk = nodes[k]\n\n    if np.isclose(t, xk):\n        return 1.0\n\n    numerator = 1.0\n    denominator = 1.0\n    for j, xj in enumerate(nodes):\n        if j != k:\n            numerator *= (t - xj)\n            denominator *= (xk - xj)\n    \n    # This prevents division by zero if t is close to another node,\n    # making the numerator near-zero.\n    if np.isclose(denominator, 0.0):\n        return 0.0\n        \n    return numerator / denominator\n\ndef solve():\n    \"\"\"\n    Main solver function to perform sparse grid interpolation and error calculation.\n    \"\"\"\n    D = 5\n    LEVELS = [1, 2, 3]\n    TEST_POINTS = np.array([\n        [0.5, 0.5, 0.5, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0],\n        [0.0, 1.0, 0.0, 1.0, 0.0], [1.0, 0.0, 1.0, 0.0, 1.0],\n        [0.25, 0.75, 0.33, 0.67, 0.5], [0.9, 0.1, 0.2, 0.8, 0.3], [0.1, 0.9, 0.8, 0.2, 0.7],\n        [0.3, 0.3, 0.7, 0.7, 0.2], [0.6, 0.4, 0.2, 0.9, 0.1], [0.15, 0.85, 0.55, 0.45, 0.35],\n        [0.8, 0.2, 0.6, 0.4, 0.5], [0.2, 0.8, 0.4, 0.6, 0.9], [0.05, 0.95, 0.25, 0.75, 0.35],\n        [0.4, 0.4, 0.4, 0.4, 0.4]\n    ])\n\n    final_errors = []\n\n    for l_level in LEVELS:\n        max_level_error = 0.0\n        \n        # Cache for tensor product interpolation results, reset for each test point.\n        tensor_interp_cache = {}\n\n        def get_tensor_interp_val(x_test_tuple, mi_tuple):\n            \"\"\"Computes or retrieves from cache the tensor product interpolant value.\"\"\"\n            if mi_tuple in tensor_interp_cache:\n                return tensor_interp_cache[mi_tuple]\n\n            nodes_per_dim = [get_1d_nodes(mi) for mi in mi_tuple]\n            node_indices_ranges = [range(len(nodes)) for nodes in nodes_per_dim]\n            \n            total_sum = 0.0\n            for index_tuple in product(*node_indices_ranges):\n                \n                grid_point = tuple(nodes_per_dim[i][index_tuple[i]] for i in range(D))\n                func_val = f_cached(grid_point)\n                \n                basis_product = 1.0\n                for i in range(D):\n                    basis_val = get_1d_lagrange_basis_eval(x_test_tuple[i], nodes_per_dim[i], index_tuple[i])\n                    basis_product *= basis_val\n                \n                total_sum += func_val * basis_product\n            \n            tensor_interp_cache[mi_tuple] = total_sum\n            return total_sum\n\n        for x_test_point in TEST_POINTS:\n            x_test_tuple = tuple(x_test_point)\n            tensor_interp_cache.clear()\n            \n            interpolant_val = 0.0\n            for q in range(min(l_level, D - 1) + 1):\n                comb_coeff = ((-1)**q) * comb(D - 1, q, exact=True)\n                if comb_coeff == 0:\n                    continue\n                \n                s = l_level - q\n                multi_indices = generate_multi_indices(D, s)\n                \n                term_sum = 0.0\n                for mi_tuple in multi_indices:\n                    term_sum += get_tensor_interp_val(x_test_tuple, mi_tuple)\n                \n                interpolant_val += comb_coeff * term_sum\n\n            exact_val = f_definition(x_test_point)\n            error = abs(exact_val - interpolant_val)\n            if error > max_level_error:\n                max_level_error = error\n        \n        final_errors.append(f\"{max_level_error:.6f}\")\n\n    print(f\"[{','.join(final_errors)}]\")\n\nsolve()\n```"}, {"introduction": "虽然稀疏网格对于光滑函数非常有效，但没有一种近似方法是普适最优的。本练习旨在挑战你探索一种特殊情境：一个复杂的全局方法其表现反被一个更简单的局部方法所超越。你将近似一个带有“扭结”或不连续性的函数 [@problem_id:2399802]，这会促使你批判性地思考不同方法间的权衡，并理解将算法假设与问题特性相匹配的至关重要性。", "id": "2399802", "problem": "考虑紧致域 $\\mathcal{D}=[-1,1]^3$，其坐标为 $(x,y,z)$。您的任务是构造一个特定的三维函数 $f:\\mathcal{D}\\to\\mathbb{R}$，该函数沿一个平面表现出不可微的扭结，使得对于一定范围的离散化参数，基于通过Smolyak稀疏网格组合的Chebyshev多项式的全局多项式插值，会比均匀张量网格上的局部分段三线性插值产生更大的均方根误差。请完全从第一性原理出发：定义该函数，精确定义两种近似算子，在一个固定的评估网格上对两者进行求值，并比较最终的误差。\n\n使用以下数学定义。\n\n1) 函数类。对于一个固定的阈值参数 $\\tau\\in\\mathbb{R}$，定义\n$$\nf_\\tau(x,y,z)=\\max\\{0,\\; x+y+z-\\tau\\},\\quad (x,y,z)\\in[-1,1]^3.\n$$\n在下方的每个测试用例中，选择并明确使用指定的特定成员 $f_{\\tau}$。\n\n2) 一维Chebyshev–Lobatto节点和重心权重。对于一个正整数 $m\\ge 1$，定义节点\n$$\nx_j=\\cos\\left(\\frac{\\pi j}{m-1}\\right),\\quad j=0,1,\\dots,m-1,\n$$\n约定当 $m=1$ 时，单个节点为 $x_0=0$。相关的重心权重 $\\lambda_j$ 为\n$$\n\\lambda_j = (-1)^j\\cdot \\delta_j,\\quad \\delta_j=\\begin{cases}\\tfrac{1}{2},& j\\in\\{0,m-1\\}\\ \\text{and}\\ m\\ge 2,\\\n$$4pt]\n1,& \\text{otherwise.}\n\\end{cases}\n$$\n当 $m=1$ 时，取 $\\lambda_0=1$。\n\n给定具有重心权重 $(\\lambda_j)_{j=0}^{m-1}$ 和数据值 $(y_j)_{j=0}^{m-1}$ 的离散节点 $(x_j)_{j=0}^{m-1}$，函数 $g$ 的一维重心插值 $U_m[g]$ 是一个次数至多为 $m-1$ 的多项式，其按点定义为\n$$\nU_m[g](x)=\n\\begin{cases}\ng(x_k),& \\text{if }x=x_k\\ \\text{for some }k,\\\n$$6pt]\n\\dfrac{\\displaystyle \\sum_{j=0}^{m-1}\\dfrac{\\lambda_j}{x-x_j}g(x_j)}{\\displaystyle \\sum_{j=0}^{m-1}\\dfrac{\\lambda_j}{x-x_j}},& \\text{otherwise.}\n\\end{cases}\n$$\n\n3) $d=3$ 维中的Smolyak稀疏网格Chebyshev插值。对于一维层级 $\\ell\\in\\mathbb{N}$，设\n$$\nm(\\ell)=\\begin{cases}\n1,& \\ell=1,\\\\\n2^{\\ell-1}+1,& \\ell\\ge 2,\n\\end{cases}\n$$\n并令 $U_{m(\\ell)}$ 为上述在基数为 $m(\\ell)$ 的Chebyshev–Lobatto节点上定义的一维插值。对于一个总层级 $L\\in\\mathbb{N}$ 和 $d=3$，定义Smolyak指标集\n$$\n\\mathcal{I}_{L}=\\left\\{\\boldsymbol{\\ell}=(\\ell_1,\\ell_2,\\ell_3)\\in\\mathbb{N}^3:\\ \\ell_i\\ge 1,\\ \\ \\ell_1+\\ell_2+\\ell_3\\le L+2\\right\\}.\n$$\n对于 $\\boldsymbol{\\ell}\\in\\mathcal{I}_L$，定义组合系数\n$$\nw(\\boldsymbol{\\ell};L)=(-1)^{\\,L+2-(\\ell_1+\\ell_2+\\ell_3)}\\binom{2}{\\,L+2-(\\ell_1+\\ell_2+\\ell_3)\\,}.\n$$\nSmolyak插值 $A_L[f]$ 是一个线性算子\n$$\nA_L[f](x,y,z)=\\sum_{\\boldsymbol{\\ell}\\in\\mathcal{I}_L} w(\\boldsymbol{\\ell};L)\\,\\big(U_{m(\\ell_1)}\\otimes U_{m(\\ell_2)}\\otimes U_{m(\\ell_3)}\\big)[f](x,y,z),\n$$\n其中 $\\otimes$ 表示沿每个坐标的一维插值的张量积应用。\n\n4) 均匀张量网格和分段三线性插值。对于一个整数 $M\\ge 2$，在 $[-1,1]$ 上定义一个均匀网格，其节点为\n$$\n\\xi_j=-1+\\frac{2j}{M-1},\\quad j=0,1,\\dots,M-1.\n$$\n分段三线性插值 $T_M[f]$ 是一个唯一函数，它在每个网格单元上是三线性的，并在所有张量网格节点上与 $f$ 的值相匹配。\n\n5) 离散均方根误差 (RMSE)。对于一个整数 $E\\ge 2$，定义一个评估网格，每轴有 $E$ 个节点\n$$\n\\zeta_k=-1+\\frac{2k}{E-1},\\quad k=0,1,\\dots,E-1.\n$$\n给定 $\\mathcal{D}$ 上 $f$ 的一个近似函数 $\\hat{f}$，定义离散RMSE为\n$$\n\\mathrm{RMSE}(\\hat{f},f)=\\left(\\frac{1}{E^3}\\sum_{i=0}^{E-1}\\sum_{j=0}^{E-1}\\sum_{k=0}^{E-1}\\big(\\hat{f}(\\zeta_i,\\zeta_j,\\zeta_k)-f(\\zeta_i,\\zeta_j,\\zeta_k)\\big)^2\\right)^{1/2}.\n$$\n\n实现一个完整的程序，对每个测试用例执行以下操作：构造指定的 $f_\\tau$，构造如上定义的 $A_L[f_\\tau]$ 和 $T_M[f_\\tau]$，在同一个评估网格上对两种近似函数和 $f_\\tau$ 进行求值，计算RMSE，并报告标量差值\n$$\n\\Delta=\\mathrm{RMSE}\\big(A_L[f_\\tau],f_\\tau\\big)-\\mathrm{RMSE}\\big(T_M[f_\\tau],f_\\tau\\big).\n$$\n正的 $\\Delta$ 值表示Smolyak网格上的Chebyshev插值性能劣于分段三线性插值。\n\n使用以下测试套件，该套件通过改变扭结位置和离散化层级来探究典型、边界邻近和较低层级的情况：\n\n- 测试用例 $1$：$\\tau=0.0$, $L=4$, $M=25$, $E=31$。\n- 测试用例 $2$：$\\tau=0.9$, $L=5$, $M=29$, $E=31$。\n- 测试用例 $3$：$\\tau=-0.2$, $L=3$, $M=21$, $E=31$。\n\n最终输出格式。您的程序应生成单行输出，其中包含来自上述测试用例的三个数值结果 $\\Delta$，格式为用方括号括起来的逗号分隔列表，每个标量值四舍五入到六位小数（例如，$[\\delta_1,\\delta_2,\\delta_3]$）。", "solution": "问题陈述已经过严格验证，并被确定为有效。它在科学上基于数值分析和逼近理论的原理，是适定的，提供了所有必要的定义和参数，并以客观、正式的语言表述。该问题构成了计算数学中的一个标准、可验证的练习，具体是比较一种全局多项式逼近方法（Smolyak-Chebyshev）与一种局部分段多项式方法（分段三线性）在处理一个表现出低光滑度的函数时的功效。因此，我们将提供一个完整的解决方案。\n\n目标是计算在域 $\\mathcal{D}=[-1,1]^3$ 上，两种近似方案对函数 $f_\\tau(x,y,z)=\\max\\{0,\\; x+y+z-\\tau\\}$ 的均方根误差之差 $\\Delta$。正的 $\\Delta$ 值表示Smolyak-Chebyshev方法产生的误差大于分段三线性方法。函数 $f_\\tau$ 在由 $x+y+z-\\tau=0$ 定义的平面上是连续但不可微的，在该处它有一个“扭结”。这种光滑性的缺乏是所要测试的核心特征。\n\n我们的步骤如下：\n1.  定义目标函数 $f_\\tau$ 和评估网格。\n2.  构造分段三线性插值 $T_M[f_\\tau]$ 并在网格上对其求值。\n3.  构造Smolyak稀疏网格插值 $A_L[f_\\tau]$ 并在同一网格上对其求值。\n4.  计算两种近似相对于真实函数的离散均方根误差 (RMSE)。\n5.  计算差值 $\\Delta = \\mathrm{RMSE}(A_L[f_\\tau]) - \\mathrm{RMSE}(T_M[f_\\tau])$。\n\n**1. 评估框架**\n\n对于每个测试用例，我们都给定函数 $f_\\tau$ 的参数 $\\tau$、Smolyak网格的层级 $L$、三线性插值的网格大小 $M$，以及评估网格的分辨率 $E$。评估网格是 $\\mathcal{D}=[-1,1]^3$ 上的一个均匀张量积网格，每轴有 $E$ 个点，由节点 $\\zeta_k=-1+\\frac{2k}{E-1}$ ($k=0, \\dots, E-1$) 给出。所有的近似函数和真实函数都将在此网格的 $E^3$ 个点上进行计算。\n\n**2. 分段三线性插值 $T_M[f_\\tau]$**\n\n该方法依赖于在均匀网格的每个单元内的局部逼近。\n首先，定义一个每轴有 $M$ 个节点的均匀张量网格：$\\xi_j=-1+\\frac{2j}{M-1}$ ($j=0, \\dots, M-1$)。这将域 $\\mathcal{D}$ 划分为 $(M-1)^3$ 个立方单元。函数 $f_\\tau$ 在所有 $M^3$ 个网格节点上进行求值。\n\n为了在评估点 $(x,y,z)$ 处找到插值 $T_M[f_\\tau]$ 的值，我们首先确定包含该点的单元 $[\\xi_{j_x}, \\xi_{j_x+1}] \\times [\\xi_{j_y}, \\xi_{j_y+1}] \\times [\\xi_{j_z}, \\xi_{j_z+1}]$。然后使用该单元八个角点的已知函数值通过三线性插值计算出该值。这可以通过利用 `scipy.ndimage.map_coordinates` 函数来高效实现，该函数执行N维分段线性插值。评估坐标 $(\\zeta_i, \\zeta_j, \\zeta_k)$ 在传递给函数之前，会被映射到 $M$ 点网格的基于索引的坐标系中。\n\n**3. Smolyak稀疏网格插值 $A_L[f_\\tau]$**\n\nSmolyak构造提供了一种从一维插值的组合中构建高维插值的方法，对于足够光滑的函数，其维度扩展性优于完全张量积。维度 $d=3$ 且层级为 $L$ 的算子是：\n$$\nA_L[f] = \\sum_{\\boldsymbol{\\ell}\\in\\mathcal{I}_L} w(\\boldsymbol{\\ell};L)\\,\\big(U_{m(\\ell_1)}\\otimes U_{m(\\ell_2)}\\otimes U_{m(\\ell_3)}\\big)[f]\n$$\n其中 $\\boldsymbol{\\ell}=(\\ell_1,\\ell_2,\\ell_3)$ 是一维层级的多重指标。其组成部分是：\n-   **指标集**：$\\mathcal{I}_{L}=\\left\\{\\boldsymbol{\\ell}\\in\\mathbb{N}^3:\\ \\ell_1+\\ell_2+\\ell_3\\le L+2\\right\\}$。\n-   **组合系数**：$w(\\boldsymbol{\\ell};L)=(-1)^{\\,L+2-|\\boldsymbol{\\ell}|_1}\\binom{2}{\\,L+2-|\\boldsymbol{\\ell}|_1\\,}$，其中 $|\\boldsymbol{\\ell}|_1=\\ell_1+\\ell_2+\\ell_3$。这些系数仅在 $L \\le |\\boldsymbol{\\ell}|_1 \\le L+2$ 时非零。\n-   **一维插值**：$U_{m(\\ell)}$ 是在 $m(\\ell)$ 个Chebyshev-Lobatto节点上的重心多项式插值。节点数为 $m(1)=1$ 和 $m(\\ell)=2^{\\ell-1}+1$ (当 $\\ell \\ge 2$)。\n\n在评估网格上对 $A_L[f_\\tau]$ 的求值过程如下：\n一个初始化为零的累加器网格用于存储最终的近似值。我们遍历活动指标集（其中 $w(\\boldsymbol{\\ell};L) \\neq 0$）中的每个多重指标 $\\boldsymbol{\\ell}$。对于每个 $\\boldsymbol{\\ell}$：\na. 分别沿 $x$、$y$ 和 $z$ 轴，由 $m(\\ell_1)$、$m(\\ell_2)$ 和 $m(\\ell_3)$ 个Chebyshev-Lobatto节点形成一个稀疏张量积网格。\nb. 在此稀疏网格的所有节点上对真实函数 $f_\\tau$ 求值。\nc. 从这个稀疏网格到密集的 $E \\times E \\times E$ 评估网格执行三维张量积重心插值。这是通过沿每个维度顺序应用一维重心插值公式来完成的。\nd. 将得到的插值结果乘以系数 $w(\\boldsymbol{\\ell};L)$ 并加到累加器网格中。\n\n一维重心插值公式因其数值稳定性而对Chebyshev点非常有效。对于一组节点 $(x_j)$、值 $(y_j)$ 和重心权重 $(\\lambda_j)$，在点 $x$ 处的插值由 $\\left(\\sum_j \\frac{\\lambda_j y_j}{x-x_j}\\right) / \\left(\\sum_j \\frac{\\lambda_j}{x-x_j}\\right)$ 给出。\n\n**4. 误差计算与比较**\n\n在评估网格上计算出两种近似 $A_L[f_\\tau]$ 和 $T_M[f_\\tau]$ 的值后，计算它们各自的离散均方根误差：\n$$\n\\mathrm{RMSE}(\\hat{f},f)=\\left(\\frac{1}{E^3}\\sum_{i,j,k=0}^{E-1}\\big(\\hat{f}(\\zeta_i,\\zeta_j,\\zeta_k)-f_\\tau(\\zeta_i,\\zeta_j,\\zeta_k)\\big)^2\\right)^{1/2}\n$$\n每个测试用例的最终结果是差值 $\\Delta = \\mathrm{RMSE}(A_L[f_\\tau]) - \\mathrm{RMSE}(T_M[f_\\tau])$。函数 $f_\\tau$ 几乎处处是线性的，但它的扭结违反了保证全局多项式方法快速收敛的光滑性假设。相比之下，分段线性方法是局部的；其误差主要局限于被扭结穿过的单元。因此，可以预期对于给定的参数，$\\Delta$ 将为正值，这表明了局部、低阶方法更优越的场景。", "answer": "```python\nimport numpy as np\nfrom scipy.special import comb\nfrom scipy.ndimage import map_coordinates\n\ndef get_cheby_grid(m, cache):\n    \"\"\"\n    Computes or retrieves from cache the Chebyshev-Lobatto nodes and barycentric weights.\n    \"\"\"\n    if m in cache:\n        return cache[m]\n\n    if m == 1:\n        nodes = np.array([0.0])\n        weights = np.array([1.0])\n        cache[m] = (nodes, weights)\n        return nodes, weights\n\n    j = np.arange(m)\n    nodes = np.cos(np.pi * j / (m - 1))\n    \n    weights = (-1.0)**j\n    weights[0] *= 0.5\n    weights[-1] *= 0.5\n    \n    cache[m] = (nodes, weights)\n    return nodes, weights\n\ndef barycentric_interp1d(x_eval, x_nodes, y_nodes, bary_weights):\n    \"\"\"\n    Performs 1D barycentric interpolation on a vector of evaluation points.\n    \"\"\"\n    Ne = x_eval.shape[0]\n    Nm = x_nodes.shape[0]\n    \n    if Nm == 1:\n        return np.full(Ne, y_nodes[0])\n\n    interp_vals = np.zeros(Ne, dtype=np.float64)\n    exact_matches = np.zeros(Ne, dtype=bool)\n\n    # Handle cases where evaluation points are very close to nodes\n    for j in range(Nm):\n        matches = np.isclose(x_eval, x_nodes[j])\n        if np.any(matches):\n            interp_vals[matches] = y_nodes[j]\n            exact_matches |= matches\n\n    # Process points that are not nodes using the vectorized formula\n    non_match_indices = np.where(~exact_matches)[0]\n    if len(non_match_indices) > 0:\n        x_sub_eval = x_eval[non_match_indices]\n        \n        diff = x_sub_eval[:, None] - x_nodes[None, :]\n        temp = bary_weights[None, :] / diff\n        \n        numerator = np.sum(temp * y_nodes[None, :], axis=1)\n        denominator = np.sum(temp, axis=1)\n\n        # Avoid division by zero, although it is unlikely for non-node points\n        result = np.divide(numerator, denominator, \n                           out=np.zeros_like(numerator), \n                           where=denominator != 0)\n        \n        interp_vals[non_match_indices] = result\n\n    return interp_vals\n\ndef compute_smolyak_approx(f, L, eval_grid_1d, cache):\n    \"\"\"\n    Computes the Smolyak sparse grid approximation on the evaluation grid.\n    \"\"\"\n    E = len(eval_grid_1d)\n    approx_values = np.zeros((E, E, E), dtype=np.float64)\n\n    # Generate Smolyak index set and coefficients\n    indices_and_coeffs = []\n    max_level_sum = L + 2\n    for l1 in range(1, max_level_sum + 1):\n        for l2 in range(1, max_level_sum - l1 + 1):\n            for l3 in range(1, max_level_sum - l1 - l2 + 1):\n                level_sum = l1 + l2 + l3\n                k = max_level_sum - level_sum\n                if 0 <= k <= 2:\n                    w = ((-1)**k) * comb(2, k, exact=True)\n                    if w != 0:\n                        indices_and_coeffs.append(((l1, l2, l3), w))\n\n    m_func = lambda l: 1 if l == 1 else 2**(l - 1) + 1\n\n    for (l1, l2, l3), w in indices_and_coeffs:\n        m1, m2, m3 = m_func(l1), m_func(l2), m_func(l3)\n        \n        nodes1, weights1 = get_cheby_grid(m1, cache)\n        nodes2, weights2 = get_cheby_grid(m2, cache)\n        nodes3, weights3 = get_cheby_grid(m3, cache)\n\n        grid_x, grid_y, grid_z = np.meshgrid(nodes1, nodes2, nodes3, indexing='ij')\n        f_on_grid = f(grid_x, grid_y, grid_z)\n        \n        # Sequentially apply 1D interpolation\n        interp1 = np.zeros((E, m2, m3), dtype=np.float64)\n        for j in range(m2):\n            for k in range(m3):\n                interp1[:, j, k] = barycentric_interp1d(eval_grid_1d, nodes1, f_on_grid[:, j, k], weights1)\n\n        interp2 = np.zeros((E, E, m3), dtype=np.float64)\n        for i in range(E):\n            for k in range(m3):\n                interp2[i, :, k] = barycentric_interp1d(eval_grid_1d, nodes2, interp1[i, :, k], weights2)\n\n        tensor_prod_interp = np.zeros((E, E, E), dtype=np.float64)\n        for i in range(E):\n            for j in range(E):\n                tensor_prod_interp[i, j, :] = barycentric_interp1d(eval_grid_1d, nodes3, interp2[i, j, :], weights3)\n        \n        approx_values += w * tensor_prod_interp\n        \n    return approx_values\n\ndef compute_trilinear_approx(f, M, eval_xx, eval_yy, eval_zz):\n    \"\"\"\n    Computes the piecewise trilinear approximation on the evaluation grid.\n    \"\"\"\n    grid_nodes_1d = np.linspace(-1., 1., M)\n    grid_x, grid_y, grid_z = np.meshgrid(grid_nodes_1d, grid_nodes_1d, grid_nodes_1d, indexing='ij')\n    f_on_uniform_grid = f(grid_x, grid_y, grid_z)\n    \n    # Map physical coordinates of the evaluation grid to the index coordinates of the M-point grid\n    h = 2.0 / (M - 1)\n    coords_x = (eval_xx - (-1)) / h\n    coords_y = (eval_yy - (-1)) / h\n    coords_z = (eval_zz - (-1)) / h\n    \n    coords = np.stack([coords_x, coords_y, coords_z])\n    \n    # Use SciPy's map_coordinates for efficient N-D linear interpolation\n    approx_values = map_coordinates(f_on_uniform_grid, coords, order=1, mode='nearest')\n    \n    return approx_values\n\ndef solve():\n    \"\"\"\n    Main driver function to run test cases and compute the error difference.\n    \"\"\"\n    test_cases = [\n        (0.0, 4, 25, 31),\n        (0.9, 5, 29, 31),\n        (-0.2, 3, 21, 31),\n    ]\n\n    results = []\n    cheby_cache = {}\n\n    for i, (tau, L, M, E) in enumerate(test_cases):\n        f = lambda x, y, z: np.maximum(0., x + y + z - tau)\n        \n        eval_grid_1d = np.linspace(-1., 1., E)\n        eval_xx, eval_yy, eval_zz = np.meshgrid(eval_grid_1d, eval_grid_1d, eval_grid_1d, indexing='ij')\n        \n        f_true_values = f(eval_xx, eval_yy, eval_zz)\n\n        # Smolyak-Chebyshev approximation\n        A_L_values = compute_smolyak_approx(f, L, eval_grid_1d, cheby_cache)\n        rmse_A = np.sqrt(np.mean((A_L_values - f_true_values)**2))\n\n        # Piecewise trilinear approximation\n        T_M_values = compute_trilinear_approx(f, M, eval_xx, eval_yy, eval_zz)\n        rmse_T = np.sqrt(np.mean((T_M_values - f_true_values)**2))\n\n        delta = rmse_A - rmse_T\n        results.append(delta)\n\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"}, {"introduction": "许多现实世界中的经济模型所涉及的函数，其变量具有不同的影响力。最后的这个练习将介绍各向异性稀疏网格，这是一种至关重要的优化技术，它能将更多的计算资源分配给更重要的维度。你将实现一个各向异性网格来近似一个10维函数，其中前两个变量占据主导地位，并将其效率与标准的各向同性网格进行直接比较 [@problem_id:2432646]。", "id": "2432646", "problem": "您的任务是使用基于 Smolyak 构造的各向异性稀疏网格，在一个超立方体上近似一个高维函数。定义域为单位超立方体 $[0,1]^{10}$。令 $d = 10$。对于每个维度 $i \\in \\{1,\\dots,d\\}$ 和每个层级 $\\ell_i \\in \\mathbb{N}$（$\\ell_i \\ge 1$），定义一维嵌套二进网格\n$$\nX_{\\ell_i} = \\left\\{ \\frac{j}{2^{\\ell_i - 1}} \\,:\\, j = 0,1,\\dots,2^{\\ell_i - 1} \\right\\} \\subset [0,1].\n$$\n对于一个多重指标 $\\boldsymbol{\\ell} = (\\ell_1,\\dots,\\ell_d)$，定义全张量网格\n$$\nG_{\\boldsymbol{\\ell}} = X_{\\ell_1} \\times \\cdots \\times X_{\\ell_d}.\n$$\n令 $U_{\\boldsymbol{\\ell}}$ 为 $G_{\\boldsymbol{\\ell}}$ 上的 $d$ 元多线性节点插值算子，它由二进网格上的一维线性拉格朗日基函数的张量积所定义。\n\n令 $\\boldsymbol{\\alpha} = (\\alpha_1,\\dots,\\alpha_d)$ 为一个正整数各向异性权重向量，并令 $Q \\in \\mathbb{N}$ 且 $Q \\ge 0$。定义下闭合指标集\n$$\n\\mathcal{I}(\\boldsymbol{\\alpha}, Q) = \\left\\{ \\boldsymbol{\\ell} \\in \\mathbb{N}^d \\,:\\, \\ell_i \\ge 1 \\text{ 对所有 } i, \\ \\sum_{i=1}^d \\alpha_i (\\ell_i - 1) \\le Q \\right\\}.\n$$\n考虑一个全网格插值的线性组合\n$$\n\\mathcal{S}_{\\boldsymbol{\\alpha},Q} = \\sum_{\\boldsymbol{\\ell} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q)} c_{\\boldsymbol{\\ell}} \\, U_{\\boldsymbol{\\ell}},\n$$\n其中系数 $\\{c_{\\boldsymbol{\\ell}}\\}$ 由插值一致性条件所刻画\n$$\n\\sum_{\\boldsymbol{k} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q), \\, \\boldsymbol{k} \\ge \\boldsymbol{\\ell}} c_{\\boldsymbol{k}} = 1 \\quad \\text{对每个 } \\boldsymbol{\\ell} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q),\n$$\n此处 $\\boldsymbol{k} \\ge \\boldsymbol{\\ell}$ 表示分量偏序 $k_i \\ge \\ell_i$（对所有 $i$ 成立）。该条件确保 $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$ 在 $\\bigcup_{\\boldsymbol{\\ell} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q)} G_{\\boldsymbol{\\ell}}$ 的每个节点上都对目标函数进行插值。\n\n您的程序必须在 $[0,1]^{10}$ 上实现 $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$，并在给定的点上进行求值。使用以下两个目标函数，每个函数都针对 $\\boldsymbol{x} = (x_1,\\dots,x_{10}) \\in [0,1]^{10}$ 定义：\n- $f_1(\\boldsymbol{x}) = \\exp(x_1) + \\sin(2\\pi x_2) + 0.1 \\sum_{i=3}^{10} 0.5^{\\,i} \\, x_i^2$.\n- $f_2(\\boldsymbol{x}) = \\log(1 + 5 x_1) + \\sqrt{1 + x_2} + 0.01 \\sum_{i=3}^{10} x_i$.\n所有三角函数的参数都应解释为弧度。自然对数函数为 $\\log(\\cdot)$，主平方根为 $\\sqrt{\\cdot}$。\n\n使用以下求值点集 $\\mathcal{E} = \\{\\boldsymbol{y}^{(k)}\\}_{k=1}^{5} \\subset [0,1]^{10}$，其中每个 $\\boldsymbol{y}^{(k)}$ 都明确给出：\n- $\\boldsymbol{y}^{(1)} = (0.13,\\,0.77,\\,0.50,\\,0.20,\\,0.80,\\,0.33,\\,0.66,\\,0.10,\\,0.90,\\,0.42)$,\n- $\\boldsymbol{y}^{(2)} = (0.31,\\,0.62,\\,0.25,\\,0.75,\\,0.40,\\,0.60,\\,0.20,\\,0.80,\\,0.35,\\,0.65)$,\n- $\\boldsymbol{y}^{(3)} = (0.73,\\,0.27,\\,0.15,\\,0.85,\\,0.55,\\,0.45,\\,0.05,\\,0.95,\\,0.22,\\,0.78)$,\n- $\\boldsymbol{y}^{(4)} = (0.50,\\,0.50,\\,0.10,\\,0.90,\\,0.30,\\,0.70,\\,0.25,\\,0.75,\\,0.40,\\,0.60)$,\n- $\\boldsymbol{y}^{(5)} = (0.21,\\,0.84,\\,0.63,\\,0.37,\\,0.12,\\,0.88,\\,0.47,\\,0.53,\\,0.19,\\,0.81)$.\n\n定义偏重前两个维度的各向异性权重为\n$$\n\\boldsymbol{\\alpha}^{\\mathrm{aniso}} = (1,\\,1,\\,4,\\,4,\\,4,\\,4,\\,4,\\,4,\\,4,\\,4),\n$$\n以及各向同性权重为\n$$\n\\boldsymbol{\\alpha}^{\\mathrm{iso}} = (1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1).\n$$\n\n测试套件。您的程序必须执行以下四个测试用例，并为每个用例报告一个标量结果：\n- 测试 1：使用 $\\boldsymbol{\\alpha} = \\boldsymbol{\\alpha}^{\\mathrm{aniso}}$ 和 $Q = 8$ 为 $f_1$ 构建 $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$。计算在 $\\mathcal{E}$ 上的最大绝对误差，\n$$\n\\max_{\\boldsymbol{y} \\in \\mathcal{E}} \\left| \\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f_1](\\boldsymbol{y}) - f_1(\\boldsymbol{y}) \\right|.\n$$\n- 测试 2：使用 $\\boldsymbol{\\alpha} = \\boldsymbol{\\alpha}^{\\mathrm{iso}}$ 和 $Q = 3$ 为 $f_1$ 构建 $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$。计算在 $\\mathcal{E}$ 上的最大绝对误差。\n- 测试 3：使用 $\\boldsymbol{\\alpha} = \\boldsymbol{\\alpha}^{\\mathrm{aniso}}$ 和 $Q = 6$ 为 $f_2$ 构建 $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$。计算在 $\\mathcal{E}$ 上的最大绝对误差。\n- 测试 4：使用 $\\boldsymbol{\\alpha} = \\boldsymbol{\\alpha}^{\\mathrm{aniso}}$ 和 $Q = 8$ 为 $f_1$ 构建 $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$。计算在单点 $\\boldsymbol{0} = (0,0,0,0,0,0,0,0,0,0)$ 上的绝对误差，\n$$\n\\left| \\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f_1](\\boldsymbol{0}) - f_1(\\boldsymbol{0}) \\right|.\n$$\n\n最终输出格式。您的程序应生成单行输出，其中按顺序包含四个结果，格式为方括号括起来的逗号分隔列表，例如\n$$\n[{\\tt r1},{\\tt r2},{\\tt r3},{\\tt r4}],\n$$\n其中每个 ${\\tt rj}$ 是一个实数（一个浮点值）。", "solution": "用户提供了一个在数值分析领域中定义明确的问题，具体涉及使用各向异性稀疏网格近似高维函数。该问题具有科学依据、内部一致，并包含解决它所需的全部信息。因此，我将着手提供一个完整的解决方案。\n\n核心任务是在指定点 $\\boldsymbol{y} \\in [0,1]^{10}$ 上对稀疏网格插值 $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f]$ 进行求值。该插值使用组合技术公式定义：\n$$\n\\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f](\\boldsymbol{y}) = \\sum_{\\boldsymbol{\\ell} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q)} c_{\\boldsymbol{\\ell}} \\, U_{\\boldsymbol{\\ell}}[f](\\boldsymbol{y})\n$$\n其中 $f$ 是目标函数，$\\boldsymbol{y}$ 是一个求值点，$\\mathcal{I}(\\boldsymbol{\\alpha}, Q)$ 是一个下闭合多重指标集，$c_{\\boldsymbol{\\ell}}$ 是组合系数，$U_{\\boldsymbol{\\ell}}[f](\\boldsymbol{y})$ 是一个全张量积多线性插值的值。\n\n该解决方案通过针对每个测试用例的一系列模块化步骤来实现：\n1.  **指标集的生成**：构建集合 $\\mathcal{I}(\\boldsymbol{\\alpha}, Q) = \\left\\{ \\boldsymbol{\\ell} \\in \\mathbb{N}^d \\,:\\, \\ell_i \\ge 1, \\ \\sum_{i=1}^d \\alpha_i (\\ell_i - 1) \\le Q \\right\\}$。为高效实现此过程，我们定义层级向量 $\\boldsymbol{k} = (\\ell_1-1, \\dots, \\ell_d-1)$，其中每个 $k_i \\ge 0$。条件变为 $\\sum_{i=1}^d \\alpha_i k_i \\le Q$。采用递归回溯算法来寻找所有满足此不等式的有效向量 $\\boldsymbol{k} \\in \\mathbb{N}_0^d$。对应的多重指标 $\\boldsymbol{\\ell} = \\boldsymbol{k} + \\mathbf{1}$ 构成了集合 $\\mathcal{I}(\\boldsymbol{\\alpha}, Q)$。该集合存储在哈希集合中以便高效查找。\n\n2.  **组合系数的计算**：系数 $c_{\\boldsymbol{\\ell}}$ 由确保算子具有插值性的一致性条件确定。对于一个下闭合指标集 $\\mathcal{I}$，此条件意味着 $c_{\\boldsymbol{\\ell}}$ 的唯一解可由容斥原理给出：\n    $$\n    c_{\\boldsymbol{\\ell}} = \\sum_{J \\subseteq \\{1, \\dots, d\\}} (-1)^{|J|} \\mathbb{I}(\\boldsymbol{\\ell} + \\mathbf{e}_J \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q))\n    $$\n    其中 $\\mathbf{e}_J = \\sum_{j \\in J} \\mathbf{e}_j$ 且 $\\mathbf{e}_j$ 是第 $j$ 个标准基向量。$\\mathbb{I}(\\cdot)$ 是指示函数，当条件成立时其值为1，否则为0。对于每个 $\\boldsymbol{\\ell} \\in \\mathcal{I}$，我们遍历维度的所有 $2^d$ 个子集 $J$，检查相邻指标 $\\boldsymbol{\\ell}+\\mathbf{e}_J$ 是否在 $\\mathcal{I}$ 中，并对带符号的贡献求和。对于 $d=10$，$2^{10} = 1024$，即每个系数需要进行1024次检查，这在计算上是可行的。\n\n3.  **全张量积插值的求值**：项 $U_{\\boldsymbol{\\ell}}[f](\\boldsymbol{y})$ 表示在点 $\\boldsymbol{y}$ 处对全张量网格 $G_{\\boldsymbol{\\ell}} = X_{\\ell_1} \\times \\cdots \\times X_{\\ell_d}$ 上的多线性插值进行求值。这是通过递归实现的。$d$ 维插值的值通过在第一维上执行一维线性插值获得，其中所需的两个网格点上的值本身是通过在其余变量上进行 $(d-1)$ 维插值来计算的。此过程经过 $d$ 步后终止，需要对包围 $\\boldsymbol{y}$ 的 $G_{\\boldsymbol{\\ell}}$ 中超矩形单元的 $2^d$ 个角点进行函数求值。\n\n4.  **通过记忆化进行优化**：由于存在冗余计算，一个朴素的实现方式在计算上是不可行的。对不同的 $\\boldsymbol{\\ell}$ 求值 $U_{\\boldsymbol{\\ell}}[f](\\boldsymbol{y})$ 会重复需要在相同的网格点上计算 $f$ 的值。为消除这种冗余，所有的函数求值 $f(\\boldsymbol{x})$ 都被记忆化（缓存）在一个字典中。当需要特定网格点 $\\boldsymbol{x}$ 上的 $f$ 值时，会先检查缓存，仅当该值之前未被计算过时，才会对函数进行求值。\n\n5.  **组装与误差计算**：对于每个测试用例和每个求值点 $\\boldsymbol{y}$，通过对所有 $\\boldsymbol{\\ell} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q)$ 的贡献 $c_{\\boldsymbol{\\ell}} U_{\\boldsymbol{\\ell}}[f](\\boldsymbol{y})$ 求和来组装最终的近似值。然后计算绝对误差为 $|\\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f](\\boldsymbol{y}) - f(\\boldsymbol{y})|$。按要求报告在求值点集 $\\mathcal{E}$ 上这些误差的最大值。\n\n对于测试用例 4，求值点为 $\\boldsymbol{y} = \\boldsymbol{0} = (0,\\dots,0)$。由于一维网格 $X_{\\ell_i}$ 总是包含 0，所以点 $\\boldsymbol{0}$ 是每个网格 $G_{\\boldsymbol{\\ell}}$ 的成员。稀疏网格构造保证了在稀疏网格并集 $\\mathcal{H}_{\\boldsymbol{\\alpha},Q} = \\bigcup_{\\boldsymbol{\\ell} \\in \\mathcal{I}} G_{\\boldsymbol{\\ell}}$ 中的所有点上都具有插值性。由于 $\\boldsymbol{0} \\in \\mathcal{H}_{\\boldsymbol{\\alpha},Q}$，我们必然有 $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f_1](\\boldsymbol{0}) = f_1(\\boldsymbol{0})$，这意味着绝对误差恰好为 0。这可作为对实现正确性的一个解析性检验。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import combinations\nimport math\n\ndef solve():\n    \"\"\"\n    Implements and evaluates an anisotropic sparse grid interpolant.\n    \"\"\"\n    d = 10\n\n    # Define target functions\n    def f1(x_tuple):\n        x = np.array(x_tuple)\n        term1 = np.exp(x[0])\n        term2 = np.sin(2 * np.pi * x[1])\n        term3 = 0.1 * np.sum([0.5**(i + 3) * x[i + 2]**2 for i in range(8)])\n        return term1 + term2 + term3\n\n    def f2(x_tuple):\n        x = np.array(x_tuple)\n        term1 = np.log(1 + 5 * x[0])\n        term2 = np.sqrt(1 + x[1])\n        term3 = 0.01 * np.sum(x[2:])\n        return term1 + term2 + term3\n\n    # Define evaluation points\n    evaluation_set = [\n        (0.13, 0.77, 0.50, 0.20, 0.80, 0.33, 0.66, 0.10, 0.90, 0.42),\n        (0.31, 0.62, 0.25, 0.75, 0.40, 0.60, 0.20, 0.80, 0.35, 0.65),\n        (0.73, 0.27, 0.15, 0.85, 0.55, 0.45, 0.05, 0.95, 0.22, 0.78),\n        (0.50, 0.50, 0.10, 0.90, 0.30, 0.70, 0.25, 0.75, 0.40, 0.60),\n        (0.21, 0.84, 0.63, 0.37, 0.12, 0.88, 0.47, 0.53, 0.19, 0.81),\n    ]\n\n    # Define anisotropy weights\n    alpha_aniso = (1, 1, 4, 4, 4, 4, 4, 4, 4, 4)\n    alpha_iso = (1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n\n    # Define test cases\n    test_cases = [\n        {'f': f1, 'alpha': alpha_aniso, 'Q': 8, 'eval_points': evaluation_set},\n        {'f': f1, 'alpha': alpha_iso,   'Q': 3, 'eval_points': evaluation_set},\n        {'f': f2, 'alpha': alpha_aniso, 'Q': 6, 'eval_points': evaluation_set},\n        {'f': f1, 'alpha': alpha_aniso, 'Q': 8, 'eval_points': [tuple([0.0]*d)]},\n    ]\n\n    memo_indices = {}\n    memo_coeffs = {}\n\n    def generate_indices(alpha, Q):\n        indices = set()\n        k_levels = []\n\n        def find_k(dim_idx, current_sum):\n            if dim_idx == d:\n                indices.add(tuple(k + 1 for k in k_levels))\n                return\n\n            max_k = (Q - current_sum) // alpha[dim_idx]\n            for ki in range(max_k + 1):\n                k_levels.append(ki)\n                find_k(dim_idx + 1, current_sum + alpha[dim_idx] * ki)\n                k_levels.pop()\n        \n        find_k(0, 0)\n        return indices\n\n    def calculate_coeffs(index_set):\n        coeffs = {}\n        e_vectors = np.identity(d, dtype=int)\n        for l_tuple in index_set:\n            l_vec = np.array(l_tuple)\n            c_l = 0\n            for size in range(d + 1):\n                for J in combinations(range(d), size):\n                    l_prime_vec = l_vec.copy()\n                    for j_idx in J:\n                        l_prime_vec[j_idx] += 1\n                    \n                    if tuple(l_prime_vec) in index_set:\n                        c_l += (-1)**size\n            coeffs[l_tuple] = c_l\n        return coeffs\n    \n    def get_interpolant_evaluator(f_func, y_tuple, l_tuple, memo_f):\n        def recursive_eval(dim, partial_point):\n            if dim == d:\n                point = tuple(partial_point)\n                if point not in memo_f:\n                    memo_f[point] = f_func(point)\n                return memo_f[point]\n\n            k = l_tuple[dim]\n            y_i = y_tuple[dim]\n\n            if k == 1:\n                # Grid is {0, 1}\n                weight = y_i\n                if abs(weight) < 1e-15: return recursive_eval(dim + 1, partial_point + [0.0])\n                if abs(weight - 1.0) < 1e-15: return recursive_eval(dim + 1, partial_point + [1.0])\n                \n                val_left = recursive_eval(dim + 1, partial_point + [0.0])\n                val_right = recursive_eval(dim + 1, partial_point + [1.0])\n                return (1.0 - weight) * val_left + weight * val_right\n            \n            m = 1 << (k - 1)  # 2**(k-1)\n            \n            if abs(y_i - 1.0) < 1e-15:\n                return recursive_eval(dim + 1, partial_point + [1.0])\n\n            pos = y_i * m\n            j = int(pos)\n            weight = pos - j\n\n            left_coord = j / m\n            \n            if weight < 1e-15:\n                return recursive_eval(dim + 1, partial_point + [left_coord])\n\n            right_coord = (j + 1) / m\n            \n            val_left = recursive_eval(dim + 1, partial_point + [left_coord])\n            val_right = recursive_eval(dim + 1, partial_point + [right_coord])\n            return (1.0 - weight) * val_left + weight * val_right\n\n        return recursive_eval(0, [])\n\n    results = []\n    for case in test_cases:\n        f_func = case['f']\n        alpha = case['alpha']\n        Q = case['Q']\n        eval_points = case['eval_points']\n        \n        case_key = (alpha, Q)\n        \n        if case_key in memo_indices:\n            index_set = memo_indices[case_key]\n        else:\n            index_set = generate_indices(alpha, Q)\n            memo_indices[case_key] = index_set\n\n        if case_key in memo_coeffs:\n            coeffs = memo_coeffs[case_key]\n        else:\n            coeffs = calculate_coeffs(index_set)\n            memo_coeffs[case_key] = coeffs\n        \n        memo_f = {}\n        max_abs_error = 0.0\n\n        for y_tuple in eval_points:\n            approx_val = 0.0\n            for l_tuple in index_set:\n                c_l = coeffs[l_tuple]\n                if abs(c_l) < 1e-15:\n                    continue\n                \n                u_l_f_y = get_interpolant_evaluator(f_func, y_tuple, l_tuple, memo_f)\n                approx_val += c_l * u_l_f_y\n            \n            true_val = f_func(y_tuple)\n            abs_error = abs(approx_val - true_val)\n            \n            if abs_error > max_abs_error:\n                max_abs_error = abs_error\n        \n        results.append(max_abs_error)\n\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```"}]}