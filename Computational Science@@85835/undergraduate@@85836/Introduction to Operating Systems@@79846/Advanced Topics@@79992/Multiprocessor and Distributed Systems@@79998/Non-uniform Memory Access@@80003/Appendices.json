{"hands_on_practices": [{"introduction": "This first exercise provides a quantitative foundation for understanding the core trade-off in Non-Uniform Memory Access (NUMA) systems. We will analyze a common strategy—replicating read-mostly data—to reduce memory access latency at the cost of increased memory consumption. By calculating the expected latency and memory overhead, you will develop a clear, data-driven perspective on when such optimizations are justified [@problem_id:3663572].", "problem": "A multiprocessor machine uses Non-Uniform Memory Access (NUMA), where the time to access memory depends on whether a thread accesses memory local to its own node or memory attached to a different node. Consider a read-mostly configuration key-value data structure used by threads uniformly distributed across nodes. Assume the following scientifically realistic parameters and conditions:\n\n- There are $8$ NUMA nodes. Threads are uniformly distributed across the $8$ nodes.\n- The average local DRAM access latency is $80$ nanoseconds, denoted $L_{\\text{loc}} = 80~\\text{ns}$.\n- The average remote DRAM access latency (to any non-local node) is $150$ nanoseconds, denoted $L_{\\text{rem}} = 150~\\text{ns}$.\n- Each configuration lookup experiences a single DRAM-dominant cache miss whose latency equals the average memory access latency for that lookup.\n- The baseline places a single shared copy of the configuration store entirely in the memory of a single node by first-touch allocation. Under the baseline, a lookup is local only if the executing thread happens to be on that node.\n- A proposed read-replica strategy keeps one read-only copy of the configuration store on every node. Replication is done at boot and there are no runtime write costs. Under this strategy, lookups are always served from local memory on each node.\n- The configuration store has size $64$ mebibytes (MiB), denoted $S = 64~\\text{MiB}$.\n\nUsing only the core definition of expected value and the distinction between local and remote DRAM access in a NUMA system, do the following:\n\n1. Derive an expression for the expected per-lookup latency under the baseline and under the per-node replica strategy.\n2. From these expressions, derive the average per-lookup latency reduction due to replication.\n3. Compute the additional memory overhead (in MiB) introduced by keeping one copy per node instead of a single copy.\n4. Define the ratio $\\rho$ as the average per-lookup latency reduction (in microseconds) divided by the additional memory overhead (in MiB). Compute $\\rho$.\n\nExpress the final ratio $\\rho$ in microseconds per mebibyte (MiB) and round your answer to four significant figures.", "solution": "The user wants to analyze the performance trade-offs of memory replication in a Non-Uniform Memory Access (NUMA) system. I will first validate the problem statement.\n\n### Step 1: Extract Givens\n- Number of NUMA nodes, $N = 8$.\n- Threads are uniformly distributed across the $8$ nodes.\n- Average local DRAM access latency, $L_{\\text{loc}} = 80~\\text{ns}$.\n- Average remote DRAM access latency, $L_{\\text{rem}} = 150~\\text{ns}$.\n- Each lookup results in a single cache miss with a latency equal to the average memory access latency.\n- Baseline strategy: A single copy of the configuration store is located on one node.\n- Replica strategy: One read-only copy of the store is located on every node.\n- Size of the configuration store, $S = 64~\\text{MiB}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, as NUMA is a standard computer architecture, and the provided latency values ($80~\\text{ns}$ and $150~\\text{ns}$) are realistic for modern systems. The problem is well-posed; it provides all necessary constants and a clear set of assumptions (e.g., uniform thread distribution) to allow for a unique, computable solution. The language is objective and precise. The problem does not violate any fundamental principles, is not based on false premises, is not metaphorical, is directly relevant to the topic of operating systems, and is not missing any critical information. The setup is self-consistent and leads to a meaningful analysis of the trade-off between memory overhead and latency reduction, a core concept in systems performance engineering.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with a full solution.\n\nThe problem requires a four-part analysis: deriving expected latencies, computing the latency reduction, calculating memory overhead, and finally, computing the ratio of latency reduction to memory overhead.\n\nLet $N$ be the number of NUMA nodes, so $N=8$. Let $L_{\\text{loc}}$ be the local access latency, $L_{\\text{loc}} = 80~\\text{ns}$, and $L_{\\text{rem}}$ be the remote access latency, $L_{\\text{rem}} = 150~\\text{ns}$. The data store size is $S = 64~\\text{MiB}$.\n\n**1. Expected Per-Lookup Latency**\n\nThe expected latency of an operation is the sum of the latencies of possible outcomes, weighted by their probabilities. We denote the expected latency for a given strategy as $E[L]$.\n\n- **Per-Node Replica Strategy:**\nIn this strategy, a full copy of the configuration store resides on each of the $N=8$ nodes. Since threads are distributed across all nodes, a thread on any given node will always find a local copy of the data. Therefore, every memory access is a local access. The latency for every lookup is deterministically $L_{\\text{loc}}$.\nThe expected latency under the replica strategy, $E[L_{\\text{rep}}]$, is:\n$$E[L_{\\text{rep}}] = L_{\\text{loc}} = 80~\\text{ns}$$\n\n- **Baseline Strategy:**\nIn the baseline, a single copy of the store is placed on one specific node (the \"memory node\"). Since threads are uniformly distributed across all $N=8$ nodes, the probability that a thread making a request is on the same node as the memory is $P(\\text{local}) = \\frac{1}{N}$. The probability that the thread is on a different node is $P(\\text{remote}) = 1 - \\frac{1}{N} = \\frac{N-1}{N}$.\nThe expected latency under the baseline strategy, $E[L_{\\text{base}}]$, is given by the law of total expectation:\n$$E[L_{\\text{base}}] = P(\\text{local}) \\cdot L_{\\text{loc}} + P(\\text{remote}) \\cdot L_{\\text{rem}}$$\n$$E[L_{\\text{base}}] = \\frac{1}{N} L_{\\text{loc}} + \\frac{N-1}{N} L_{\\text{rem}}$$\nSubstituting the given values:\n$$E[L_{\\text{base}}] = \\frac{1}{8} (80~\\text{ns}) + \\frac{8-1}{8} (150~\\text{ns}) = \\frac{1}{8} (80~\\text{ns}) + \\frac{7}{8} (150~\\text{ns})$$\n$$E[L_{\\text{base}}] = 10~\\text{ns} + \\frac{1050}{8}~\\text{ns} = 10~\\text{ns} + 131.25~\\text{ns} = 141.25~\\text{ns}$$\n\n**2. Average Per-Lookup Latency Reduction**\n\nThe average latency reduction, $\\Delta L$, is the difference between the expected latency of the baseline strategy and the replica strategy.\n$$\\Delta L = E[L_{\\text{base}}] - E[L_{\\text{rep}}]$$\n$$\\Delta L = 141.25~\\text{ns} - 80~\\text{ns} = 61.25~\\text{ns}$$\nSymbolically, the reduction is:\n$$\\Delta L = \\left(\\frac{1}{N} L_{\\text{loc}} + \\frac{N-1}{N} L_{\\text{rem}}\\right) - L_{\\text{loc}} = \\frac{N-1}{N} L_{\\text{rem}} - \\left(1 - \\frac{1}{N}\\right) L_{\\text{loc}} = \\frac{N-1}{N} (L_{\\text{rem}} - L_{\\text{loc}})$$\nUsing the symbolic form with our values:\n$$\\Delta L = \\frac{8-1}{8} (150~\\text{ns} - 80~\\text{ns}) = \\frac{7}{8} (70~\\text{ns}) = 61.25~\\text{ns}$$\nThe problem requires this value in microseconds for the final ratio. Since $1~\\mu s = 1000~\\text{ns}$:\n$$\\Delta L = 61.25 \\times 10^{-3}~\\mu s = 0.06125~\\mu s$$\n\n**3. Additional Memory Overhead**\n\nThe memory overhead is the additional memory consumed by the replica strategy compared to the baseline.\n- Memory used in baseline: $M_{\\text{base}} = S = 64~\\text{MiB}$.\n- Memory used in replica strategy: $M_{\\text{rep}} = N \\times S = 8 \\times 64~\\text{MiB} = 512~\\text{MiB}$.\n\nThe additional memory overhead, $\\Delta M$, is:\n$$\\Delta M = M_{\\text{rep}} - M_{\\text{base}} = NS - S = (N-1)S$$\nSubstituting the given values:\n$$\\Delta M = (8-1) \\times 64~\\text{MiB} = 7 \\times 64~\\text{MiB} = 448~\\text{MiB}$$\n\n**4. Ratio $\\rho$**\n\nThe ratio $\\rho$ is defined as the average per-lookup latency reduction (in microseconds) divided by the additional memory overhead (in MiB).\n$$\\rho = \\frac{\\Delta L \\text{ (in } \\mu s)}{\\Delta M \\text{ (in MiB)}}$$\nUsing the values calculated in the previous steps:\n$$\\rho = \\frac{0.06125}{448}~\\frac{\\mu s}{\\text{MiB}}$$\nNow, we perform the division:\n$$\\rho \\approx 0.00013671875~\\frac{\\mu s}{\\text{MiB}}$$\nThe problem requires the answer to be rounded to four significant figures.\nThe first four significant figures are $1$, $3$, $6$, and $7$. The following digit is $1$, so we round down.\n$$\\rho \\approx 0.0001367~\\frac{\\mu s}{\\text{MiB}}$$\nExpressing this result in scientific notation:\n$$\\rho = 1.367 \\times 10^{-4}~\\frac{\\mu s}{\\text{MiB}}$$", "answer": "$$\\boxed{1.367 \\times 10^{-4}}$$", "id": "3663572"}, {"introduction": "Once we understand the cost of remote memory access, the next challenge is to detect when it becomes a problem in a running application. This practice challenges you to design a scientifically sound experiment to monitor NUMA imbalance using hardware performance counters. You will learn to define robust metrics and create a decision rule for triggering optimizations like page migration, moving from static analysis to dynamic, real-world performance engineering [@problem_id:3663563].", "problem": "An operating system engineer is investigating Non-Uniform Memory Access (NUMA) imbalance on a shared-memory server. Non-Uniform Memory Access (NUMA) means that the latency and bandwidth of memory accesses depend on the relative location of the core and the memory controller that serves the data. Accesses to memory attached to the same socket (local) have lower latency and higher bandwidth than accesses served by another socket via an interconnect (remote). Modern processors expose hardware performance monitoring unit (PMU) events that can classify last-level-cache (LLC) miss services by the memory controller domain that fulfilled them, making it possible to count local versus remote dynamic random-access memory (DRAM) services.\n\nConsider a parallel application with threads pinned to specific cores on a symmetric $2$-socket NUMA machine. The engineer wants to design an experiment that uses hardware performance counters to detect persistent NUMA imbalance at run time and to trigger page migration when warranted. The design must:\n\n- Use PMU events that distinguish DRAM services of LLC misses by home node (local versus remote).\n- Aggregate measurements across threads over fixed-length windows of duration $\\Delta t$ while the application is in steady state, with threads pinned and no changes in thread count within a window.\n- Define two metrics over a window: $R$ (remote DRAM services of LLC misses) and $L$ (local DRAM services of LLC misses).\n- Make a decision using a rule of the form “remote-to-local imbalance exceeds a threshold” to trigger migration, while guarding against noise from very small sample sizes.\n\nWhich option below gives a scientifically sound experimental design that correctly defines $R$ and $L$ from PMU data, accounts for confounding factors, and specifies a robust decision rule based on a threshold $\\theta$?\n\nA. Pin threads to cores. For each window of length $\\Delta t$, read per-core PMU events that count LLC misses to remote DRAM and LLC misses to local DRAM, and sum them across all application threads to obtain $R$ and $L$. Define the ratio $\\rho = R/L$. Trigger page migration for a thread’s hot pages if $\\rho > \\theta$ is observed for $k \\ge 2$ consecutive windows and the total DRAM services satisfy $R+L \\ge M$, where $k$ and $M$ are engineer-chosen constants to limit noise. Do not include LLC hits or cache-to-cache transfers in $R$ or $L$. Keep thread placement fixed during measurement windows.\n\nB. Do not pin threads. For each window of length $\\Delta t$, read the total inter-socket interconnect bytes as $R$ and the total retired instructions as $L$. If $R/L > \\theta$, migrate all pages of the process to the node that currently retires more instructions. Cache hits can be ignored because they correlate with instructions retired.\n\nC. Pin threads. For each window of length $\\Delta t$, define $R$ as the number of LLC hits on lines whose last writer was a remote socket (cache-to-cache transfers), and $L$ as the number of LLC hits. If $R/L > \\theta$, migrate pages recently touched by threads with the highest LLC hit counts. Treat cache-to-cache transfers as remote DRAM because they traverse the interconnect.\n\nD. Pin threads. For each window of length $\\Delta t$, define $R$ as the number of minor page faults on pages whose NUMA node does not match the running thread’s node and $L$ as the number of minor page faults on matching-node pages. If $R/L > \\theta$, migrate the pages that faulted remotely. Page faults are sufficient to characterize steady-state memory locality.\n\nE. Pin threads. For each window of length $\\Delta t$, read per-core PMU events that count DRAM read bytes to remote nodes as $R$ and DRAM read bytes to local nodes as $L$. Trigger migration when $R/L > \\theta$ in a single window, regardless of total sample size, and migrate all pages of the process at once to the node with larger $L$ to maximize locality. Writes are ignored to reduce overhead.\n\nSelect the best option(s).", "solution": "The problem statement is valid. It describes a realistic and well-defined engineering challenge in the domain of operating systems and computer architecture. The premises are scientifically sound, the terminology is precise, and the requirements for a solution are clearly specified. The problem is self-contained and free of contradictions or ambiguities.\n\nA scientifically sound experimental design to detect and react to NUMA imbalance, as described in the problem, must satisfy several key criteria:\n\n1.  **Correct Metrics ($R$ and $L$):** The metrics must directly measure the phenomenon of interest. The problem explicitly defines them as counts of last-level-cache (LLC) misses serviced by remote and local DRAM, respectively. These must be obtained using the specified hardware performance monitoring unit (PMU) events. Any design using other metrics, such as LLC hits, cache-to-cache transfers, or page faults, is invalid.\n2.  **Stable Measurement Context:** To meaningfully distinguish between \"local\" and \"remote\" accesses, the location of the thread relative to the memory must be stable during the measurement window. This necessitates pinning threads to specific cores on a given socket. Failure to pin threads would confound the data, as a thread could migrate between sockets, changing which memory is local.\n3.  **Robust Decision Rule:** The rule for triggering an action must be resistant to noise and transient behavior. A simple threshold crossing in a single time window is not robust. A sound design would incorporate mechanisms to ensure the detected imbalance is both statistically significant (e.g., based on a sufficiently large sample of memory accesses) and persistent over time (e.g., observed across multiple consecutive windows).\n4.  **Appropriate Action:** The corrective action should be targeted and effective. Upon detecting a NUMA imbalance, migrating the specific memory pages that are being accessed remotely (\"hot\" pages) to the NUMA node of the accessing thread(s) is the most direct and efficient solution. Migrating all pages of a process is often a blunt and overly disruptive approach.\n\nBased on these principles, we can now evaluate each of the given options.\n\n### Option-by-Option Analysis\n\n**A. Pin threads to cores. For each window of length $\\Delta t$, read per-core PMU events that count LLC misses to remote DRAM and LLC misses to local DRAM, and sum them across all application threads to obtain $R$ and $L$. Define the ratio $\\rho = R/L$. Trigger page migration for a thread’s hot pages if $\\rho > \\theta$ is observed for $k \\ge 2$ consecutive windows and the total DRAM services satisfy $R+L \\ge M$, where $k$ and $M$ are engineer-chosen constants to limit noise. Do not include LLC hits or cache-to-cache transfers in $R$ or $L$. Keep thread placement fixed during measurement windows.**\n\n-   **Metrics:** The definitions of $R$ and $L$ as sums of PMU counts for remote and local DRAM services of LLC misses are perfectly aligned with the problem's requirements. It correctly excludes LLC hits and cache-to-cache transfers.\n-   **Context:** It correctly specifies pinning threads and keeping the placement fixed.\n-   **Decision Rule:** The rule is exceptionally robust. It uses the required ratio $\\rho = R/L$. It guards against transient spikes by requiring the condition to hold for $k \\ge 2$ consecutive windows. It guards against noise from small sample sizes by requiring a minimum total number of events, $R+L \\ge M$.\n-   **Action:** The action, \"trigger page migration for a thread’s hot pages,\" is targeted and appropriate.\n\n**Verdict:** **Correct**. This option describes a comprehensive and scientifically sound experimental design that meets all the criteria specified in the problem statement.\n\n**B. Do not pin threads. For each window of length $\\Delta t$, read the total inter-socket interconnect bytes as $R$ and the total retired instructions as $L$. If $R/L > \\theta$, migrate all pages of the process to the node that currently retires more instructions. Cache hits can be ignored because they correlate with instructions retired.**\n\n-   **Metrics:** The definitions of $R$ and $L$ are incorrect. $R$ is defined as interconnect traffic in bytes, which includes more than just remote DRAM accesses (e.g., cache-to-cache traffic). $L$ is defined as retired instructions, which measures computation, not local memory access. This violates the core requirement of the problem.\n-   **Context:** It explicitly states \"Do not pin threads,\" which is a fatal flaw as it makes the distinction between local and remote accesses unstable and difficult to interpret over a time window.\n-   **Decision Rule:** The rule $R/L > \\theta$ is based on flawed metrics and does not guard against noise or small samples.\n-   **Action:** Migrating all pages is a heavyweight action, and the heuristic of moving to the node with more retired instructions is not necessarily optimal for memory locality.\n\n**Verdict:** **Incorrect**. This design is flawed in its fundamental assumptions and choices of metrics.\n\n**C. Pin threads. For each window of length $\\Delta t$, define $R$ as the number of LLC hits on lines whose last writer was a remote socket (cache-to-cache transfers), and $L$ as the number of LLC hits. If $R/L > \\theta$, migrate pages recently touched by threads with the highest LLC hit counts. Treat cache-to-cache transfers as remote DRAM because they traverse the interconnect.**\n\n-   **Metrics:** The definitions of $R$ and $L$ are incorrect. The problem requires measuring LLC misses serviced by DRAM, not LLC hits or cache-to-cache transfers. Conflating cache-to-cache transfers with DRAM accesses is a conceptual error, especially when the specified PMU capabilities can distinguish them.\n-   **Decision Rule:** The rule $R/L > \\theta$ is simplistic and fails to guard against noise or small sample sizes, a specific requirement of the design.\n-   **Action:** The proposed action is illogical. A high LLC hit count is a sign of good data locality; migrating pages for threads with high hit counts is counter-productive. One should focus on threads causing misses.\n\n**Verdict:** **Incorrect**. The metrics are wrong, the decision rule is not robust, and the proposed action is based on flawed reasoning.\n\n**D. Pin threads. For each window of length $\\Delta t$, define $R$ as the number of minor page faults on pages whose NUMA node does not match the running thread’s node and $L$ as the number of minor page faults on matching-node pages. If $R/L > \\theta$, migrate the pages that faulted remotely. Page faults are sufficient to characterize steady-state memory locality.**\n\n-   **Metrics:** The definitions of $R$ and $L$ are based on page faults. This is incorrect. Page faults are OS-level events that are far too infrequent in most steady-state parallel applications to serve as a reliable proxy for overall memory access locality. The vast majority of memory accesses are handled directly by hardware (TLB, caches, memory controllers) without OS intervention. The assertion that page faults are sufficient is factually wrong. The problem specifies using PMU events for LLC misses for precisely this reason.\n-   **Decision Rule:** The rule $R/L > \\theta$ is not robust, lacking guards against noise.\n\n**Verdict:** **Incorrect**. The choice of page faults as the primary metric is scientifically unsound for characterizing steady-state NUMA effects as described.\n\n**E. Pin threads. For each window of length $\\Delta t$, read per-core PMU events that count DRAM read bytes to remote nodes as $R$ and DRAM read bytes to local nodes as $L$. Trigger migration when $R/L > \\theta$ in a single window, regardless of total sample size, and migrate all pages of the process at once to the node with larger $L$ to maximize locality. Writes are ignored to reduce overhead.**\n\n-   **Metrics:** The metrics are DRAM bytes, not event counts of LLC misses as specified. Ignoring writes can also be a significant omission, as write traffic can contribute to NUMA imbalance.\n-   **Decision Rule:** The rule is explicitly not robust: \"in a single window, regardless of total sample size.\" This directly violates the problem's requirement to \"guard against noise from very small sample sizes.\"\n-   **Action:** The action is poor. Migrating all pages is inefficient. Migrating to the node with larger $L$ (local accesses) is nonsensical; the goal is to resolve remote accesses, not reinforce local ones.\n\n**Verdict:** **Incorrect**. This option fails critically on the robustness of its decision rule and proposes a questionable and inefficient migration strategy.", "answer": "$$\\boxed{A}$$", "id": "3663563"}, {"introduction": "This final practice integrates your knowledge into a realistic case study involving a memory leak on a production server. You will analyze how different operating system-level memory policies and resource-containment mechanisms, such as control groups, dictate system behavior under stress. By predicting the outcomes of different configurations, you will see how NUMA-aware resource management is crucial for building resilient and performant systems [@problem_id:3663644].", "problem": "Consider a production web server deployed on a machine with Non-Uniform Memory Access (NUMA). The system has $N=2$ nodes: Node $0$ and Node $1$, each with $128$ gibibytes (GiB) of physical memory. The server uses $W=16$ worker processes, with $8$ workers pinned to Central Processing Unit (CPU) cores on Node $0$ and $8$ pinned to CPU cores on Node $1$. The operating system uses a first-touch page allocation policy: pages are allocated on the NUMA node where the allocating thread first touches the memory, subject to the process’s memory policy.\n\nAll workers are identical except for a bug that introduces a memory leak confined to the $8$ workers on Node $0$. Each Node $0$ worker has a steady-state working set of $3$ GiB and additionally leaks $0.5$ GiB per minute of anonymous, unswappable memory. Assume swap is disabled. The operating system’s own memory usage on Node $0$ is approximately $4$ GiB. The workload is largely compute and network bound with negligible file cache, so page cache reclaim does not materially slow the leak’s consumption of free memory.\n\nTwo experiments are conducted:\n\n- Experiment $1$: Node $0$ workers are placed in a memory control group (memcg) limited to Node $0$’s capacity and configured with a strict memory policy (MPOL\\_BIND) allowing allocations only on Node $0$ (cpuset.mems restricted to Node $0$). Node $1$ workers use Node $1$ without leaks and have no special limits other than being pinned to Node $1$ CPUs.\n\n- Experiment $2$: Node $0$ workers are configured with a preferred memory policy (MPOL\\_PREFERRED) to Node $0$ but permitted to allocate from Node $1$ when Node $0$ is out of free memory (cpuset.mems includes Node $0$ and Node $1$). The memcg for Node $0$ workers allows growth across both nodes up to the combined capacity.\n\nBased on fundamental definitions of Non-Uniform Memory Access (NUMA), first-touch allocation, memory policies, and memory control groups (memcg), select all statements that are correct.\n\nA. In Experiment $1$, when Node $0$ exhausts free memory, the Out Of Memory (OOM) event is confined to the Node $0$ memory control group, and allocations for its workers do not spill into Node $1$.\n\nB. In Experiment $2$, once Node $0$ exhausts free memory, subsequent allocations for Node $0$ workers are satisfied from Node $1$, and their average memory access time increases due to remote accesses.\n\nC. Given the leak rate and initial free memory on Node $0$, the time at which Node $0$ first stops allocating locally and begins falling back to remote pages in Experiment $2$ is approximately $25$ minutes, assuming the leak uses anonymous unswappable memory and swap is disabled.\n\nD. In Experiment $1$, the Linux Out Of Memory (OOM) killer terminates workers on both nodes because OOM decisions are always system-wide and ignore memory control groups and allowed node sets.\n\nE. In Experiment $2$, Node $1$ workers are guaranteed to see no throughput degradation during remote spillover because their pages remain local; cross-node traffic does not contend for the Node $1$ memory controllers or interconnect.", "solution": "Herein, I will first validate the problem statement and, finding it valid, proceed to a detailed analysis of the system's behavior and an evaluation of each option.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   System Architecture: Non-Uniform Memory Access (NUMA)\n*   Number of NUMA nodes: $N=2$ (Node $0$ and Node $1$)\n*   Memory per node: $128$ gibibytes (GiB)\n*   Total worker processes: $W=16$\n*   Worker pinning: $8$ workers pinned to CPU cores on Node $0$; $8$ workers pinned to CPU cores on Node $1$.\n*   OS allocation policy: first-touch page allocation.\n*   Memory leak: Confined to the $8$ workers on Node $0$.\n*   Node $0$ worker memory profile:\n    *   Steady-state working set: $3$ GiB per worker.\n    *   Leak rate: $0.5$ GiB per minute per worker.\n    *   Leaked memory type: anonymous, unswappable.\n*   Swap: disabled.\n*   OS memory usage on Node $0$: approximately $4$ GiB.\n*   File cache: negligible impact.\n*   Experiment $1$ configuration for Node $0$ workers:\n    *   Memory control group (memcg) limited to Node $0$'s capacity.\n    *   Strict memory policy (MPOL_BIND) allowing allocations only on Node $0$.\n    *   `cpuset.mems` is restricted to Node $0$.\n*   Experiment $2$ configuration for Node $0$ workers:\n    *   Preferred memory policy (MPOL_PREFERRED) to Node $0$.\n    *   Permitted to allocate from Node $1$ on fallback.\n    *   `cpuset.mems` includes Node $0$ and Node $1$.\n    *   The memcg allows growth across both nodes.\n\n**Step 2: Validate Using Extracted Givens**\n\n*   **Scientifically Grounded:** The problem is firmly rooted in the principles of modern computer architecture and operating systems, specifically NUMA, CPU/memory pinning (`cpuset`), memory allocation policies (`mbind` with `MPOL_BIND`/`MPOL_PREFERRED`), and resource isolation (`memcg`). These are all well-defined, real-world mechanisms, particularly in the Linux kernel.\n*   **Well-Posed:** The problem is clearly stated with two distinct experimental setups. The provided numerical data is sufficient to analyze a time-dependent behavior (the memory leak). The questions posed relate directly to the expected outcomes of the described configurations.\n*   **Objective:** The language is technical and precise. It describes system configurations and behaviors without subjectivity or ambiguity. The terms used have specific, formal meanings in the context of operating systems.\n*   **Consistency and Completeness:** The setup is self-consistent. The parameters (memory sizes, worker counts, leak rates) are provided and allow for quantitative analysis. For instance, the total memory, OS overhead, and initial worker sets are specified, allowing a calculation of the initial free memory. There are no contradictions.\n*   **Realism:** The scenario is highly realistic. Production servers often run on NUMA hardware, and memory leaks are a common class of software bug. The use of cgroups (`memcg`, `cpuset`) to manage workloads is standard practice in containerized and virtualized environments. The described policies and their effects are true to life.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is valid. It is scientifically sound, well-posed, objective, and complete. I will proceed with a full solution.\n\n### Derivation and Option Analysis\n\nFirst, I will establish the initial state of memory on Node $0$.\n*   Total memory on Node $0$: $128$ GiB.\n*   Memory consumed by the operating system on Node $0$: $4$ GiB.\n*   Initial steady-state memory consumed by the $8$ workers on Node $0$: $8 \\text{ workers} \\times 3 \\text{ GiB/worker} = 24$ GiB.\n*   Total initial memory usage on Node $0$: $4 \\text{ GiB} + 24 \\text{ GiB} = 28$ GiB.\n*   Initial free memory on Node $0$ available to be consumed by the leak: $128 \\text{ GiB} - 28 \\text{ GiB} = 100$ GiB.\n\nNext, I will determine the total rate of memory consumption by the leak.\n*   Total leak rate from $8$ workers on Node $0$: $8 \\text{ workers} \\times 0.5 \\text{ GiB/min/worker} = 4$ GiB/min.\n\nNow, I will analyze the behavior in each experiment and evaluate the corresponding options.\n\n**A. In Experiment 1, when Node 0 exhausts free memory, the Out Of Memory (OOM) event is confined to the Node 0 memory control group, and allocations for its workers do not spill into Node 1.**\n\n*   **Analysis:** In Experiment $1$, the Node $0$ workers are subject to two constraints that prevent allocation on Node $1$:\n    1.  `cpuset.mems` is restricted to Node $0$. This is a powerful cgroup-based constraint that forbids the kernel from allocating pages for these processes on any other node.\n    2.  The `MPOL_BIND` policy is also specified, which strictly requires allocations on the given nodemask (Node $0$). If memory is unavailable on the specified node(s), the allocation fails.\n    Consequently, when the $100$ GiB of free memory on Node $0$ is exhausted, any further allocation request from a Node $0$ worker will fail. This allocation failure will trigger an Out Of Memory (OOM) condition.\n    The problem states these workers are in a memory control group (`memcg`). A primary function of `memcg` is to isolate memory resources. When a process within a `memcg` triggers an OOM condition by exceeding the cgroup's limits, the kernel invokes the OOM killer specifically for that `memcg` (a \"memcg OOM\"). It does not trigger a system-wide OOM. Therefore, the OOM event is confined to the Node $0$ `memcg`, and processes outside this group (like the workers on Node $1$) are not considered candidates for termination. The statement is entirely consistent with the expected behavior of these Linux kernel mechanisms.\n*   **Verdict: Correct**\n\n**B. In Experiment 2, once Node 0 exhausts free memory, subsequent allocations for Node 0 workers are satisfied from Node 1, and their average memory access time increases due to remote accesses.**\n\n*   **Analysis:** In Experiment $2$, the policy is `MPOL_PREFERRED` for Node $0$, and `cpuset.mems` allows allocations on both Node $0$ and Node $1$. The \"preferred\" policy directs the kernel to attempt allocations on Node $0$ first. However, unlike `MPOL_BIND`, it allows the kernel to fall back to other allowed nodes (per `cpuset.mems`) if the preferred node is out of memory.\n    Therefore, once Node $0$ is full, subsequent allocation requests from Node $0$ workers will be serviced by allocating pages from Node $1$. Since the worker processes are running on Node $0$ CPUs, any accesses to these newly allocated pages on Node $1$ will be \"remote\" accesses. Remote memory access incurs higher latency and offers lower bandwidth compared to local memory access due to the need to traverse the NUMA interconnect (e.g., QPI or UPI). As the proportion of remote pages in a process's working set increases, its average memory access time will increase.\n*   **Verdict: Correct**\n\n**C. Given the leak rate and initial free memory on Node 0, the time at which Node 0 first stops allocating locally and begins falling back to remote pages in Experiment 2 is approximately 25 minutes, assuming the leak uses anonymous unswappable memory and swap is disabled.**\n\n*   **Analysis:** This is a quantitative claim that can be verified with the givens.\n    *   Initial free memory on Node $0$: $100$ GiB (as calculated above).\n    *   Total leak rate: $4$ GiB/min (as calculated above).\n    The time to exhaust the free memory is the total free memory divided by the rate of consumption.\n    $$ \\text{Time} = \\frac{\\text{Initial Free Memory}}{\\text{Total Leak Rate}} = \\frac{100 \\text{ GiB}}{4 \\text{ GiB/min}} = 25 \\text{ minutes} $$\n    At the $25$-minute mark, Node $0$ will have exhausted its free memory. In Experiment $2$, this is precisely the point at which the `MPOL_PREFERRED` policy will cause the kernel to begin falling back to Node $1$ for new allocations. The calculation is correct.\n*   **Verdict: Correct**\n\n**D. In Experiment 1, the Linux Out Of Memory (OOM) killer terminates workers on both nodes because OOM decisions are always system-wide and ignore memory control groups and allowed node sets.**\n\n*   **Analysis:** This statement is fundamentally incorrect. It misrepresents the purpose and function of memory control groups (`memcg`). A key feature of `memcg` is to enable per-group memory accounting and limit enforcement. When a cgroup's memory limit is breached, it triggers a `memcg`-specific OOM event. The OOM killer is then invoked only on the processes within that offending cgroup. The statement \"OOM decisions are always system-wide\" is false; `memcg` OOM events are, by design, localized. Therefore, the workers on Node $1$, which are not in the Node $0$ workers' `memcg`, would be completely unaffected by the OOM event.\n*   **Verdict: Incorrect**\n\n**E. In Experiment 2, Node 1 workers are guaranteed to see no throughput degradation during remote spillover because their pages remain local; cross-node traffic does not contend for the Node 1 memory controllers or interconnect.**\n\n*   **Analysis:** This statement is incorrect. It makes a false claim about the nature of NUMA hardware. The NUMA interconnect, which connects Node $0$ and Node $1$, is a shared resource. The memory controller on Node $1$ is also a shared resource. When Node $0$ CPUs begin accessing memory physically located on Node $1$ (the \"remote spillover\"), this traffic must traverse the interconnect and be serviced by the Node $1$ memory controller. This creates contention. The local memory requests from Node $1$ workers must compete with the remote memory requests from Node $0$ workers for access to the Node $1$ memory controller and for bandwidth on the interconnect. This contention will increase the effective memory latency for Node $1$ workers, which can lead to performance and throughput degradation. The word \"guaranteed\" makes the statement definitively false, as any level of contention violates this guarantee.\n*   **Verdict: Incorrect**", "answer": "$$\\boxed{ABC}$$", "id": "3663644"}]}