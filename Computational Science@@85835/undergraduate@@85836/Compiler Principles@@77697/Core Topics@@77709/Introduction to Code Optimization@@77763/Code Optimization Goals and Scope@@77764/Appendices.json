{"hands_on_practices": [{"introduction": "Loop unrolling is a fundamental compiler optimization that can significantly improve performance by reducing loop control overhead and increasing instruction-level parallelism. However, this performance gain comes at the direct cost of increased code size. This practice challenges you to formalize this classic trade-off, developing a mathematical model for execution time and finding the optimal unroll factor that minimizes it, all while respecting a strict code size budget.", "problem": "Consider a simple-counted loop over an induction variable $i$ with stride $s$ that executes exactly $N$ iterations. The loop body performs computation and accesses memory with address stride $s$ per iteration. Assume the following cost model on a single-core Central Processing Unit (CPU), expressed in cycles:\n- The data-processing part of the body (excluding loop control and stall) costs $b$ cycles per original iteration.\n- The loop control (increment of $i$ by $s$, comparison to a bound, and branch) costs $h$ cycles per original iteration if executed without unrolling.\n- When iterations are executed strictly serially, memory latency and pipeline hazards contribute an additional stall cost of $d(s)$ cycles per original iteration. Assume iterations are independent so that when $u$ copies of the body are unrolled and interleaved, these stalls are fully overlappable across the $u$ copies (that is, inter-iteration overlap can amortize latency).\n- Unrolling by a factor $u$ replicates the loop body $u$ times, reduces the number of loop-control executions proportionally, and leaves the data-processing work $b$ per original iteration unchanged.\n\nLet the code size of a single copy of the loop body be $S$ bytes, and suppose there is a hard budget $M$ bytes such that the generated loop must satisfy $u\\cdot S \\leq M$. Assume $M \\geq S$ so that at least the non-unrolled body fits, and that $u$ must be a positive integer not exceeding $N$.\n\nUsing only the fundamental definitions that total execution time in cycles is the product of dynamic work and per-unit cost, that loop unrolling reduces dynamic loop-control executions by a factor of $u$, and that fully overlappable stalls can be amortized uniformly across independent work, derive the execution time $T(u)$ in cycles as a function of $u$, and then determine the integer unroll factor $u^{\\star}$ that minimizes $T(u)$ subject to $u \\cdot S \\leq M$ and $1 \\leq u \\leq N$. Express your final answer as a single closed-form symbolic expression for $u^{\\star}$ in terms of $M$, $S$, and $N$.", "solution": "The problem statement is evaluated as valid. It presents a well-defined optimization problem grounded in the principles of compiler theory and computer architecture. The provided cost model is a standard, albeit simplified, representation of the performance effects of loop unrolling, suitable for analytical treatment. The objective and constraints are stated with sufficient precision to allow for a formal derivation of a unique solution.\n\nThe phrase \"reduces the number of loop-control executions proportionally\" and the requirement for a closed-form solution in terms of only $M$, $S$, and $N$ strongly indicate the intention to use a simplified continuous model for the number of loop executions, which is $\\frac{N}{u}$. This model is common in high-level performance analysis and assumes that the total number of iterations $N$ is large enough that the effects of a potential remainder loop (when $N$ is not an integer multiple of $u$) are negligible. The following derivation will proceed under this standard and contextually justified assumption.\n\nThe total execution time, $T(u)$, is the sum of the costs of its constituent parts: data processing, loop control, and stalls. We derive the total cost for each part over the $N$ original iterations as a function of the unroll factor $u$.\n\n$1$. Data-Processing Cost ($C_{proc}$): The problem states that the data-processing work costs $b$ cycles per original iteration and is unchanged by unrolling. Therefore, the total data-processing cost is constant.\n$$C_{proc} = N \\cdot b$$\n\n$2$. Loop Control Cost ($C_{ctrl}$): For an un-unrolled loop, the control overhead is incurred $N$ times. When the loop is unrolled by a factor $u$, the loop body is replicated $u$ times, and the new loop runs for $\\frac{N}{u}$ iterations. The loop control logic (increment, compare, branch), which costs $h$ cycles, is executed once per iteration of this new loop. Thus, the total loop control cost is:\n$$C_{ctrl}(u) = \\frac{N}{u} \\cdot h$$\n\n$3$. Stall Cost ($C_{stall}$): In the serial case, each iteration incurs a stall of $d(s)$ cycles. When $u$ iterations are unrolled, the stalls are \"fully overlappable\". This means that for each block of $u$ original iterations, the total stall cost is $d(s)$, not $u \\cdot d(s)$. Since there are $\\frac{N}{u}$ such blocks in total, the total stall cost is:\n$$C_{stall}(u) = \\frac{N}{u} \\cdot d(s)$$\n\nThe total execution time $T(u)$ is the sum of these components.\n$$T(u) = C_{proc} + C_{ctrl}(u) + C_{stall}(u) = N \\cdot b + \\frac{N}{u} \\cdot h + \\frac{N}{u} \\cdot d(s)$$\nWe can factor this expression to isolate the terms dependent on $u$:\n$$T(u) = N \\left( b + \\frac{h + d(s)}{u} \\right)$$\n\nNext, we must find the integer unroll factor $u^{\\star}$ that minimizes $T(u)$ subject to the given constraints.\nThe constraints on $u$ are:\n- $u$ must be a positive integer: $u \\in \\{1, 2, 3, \\dots\\}$.\n- The code size of the unrolled loop body must not exceed the budget $M$: $u \\cdot S \\leq M$. Since $u$ must be an integer, this is equivalent to $u \\leq \\left\\lfloor \\frac{M}{S} \\right\\rfloor$.\n- The unroll factor cannot be greater than the number of iterations: $u \\leq N$.\n\nCombining these constraints, the set of feasible integer values for $u$ is given by $1 \\leq u \\leq \\min\\left(N, \\left\\lfloor \\frac{M}{S} \\right\\rfloor\\right)$. The problem states $M \\geq S$, which ensures that $\\left\\lfloor \\frac{M}{S} \\right\\rfloor \\geq 1$, so this range is never empty.\n\nTo minimize $T(u) = N \\left( b + \\frac{h + d(s)}{u} \\right)$, we analyze its dependence on $u$. The parameters $N$, $b$, $h$, and $d(s)$ are non-negative constants for a given loop. We assume that $h + d(s) > 0$, as otherwise $T(u)$ would be independent of $u$ and the optimization would be trivial. With $h + d(s) > 0$, the term $\\frac{h+d(s)}{u}$ is a strictly decreasing function of $u$ for $u > 0$. Consequently, the entire expression for $T(u)$ is a strictly decreasing function of $u$.\n\nTo minimize a strictly decreasing function over a closed interval of integers, one must choose the largest integer value in that interval. The largest permissible integer value for $u$ is the upper bound of the feasible set.\n$$u_{max} = \\min\\left(N, \\left\\lfloor \\frac{M}{S} \\right\\rfloor\\right)$$\nTherefore, the optimal unroll factor $u^{\\star}$ is this maximum allowed value.\n$$u^{\\star} = \\min\\left(N, \\left\\lfloor \\frac{M}{S} \\right\\rfloor\\right)$$\nThis expression gives the integer unroll factor that minimizes the execution time under the given model and constraints.", "answer": "$$\\boxed{\\min\\left(N, \\left\\lfloor \\frac{M}{S} \\right\\rfloor\\right)}$$", "id": "3628478"}, {"introduction": "In many real-world scenarios, a compiler must balance multiple, often conflicting, optimization goals rather than adhering to a single hard constraint. A common way to formalize this is by defining a cost function that weighs the relative importance of different metrics, such as execution speed and code size. This exercise guides you through this advanced concept by modeling the effects of tail duplication, a technique that can improve branch prediction, and then using calculus to determine the optimal degree of optimization that minimizes a weighted objective function.", "problem": "Consider a Control Flow Graph (CFG) for a hot conditional branch that is executed $n$ times per run. The branch predicts one of two successor paths that reconverge at a join region. A compiler pass applies tail duplication: it duplicates the first $L$ instructions from the join region into each successor to specialize each path. Assume the following empirically supported relationships hold due to improved path correlation and layout following duplication:\n- The branch misprediction probability changes from $p_0$ (without duplication) to $p(L) = p_0 \\exp(-\\alpha L)$ for some $\\alpha > 0$.\n- The code size changes from $S_0$ (without duplication) to $S(L) = S_0 + \\delta L$ for some $\\delta > 0$.\n- The expected execution time for the entire run changes from $T_0$ (without duplication) to $T(L) = T_0 + n \\tau p(L)$, where $\\tau > 0$ is the misprediction penalty in cycles.\n\nThese relationships reflect a case where tail duplication improves branch prediction quality (decreasing $p(L)$) while increasing code size $S(L)$.\n\nDefine the compiler optimization goal as minimizing a trade-off objective $C(S,T)$ that balances execution time and code size, given by\n$$\nC\\big(S(L), T(L)\\big) = w_T T(L) + w_S S(L),\n$$\nwith $w_T > 0$ and $w_S > 0$ indicating the importance weights on time and size, respectively. The scope of duplication is the continuous relaxation $L \\geq 0$.\n\nStarting only from these definitions, derive the closed-form expression for the duplication scope $L^{\\ast}$ that minimizes $C\\big(S(L), T(L)\\big)$ in the unconstrained continuous model. Assume $w_T n \\tau p_0 \\alpha > w_S \\delta$ so that the minimizer is an interior point with $L^{\\ast} > 0$. Express your final answer as a single closed-form symbolic expression for $L^{\\ast}$ with no numerical substitution.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- A conditional branch is executed $n$ times per run.\n- The duplication scope is denoted by $L$, representing the number of duplicated instructions, with $L \\geq 0$.\n- The branch misprediction probability as a function of $L$ is $p(L) = p_0 \\exp(-\\alpha L)$, where $p_0$ is the initial probability and $\\alpha > 0$ is a constant.\n- The code size as a function of $L$ is $S(L) = S_0 + \\delta L$, where $S_0$ is the initial size and $\\delta > 0$ is a constant.\n- The expected execution time as a function of $L$ is $T(L) = T_0 + n \\tau p(L)$, where $T_0$ is the initial time and $\\tau > 0$ is the misprediction penalty.\n- The objective function to minimize is $C\\big(S(L), T(L)\\big) = w_T T(L) + w_S S(L)$, with weights $w_T > 0$ and $w_S > 0$.\n- The scope is a continuous relaxation, $L \\geq 0$.\n- An explicit assumption is made: $w_T n \\tau p_0 \\alpha > w_S \\delta$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is a standard optimization exercise in the field of compiler design. It models a common trade-off between execution speed (performance) and code size (memory footprint). The functional forms chosen for misprediction probability (exponential decay) and code size (linear growth) are plausible and mathematically tractable models for the effects of an optimization like tail duplication. The problem is firmly rooted in computer science and applied mathematics.\n- **Well-Posed**: The problem is well-posed. It asks for the minimization of a clearly defined, continuous, and differentiable function over a specified domain. An additional constraint ($w_T n \\tau p_0 \\alpha > w_S \\delta$) is provided to ensure the existence of a non-trivial interior solution, which indicates careful problem construction.\n- **Objective**: All terms are defined with mathematical precision. The language is unambiguous and free of subjective content.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-defined mathematical optimization problem derived from a plausible model in compiler theory. A solution will be derived.\n\nThe objective function $C$ is a function of the duplication scope $L$. We first express $C$ solely in terms of $L$ by substituting the given definitions for $T(L)$ and $S(L)$.\nThe cost function is given as:\n$$\nC(L) = w_T T(L) + w_S S(L)\n$$\nSubstituting the expressions for $T(L)$ and $S(L)$:\n$$\nT(L) = T_0 + n \\tau p(L) = T_0 + n \\tau p_0 \\exp(-\\alpha L)\n$$\n$$\nS(L) = S_0 + \\delta L\n$$\nThus, the objective function becomes:\n$$\nC(L) = w_T \\big(T_0 + n \\tau p_0 \\exp(-\\alpha L)\\big) + w_S (S_0 + \\delta L)\n$$\nWe can rearrange this expression by separating terms that depend on $L$ from constant terms. Let $K = w_T T_0 + w_S S_0$ be the constant part of the cost.\n$$\nC(L) = K + w_T n \\tau p_0 \\exp(-\\alpha L) + w_S \\delta L\n$$\nTo find the value of $L$ that minimizes $C(L)$ for $L \\ge 0$, we apply calculus. We compute the first derivative of $C(L)$ with respect to $L$.\n$$\n\\frac{dC}{dL} = \\frac{d}{dL} \\big( K + w_T n \\tau p_0 \\exp(-\\alpha L) + w_S \\delta L \\big)\n$$\nThe derivative of the constant $K$ is zero. For the other terms:\n$$\n\\frac{dC}{dL} = w_T n \\tau p_0 \\frac{d}{dL}\\big(\\exp(-\\alpha L)\\big) + w_S \\delta \\frac{d}{dL}(L)\n$$\n$$\n\\frac{dC}{dL} = w_T n \\tau p_0 \\big(-\\alpha \\exp(-\\alpha L)\\big) + w_S \\delta\n$$\n$$\n\\frac{dC}{dL} = -w_T n \\tau p_0 \\alpha \\exp(-\\alpha L) + w_S \\delta\n$$\nTo find the critical points, we set the first derivative to zero and solve for $L$. Let the optimal value be $L^{\\ast}$.\n$$\n-w_T n \\tau p_0 \\alpha \\exp(-\\alpha L^{\\ast}) + w_S \\delta = 0\n$$\n$$\nw_S \\delta = w_T n \\tau p_0 \\alpha \\exp(-\\alpha L^{\\ast})\n$$\nWe can now solve for the exponential term:\n$$\n\\exp(-\\alpha L^{\\ast}) = \\frac{w_S \\delta}{w_T n \\tau p_0 \\alpha}\n$$\nTo solve for $L^{\\ast}$, we take the natural logarithm of both sides:\n$$\n\\ln\\big(\\exp(-\\alpha L^{\\ast})\\big) = \\ln\\left(\\frac{w_S \\delta}{w_T n \\tau p_0 \\alpha}\\right)\n$$\n$$\n-\\alpha L^{\\ast} = \\ln\\left(\\frac{w_S \\delta}{w_T n \\tau p_0 \\alpha}\\right)\n$$\nUsing the logarithmic identity $\\ln(1/x) = -\\ln(x)$:\n$$\n\\alpha L^{\\ast} = -\\ln\\left(\\frac{w_S \\delta}{w_T n \\tau p_0 \\alpha}\\right) = \\ln\\left(\\left(\\frac{w_S \\delta}{w_T n \\tau p_0 \\alpha}\\right)^{-1}\\right) = \\ln\\left(\\frac{w_T n \\tau p_0 \\alpha}{w_S \\delta}\\right)\n$$\nFinally, we solve for $L^{\\ast}$:\n$$\nL^{\\ast} = \\frac{1}{\\alpha} \\ln\\left(\\frac{w_T n \\tau p_0 \\alpha}{w_S \\delta}\\right)\n$$\nTo confirm that this critical point corresponds to a minimum, we examine the second derivative of $C(L)$:\n$$\n\\frac{d^2C}{dL^2} = \\frac{d}{dL} \\left( -w_T n \\tau p_0 \\alpha \\exp(-\\alpha L) + w_S \\delta \\right)\n$$\n$$\n\\frac{d^2C}{dL^2} = -w_T n \\tau p_0 \\alpha \\frac{d}{dL}\\big(\\exp(-\\alpha L)\\big) = -w_T n \\tau p_0 \\alpha \\big(-\\alpha \\exp(-\\alpha L)\\big)\n$$\n$$\n\\frac{d^2C}{dL^2} = w_T n \\tau p_0 \\alpha^2 \\exp(-\\alpha L)\n$$\nBy definition, all parameters $w_T, n, \\tau, p_0, \\alpha$ are positive. The term $\\alpha^2$ is also positive. The exponential function $\\exp(-\\alpha L)$ is positive for all real $L$. Consequently, $\\frac{d^2C}{dL^2} > 0$ for all $L$. This shows that the function $C(L)$ is strictly convex, and thus the critical point $L^{\\ast}$ is a unique global minimum.\n\nThe problem statement includes the assumption $w_T n \\tau p_0 \\alpha > w_S \\delta$. This implies that the ratio $\\frac{w_T n \\tau p_0 \\alpha}{w_S \\delta} > 1$. The natural logarithm of a number greater than $1$ is positive. Since $\\alpha > 0$, our expression for $L^{\\ast}$ yields a positive value, $L^{\\ast} > 0$. This confirms that the minimum occurs at an interior point of the domain $L \\geq 0$, as anticipated by the problem's setup. If this condition were not met, the argument of the logarithm would be less than or equal to $1$, yielding $L^{\\ast} \\le 0$, and the minimum would occur at the boundary $L=0$.\nThe derived expression for $L^{\\ast}$ is the closed-form solution.", "answer": "$$\n\\boxed{\\frac{1}{\\alpha} \\ln\\left(\\frac{w_T n \\tau p_0 \\alpha}{w_S \\delta}\\right)}\n$$", "id": "3628492"}, {"introduction": "The foremost goal of any compiler optimization is to preserve program correctness; performance is secondary to safety. The scope of an optimization is therefore fundamentally determined by the program's execution semantics, a boundary that becomes critically important in concurrent systems. This thought experiment explores how a standard, seemingly safe optimization for a single-threaded program can introduce a severe memory safety violation in a multi-threaded context, underscoring that correctness defines the ultimate scope for all transformations.", "problem": "Consider a shared-memory program in a simple imperative language with two threads, a shared array $A$ of capacity $N$, and a shared integer $n$ that represents the current logical length of $A$ satisfying $0 \\leq n \\leq N$. The memory model is Sequential Consistency (SC), meaning that all operations from all threads can be viewed as interleaved in a single total order that respects each thread’s program order. The compiler must preserve observable behavior under this SC model, where observable behavior includes memory safety (no out-of-bounds access), values read and written to shared locations, and control-flow decisions that depend on shared data.\n\nThread $T_1$ performs a loop that aggregates elements of $A$ subject to a per-iteration bounds check using the current value of $n$:\n- For each integer $i$ starting at $0$ and increasing by $1$ up to and including $N-1$, the loop body reads $n$ and executes: if $i < n$, then it reads $A[i]$ and adds it to a local accumulator $s$. The accumulation is a local computation and does not write to shared memory. Formally, on iteration $i$, $T_1$ evaluates the predicate $i < n$ using the value of $n$ read at that iteration and accesses $A[i]$ if and only if that predicate is true.\n\nThread $T_2$ may concurrently decrease $n$ during the execution of $T_1$ to reflect a shrink of the logical array. Furthermore, when $T_2$ decreases $n$ to some value $n'$, it may immediately reuse or invalidate positions $A[j]$ for all integers $j$ with $n' \\leq j \\leq N-1$ (for example, by transferring those positions to another pool or marking them as inaccessible). Assume that all reads and writes of $n$ and $A[\\cdot]$ are atomic in the SC sense, but there is no synchronization that establishes a happens-before relation between $T_1$ and $T_2$.\n\nA compiler considers the following optimization of $T_1$: instead of reading $n$ and checking $i < n$ on every iteration, it takes a single snapshot $n_0$ of $n$ before the loop and replaces the guarded loop by an unguarded loop that runs $i$ from $0$ to $n_0 - 1$, accessing $A[i]$ on every iteration. In other words, it transforms iteration-wise checks $i < n$ into a fixed bound $n_0$, assuming that $n$ is invariant during the loop.\n\nBased on the above scenario and the SC model, which of the following statements about the correctness of this optimization are true?\n\nA. Under Sequential Consistency, if $T_2$ can decrease $n$ during $T_1$’s loop and immediately invalidate $A[j]$ for $j \\geq n$ at the moment of decrease, then replacing per-iteration checks $i < n$ with a single pre-loop snapshot $n_0$ is not semantics-preserving, because the optimized loop may access $A[i]$ for indices $i$ such that $i \\geq n$ at the time of access, violating memory safety.\n\nB. Declaring $n$ as an atomic variable alone suffices to make the optimization sound, because atomicity ensures visibility of concurrent writes and thus the single snapshot $n_0$ is an acceptable replacement for repeated checks.\n\nC. The optimization is sound if the optimization scope is constrained to cases where there is a proof (for example, via whole-program analysis or language-level guarantees) that no thread other than $T_1$ writes to $n$ in the dynamic extent of the loop, so that $n$ is effectively invariant while the loop runs.\n\nD. The optimization is sound regardless of concurrency, because $i$ increases monotonically and $i < n$ at loop entry implies that $i < n$ for all subsequent iterations.\n\nE. The optimization is sound if the compiler’s scope is defined as intra-thread semantics that ignores concurrent writes from other threads to shared variables, since correctness can be judged by the behavior of $T_1$ alone.\n\nSelect all that apply. Justify your choice using the SC model and the stated notion of observable behavior. Your justification should make explicit use of the definitions of Sequential Consistency, observable behavior, and the role of per-iteration checks $i < n$ versus a pre-loop snapshot $n_0$ in determining which $A[i]$ are accessed.", "solution": "The problem statement is analyzed for validity before proceeding to a solution.\n\n### Step 1: Extract Givens\n- **Threads and Memory:** A shared-memory program with $2$ threads, $T_1$ and $T_2$.\n- **Shared Variables:**\n    - An array $A$ of capacity $N$.\n    - An integer $n$ representing the logical length of $A$, with $0 \\leq n \\leq N$.\n- **Memory Model:** Sequential Consistency (SC). All operations are part of a single total order that respects the program order of each thread.\n- **Observable Behavior:** The compiler must preserve memory safety (no out-of-bounds access), values read/written to shared locations, and control-flow decisions dependent on shared data.\n- **Thread $T_1$ (Original Behavior):**\n    - Executes a loop for integer $i$ from $0$ to $N-1$.\n    - In each iteration $i$, it reads $n$ and checks if $i < n$.\n    - If and only if $i < n$ is true, it reads $A[i]$ and adds it to a local accumulator $s$.\n- **Thread $T_2$ (Concurrent Behavior):**\n    - May decrease the value of $n$ to a new value $n'$.\n    - Upon decreasing $n$ to $n'$, it may immediately invalidate memory locations $A[j]$ for all $j$ such that $n' \\leq j \\leq N-1$.\n- **Atomicity and Synchronization:**\n    - All reads and writes to $n$ and elements of $A$ are atomic.\n    - There is no synchronization that establishes a happens-before relation between $T_1$ and $T_2$.\n- **The Compiler Optimization:**\n    - The loop in $T_1$ is transformed.\n    - A single snapshot $n_0$ of $n$ is taken before the loop begins.\n    - The loop is changed to iterate from $i=0$ to $n_0-1$.\n    - Inside this new loop, $A[i]$ is accessed unconditionally.\n    - This optimization effectively assumes $n$ is loop-invariant.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is grounded in the well-established domain of compiler construction and concurrent programming. The concepts of Sequential Consistency, data races, atomic operations, and loop-invariant code motion are fundamental topics in computer science. The scenario described is a classic example of a Time-of-check-to-time-of-use (TOCTOU) race condition introduced by a seemingly correct optimization.\n- **Well-Posed:** The problem provides a clear and formal description of the system, the memory model, the behavior of the threads, and the specific optimization being considered. The question asks for an evaluation of the optimization's correctness based on these precise definitions. A definite answer can be derived.\n- **Objective:** The language used is formal and unambiguous. Terms like \"Sequential Consistency\", \"atomic\", and \"observable behavior\" have precise technical meanings.\n\nThe problem statement has no scientific or factual unsoundness, is formalizable, is complete, describes a realistic scenario in systems programming, and is well-posed.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A solution will be derived.\n\n### Derivation of Solution\nThe correctness of the optimization hinges on whether all observable behaviors of the optimized program are also legal behaviors of the original program under the Sequential Consistency (SC) model. Observable behavior explicitly includes memory safety.\n\nLet's analyze the behavior of the original program.\nIn the original program, Thread $T_1$ executes a loop from $i=0$ to $N-1$. In each iteration $i$, it performs two distinct serial operations:\n1.  Read the current value of the shared variable $n$.\n2.  Based on the value read, decide whether to access $A[i]$.\n\nConsider a possible interleaving under SC. Let the initial state be $n=k$ for some $k > 0$.\n- $T_1$ starts its loop. For $i=0, 1, \\dots, m-1$ (where $m < k$), it reads $n=k$, finds $i<k$ to be true, and accesses $A[i]$.\n- Now, an operation from $T_2$ is interleaved. $T_2$ writes a new value to $n$, say $n_{new} = m$. According to the problem, $T_2$ may then invalidate memory locations $A[m], A[m+1], \\dots$.\n- $T_1$ continues its loop at iteration $i=m$. It reads the current value of $n$, which is now $n_{new}=m$.\n- $T_1$ evaluates the predicate $i < n$, which is $m < m$. This is false.\n- Consequently, $T_1$ does **not** access $A[m]$. The check successfully prevents a memory access to an index that is no longer part of the logical array, thus ensuring memory safety.\n\nNow, let's analyze the behavior of the optimized program.\nThe optimized program first reads $n$ into a private local variable $n_0$. Let's assume it reads $n_0=k$. The loop then becomes `for i from 0 to k-1, access A[i]`. This loop no longer contains any reads of the shared variable $n$.\n\nConsider the same interleaving under SC:\n- $T_1$ is about to start its loop. It executes the pre-loop code: read $n$ into $n_0$. At this time, $n=k$, so $n_0=k$.\n- $T_1$ begins its loop, which will run for $i$ from $0$ to $k-1$.\n- For $i=0, 1, \\dots, m-1$, $T_1$ accesses $A[i]$.\n- Now, an operation from $T_2$ is interleaved. $T_2$ writes $n_{new}=m$ to $n$ and invalidates $A[m], A[m+1], \\dots$.\n- $T_1$ continues its loop at iteration $i=m$. The loop's upper bound is fixed at $n_0-1 = k-1$. Since $m < k$, the loop continues.\n- $T_1$ proceeds to access $A[m]$.\n- At this point in the total order of operations, the shared variable $n$ has the value $m$, and the location $A[m]$ has been invalidated by $T_2$. The access to $A[m]$ by $T_1$ is an access to an invalid memory location. This constitutes a memory safety violation.\n\nThis new behavior—the memory safety violation—was not possible in the original program. Therefore, the optimization is not semantics-preserving. It introduces a bug (a data race leading to a safety violation).\n\n### Option-by-Option Analysis\n\n**A. Under Sequential Consistency, if $T_2$ can decrease $n$ during $T_1$’s loop and immediately invalidate $A[j]$ for $j \\geq n$ at the moment of decrease, then replacing per-iteration checks $i < n$ with a single pre-loop snapshot $n_0$ is not semantics-preserving, because the optimized loop may access $A[i]$ for indices $i$ such that $i \\geq n$ at the time of access, violating memory safety.**\n- **Justification:** This statement accurately describes the flaw derived above. The optimization creates a time-of-check-to-time-of-use (TOCTOU) vulnerability. The \"check\" is the single read of $n$ into $n_0$ before the loop, and the \"use\" is the access to $A[i]$ within the loop. If $n$ is decreased by $T_2$ between the check and the use, the optimized loop in $T_1$ may access an index $i$ that is valid according to the stale snapshot $n_0$ but invalid according to the current value of $n$. This violates memory safety, which is part of the observable behavior the compiler must preserve.\n- **Verdict:** **Correct**.\n\n**B. Declaring $n$ as an atomic variable alone suffices to make the optimization sound, because atomicity ensures visibility of concurrent writes and thus the single snapshot $n_0$ is an acceptable replacement for repeated checks.**\n- **Justification:** The problem statement already assumes that reads and writes of $n$ are atomic. Atomicity of an operation prevents torn reads or writes; it ensures that a read sees either the entire old value or the entire new value, not a mix. However, it does not prevent the TOCTOU race. The problem is not the atomicity of the single read of $n_0$, but the fact that the value read can become stale later during the loop's execution. A single atomic read does not provide any guarantee about the variable's value at a later point in time. The reasoning in the option is flawed.\n- **Verdict:** **Incorrect**.\n\n**C. The optimization is sound if the optimization scope is constrained to cases where there is a proof (for example, via whole-program analysis or language-level guarantees) that no thread other than $T_1$ writes to $n$ in the dynamic extent of the loop, so that $n$ is effectively invariant while the loop runs.**\n- **Justification:** The entire problem arises from the concurrent modification of $n$ by $T_2$. If it can be proven that no other thread modifies $n$ while $T_1$'s loop is executing, then $n$ is functionally a loop-invariant variable. In that case, reading $n$ once before the loop is semantically identical to reading it in every iteration, as the value would be the same each time. The transformation would be a valid instance of Loop-Invariant Code Motion (LICM). Therefore, under this very strong condition, the optimization is sound.\n- **Verdict:** **Correct**.\n\n**D. The optimization is sound regardless of concurrency, because $i$ increases monotonically and $i < n$ at loop entry implies that $i < n$ for all subsequent iterations.**\n- **Justification:** This statement makes a false assumption. It implicitly assumes that $n$ is constant or non-decreasing. The problem explicitly states that $T_2$ may *decrease* $n$. If $i$ increases while $n$ decreases, the condition $i < n$ can easily change from true to false. For example, if at one point $i=4$ and $n=5$ ($i<n$ is true), in the next moment $i$ could be $5$ and $n$ could be $3$, making $i<n$ false. The reasoning presented is logically invalid in the context of the problem.\n- **Verdict:** **Incorrect**.\n\n**E. The optimization is sound if the compiler’s scope is defined as intra-thread semantics that ignores concurrent writes from other threads to shared variables, since correctness can be judged by the behavior of $T_1$ alone.**\n- **Justification:** This proposes a fundamentally incorrect model for compiling concurrent programs. A compiler for a language with shared-memory concurrency cannot ignore the effects of other threads. The semantics of such languages (like the SC model specified here) are defined precisely to handle the interactions between threads. Judging correctness based on \"intra-thread semantics\" alone is a recipe for introducing data races and other concurrency bugs. An optimization is only sound if it preserves the semantics of the full program, including all inter-thread behaviors allowed by the memory model. Claiming an optimization is \"sound\" within a flawed model that ignores the core problem is a meaningless and dangerous assertion.\n- **Verdict:** **Incorrect**.", "answer": "$$\\boxed{AC}$$", "id": "3628541"}]}