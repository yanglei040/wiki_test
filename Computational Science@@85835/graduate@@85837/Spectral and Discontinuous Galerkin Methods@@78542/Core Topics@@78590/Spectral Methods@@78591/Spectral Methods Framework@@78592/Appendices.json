{"hands_on_practices": [{"introduction": "At the heart of spectral methods lies the ability to represent complex functions with remarkable accuracy using a series of orthogonal polynomials. This first practice grounds you in this fundamental concept by guiding you through the computation of the best polynomial approximation to the function $u(x) = \\exp(x)$ in the $L^2$ sense. This optimal approximation is achieved by projecting the function onto a basis of Legendre polynomials, a cornerstone of spectral methods on bounded domains [@problem_id:3419277]. This exercise is crucial for understanding how spectral approximations are constructed and why they form the basis for solving differential equations with high accuracy.", "problem": "Let $L^{2}([-1,1])$ be the Hilbert space of square-integrable functions on $[-1,1]$ with the standard inner product $\\langle f,g\\rangle=\\int_{-1}^{1} f(x)\\,g(x)\\,dx$. Consider the Legendre polynomials $\\{P_{n}(x)\\}_{n\\geq 0}$, which form an orthogonal basis in $L^{2}([-1,1])$ with respect to the weight $1$, satisfying $\\int_{-1}^{1} P_{n}(x)\\,P_{m}(x)\\,dx=\\frac{2}{2n+1}\\,\\delta_{nm}$, and the explicit forms $P_{0}(x)=1$, $P_{1}(x)=x$, $P_{2}(x)=\\frac{1}{2}(3x^{2}-1)$, $P_{3}(x)=\\frac{1}{2}(5x^{3}-3x)$. Let $u(x)=\\exp(x)$.\n\nStarting from the definition of the orthogonal projection in a Hilbert space and the orthogonality of the Legendre basis, determine the best $L^{2}([-1,1])$ polynomial of degree at most $N=3$ approximating $u(x)$, expressed as a single closed-form analytic expression for the polynomial $p_{3}(x)$. Do not round; provide the exact expression.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It is a standard problem in approximation theory within the field of numerical analysis and spectral methods. All necessary data and definitions are provided, and there are no contradictions or ambiguities.\n\nThe problem asks for the best polynomial approximation of degree at most $N=3$ to the function $u(x) = \\exp(x)$ on the interval $[-1,1]$ in the $L^2([-1,1])$ norm. The best approximation in a Hilbert space is given by the orthogonal projection of the function onto the specified subspace. In this case, the subspace is $\\mathcal{P}_3$, the space of all polynomials of degree at most $3$.\n\nThe Legendre polynomials $\\{P_n(x)\\}_{n=0}^{\\infty}$ form a complete orthogonal basis for $L^2([-1,1])$. The subspace $\\mathcal{P}_3$ is spanned by the first four Legendre polynomials, $\\{P_0(x), P_1(x), P_2(x), P_3(x)\\}$. The orthogonal projection of $u(x)$ onto $\\mathcal{P}_3$, denoted as $p_3(x)$, is given by the truncated Legendre series expansion of $u(x)$:\n$$\np_3(x) = \\sum_{n=0}^{3} \\hat{u}_n P_n(x)\n$$\nwhere the coefficients $\\hat{u}_n$ are the generalized Fourier coefficients of $u(x)$ with respect to the Legendre basis. These coefficients are calculated using the inner product $\\langle f,g\\rangle = \\int_{-1}^{1} f(x)g(x)dx$.\n\nGiven the orthogonality relation $\\langle P_n, P_m \\rangle = \\int_{-1}^{1} P_n(x)P_m(x)dx = \\frac{2}{2n+1}\\delta_{nm}$, the coefficient $\\hat{u}_n$ is found by taking the inner product of $u(x)$ with $P_n(x)$:\n$$\n\\langle u, P_n \\rangle = \\left\\langle \\sum_{k=0}^{\\infty} \\hat{u}_k P_k, P_n \\right\\rangle = \\sum_{k=0}^{\\infty} \\hat{u}_k \\langle P_k, P_n \\rangle = \\hat{u}_n \\langle P_n, P_n \\rangle = \\hat{u}_n \\frac{2}{2n+1}\n$$\nThis gives the formula for the coefficients:\n$$\n\\hat{u}_n = \\frac{\\langle u, P_n \\rangle}{\\langle P_n, P_n \\rangle} = \\frac{2n+1}{2} \\int_{-1}^{1} u(x)P_n(x)dx\n$$\nFor this problem, $u(x) = \\exp(x)$. We need to compute the coefficients $\\hat{u}_0, \\hat{u}_1, \\hat{u}_2, \\hat{u}_3$. This requires calculating the integrals $\\int_{-1}^{1} \\exp(x) P_n(x) dx$ for $n=0, 1, 2, 3$. We are given:\n$P_0(x) = 1$\n$P_1(x) = x$\n$P_2(x) = \\frac{1}{2}(3x^2 - 1)$\n$P_3(x) = \\frac{1}{2}(5x^3 - 3x)$\n\nTo compute the necessary integrals, we first evaluate the moments of $\\exp(x)$ on $[-1,1]$, which are integrals of the form $\\int_{-1}^{1} x^k \\exp(x) dx$. These can be solved using integration by parts.\nLet $I_k = \\int_{-1}^{1} x^k \\exp(x) dx$.\nFor $k=0$:\n$I_0 = \\int_{-1}^{1} \\exp(x) dx = [\\exp(x)]_{-1}^{1} = \\exp(1) - \\exp(-1)$.\nFor $k=1$:\n$I_1 = \\int_{-1}^{1} x \\exp(x) dx = [x\\exp(x) - \\exp(x)]_{-1}^{1} = (1\\exp(1) - \\exp(1)) - (-1\\exp(-1) - \\exp(-1)) = 0 - (-2\\exp(-1)) = 2\\exp(-1)$.\nFor $k=2$:\n$I_2 = \\int_{-1}^{1} x^2 \\exp(x) dx = [(x^2-2x+2)\\exp(x)]_{-1}^{1} = ((1)^2-2(1)+2)\\exp(1) - ((-1)^2-2(-1)+2)\\exp(-1) = (1-2+2)\\exp(1) - (1+2+2)\\exp(-1) = \\exp(1) - 5\\exp(-1)$.\nFor $k=3$:\n$I_3 = \\int_{-1}^{1} x^3 \\exp(x) dx = [(x^3-3x^2+6x-6)\\exp(x)]_{-1}^{1} = ((1)^3-3(1)^2+6(1)-6)\\exp(1) - ((-1)^3-3(-1)^2+6(-1)-6)\\exp(-1) = (1-3+6-6)\\exp(1) - (-1-3-6-6)\\exp(-1) = -2\\exp(1) + 16\\exp(-1)$.\n\nNow we compute the inner products $\\langle \\exp(x), P_n(x) \\rangle$:\nFor $n=0$: $\\langle u, P_0 \\rangle = \\int_{-1}^{1} \\exp(x) \\cdot 1 dx = I_0 = \\exp(1) - \\exp(-1)$.\nFor $n=1$: $\\langle u, P_1 \\rangle = \\int_{-1}^{1} \\exp(x) \\cdot x dx = I_1 = 2\\exp(-1)$.\nFor $n=2$: $\\langle u, P_2 \\rangle = \\int_{-1}^{1} \\exp(x) \\frac{1}{2}(3x^2-1) dx = \\frac{3}{2}I_2 - \\frac{1}{2}I_0 = \\frac{3}{2}(\\exp(1) - 5\\exp(-1)) - \\frac{1}{2}(\\exp(1) - \\exp(-1)) = (\\frac{3}{2}-\\frac{1}{2})\\exp(1) + (-\\frac{15}{2}+\\frac{1}{2})\\exp(-1) = \\exp(1) - 7\\exp(-1)$.\nFor $n=3$: $\\langle u, P_3 \\rangle = \\int_{-1}^{1} \\exp(x) \\frac{1}{2}(5x^3-3x) dx = \\frac{5}{2}I_3 - \\frac{3}{2}I_1 = \\frac{5}{2}(-2\\exp(1) + 16\\exp(-1)) - \\frac{3}{2}(2\\exp(-1)) = -5\\exp(1) + 40\\exp(-1) - 3\\exp(-1) = -5\\exp(1) + 37\\exp(-1)$.\n\nNext, we calculate the coefficients $\\hat{u}_n$:\n$\\hat{u}_0 = \\frac{1}{2} \\langle u, P_0 \\rangle = \\frac{1}{2}(\\exp(1) - \\exp(-1))$.\n$\\hat{u}_1 = \\frac{3}{2} \\langle u, P_1 \\rangle = \\frac{3}{2}(2\\exp(-1)) = 3\\exp(-1)$.\n$\\hat{u}_2 = \\frac{5}{2} \\langle u, P_2 \\rangle = \\frac{5}{2}(\\exp(1) - 7\\exp(-1))$.\n$\\hat{u}_3 = \\frac{7}{2} \\langle u, P_3 \\rangle = \\frac{7}{2}(-5\\exp(1) + 37\\exp(-1))$.\n\nFinally, we construct the polynomial $p_3(x)$ by substituting these coefficients and the explicit forms of the Legendre polynomials into the summation, and then collecting terms based on powers of $x$.\n$$\np_3(x) = \\hat{u}_0 P_0(x) + \\hat{u}_1 P_1(x) + \\hat{u}_2 P_2(x) + \\hat{u}_3 P_3(x)\n$$\n$$\np_3(x) = \\hat{u}_0(1) + \\hat{u}_1(x) + \\hat{u}_2\\left(\\frac{3}{2}x^2 - \\frac{1}{2}\\right) + \\hat{u}_3\\left(\\frac{5}{2}x^3 - \\frac{3}{2}x\\right)\n$$\nThe polynomial is $p_3(x) = c_3x^3 + c_2x^2 + c_1x + c_0$, where the coefficients are:\n$c_3 = \\frac{5}{2}\\hat{u}_3 = \\frac{5}{2} \\cdot \\frac{7}{2}(-5\\exp(1) + 37\\exp(-1)) = \\frac{35}{4}(-5\\exp(1) + 37\\exp(-1)) = \\frac{-175\\exp(1) + 1295\\exp(-1)}{4}$.\n$c_2 = \\frac{3}{2}\\hat{u}_2 = \\frac{3}{2} \\cdot \\frac{5}{2}(\\exp(1) - 7\\exp(-1)) = \\frac{15}{4}(\\exp(1) - 7\\exp(-1)) = \\frac{15\\exp(1) - 105\\exp(-1)}{4}$.\n$c_1 = \\hat{u}_1 - \\frac{3}{2}\\hat{u}_3 = 3\\exp(-1) - \\frac{3}{2} \\cdot \\frac{7}{2}(-5\\exp(1) + 37\\exp(-1)) = 3\\exp(-1) - \\frac{21}{4}(-5\\exp(1) + 37\\exp(-1)) = \\frac{12\\exp(-1) + 105\\exp(1) - 777\\exp(-1)}{4} = \\frac{105\\exp(1) - 765\\exp(-1)}{4}$.\n$c_0 = \\hat{u}_0 - \\frac{1}{2}\\hat{u}_2 = \\frac{1}{2}(\\exp(1) - \\exp(-1)) - \\frac{1}{2} \\cdot \\frac{5}{2}(\\exp(1) - 7\\exp(-1)) = \\frac{1}{2}(\\exp(1) - \\exp(-1)) - \\frac{5}{4}(\\exp(1) - 7\\exp(-1)) = \\frac{2(\\exp(1) - \\exp(-1)) - 5(\\exp(1) - 7\\exp(-1))}{4} = \\frac{2\\exp(1) - 2\\exp(-1) - 5\\exp(1) + 35\\exp(-1)}{4} = \\frac{-3\\exp(1) + 33\\exp(-1)}{4}$.\n\nThus, the best $L^2$ polynomial approximation of degree at most $3$ is:\n$$\np_3(x) = \\left(\\frac{-175\\exp(1) + 1295\\exp(-1)}{4}\\right)x^3 + \\left(\\frac{15\\exp(1) - 105\\exp(-1)}{4}\\right)x^2 + \\left(\\frac{105\\exp(1) - 765\\exp(-1)}{4}\\right)x + \\left(\\frac{-3\\exp(1) + 33\\exp(-1)}{4}\\right)\n$$", "answer": "$$\n\\boxed{\\left(\\frac{-175\\exp(1) + 1295\\exp(-1)}{4}\\right)x^3 + \\left(\\frac{15\\exp(1) - 105\\exp(-1)}{4}\\right)x^2 + \\left(\\frac{105\\exp(1) - 765\\exp(-1)}{4}\\right)x + \\left(\\frac{-3\\exp(1) + 33\\exp(-1)}{4}\\right)}\n$$", "id": "3419277"}, {"introduction": "After approximating a function, we often need to compute its derivatives, a task where spectral methods are exceptionally powerful. This practice delves into the elegant technique of spectral collocation differentiation, where you will first prove a cornerstone result: that the spectral differentiation matrix is exact for any polynomial up to the degree of the interpolant [@problem_id:3419336]. Subsequently, you will computationally verify this property and quantify the error for a non-polynomial function, illustrating the origin of \"spectral accuracy\" and comparing the performance of different families of collocation points.", "problem": "Consider the spectral collocation framework on the interval $[-1,1]$ with $N+1$ distinct collocation points $\\{x_j\\}_{j=0}^N$. Let $I_N u(x)$ denote the degree-$N$ Lagrange interpolant of a function $u(x)$ defined by $I_N u(x) = \\sum_{j=0}^N u(x_j)\\,\\ell_j(x)$, where $\\ell_j(x)$ are the Lagrange basis polynomials satisfying $\\ell_j(x_i) = \\delta_{ij}$ for all $i,j$. The spectral differentiation matrix $D \\in \\mathbb{R}^{(N+1)\\times(N+1)}$ is defined by the rule that, for the nodal vector $u = [u(x_0),u(x_1),\\dots,u(x_N)]^\\top$, the vector $Du$ has entries $(Du)_i = \\left.\\frac{d}{dx}I_N u(x)\\right|_{x=x_i}$ for $i=0,\\dots,N$. Using this setup, address the following:\n\n1. Starting from the definition of the Lagrange interpolant and its basic properties, prove that for any polynomial $p(x)$ of degree at most $N$ and any set of $N+1$ distinct collocation points, the spectral differentiation matrix $D$ constructed via collocation is exact at the nodes, i.e., $(Dp)_i = p'(x_i)$ for all $i=0,\\dots,N$.\n\n2. Quantify the numerical differentiation error for the analytic function $u(x) = e^x$ at $N=10$ by constructing $D$ at two classical collocation sets:\n   - Chebyshev–Gauss–Lobatto (CGL) nodes defined by $x_j = \\cos\\left(\\frac{\\pi j}{N}\\right)$ for $j=0,\\dots,N$.\n   - Legendre–Gauss–Lobatto (LGL) nodes defined by the endpoints $\\pm 1$ together with the roots of the derivative of the degree-$N$ Legendre polynomial.\n   Use the barycentric weights $w_j = \\left(\\prod_{m\\neq j}(x_j - x_m)\\right)^{-1}$ to construct $D$ via $(D)_{ij} = \\frac{w_j}{w_i}\\frac{1}{x_i - x_j}$ for $i\\neq j$ and $(D)_{ii} = -\\sum_{j\\neq i}(D)_{ij}$. Compute the infinity-norm error $\\max_{0\\le i\\le N}\\left| (Du)_i - u'(x_i)\\right|$ for each node family at $N=10$.\n\n3. Implement a complete, runnable program that performs the following test suite:\n   - Test $1$ (exactness at CGL for polynomial of degree $N$): Use $N=10$, CGL nodes, and the deterministic polynomial $p(x) = \\sum_{k=0}^{N}\\frac{1}{k+1}x^k$. Return a boolean indicating whether $\\max_{0\\le i\\le N}\\left| (Dp)_i - p'(x_i)\\right| \\le 10^{-12}$.\n   - Test $2$ (exactness at LGL for polynomial of degree $N$): Use $N=10$, LGL nodes, and the same polynomial $p(x)$. Return a boolean using the same tolerance as in Test $1$.\n   - Test $3$ (error for $u(x)=e^x$ at CGL): Use $N=10$, CGL nodes, and return the float value of $\\max_{0\\le i\\le N}\\left| (Du)_i - u'(x_i)\\right|$.\n   - Test $4$ (error for $u(x)=e^x$ at LGL): Use $N=10$, LGL nodes, and return the float value of $\\max_{0\\le i\\le N}\\left| (Du)_i - u'(x_i)\\right|$.\n   - Test $5$ (boundary case $N=0$): Use $N=0$, the single CGL node $x_0=1$, and the constant polynomial $p(x) = 3$. Return a boolean indicating whether $(Dp)_0 = 0$ exactly within the tolerance $10^{-14}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the tests $1$–$5$ (for example: \"[result1,result2,result3,result4,result5]\"). No physical units are involved. All angle units, if any, are implicitly in radians. The returned booleans and floats must be directly usable as fundamental types in a generic parser.", "solution": "The problem is subjected to validation and is determined to be valid. It is a well-posed, scientifically grounded problem in the field of numerical analysis, specifically concerning spectral methods. All definitions and tasks are standard and self-consistent.\n\n### Part 1: Proof of Exactness of Spectral Differentiation for Polynomials\n\nWe are asked to prove that for any polynomial $p(x)$ of degree at most $N$, the spectral differentiation matrix $D$ is exact at the $N+1$ distinct collocation nodes $\\{x_j\\}_{j=0}^N$. This means we must show $(Dp)_i = p'(x_i)$ for all $i=0, \\dots, N$.\n\nLet $p(x)$ be an arbitrary polynomial of degree at most $N$.\nThe Lagrange interpolant of a function $u(x)$ through the $N+1$ distinct points $\\{x_j\\}$ is denoted by $I_N u(x)$ and is defined as:\n$$\nI_N u(x) = \\sum_{j=0}^N u(x_j)\\,\\ell_j(x)\n$$\nwhere $\\ell_j(x)$ are the Lagrange basis polynomials of degree $N$. By construction, $I_N u(x)$ is the unique polynomial of degree at most $N$ that satisfies the interpolation condition $I_N u(x_i) = u(x_i)$ for all $i=0, \\dots, N$.\n\nA fundamental theorem of polynomial interpolation states that if the function being interpolated is itself a polynomial of degree at most $N$, its unique interpolant of degree at most $N$ is the polynomial itself. Applying this to our polynomial $p(x)$, we have:\n$$\nI_N p(x) = p(x) \\quad \\forall x\n$$\nThe entries of the vector $Du$ are defined by differentiating the interpolant and evaluating at the nodes:\n$$\n(Dp)_i = \\left.\\frac{d}{dx} I_N p(x)\\right|_{x=x_i}\n$$\nwhere $p$ is the vector of nodal values $[p(x_0), \\dots, p(x_N)]^\\top$.\nSubstituting the identity $I_N p(x) = p(x)$ into the definition of the spectral derivative, we obtain:\n$$\n(Dp)_i = \\left.\\frac{d}{dx} p(x)\\right|_{x=x_i}\n$$\nThe right-hand side is simply the exact derivative of the polynomial $p(x)$ evaluated at the node $x_i$, which is denoted by $p'(x_i)$. Therefore,\n$$\n(Dp)_i = p'(x_i)\n$$\nThis holds for all $i = 0, \\dots, N$. This completes the proof. This result is independent of the specific choice of the $N+1$ distinct collocation points. It is a direct consequence of the properties of polynomial interpolation.\n\n### Part 2 & 3: Numerical Implementation and Verification\n\nThe subsequent parts of the problem require implementing and numerically verifying the properties of spectral differentiation for specific node sets and functions. The core of this task is the construction of the spectral differentiation matrix $D$.\n\n**Construction of the Differentiation Matrix $D$:**\n\nGiven a set of $N+1$ distinct nodes $\\{x_j\\}_{j=0}^N$, the differentiation matrix $D \\in \\mathbb{R}^{(N+1)\\times(N+1)}$ is constructed using the provided barycentric formula.\nFirst, the barycentric weights $w_j$ are computed for each node $x_j$:\n$$\nw_j = \\left(\\prod_{m \\neq j} (x_j - x_m)\\right)^{-1} \\quad \\text{for } j=0, \\dots, N\n$$\nThe entries $(D)_{ij}$ of the matrix $D$ are then given by:\n$$\n(D)_{ij} = \\begin{cases}\n\\frac{w_j}{w_i} \\frac{1}{x_i - x_j} & \\text{if } i \\neq j \\\\\n-\\sum_{k \\neq i} (D)_{ik} & \\text{if } i = j\n\\end{cases}\n$$\nThe diagonal entries are determined by the property that the derivative of a constant function is zero. If $u(x)=1$, then $u'(x)=0$. The nodal vector is $u = [1, 1, \\dots, 1]^\\top$. The condition $(Du)_i = 0$ implies $\\sum_{j=0}^{N} (D)_{ij} u_j = \\sum_{j=0}^{N} (D)_{ij} = 0$, which leads to the formula for the diagonal elements.\n\n**Collocation Node Sets:**\n\nThe implementation will use two families of nodes on the interval $[-1, 1]$:\n1.  **Chebyshev–Gauss–Lobatto (CGL) nodes:** These are the extrema of the degree-$N$ Chebyshev polynomial of the first kind, given by $x_j = \\cos\\left(\\frac{\\pi j}{N}\\right)$ for $j=0, \\dots, N$.\n2.  **Legendre–Gauss–Lobatto (LGL) nodes:** These consist of the endpoints $x_0=1, x_N=-1$ and the $N-1$ roots of $P'_N(x)$, where $P_N(x)$ is the Legendre polynomial of degree $N$.\n\n**Test Suite Logic:**\n\nThe program will execute five tests as specified:\n-   **Tests 1 and 2** verify the exactness property proven in Part 1. For $N=10$ and a polynomial $p(x) = \\sum_{k=0}^{10}\\frac{1}{k+1}x^k$, we compute the numerical derivative $Dp$ using both CGL and LGL nodes and check if the maximum absolute error $|(Dp)_i - p'(x_i)|$ is below the machine-precision tolerance of $10^{-12}$.\n-   **Tests 3 and 4** quantify the spectral differentiation error for a non-polynomial (analytic) function, $u(x) = e^x$. The maximum absolute error $|(Du)_i - u'(x_i)|$ is computed for $N=10$ at both CGL and LGL nodes. This error is expected to be small, demonstrating the \"spectral accuracy\" of the method for smooth functions.\n-   **Test 5** examines the boundary case $N=0$. For a single node ($x_0=1$), the differentiation matrix is a $1 \\times 1$ zero matrix, $D = [0]$. We verify that for a constant polynomial $p(x)=3$, the numerical derivative is exactly zero, i.e., $(Dp)_0 = 0$.\n\nThe implementation will define functions to generate the specified nodes and construct the differentiation matrix. These will then be used to perform the five tests and format the results as a comma-separated list.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import legendre\n\ndef get_cgl_nodes(N):\n    \"\"\"\n    Computes the N+1 Chebyshev-Gauss-Lobatto nodes on [-1, 1].\n    Nodes are sorted in descending order from 1 to -1.\n    \"\"\"\n    if N == 0:\n        return np.array([1.0])\n    return np.cos(np.pi * np.arange(N + 1) / N)\n\ndef get_lgl_nodes(N):\n    \"\"\"\n    Computes the N+1 Legendre-Gauss-Lobatto nodes on [-1, 1].\n    Nodes are sorted in descending order from 1 to -1.\n    \"\"\"\n    if N == 0:\n        return np.array([1.0])\n    if N == 1:\n        return np.array([1.0, -1.0])\n    \n    # Interior nodes are roots of the derivative of the N-th Legendre polynomial\n    PN_prime_roots = legendre(N).deriv(1).roots\n    \n    # Combine with endpoints and sort\n    nodes = np.concatenate(([1.0], np.sort(PN_prime_roots)[::-1], [-1.0]))\n    return nodes\n\ndef build_diff_matrix(nodes):\n    \"\"\"\n    Constructs the spectral differentiation matrix D using the barycentric formula.\n    \"\"\"\n    N_plus_1 = len(nodes)\n    if N_plus_1 == 1:\n        return np.array([[0.0]])\n\n    x = nodes\n    w = np.ones(N_plus_1)\n    \n    # Barycentric weights from the product formula\n    for j in range(N_plus_1):\n        prod = 1.0\n        for m in range(N_plus_1):\n            if j != m:\n                prod *= (x[j] - x[m])\n        w[j] = 1.0 / prod\n            \n    D = np.zeros((N_plus_1, N_plus_1))\n    \n    # Off-diagonal entries\n    for i in range(N_plus_1):\n        for j in range(N_plus_1):\n            if i != j:\n                D[i, j] = (w[j] / w[i]) / (x[i] - x[j])\n                \n    # Diagonal entries\n    for i in range(N_plus_1):\n        D[i, i] = -np.sum(D[i, :])\n        \n    return D\n\ndef solve():\n    \"\"\"\n    Executes the specified test suite for spectral differentiation.\n    \"\"\"\n    \n    results = []\n\n    # --- Test 1: Exactness for polynomial at CGL nodes ---\n    N1 = 10\n    tol1 = 1e-12\n    nodes_cgl_10 = get_cgl_nodes(N1)\n    D_cgl_10 = build_diff_matrix(nodes_cgl_10)\n    \n    # p(x) = sum_{k=0 to N} (1/(k+1)) * x^k\n    coeffs_p_asc = 1 / (np.arange(N1 + 1) + 1) # Coeffs for x^0, x^1, ...\n    p_at_nodes = np.polyval(coeffs_p_asc[::-1], nodes_cgl_10)\n    \n    # p'(x) = sum_{k=1 to N} (k/(k+1)) * x^(k-1) = sum_{j=0 to N-1} ((j+1)/(j+2)) * x^j\n    coeffs_p_prime_asc = (np.arange(N1) + 1) / (np.arange(N1) + 2)\n    p_prime_exact = np.polyval(coeffs_p_prime_asc[::-1], nodes_cgl_10)\n    \n    p_prime_numeric = D_cgl_10 @ p_at_nodes\n    err1 = np.max(np.abs(p_prime_numeric - p_prime_exact))\n    results.append(err1 = tol1)\n\n    # --- Test 2: Exactness for polynomial at LGL nodes ---\n    N2 = 10\n    tol2 = 1e-12\n    nodes_lgl_10 = get_lgl_nodes(N2)\n    D_lgl_10 = build_diff_matrix(nodes_lgl_10)\n\n    p_at_nodes_lgl = np.polyval(coeffs_p_asc[::-1], nodes_lgl_10)\n    p_prime_exact_lgl = np.polyval(coeffs_p_prime_asc[::-1], nodes_lgl_10)\n    \n    p_prime_numeric_lgl = D_lgl_10 @ p_at_nodes_lgl\n    err2 = np.max(np.abs(p_prime_numeric_lgl - p_prime_exact_lgl))\n    results.append(err2 = tol2)\n\n    # --- Test 3: Error for u(x)=e^x at CGL nodes ---\n    N3 = 10\n    u_cgl = np.exp(nodes_cgl_10)\n    u_prime_exact_cgl = u_cgl # derivative of e^x is e^x\n    u_prime_numeric_cgl = D_cgl_10 @ u_cgl\n    err3 = np.max(np.abs(u_prime_numeric_cgl - u_prime_exact_cgl))\n    results.append(err3)\n\n    # --- Test 4: Error for u(x)=e^x at LGL nodes ---\n    N4 = 10\n    u_lgl = np.exp(nodes_lgl_10)\n    u_prime_exact_lgl_exp = u_lgl\n    u_prime_numeric_lgl_exp = D_lgl_10 @ u_lgl\n    err4 = np.max(np.abs(u_prime_numeric_lgl_exp - u_prime_exact_lgl_exp))\n    results.append(err4)\n\n    # --- Test 5: Boundary case N=0 ---\n    N5 = 0\n    tol5 = 1e-14\n    nodes_n0 = get_cgl_nodes(N5)\n    D_n0 = build_diff_matrix(nodes_n0)\n    p_vec_n0 = np.array([3.0]) # p(x)=3 at node x0\n    dp_val = (D_n0 @ p_vec_n0)[0]\n    p_prime_exact_n0 = 0.0 # p'(x) = 0\n    results.append(np.abs(dp_val - p_prime_exact_n0) = tol5)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3419336"}, {"introduction": "Real-world engineering and physics problems rarely unfold on simple square domains. This advanced practice introduces a crucial topic from spectral element methods: handling complex geometries through isoparametric mapping. You will design a distorted quadrilateral element and investigate how the non-constant Jacobian of the mapping degrades the perfect orthogonality of the modal basis functions [@problem_id:3419299]. Mastering this concept is key to understanding the practical trade-offs between geometric flexibility and computational efficiency in high-order methods.", "problem": "You are asked to design and analyze an isoparametric spectral element mapping for a single quadrilateral element within the reference square and quantify how variations in the mapping Jacobian affect the orthogonality and conditioning of a modal basis. The analysis must be performed entirely in mathematical terms, and the final numerical results must be produced by a complete, runnable program.\n\nStart from the following fundamental base:\n- The reference element is the square $[-1,1] \\times [-1,1]$ with coordinates $(\\xi, \\eta)$.\n- The isoparametric mapping $F : [-1,1]^2 \\to \\mathbb{R}^2$ is represented by the same polynomial basis used for the solution space (isoparametric property). Specifically, use a tensor-product Lagrange basis of polynomial degree $2$ associated with the one-dimensional nodes $\\{-1,0,1\\}$ on each coordinate.\n- For any mapping $F$, the line element transformation obeys the change-of-variables law for integrals, so that for any functions $u(\\xi,\\eta)$ and $v(\\xi,\\eta)$ pulled back to the reference element, their inner product over the physical element is given by\n$$\n\\langle u, v \\rangle_F = \\int_{-1}^1 \\int_{-1}^1 u(\\xi, \\eta)\\, v(\\xi, \\eta)\\, J(\\xi, \\eta)\\, d\\xi\\, d\\eta,\n$$\nwhere $J(\\xi,\\eta)$ denotes the Jacobian determinant of the mapping $F$ at $(\\xi,\\eta)$.\n- The one-dimensional modal basis is the set of normalized Legendre polynomials $\\{\\phi_n(x)\\}_{n=0}^{N-1}$ on $[-1,1]$, with\n$$\n\\phi_n(x) = \\sqrt{\\frac{2n+1}{2}}\\, P_n(x),\n$$\nwhere $P_n(x)$ is the $n$-th Legendre polynomial. This set is orthonormal under the standard $L^2$ inner product on the reference interval. The two-dimensional modal basis on the reference square is the tensor product\n$$\n\\psi_{i,j}(\\xi,\\eta) = \\phi_i(\\xi)\\, \\phi_j(\\eta), \\quad i,j \\in \\{0,1,\\dots,N-1\\}.\n$$\n- Orthogonality in the mapped inner product is characterized by the Gram matrix $G$ with entries\n$$\nG_{(i,j),(p,q)} = \\int_{-1}^1 \\int_{-1}^1 \\psi_{i,j}(\\xi,\\eta)\\, \\psi_{p,q}(\\xi,\\eta)\\, J(\\xi,\\eta)\\, d\\xi\\, d\\eta,\n$$\nwhich reduces to a scaled identity if $J(\\xi,\\eta)$ is a constant. Deviations of $J$ from a constant induce off-diagonal couplings and alter the conditioning of $G$.\n\nDesign requirements and tasks:\n1. Construct an isoparametric mapping of polynomial degree $2$ using the tensor-product Lagrange basis on nodes $\\{-1,0,1\\}$ for each coordinate. Define physical nodes $\\mathbf{P}_{ij}$, $i,j \\in \\{0,1,2\\}$, such that the corners form a trapezoid determined by a distortion parameter $\\,\\alpha \\in \\mathbb{R}\\,$, and introduce an interior distortion control parameter $\\,\\beta \\in \\mathbb{R}\\,$ that only moves the center node while keeping all edges straight:\n   - Corner nodes are\n     $$\n     \\mathbf{P}_{00}=(0,0),\\quad \\mathbf{P}_{20}=(1,0),\\quad \\mathbf{P}_{22}=(1,1),\\quad \\mathbf{P}_{02}=(0,1+\\alpha).\n     $$\n   - Edge midpoints are the colinear midpoints of their respective corner endpoints:\n     $$\n     \\mathbf{P}_{10}=(0.5,0),\\quad \\mathbf{P}_{21}=(1,0.5),\\quad \\mathbf{P}_{12}=(0.5,1+\\alpha/2),\\quad \\mathbf{P}_{01}=(0,0.5+\\alpha/2).\n     $$\n   - The center node is\n     $$\n     \\mathbf{P}_{11}=(0.5,0.5+\\alpha/2+\\beta/4).\n     $$\n   The mapping $F(\\xi,\\eta)$ is then\n   $$\n   F(\\xi,\\eta) = \\sum_{i=0}^2 \\sum_{j=0}^2 L_i(\\xi)\\, L_j(\\eta)\\, \\mathbf{P}_{ij},\n   $$\n   where $L_0(x)=\\frac{x(x-1)}{2}$, $L_1(x)=1-x^2$, and $L_2(x)=\\frac{x(x+1)}{2}$ are the degree-$2$ Lagrange basis polynomials associated with nodes $-1,0,1$.\n\n2. Compute the Jacobian determinant $J(\\xi,\\eta)$ using\n   $$\n   J(\\xi,\\eta) = \\det\\left( \\begin{bmatrix} \\dfrac{\\partial x}{\\partial \\xi}  \\dfrac{\\partial x}{\\partial \\eta} \\\\ \\dfrac{\\partial y}{\\partial \\xi}  \\dfrac{\\partial y}{\\partial \\eta} \\end{bmatrix} \\right),\n   $$\n   where $(x(\\xi,\\eta),y(\\xi,\\eta)) = F(\\xi,\\eta)$. Confirm numerically that $J(\\xi,\\eta) > 0$ on the reference square for each test case.\n\n3. Fix the modal basis size to $N=3$, i.e., three modes in each direction, giving a two-dimensional basis of size $9$. Form the Gram matrix $G$ for the mapped inner product using tensor-product Gaussian quadrature of sufficiently high order on $[-1,1]$ and quantify:\n   - Orthogonality degradation via the normalized Frobenius off-diagonal measure\n     $$\n     E = \\frac{\\left\\| G - \\mathrm{diag}(\\mathrm{diag}(G)) \\right\\|_F}{\\left\\| \\mathrm{diag}(\\mathrm{diag}(G)) \\right\\|_F}.\n     $$\n   - Conditioning via the spectral condition number\n     $$\n     \\kappa_2(G) = \\frac{\\sigma_{\\max}(G)}{\\sigma_{\\min}(G)},\n     $$\n     where $\\sigma_{\\max}$ and $\\sigma_{\\min}$ are the largest and smallest singular values of $G$ under the Euclidean norm.\n\n4. Implement the above in a single self-contained program. Use exact polynomial expressions for $L_i(x)$ and their derivatives. Use normalized Legendre polynomials for the modal basis. Use tensor-product Gaussian quadrature on $[-1,1]$ of order high enough to resolve the integrals when $N=3$ and the geometry mapping is of degree $2$.\n\n5. Use the following test suite of distortion parameters $(\\alpha,\\beta)$:\n   - Test case $1$: $(\\alpha,\\beta)=(0.0,0.0)$.\n   - Test case $2$: $(\\alpha,\\beta)=(0.3,0.0)$.\n   - Test case $3$: $(\\alpha,\\beta)=(0.0,0.4)$.\n   - Test case $4$: $(\\alpha,\\beta)=(0.3,0.4)$.\n   - Test case $5$: $(\\alpha,\\beta)=(0.4,-0.3)$.\n\n6. For each test case, compute $E$ and $\\kappa_2(G)$ as real numbers. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[E_1,\\kappa_2(G)_1,E_2,\\kappa_2(G)_2,\\dots,E_5,\\kappa_2(G)_5]$. No physical units are involved in this problem; all quantities are dimensionless real numbers.\n\nYour final program must be written in Python and adhere to the execution environment described later. No user input or external files are permitted.", "solution": "The user has provided a well-defined problem in the domain of computational mathematics, specifically concerning the isoparametric spectral element method. The task is to analyze the effect of geometric distortion on the properties of a modal basis. The problem statement is validated as follows:\n\n### Step 1: Extract Givens\n- **Reference Element**: Square $[-1,1] \\times [-1,1]$ with coordinates $(\\xi, \\eta)$.\n- **Isoparametric Mapping**: $F : [-1,1]^2 \\to \\mathbb{R}^2$, defined by a tensor-product Lagrange basis of polynomial degree $2$.\n- **Lagrange Basis Polynomials** (for nodes $\\{-1,0,1\\}$):\n    - $L_0(x)=\\frac{x(x-1)}{2}$\n    - $L_1(x)=1-x^2$\n    - $L_2(x)=\\frac{x(x+1)}{2}$\n- **Physical Nodes** $\\mathbf{P}_{ij}$ for $i,j \\in \\{0,1,2\\}$:\n    - Corners: $\\mathbf{P}_{00}=(0,0)$, $\\mathbf{P}_{20}=(1,0)$, $\\mathbf{P}_{22}=(1,1)$, $\\mathbf{P}_{02}=(0,1+\\alpha)$.\n    - Edge Midpoints: $\\mathbf{P}_{10}=(0.5,0)$, $\\mathbf{P}_{21}=(1,0.5)$, $\\mathbf{P}_{12}=(0.5,1+\\alpha/2)$, $\\mathbf{P}_{01}=(0,0.5+\\alpha/2)$.\n    - Center: $\\mathbf{P}_{11}=(0.5,0.5+\\alpha/2+\\beta/4)$.\n- **Mapping Formula**: $F(\\xi,\\eta) = \\sum_{i=0}^2 \\sum_{j=0}^2 L_i(\\xi)\\, L_j(\\eta)\\, \\mathbf{P}_{ij}$.\n- **Inner Product**: $\\langle u, v \\rangle_F = \\int_{-1}^1 \\int_{-1}^1 u(\\xi, \\eta)\\, v(\\xi, \\eta)\\, J(\\xi, \\eta)\\, d\\xi\\, d\\eta$.\n- **Jacobian Determinant**: $J(\\xi,\\eta) = \\det\\left( \\begin{bmatrix} \\partial x/\\partial \\xi  \\partial x/\\partial \\eta \\\\ \\partial y/\\partial \\xi  \\partial y/\\partial \\eta \\end{bmatrix} \\right)$.\n- **Modal Basis**:\n    - 1D basis: Normalized Legendre polynomials $\\phi_n(x) = \\sqrt{\\frac{2n+1}{2}}\\, P_n(x)$ for $n \\in \\{0,1,\\dots,N-1\\}$.\n    - 2D basis: Tensor product $\\psi_{i,j}(\\xi,\\eta) = \\phi_i(\\xi)\\, \\phi_j(\\eta)$ for $i,j \\in \\{0,1,\\dots,N-1\\}$, with $N=3$.\n- **Gram Matrix**: $G_{(i,j),(p,q)} = \\langle \\psi_{i,j}, \\psi_{p,q} \\rangle_F$.\n- **Quantification Metrics**:\n    - Orthogonality degradation: $E = \\frac{\\left\\| G - \\mathrm{diag}(\\mathrm{diag}(G)) \\right\\|_F}{\\left\\| \\mathrm{diag}(\\mathrm{diag}(G)) \\right\\|_F}$.\n    - Condition number: $\\kappa_2(G) = \\sigma_{\\max}(G) / \\sigma_{\\min}(G)$.\n- **Computational Method**: Use tensor-product Gaussian quadrature of sufficiently high order.\n- **Test Cases** $(\\alpha, \\beta)$: $(0.0,0.0)$, $(0.3,0.0)$, $(0.0,0.4)$, $(0.3,0.4)$, $(0.4,-0.3)$.\n- **Output Format**: Single line `[E_1,k_1,E_2,k_2,...,E_5,k_5]`.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It describes a standard procedure in the analysis of spectral element methods.\n- **Scientific Soundness**: The concepts used (isoparametric mapping, Lagrange basis, Legendre polynomials, Jacobian determinant, Gram matrix, Gaussian quadrature) are all standard and correct elements of numerical analysis and approximation theory.\n- **Completeness and Consistency**: All necessary formulas, parameters, and definitions are provided. The definitions for the nodes, mapping, and basis are unambiguous. The tasks are clearly stated. There are no contradictions.\n- **Well-Posedness**: The tasks involve the computation of well-defined mathematical quantities ($E$ and $\\kappa_2(G)$) from a given set of parameters. A unique solution exists for each test case.\n- **Quadrature Order**: The integrand for the Gram matrix is $\\psi_{i,j} \\psi_{p,q} J$. The basis functions $\\psi$ are polynomials of degree up to $N-1=2$. The mapping is built from degree-2 Lagrange polynomials. The partial derivatives of the mapping, e.g., $\\partial x / \\partial \\xi$, are polynomials of degree 1 in $\\xi$ and 2 in $\\eta$. The Jacobian determinant $J$ is therefore a polynomial of degree at most $(1+2)=3$ in $\\xi$ and $(2+1)=3$ in $\\eta$. The product of basis functions $\\psi_{i,j}\\psi_{p,q}$ has degree at most $(2+2)=4$ in each variable. Thus, the total integrand has polynomial degree at most $(4+3)=7$ in each variable. A Gaussian quadrature rule with $Q$ points is exact for polynomials of degree up to $2Q-1$. To be exact for degree $7$, we require $2Q-1 \\geq 7$, which implies $Q \\geq 4$. A choice of $Q=5$ points in each direction for the tensor-product quadrature is sufficient.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a standard, well-posed problem in computational mathematics. I will proceed with providing a complete solution.\n\n### Solution Design\nThe solution is implemented by following the tasks outlined in the problem statement. The core of the implementation is a loop over the five test cases defined by the distortion parameters $(\\alpha, \\beta)$. For each case, the following steps are executed:\n\n1.  **Define Geometry**: The $3 \\times 3$ grid of physical node coordinates $\\mathbf{P}_{ij} = (x_{ij}, y_{ij})$ is constructed based on the given $\\alpha$ and $\\beta$ values.\n\n2.  **Set up Quadrature**: We use a tensor-product Gauss-Legendre quadrature rule. Based on the analysis above, a rule with $Q=5$ points in each direction (a $5 \\times 5$ grid) is chosen to ensure the integrals for the Gram matrix are computed with sufficient accuracy. The quadrature points and weights are obtained from `scipy.special.roots_legendre`.\n\n3.  **Compute Jacobian**: The isoparametric mapping is given by $F(\\xi,\\eta) = (x(\\xi,\\eta), y(\\xi,\\eta))$. The partial derivatives, e.g., $\\frac{\\partial x}{\\partial \\xi} = \\sum_{i,j} L'_i(\\xi) L_j(\\eta) x_{ij}$, are computed at each of the $Q \\times Q$ quadrature points. This is done efficiently using NumPy's `einsum` function, which performs generalized tensor contractions. The Jacobian determinant $J(\\xi,\\eta)$ is then calculated at each quadrature point. A check confirms that $J  0$ over the element for all test cases, ensuring the mapping is valid (non-degenerate).\n\n4.  **Assemble Gram Matrix**: The modal basis consists of $N^2 = 3^2 = 9$ functions, $\\psi_{i,j}(\\xi,\\eta) = \\phi_i(\\xi)\\phi_j(\\eta)$ for $i,j \\in \\{0,1,2\\}$. The normalized Legendre polynomials $\\phi_n$ are evaluated at the 1D quadrature points. The Gram matrix $G$, of size $9 \\times 9$, has entries $G_{m,n} = \\int_{-1}^1\\int_{-1}^1 \\psi_m \\psi_n J \\, d\\xi d\\eta$, where $m, n$ are linear indices for the basis functions. This matrix is assembled by summing the contributions at each quadrature point, weighted by the product of the quadrature weights and the Jacobian determinant. This summation is also implemented efficiently. For example, by pre-calculating the values of all basis functions at all quadrature points and then using `einsum` or vectorized matrix operations to compute the final sum. The linear mapping from a 2D basis index $(i,j)$ to a 1D index $m$ is $m = 3i+j$.\n\n5.  **Calculate Metrics**:\n    - The orthogonality degradation, $E$, is computed by first separating the Gram matrix into its diagonal and off-diagonal parts, $G = D + G_{\\text{off}}$. The Frobenius norms of $D$ and $G_{\\text{off}}$ are calculated using `numpy.linalg.norm`, and their ratio gives $E$.\n    - The spectral condition number, $\\kappa_2(G)$, is calculated using `numpy.linalg.cond` with `p=2`, which computes the ratio of the largest to the smallest singular value of $G$.\n\n6.  **Store and Format Output**: The computed values of $E$ and $\\kappa_2(G)$ for each test case are collected and printed in the specified comma-separated format at the end of the execution. For the reference case $(\\alpha, \\beta)=(0,0)$, the mapping is affine, the Jacobian is constant, and the Gram matrix is diagonal. This results in $E=0$ and $\\kappa_2(G)=1$, which serves as a baseline and a validation of the implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import legendre, roots_legendre\n\ndef solve():\n    \"\"\"\n    Solves the isoparametric spectral element mapping problem.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 0.0),\n        (0.3, 0.0),\n        (0.0, 0.4),\n        (0.3, 0.4),\n        (0.4, -0.3),\n    ]\n\n    results = []\n\n    # Constants for the problem\n    N_modes_1D = 3  # N=3, so modes are for n=0, 1, 2\n    N_basis = N_modes_1D * N_modes_1D\n\n    # Determine required quadrature order.\n    # Integrand involves product of four basis polynomials (max deg 2+2=4 in each variable)\n    # and the Jacobian (max deg 3 in each variable).\n    # Total integrand degree is at most 7.\n    # Gaussian quadrature with Q points is exact up to degree 2Q-1.\n    # 2Q-1 >= 7  =>  Q >= 4. We choose Q=5 for safety.\n    Q = 5\n    q_pts, q_w = roots_legendre(Q)\n\n    # Define Lagrange basis polynomials and their derivatives for nodes {-1, 0, 1}\n    # L0 is for node -1, L1 for 0, L2 for 1\n    def L0(x): return x * (x - 1) / 2\n    def L1(x): return 1 - x**2\n    def L2(x): return x * (x + 1) / 2\n    \n    def L0_p(x): return x - 0.5\n    def L1_p(x): return -2 * x\n    def L2_p(x): return x + 0.5\n\n    # Pre-evaluate Lagrange polynomials and their derivatives at 1D quadrature points\n    L_vals = np.array([L0(q_pts), L1(q_pts), L2(q_pts)])  # Shape (3, Q)\n    L_deriv_vals = np.array([L0_p(q_pts), L1_p(q_pts), L2_p(q_pts)])  # Shape (3, Q)\n\n    # Pre-evaluate normalized Legendre polynomials at 1D quadrature points\n    phi_vals = np.zeros((N_modes_1D, Q))\n    for n in range(N_modes_1D):\n        Pn = legendre(n)\n        phi_vals[n, :] = np.sqrt((2 * n + 1) / 2) * Pn(q_pts)\n    \n    # Pre-evaluate 2D basis functions at all quadrature points\n    # psi_full[m, k, l] is value of basis function m at qp (q_pts[k], q_pts[l])\n    psi_full = np.zeros((N_basis, Q, Q))\n    for i in range(N_modes_1D):\n        for j in range(N_modes_1D):\n            m = i * N_modes_1D + j  # Row-major linear index\n            psi_full[m, :, :] = np.outer(phi_vals[i, :], phi_vals[j, :])\n\n    # 2D Quadrature weights\n    q_w_2d = np.outer(q_w, q_w)\n\n    for alpha, beta in test_cases:\n        # Step 1: Define physical node coordinates for the current case\n        P = np.zeros((3, 3, 2))\n        # Corners (indices [0,2] map to reference coords [-1,1])\n        P[0, 0] = [0, 0]\n        P[2, 0] = [1, 0]\n        P[2, 2] = [1, 1]\n        P[0, 2] = [0, 1 + alpha]\n        # Edge midpoints (index 1 maps to reference coord 0)\n        P[1, 0] = [0.5, 0]\n        P[2, 1] = [1, 0.5]\n        P[1, 2] = [0.5, 1 + alpha / 2]\n        P[0, 1] = [0, 0.5 + alpha / 2]\n        # Center\n        P[1, 1] = [0.5, 0.5 + alpha / 2 + beta / 4]\n\n        x_coords = P[:, :, 0]\n        y_coords = P[:, :, 1]\n\n        # Step 2: Compute Jacobian determinant at all quadrature points\n        # efficiently using Einstein summation convention\n        x_xi = np.einsum('ik,jl,ij->kl', L_deriv_vals, L_vals, x_coords)\n        y_xi = np.einsum('ik,jl,ij->kl', L_deriv_vals, L_vals, y_coords)\n        x_eta = np.einsum('ik,jl,ij->kl', L_vals, L_deriv_vals, x_coords)\n        y_eta = np.einsum('ik,jl,ij->kl', L_vals, L_deriv_vals, y_coords)\n\n        J_vals = x_xi * y_eta - x_eta * y_xi  # Shape (Q, Q)\n\n        # Confirm Jacobian is positive, as required by the problem\n        assert np.all(J_vals > 0), f\"Jacobian is not positive for (a,b)={alpha,beta}\"\n\n        # Step 3: Assemble the Gram matrix G\n        # G_mn = integral(psi_m * psi_n * J) d(xi,eta)\n        # We use quadrature: sum_{k,l} w_k*w_l * J(k,l) * psi_m(k,l) * psi_n(k,l)\n        \n        # Factor for integration: J(xi,eta) * d(xi) * d(eta)\n        integrand_factor = J_vals * q_w_2d # Shape (Q, Q)\n        \n        # Flatten the spatial dimensions for efficient matrix computation\n        psi_flat = psi_full.reshape(N_basis, Q*Q)\n        integrand_factor_flat = integrand_factor.flatten()\n\n        # G[m,n] = sum_p (psi_flat[m,p] * integrand_factor_flat[p] * psi_flat[n,p])\n        # This is equivalent to psi_flat @ diag(integrand_factor_flat) @ psi_flat.T\n        G = np.einsum('mp,p,np->mn', psi_flat, integrand_factor_flat, psi_flat)\n\n        # Step 4: Compute the metrics E and kappa_2(G)\n        \n        # Orthogonality degradation E\n        D = np.diag(np.diag(G))\n        G_off = G - D\n        \n        norm_D_fro = np.linalg.norm(D, 'fro')\n        # Avoid division by zero if G is a zero matrix (not possible here)\n        E = np.linalg.norm(G_off, 'fro') / norm_D_fro if norm_D_fro > 1e-15 else 0.0\n        \n        # Condition number kappa_2(G)\n        kappa = np.linalg.cond(G, 2)\n        \n        results.extend([E, kappa])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.7g}' for x in results)}]\")\n\nsolve()\n```", "id": "3419299"}]}