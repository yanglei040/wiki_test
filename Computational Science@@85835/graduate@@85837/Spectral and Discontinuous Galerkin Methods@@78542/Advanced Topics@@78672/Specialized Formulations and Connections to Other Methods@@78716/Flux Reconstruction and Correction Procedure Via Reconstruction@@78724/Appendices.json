{"hands_on_practices": [{"introduction": "Transitioning from theory to practice begins with implementation and stability analysis. This first exercise guides you through building a one-dimensional Flux Reconstruction (FR) scheme and investigating a crucial practical question: how does the choice of nodal distribution within an element impact numerical stability? By implementing the scheme for different solution points and analyzing the resulting system's eigenvalues, you will gain hands-on experience with the core components of FR and the methods used to assess its robustness [@problem_id:3386483].", "problem": "Consider the one-dimensional linear advection conservation law on a periodic domain,\n$$\nu_t + a\\,u_x = 0,\\quad x\\in[0,L],\\ t\\ge 0,\n$$\nwith constant advection speed $a>0$ and periodic boundary conditions. Discretize the domain into $N$ uniform elements of size $h=L/N$ and approximate the solution in each element by a polynomial of degree $p$ in the reference coordinate $\\xi\\in[-1,1]$. Within each element, use a nodal representation with $n=p+1$ distinct nodes $\\{\\xi_i\\}_{i=1}^n$ and their associated discrete mass matrix defined by quadrature weights.\n\nThe Flux Reconstruction (FR) method, also referred to as Correction Procedure via Reconstruction (CPR), augments the nodal derivative of the interpolated flux with a correction built from interface flux differences and chosen correction functions. In the strong, element-wise form with periodic coupling, and for $a>0$ (inflow at the left face), the semi-discrete scheme can be written as\n$$\n\\frac{d\\mathbf{u}_e}{dt} \\;=\\; -\\frac{2a}{h}\\left( \\mathbf{D}\\,\\mathbf{u}_e \\;+\\; c\\,\\mathbf{g}'_L\\left( u^{\\text{num}}_{e-1,R} - u^{\\text{poly}}_{e,L} \\right)\\right),\n$$\nwhere $\\mathbf{u}_e\\in\\mathbb{R}^n$ is the vector of nodal solution values in element $e$, $\\mathbf{D}\\in\\mathbb{R}^{n\\times n}$ is the nodal differentiation matrix for the chosen nodes, $u^{\\text{poly}}_{e,L}$ is the left boundary value obtained by interpolation, $u^{\\text{num}}_{e-1,R}$ is the incoming right boundary value from the left neighbor via the numerical flux, and $c$ is a scalar correction amplitude. The boundary values emerge from evaluating the nodal Lagrange basis at $\\xi=-1$ and $\\xi=+1$. Denote the corresponding evaluation row vectors by $\\mathbf{v}_L^T$ and $\\mathbf{v}_R^T$, respectively, so that $u^{\\text{poly}}_{e,L}=\\mathbf{v}_L^T\\mathbf{u}_e$ and $u^{\\text{num}}_{e-1,R}=\\mathbf{v}_R^T\\mathbf{u}_{e-1}$ for $a>0$ with upwinding. The discrete mass matrix $\\mathbf{M}\\in\\mathbb{R}^{n\\times n}$ is defined as the diagonal matrix of quadrature weights associated with the chosen nodal distribution, providing the discrete inner product $\\langle \\mathbf{u},\\mathbf{v}\\rangle_{\\mathbf{M}}=\\mathbf{u}^T\\mathbf{M}\\mathbf{v}$.\n\nYou will compare three nodal distributions:\n- Legendre–Gauss nodes with their exact Gauss quadrature weights.\n- Legendre–Gauss–Lobatto nodes with Lobatto quadrature weights.\n- Equidistant nodes on $[-1,1]$ with closed Newton–Cotes quadrature weights determined by exactness on polynomials up to degree $n-1$.\n\nFor the correction function at the left face, select the unique polynomial of degree $p+1$ defined by\n$$\ng_L(\\xi) = s_p\\;\\frac{1-\\xi}{2}\\;P_p(\\xi),\\quad s_p := \\frac{1}{P_p(-1)} = (-1)^p,\n$$\nwhere $P_p(\\xi)$ is the Legendre polynomial of degree $p$. This choice enforces $g_L(-1)=1$, $g_L(+1)=0$, and yields a non-constant derivative\n$$\ng_L'(\\xi) = s_p\\left[ -\\frac{1}{2}P_p(\\xi) + \\frac{1-\\xi}{2}P_p'(\\xi)\\right],\n$$\nwhose nodal sampling produces the vector $\\mathbf{g}'_L\\in\\mathbb{R}^n$ used in the FR correction.\n\nDefine the global semi-discrete operator $\\mathbf{A}\\in\\mathbb{R}^{(Nn)\\times(Nn)}$ that advances the concatenated state $\\mathbf{U}\\in\\mathbb{R}^{Nn}$ formed by stacking all element-wise nodal states $\\{\\mathbf{u}_e\\}_{e=0}^{N-1}$. With periodic coupling, the operator has block structure with intra-element terms involving $\\mathbf{D}$ and inter-element inflow coupling through $\\mathbf{g}'_L\\mathbf{v}_R^T$. Let the global block-diagonal mass matrix be $\\mathbf{M}_{\\text{glob}}=\\operatorname{diag}(\\mathbf{M},\\dots,\\mathbf{M})\\in\\mathbb{R}^{(Nn)\\times(Nn)}$.\n\nAs a stability metric based on the energy in the $\\mathbf{M}_{\\text{glob}}$ inner product, consider the symmetric operator\n$$\n\\mathbf{S}_{\\mathbf{M}} = \\frac{1}{2}\\left(\\mathbf{M}_{\\text{glob}}\\mathbf{A} + \\mathbf{A}^T\\mathbf{M}_{\\text{glob}}\\right).\n$$\nIts generalized eigenvalues with respect to $\\mathbf{M}_{\\text{glob}}$ characterize the instantaneous energy growth rate: non-positivity of the largest eigenvalue implies non-increasing discrete energy. To compare nodal distributions, you must:\n1. Construct $\\mathbf{A}$ for each nodal distribution and given $(p,N)$ using upwind numerical flux for $a>0$ on a periodic domain of length $L=1$.\n2. Compute the largest generalized eigenvalue of $\\mathbf{S}_{\\mathbf{M}}$ with respect to $\\mathbf{M}_{\\text{glob}}$, equivalently the maximum eigenvalue of the symmetric matrix $\\mathbf{C}=\\mathbf{M}_{\\text{glob}}^{-1/2}\\mathbf{S}_{\\mathbf{M}}\\mathbf{M}_{\\text{glob}}^{-1/2}$, as a real scalar stability indicator.\n3. Derive a correction scaling law $c_{\\text{norm}}$ for each nodal distribution that normalizes the effect of the discrete mass matrix across $n$ points, by requiring that the $\\mathbf{M}$-weighted norm of the derivative correction vector be equalized across nodal sets. Specifically, choose for each nodal set $S$,\n$$\nc_{\\text{norm}}^{(S)} \\ \\text{such that}\\ \\ \\left\\|\\mathbf{g}'_L\\right\\|_{\\mathbf{M}^{(S)}} \\, c_{\\text{norm}}^{(S)} \\ = \\ \\left\\|\\mathbf{g}'_L\\right\\|_{\\mathbf{M}^{(\\text{ref})}},\n$$\nwhere $\\|\\mathbf{w}\\|_{\\mathbf{M}} := \\sqrt{\\mathbf{w}^T\\mathbf{M}\\mathbf{w}}$ and $(\\text{ref})$ denotes a fixed reference nodal set. Use Legendre–Gauss–Lobatto as the reference in this task. Implement this normalization in the FR operator by scaling the correction amplitude $c$ with $c_{\\text{norm}}^{(S)}$.\n4. Quantify the impact of nodal distributions on the stability metric before and after applying the normalization.\n\nYour program must implement the above and produce numerical results for the following test suite, with $a=1$ and $L=1$:\n- Test case 1: $(p=2, N=10)$.\n- Test case 2: $(p=3, N=8)$.\n- Test case 3: $(p=5, N=6)$.\n\nFor each test case, and for each nodal distribution in the order Legendre–Gauss, Legendre–Gauss–Lobatto, Equidistant, compute two real numbers:\n- The largest eigenvalue of $\\mathbf{C}$ with $c=1$ (no normalization).\n- The largest eigenvalue of $\\mathbf{C}$ with the normalized correction amplitude $c=c_{\\text{norm}}^{(S)}$ derived above.\n\nThe final output format must be a single line containing a list of three lists, one per test case. Each inner list must contain six floating-point numbers in the order\n$$\n\\big[\\lambda_{\\max}^{\\text{Gauss}},\\ \\lambda_{\\max}^{\\text{Lobatto}},\\ \\lambda_{\\max}^{\\text{Equid}},\\ \\lambda_{\\max,\\text{norm}}^{\\text{Gauss}},\\ \\lambda_{\\max,\\text{norm}}^{\\text{Lobatto}},\\ \\lambda_{\\max,\\text{norm}}^{\\text{Equid}}\\big],\n$$\nwhere each $\\lambda_{\\max}$ is the largest eigenvalue of $\\mathbf{C}$ for the specified configuration. The line must be printed exactly as a Python list of lists, for example:\n`[[x_{11},x_{12},x_{13},x_{14},x_{15},x_{16}],[x_{21},...,x_{26}],[x_{31},...,x_{36}]]`\nNo units are involved in this problem. Angles are not used. Percentages are not used.", "solution": "The user wants to analyze the stability of the Flux Reconstruction (FR) / Correction Procedure via Reconstruction (CPR) method for solving the one-dimensional linear advection equation. This analysis involves comparing three different sets of nodal points within each element: Legendre-Gauss (LG), Legendre-Gauss-Lobatto (LGL), and Equidistant nodes. The stability will be quantified by computing the largest eigenvalue of a specific symmetrized operator derived from the semi-discrete system. The problem also requires deriving and applying a normalization factor to the correction term to ensure a fair comparison between the different nodal distributions.\n\n### **Problem Validation**\n\n**Step 1: Extract Givens**\n\n- **Conservation Law**: $u_t + a\\,u_x = 0$, for $x\\in[0,L]$, $t\\ge 0$.\n- **Advection Speed**: Constant $a>0$.\n- **Boundary Conditions**: Periodic on $[0,L]$.\n- **Discretization**: $N$ uniform elements, size $h=L/N$.\n- **Approximation**: Polynomial of degree $p$ on $n=p+1$ nodes $\\{\\xi_i\\}_{i=1}^n$ in the reference element $\\xi\\in[-1,1]$.\n- **Nodal Sets**:\n  1. Legendre-Gauss (LG) nodes and weights.\n  2. Legendre-Gauss-Lobatto (LGL) nodes and weights.\n  3. Equidistant nodes with closed Newton-Cotes weights (exact for polynomials up to degree $n-1$).\n- **Semi-Discrete FR/CPR Form**: $\\frac{d\\mathbf{u}_e}{dt} = -\\frac{2a}{h}\\left( \\mathbf{D}\\,\\mathbf{u}_e + c\\,\\mathbf{g}'_L\\left( u^{\\text{num}}_{e-1,R} - u^{\\text{poly}}_{e,L} \\right)\\right)$.\n- **Terms in FR Form**:\n  - $\\mathbf{u}_e$: Vector of nodal solution values in element $e$.\n  - $\\mathbf{D}$: Nodal differentiation matrix.\n  - $u^{\\text{poly}}_{e,L} = \\mathbf{v}_L^T\\mathbf{u}_e$: Interpolated solution at the left boundary ($\\xi=-1$).\n  - $u^{\\text{num}}_{e-1,R} = \\mathbf{v}_R^T\\mathbf{u}_{e-1}$: Numerical flux from the right boundary of the left neighbor (upwinding for $a>0$).\n  - $c$: Scalar correction amplitude.\n  - $\\mathbf{g}'_L$: Vector of nodal values of the derivative of the correction function $g_L(\\xi)$.\n- **Correction Function**: $g_L(\\xi) = s_p\\;\\frac{1-\\xi}{2}\\;P_p(\\xi)$ with $s_p = (-1)^p$, where $P_p(\\xi)$ is the Legendre polynomial of degree $p$.\n- **Mass Matrix**: $\\mathbf{M}$ is a diagonal matrix of quadrature weights. Global mass matrix is $\\mathbf{M}_{\\text{glob}}=\\operatorname{diag}(\\mathbf{M},\\dots,\\mathbf{M})$.\n- **Stability Metric**: Largest eigenvalue of the symmetric matrix $\\mathbf{C}=\\mathbf{M}_{\\text{glob}}^{-1/2}\\mathbf{S}_{\\mathbf{M}}\\mathbf{M}_{\\text{glob}}^{-1/2}$, where $\\mathbf{S}_{\\mathbf{M}} = \\frac{1}{2}\\left(\\mathbf{M}_{\\text{glob}}\\mathbf{A} + \\mathbf{A}^T\\mathbf{M}_{\\text{glob}}\\right)$, and $\\mathbf{A}$ is the global semi-discrete operator ($\\frac{d\\mathbf{U}}{dt} = \\mathbf{A}\\mathbf{U}$).\n- **Correction Normalization**: Determine $c_{\\text{norm}}^{(S)}$ for each nodal set $S$ such that $\\left\\|\\mathbf{g}'_L\\right\\|_{\\mathbf{M}^{(S)}} \\, c_{\\text{norm}}^{(S)} = \\left\\|\\mathbf{g}'_L\\right\\|_{\\mathbf{M}^{(\\text{ref})}}$, where the reference set is LGL. The norm is $\\|\\mathbf{w}\\|_{\\mathbf{M}} = \\sqrt{\\mathbf{w}^T\\mathbf{M}\\mathbf{w}}$.\n- **Constants**: $a=1$, $L=1$.\n- **Test Cases**: $(p=2, N=10)$; $(p=3, N=8)$; $(p=5, N=6)$.\n- **Required Output**: For each test case, a list of six eigenvalues: $[\\lambda_{\\max}^{\\text{LG}}, \\lambda_{\\max}^{\\text{LGL}}, \\lambda_{\\max}^{\\text{Equid}}]$ with $c=1$, followed by $[\\lambda_{\\max,\\text{norm}}^{\\text{LG}}, \\lambda_{\\max,\\text{norm}}^{\\text{LGL}}, \\lambda_{\\max,\\text{norm}}^{\\text{Equid}}]$ with $c=c_{\\text{norm}}$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem statement is examined against the validation criteria.\n\n- **Scientific Groundedness**: The problem is a standard exercise in the field of high-order numerical methods for PDEs, specifically focusing on the spectral properties of the FR/CPR formulation. All concepts (nodal bases, differentiation matrices, FR correction, energy stability analysis) are well-established in the scientific literature.\n- **Well-Posedness**: The problem is mathematically and algorithmically well-defined. All components necessary for the construction of the operators and subsequent eigenvalue analysis are specified. The tasks lead to a unique set of numerical results.\n- **Objectivity**: The problem is stated using precise, objective mathematical language, free from ambiguity or subjective claims.\n- **Flaw Checklist**:\n  1.  **Scientific/Factual Unsoundness**: None. The formulation is correct.\n  2.  **Non-Formalizable/Irrelevant**: None. The problem is a formal numerical analysis task directly relevant to its stated topic.\n  3.  **Incomplete/Contradictory Setup**: None. All required definitions for nodes, weights, matrices, and operators are provided or can be uniquely derived from standard principles.\n  4.  **Unrealistic/Infeasible**: None. The problem is a standard numerical experiment with feasible parameters.\n  5.  **Ill-Posed/Poorly Structured**: None. The problem is well-structured and leads to a unique, meaningful numerical solution (the set of eigenvalues).\n  6.  **Pseudo-Profound/Trivial**: None. The problem requires a non-trivial implementation of several concepts from numerical analysis and linear algebra, representing a legitimate computational task.\n  7.  **Outside Scientific Verifiability**: None. The results are numerically computable and can be independently verified.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A complete solution will be provided.\n\n### **Solution Methodology**\n\nThe solution requires constructing the global semi-discrete operator $\\mathbf{A}$ and the global mass matrix $\\mathbf{M}_{\\text{glob}}$ for various configurations of polynomial degree $p$, element count $N$, and nodal distributions. The core steps for each configuration are:\n\n1.  **Generate Nodal Data**: For each nodal set (LG, LGL, Equidistant) with $n=p+1$ points:\n    -   Compute the node locations $\\{\\xi_i\\}_{i=1}^n \\in [-1, 1]$.\n    -   Compute the corresponding quadrature weights $\\{w_i\\}_{i=1}^n$. These form the diagonal of the element mass matrix $\\mathbf{M}$.\n\n2.  **Construct Element-wise Matrices**:\n    -   **Differentiation Matrix ($\\mathbf{D}$)**: Compute the $n \\times n$ matrix where $D_{ij} = \\ell_j'(\\xi_i)$, with $\\ell_j$ being the $j$-th Lagrange polynomial. A stable method using barycentric weights will be employed.\n    -   **Boundary Evaluation Vectors ($\\mathbf{v}_L^T, \\mathbf{v}_R^T$)**: These are row vectors containing the values of the Lagrange basis functions at $\\xi=-1$ and $\\xi=1$, respectively. For LGL nodes, which include the endpoints, these vectors are trivial (e.g., $\\mathbf{v}_L^T = [1, 0, \\dots, 0]$). For other nodes, they must be computed.\n    -   **Correction Function Vector ($\\mathbf{g}'_L$)**: Evaluate the derivative of the specified correction function, $g_L'(\\xi) = (-1)^p\\left[ -\\frac{1}{2}P_p(\\xi) + \\frac{1-\\xi}{2}P_p'(\\xi)\\right]$, at each nodal point $\\xi_i$ to form the vector $\\mathbf{g}'_L$.\n\n3.  **Normalization Factor Calculation**:\n    -   For each nodal set $S \\in \\{\\text{LG, LGL, Equidistant}\\}$, calculate the normalization factor $c_{\\text{norm}}^{(S)} = \\frac{\\|\\mathbf{g}'_L\\|_{\\mathbf{M}^{(\\text{LGL})}}}{\\|\\mathbf{g}'_L\\|_{\\mathbf{M}^{(S)}}}$. The reference norm is computed using the LGL nodes and weights for the given degree $p$. The $\\mathbf{M}$-norm is $\\|\\mathbf{w}\\|_{\\mathbf{M}} = \\sqrt{\\mathbf{w}^T \\mathbf{M} \\mathbf{w}}$. By definition, $c_{\\text{norm}}^{(\\text{LGL})} = 1$.\n\n4.  **Assemble Global Operator ($\\mathbf{A}$)**:\n    -   The global operator $\\mathbf{A}$ is an $(Nn) \\times (Nn)$ block-circulant matrix. Its definition arises from substituting the numerical flux into the FR scheme:\n      $$ \\frac{d\\mathbf{u}_e}{dt} = -\\frac{2a}{h}\\left( (\\mathbf{D} - c\\mathbf{g}'_L\\mathbf{v}_L^T)\\mathbf{u}_e + (c\\mathbf{g}'_L\\mathbf{v}_R^T)\\mathbf{u}_{e-1} \\right) $$\n    -   The diagonal blocks are $\\mathbf{A}_{ee} = -\\frac{2a}{h}(\\mathbf{D} - c\\mathbf{g}'_L\\mathbf{v}_L^T)$.\n    -   The sub-diagonal blocks (and the top-right block due to periodicity) are $\\mathbf{A}_{e,e-1} = -\\frac{2a}{h}(c\\mathbf{g}'_L\\mathbf{v}_R^T)$.\n    -   This is performed for both $c=1$ and $c=c_{\\text{norm}}^{(S)}$.\n\n5.  **Compute Stability Metric**:\n    -   Construct the global mass matrix $\\mathbf{M}_{\\text{glob}}$ as a block-diagonal matrix of the element mass matrices $\\mathbf{M}$.\n    -   Form the symmetric part of the mass-scaled operator: $\\mathbf{S}_{\\mathbf{M}} = \\frac{1}{2}(\\mathbf{M}_{\\text{glob}}\\mathbf{A} + \\mathbf{A}^T\\mathbf{M}_{\\text{glob}})$.\n    -   To find the generalized eigenvalues of $(\\mathbf{S_M}, \\mathbf{M}_{\\text{glob}})$, we solve the equivalent standard eigenvalue problem for the symmetric matrix $\\mathbf{C} = \\mathbf{M}_{\\text{glob}}^{-1/2} \\mathbf{S_M} \\mathbf{M}_{\\text{glob}}^{-1/2}$.\n    -   The largest eigenvalue of $\\mathbf{C}$ is computed. Since $\\mathbf{C}$ is symmetric, its eigenvalues are real, and efficient algorithms can be used. A non-positive value implies non-increasing energy (stability).\n\nThis entire process is repeated for each test case $(p, N)$ and each of the three nodal distributions.", "answer": "```python\nimport numpy as np\nfrom scipy.special import legendre, roots_legendre, roots_jacobi\nfrom scipy.linalg import eigh\n\ndef get_nodes_and_weights(n, node_type):\n    \"\"\"\n    Computes nodal points and quadrature weights for a given type.\n    n: number of points (p+1)\n    node_type: 'gauss', 'lobatto', or 'equidistant'\n    \"\"\"\n    if node_type == 'gauss':\n        nodes, weights = roots_legendre(n)\n        return np.array(nodes), np.array(weights)\n    elif node_type == 'lobatto':\n        p = n - 1\n        if n == 1:\n            return np.array([0.0]), np.array([2.0])\n        if n == 2:\n            return np.array([-1.0, 1.0]), np.array([1.0, 1.0])\n        \n        interior_nodes, _ = roots_jacobi(n - 2, 1, 1)\n        nodes = np.concatenate(([-1.0], np.sort(interior_nodes), [1.0]))\n        \n        P_p = legendre(p)\n        weights = 2.0 / (n * p * P_p(nodes)**2)\n        return nodes, weights\n    elif node_type == 'equidistant':\n        nodes = np.linspace(-1.0, 1.0, n)\n        # Newton-Cotes weights by solving Vandermonde system\n        V = np.vander(nodes, n, increasing=True)\n        rhs = np.zeros(n)\n        for i in range(n):\n            rhs[i] = (1.0 - (-1.0)**(i + 1)) / (i + 1)\n        weights = np.linalg.solve(V.T, rhs)\n        return nodes, weights\n    else:\n        raise ValueError(f\"Unknown node type: {node_type}\")\n\ndef lagrange_diff_matrix(nodes):\n    \"\"\"\n    Computes the differentiation matrix using the barycentric formula.\n    \"\"\"\n    n = len(nodes)\n    D = np.zeros((n, n))\n    \n    # Barycentric weights\n    w = np.ones(n)\n    for j in range(n):\n        for k in range(n):\n            if k != j:\n                w[j] *= (nodes[j] - nodes[k])\n    w = 1.0 / w\n    \n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                D[i, j] = (w[j] / w[i]) / (nodes[i] - nodes[j])\n                \n    for i in range(n):\n        D[i, i] = -np.sum(D[i, :])\n    return D\n\ndef get_boundary_vectors(nodes):\n    \"\"\"\n    Computes Lagrange basis evaluation vectors at xi = -1 and xi = 1.\n    \"\"\"\n    n = len(nodes)\n    # Check if nodes are Lobatto type (endpoints included)\n    if np.isclose(nodes[0], -1.0) and np.isclose(nodes[-1], 1.0):\n        vL = np.zeros(n)\n        vL[0] = 1.0\n        vR = np.zeros(n)\n        vR[-1] = 1.0\n        return vL, vR\n    \n    # Otherwise, compute via explicit Lagrange polynomial evaluation.\n    def lagrange_eval(x, j, eval_nodes):\n        num, den = 1.0, 1.0\n        for m, xm in enumerate(eval_nodes):\n            if m != j:\n                num *= (x - xm)\n                den *= (eval_nodes[j] - xm)\n        return num / den\n\n    vL = np.array([lagrange_eval(-1.0, j, nodes) for j in range(n)])\n    vR = np.array([lagrange_eval(1.0, j, nodes) for j in range(n)])\n    return vL, vR\n\ndef get_correction_vector(p, nodes):\n    \"\"\"\n    Computes the nodal vector for the derivative of the g_L correction function.\n    \"\"\"\n    sp = (-1.0)**p\n    Pp = legendre(p)\n    Pp_prime = Pp.deriv(1)\n    \n    g_prime_vals = sp * (-0.5 * Pp(nodes) + 0.5 * (1.0 - nodes) * Pp_prime(nodes))\n    return g_prime_vals\n\ndef compute_max_eigenvalue(A, M_glob):\n    \"\"\"\n    Computes the largest generalized eigenvalue of (S_M, M_glob).\n    \"\"\"\n    S_M = 0.5 * (M_glob @ A + A.T @ M_glob)\n    # Using scipy.linalg.eigh for generalized symmetric eigenproblem\n    # It is generally more stable than forming C explicitly.\n    # It returns eigenvalues in ascending order.\n    eigvals = eigh(S_M, M_glob, eigvals_only=True)\n    return eigvals[-1]\n\ndef solve_one_case(p, N, a, L):\n    \"\"\"\n    Solves the problem for one (p, N) test case.\n    \"\"\"\n    n = p + 1\n    h = L / N\n    \n    node_types = ['gauss', 'lobatto', 'equidistant']\n    \n    # Calculate normalization constants\n    ref_nodes, ref_weights = get_nodes_and_weights(n, 'lobatto')\n    ref_g_prime = get_correction_vector(p, ref_nodes)\n    ref_M_elem = np.diag(ref_weights)\n    norm_ref = np.sqrt(ref_g_prime.T @ ref_M_elem @ ref_g_prime)\n    \n    c_norms = {}\n    for nt in node_types:\n        nodes_s, weights_s = get_nodes_and_weights(n, nt)\n        g_prime_s = get_correction_vector(p, nodes_s)\n        M_elem_s = np.diag(weights_s)\n        norm_s = np.sqrt(g_prime_s.T @ M_elem_s @ g_prime_s)\n        c_norms[nt] = norm_ref / norm_s if norm_s > 1e-15 else 1.0\n\n    unnormalized_eigs = []\n    normalized_eigs = []\n\n    for node_type in node_types:\n        nodes, weights = get_nodes_and_weights(n, node_type)\n        D = lagrange_diff_matrix(nodes)\n        M_elem = np.diag(weights)\n        vL_T, vR_T = get_boundary_vectors(nodes)\n        g_prime_vec = get_correction_vector(p, nodes)\n        \n        # Calculate for both c=1 and c=c_norm\n        for c_val, eig_list in zip([1.0, c_norms[node_type]], [unnormalized_eigs, normalized_eigs]):\n            dof = N * n\n            \n            # semi-discrete operator blocks\n            prefactor = -2.0 * a / h\n            A_diag_block = prefactor * (D - c_val * np.outer(g_prime_vec, vL_T))\n            A_offdiag_block = prefactor * (c_val * np.outer(g_prime_vec, vR_T))\n            \n            # Assemble global operator A\n            A = np.zeros((dof, dof))\n            for e in range(N):\n                e_slice = slice(e * n, (e + 1) * n)\n                em1 = (e - 1 + N) % N\n                em1_slice = slice(em1 * n, (em1 + 1) * n)\n                \n                A[e_slice, e_slice] = A_diag_block\n                A[e_slice, em1_slice] = A_offdiag_block\n                \n            M_glob = np.kron(np.eye(N), M_elem)\n            \n            max_eig = compute_max_eigenvalue(A, M_glob)\n            eig_list.append(max_eig)\n\n    return unnormalized_eigs + normalized_eigs\n\ndef solve():\n    \"\"\"\n    Main driver function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        (2, 10),  # p, N\n        (3, 8),\n        (5, 6),\n    ]\n    a = 1.0\n    L = 1.0\n    \n    final_results = []\n    for p, N in test_cases:\n        case_results = solve_one_case(p, N, a, L)\n        final_results.append(case_results)\n        \n    # The output format must be a Python list of lists.\n    # str(list) provides the canonical representation.\n    # The example in the prompt `[[x11,x12,...]]` has no spaces.\n    # We will remove them to match the example's formatting precisely.\n    print(str(final_results).replace(\" \", \"\"))\n\nsolve()\n\n```", "id": "3386483"}, {"introduction": "A stable scheme is necessary but not sufficient; it must also be accurate. When dealing with nonlinear conservation laws, a primary threat to accuracy is aliasing error, which occurs when the numerical quadrature cannot exactly integrate the polynomial flux approximation. This analytical practice challenges you to determine the precise conditions required to prevent such errors, ensuring the FR scheme achieves its full design order of accuracy for nonlinear problems [@problem_id:3386485].", "problem": "Consider the one-dimensional conservation law $u_{t} + \\partial_{x} f(u) = 0$ on a periodic domain, discretized by the Flux Reconstruction (FR) method, also known as the Correction Procedure via Reconstruction (CPR), on a uniform mesh of affine-mapped elements with local coordinate $\\xi \\in [-1,1]$. On each element, the approximate solution $u^{h}(\\xi)$ is represented by a polynomial of degree $N$ in $\\xi$, constructed from nodal values at the Gauss–Lobatto–Legendre (GLL) points. The weak form of the volume term is taken in the standard FR/DG-integrated-by-parts form and evaluated with a volume quadrature.\n\nAssume a nonlinear flux $f(u) = u^{p}$ with an integer $p \\geq 2$. The design-order of accuracy for smooth solutions, in the absence of quadrature aliasing and assuming sufficiently regular correction functions, is $N+1$. Under-integration (aliasing) in the volume term arises when the chosen quadrature does not integrate the polynomial integrand of the volume term exactly, thereby degrading the formal order of accuracy below $N+1$.\n\nUsing only well-tested facts about polynomial degrees under composition and multiplication, and the classical exactness properties of Gauss–Lobatto–Legendre quadrature with $Q$ points (exact for polynomials up to degree $2Q-3$) and Gauss–Legendre quadrature with $M$ points (exact up to degree $2M-1$), carry out the following:\n\n1. Identify the maximum polynomial degree of the volume integrand that must be exactly integrated on each element for the weak form with test functions of degree at most $N$.\n2. Explain how, for $Q=N+1$, this leads to aliasing for $p \\geq 2$ and thus degrades the formal order of accuracy from the $N+1$ design-order.\n3. Propose a sufficient over-integration condition by choosing a Gauss–Legendre volume quadrature with $M$ points so that the volume term is integrated exactly and the design-order $N+1$ is recovered.\n\nYour final answer must be a single closed-form analytic expression giving the minimal $M$ in terms of $N$ and $p$ that guarantees exact integration of the volume term. No numerical approximation or rounding is required.", "solution": "The problem statement is evaluated as scientifically sound, well-posed, objective, and self-contained. It presents a standard analysis problem in the field of high-order numerical methods for partial differential equations. All provided information is factually correct and pertinent to the task. Therefore, a full solution is warranted.\n\nThe general one-dimensional conservation law is given by\n$$\nu_{t} + \\partial_{x} f(u) = 0\n$$\nwhere $u$ is the conserved variable and $f(u)$ is the flux function. For this problem, the flux is specified as a nonlinear function $f(u) = u^{p}$ for some integer $p \\geq 2$.\n\nThe domain is discretized into elements. On a reference element, with local coordinate $\\xi \\in [-1, 1]$, the solution $u^{h}(\\xi)$ is approximated by a polynomial of degree at most $N$. The test functions $v^{h}(\\xi)$ for the weak formulation are drawn from the same polynomial space, so $\\deg(v^{h}) \\leq N$.\n\nThe weak form of the governing equation, when integrated by parts on a given element $K$ and transformed to the reference element, involves a volume integral term of the form:\n$$\n\\int_{-1}^{1} f(u^{h}(\\xi)) \\frac{d v^{h}}{d \\xi} \\, d\\xi\n$$\nA numerical quadrature scheme is employed to approximate this integral. The accuracy of the overall numerical method is contingent upon the exactness of this quadrature for the specific integrand polynomial. The integrand is the product $I(\\xi) = f(u^{h}(\\xi)) \\frac{d v^{h}}{d \\xi}$.\n\n**1. Maximum Polynomial Degree of the Volume Integrand**\n\nTo determine the required exactness of the quadrature rule, we must first establish the maximum possible polynomial degree of the integrand $I(\\xi)$. This analysis relies on fundamental properties of polynomial multiplication and composition.\n\n- The solution approximation $u^{h}(\\xi)$ is a polynomial of degree at most $N$. We denote this as $\\deg(u^{h}) \\leq N$.\n- The flux function is $f(u) = u^{p}$. When applied to the polynomial solution approximation, we get $f(u^{h}(\\xi)) = (u^{h}(\\xi))^{p}$. The degree of the composition is the product of the degrees, so $\\deg(f(u^{h})) = p \\cdot \\deg(u^{h})$. The maximum degree is thus $p N$.\n- The test function $v^{h}(\\xi)$ is also a polynomial of degree at most $N$, so $\\deg(v^{h}) \\leq N$.\n- Its derivative with respect to the local coordinate $\\xi$, $\\frac{d v^{h}}{d \\xi}$, is a polynomial of degree at most $N-1$. So, $\\deg(\\frac{d v^{h}}{d \\xi}) \\leq N-1$.\n- The integrand $I(\\xi)$ is the product of $f(u^{h}(\\xi))$ and $\\frac{d v^{h}}{d \\xi}$. The degree of a product of polynomials is the sum of their individual degrees.\n- Therefore, the maximum degree of the integrand is:\n$$\n\\deg(I) = \\deg(f(u^{h})) + \\deg\\left(\\frac{d v^{h}}{d \\xi}\\right) = pN + (N-1) = (p+1)N - 1\n$$\nFor the weak form to be satisfied without introducing quadrature error, the volume integral must be computed exactly. This requires a quadrature rule that is exact for all polynomials up to degree $(p+1)N - 1$.\n\n**2. Aliasing with Standard GLL Quadrature**\n\nThe problem states that a common approach is to use the $Q$ Gauss–Lobatto–Legendre (GLL) points, where $Q = N+1$, for the volume quadrature. The classical theory of numerical quadrature states that a GLL rule with $Q$ points integrates polynomials of degree up to $2Q-3$ exactly.\n\n- Substituting $Q = N+1$ into the GLL exactness formula, we find that this quadrature rule is exact for polynomials of degree up to:\n$$\n2(N+1) - 3 = 2N + 2 - 3 = 2N - 1\n$$\n- Aliasing, which leads to a degradation of the formal order of accuracy, occurs when the degree of the integrand exceeds the degree for which the quadrature rule is exact. This condition is:\n$$\n\\deg(I) > 2N - 1\n$$\n- Substituting the expression for $\\deg(I)$ from Part 1:\n$$\n(p+1)N - 1 > 2N - 1\n$$\n- This inequality simplifies to:\n$$\n(p+1)N > 2N\n$$\n- Assuming a non-trivial polynomial approximation space (i.e., $N \\geq 1$), we can divide by $N$:\n$$\np+1 > 2 \\implies p > 1\n$$\n- The problem specifies that $p$ is an integer with $p \\geq 2$. For any such $p$, the condition $p > 1$ is satisfied. Consequently, for any nonlinear flux of the form $u^p$ with $p \\geq 2$, the degree of the volume integrand, $(p+1)N-1$, is strictly greater than the degree of exactness, $2N-1$, provided by the standard $Q=N+1$ GLL quadrature. This inexact integration introduces aliasing errors, which corrupt the numerical solution and prevent the method from achieving its design order of accuracy of $N+1$.\n\n**3. Sufficient Over-integration Condition**\n\nTo recover the design order of accuracy, the volume integral must be computed exactly. This can be achieved by employing a quadrature rule with a sufficiently high degree of exactness, a technique known as over-integration. We are tasked with finding a sufficient number of points, $M$, for a Gauss–Legendre (GL) volume quadrature.\n\n- A GL quadrature rule with $M$ points is known to be exact for polynomials of degree up to $2M-1$.\n- To ensure exact integration of the volume term, the degree of exactness of the GL rule must be greater than or equal to the maximum degree of the integrand polynomial, $\\deg(I)$.\n- This gives the condition:\n$$\n2M - 1 \\geq \\deg(I)\n$$\n- Substituting the derived degree of the integrand:\n$$\n2M - 1 \\geq (p+1)N - 1\n$$\n- Simplifying the inequality:\n$$\n2M \\geq (p+1)N\n$$\n- Solving for $M$:\n$$\nM \\geq \\frac{(p+1)N}{2}\n$$\n- Since $M$ must be an integer representing the number of quadrature points, the minimal integer value for $M$ that satisfies this condition is the smallest integer not less than $\\frac{(p+1)N}{2}$. This is given by the ceiling function.\n- The minimal number of Gauss–Legendre points required to guarantee exact integration of the volume term is:\n$$\nM_{min} = \\left\\lceil \\frac{(p+1)N}{2} \\right\\rceil\n$$\nThis choice of $M$ ensures that no aliasing error is introduced from the volume term integration, which is a necessary condition for the FR/CPR scheme to achieve its formal design order of accuracy of $N+1$ for smooth solutions.", "answer": "$$\\boxed{\\left\\lceil \\frac{(p+1)N}{2} \\right\\rceil}$$", "id": "3386485"}, {"introduction": "Beyond stability and accuracy lies the domain of performance, where algorithmic theory meets hardware reality. This final practice shifts focus to the computational efficiency of the FR correction step, particularly on parallel processors like GPUs, where memory access patterns are paramount. Using a performance model, you will analyze how different data layouts and loop structures affect memory access efficiency and overall runtime, providing crucial insights into optimizing high-order methods for modern high-performance computing environments [@problem_id:3386482].", "problem": "Consider the correction step in the flux reconstruction (FR) method, also known as the correction procedure via reconstruction (CPR). In one spatial dimension with polynomial degree per element denoted by $p$, let the number of solution points per element be $n_p = p + 1$. For element index $e \\in \\{0,1,\\dots,E-1\\}$ and node index $i \\in \\{0,1,\\dots,n_p-1\\}$, the canonical FR correction for the semi-discrete residual $R[e,i]$ can be written in the form\n$$\nR[e,i] \\leftarrow R[e,i] + G_L[i]\\cdot J_L[e] + G_R[i]\\cdot J_R[e],\n$$\nwhere $G_L[i]$ and $G_R[i]$ are nodal values of correction functions and $J_L[e]$ and $J_R[e]$ are face-jump contributions per element. This update is applied for all $E \\cdot n_p$ degrees of freedom and is an $O(N)$ kernel with $N = E \\cdot n_p$.\n\nYou are asked to model two graphics processing unit (GPU) kernel orderings for the FR correction and analyze how reordering the computation affects memory coalescing and occupancy, and thus predicted runtime. Define two orderings:\n- Ordering A (element-major traversal): each block updates one element; threads iterate over node index $i$.\n- Ordering B (node-major traversal): each block updates one node index $i$ across multiple elements; threads iterate over element index $e$.\n\nAssume the following memory layouts for the residual array $R$:\n- Element-major layout (EM): flattened index $k = e \\cdot n_p + i$.\n- Node-major layout (NM): flattened index $k = i \\cdot E + e$.\n\nFor both orderings, assume each thread performs the following operations for the correction step:\n- Arithmetic: $c_{op} = 4$ floating-point operations (two multiplications and two additions).\n- Memory:\n  - Read two doubles for $G_L[i], G_R[i]$ with total bytes $b_G = 16$ (assume perfectly cacheable and thus fully coalesced).\n  - Read two doubles for $J_L[e], J_R[e]$ with total bytes $b_J = 16$ (assume broadcast across a block and thus fully coalesced).\n  - Read-modify-write one double for $R[e,i]$ with total bytes $b_R = 16$ (the only memory whose coalescing depends on the ordering and layout).\n\nAssume the following performance model:\n- Warp size $W = 32$.\n- For $R$ access, define the stride $s$ as the address difference (in units of doubles) between consecutive threads in a warp. The coalescing efficiency for the $R$ access is modeled as\n$$\n\\epsilon(s) = \\begin{cases}\n1/s, & 1 \\le s \\le W,\\\\\n1/W, & s \\ge W,\n\\end{cases}\n$$\nand is applied only to the $b_R$ contribution. All other bytes are considered fully coalesced with efficiency $1$.\n- Mapping between ordering, layout, and $R$-stride:\n  - For element-major layout (EM): ordering A has $s_A = 1$ and ordering B has $s_B = n_p$.\n  - For node-major layout (NM): ordering A has $s_A = E$ and ordering B has $s_B = 1$.\n- Streaming multiprocessor (SM) resource limits:\n  - Maximum threads per SM $T_{SM}$,\n  - Maximum warps per SM $W_{SM}$,\n  - Maximum resident blocks per SM $B_{SM}$,\n  - Register file size per SM $R_{SM}$ (in register units),\n  - Shared memory per SM $S_{SM}$ (in bytes).\n- Kernel resource usage:\n  - Threads per block $T_b$,\n  - Registers per thread $R_t$,\n  - Shared memory per block $S_b$.\n- Occupancy derivation per SM:\n  - Warps per block $w_b = \\lceil T_b / W \\rceil$,\n  - Block limit by threads $B_T = \\left\\lfloor \\frac{T_{SM}}{T_b} \\right\\rfloor$,\n  - Block limit by warps $B_W = \\left\\lfloor \\frac{W_{SM}}{w_b} \\right\\rfloor$,\n  - Block limit by registers $B_R = \\left\\lfloor \\frac{R_{SM}}{R_t \\cdot T_b} \\right\\rfloor$,\n  - Block limit by shared memory $B_S = \\left\\lfloor \\frac{S_{SM}}{S_b} \\right\\rfloor$ if $S_b > 0$, otherwise $+\\infty$,\n  - Resident blocks $B_{res} = \\min\\{B_{SM}, B_T, B_W, B_R, B_S\\}$,\n  - Active warps $W_{act} = B_{res} \\cdot w_b$,\n  - Occupancy $\\mathrm{occ} = \\min\\left(1, \\frac{W_{act}}{W_{SM}}\\right)$.\n- Device-peak compute throughput $P_{peak}$ (in floating-point operations per second).\n- Device-peak memory bandwidth $B_{peak}$ (in bytes per second).\n- For total unknowns $N = E \\cdot n_p$, the compute time is modeled as\n$$\nt_{comp} = \\frac{N \\cdot c_{op}}{P_{peak} \\cdot \\mathrm{occ}},\n$$\nand the memory time is modeled as\n$$\nt_{mem} = N \\cdot \\left(\\frac{b_R}{B_{peak} \\cdot \\epsilon(s)} + \\frac{b_G + b_J}{B_{peak}}\\right).\n$$\n- Kernel time is the maximum of compute and memory times:\n$$\nt = \\max\\{t_{comp}, t_{mem}\\}.\n$$\n\nYour task is to write a program that, given a small test suite, computes for each case the predicted speedup from reordering the FR correction kernel to maximize coalescing, i.e., the ratio of the slower ordering’s predicted runtime to the faster ordering’s predicted runtime. For each test, consider both orderings A and B under the specified layout and hardware constraints, and return the speedup of the better ordering over the worse one.\n\nUse the following fixed hardware parameters for all tests:\n- Warp size $W = 32$,\n- Maximum threads per SM $T_{SM} = 2048$,\n- Maximum warps per SM $W_{SM} = 64$,\n- Maximum resident blocks per SM $B_{SM} = 32$,\n- Register file size per SM $R_{SM} = 65536$,\n- Shared memory per SM $S_{SM} = 65536$ bytes,\n- Device-peak compute throughput $P_{peak} = 10^{13}$ floating-point operations per second,\n- Device-peak memory bandwidth $B_{peak} = 6 \\times 10^{11}$ bytes per second.\n\nUse the following fixed kernel-level parameters for all tests:\n- Operation count per thread $c_{op} = 4$ floating-point operations,\n- Byte counts per thread $b_G = 16$, $b_J = 16$, $b_R = 16$.\n\nTest suite:\n- Test $1$ (happy path, element-major layout benefits element-major traversal):\n  - $E = 4096$, $p = 7$ (thus $n_p = 8$), layout $\\text{EM}$,\n  - Ordering A: $T_b = 32$, $R_t = 48$, $S_b = 0$,\n  - Ordering B: $T_b = 256$, $R_t = 64$, $S_b = 0$.\n- Test $2$ (layout flipped, node-major layout benefits node-major traversal):\n  - $E = 4096$, $p = 7$ (thus $n_p = 8$), layout $\\text{NM}$,\n  - Ordering A: $T_b = 32$, $R_t = 48$, $S_b = 0$,\n  - Ordering B: $T_b = 256$, $R_t = 64$, $S_b = 0$.\n- Test $3$ (register-pressure boundary, large $p$):\n  - $E = 8192$, $p = 31$ (thus $n_p = 32$), layout $\\text{EM}$,\n  - Ordering A: $T_b = 32$, $R_t = 128$, $S_b = 0$,\n  - Ordering B: $T_b = 256$, $R_t = 64$, $S_b = 0$.\n- Test $4$ (small problem size edge case):\n  - $E = 16$, $p = 1$ (thus $n_p = 2$), layout $\\text{NM}$,\n  - Ordering A: $T_b = 32$, $R_t = 24$, $S_b = 0$,\n  - Ordering B: $T_b = 64$, $R_t = 24$, $S_b = 0$.\n- Test $5$ (shared memory occupancy limit for ordering A):\n  - $E = 2048$, $p = 15$ (thus $n_p = 16$), layout $\\text{EM}$,\n  - Ordering A: $T_b = 32$, $R_t = 48$, $S_b = 32768$,\n  - Ordering B: $T_b = 256$, $R_t = 32$, $S_b = 0$.\n\nFor each test, compute the predicted runtimes $t_A$ and $t_B$ using the model above, then compute the speedup as\n$$\nS = \\frac{\\max\\{t_A, t_B\\}}{\\min\\{t_A, t_B\\}}.\n$$\n\nYour program should produce a single line of output containing the five speedups for tests $1$ through $5$ as a comma-separated list enclosed in square brackets, with each speedup rounded to six decimal places (e.g., `[1.234567,2.000000, ...]` ). Angles are not involved, so no angle unit is required. No physical units need to be reported in the output; the numeric values are dimensionless ratios.", "solution": "The problem requires an analysis of GPU kernel performance for the correction step in the Flux Reconstruction (FR) method. Specifically, we are to model the runtime of two distinct computational orderings, termed Ordering A (element-major) and Ordering B (node-major), under two different memory layouts for the residual array $R$: element-major (EM) and node-major (NM). The goal is to predict the speedup achieved by selecting the more performant ordering for a given layout and set of kernel parameters.\n\nThe solution involves a step-by-step application of the provided performance model. For each test case, we must calculate the predicted runtime, $t$, for both Ordering A and Ordering B. The runtime is defined as the maximum of the compute-bound time, $t_{comp}$, and the memory-bound time, $t_{mem}$. The speedup, $S$, is then the ratio of the larger runtime to the smaller one.\n\nThe calculation proceeds as follows for each ordering:\n\nFirst, we determine the occupancy, $\\mathrm{occ}$, which represents the fraction of a streaming multiprocessor's (SM's) computational resources that are actively utilized by a given kernel launch configuration. The occupancy is limited by the most constrained resource, be it threads, warps, registers, or shared memory. The following quantities are defined from the problem statement:\n- Warp size: $W$\n- Maximum threads per SM: $T_{SM}$\n- Maximum warps per SM: $W_{SM}$\n- Maximum resident blocks per SM: $B_{SM}$\n- Register file size per SM: $R_{SM}$\n- Shared memory per SM: $S_{SM}$\n- Threads per block for the kernel: $T_b$\n- Registers per thread: $R_t$\n- Shared memory per block: $S_b$\n\nFrom these, we derive the resource limits on the number of concurrent blocks per SM:\n1.  Warps per block: $w_b = \\lceil T_b / W \\rceil$.\n2.  Block limit due to threads: $B_T = \\left\\lfloor \\dfrac{T_{SM}}{T_b} \\right\\rfloor$.\n3.  Block limit due to warps: $B_W = \\left\\lfloor \\dfrac{W_{SM}}{w_b} \\right\\rfloor$.\n4.  Block limit due to registers: $B_R = \\left\\lfloor \\dfrac{R_{SM}}{R_t \\cdot T_b} \\right\\rfloor$.\n5.  Block limit due to shared memory: $B_S = \\left\\lfloor \\dfrac{S_{SM}}{S_b} \\right\\rfloor$ if $S_b > 0$, otherwise $B_S$ is effectively infinite.\n\nThe number of resident blocks per SM, $B_{res}$, is the minimum of these limits and the architectural maximum:\n$$\nB_{res} = \\min\\{B_{SM}, B_T, B_W, B_R, B_S\\}\n$$\nThe number of active warps per SM is then $W_{act} = B_{res} \\cdot w_b$. Finally, the occupancy is:\n$$\n\\mathrm{occ} = \\min\\left(1, \\dfrac{W_{act}}{W_{SM}}\\right)\n$$\n\nSecond, we calculate the compute-bound time, $t_{comp}$. This time is inversely proportional to the effective peak compute throughput, which is the device's peak throughput $P_{peak}$ scaled by the occupancy. For a problem with a total number of degrees of freedom $N = E \\cdot n_p$ (where $n_p = p+1$) and $c_{op}$ floating-point operations per degree of freedom, the compute time is:\n$$\nt_{comp} = \\frac{N \\cdot c_{op}}{P_{peak} \\cdot \\mathrm{occ}}\n$$\n\nThird, we calculate the memory-bound time, $t_{mem}$. This time depends on the amount of data transferred and the effective memory bandwidth. The key factor differentiating the orderings is the memory coalescing for the read-modify-write access to the residual array $R$. The model defines a coalescing efficiency, $\\epsilon(s)$, which is a function of the stride, $s$, between memory addresses accessed by consecutive threads in a warp. The stride itself is determined by the combination of the memory layout (EM or NM) and the kernel traversal ordering (A or B).\nThe given mapping is:\n- For element-major layout (EM): Ordering A has stride $s_A = 1$; Ordering B has $s_B = n_p$.\n- For node-major layout (NM): Ordering A has stride $s_A = E$; Ordering B has $s_B = 1$.\n\nThe coalescing efficiency for the $R$ access is given by:\n$$\n\\epsilon(s) = \\begin{cases}\n1/s, & 1 \\le s \\le W,\\\\\n1/W, & s \\ge W,\n\\end{cases}\n$$\nThe total memory time for all $N$ degrees of freedom is the sum of contributions from the non-coalescing-dependent accesses ($G_L, G_R, J_L, J_R$ with total bytes $b_G + b_J$) and the coalescing-dependent access ($R$ with bytes $b_R$):\n$$\nt_{mem} = N \\cdot \\left(\\frac{b_R}{B_{peak} \\cdot \\epsilon(s)} + \\frac{b_G + b_J}{B_{peak}}\\right)\n$$\nA stride of $s=1$ yields a perfect coalescing efficiency of $\\epsilon(1)=1$, minimizing the memory time. Larger strides degrade performance by reducing the effective bandwidth for the $R$ array access.\n\nFourth, the predicted kernel runtime, $t$, is the maximum of the compute and memory times, as a kernel can be either compute-bound or memory-bound:\n$$\nt = \\max\\{t_{comp}, t_{mem}\\}\n$$\n\nFinally, for each test case, we compute the runtimes $t_A$ for Ordering A and $t_B$ for Ordering B. The speedup $S$ is the ratio of the slower time to the faster time, indicating the performance gain from choosing the optimal ordering:\n$$\nS = \\frac{\\max\\{t_A, t_B\\}}{\\min\\{t_A, t_B\\}}\n$$\nThis procedure is applied systematically to all provided test cases to derive the final results.", "answer": "```python\nimport numpy as np\nimport math\n\n# Define fixed hardware and kernel parameters\nW = 32\nT_SM = 2048\nW_SM = 64\nB_SM = 32\nR_SM = 65536\nS_SM = 65536\nP_peak = 1e13\nB_peak = 6e11\nc_op = 4\nb_G = 16\nb_J = 16\nb_R = 16\n\ndef calculate_time(E, p, layout, T_b, R_t, S_b, ordering):\n    \"\"\"\n    Calculates the predicted runtime for a given configuration based on the provided performance model.\n    \"\"\"\n    # 1. Calculate derived problem-specific values\n    n_p = p + 1\n    N = E * n_p\n\n    # 2. Calculate Occupancy (occ)\n    w_b = math.ceil(T_b / W)\n    \n    B_T = math.floor(T_SM / T_b)\n    B_W = math.floor(W_SM / w_b)\n\n    # Prevent division by zero if T_b or R_t is zero, though not expected from problem statement.\n    if R_t * T_b > 0:\n        B_R = math.floor(R_SM / (R_t * T_b))\n    else:\n        B_R = float('inf')\n\n    if S_b > 0:\n        B_S = math.floor(S_SM / S_b)\n    else:\n        B_S = float('inf')\n\n    B_res = min(B_SM, B_T, B_W, B_R, B_S)\n    W_act = B_res * w_b\n    occ = min(1.0, W_act / W_SM) if W_SM > 0 else 0.0\n\n    # 3. Calculate Compute Time (t_comp)\n    # Prevent division by zero if P_peak or occ is zero\n    if P_peak > 0 and occ > 0:\n        t_comp = (N * c_op) / (P_peak * occ)\n    else:\n        t_comp = float('inf')\n\n    # 4. Calculate Memory Time (t_mem)\n    # Determine stride s\n    if layout == 'EM':\n        s = 1 if ordering == 'A' else n_p\n    elif layout == 'NM':\n        s = E if ordering == 'A' else 1\n    else:\n        raise ValueError(\"Invalid memory layout specified.\")\n\n    # Calculate coalescing efficiency epsilon(s)\n    if 1 <= s <= W:\n        epsilon_s = 1.0 / s\n    else: # s > W\n        epsilon_s = 1.0 / W\n    \n    # Calculate t_mem\n    # Prevent division by zero if B_peak or epsilon_s is zero\n    if B_peak > 0 and epsilon_s > 0:\n        term_R = b_R / (B_peak * epsilon_s)\n        term_GJ = (b_G + b_J) / B_peak\n        t_mem = N * (term_R + term_GJ)\n    else:\n        t_mem = float('inf')\n\n    # 5. Calculate Kernel Time (t)\n    t = max(t_comp, t_mem)\n\n    return t\n\ndef solve():\n    \"\"\"\n    Processes the test suite to calculate speedups for each case.\n    \"\"\"\n    test_cases = [\n        # Test 1: E, p, layout, (T_b_A, R_t_A, S_b_A), (T_b_B, R_t_B, S_b_B)\n        (4096, 7, 'EM', (32, 48, 0), (256, 64, 0)),\n        # Test 2\n        (4096, 7, 'NM', (32, 48, 0), (256, 64, 0)),\n        # Test 3\n        (8192, 31, 'EM', (32, 128, 0), (256, 64, 0)),\n        # Test 4\n        (16, 1, 'NM', (32, 24, 0), (64, 24, 0)),\n        # Test 5\n        (2048, 15, 'EM', (32, 48, 32768), (256, 32, 0)),\n    ]\n\n    results = []\n    for case in test_cases:\n        E, p, layout, params_A, params_B = case\n        T_b_A, R_t_A, S_b_A = params_A\n        T_b_B, R_t_B, S_b_B = params_B\n\n        t_A = calculate_time(E, p, layout, T_b_A, R_t_A, S_b_A, 'A')\n        t_B = calculate_time(E, p, layout, T_b_B, R_t_B, S_b_B, 'B')\n        \n        if min(t_A, t_B) > 0:\n            speedup = max(t_A, t_B) / min(t_A, t_B)\n        else:\n            speedup = 1.0 # Should not happen with valid inputs\n\n        results.append(f\"{speedup:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3386482"}]}