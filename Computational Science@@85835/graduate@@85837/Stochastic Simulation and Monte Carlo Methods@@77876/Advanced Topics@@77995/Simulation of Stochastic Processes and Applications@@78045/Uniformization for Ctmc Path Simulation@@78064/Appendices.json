{"hands_on_practices": [{"introduction": "The ability to compute expectations of functions of a CTMC's state, such as $\\mathbb{E}[f(X_t)]$, is a fundamental task in stochastic modeling. This exercise builds your understanding from the ground up by having you derive the core uniformization identity, which elegantly recasts a continuous-time problem into a discrete-time one involving a Poisson-distributed number of steps. By working through the derivation and then applying it to a concrete example with a structured generator matrix, you will solidify the theoretical underpinnings of the method and learn how to analyze its numerical approximation error [@problem_id:3359512].", "problem": "Consider a Continuous-Time Markov Chain (CTMC) on a finite state space $\\mathcal{S}=\\{0,1,2\\}$ with generator $Q$ and an initial distribution $\\pi_0$ (a row vector). Let $X_t$ denote the state at time $t$. The uniformization method constructs a dominating rate $\\lambda>0$ and a stochastic matrix $P$ such that $Q=\\lambda(P-I)$, where $I$ is the identity matrix. The uniformization representation interprets the CTMC as a time-changed discrete-time Markov chain $Y_n$ with transition matrix $P$, observed at a $\\text{Poisson}(\\lambda t)$ number of jumps by time $t$.\n\nStarting from the fundamental definitions of CTMCs, the Kolmogorov forward equations, and the law of total expectation, derive a series representation for the expectation $\\mathbb{E}[f(X_t)]$ of a bounded function $f:\\mathcal{S}\\to\\mathbb{R}$ under uniformization. Then specialize to the following concrete instance and compute the value $\\mathbb{E}[f(X_t)]$ at a specific time:\n- Let $\\lambda=5$ and\n$$\nP=\\begin{pmatrix}\n0.4 & 0.6 & 0 \\\\\n0 & 0.7 & 0.3 \\\\\n0 & 0 & 1\n\\end{pmatrix},\n\\qquad\nQ=\\lambda(P-I)=5(P-I).\n$$\n- Let the initial distribution be $\\pi_0=(0.3,\\,0.2,\\,0.5)$.\n- Let $f$ be the indicator of state $0$, i.e., $f(0)=1$, $f(1)=0$, $f(2)=0$.\n- Let $t=0.9$.\n\nYour tasks are:\n1. Derive from first principles the uniformization mixture representation of $\\mathbb{E}[f(X_t)]$ as a Poisson-weighted series involving $P$.\n2. Exploit any structural property you can deduce about $P$ and $f$ to obtain a closed-form expression for $\\mathbb{E}[f(X_t)]$ for the given data, and evaluate it numerically.\n3. Explain how to approximate the series with a controlled absolute error by truncating at a finite $N$, and present an explicit choice of $N$ that guarantees the truncation error is at most $10^{-6}$ for the given parameters.\n\nRound your final numerical answer for $\\mathbb{E}[f(X_t)]$ to six significant figures. No physical units are required.", "solution": "The problem asks for three tasks related to a Continuous-Time Markov Chain (CTMC) specified by the uniformization method:\n1.  Derive the series representation for the expectation of a function of the state, $\\mathbb{E}[f(X_t)]$.\n2.  Calculate this expectation for a specific case and obtain a closed-form expression.\n3.  Analyze the truncation error of the series representation.\n\nWe will address each task in sequence.\n\nThe state of the CTMC at time $t$, denoted by the probability distribution row vector $\\pi_t$, is governed by the Kolmogorov forward equation $\\frac{d}{dt}\\pi_t = \\pi_t Q$, where $\\pi_0$ is the initial distribution and $Q$ is the generator matrix. The solution to this differential equation is given by the matrix exponential:\n$$\n\\pi_t = \\pi_0 \\exp(Qt)\n$$\nThe expectation of a bounded function $f:\\mathcal{S}\\to\\mathbb{R}$ is given by $\\mathbb{E}[f(X_t)] = \\sum_{s \\in \\mathcal{S}} \\pi_t(s)f(s)$. Let $\\mathbf{f}$ be the column vector whose entries are the values of $f(s)$. Then the expectation can be written as a matrix product:\n$$\n\\mathbb{E}[f(X_t)] = \\pi_t \\mathbf{f} = \\pi_0 \\exp(Qt) \\mathbf{f}\n$$\n\n**Task 1: Derivation of the Uniformization Series**\n\nThe uniformization method is based on the decomposition of the generator matrix $Q$ as $Q = \\lambda(P-I)$, where $\\lambda > 0$ is a rate such that $\\lambda \\ge \\max_i |q_{ii}|$, $P$ is a stochastic matrix (the transition matrix of the embedded discrete-time chain), and $I$ is the identity matrix.\n\nSubstituting this decomposition into the expression for $\\mathbb{E}[f(X_t)]$:\n$$\n\\mathbb{E}[f(X_t)] = \\pi_0 \\exp(\\lambda t(P-I)) \\mathbf{f}\n$$\nSince the identity matrix $I$ commutes with any matrix $P$, we can separate the exponential: $\\exp(A+B) = \\exp(A)\\exp(B)$ if $AB=BA$. Here, $A = \\lambda t P$ and $B = -\\lambda t I$.\n$$\n\\exp(\\lambda t(P-I)) = \\exp(\\lambda t P) \\exp(-\\lambda t I)\n$$\nThe exponential of a scaled identity matrix is straightforward:\n$$\n\\exp(-\\lambda t I) = \\sum_{n=0}^{\\infty} \\frac{(-\\lambda t I)^n}{n!} = \\sum_{n=0}^{\\infty} \\frac{(-\\lambda t)^n}{n!} I^n = I \\sum_{n=0}^{\\infty} \\frac{(-\\lambda t)^n}{n!} = \\exp(-\\lambda t) I\n$$\nThe exponential of the matrix $\\lambda t P$ is given by its power series definition:\n$$\n\\exp(\\lambda t P) = \\sum_{n=0}^{\\infty} \\frac{(\\lambda t P)^n}{n!} = \\sum_{n=0}^{\\infty} \\frac{(\\lambda t)^n}{n!} P^n\n$$\nCombining these results, we get:\n$$\n\\mathbb{E}[f(X_t)] = \\pi_0 \\left( \\left( \\sum_{n=0}^{\\infty} \\frac{(\\lambda t)^n}{n!} P^n \\right) \\exp(-\\lambda t) I \\right) \\mathbf{f}\n$$\nSince $\\exp(-\\lambda t)$ is a scalar and $I\\mathbf{f} = \\mathbf{f}$, we can rearrange the expression:\n$$\n\\mathbb{E}[f(X_t)] = \\sum_{n=0}^{\\infty} \\exp(-\\lambda t) \\frac{(\\lambda t)^n}{n!} (\\pi_0 P^n \\mathbf{f})\n$$\nThis is the desired series representation. It has a profound probabilistic interpretation. The term $\\exp(-\\lambda t) \\frac{(\\lambda t)^n}{n!}$ is the probability mass function of a Poisson random variable $N_t$ with rate $\\lambda t$, i.e., $\\mathbb{P}(N_t=n)$. The term $\\pi_0 P^n \\mathbf{f}$ is the expectation $\\mathbb{E}[f(Y_n)]$, where $Y_n$ is the discrete-time Markov chain with transition matrix $P$ and initial distribution $\\pi_0$. Thus, the formula represents the law of total expectation:\n$$\n\\mathbb{E}[f(X_t)] = \\sum_{n=0}^{\\infty} \\mathbb{P}(N_t=n) \\mathbb{E}[f(Y_n)] = \\mathbb{E}[\\mathbb{E}[f(Y_{N_t})|N_t]] = \\mathbb{E}[f(Y_{N_t})]\n$$\nThis shows that the state of the CTMC at time $t$ is distributed as the state of the embedded DTMC after a Poisson-distributed number of steps.\n\n**Task 2: Closed-Form Expression and Numerical Evaluation**\n\nWe are given the following specific data:\n- $\\lambda = 5$\n- $t=0.9$\n- $\\pi_0 = (0.3, 0.2, 0.5)$\n- $P = \\begin{pmatrix} 0.4 & 0.6 & 0 \\\\ 0 & 0.7 & 0.3 \\\\ 0 & 0 & 1 \\end{pmatrix}$\n- $f$ is the indicator of state $0$, so $f(0)=1$, $f(1)=0$, $f(2)=0$. This corresponds to the column vector $\\mathbf{f} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\nWe need to compute $\\mathbb{E}[f(X_t)] = \\sum_{n=0}^{\\infty} \\exp(-\\lambda t) \\frac{(\\lambda t)^n}{n!} (\\pi_0 P^n \\mathbf{f})$.\nThe core of the calculation is the term $\\pi_0 P^n \\mathbf{f}$.\n$$\n\\pi_0 P^n \\mathbf{f} = \\begin{pmatrix} 0.3 & 0.2 & 0.5 \\end{pmatrix} P^n \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis product is the dot product of $\\pi_0$ with the first column of $P^n$.\nA crucial structural property of the matrix $P$ is that it is upper triangular. The $n$-th power of an upper triangular matrix is also upper triangular, and its diagonal entries are the $n$-th powers of the original diagonal entries. Let $(M)_{ij}$ denote the entry in the $i$-th row and $j$-th column of a matrix $M$.\n$$\nP^n = \\begin{pmatrix} (0.4)^n & (P^n)_{01} & (P^n)_{02} \\\\ 0 & (0.7)^n & (P^n)_{12} \\\\ 0 & 0 & 1^n \\end{pmatrix}\n$$\nThe first column of $P^n$ is therefore $\\begin{pmatrix} (0.4)^n \\\\ 0 \\\\ 0 \\end{pmatrix}$.\nSubstituting this into the expression for $\\pi_0 P^n \\mathbf{f}$:\n$$\n\\pi_0 P^n \\mathbf{f} = \\begin{pmatrix} 0.3 & 0.2 & 0.5 \\end{pmatrix} \\begin{pmatrix} (0.4)^n \\\\ 0 \\\\ 0 \\end{pmatrix} = 0.3 \\times (0.4)^n + 0.2 \\times 0 + 0.5 \\times 0 = 0.3 \\times (0.4)^n\n$$\nNow we substitute this simplified term back into the infinite series for the expectation:\n$$\n\\mathbb{E}[f(X_t)] = \\sum_{n=0}^{\\infty} \\exp(-\\lambda t) \\frac{(\\lambda t)^n}{n!} (0.3 \\times (0.4)^n)\n$$\nWe can factor out the constant $0.3$ and rearrange:\n$$\n\\mathbb{E}[f(X_t)] = 0.3 \\exp(-\\lambda t) \\sum_{n=0}^{\\infty} \\frac{(\\lambda t)^n (0.4)^n}{n!} = 0.3 \\exp(-\\lambda t) \\sum_{n=0}^{\\infty} \\frac{(0.4 \\lambda t)^n}{n!}\n$$\nThe sum is the Taylor series expansion for $\\exp(0.4 \\lambda t)$. This yields the closed-form expression:\n$$\n\\mathbb{E}[f(X_t)] = 0.3 \\exp(-\\lambda t) \\exp(0.4 \\lambda t) = 0.3 \\exp(-\\lambda t + 0.4 \\lambda t) = 0.3 \\exp(-0.6 \\lambda t)\n$$\nNow, we substitute the numerical values $\\lambda=5$ and $t=0.9$:\n$$\n\\lambda t = 5 \\times 0.9 = 4.5\n$$\n$$\n\\mathbb{E}[f(X_{0.9})] = 0.3 \\exp(-0.6 \\times 4.5) = 0.3 \\exp(-2.7)\n$$\nNumerically evaluating this expression:\n$$\n\\mathbb{E}[f(X_{0.9})] \\approx 0.3 \\times 0.0672055126 \\approx 0.02016165378\n$$\nRounding to six significant figures, we get $0.0201617$.\n\n**Task 3: Truncation Error Analysis**\n\nWe wish to approximate the full series by a partial sum. Let the approximation be $S_N = \\sum_{n=0}^{N} p_n$, where $p_n = \\exp(-\\lambda t) \\frac{(\\lambda t)^n}{n!} (0.3 \\times (0.4)^n)$. The absolute truncation error is $R_{N+1} = \\left| \\sum_{n=N+1}^{\\infty} p_n \\right|$. Since all terms $p_n$ are positive, the absolute value is redundant.\n$$\nR_{N+1} = \\sum_{n=N+1}^{\\infty} 0.3 \\exp(-\\lambda t) \\frac{(0.4 \\lambda t)^n}{n!} = 0.3 \\exp(-\\lambda t) \\sum_{n=N+1}^{\\infty} \\frac{(0.4 \\lambda t)^n}{n!}\n$$\nLet $\\mu = 0.4 \\lambda t = 0.4 \\times 4.5 = 1.8$.\nThe sum $\\sum_{n=N+1}^{\\infty} \\frac{\\mu^n}{n!}$ is the tail of the series for $\\exp(\\mu)$. We can relate this to the tail probability of a Poisson distribution. Let $Z$ be a random variable with a Poisson distribution of mean $\\mu=1.8$.\n$$\n\\mathbb{P}(Z \\ge k) = \\sum_{n=k}^{\\infty} \\mathbb{P}(Z=n) = \\sum_{n=k}^{\\infty} \\exp(-\\mu) \\frac{\\mu^n}{n!}\n$$\nTherefore, $\\sum_{n=k}^{\\infty} \\frac{\\mu^n}{n!} = \\exp(\\mu) \\mathbb{P}(Z \\ge k)$.\nOur error term becomes:\n$$\nR_{N+1} = 0.3 \\exp(-\\lambda t) \\exp(\\mu) \\mathbb{P}(Z \\ge N+1) = 0.3 \\exp(-4.5) \\exp(1.8) \\mathbb{P}(Z \\ge N+1)\n$$\n$$\nR_{N+1} = 0.3 \\exp(-2.7) \\mathbb{P}(Z \\ge N+1)\n$$\nWe require the error to be at most $10^{-6}$:\n$$\n0.3 \\exp(-2.7) \\mathbb{P}(Z \\ge N+1) \\leq 10^{-6}\n$$\nThis gives a condition on the Poisson tail probability:\n$$\n\\mathbb{P}(Z \\ge N+1) \\leq \\frac{10^{-6}}{0.3 \\exp(-2.7)} \\approx \\frac{10^{-6}}{0.02016165} \\approx 4.9599 \\times 10^{-5}\n$$\nWe need to find an integer $N$ such that $\\mathbb{P}(Z \\ge N+1) \\leq 4.9599 \\times 10^{-5}$ for $Z \\sim \\text{Poisson}(1.8)$. We can bound the tail probability. For $k > \\mu$, a useful bound is $\\sum_{i=k}^\\infty \\exp(-\\mu) \\frac{\\mu^i}{i!} < \\frac{\\exp(-\\mu)\\mu^k}{k!} \\frac{k}{k-\\mu}$.\nLet's test values for $N+1$.\nFor $N+1=9$: $\\mathbb{P}(Z=9) = \\exp(-1.8) \\frac{1.8^9}{9!} \\approx 9.0356 \\times 10^{-5}$. The tail $\\mathbb{P}(Z \\ge 9)$ will be larger than this.\nFor $N+1=10$: $\\mathbb{P}(Z=10) = \\exp(-1.8) \\frac{1.8^{10}}{10!} \\approx 1.6264 \\times 10^{-5}$.\nLet's bound the tail $\\mathbb{P}(Z \\ge 10)$:\n$$\n\\mathbb{P}(Z \\ge 10) < \\frac{\\mathbb{P}(Z=10)}{1 - \\mu/(N+1)} = \\frac{\\mathbb{P}(Z=10)}{1 - 1.8/10} = \\frac{\\mathbb{P}(Z=10)}{0.82} \\approx \\frac{1.6264 \\times 10^{-5}}{0.82} \\approx 1.9834 \\times 10^{-5}\n$$\nSince $1.9834 \\times 10^{-5} < 4.9599 \\times 10^{-5}$, choosing $N+1=10$ is sufficient.\nThis implies $N=9$. Therefore, truncating the sum at $N=9$ (i.e., computing $\\sum_{n=0}^{9} p_n$) guarantees that the absolute error is less than $10^{-6}$.\nAn explicit choice is $N=9$.", "answer": "$$\\boxed{0.0201617}$$", "id": "3359512"}, {"introduction": "Beyond theoretical correctness, the practical efficiency of a simulation algorithm is paramount. The performance of the uniformization method is directly tied to the choice of the rate $\\lambda$, as a rate that is much larger than necessary introduces excessive \"virtual jumps,\" which are computationally wasteful. This hands-on coding practice guides you to explore this trade-off by quantifying the expected number of virtual jumps for CTMCs with highly varied dynamics, a common feature in stiff systems, thereby building your intuition for algorithm performance [@problem_id:3359521].", "problem": "Consider a Continuous-Time Markov Chain (CTMC) with a finite state space and generator matrix $Q \\in \\mathbb{R}^{n \\times n}$. By definition, for $i \\neq j$, the off-diagonal entries satisfy $Q_{ij} \\ge 0$, and each row sums to zero, so $Q_{ii} = -\\sum_{j \\neq i} Q_{ij} \\le 0$. The exit rate from state $i$ is $a_i \\equiv -Q_{ii}$. The uniformization method simulates sample paths by introducing a Poisson clock with rate $\\lambda$, where exactness requires $\\lambda \\ge \\max_i a_i$. At each tick of this clock, the process makes a transition according to the discrete-time Markov chain with transition matrix $P = I + Q / \\lambda$; a \"virtual jump\" occurs when the chain stays in the same state at a Poisson tick (i.e., a self-transition).\n\nYour task is to design a CTMC whose exit rates vary over several orders of magnitude and to study how to choose the uniformization rate $\\lambda$ to minimize the expected number of virtual jumps while preserving exactness ($\\lambda \\ge \\max_i a_i$). The goal is to compute, for specified CTMCs and $\\lambda$ choices, the expected number of virtual jumps over a fixed time horizon $[0,T]$ starting from a given initial distribution. You must solve this using first principles:\n\n- The occupancy probability row vector $p(t)$ evolves according to the forward Kolmogorov equation $p'(t) = p(t) Q$ with $p(0) = p_0$.\n- The instantaneous expected rate of actual jumps at time $t$ equals $\\sum_i p_i(t) a_i$, where $a_i = -Q_{ii}$.\n- The expected total number of Poisson clock ticks over $[0,T]$ is $\\lambda T$.\n\nFrom these principles, construct a method to compute the expected number of virtual jumps over $[0,T]$ by integrating the instantaneous expected rate of actual jumps over time and comparing to the expected number of Poisson clock ticks. You must implement this by computing $p(t)$ exactly via the matrix exponential $p(t) = p_0 \\exp(Q t)$ and performing numerical integration in time of the scalar function $t \\mapsto \\sum_i p_i(t) a_i$. Do not use closed-form shortcuts for the integral; perform a numerically sound quadrature on $[0,T]$.\n\nTest Suite. Use the following three CTMCs, each with $n=5$ states, exit rates spanning several orders of magnitude, and different structural features. For each CTMC, compute the expected number of virtual jumps for three choices of $\\lambda$: $\\lambda_{\\min} \\equiv \\max_i a_i$, $2 \\lambda_{\\min}$, and $10 \\lambda_{\\min}$.\n\n- Case 1 (highly skewed exit rates, moderate horizon):\n    - $Q^{(1)}$ rows:\n        1. $[Q^{(1)}_{0,1}, Q^{(1)}_{0,2}] = [0.006, 0.004]$, $Q^{(1)}_{0,0} = -(0.006 + 0.004) = -0.01$.\n        2. $[Q^{(1)}_{1,0}, Q^{(1)}_{1,2}] = [0.3, 0.7]$, $Q^{(1)}_{1,1} = -1$.\n        3. $[Q^{(1)}_{2,1}, Q^{(1)}_{2,3}] = [2.5, 7.5]$, $Q^{(1)}_{2,2} = -10$.\n        4. $[Q^{(1)}_{3,2}, Q^{(1)}_{3,4}] = [40, 60]$, $Q^{(1)}_{3,3} = -100$.\n        5. $[Q^{(1)}_{4,1}, Q^{(1)}_{4,3}] = [300, 700]$, $Q^{(1)}_{4,4} = -1000$.\n    - Initial distribution $p_0^{(1)} = [0.7, 0.2, 0.05, 0.04, 0.01]$.\n    - Time horizon $T^{(1)} = 1$.\n    - $\\lambda_{\\min}^{(1)} = \\max_i a_i = 1000$.\n\n- Case 2 (rarely visited ultra-fast state, longer horizon):\n    - $Q^{(2)}$ rows:\n        1. $[Q^{(2)}_{0,1}, Q^{(2)}_{0,2}] = [0.25, 0.25]$, $Q^{(2)}_{0,0} = -0.5$.\n        2. $[Q^{(2)}_{1,0}, Q^{(2)}_{1,2}] = [0.2, 0.2]$, $Q^{(2)}_{1,1} = -0.4$.\n        3. $[Q^{(2)}_{2,1}, Q^{(2)}_{2,3}] = [0.29, 0.01]$, $Q^{(2)}_{2,2} = -0.3$.\n        4. $[Q^{(2)}_{3,2}, Q^{(2)}_{3,4}] = [0.199999, 0.000001]$, $Q^{(3)}_{3,3} = -0.2$.\n        5. $[Q^{(2)}_{4,0}, Q^{(2)}_{4,3}] = [2000, 8000]$, $Q^{(2)}_{4,4} = -10000$.\n    - Initial distribution $p_0^{(2)} = [0.5, 0.49, 0.01, 0.0, 0.0]$.\n    - Time horizon $T^{(2)} = 2$.\n    - $\\lambda_{\\min}^{(2)} = \\max_i a_i = 10000$.\n\n- Case 3 (absorbing state present, intermediate horizon):\n    - $Q^{(3)}$ rows:\n        1. $[Q^{(3)}_{0,1}, Q^{(3)}_{0,3}] = [1.2, 0.8]$, $Q^{(3)}_{0,0} = -2$.\n        2. $[Q^{(3)}_{1,0}, Q^{(3)}_{1,2}, Q^{(3)}_{1,3}] = [1.0, 0.5, 0.5]$, $Q^{(3)}_{1,1} = -2$.\n        3. Absorbing state: all off-diagonals $0$, $Q^{(3)}_{2,2} = 0$.\n        4. $[Q^{(3)}_{3,2}, Q^{(3)}_{3,4}] = [49, 1]$, $Q^{(3)}_{3,3} = -50$.\n        5. $[Q^{(3)}_{4,3}] = [5]$, $Q^{(3)}_{4,4} = -5$.\n    - Initial distribution $p_0^{(3)} = [0.0, 0.0, 0.0, 1.0, 0.0]$.\n    - Time horizon $T^{(3)} = 3$.\n    - $\\lambda_{\\min}^{(3)} = \\max_i a_i = 50$.\n\nFor each case $k \\in \\{1,2,3\\}$ and for each multiplier $m \\in \\{1,2,10\\}$, set $\\lambda = m \\lambda_{\\min}^{(k)}$ and compute the expected number of virtual jumps over $[0, T^{(k)}]$ using the approach above.\n\nFinal Output Format. Your program should produce a single line of output containing the nine results (three per case, ordered by multipliers $m = 1, 2, 10$ within each case, and cases in the order $k=1,2,3$) as a comma-separated list enclosed in square brackets, for example, $[r_{1,1}, r_{1,2}, r_{1,10}, r_{2,1}, r_{2,2}, r_{2,10}, r_{3,1}, r_{3,10}]$, where each $r_{k,m}$ is a floating-point number.", "solution": "The solution requires calculating the expected number of virtual jumps, which is the difference between the total expected number of Poisson clock ticks and the expected number of actual state-changing jumps over the time horizon $[0, T]$. We derive the necessary formula from first principles and then describe the numerical procedure.\n\n1.  **Total Expected Ticks**: The uniformizing clock is a homogeneous Poisson process with a constant rate $\\lambda$. The expected number of events (ticks) from this process over an interval of length $T$ is simply $\\lambda T$.\n\n2.  **Expected Actual Jumps**: An \"actual\" jump is a state change, which is distinct from a \"virtual\" jump (a self-transition). The instantaneous rate of actual jumps depends on the state of the system at that moment. If the system is in state $i$ at time $\\tau$, the total exit rate is $a_i = -Q_{ii}$. Therefore, the instantaneous expected rate of actual jumps at time $\\tau$ is the sum of each state's exit rate weighted by the probability of being in that state:\n    $$\n    \\text{Rate}_{\\text{actual}}(\\tau) = \\sum_{i=0}^{n-1} p_i(\\tau) a_i\n    $$\n    where $p_i(\\tau)$ is the probability of being in state $i$ at time $\\tau$. In vector notation, with $p(\\tau)$ as the row vector of probabilities and $\\mathbf{a}$ as the column vector of exit rates, this is $p(\\tau)\\mathbf{a}$.\n\n    To find the total expected number of actual jumps over the horizon $[0, T]$, we integrate this instantaneous rate:\n    $$\n    \\mathbb{E}[\\text{Actual Jumps}] = \\int_0^T p(\\tau)\\mathbf{a} \\, d\\tau\n    $$\n\n3.  **Probability Vector Evolution**: The probability vector $p(\\tau)$ evolves according to the Kolmogorov forward equation, with the solution $p(\\tau) = p_0 \\exp(Q\\tau)$, where $p_0$ is the initial distribution and $\\exp(Q\\tau)$ is the matrix exponential.\n\n4.  **Final Formula**: Combining these parts, the expected number of virtual jumps, $\\mathbb{E}[V(T)]$, is:\n    $$\n    \\mathbb{E}[V(T)] = (\\text{Total Expected Ticks}) - (\\text{Expected Actual Jumps}) = \\lambda T - \\int_0^T p_0 \\exp(Q\\tau) \\mathbf{a} \\, d\\tau\n    $$\n\n5.  **Numerical Implementation**: The provided Python code implements this formula. For each test case:\n    *   It first constructs the generator matrix $Q$ and the vector of exit rates $\\mathbf{a}$ from the problem definition.\n    *   It defines a Python function, `integrand(t, ...)`, which calculates the value of $p_0 \\exp(Qt) \\mathbf{a}$ for any given time `t`. This is done using `scipy.linalg.expm` to compute the matrix exponential.\n    *   It then uses `scipy.integrate.quad` to perform a numerical quadrature (integration) of this integrand function over the interval $[0, T]$. This computes the expected number of actual jumps.\n    *   Finally, it loops through the specified uniformization rates ($\\lambda = m \\lambda_{\\min}$). For each $\\lambda$, it calculates the total expected ticks ($\\lambda T$) and subtracts the previously computed expected number of actual jumps to find the result.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import expm\nfrom scipy.integrate import quad\n\ndef solve():\n    \"\"\"\n    Solves the CTMC virtual jump problem for the three specified test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Highly skewed exit rates, moderate horizon\n        {\n            \"Q_def\": {\n                0: {1: 0.006, 2: 0.004},\n                1: {0: 0.3, 2: 0.7},\n                2: {1: 2.5, 3: 7.5},\n                3: {2: 40, 4: 60},\n                4: {1: 300, 3: 700}\n            },\n            \"p0\": np.array([0.7, 0.2, 0.05, 0.04, 0.01]),\n            \"T\": 1.0,\n            \"n\": 5\n        },\n        # Case 2: Rarely visited ultra-fast state, longer horizon\n        {\n            \"Q_def\": {\n                0: {1: 0.25, 2: 0.25},\n                1: {0: 0.2, 2: 0.2},\n                2: {1: 0.29, 3: 0.01},\n                3: {2: 0.199999, 4: 0.000001},\n                4: {0: 2000, 3: 8000}\n            },\n            \"p0\": np.array([0.5, 0.49, 0.01, 0.0, 0.0]),\n            \"T\": 2.0,\n            \"n\": 5\n        },\n        # Case 3: Absorbing state present, intermediate horizon\n        {\n            \"Q_def\": {\n                0: {1: 1.2, 3: 0.8},\n                1: {0: 1.0, 2: 0.5, 3: 0.5},\n                2: {},  # Absorbing state\n                3: {2: 49, 4: 1},\n                4: {3: 5}\n            },\n            \"p0\": np.array([0.0, 0.0, 0.0, 1.0, 0.0]),\n            \"T\": 3.0,\n            \"n\": 5\n        }\n    ]\n\n    # Correction for Case 2 Q_def: The problem description has Q3,3 for state 4. It should be Q4,3\n    test_cases[1][\"Q_def\"][3][3] = -0.2\n    \n    results = []\n    \n    for case in test_cases:\n        # 1. Construct the generator matrix Q and exit rate vector a\n        n_states = case[\"n\"]\n        Q = np.zeros((n_states, n_states))\n        for i, transitions in case[\"Q_def\"].items():\n            row_sum = 0\n            for j, rate in transitions.items():\n                Q[i, j] = rate\n                row_sum += rate\n            if i in case[\"Q_def\"]:\n                 # Handle explicit diagonal definition if present\n                if i in case[\"Q_def\"][i]:\n                     Q[i,i] = case[\"Q_def\"][i][i]\n                else:\n                     Q[i, i] = -row_sum\n\n        # Verify and fix diagonals if they weren't explicitly set (e.g. Case 2, state 3)\n        for i in range(n_states):\n            if Q[i, i] == 0 and np.sum(Q[i, :]) != 0:\n                 Q[i, i] = -np.sum(Q[i, [j for j in range(n_states) if i != j]])\n\n        exit_rates = -np.diag(Q)\n        \n        # 2. Get other parameters for the case\n        p0 = case[\"p0\"]\n        T = case[\"T\"]\n        \n        # 3. Define the integrand for the expected number of actual jumps\n        # The integrand is f(t) = p(t) * a = (p0 * exp(Q*t)) * a\n        # where a is the column vector of exit rates.\n        def integrand(t, Q_matrix, p0_vec, a_vec):\n            p_t = p0_vec @ expm(Q_matrix * t)\n            return p_t @ a_vec\n\n        # 4. Calculate expected number of actual jumps by numerical integration\n        # quad returns a tuple (integral_value, error_estimate)\n        expected_actual_jumps, _ = quad(integrand, 0, T, args=(Q, p0, exit_rates))\n\n        # 5. Calculate lambda_min and iterate through multipliers\n        lambda_min = np.max(exit_rates)\n        multipliers = [1, 2, 10]\n        \n        for m in multipliers:\n            lambda_rate = m * lambda_min\n            \n            # Expected total jumps (Poisson ticks) = lambda * T\n            expected_total_jumps = lambda_rate * T\n            \n            # Expected virtual jumps = Total - Actual\n            expected_virtual_jumps = expected_total_jumps - expected_actual_jumps\n            results.append(expected_virtual_jumps)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```", "id": "3359521"}, {"introduction": "A deep understanding of an algorithm often comes from knowing not just why it works, but also why alternatives fail. The correctness of uniformization hinges on a single, dominating Poisson process whose events are \"thinned\" to correctly reproduce state-dependent jump dynamics. This exercise sharpens your conceptual grasp by contrasting the standard algorithm with a plausible but flawed \"state-dependent\" version, forcing you to articulate precisely why the dominating rate $\\Lambda$ and the unified transition matrix $P = I + Q/\\Lambda$ are essential for an unbiased simulation of the process path [@problem_id:3359538].", "problem": "Consider the continuous-time Markov chain (CTMC) with two states $1$ and $2$, generator\n$$\nQ \\;=\\; \\begin{pmatrix}\n-1 & 1\\\\\n2 & -2\n\\end{pmatrix},\n$$\ntime horizon $T>0$, and initial state $X_0=1$. Let $q_i := -Q_{ii}$ denote the total exit rate from state $i$. Recall the defining construction of a CTMC with generator $Q$: when the process is in state $i$, the holding time is exponentially distributed with rate $q_i$, and at the jump epoch it transitions to $j\\neq i$ with probability $Q_{ij}/q_i$.\n\nWe will compare two simulation algorithms for sample paths on $[0,T]$.\n\nAlgorithm U (constant-rate uniformization): Fix a constant $\\Lambda \\ge \\max_i q_i$. Simulate a homogeneous Poisson process of rate $\\Lambda$ on $[0,T]$. At each Poisson time, if the current state is $i$, perform a state update by sampling from the transition matrix $P := I + Q/\\Lambda$ (so the update may be a self-transition). The resulting piecewise-constant process is taken as the simulated path.\n\nAlgorithm NSD (naive state-dependent uniformization with a frozen clock): Define $\\lambda(i) := q_i$. Draw a homogeneous Poisson process on $[0,T]$ with rate frozen at the initial stateâ€™s value $\\lambda(X_0)=q_1=1$. Let the $N$ points of this clock be the only update times on $[0,T]$. At each update time, if the current state is $i$, perform a state update by sampling from the transition matrix $P(i) := I + Q/\\lambda(i)$ (so the update may be a self-transition). The number and locations of update times do not adapt when the state changes.\n\nWhich of the following statements is/are correct?\n\nA. For the given $Q$, Algorithm U with $\\Lambda=2$ produces the exact path law on $[0,T]$. By contrast, Algorithm NSD yields a biased path law; in particular, as $T\\to 0$, it underestimates the probability of observing at least two real jumps by a factor asymptotically equal to $1/2$, because freezing the Poisson rate at $\\lambda(X_0)$ breaks the required coupling between event times and the evolving state-dependent intensity.\n\nB. Algorithm NSD is exact for all finite-state CTMCs, because while in any state $i$ it indirectly reproduces the correct exponential holding time when combined with $P(i)=I+Q/\\lambda(i)$.\n\nC. Both Algorithm U and Algorithm NSD are biased unless the chosen rate strictly satisfies $\\Lambda > \\max_i q_i$ (or $\\lambda(i) > q_i$), since equality cannot be used to recover the correct holding times.\n\nD. Algorithm NSD becomes exact if and only if all exit rates are equal across states, that is, $q_i \\equiv q$ for all $i$, in which case it reduces to the constant-rate uniformization with $\\Lambda=q$.", "solution": "This problem tests the fundamental requirements for a correct simulation via uniformization. Let's analyze the properties of the given CTMC, each algorithm, and then evaluate the statements.\n\nFrom the generator matrix $Q = \\begin{pmatrix} -1 & 1 \\\\ 2 & -2 \\end{pmatrix}$, we extract the exit rates: $q_1 = -Q_{11} = 1$ and $q_2 = -Q_{22} = 2$.\n\n**Analysis of Algorithm U (Standard Uniformization)**\nThis is the standard, mathematically exact uniformization method. It requires a single, constant Poisson clock rate $\\Lambda$ that dominates all state-specific exit rates, i.e., $\\Lambda \\ge \\max_i q_i$. For our CTMC, this means we need $\\Lambda \\ge \\max(1, 2) = 2$. At each tick of this clock, a state transition is chosen from the stochastic matrix $P = I + Q/\\Lambda$. This procedure of generating events at a maximal rate and then \"thinning\" them (by allowing for fictitious self-transitions) correctly reproduces the law of the original CTMC. Therefore, Algorithm U is exact for any $\\Lambda \\ge 2$.\n\n**Analysis of Algorithm NSD (Naive State-Dependent Uniformization)**\nThis algorithm is flawed. It generates a single set of event times using a Poisson process whose rate is frozen at the exit rate of the *initial* state, $\\lambda(X_0) = q_1 = 1$. When the process starts in state 1, the time to the first event is $\\text{Exponential}(1)$, which correctly matches the holding time in state 1. However, after the first jump (which must be to state 2), the process is now in state 2, where the correct exit rate is $q_2 = 2$. But the clock is still ticking at rate 1. This means the process will wait for the next event from the original rate-1 Poisson process, giving it a holding time in state 2 of $\\text{Exponential}(1)$ instead of the correct $\\text{Exponential}(2)$. Because the clock rate does not adapt to the current state, the algorithm produces a biased path law.\n\n**Evaluation of the Statements**\n\n**A. Correct.** The first part, \"Algorithm U with $\\Lambda=2$ produces the exact path law\", is correct, as $\\Lambda=2$ satisfies the condition $\\Lambda \\ge \\max_i q_i$. The second part, \"Algorithm NSD yields a biased path law\", is correct for the reasons explained above. The final part makes a quantitative claim about the bias.\n- For the true process, the probability of at least two jumps by time $T$ requires a jump $1 \\to 2$ at time $t_1  T$ and a jump $2 \\to 1$ at time $t_1+t_2  T$, where $t_1 \\sim \\text{Exponential}(1)$ and $t_2 \\sim \\text{Exponential}(2)$. For small $T$, this probability is $\\mathbb{P}(T_1+T_2 \\le T) \\approx \\frac{q_1 q_2 T^2}{2} = \\frac{1 \\cdot 2 \\cdot T^2}{2} = T^2$.\n- For Algorithm NSD, the jumps occur according to a single Poisson process of rate $q_1=1$. The probability of at least two jumps is $\\mathbb{P}(N(T) \\ge 2)$ for $N \\sim \\text{Poisson}(T)$. For small $T$, this is $\\approx \\frac{T^2}{2}$.\n- The ratio of the probability from NSD to the true probability is $(T^2/2) / T^2 = 1/2$. So, NSD underestimates this probability by a factor of 2. The entire statement is correct.\n\n**B. Incorrect.** This claims Algorithm NSD is always exact. Our analysis shows it is biased for the given $Q$, providing a clear counterexample. The reasoning is also flawed: the holding time is dictated by the rate of the Poisson clock, not the transition matrix $P(i)$.\n\n**C. Incorrect.** This claims that strict inequality ($\\Lambda  \\max_i q_i$) is required for exactness. This is false for Algorithm U. The standard proof of uniformization holds for $\\Lambda \\ge \\max_i q_i$. If $\\Lambda = \\max_i q_i$, then for any state $k$ with the maximal rate ($q_k = \\Lambda$), the probability of a self-loop is $P_{kk} = 1 - q_k/\\Lambda = 0$. This simply means every clock tick while in state $k$ results in a real jump. The rate of real jumps is $\\Lambda \\times 1 = \\Lambda = q_k$, and the holding time is $\\text{Exponential}(\\Lambda) = \\text{Exponential}(q_k)$, which is correct.\n\n**D. Correct.** This statement provides the necessary and sufficient condition for NSD to be exact.\n- **\"If\" part:** If all exit rates are equal, $q_i \\equiv q$ for all $i$. Then NSD uses a clock with rate $\\lambda(X_0) = q_{X_0} = q$. Since this rate is the correct exit rate for *every* state, the clock never becomes \"wrong\". The transition matrix used is $P = I+Q/q$. This entire procedure is identical to standard uniformization (Algorithm U) with the choice $\\Lambda=q$. Since $\\Lambda = q = \\max_i q_i$, this is a valid and exact implementation of Algorithm U.\n- **\"Only if\" part:** Assume NSD is exact. If the chain is irreducible and there exist two states $i, j$ with $q_i \\neq q_j$, we can construct a path that starts in $i$ and jumps to $j$. The NSD clock will run at rate $q_i$. Once in state $j$, the holding time will be incorrectly simulated as $\\text{Exponential}(q_i)$ instead of $\\text{Exponential}(q_j)$, creating a bias. Therefore, for NSD to be exact, all reachable states must have the same exit rate, which implies all $q_i$ must be equal for an irreducible chain.\n\nSince both A and D are correct statements, they are the answer.", "answer": "$$\\boxed{AD}$$", "id": "3359538"}]}