{"hands_on_practices": [{"introduction": "Simulating mixed random variables requires careful handling of both their discrete and continuous components. A common but flawed shortcut is to ignore the discrete point masses and sample only from the normalized continuous part. This exercise delves into the consequences of this error, challenging you to analytically derive the resulting bias in a Monte Carlo estimation. By working through this problem, you will gain a rigorous understanding of how each part of a mixed distribution contributes to its expectation and the importance of correct implementation.", "problem": "Consider a mixed random variable $X$ on $[0,\\infty)$ defined by the following decomposition of its probability measure into a discrete atom and an absolutely continuous part. The atom is located at $x=0$ with mass $\\alpha \\in (0,1)$, that is $\\mathbb{P}(X=0)=\\alpha$. Conditional on $X0$, the continuous component has an exponential distribution with rate $\\lambda0$, so that the density on $(0,\\infty)$ is $f(x)=(1-\\alpha)\\lambda \\exp(-\\lambda x)$. The cumulative distribution function $F$ therefore satisfies $F(0)=\\alpha$ and, for $x0$, $F(x)=\\alpha+(1-\\alpha)(1-\\exp(-\\lambda x))$.\n\nYou are tasked with estimating the quantity $\\theta=\\mathbb{E}[\\exp(-\\beta X)]$ for a fixed $\\beta0$ using Monte Carlo simulation. A practitioner, unaware of the need for explicit randomization at jumps, implements a naive inverse transform sampling scheme that ignores the atom: they construct a continuous proxy cumulative distribution function by normalizing the absolutely continuous component to total mass $1$, namely $F_{\\mathrm{cont}}(x)=(1-\\alpha)^{-1}(F(x)-\\alpha)=1-\\exp(-\\lambda x)$ for $x\\geq 0$, and then generate samples via $X_{\\mathrm{naive}}=F_{\\mathrm{cont}}^{-1}(U)$ with $U\\sim \\mathrm{Uniform}(0,1)$. In other words, the naive simulator samples $X_{\\mathrm{naive}}$ from a purely exponential distribution with rate $\\lambda$ and omits the randomization that would select the atom at $x=0$ with probability $\\alpha$.\n\nStarting only from the definitions of mixed random variables, their cumulative distribution functions and the definition of expectation as an integral with respect to the probability measure, derive the exact bias of the resulting Monte Carlo estimator as the number of samples tends to infinity. That is, derive the closed-form expression for\n$$\n\\mathrm{Bias}=\\lim_{n\\to\\infty}\\left(\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\exp(-\\beta X_{\\mathrm{naive},i})\\right]-\\theta\\right),\n$$\nwhere $X_{\\mathrm{naive},i}$ are independent and identically distributed draws from the naive inverse scheme described above. Express your final answer as a single closed-form analytic expression in terms of $\\alpha$, $\\lambda$, and $\\beta$. No rounding is required.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- A mixed random variable $X$ is defined on the support $[0, \\infty)$.\n- Its probability measure has a discrete atom at $x=0$ with mass $\\mathbb{P}(X=0)=\\alpha$, where $\\alpha \\in (0,1)$.\n- Its absolutely continuous component is defined on $(0, \\infty)$ with probability density $f(x)=(1-\\alpha)\\lambda \\exp(-\\lambda x)$ for $\\lambda0$. This corresponds to the density of the variable $X$ conditional on $X0$ being $\\lambda \\exp(-\\lambda x)$.\n- The cumulative distribution function (CDF) is $F(x) = \\alpha$ for $x=0$ and $F(x)=\\alpha+(1-\\alpha)(1-\\exp(-\\lambda x))$ for $x0$.\n- The quantity to be estimated is $\\theta=\\mathbb{E}[\\exp(-\\beta X)]$ for a fixed constant $\\beta0$.\n- A naive Monte Carlo sampling scheme generates samples $X_{\\mathrm{naive}}$ from a purely exponential distribution with rate $\\lambda$. The problem states this is done via $X_{\\mathrm{naive}}=F_{\\mathrm{cont}}^{-1}(U)$ with $U\\sim \\mathrm{Uniform}(0,1)$ and $F_{\\mathrm{cont}}(x)=1-\\exp(-\\lambda x)$.\n- The task is to derive the asymptotic bias of the Monte Carlo estimator:\n$$\n\\mathrm{Bias}=\\lim_{n\\to\\infty}\\left(\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\exp(-\\beta X_{\\mathrm{naive},i})\\right]-\\theta\\right)\n$$\nwhere $X_{\\mathrm{naive},i}$ are independent and identically distributed (i.i.d.) draws.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is grounded in the established principles of probability theory, specifically concerning mixed random variables, expectation, and Monte Carlo simulation. The concepts of CDF, probability density, atoms, and inverse transform sampling are standard.\n- **Well-Posedness**: The problem is well-posed. It asks for a definite, computable quantity (asymptotic bias) based on clearly defined distributions and parameters. The parameters $\\alpha$, $\\lambda$, and $\\beta$ are constrained, ensuring the requested expectations are finite and well-defined.\n- **Objectivity**: The language is precise, mathematical, and free of subjective content.\n- **Consistency and Completeness**: The provided information is self-consistent. The CDF $F(x)$ correctly combines the point mass at $x=0$ with the integral of the given density on $(0, \\infty)$: $\\mathbb{P}(X \\le x) = \\mathbb{P}(X=0) + \\int_0^x (1-\\alpha)\\lambda \\exp(-\\lambda t) dt = \\alpha + (1-\\alpha)[-\\exp(-\\lambda t)]_0^x = \\alpha + (1-\\alpha)(1-\\exp(-\\lambda x))$, which matches the provided $F(x)$. All necessary information is given.\n\n**Step 3: Verdict and Action**\nThe problem is valid. The solution process will now commence.\n\nThe bias of an estimator $\\hat{\\theta}_n = \\frac{1}{n}\\sum_{i=1}^{n} g(X_i)$ for a parameter $\\theta = \\mathbb{E}[g(X)]$ is defined as $\\mathbb{E}[\\hat{\\theta}_n] - \\theta$. In this problem, the estimator is constructed using samples $X_{\\mathrm{naive},i}$ instead of the correct samples $X_i$. The quantity to be calculated is the asymptotic bias.\n\nFirst, let's simplify the expression for the bias. The estimator is $\\hat{\\theta}_{\\mathrm{naive}, n} = \\frac{1}{n}\\sum_{i=1}^{n}\\exp(-\\beta X_{\\mathrm{naive},i})$. By linearity of expectation and the fact that the samples $X_{\\mathrm{naive},i}$ are i.i.d., its expectation is:\n$$\n\\mathbb{E}\\left[\\hat{\\theta}_{\\mathrm{naive}, n}\\right] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\exp(-\\beta X_{\\mathrm{naive},i})\\right] = \\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}[\\exp(-\\beta X_{\\mathrm{naive},i})] = \\frac{1}{n} \\cdot n \\cdot \\mathbb{E}[\\exp(-\\beta X_{\\mathrm{naive}})] = \\mathbb{E}[\\exp(-\\beta X_{\\mathrm{naive}})]\n$$\nThis expectation does not depend on the sample size $n$. The bias is therefore constant with respect to $n$. The limit in the problem's definition of bias is thus simply the bias itself:\n$$\n\\mathrm{Bias} = \\mathbb{E}[\\exp(-\\beta X_{\\mathrm{naive}})] - \\theta = \\mathbb{E}[\\exp(-\\beta X_{\\mathrm{naive}})] - \\mathbb{E}[\\exp(-\\beta X)]\n$$\nThe problem reduces to calculating two expectations and finding their difference.\n\n**1. Calculation of the true expectation, $\\theta = \\mathbb{E}[\\exp(-\\beta X)]$**\n\nThe expectation of a function $g(X)$ of a mixed random variable $X$ is computed by integrating $g(x)$ with respect to the probability measure $dP_X(x)$. This measure consists of a discrete part (a point mass) and a continuous part.\n$$\n\\theta = \\mathbb{E}[\\exp(-\\beta X)] = \\int_{[0,\\infty)} \\exp(-\\beta x) \\, dP_X(x)\n$$\nThis integral can be decomposed into the sum of the contribution from the atom at $x=0$ and the integral over the continuous part on $(0,\\infty)$:\n$$\n\\theta = \\exp(-\\beta \\cdot 0) \\cdot \\mathbb{P}(X=0) + \\int_0^{\\infty} \\exp(-\\beta x) f(x) \\, dx\n$$\nUsing the given values, $\\mathbb{P}(X=0)=\\alpha$ and $f(x)=(1-\\alpha)\\lambda \\exp(-\\lambda x)$ for $x0$:\n$$\n\\theta = \\exp(0) \\cdot \\alpha + \\int_0^{\\infty} \\exp(-\\beta x) (1-\\alpha)\\lambda \\exp(-\\lambda x) \\, dx\n$$\n$$\n\\theta = 1 \\cdot \\alpha + (1-\\alpha)\\lambda \\int_0^{\\infty} \\exp(-(\\lambda+\\beta)x) \\, dx\n$$\nThe integral is evaluated as:\n$$\n\\int_0^{\\infty} \\exp(-(\\lambda+\\beta)x) \\, dx = \\left[ \\frac{\\exp(-(\\lambda+\\beta)x)}{-(\\lambda+\\beta)} \\right]_0^{\\infty} = 0 - \\frac{1}{-(\\lambda+\\beta)} = \\frac{1}{\\lambda+\\beta}\n$$\nThe integral converges since $\\lambda0$ and $\\beta0$, which implies $\\lambda+\\beta0$.\nSubstituting this result back into the expression for $\\theta$:\n$$\n\\theta = \\alpha + (1-\\alpha) \\lambda \\frac{1}{\\lambda+\\beta} = \\alpha + (1-\\alpha) \\frac{\\lambda}{\\lambda+\\beta}\n$$\n\n**2. Calculation of the naive expectation, $\\mathbb{E}[\\exp(-\\beta X_{\\mathrm{naive}})]$**\n\nThe problem states that $X_{\\mathrm{naive}}$ is sampled from a purely exponential distribution with rate $\\lambda$. This is consistent with the inverse transform method applied to $F_{\\mathrm{cont}}(x) = 1-\\exp(-\\lambda x)$. The probability density function of $X_{\\mathrm{naive}}$ is $f_{\\mathrm{naive}}(x) = \\lambda \\exp(-\\lambda x)$ for $x \\ge 0$.\nThe expectation is calculated as:\n$$\n\\mathbb{E}[\\exp(-\\beta X_{\\mathrm{naive}})] = \\int_0^{\\infty} \\exp(-\\beta x) f_{\\mathrm{naive}}(x) \\, dx = \\int_0^{\\infty} \\exp(-\\beta x) \\lambda \\exp(-\\lambda x) \\, dx\n$$\n$$\n\\mathbb{E}[\\exp(-\\beta X_{\\mathrm{naive}})] = \\lambda \\int_0^{\\infty} \\exp(-(\\lambda+\\beta)x) \\, dx\n$$\nThis is the same integral form as encountered before, multiplied by $\\lambda$:\n$$\n\\mathbb{E}[\\exp(-\\beta X_{\\mathrm{naive}})] = \\lambda \\left( \\frac{1}{\\lambda+\\beta} \\right) = \\frac{\\lambda}{\\lambda+\\beta}\n$$\nThis result is also recognizable as the moment-generating function of an exponential distribution with rate $\\lambda$, $M(t)=\\frac{\\lambda}{\\lambda-t}$, evaluated at $t=-\\beta$.\n\n**3. Calculation of the Bias**\n\nFinally, we compute the bias by subtracting the true expectation $\\theta$ from the naively computed expectation.\n$$\n\\mathrm{Bias} = \\mathbb{E}[\\exp(-\\beta X_{\\mathrm{naive}})] - \\theta = \\frac{\\lambda}{\\lambda+\\beta} - \\left( \\alpha + (1-\\alpha) \\frac{\\lambda}{\\lambda+\\beta} \\right)\n$$\nDistributing the negative sign:\n$$\n\\mathrm{Bias} = \\frac{\\lambda}{\\lambda+\\beta} - \\alpha - (1-\\alpha) \\frac{\\lambda}{\\lambda+\\beta}\n$$\nGroup the terms containing the fraction:\n$$\n\\mathrm{Bias} = \\left(1 - (1-\\alpha)\\right) \\frac{\\lambda}{\\lambda+\\beta} - \\alpha\n$$\n$$\n\\mathrm{Bias} = (1 - 1 + \\alpha) \\frac{\\lambda}{\\lambda+\\beta} - \\alpha\n$$\n$$\n\\mathrm{Bias} = \\alpha \\frac{\\lambda}{\\lambda+\\beta} - \\alpha\n$$\nFactor out the common term $\\alpha$:\n$$\n\\mathrm{Bias} = \\alpha \\left( \\frac{\\lambda}{\\lambda+\\beta} - 1 \\right)\n$$\nCombine the terms inside the parenthesis over a common denominator:\n$$\n\\mathrm{Bias} = \\alpha \\left( \\frac{\\lambda - (\\lambda+\\beta)}{\\lambda+\\beta} \\right) = \\alpha \\left( \\frac{\\lambda - \\lambda - \\beta}{\\lambda+\\beta} \\right) = \\alpha \\left( \\frac{-\\beta}{\\lambda+\\beta} \\right)\n$$\nThus, the final expression for the bias is:\n$$\n\\mathrm{Bias} = -\\frac{\\alpha\\beta}{\\lambda+\\beta}\n$$\nThe bias is negative, which is expected. The naive sampler ignores the atom at $x=0$, where the function $g(x)=\\exp(-\\beta x)$ attains its maximum value of $g(0)=1$. By sampling only from the continuous part where $x0$ and $g(x)1$, the naive estimator systematically underestimates the true mean.", "answer": "$$\n\\boxed{-\\frac{\\alpha\\beta}{\\lambda+\\beta}}\n$$", "id": "3333837"}, {"introduction": "Recognizing the distinct structure of a mixed random variable is the first step; the next is to exploit it for computational gain. This practice guides you through the design of a stratified Monte Carlo estimator, a powerful variance reduction technique perfectly suited for mixed distributions. You will derive the optimal allocation of simulation budget between the discrete and continuous strata, revealing a profound principle of efficient simulation: concentrate your effort where there is uncertainty.", "problem": "Construct and analyze a stratified Monte Carlo estimator for a mixed random variable using only fundamental definitions and principles. Let $X$ be a mixed random variable with a discrete atom at $0$ and a continuous component on $(0,\\infty)$. Specifically, assume $\\mathbb{P}(X=0)=p$ and the conditional distribution of $X$ given $X0$ has a probability density function $g$ on $(0,\\infty)$; equivalently, $X$ has a mixed distribution with mass $p$ at $0$ and density $(1-p)g(x)$ on $(0,\\infty)$. Given a measurable function $h:(0,\\infty)\\to\\mathbb{R}$ with $h(0)$ defined, your task is to derive a stratified estimator for $\\mathbb{E}[h(X)]$ by stratifying on the sets $\\{X=0\\}$ and $\\{X0\\}$ and to optimize the sample allocation between strata under a cost constraint to minimize variance.\n\nBase your derivation on the following fundamental definitions and well-tested facts:\n- The law of total expectation: $\\mathbb{E}[h(X)] = \\mathbb{E}[h(X)\\mid X=0]\\mathbb{P}(X=0) + \\mathbb{E}[h(X)\\mid X0]\\mathbb{P}(X0)$.\n- For a stratified estimator using independent samples within strata with allocations $n_0$ samples from $\\{X=0\\}$ and $n_1$ samples from $\\{X0\\}$, with stratum weights $w_0=\\mathbb{P}(X=0)$ and $w_1=\\mathbb{P}(X0)$, the unbiased estimator is $\\widehat{\\mu} = w_0 \\widehat{\\mu}_0 + w_1 \\widehat{\\mu}_1$, where $\\widehat{\\mu}_i$ is the sample mean of $h(X)$ within stratum $i\\in\\{0,1\\}$.\n- The variance of the stratified estimator under independence is $\\mathrm{Var}(\\widehat{\\mu}) = \\sum_{i\\in\\{0,1\\}} \\frac{w_i^2 \\sigma_i^2}{n_i}$ where $\\sigma_i^2$ is the within-stratum variance of $h(X)$.\n- A fixed computational budget modeled by a linear cost constraint $\\sum_{i\\in\\{0,1\\}} c_i n_i \\le B$, with known positive per-sample costs $c_0$ and $c_1$, and a fixed budget $B$ (interpreted as total allowable cost).\n\nYour tasks:\n- Derive from first principles an optimal allocation rule $(n_0^\\star,n_1^\\star)$ that minimizes $\\mathrm{Var}(\\widehat{\\mu})$ subject to the budget constraint $\\sum_{i\\in\\{0,1\\}} c_i n_i \\le B$, for given $p$ and $g$ and measurable $h$.\n- Specialize your derivation to the given mixed structure where $\\{X=0\\}$ is an atom, and clearly state what happens to the optimal allocation in this case.\n- Construct an implementable stratified estimator that uses the derived optimal allocation. If any stratum does not require sampling under the optimal allocation, explain why and how your estimator simplifies in that case.\n- Implement the estimator and compute its value for each of the test cases below using a pseudorandom generator with a fixed seed $s$ per test case so that the output is deterministic. Use independent streams across test cases by seeding with $s+k$ for test case index $k$ starting at $0$.\n\nTest suite (each case specifies $p$, continuous part $g$, function $h$, budget $B$ with unitless costs, and per-sample costs $c_0,c_1$):\n- Case $1$: $p=\\frac{1}{3}$, $g$ is exponential with rate $\\lambda=2$ (density $g(x)=2 e^{-2x}$ on $(0,\\infty)$), $h(x)=x + \\mathbf{1}\\{x1\\}$, budget $B=1000$, costs $c_0=1$, $c_1=1$, seed $s=12345$.\n- Case $2$: $p=0.99$, $g$ is gamma with shape $k=3$ and scale $\\theta=1$ (density $g(x)=\\frac{x^{k-1}e^{-x/\\theta}}{\\Gamma(k)\\theta^k}$ on $(0,\\infty)$), $h(x)=x^2$, budget $B=200$, costs $c_0=1$, $c_1=1$, seed $s=12345$.\n- Case $3$: $p=0.05$, $g$ is lognormal with parameters $\\mu=0$ and $\\sigma=1$ (i.e., $\\log Y\\sim \\mathcal{N}(0,1)$ for $Y\\sim g$), $h(x)=\\mathbf{1}\\{x2\\}$, budget $B=500$, costs $c_0=1$, $c_1=1$, seed $s=12345$.\n- Case $4$: $p=1$, $g$ is exponential with rate $\\lambda=1$ (density $g(x)=e^{-x}$ on $(0,\\infty)$), $h(x)=\\sqrt{x+1}-1$, budget $B=50$, costs $c_0=1$, $c_1=1$, seed $s=12345$.\n\nFor each case, in addition to computing the stratified estimator with the optimal allocation, also compute the exact value of $\\mathbb{E}[h(X)]$ using analytic formulas derived from the definitions wherever they are available:\n- For exponential with rate $\\lambda$, $\\mathbb{E}[Y]=\\frac{1}{\\lambda}$ and $\\mathbb{P}(Yt)=e^{-\\lambda t}$ for $t0$.\n- For gamma with shape $k$ and scale $\\theta$, $\\mathbb{E}[Y^2]=\\theta^2 k(1+k)$.\n- For lognormal with parameters $\\mu$ and $\\sigma$, $\\mathbb{P}(Yt)=1-\\Phi\\left(\\frac{\\ln t - \\mu}{\\sigma}\\right)$ for $t0$, where $\\Phi$ is the cumulative distribution function of the standard normal distribution.\n\nYour program must:\n- Implement the optimal allocation computation and the stratified estimator based on your derivation.\n- For each test case, output the optimal stratum sample counts $(n_0^\\star,n_1^\\star)$ as integers, the stratified estimator value as a floating-point number, and the exact value as a floating-point number computed from the given analytic formulas.\n- Use the specified seeds to make the simulation results deterministic.\n- Ensure that the budget constraint $\\sum_{i\\in\\{0,1\\}} c_i n_i \\le B$ is satisfied.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case in the order $1,2,3,4$, append in order four entries: $n_0^\\star$, $n_1^\\star$, the stratified estimator value, and the exact value. For example, an output with two cases would look like $[n_{0,1}^\\star,n_{1,1}^\\star,\\widehat{\\mu}_1,\\mu_1,n_{0,2}^\\star,n_{1,2}^\\star,\\widehat{\\mu}_2,\\mu_2]$; extend this pattern for all four cases.", "solution": "The problem requires the derivation and implementation of a stratified Monte Carlo estimator for a mixed random variable. The solution proceeds in two stages: first, a formal derivation of the optimal sample allocation and the resulting estimator, and second, the application of this framework to the specific test cases provided.\n\n### Principle-Based Derivation and Design\n\nLet $X$ be a mixed random variable with distribution $\\mathbb{P}(X=0)=p$ and a conditional probability density function $g(x)$ on $(0, \\infty)$ for $X0$. We are tasked with estimating $\\mathbb{E}[h(X)]$ for a given function $h$.\n\nThe problem specifies stratifying the sample space into two strata: $S_0 = \\{x | x=0\\}$ and $S_1 = \\{x | x0\\}$. The corresponding stratum weights are the probabilities of these events:\n$w_0 = \\mathbb{P}(X \\in S_0) = \\mathbb{P}(X=0) = p$\n$w_1 = \\mathbb{P}(X \\in S_1) = \\mathbb{P}(X0) = 1-p$\n\nThe law of total expectation provides the exact value we wish to estimate:\n$$\n\\mathbb{E}[h(X)] = \\mathbb{E}[h(X) | X \\in S_0] w_0 + \\mathbb{E}[h(X) | X \\in S_1] w_1\n$$\nLet $\\mu_i = \\mathbb{E}[h(X) | X \\in S_i]$ be the conditional mean of $h(X)$ in stratum $i$. Then $\\mathbb{E}[h(X)] = \\mu_0 w_0 + \\mu_1 w_1$.\n\nA stratified Monte Carlo estimator for $\\mathbb{E}[h(X)]$ is given by $\\widehat{\\mu} = w_0 \\widehat{\\mu}_0 + w_1 \\widehat{\\mu}_1$, where $\\widehat{\\mu}_i$ is the sample mean of $h(X)$ based on $n_i$ samples drawn from stratum $i$. The variance of this estimator, assuming independent samples, is:\n$$\n\\mathrm{Var}(\\widehat{\\mu}) = \\frac{w_0^2 \\sigma_0^2}{n_0} + \\frac{w_1^2 \\sigma_1^2}{n_1}\n$$\nwhere $\\sigma_i^2 = \\mathrm{Var}(h(X) | X \\in S_i)$ is the variance of $h(X)$ within stratum $i$.\n\n**1. Optimal Sample Allocation**\n\nOur goal is to minimize this variance subject to a linear budget constraint $\\sum_{i=0,1} c_i n_i \\le B$, where $c_i  0$ is the cost per sample in stratum $i$ and $B$ is the total budget. To minimize variance, we should use the entire budget, so the constraint becomes an equality: $c_0 n_0 + c_1 n_1 = B$.\n\nThis is a constrained optimization problem that can be solved using the method of Lagrange multipliers. The Lagrangian is:\n$$\nL(n_0, n_1, \\lambda) = \\left( \\frac{w_0^2 \\sigma_0^2}{n_0} + \\frac{w_1^2 \\sigma_1^2}{n_1} \\right) + \\lambda(c_0 n_0 + c_1 n_1 - B)\n$$\nSetting the partial derivatives with respect to $n_0$ and $n_1$ to zero yields:\n$$\n\\frac{\\partial L}{\\partial n_i} = -\\frac{w_i^2 \\sigma_i^2}{n_i^2} + \\lambda c_i = 0 \\implies n_i^2 = \\frac{w_i^2 \\sigma_i^2}{\\lambda c_i} \\implies n_i = \\frac{w_i \\sigma_i}{\\sqrt{\\lambda c_i}}\n$$\nThis implies that the optimal allocation $n_i^\\star$ is proportional to $w_i \\sigma_i / \\sqrt{c_i}$. To find the constant of proportionality, we substitute this into the budget constraint:\n$$\n\\sum_{i=0,1} c_i \\left( \\frac{w_i \\sigma_i}{\\sqrt{\\lambda c_i}} \\right) = B \\implies \\frac{1}{\\sqrt{\\lambda}} \\sum_{i=0,1} w_i \\sigma_i \\sqrt{c_i} = B \\implies \\frac{1}{\\sqrt{\\lambda}} = \\frac{B}{\\sum_{j=0,1} w_j \\sigma_j \\sqrt{c_j}}\n$$\nSubstituting this back into the expression for $n_i$ gives the optimal allocation (for real-valued $n_i$):\n$$\nn_i^\\star = B \\frac{w_i \\sigma_i / \\sqrt{c_i}}{\\sum_{j=0,1} w_j \\sigma_j \\sqrt{c_j}}\n$$\n\n**2. Specialization to the Mixed Distribution**\n\nWe now apply this general result to our specific stratification.\n-   **Stratum $S_0 = \\{X=0\\}$:** A sample from this stratum is always $X=0$. Thus, the value $h(X)$ is always the constant $h(0)$. The variance of a constant is zero. Therefore, the within-stratum variance $\\sigma_0^2$ is:\n    $$\n    \\sigma_0^2 = \\mathrm{Var}(h(X) | X=0) = 0\n    $$\n-   **Stratum $S_1 = \\{X0\\}$:** A sample from this stratum follows the distribution with density $g(x)$. The variance $\\sigma_1^2$ is the variance of $h(Y)$ where $Y \\sim g$, i.e., $\\sigma_1^2 = \\mathrm{Var}(h(Y))$.\n\nThe fact that $\\sigma_0 = 0$ dramatically simplifies the optimal allocation. Substituting $\\sigma_0=0$ into the allocation formula:\n$$\nn_0^\\star = B \\frac{w_0 (0) / \\sqrt{c_0}}{w_0 (0) \\sqrt{c_0} + w_1 \\sigma_1 \\sqrt{c_1}} = 0\n$$\n$$\nn_1^\\star = B \\frac{w_1 \\sigma_1 / \\sqrt{c_1}}{w_0 (0) \\sqrt{c_0} + w_1 \\sigma_1 \\sqrt{c_1}} = B \\frac{w_1 \\sigma_1 / \\sqrt{c_1}}{w_1 \\sigma_1 \\sqrt{c_1}} = \\frac{B}{c_1}\n$$\nThis derivation holds if $w_1 \\sigma_1  0$, i.e., $p1$ and $h(X)$ is not constant on $(0, \\infty)$. Since sample sizes must be integers, we take the floor to ensure the budget constraint is satisfied:\n$$\nn_0^\\star = 0 \\quad \\text{and} \\quad n_1^\\star = \\left\\lfloor \\frac{B}{c_1} \\right\\rfloor\n$$\nThis result is intuitive: since the outcome in stratum $S_0$ is deterministic, there is no uncertainty to reduce. All of the simulation budget should be allocated to stratum $S_1$, which is the sole source of variance.\n\n**3. The Resulting Estimator**\n\nWith the optimal allocation $n_0^\\star=0$, we do not perform any sampling for stratum $S_0$. Instead of a sample mean $\\widehat{\\mu}_0$, we use the exact conditional expectation $\\mu_0 = \\mathbb{E}[h(X)|X=0] = h(0)$. For stratum $S_1$, we generate $n_1^\\star$ samples $Y_1, \\dots, Y_{n_1^\\star}$ from the density $g(x)$ and compute the sample mean $\\widehat{\\mu}_1 = \\frac{1}{n_1^\\star} \\sum_{j=1}^{n_1^\\star} h(Y_j)$.\n\nThe final stratified estimator is:\n$$\n\\widehat{\\mu} = w_0 \\mu_0 + w_1 \\widehat{\\mu}_1 = p \\cdot h(0) + (1-p) \\frac{1}{n_1^\\star} \\sum_{j=1}^{n_1^\\star} h(Y_j)\n$$\n\n**4. Edge Case: $p=1$**\n\nIf $p=1$, the random variable $X$ is deterministically $0$. Thus, $\\mathbb{E}[h(X)] = h(0)$ exactly. No estimation is required. In this case, $w_0=1$ and $w_1=0$. The variance of the stratified estimator is $\\mathrm{Var}(\\widehat{\\mu}) = \\frac{1^2 \\cdot 0^2}{n_0} + \\frac{0^2 \\cdot \\sigma_1^2}{n_1} = 0$ for any non-zero $n_0, n_1$. Since the variance is already zero without any sampling, the optimal allocation that minimizes variance (and cost) is $n_0^\\star = 0$ and $n_1^\\star = 0$. The estimator's value is simply the exact value, $h(0)$.\n\n**5. Analytic Calculation of Exact Values**\n\nFor each test case, the exact value is $\\mu = \\mathbb{E}[h(X)] = p \\cdot h(0) + (1-p) \\mathbb{E}[h(Y)]$, where $Y \\sim g$. The values of $h(0)$ and $\\mathbb{E}[h(Y)]$ are computed as follows:\n-   **Case 1:** $p=\\frac{1}{3}$, $Y \\sim \\text{Exp}(\\lambda=2)$, $h(x)=x + \\mathbf{1}\\{x1\\}$. $h(0)=0$. $\\mathbb{E}[h(Y)] = \\mathbb{E}[Y] + \\mathbb{P}(Y1) = \\frac{1}{\\lambda} + e^{-\\lambda} = \\frac{1}{2} + e^{-2}$.\n    $\\mu = (1-\\frac{1}{3})(\\frac{1}{2} + e^{-2}) = \\frac{1}{3} + \\frac{2}{3}e^{-2}$.\n-   **Case 2:** $p=0.99$, $Y \\sim \\text{Gamma}(k=3, \\theta=1)$, $h(x)=x^2$. $h(0)=0$. $\\mathbb{E}[h(Y)] = \\mathbb{E}[Y^2] = \\theta^2 k(k+1) = 1^2 \\cdot 3(4) = 12$.\n    $\\mu = (1-0.99)(12) = 0.12$.\n-   **Case 3:** $p=0.05$, $Y \\sim \\text{Lognormal}(\\mu_{ln}=0, \\sigma_{ln}=1)$, $h(x)=\\mathbf{1}\\{x2\\}$. $h(0)=0$. $\\mathbb{E}[h(Y)] = \\mathbb{P}(Y2) = 1-\\Phi(\\frac{\\ln(2)-\\mu_{ln}}{\\sigma_{ln}}) = 1-\\Phi(\\ln 2)$, where $\\Phi$ is the standard normal CDF.\n    $\\mu = (1-0.05)(1-\\Phi(\\ln 2))$.\n-   **Case 4:** $p=1$, $h(x)=\\sqrt{x+1}-1$. $h(0)=0$. The distribution is a point mass at $0$.\n    $\\mu = \\mathbb{E}[h(X)] = h(0) = 0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Computes stratified Monte Carlo estimates for a mixed random variable\n    based on optimal allocation derived from first principles.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            'p': 1/3,\n            'g': {'type': 'exponential', 'lambda': 2},\n            'h': lambda x: x + (x  1).astype(float),\n            'B': 1000, 'c0': 1, 'c1': 1, 's': 12345\n        },\n        {\n            'p': 0.99,\n            'g': {'type': 'gamma', 'k': 3, 'theta': 1},\n            'h': lambda x: x**2,\n            'B': 200, 'c0': 1, 'c1': 1, 's': 12345\n        },\n        {\n            'p': 0.05,\n            'g': {'type': 'lognormal', 'mu': 0, 'sigma': 1},\n            'h': lambda x: (x  2).astype(float),\n            'B': 500, 'c0': 1, 'c1': 1, 's': 12345\n        },\n        {\n            'p': 1,\n            'g': {'type': 'exponential', 'lambda': 1},\n            'h': lambda x: np.sqrt(x + 1) - 1,\n            'B': 50, 'c0': 1, 'c1': 1, 's': 12345\n        }\n    ]\n\n    results = []\n    \n    for k, case in enumerate(test_cases):\n        p = case['p']\n        g_params = case['g']\n        h = case['h']\n        B, c0, c1 = case['B'], case['c0'], case['c1']\n        seed = case['s']\n\n        # h(0) is needed for both estimator and exact value.\n        h_at_0 = h(np.array([0.0]))[0]\n        \n        # --- Optimal Allocation and Estimation ---\n        n0_star, n1_star = 0, 0\n        estimator_val = h_at_0  # Default for p=1\n\n        # The optimal allocation is n0=0, n1=floor(B/c1) unless p=1.\n        if p == 1:\n            # If p=1, X=0 deterministically. No sampling needed.\n            n0_star = 0\n            n1_star = 0\n            estimator_val = h_at_0\n        else:\n            # Allocate entire budget to stratum 1 (the continuous part).\n            n0_star = 0\n            n1_star = int(np.floor(B / c1))\n\n            if n1_star  0:\n                rng = np.random.default_rng(seed + k)\n                samples_g = np.array([])\n                \n                if g_params['type'] == 'exponential':\n                    # scale = 1/lambda\n                    samples_g = rng.exponential(scale=1.0/g_params['lambda'], size=n1_star)\n                elif g_params['type'] == 'gamma':\n                    # shape=k, scale=theta\n                    samples_g = rng.gamma(shape=g_params['k'], scale=g_params['theta'], size=n1_star)\n                elif g_params['type'] == 'lognormal':\n                    # mean=mu, sigma=sigma\n                    samples_g = rng.lognormal(mean=g_params['mu'], sigma=g_params['sigma'], size=n1_star)\n                \n                h_samples = h(samples_g)\n                mu1_hat = np.mean(h_samples)\n                estimator_val = p * h_at_0 + (1 - p) * mu1_hat\n            else: # If budget is too small (n1_star = 0)\n                # No samples can be drawn, the estimator cannot be computed with sampling.\n                # In this problem setting, B/c1 = 50, so n1_star  0.\n                # If it were 0, the best one could do is use the known part.\n                estimator_val = p * h_at_0\n\n        # --- Analytic Calculation of Exact Value ---\n        exact_val = 0.0\n        \n        if g_params['type'] == 'exponential':\n            # E[h(Y)] for Y ~ Exp(lambda) and h(x) = x + 1_{x1}\n            # E[Y] + P(Y1) = 1/lambda + exp(-lambda)\n            lam = g_params['lambda']\n            E_hY = 1.0/lam + np.exp(-lam)\n        elif g_params['type'] == 'gamma':\n            # E[Y^2] for Y ~ Gamma(k, theta) = theta^2 * k * (k+1)\n            k_shape = g_params['k']\n            theta_scale = g_params['theta']\n            E_hY = (theta_scale**2) * k_shape * (k_shape + 1)\n        elif g_params['type'] == 'lognormal':\n            # E[1_{Y2}] for Y ~ Lognormal(mu, sigma) = P(Y2)\n            # P(Y2) = 1 - Phi((ln(2)-mu)/sigma)\n            mu_ln = g_params['mu']\n            sigma_ln = g_params['sigma']\n            E_hY = 1.0 - norm.cdf((np.log(2) - mu_ln) / sigma_ln)\n\n        if p == 1:\n            exact_val = h_at_0\n        else:\n            exact_val = p * h_at_0 + (1 - p) * E_hY\n        \n        results.extend([n0_star, n1_star, estimator_val, exact_val])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.7f}' if isinstance(x, float) else str(x) for x in results)}]\")\n\nsolve()\n```", "id": "3333855"}, {"introduction": "An advanced method is only truly valuable if its benefits can be quantified. In this final practice, you will perform a comparative analysis, pitting the stratified estimator against a plain Monte Carlo approach. By calculating the variance ratio under Neyman allocation, you will precisely measure the efficiency gain and see firsthand how stratification is especially powerful when dealing with rare components of a mixed distribution.", "problem": "Let $X$ be a mixed random variable with two mutually exclusive strata: an atomic stratum and a continuous stratum. The atomic stratum is selected with probability $\\theta \\in (0,1)$, in which case $X$ equals one of three atoms $\\{a_1,a_2,a_3\\} = \\{0.1,0.5,0.9\\}$ with conditional probabilities $\\pi = (\\pi_1,\\pi_2,\\pi_3) = (0.5,0.3,0.2)$. The continuous stratum is selected with probability $1-\\theta$, in which case $X$ has the Beta distribution $\\mathrm{Beta}(2,5)$ supported on the interval $[0,1]$. Consider the target event set $A = \\{0.5,1.1\\} \\cup [0.6,0.8]$, which contains isolated points and an interval.\n\nYour task is to construct an estimator for the probability $\\mathbb{P}(X \\in A)$ by stratifying over the atomic set and the continuous complement, and to apply Neyman allocation for an equal-cost sampling budget to minimize the variance of the stratified estimator. Then, compare the stratified estimator to plain Monte Carlo (Monte Carlo (MC)) sampling in regimes where the atomic mass is rare.\n\nWork in the following purely mathematical and algorithmic terms.\n\n1. Define the indicator random variable $Z = \\mathbf{1}\\{X \\in A\\}$. The plain Monte Carlo estimator uses independent and identically distributed samples of $X$ from its full mixed distribution. The stratified estimator partitions the sampling between the atomic stratum and the continuous stratum, with sample counts allocated using Neyman allocation under equal per-sample costs.\n2. For each test case below, compute:\n   - The exact value of $\\mathbb{P}(X \\in A)$ using the mixture structure and the Beta cumulative distribution function.\n   - The theoretical variance of the plain Monte Carlo estimator using independent samples of size $n$.\n   - The theoretical variance of the stratified estimator under Neyman allocation with the same total sample size $n$, where the allocation is between the atomic stratum and the continuous stratum.\n   - The variance ratio defined as the stratified variance divided by the plain Monte Carlo variance.\n   - The integer sample counts assigned to the atomic and continuous strata under Neyman allocation, respecting the total budget.\n\nFundamental base: Use the definitions of mixed random variables, conditional probability, indicator random variables, and variance of sample means under independent sampling. The atomic stratum consists only of point masses at $\\{0.1,0.5,0.9\\}$, and the continuous stratum is absolutely continuous with respect to Lebesgue measure. Under equal per-sample costs, Neyman allocation minimizes the variance of the stratified estimator for a fixed total sample size $n$.\n\nSampling budget and cost: Assume equal per-sample costs across strata and a fixed total sample size of $n = 10000$ samples. All calculations are unitless real numbers; express any numerical answers as real-valued decimals.\n\nTest suite: Evaluate the following four parameter regimes for the atomic mass $\\theta$:\n- Case $1$: $\\theta = 0.0005$.\n- Case $2$: $\\theta = 0.01$.\n- Case $3$: $\\theta = 0.2$.\n- Case $4$: $\\theta = 0.5$.\n\nAnswer specification: For each test case, produce a list containing six entries in the following order:\n- The exact probability $\\mathbb{P}(X \\in A)$ as a float.\n- The plain Monte Carlo variance as a float.\n- The stratified variance under Neyman allocation as a float.\n- The variance ratio (stratified variance divided by plain Monte Carlo variance) as a float.\n- The allocated integer sample count for the atomic stratum.\n- The allocated integer sample count for the continuous stratum.\n\nFinal output format: Your program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets, where each element is the list for one test case in the order specified above (for example, $[\\text{case1},\\text{case2},\\text{case3},\\text{case4}]$). No extra text should be printed.", "solution": "The problem requires a comparative analysis of plain Monte Carlo (MC) sampling and stratified sampling for estimating the probability of an event for a mixed random variable. We will first establish the mathematical framework for the problem, including the exact probability, and then derive the variances for both the plain MC and stratified estimators.\n\nLet $X$ be the mixed random variable. The sample space is partitioned into two strata: an atomic stratum, denoted by an indicator $S=1$, and a continuous stratum, denoted by $S=2$. The probabilities of selecting these strata are given by $\\mathbb{P}(S=1) = \\theta$ and $\\mathbb{P}(S=2) = 1-\\theta$.\n\nIf $S=1$ (atomic stratum), $X$ takes one of the values from the set $\\{a_1, a_2, a_3\\} = \\{0.1, 0.5, 0.9\\}$ with conditional probabilities $\\pi = (\\pi_1, \\pi_2, \\pi_3) = (0.5, 0.3, 0.2)$.\n\nIf $S=2$ (continuous stratum), $X$ follows a Beta distribution, $X \\sim \\mathrm{Beta}(\\alpha, \\beta)$ with parameters $\\alpha=2$ and $\\beta=5$, over the support interval $[0,1]$.\n\nThe target event set is $A = \\{0.5, 1.1\\} \\cup [0.6, 0.8]$. We are interested in estimating $p = \\mathbb{P}(X \\in A)$. The quantity to be estimated is the expectation of the indicator random variable $Z = \\mathbf{1}\\{X \\in A\\}$, i.e., $p = \\mathbb{E}[Z]$.\n\n**1. Exact Probability $\\mathbb{P}(X \\in A)$**\n\nBy the law of total probability, the exact probability $p$ is given by the mixture:\n$$p = \\mathbb{P}(X \\in A | S=1)\\mathbb{P}(S=1) + \\mathbb{P}(X \\in A | S=2)\\mathbb{P}(S=2)$$\nLet $p_1 = \\mathbb{P}(X \\in A | S=1)$ and $p_2 = \\mathbb{P}(X \\in A | S=2)$. The stratum weights are $W_1 = \\mathbb{P}(S=1) = \\theta$ and $W_2 = \\mathbb{P}(S=2) = 1-\\theta$. The probability is then $p = W_1 p_1 + W_2 p_2$.\n\nFirst, we calculate the within-stratum probability $p_1$. In the atomic stratum, $X$ can be $0.1$, $0.5$, or $0.9$. We check which of these values lie in the set $A = \\{0.5, 1.1\\} \\cup [0.6, 0.8]$.\n- $X=0.1$: $0.1 \\notin A$.\n- $X=0.5$: $0.5 \\in A$.\n- $X=0.9$: $0.9 \\notin A$.\nTherefore, the event $X \\in A$ occurs only if $X=0.5$. The conditional probability of this is $\\pi_2$.\n$$p_1 = \\mathbb{P}(X=0.5 | S=1) = \\pi_2 = 0.3$$\n\nNext, we calculate the within-stratum probability $p_2$. In the continuous stratum, $X \\sim \\mathrm{Beta}(2,5)$. For a continuous random variable, the probability of taking any single value is $0$. Thus, $\\mathbb{P}(X \\in \\{0.5, 1.1\\} | S=2) = 0$. The probability $p_2$ is determined solely by the interval part of $A$.\n$$p_2 = \\mathbb{P}(X \\in [0.6, 0.8] | S=2)$$\nLet $F_{\\beta}(x; \\alpha, \\beta)$ be the cumulative distribution function (CDF) of the Beta distribution. Then:\n$$p_2 = F_{\\beta}(0.8; 2, 5) - F_{\\beta}(0.6; 2, 5)$$\nUsing computational evaluation, $p_2 \\approx 0.02677028$.\n\nThe total probability is $p = \\theta(0.3) + (1-\\theta)p_2$. This value depends on the test case parameter $\\theta$.\n\n**2. Plain Monte Carlo Estimator Variance**\n\nThe plain MC estimator for $p$ is $\\hat{p}_{MC} = \\frac{1}{n} \\sum_{i=1}^n Z_i$, where $Z_i$ are independent and identically distributed samples of $Z = \\mathbf{1}\\{X \\in A\\}$. Since $Z$ is a Bernoulli random variable with parameter $p$, its variance is $\\mathrm{Var}(Z) = p(1-p)$. The variance of the estimator is:\n$$\\mathrm{Var}(\\hat{p}_{MC}) = \\frac{\\mathrm{Var}(Z)}{n} = \\frac{p(1-p)}{n}$$\nThis variance will be calculated for each value of $\\theta$ using the corresponding value of $p$. The total sample size is given as $n=10000$.\n\n**3. Stratified Estimator Variance with Neyman Allocation**\n\nThe stratified estimator for $p$ is $\\hat{p}_{strat} = W_1 \\hat{p}_1 + W_2 \\hat{p}_2$, where $\\hat{p}_j$ is the sample mean of the indicator variable within stratum $j$, based on $n_j$ samples drawn from that stratum. The variance of this estimator is:\n$$\\mathrm{Var}(\\hat{p}_{strat}) = \\frac{W_1^2 \\sigma_1^2}{n_1} + \\frac{W_2^2 \\sigma_2^2}{n_2}$$\nwhere $n_1+n_2=n$ and $\\sigma_j^2$ is the variance of the indicator variable within stratum $j$.\n$$\\sigma_1^2 = \\mathrm{Var}(Z|S=1) = p_1(1-p_1) = 0.3(1-0.3) = 0.21$$\n$$\\sigma_2^2 = \\mathrm{Var}(Z|S=2) = p_2(1-p_2) \\approx 0.02677028 \\times (1 - 0.02677028) \\approx 0.02605301$$\n\nNeyman allocation minimizes $\\mathrm{Var}(\\hat{p}_{strat})$ for a fixed total sample size $n$ and equal per-sample costs. The sample size $n_j$ for stratum $j$ is allocated proportionally to $W_j \\sigma_j$:\n$$n_j = n \\frac{W_j \\sigma_j}{\\sum_{k=1}^2 W_k \\sigma_k}$$\nwhere $\\sigma_j = \\sqrt{p_j(1-p_j)}$. For our two strata:\n$$n_1 = n \\frac{W_1 \\sigma_1}{W_1 \\sigma_1 + W_2 \\sigma_2} \\quad \\text{and} \\quad n_2 = n \\frac{W_2 \\sigma_2}{W_1 \\sigma_1 + W_2 \\sigma_2}$$\nSince the sample counts must be integers, we calculate the ideal, real-valued allocation $n_1^*$ and round it to the nearest integer. The remaining samples are assigned to the other stratum to maintain the total budget $n$.\n$$n_1 = \\text{round}(n_1^*) \\quad \\text{and} \\quad n_2 = n - n_1$$\nWith these integer allocations, the variance of the stratified estimator is calculated using the general variance formula:\n$$\\mathrm{Var}(\\hat{p}_{strat}) = \\frac{W_1^2 \\sigma_1^2}{n_1} + \\frac{W_2^2 \\sigma_2^2}{n_2}$$\n\n**4. Calculation Summary and Variance Ratio**\n\nFor each test case value of $\\theta$, we perform the following calculations:\n1.  Calculate stratum weights $W_1 = \\theta$ and $W_2 = 1-\\theta$.\n2.  Compute the exact probability $p = W_1 p_1 + W_2 p_2$.\n3.  Compute the plain MC variance $\\mathrm{Var}(\\hat{p}_{MC}) = p(1-p)/n$.\n4.  Calculate the Neyman allocation proportions based on $W_1, \\sigma_1, W_2, \\sigma_2$.\n5.  Determine the integer sample counts $n_1$ and $n_2$ for a total budget of $n=10000$.\n6.  Compute the resulting stratified variance $\\mathrm{Var}(\\hat{p}_{strat})$.\n7.  Compute the variance ratio $\\mathrm{Var}(\\hat{p}_{strat}) / \\mathrm{Var}(\\hat{p}_{MC})$.\n\nThe required constants are:\n- $n = 10000$\n- $p_1 = 0.3$\n- $\\sigma_1 = \\sqrt{0.3 \\times 0.7} = \\sqrt{0.21} \\approx 0.45825757$\n- $p_2 = F_{\\beta}(0.8; 2, 5) - F_{\\beta}(0.6; 2, 5) \\approx 0.02677028$\n- $\\sigma_2 = \\sqrt{p_2(1-p_2)} \\approx 0.16140944$\n\nThese steps are implemented for each value of $\\theta$ in the provided test suite. The results demonstrate how Neyman allocation adapts the sampling effort. When $\\theta$ is small, the atomic stratum is rare but has high variance ($\\sigma_1  \\sigma_2$), so it is sampled more heavily than its weight $W_1$ would suggest. As $\\theta$ increases, the allocation increasingly reflects the stratum weights. In all valid cases, stratified sampling with Neyman allocation is expected to yield a variance less than or equal to that of plain Monte Carlo, resulting in a variance ratio less than or equal to $1$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import beta\n\ndef solve():\n    \"\"\"\n    Calculates exact probability, MC variance, stratified variance, variance ratio,\n    and sample allocations for a mixed random variable problem under four parameter regimes.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        0.0005, # Case 1\n        0.01,   # Case 2\n        0.2,    # Case 3\n        0.5,    # Case 4\n    ]\n\n    # --- Problem Constants ---\n    # Total sample size\n    n = 10000\n\n    # Atomic stratum properties\n    # Atoms: {0.1, 0.5, 0.9} with probs (0.5, 0.3, 0.2)\n    # Target set A = {0.5, 1.1} U [0.6, 0.8]\n    # Only atom 0.5 is in A.\n    p1 = 0.3  # P(X in A | atomic) = P(X=0.5)\n    sigma1_sq = p1 * (1 - p1)\n    sigma1 = np.sqrt(sigma1_sq)\n\n    # Continuous stratum properties\n    # X ~ Beta(2, 5) on [0, 1]\n    # Target set A = {0.5, 1.1} U [0.6, 0.8]\n    # For a continuous RV, P(X=x) = 0.\n    alpha, beta_param = 2, 5\n    p2 = beta.cdf(0.8, alpha, beta_param) - beta.cdf(0.6, alpha, beta_param) # P(X in [0.6, 0.8])\n    sigma2_sq = p2 * (1 - p2)\n    sigma2 = np.sqrt(sigma2_sq)\n    \n    results = []\n    for theta in test_cases:\n        # --- Calculations for the current test case (theta) ---\n\n        # Stratum weights\n        w1 = theta\n        w2 = 1 - theta\n\n        # 1. Exact probability P(X in A)\n        p_exact = w1 * p1 + w2 * p2\n\n        # 2. Plain Monte Carlo variance\n        var_mc = (p_exact * (1 - p_exact)) / n\n\n        # 3. Neyman allocation\n        # Denominator for allocation formula\n        alloc_denom = w1 * sigma1 + w2 * sigma2\n        \n        # Ideal (real-valued) allocation for stratum 1\n        n1_ideal = n * (w1 * sigma1) / alloc_denom\n        \n        # 5. Integer sample counts\n        # Round n1 and derive n2 to respect the total budget\n        n1_alloc = int(np.round(n1_ideal))\n        n2_alloc = n - n1_alloc\n\n        # Handle edge cases where rounding might lead to a zero allocation, although\n        # not expected for the given parameters.\n        if n1_alloc = 0: n1_alloc = 1; n2_alloc = n - 1\n        if n2_alloc = 0: n2_alloc = 1; n1_alloc = n - 1\n\n        # 4. Stratified variance under Neyman allocation\n        # Calculated using the actual integer allocations\n        var_strat = (w1**2 * sigma1_sq / n1_alloc) + (w2**2 * sigma2_sq / n2_alloc)\n        \n        # 5. Variance ratio\n        variance_ratio = var_strat / var_mc\n        \n        # Compile results for the case\n        case_results = [\n            p_exact,\n            var_mc,\n            var_strat,\n            variance_ratio,\n            n1_alloc,\n            n2_alloc,\n        ]\n        results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # The format [list1, list2, ...] is produced by str() on a list of lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3333778"}]}