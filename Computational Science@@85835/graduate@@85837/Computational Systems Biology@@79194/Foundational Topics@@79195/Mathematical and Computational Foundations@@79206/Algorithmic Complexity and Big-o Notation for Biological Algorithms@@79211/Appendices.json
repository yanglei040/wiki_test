{"hands_on_practices": [{"introduction": "In modern bioinformatics, we often face the challenge of processing massive datasets that are too large to fit into memory, such as streams of sequencing reads. This exercise introduces the paradigm of streaming algorithms, which process data on-the-fly using minimal space. You will explore reservoir sampling, a classic technique for this scenario, and analyze its performance not just in terms of computational resources but also its statistical accuracy as an estimator [@problem_id:3288334].", "problem": "Design and analyze a streaming algorithm for estimating the proportion of a target $k$-mer among single-cell ribonucleic acid sequencing (scRNA-seq) reads using constant memory. Each read is processed online, and a read either contains the target $k$-mer or it does not. Model the stream as a sequence of independent indicators $\\{X_t\\}_{t=1}^n$, where $X_t \\in \\{0,1\\}$ equals $1$ if the $t$-th read contains the target $k$-mer, and $0$ otherwise. Let the true but unknown proportion be $p = \\frac{1}{n}\\sum_{t=1}^n X_t$. Your algorithm must use a uniform reservoir sample of fixed size $m$ that does not depend on $n$ (so the total memory is $O(1)$ in Big-O notation), and produce an estimator $\\hat{p}$ of $p$ after seeing the first $n$ reads.\n\nConstraints and goals:\n- You must use a reservoir of size $m$ maintained via classical reservoir sampling, where for each incoming item at time $t$ with $t \\ge 1$, the algorithm ensures that, after processing $t$ items, the reservoir contains a uniformly random subset of size $\\min\\{m,t\\}$ of the items seen so far.\n- The estimator must be the sample mean of the indicators in the reservoir, $\\hat{p} = \\frac{1}{m}\\sum_{i=1}^{m} Y_i$, where $Y_i \\in \\{0,1\\}$ denotes whether the $i$-th reservoir entry contains the target $k$-mer at the terminal time $n$ (if $n \\ge m$; otherwise define the mean over the actual reservoir size).\n- Prove that $\\mathbb{E}[\\hat{p}] = p$ and derive the expected squared error $\\mathbb{E}\\big[(\\hat{p}-p)^2\\big]$ as a function of $n$, $m$, and $p$. Express the final formula in simplest terms using $n$, $m$, and $p$ only. State any conditions on $n$ and $m$ for which your expression is valid.\n- Analyze the algorithmic complexity using Big-O notation for both time and space, and justify why the space is $O(1)$ with respect to $n$ given fixed $m$.\n\nImplementation task:\n- Implement a program that, given a test suite of $(n,m,p)$ tuples, computes the theoretical expected squared error $\\mathbb{E}\\big[(\\hat{p}-p)^2\\big]$ under the reservoir sampling model described above. Assume $n \\ge 2$ and $1 \\le m \\le n$ unless explicitly specified, and interpret $p$ as a decimal in $[0,1]$ (not a percentage). Your program should not simulate; it should compute the closed-form expression you derived.\n\nTest suite:\n- Use exactly the following parameter tuples $(n,m,p)$:\n  - $(n=\\;1000000,\\; m=\\;5,\\; p=\\;0.3)$\n  - $(n=\\;1000,\\; m=\\;1,\\; p=\\;0.2)$\n  - $(n=\\;50,\\; m=\\;5,\\; p=\\;0.5)$\n  - $(n=\\;10,\\; m=\\;3,\\; p=\\;0)$\n  - $(n=\\;10,\\; m=\\;3,\\; p=\\;1)$\n  - $(n=\\;7,\\; m=\\;7,\\; p=\\;0.35)$\n  - $(n=\\;2,\\; m=\\;1,\\; p=\\;0.4)$\n\nAnswer specification:\n- For each test case, output the expected squared error as a floating-point number rounded to exactly $10$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $\\texttt{[0.1234000000,0.0000000000]}$.", "solution": "The problem as stated is formally sound and tractable. It is scientifically grounded in probability theory and algorithm analysis, specifically the domain of streaming algorithms and finite population sampling. The problem is well-posed, with all necessary information provided and unambiguous objectives. There are no contradictions, factual errors, or subjective elements. Therefore, a formal solution can be constructed.\n\nThe core of the problem is to analyze the statistical properties of an estimator derived from reservoir sampling. We model the input stream as a finite population of $n$ indicators, $\\{X_t\\}_{t=1}^n$, where $X_t \\in \\{0, 1\\}$. The population proportion of successes (reads containing the target $k$-mer) is $p = \\frac{1}{n}\\sum_{t=1}^n X_t$. The reservoir sampling algorithm draws a simple random sample of size $m$ without replacement from this population, provided $n \\ge m$. Let this sample be denoted by $\\{Y_i\\}_{i=1}^m$. If $n  m$, the reservoir simply contains all $n$ items from the stream.\n\nThe estimator for $p$ is the sample mean of the elements in the final reservoir.\nIf $n \\ge m$, the reservoir has size $m$, and the estimator is $\\hat{p} = \\frac{1}{m}\\sum_{i=1}^m Y_i$.\nIf $n  m$, the reservoir has size $n$, and the estimator is $\\hat{p} = \\frac{1}{n}\\sum_{i=1}^n Y_i$. In this case, the reservoir contains all stream elements, so $Y_i = X_i$ for all $i \\in \\{1, \\dots, n\\}$. This immediately gives $\\hat{p} = \\frac{1}{n}\\sum_{i=1}^n X_i = p$. The estimation is exact, so the error $\\hat{p}-p$ is $0$, and the expected squared error is $\\mathbb{E}[(\\hat{p}-p)^2] = 0$.\n\nThe remainder of the analysis focuses on the case $n \\ge m$, which covers all provided test cases (including the boundary case $n=m$).\n\n**1. Proof of Unbiasedness: $\\mathbb{E}[\\hat{p}] = p$**\n\nThe estimator is $\\hat{p} = \\frac{1}{m}\\sum_{i=1}^m Y_i$. By linearity of expectation, its expected value is:\n$$ \\mathbb{E}[\\hat{p}] = \\mathbb{E}\\left[\\frac{1}{m}\\sum_{i=1}^m Y_i\\right] = \\frac{1}{m}\\sum_{i=1}^m \\mathbb{E}[Y_i] $$\nThe set $\\{Y_i\\}_{i=1}^m$ is a simple random sample drawn without replacement from the finite population $\\{X_t\\}_{t=1}^n$. Due to the symmetry of this sampling process, each element $Y_i$ in the sample is an identically distributed random variable. Specifically, any $Y_i$ has the same distribution as a single element drawn uniformly from the population.\nThe expected value of any such element $Y_i$ is therefore equal to the population mean, $p$:\n$$ \\mathbb{E}[Y_i] = \\frac{1}{n}\\sum_{t=1}^n X_t = p $$\nSubstituting this back into the expression for $\\mathbb{E}[\\hat{p}]$:\n$$ \\mathbb{E}[\\hat{p}] = \\frac{1}{m}\\sum_{i=1}^m p = \\frac{1}{m}(m \\cdot p) = p $$\nThus, the estimator $\\hat{p}$ is an unbiased estimator of the true proportion $p$.\n\n**2. Derivation of the Expected Squared Error: $\\mathbb{E}[(\\hat{p}-p)^2]$**\n\nSince the estimator is unbiased, the expected squared error is equal to its variance:\n$$ \\mathbb{E}[(\\hat{p}-p)^2] = \\text{Var}(\\hat{p}) $$\nThe variance of the sample mean is:\n$$ \\text{Var}(\\hat{p}) = \\text{Var}\\left(\\frac{1}{m}\\sum_{i=1}^m Y_i\\right) = \\frac{1}{m^2} \\text{Var}\\left(\\sum_{i=1}^m Y_i\\right) $$\nThe variance of the sum can be expanded into variance and covariance terms:\n$$ \\text{Var}\\left(\\sum_{i=1}^m Y_i\\right) = \\sum_{i=1}^m \\text{Var}(Y_i) + \\sum_{i\\neq j} \\text{Cov}(Y_i, Y_j) $$\nSince the $Y_i$ are identically distributed, all variance terms are equal, and all covariance terms for distinct pairs $(i,j)$ are equal. Thus:\n$$ \\text{Var}\\left(\\sum_{i=1}^m Y_i\\right) = m \\cdot \\text{Var}(Y_1) + m(m-1) \\cdot \\text{Cov}(Y_1, Y_2) $$\nWe must now find $\\text{Var}(Y_1)$ and $\\text{Cov}(Y_1, Y_2)$.\n\nFirst, the variance of a single draw $Y_1$. Since $Y_1 \\in \\{0, 1\\}$, we have $Y_1^2 = Y_1$.\n$$ \\text{Var}(Y_1) = \\mathbb{E}[Y_1^2] - (\\mathbb{E}[Y_1])^2 = \\mathbb{E}[Y_1] - p^2 = p - p^2 = p(1-p) $$\nThis quantity is the population variance of the original set $\\{X_t\\}_{t=1}^n$, which we can denote as $\\sigma_X^2 = p(1-p)$.\n\nNext, the covariance between two distinct draws, $Y_1$ and $Y_2$.\n$$ \\text{Cov}(Y_1, Y_2) = \\mathbb{E}[Y_1 Y_2] - \\mathbb{E}[Y_1]\\mathbb{E}[Y_2] = \\mathbb{E}[Y_1 Y_2] - p^2 $$\nThe term $\\mathbb{E}[Y_1 Y_2]$ is the probability that both $Y_1$ and $Y_2$ are equal to $1$. This corresponds to drawing two $1$s without replacement from the population. The population contains $np$ ones and $n(1-p)$ zeros.\n$$ \\mathbb{E}[Y_1 Y_2] = P(Y_1=1 \\text{ and } Y_2=1) = P(Y_1=1) \\cdot P(Y_2=1 | Y_1=1) $$\nThe probability of the first draw being a $1$ is $P(Y_1=1) = \\frac{np}{n} = p$. Given the first draw was a $1$, there are $np-1$ ones left in a population of size $n-1$. So, $P(Y_2=1 | Y_1=1) = \\frac{np-1}{n-1}$.\nTherefore:\n$$ \\mathbb{E}[Y_1 Y_2] = p \\cdot \\frac{np-1}{n-1} $$\nNow we can compute the covariance:\n$$ \\text{Cov}(Y_1, Y_2) = p \\frac{np-1}{n-1} - p^2 = \\frac{p(np-1) - p^2(n-1)}{n-1} = \\frac{np^2 - p - np^2 + p^2}{n-1} = \\frac{p^2-p}{n-1} = -\\frac{p(1-p)}{n-1} $$\nSubstituting the variance and covariance back into the expression for $\\text{Var}(\\hat{p})$:\n$$ \\text{Var}(\\hat{p}) = \\frac{1}{m^2} \\left[ m \\cdot p(1-p) + m(m-1) \\cdot \\left(-\\frac{p(1-p)}{n-1}\\right) \\right] $$\n$$ \\text{Var}(\\hat{p}) = \\frac{p(1-p)}{m} \\left[ 1 - \\frac{m-1}{n-1} \\right] $$\n$$ \\text{Var}(\\hat{p}) = \\frac{p(1-p)}{m} \\left[ \\frac{(n-1) - (m-1)}{n-1} \\right] $$\n$$ \\text{Var}(\\hat{p}) = \\frac{p(1-p)}{m} \\left( \\frac{n-m}{n-1} \\right) $$\nThis expression for the expected squared error is valid for $n \\ge m \\ge 1$ and $n \\ge 2$. The condition $n \\ge 2$ is necessary to ensure the denominator $n-1$ is non-zero. The problem statement guarantees $n \\ge 2$ and $1 \\le m \\le n$. Note that if $n=m$, the term $(n-m)$ becomes $0$, correctly yielding an error of $0$.\n\n**3. Algorithmic Complexity Analysis**\n\nThe algorithm under consideration is classical reservoir sampling (e.g., Algorithm R by Vitter).\n- **Time Complexity**: The algorithm processes each of the $n$ items in the stream once. For each item at index $t$ (for $t  m$), it performs a constant number of operations: generating one random number, one comparison, and possibly one array replacement. Thus, processing each item takes $O(1)$ time. The total time complexity for a stream of length $n$ is $O(n)$.\n- **Space Complexity**: The algorithm's memory usage is dominated by the storage for the reservoir, which has a fixed size $m$. The problem specifies that $m$ is a constant that does not depend on the stream length $n$. Additionally, a counter for the number of items seen is required, which uses $O(\\log n)$ space, but in the context of streaming complexity this is typically considered part of the $O(1)$ space budget if it fits in a machine word. Since $m$ is constant with respect to $n$, the space complexity is $O(m) = O(1)$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the theoretical expected squared error for an estimator based on reservoir sampling.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1000000, 5, 0.3),\n        (1000, 1, 0.2),\n        (50, 5, 0.5),\n        (10, 3, 0),\n        (10, 3, 1),\n        (7, 7, 0.35),\n        (2, 1, 0.4),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, m, p = case\n        \n        # The derived formula for the expected squared error (which is Var(p_hat)) is:\n        # E[(p_hat - p)^2] = (p * (1 - p) / m) * ((n - m) / (n - 1))\n        # This formula is valid for n = 2 and 1 = m = n, which all test cases satisfy.\n\n        # Handle the edge case where the denominator n-1 would be zero.\n        # The problem states n = 2, so this is not strictly needed for the test suite\n        # but is good practice for a general function.\n        if n = 1:\n            # If n=1, then m=1, the sample is the whole population, so error is 0.\n            # If n=0, the problem is ill-defined.\n            error = 0.0\n        # If p=0 or p=1, the population is homogeneous, so error is 0.\n        # If n=m, the sample is the whole population, so error is 0.\n        # The formula correctly handles these cases, as (p*(1-p)) or (n-m) will be 0.\n        else:\n            # Ensure floating-point division\n            n_float = float(n)\n            m_float = float(m)\n            p_float = float(p)\n\n            # Calculation using the derived formula.\n            # Factor 1: Variance of a single draw scaled by sample size\n            term1 = (p_float * (1.0 - p_float)) / m_float\n            # Factor 2: Finite population correction factor\n            term2 = (n_float - m_float) / (n_float - 1.0)\n            \n            error = term1 * term2\n\n        # Format the result to exactly 10 decimal places.\n        results.append(f\"{error:.10f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\n# Execute the solver function\nsolve()\n```", "id": "3288334"}, {"introduction": "Dynamic programming algorithms like the Viterbi algorithm for Hidden Markov Models (HMMs) are fundamental to sequence analysis, offering guaranteed optimal solutions for problems like gene annotation. However, their computational cost can be prohibitive for large state spaces. This practice explores a common and essential trade-off in algorithm design: sacrificing guaranteed optimality for significant gains in speed by using a heuristic like beam search. Through this problem, you will learn to formally analyze this trade-off and make principled decisions about algorithmic parameters to balance accuracy and performance [@problem_id:3288378].", "problem": "Consider a Hidden Markov Model (HMM) for gene annotation with $S$ hidden states and a genomic observation sequence of length $T$. An HMM is defined by an initial distribution over states, a transition probability matrix, and emission probability distributions. The Viterbi decoding computes the most probable hidden state sequence given the observations by dynamic programming. In a fully connected HMM, at each time step $t \\in \\{1,\\dots,T\\}$ the Viterbi recurrence for each destination state considers all $S$ possible predecessor states, leading to an asymptotic cost of $O(T S^2)$. Beam search is a heuristic that prunes the state space at each time step by retaining only the top $b$ states ranked by partial path scores; transitions are evaluated only from this beam to all $S$ destination states, resulting in a cost of $O(T b S)$, which reduces to $O(T S)$ when $b$ is bounded independently of $S$.\n\nTo preserve decoding accuracy, the pruned beam must contain the predecessor state on the optimal path at each step. Formalize accuracy using a probabilistic rank model grounded in order-statistics: let $R_t$ be the random rank (among $S$ states, with rank $1$ being best) of the predecessor state that leads to the globally optimal Viterbi path at time $t$. Assume the ranks $R_t$ are independent and identically distributed (i.i.d.) across $t$ and satisfy a geometric tail bound parameterized by $p \\in (0,1)$, namely $\\mathbb{P}(R_t  r) = (1-p)^r$ for integers $r \\ge 0$, truncated at $S$. This tail bound is consistent with widely used score gap models in HMM decoding where the probability that a given competitor outperforms the optimal predecessor decays exponentially with rank. Under this model, the probability that the optimal predecessor is within the top $b$ at a single step is $\\mathbb{P}(R_t \\le b) = 1 - (1-p)^b$. Assuming independence across $T$ steps, the probability that the optimal path survives pruning over all $T$ steps is $\\left(1 - (1-p)^b\\right)^T$.\n\nYour tasks are:\n- Derive from first principles and definitions the asymptotic costs $O(T S^2)$ for the Viterbi algorithm and $O(T b S)$ for beam search, starting from the dynamic programming recurrence.\n- Using the probabilistic rank model above, derive the minimal integer beam width $b(T,S,p,\\varepsilon)$ that guarantees $\\left(1 - (1-p)^b\\right)^T \\ge 1 - \\varepsilon$ for a specified accuracy tolerance $\\varepsilon \\in (0,1)$ (expressed as a decimal, not a percentage), subject to the constraint $b \\le S$. Explicitly state how to choose $b$ when the unconstrained minimal integer exceeds $S$.\n- Implement a program that, for the following test suite of parameter values $(T,S,p,\\varepsilon)$, computes $b$ for each case and returns for each case a pair consisting of the chosen $b$ and a boolean indicating whether both accuracy is guaranteed (i.e., $\\left(1 - (1-p)^b\\right)^T \\ge 1 - \\varepsilon$) and asymptotic cost is reduced relative to $O(T S^2)$ (i.e., $b  S$ so that $O(T b S) = o(T S^2)$):\n    1. $T = 1000$, $S = 50$, $p = 0.3$, $\\varepsilon = 0.01$.\n    2. $T = 1$, $S = 1000$, $p = 0.3$, $\\varepsilon = 0.01$.\n    3. $T = 10000$, $S = 10000$, $p = 0.1$, $\\varepsilon = 0.1$.\n    4. $T = 500$, $S = 200$, $p = 0.25$, $\\varepsilon = 0.000001$.\n    5. $T = 1000$, $S = 500$, $p = 0.01$, $\\varepsilon = 0.01$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a two-element list of the form $[b,\\text{boolean}]$. For example, a valid output format is $[[b_1,\\text{boolean}_1],[b_2,\\text{boolean}_2],\\dots]$. No physical units apply to this problem. Angles are not used. All probabilities such as $\\varepsilon$ must be provided and interpreted as decimals.", "solution": "The problem is assessed to be valid as it is scientifically grounded in established principles of algorithm analysis and probabilistic modeling, is well-posed, objective, and contains all necessary information for a unique solution.\n\n### Part 1: Derivation of Asymptotic Costs\n\n#### Viterbi Algorithm Complexity: $O(T S^2)$\n\nThe Viterbi algorithm finds the most probable sequence of hidden states given a sequence of observations. It is a dynamic programming algorithm. Let $S$ be the number of hidden states and $T$ be the length of the observation sequence. Let $\\delta_t(j)$ be the probability of the most likely path of length $t$ ending in state $j$. The core of the algorithm is the recurrence relation:\n$$ \\delta_t(j) = \\left( \\max_{i=1,\\dots,S} \\left[ \\delta_{t-1}(i) \\cdot a_{ij} \\right] \\right) \\cdot e_j(o_t) \\quad \\text{for } j=1,\\dots,S \\text{ and } t=1,\\dots,T $$\nwhere $a_{ij}$ is the transition probability from state $i$ to state $j$, and $e_j(o_t)$ is the emission probability of observation $o_t$ from state $j$.\n\nTo analyze the computational cost, we count the number of elementary arithmetic operations. For numerical stability, computations are typically performed in log-space, where multiplications become additions. The recurrence becomes:\n$$ \\log \\delta_t(j) = \\left( \\max_{i=1,\\dots,S} \\left[ \\log \\delta_{t-1}(i) + \\log a_{ij} \\right] \\right) + \\log e_j(o_t) $$\n\nLet's analyze the cost for a single time step $t$:\n1.  To compute the value for a single destination state $j$, we must evaluate the expression inside the $\\max$ operator for each possible predecessor state $i \\in \\{1, \\dots, S\\}$.\n2.  For each $i$, this involves one addition: $\\log \\delta_{t-1}(i) + \\log a_{ij}$. This is performed $S$ times for a fixed $j$.\n3.  Then, we must find the maximum of these $S$ values. This requires $S-1$ comparisons.\n4.  Finally, one more addition is performed to add the log-emission probability.\n5.  Thus, for each destination state $j$, the number of operations is proportional to $S$ (for the additions) plus $S-1$ (for the comparisons), which is $O(S)$.\n\nSince this computation must be performed for all $S$ destination states ($j=1, \\dots, S$), the total cost for a single time step $t$ is $S \\times O(S) = O(S^2)$.\n\nThis process is repeated for each time step from $t=1$ to $t=T$. The total asymptotic cost is the cost per time step multiplied by the number of time steps.\n$$ \\text{Total Cost} = T \\times O(S^2) = O(T S^2) $$\n\n#### Beam Search Algorithm Complexity: $O(T b S)$\n\nBeam search is a heuristic that reduces complexity by pruning the search space. At each time step, it only keeps a \"beam\" of the $b$ most promising states, where $b$ is the beam width.\n\nLet $\\mathcal{B}_{t-1}$ be the set of $b$ states with the highest scores $\\delta_{t-1}(i)$ at time $t-1$. The recurrence relation is modified to consider transitions only from states within this beam:\n$$ \\log \\delta_t(j) = \\left( \\max_{i \\in \\mathcal{B}_{t-1}} \\left[ \\log \\delta_{t-1}(i) + \\log a_{ij} \\right] \\right) + \\log e_j(o_t) $$\n\nLet's analyze the cost for a single time step $t$:\n1.  To compute the score for a single destination state $j$, we now iterate only over the $b$ states in the beam $\\mathcal{B}_{t-1}$.\n2.  This involves $b$ additions and finding the maximum of $b$ values (which takes $b-1$ comparisons). The cost per destination state $j$ is therefore $O(b)$.\n3.  This calculation is performed for all $S$ possible destination states, so the cost of computing all candidate scores for time $t$ is $S \\times O(b) = O(bS)$.\n4.  After computing the $S$ new scores, we must identify the top $b$ states to form the next beam, $\\mathcal{B}_t$. This can be done efficiently in $O(S)$ time using a linear-time selection algorithm (e.g., Quickselect) to find the $b$-th largest score and then filtering.\n5.  The total cost for one time step is the sum of these two steps: $O(bS) + O(S)$. Since $b \\ge 1$, this simplifies to $O(bS)$.\n\nThis process is repeated for all $T$ time steps. The total asymptotic cost is:\n$$ \\text{Total Cost} = T \\times O(bS) = O(T b S) $$\nWhen $b  S$, $O(T b S)$ is an asymptotic improvement over $O(T S^2)$. Formally, if $b  S$, then $T b S = o(T S^2)$.\n\n### Part 2: Derivation of Minimal Beam Width $b$\n\nWe are given the requirement that the probability of the optimal path surviving pruning over all $T$ steps must be at least $1-\\varepsilon$:\n$$ \\left(1 - (1-p)^b\\right)^T \\ge 1 - \\varepsilon $$\nwhere $p \\in (0,1)$ is the parameter of the rank model, $b$ is the beam width, and $\\varepsilon \\in (0,1)$ is the accuracy tolerance. We need to find the minimal integer $b$ that satisfies this inequality, subject to $b \\le S$.\n\n1.  Take the $T$-th root of both sides. Since both sides are positive, the inequality direction is preserved.\n    $$ 1 - (1-p)^b \\ge (1 - \\varepsilon)^{1/T} $$\n2.  Rearrange the terms to isolate the term with $b$:\n    $$ (1-p)^b \\le 1 - (1 - \\varepsilon)^{1/T} $$\n3.  Take the logarithm of both sides. Since $p \\in (0,1)$, we have $0  1-p  1$, which means $\\log(1-p)$ is negative. Therefore, when we apply the logarithm and later divide by $\\log(1-p)$, we must reverse the inequality sign.\n    $$ b \\log(1-p) \\le \\log\\left(1 - (1 - \\varepsilon)^{1/T}\\right) $$\n4.  Divide by $\\log(1-p)$ and reverse the inequality:\n    $$ b \\ge \\frac{\\log\\left(1 - (1 - \\varepsilon)^{1/T}\\right)}{\\log(1-p)} $$\n    The base of the logarithm can be any value greater than $1$; the natural logarithm is standard.\n\nSince $b$ must be an integer, the minimal unconstrained integer beam width, let's call it $b_{req}$, is the smallest integer satisfying this condition, which is given by the ceiling function:\n$$ b_{req} = \\left\\lceil \\frac{\\ln\\left(1 - (1 - \\varepsilon)^{1/T}\\right)}{\\ln(1-p)} \\right\\rceil $$\n\nThe problem imposes the physical constraint that the beam width cannot exceed the total number of states, i.e., $b \\le S$. Therefore, the chosen beam width, $b_{chosen}$, must be:\n$$ b_{chosen} = \\min(S, b_{req}) $$\n\nIf the calculated required beam width $b_{req}$ exceeds the number of states $S$, we are forced to choose $b_{chosen} = S$. In this scenario, the chosen beam width $b_{chosen}$ is smaller than the required width $b_{req}$, so the accuracy guarantee $\\left(1 - (1-p)^{b_{chosen}}\\right)^T \\ge 1 - \\varepsilon$ will not be met. Furthermore, since $b_{chosen} = S$, there is no asymptotic cost reduction compared to the full Viterbi algorithm ($O(TBS) = O(TS^2)$).\n\n### Part 3: Implementation Logic\n\nFor each test case $(T, S, p, \\varepsilon)$:\n1.  Calculate the required beam width $b_{req}$ using the derived formula. For numerical stability with small $\\varepsilon$, the expression $(1-\\varepsilon)^{1/T}$ can be computed as $\\exp(\\frac{1}{T}\\ln(1-\\varepsilon))$. The terms $1 - \\exp(\\dots)$ and $\\ln(1-p)$ are best computed using `expm1` and `log1p` functions respectively to maintain precision for arguments close to $0$.\n2.  Determine the chosen beam width $b_{chosen} = \\min(S, b_{req})$.\n3.  Evaluate the two conditions for the boolean flag:\n    a.  **Accuracy Guarantee**: The accuracy is guaranteed if and only if the chosen beam width is sufficient, i.e., $b_{chosen} \\ge b_{req}$. This is equivalent to checking if $S \\ge b_{req}$.\n    b.  **Asymptotic Cost Reduction**: The cost is reduced if $b_{chosen}  S$.\n4.  The final boolean value is `True` if both conditions are met, and `False` otherwise. This simplifies to the single condition $b_{req}  S$. If $b_{req}  S$, we choose $b_{chosen} = b_{req}$, satisfying both $b_{chosen}  S$ and $b_{chosen} \\ge b_{req}$. If $b_{req} \\ge S$, it is impossible to satisfy both conditions simultaneously.\n5.  Return the pair $[b_{chosen}, \\text{boolean}]$.", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the beam search parameter problem for a suite of test cases.\n    For each case, it computes the minimal beam width 'b' that guarantees\n    a certain accuracy and determines if this choice also reduces\n    asymptotic complexity relative to the Viterbi algorithm.\n    \"\"\"\n\n    # Test suite of parameters (T, S, p, epsilon)\n    test_cases = [\n        (1000, 50, 0.3, 0.01),\n        (1, 1000, 0.3, 0.01),\n        (10000, 10000, 0.1, 0.1),\n        (500, 200, 0.25, 0.000001),\n        (1000, 500, 0.01, 0.01),\n    ]\n\n    results = []\n    for T, S, p, epsilon in test_cases:\n        # Using numerically stable functions for calculations involving\n        # numbers close to 0 or 1.\n        # log(1-x) = log1p(-x)\n        # exp(x)-1 = expm1(x)\n        #\n        # Derivation:\n        # (1 - (1-p)^b)^T = 1 - epsilon\n        # 1 - (1-p)^b = (1 - epsilon)^(1/T)\n        # (1-p)^b = 1 - (1 - epsilon)^(1/T)\n        # b * log(1-p) = log(1 - (1-epsilon)^(1/T))\n        # b = log(1 - (1-epsilon)^(1/T)) / log(1-p)  (log(1-p) is negative)\n        \n        # (1-epsilon)^(1/T) = exp(log((1-epsilon)^(1/T))) = exp( (1/T) * log(1-epsilon) )\n        # Using log1p for log(1-epsilon) and expm1 for exp(x)-1:\n        # log(1 - exp( (1/T) * log1p(-epsilon) ))\n        # let term_in_log = -expm1((1/T) * log1p(-epsilon))\n        try:\n            log_numerator_arg = -math.expm1(math.log1p(-epsilon) / T)\n            \n            # Handle potential domain error if log_numerator_arg is not positive.\n            # This can happen if T is extremely large and epsilon is close to 1,\n            # but is unlikely with the given test cases.\n            if log_numerator_arg = 0:\n                # This case implies an effectively infinite required beam width,\n                # which is unrealistic but we handle it.\n                b_required_float = float('inf')\n            else:\n                numerator = math.log(log_numerator_arg)\n                denominator = math.log1p(-p)\n                b_required_float = numerator / denominator\n\n        except (ValueError, OverflowError):\n            b_required_float = float('inf')\n\n        # The required beam width must be an integer, so we take the ceiling.\n        if math.isinf(b_required_float) or math.isnan(b_required_float):\n            b_required = S + 1 # Effectively infinite, force to be  S\n        else:\n            b_required = int(math.ceil(b_required_float))\n\n        # The chosen beam width is constrained by the number of states S.\n        b_chosen = min(S, b_required)\n\n        # The accuracy is guaranteed if the chosen b is at least the required b.\n        # This is equivalent to S being large enough (S = b_required).\n        is_accuracy_guaranteed = (b_chosen = b_required)\n\n        # The asymptotic cost is reduced if b  S.\n        is_cost_reduced = (b_chosen  S)\n\n        # The final boolean flag is True only if BOTH conditions are met.\n        success_flag = is_accuracy_guaranteed and is_cost_reduced\n\n        # Append the pair [chosen_b, boolean_flag] to results.\n        results.append([b_chosen, success_flag])\n    \n    # Format the output as a string representing a list of lists.\n    # str() on a Python list [val, bool] produces '[val, True]' or '[val, False]',\n    # which matches the required output format style.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3288378"}, {"introduction": "Many critical problems in systems biology, such as exact inference on biochemical networks, are NP-hard, meaning their worst-case complexity grows exponentially with network size. However, biological networks are not random graphs; they possess distinct structural properties. This advanced exercise introduces the concept of fixed-parameter tractability, demonstrating how we can exploit the structure of a network—specifically, its treewidth—to create algorithms that are efficient for the structured inputs commonly found in biology, even if the general problem is intractable [@problem_id:3288367].", "problem": "You are given undirected interaction graphs that abstract sparse biochemical reaction or signaling pathways. Consider exact probabilistic inference on these networks modeled as a factor graph where each molecular species state is a random variable with finite domain size $k$, and interactions induce factors over the corresponding variable scopes. Exact inference by variable elimination processes variables one by one, replacing factors that include an eliminated variable by their marginalization. The computational cost at each elimination step is dominated by multiplying all factors that include the eliminated variable and then summing out that variable.\n\nFundamental definitions:\n- A factor graph induces an undirected graph $G = (V,E)$ on variables $V$, where an edge in $E$ connects two variables that appear together in some factor scope.\n- For an elimination ordering $\\pi$ on $V$, the induced graph is formed by, at each elimination of a vertex $v$, adding fill-in edges to make the current neighborhood of $v$ a clique before removing $v$. The induced width of $\\pi$ is the maximum over all eliminated vertices of the current degree of the vertex at its elimination. The treewidth $t$ of $G$ is the minimum induced width over all orderings.\n- If each variable has domain size $k$, then when eliminating a variable whose current neighborhood forms a clique of size $w$, the dominant arithmetic cost of that step is proportional to $k^{w+1}$.\n\nTask overview:\n1. From first principles, derive an asymptotic running-time bound for exact inference by variable elimination on a factor graph with $n$ variables and treewidth $t$, where the bound is of the form $O(f(t)\\,n)$ and the function $f$ is exponential in $t$. Do not assume any specialized formulas; derive the expression using only the above definitions and the fact that the scope size at each elimination step is bounded by the induced clique size.\n2. Implement a program that:\n   - Computes an upper bound $\\hat{t}$ on the treewidth $t$ using the min-fill heuristic on an undirected graph $G$. The heuristic repeatedly eliminates a vertex whose neighbors require the minimum number of fill-in edges to become a clique (ties broken by smaller current degree, then by smaller vertex index), adding the necessary fill-in edges and recording the maximum neighborhood size encountered; the final maximum neighborhood size is $\\hat{t}$.\n   - For binary variables with $k=2$, computes the per-instance asymptotic upper bound on the number of arithmetic operations for variable elimination as $n \\cdot 2^{\\hat{t}+1}$.\n3. Use the following test suite of undirected graphs that model curated pathway modules. Vertices are indexed from $0$ to $n-1$. Each test item is specified by the pair $(n,\\text{edges})$, where $\\text{edges}$ is a set of unordered pairs:\n   - Boundary case (isolated variable): $(n,\\text{edges}) = \\left(1,\\ \\varnothing\\right)$.\n   - Pathway-like linear chain with $n=10$: $(n,\\text{edges}) = \\left(10,\\ \\{(0,1),(1,2),(2,3),(3,4),(4,5),(5,6),(6,7),(7,8),(8,9)\\}\\right)$.\n   - Star module with $n=10$ and center at $0$: $(n,\\text{edges}) = \\left(10,\\ \\{(0,1),(0,2),(0,3),(0,4),(0,5),(0,6),(0,7),(0,8),(0,9)\\}\\right)$.\n   - Cycle motif with $n=8$: $(n,\\text{edges}) = \\left(8,\\ \\{(0,1),(1,2),(2,3),(3,4),(4,5),(5,6),(6,7),(7,0)\\}\\right)$.\n   - Two-row grid module ($2\\times 4$) with $n=8$: $(n,\\text{edges}) = \\left(8,\\ \\{(0,1),(1,2),(2,3),(4,5),(5,6),(6,7),(0,4),(1,5),(2,6),(3,7)\\}\\right)$.\n   - Clique core with tail (dense hub within a module): $(n,\\text{edges}) = \\left(7,\\ \\{(0,1),(0,2),(0,3),(1,2),(1,3),(2,3),(3,4),(4,5),(5,6)\\}\\right)$.\n4. Assume binary variables with $k=2$ for all test cases. Your program should process the test suite in the order listed above and output, for each test case, the pair $[\\hat{t}, n \\cdot 2^{\\hat{t}+1}]$.\n5. Final output format requirement: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a two-element list. For example, an output for three cases would look like $[[a_1,b_1],[a_2,b_2],[a_3,b_3]]$ with $a_i$ and $b_i$ being integers.\n\nThere are no physical units involved in this task. All angles, if any appear, should be understood in radians, but none are used here. All requested numerical outputs are integers.", "solution": "The user has provided a valid problem statement. It is scientifically grounded in the principles of probabilistic graphical models and computational complexity, well-posed with clear definitions and objectives, and free of any of the invalidating flaws outlined in the instructions. I will now proceed with a complete solution.\n\n### Part 1: Derivation of the Asymptotic Running-Time Bound\n\nThe task is to derive an asymptotic running-time bound for exact inference by variable elimination on a factor graph with $n$ variables and treewidth $t$. The bound must be of the form $O(f(t)\\,n)$, where $f$ is an exponential function of $t$, and derived from the given first principles.\n\n1.  **Total Computational Cost**: The variable elimination algorithm processes each of the $n$ variables in the graph $G=(V, E)$ according to a specific elimination ordering, $\\pi = (v_1, v_2, \\dots, v_n)$. The total computational cost is the sum of the costs of eliminating each variable.\n    $$ \\text{Total Cost}(\\pi) = \\sum_{i=1}^{n} \\text{Cost}(\\text{eliminate } v_i) $$\n\n2.  **Cost of a Single Elimination Step**: The problem states that the cost of eliminating a single variable $v_i$ is proportional to $k^{w_i+1}$, where $k$ is the domain size of each variable and $w_i$ is the size of the clique formed by the neighborhood of $v_i$ at the moment of its elimination. This size $w_i$ is precisely the degree of vertex $v_i$ in the graph as it exists just before the $i$-th step. Let $d_i(v_i)$ be this degree. The cost of eliminating $v_i$ is thus $C \\cdot k^{d_i(v_i)+1}$ for some constant of proportionality $C$.\n\n3.  **Upper Bounding the Cost with Induced Width**: The induced width of the elimination ordering $\\pi$, denoted $w^*(\\pi)$, is defined as the maximum degree of any vertex at the time of its elimination over the entire process.\n    $$ w^*(\\pi) = \\max_{i \\in \\{1, \\dots, n\\}} d_i(v_i) $$\n    Therefore, for any variable $v_i$ in the ordering $\\pi$, its degree at elimination is bounded by the induced width: $d_i(v_i) \\leq w^*(\\pi)$. We can use this to establish an upper bound on the total cost for a given ordering $\\pi$:\n    $$ \\text{Total Cost}(\\pi) = \\sum_{i=1}^{n} C \\cdot k^{d_i(v_i)+1} \\leq \\sum_{i=1}^{n} C \\cdot k^{w^*(\\pi)+1} $$\n    Since the term $C \\cdot k^{w^*(\\pi)+1}$ is constant with respect to the summation over $n$ variables, we have:\n    $$ \\text{Total Cost}(\\pi) \\leq n \\cdot C \\cdot k^{w^*(\\pi)+1} $$\n\n4.  **Introducing Treewidth**: The variable elimination algorithm's efficiency depends critically on finding a good elimination ordering. An optimal ordering is one that minimizes the induced width. The treewidth, $t$, of the graph $G$ is defined as the minimum possible induced width over all possible elimination orderings.\n    $$ t = \\min_{\\pi} w^*(\\pi) $$\n    If we use an optimal elimination ordering, $\\pi_{\\text{opt}}$, then its induced width is equal to the treewidth of the graph, i.e., $w^*(\\pi_{\\text{opt}}) = t$.\n\n5.  **Final Asymptotic Bound**: By substituting the treewidth $t$ into our cost inequality for an optimal ordering, we obtain the upper bound on the complexity of the best-case variable elimination:\n    $$ \\text{Total Cost}(\\pi_{\\text{opt}}) \\leq n \\cdot C \\cdot k^{t+1} $$\n    In terms of asymptotic complexity, we abstract away the constant factor $C$. The running time is therefore bounded by:\n    $$ O(n \\cdot k^{t+1}) $$\n    This expression matches the required form $O(f(t) \\cdot n)$, where $f(t) = k^{t+1}$ is an exponential function of the treewidth $t$. This completes the derivation.\n\n### Part 2: Algorithmic Implementation\n\nThe second part of the task involves implementing an algorithm to compute an upper bound on the treewidth, $\\hat{t}$, using the min-fill heuristic, and then calculating the total cost using this bound.\n\n**Min-Fill Heuristic Logic**\nThe algorithm proceeds iteratively, eliminating one vertex at each step. In each step, we must select which vertex to eliminate from the set of remaining vertices. The selection follows a strict set of rules:\n1.  **Primary Criterion (Min-Fill)**: Choose the vertex that requires the minimum number of \"fill-in\" edges to make its current neighborhood a clique. The number of fill-in edges for a vertex with $d$ neighbors is $\\frac{d(d-1)}{2} - m$, where $m$ is the number of edges already present between pairs of its neighbors.\n2.  **First Tie-Breaker (Min-Degree)**: If multiple vertices have the same minimum fill-in count, choose from this subset the one with the smallest current degree.\n3.  **Second Tie-Breaker (Min-Index)**: If a tie still persists, choose the vertex with the smallest integer index.\n\n**Process:**\n1.  Initialize the graph from the given vertex count $n$ and edge set. An adjacency list representation (e.g., a dictionary of sets in Python) is suitable.\n2.  Maintain a set of remaining vertices, initially containing all vertices from $0$ to $n-1$.\n3.  Initialize a variable `max_width_encountered` to $0$.\n4.  Loop $n$ times to eliminate each vertex:\n    a.  Among all remaining vertices, find the best vertex to eliminate by applying the min-fill heuristic with its tie-breaking rules. A clean way to implement this is to iterate through the candidates, calculating a tuple `(fill_count, degree, index)` for each, and finding the lexicographically smallest tuple.\n    b.  Once the vertex $v$ to be eliminated is chosen, record its current degree, $d_v$. Update `max_width_encountered = max(max_width_encountered, d_v)`.\n    c.  Modify the graph: add all necessary fill-in edges between the neighbors of $v$ to make them a clique.\n    d.  Remove $v$ from the graph structure: remove it from the set of remaining vertices and from the adjacency lists of its former neighbors.\n5.  After the loop terminates, `max_width_encountered` holds the value of $\\hat{t}$, the induced width of the ordering found by the heuristic. This serves as an upper bound for the true treewidth $t$.\n\n**Cost Calculation:**\nFor each test case with $n$ variables, binary domain size ($k=2$), and a computed treewidth upper bound $\\hat{t}$, the total number of arithmetic operations is calculated as $n \\cdot 2^{\\hat{t}+1}$.\nThe final program implements this logic for each test case and formats the output as a list of `[`t_hat`, cost]` pairs.", "answer": "```python\nimport sys\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    It computes an upper bound on treewidth using the min-fill heuristic\n    and then calculates the corresponding complexity bound for variable elimination.\n    \"\"\"\n\n    # The problem specifies that numpy and scipy are allowed, but they are not\n    # necessary for this specific implementation. Using only standard libraries.\n\n    test_cases = [\n        (1, set()),\n        (10, {(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9)}),\n        (10, {(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9)}),\n        (8, {(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 0)}),\n        (8, {(0, 1), (1, 2), (2, 3), (4, 5), (5, 6), (6, 7), (0, 4), (1, 5), (2, 6), (3, 7)}),\n        (7, {(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3), (3, 4), (4, 5), (5, 6)}),\n    ]\n\n    results = []\n    for n, edges in test_cases:\n        t_hat = compute_treewidth_upper_bound(n, edges)\n        # Using k=2 as specified for binary variables\n        # Cost is n * 2^(t_hat + 1). Use bit shift for efficiency and to handle large numbers.\n        cost = n * (1  (t_hat + 1))\n        results.append([t_hat, cost])\n\n    # Format the final output string as specified\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\ndef compute_treewidth_upper_bound(n, edges_set):\n    \"\"\"\n    Computes an upper bound on treewidth (the induced width) for a given graph\n    using the min-fill heuristic.\n\n    Args:\n        n (int): The number of vertices in the graph.\n        edges_set (set): A set of tuples representing the undirected edges.\n\n    Returns:\n        int: The computed treewidth upper bound (t_hat).\n    \"\"\"\n    if n = 1:\n        return 0\n\n    # Adjacency list representation using a dictionary of sets\n    adj = {i: set() for i in range(n)}\n    for u, v in edges_set:\n        adj[u].add(v)\n        adj[v].add(u)\n\n    remaining_nodes = set(range(n))\n    max_width_encountered = 0\n\n    # Eliminate one node in each iteration\n    for _ in range(n):\n        # Store (fill_count, degree, index) to find the best node to eliminate.\n        # Python's tuple comparison handles the tie-breaking logic automatically.\n        best_node_info = (float('inf'), float('inf'), -1)\n\n        # Iterate through remaining nodes in sorted order to ensure deterministic tie-breaking.\n        for node in sorted(list(remaining_nodes)):\n            neighbors = adj[node]\n            degree = len(neighbors)\n            \n            fill_count = 0\n            if degree > 1:\n                neighbor_list = list(neighbors)\n                for i in range(degree):\n                    for j in range(i + 1, degree):\n                        u, v = neighbor_list[i], neighbor_list[j]\n                        if v not in adj[u]:\n                            fill_count += 1\n            \n            current_node_info = (fill_count, degree, node)\n            if current_node_info  best_node_info:\n                best_node_info = current_node_info\n\n        _, degree_at_elimination, node_to_eliminate = best_node_info\n        \n        max_width_encountered = max(max_width_encountered, degree_at_elimination)\n        \n        # Add fill-in edges (triangulate the neighbors)\n        neighbors_to_connect = list(adj[node_to_eliminate])\n        for i in range(len(neighbors_to_connect)):\n            for j in range(i + 1, len(neighbors_to_connect)):\n                u, v = neighbors_to_connect[i], neighbors_to_connect[j]\n                adj[u].add(v)\n                adj[v].add(u)\n        \n        # Remove the node from the graph\n        for neighbor in neighbors_to_connect:\n            adj[neighbor].remove(node_to_eliminate)\n        del adj[node_to_eliminate]\n        remaining_nodes.remove(node_to_eliminate)\n        \n    return max_width_encountered\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3288367"}]}