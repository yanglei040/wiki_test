{"hands_on_practices": [{"introduction": "Critical phenomena in gravitational collapse are characterized by the emergence of universal scaling laws near the threshold of black hole formation. This exercise guides you through the foundational theoretical argument that connects the macroscopic scaling of black hole mass, $M_{\\mathrm{BH}} \\propto |p - p_*|^{\\gamma}$, to the microscopic properties of the intermediate critical solution. By applying dimensional analysis and the principles of linear stability theory, you will derive the celebrated relationship between the mass-scaling exponent $\\gamma$ and the growth rate of the critical solution's single unstable mode, demonstrating how instability at one scale dictates the physics at another [@problem_id:3471159].", "problem": "Consider spherically symmetric gravitational collapse of a minimally coupled, massless scalar field in General Relativity (GR), in spacetime dimension $3+1$, with asymptotically flat boundary conditions. Work in geometric units $G=c=1$. The Einstein equations are $G_{ab}=8\\pi T_{ab}$ with scalar-field stress-energy tensor $T_{ab}=\\nabla_{a}\\phi\\,\\nabla_{b}\\phi-\\frac{1}{2}g_{ab}\\,\\nabla_{c}\\phi\\,\\nabla^{c}\\phi$. Assume the absence of any intrinsic mass scales in the matter sector and the field equations.\n\nConsider a one-parameter family of smooth, compactly supported initial data labeled by a real parameter $p$, such that there exists a threshold $p_{*}$ separating dispersion ($p<p_{*}$) from black hole formation ($p>p_{*}$). Near $p=p_{*}$, assume the existence of a discretely self-similar (DSS) critical solution $Z_{*}(\\tau,x)$ for the dimensionless set of variables $Z$ that describe the geometry and scalar field in scale-adapted coordinates, where $\\tau$ is the logarithmic scale coordinate and $x$ is a dimensionless radial variable. The DSS property is $Z_{*}(\\tau+\\Delta,x)=Z_{*}(\\tau,x)$ for some finite echoing period $\\Delta>0$.\n\nAssume further that linear perturbations about $Z_{*}$ admit a single unstable mode with (real) growth exponent $\\kappa>0$, so that, for initial data with $|p-p_{*}|\\ll 1$, the leading perturbation amplitude behaves as $A(\\tau)\\propto (p-p_{*})\\,\\exp(\\kappa\\,\\tau)$ in the linear regime, while all other modes decay. The solution tracks $Z_{*}$ until a departure scale $\\tau=\\tau_{*}$ is reached, defined implicitly by the breakdown of linearity when $|A(\\tau_{*})|\\sim \\mathcal{O}(1)$. Let $L_{*}$ denote the physical length scale associated with $\\tau_{*}$ via $L_{*}\\propto \\exp(-\\tau_{*})$. Use only dimensional analysis, the scale-invariance of the field equations in $G=c=1$ units, and the above properties of the critical solution and its unstable mode to determine the exponent $\\gamma$ governing the leading power-law dependence of the black hole mass $M_{\\mathrm{BH}}$ on the distance to criticality $|p-p_{*}|$ in the supercritical regime, i.e., $M_{\\mathrm{BH}}\\propto |p-p_{*}|^{\\gamma}$ up to a multiplicative factor that may be modulated by a bounded function of $\\ln|p-p_{*}|$ due to discrete self-similarity.\n\nWhat is the closed-form analytic expression for $\\gamma$ in terms of $\\kappa$? Provide your final answer as a single expression. No numerical approximation or rounding is required. Express your answer without units.", "solution": "The problem asks for the scaling exponent $\\gamma$ in the relation for the black hole mass, $M_{\\mathrm{BH}} \\propto |p-p_{*}|^{\\gamma}$, in the context of critical gravitational collapse of a massless scalar field. The derivation will rely on dimensional analysis and the properties of the critical solution and its perturbations as described in the problem statement.\n\nFirst, we perform a validation of the problem statement.\n\n### Step 1: Extract Givens\n- The system is a spherically symmetric, minimally coupled, massless scalar field in $3+1$ dimensional General Relativity.\n- Geometric units are used: $G=c=1$.\n- The field equations are $G_{ab}=8\\pi T_{ab}$ with $T_{ab}=\\nabla_{a}\\phi\\,\\nabla_{b}\\phi-\\frac{1}{2}g_{ab}\\,\\nabla_{c}\\phi\\,\\nabla^{c}\\phi$.\n- The theory has no intrinsic mass scales.\n- A one-parameter family of initial data, labeled by $p$, has a critical threshold $p_{*}$. For $p>p_{*}$, black holes form; for $p<p_{*}$, the field disperses.\n- There exists a discretely self-similar (DSS) critical solution $Z_{*}(\\tau,x)$ with echoing period $\\Delta$, where $\\tau$ is a logarithmic scale coordinate.\n- Linear perturbations about $Z_{*}$ have a single unstable mode with a real growth exponent $\\kappa > 0$.\n- The amplitude of the unstable perturbation for initial data near criticality ($|p-p_{*}|\\ll 1$) evolves as $A(\\tau) \\propto (p-p_{*})\\,\\exp(\\kappa\\,\\tau)$.\n- The solution deviates from the critical one at a scale $\\tau=\\tau_{*}$, where the linear approximation breaks down, i.e., $|A(\\tau_{*})| \\sim \\mathcal{O}(1)$.\n- The physical length scale $L_{*}$ at this departure time is related to $\\tau_{*}$ by $L_{*} \\propto \\exp(-\\tau_{*})$.\n- The black hole mass in the supercritical regime ($p>p_{*}$) follows a power law: $M_{\\mathrm{BH}}\\propto |p-p_{*}|^{\\gamma}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem describes Choptuik scaling, a well-established and numerically verified phenomenon in the field of numerical relativity and gravitational waves. The underlying physics (General Relativity coupled to a scalar field) and mathematical concepts (critical phenomena, self-similarity, linear stability analysis) are standard and correct.\n- **Well-Posed:** The problem provides a clear objective (to find $\\gamma$) and supplies all the necessary assumptions and relationships to derive it. The existence of a unique solution for $\\gamma$ is expected based on the provided information.\n- **Objective:** The language is precise and technical, free from subjective or ambiguous statements. All terms like \"critical solution,\" \"unstable mode,\" and \"scaling law\" have well-defined meanings in this context.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a standard derivation in theoretical physics based on sound principles. I will proceed with the solution.\n\n### Derivation of the Scaling Exponent $\\gamma$\n\nThe solution for a near-critical initial data parameter $p$ tracks the critical solution until the growing linear perturbation becomes large. We focus on the supercritical case, $p>p_{*}$, which leads to black hole formation.\n\n1.  **Determine the departure time $\\tau_{*}$:**\n    The amplitude of the single unstable mode is given by the proportionality $A(\\tau) \\propto (p-p_{*})\\,\\exp(\\kappa\\,\\tau)$. Let's write this with a constant of proportionality $C_0$, which depends on the specific definition of the amplitude and the family of initial data:\n    $$A(\\tau) = C_0 (p-p_{*})\\,\\exp(\\kappa\\,\\tau)$$\n    The linear approximation is valid as long as $|A(\\tau)| \\ll 1$. The solution departs significantly from the critical solution at a time $\\tau=\\tau_{*}$ when the perturbation becomes of order unity:\n    $$|A(\\tau_{*})| = |C_0 (p-p_{*})| \\exp(\\kappa\\,\\tau_{*}) \\sim 1$$\n    Since we are in the supercritical regime, $p-p_{*} > 0$, we have:\n    $$C_0 (p-p_{*}) \\exp(\\kappa\\,\\tau_{*}) \\approx K$$\n    where $K$ is some constant of order unity. We can rearrange this to find an expression for $\\exp(\\kappa\\,\\tau_{*})$:\n    $$\\exp(\\kappa\\,\\tau_{*}) \\approx \\frac{K}{C_0(p-p_{*})}$$\n    This shows that as $p$ approaches $p_{*}$, $\\tau_{*}$ tends to infinity. In terms of scaling, we can write:\n    $$\\exp(\\kappa\\,\\tau_{*}) \\propto (p-p_{*})^{-1}$$\n\n2.  **Relate $\\tau_{*}$ to the characteristic physical scale $L_{*}$:**\n    The problem states that the characteristic physical length scale $L_{*}$ at the departure time $\\tau_{*}$ is given by the relation $L_{*} \\propto \\exp(-\\tau_{*})$. We need to express this in terms of $(p-p_{*})$. From the previous step, we have $\\exp(\\kappa\\,\\tau_{*}) \\propto (p-p_{*})^{-1}$. We can solve for $\\exp(\\tau_{*})$:\n    $$\\exp(\\tau_{*}) = \\left(\\exp(\\kappa\\,\\tau_{*})\\right)^{1/\\kappa} \\propto \\left((p-p_{*})^{-1}\\right)^{1/\\kappa} = (p-p_{*})^{-1/\\kappa}$$\n    Now we can find the scaling for the length $L_{*}$:\n    $$L_{*} \\propto \\exp(-\\tau_{*}) = \\frac{1}{\\exp(\\tau_{*})} \\propto \\frac{1}{(p-p_{*})^{-1/\\kappa}} = (p-p_{*})^{1/\\kappa}$$\n\n3.  **Relate the physical scale $L_{*}$ to the black hole mass $M_{\\mathrm{BH}}$:**\n    The problem states that the theory (massless scalar field in GR) has no intrinsic mass or length scales. Therefore, when a black hole forms from the gravitational collapse, its mass $M_{\\mathrm{BH}}$ must be determined by the only characteristic scale available at the moment of collapse. This scale is the physical length scale $L_{*}$ at which the solution departed from the critical, self-similar evolution. In geometric units ($G=c=1$), mass and length have the same physical dimension. Thus, by dimensional analysis, the mass of the black hole must be proportional to this characteristic length scale:\n    $$M_{\\mathrm{BH}} \\propto L_{*}$$\n    The discrete self-similarity of the critical solution implies that the constant of proportionality is not a strict constant but rather a bounded, periodic function of $\\ln|p-p_{*}|$. However, this does not affect the leading power-law behavior.\n\n4.  **Determine the scaling exponent $\\gamma$:**\n    By combining the results from the previous steps, we can establish the scaling law for the black hole mass:\n    $$M_{\\mathrm{BH}} \\propto L_{*} \\propto (p-p_{*})^{1/\\kappa}$$\n    For the supercritical regime, $|p-p_{*}| = p-p_{*}$. The problem defines the scaling law as $M_{\\mathrm{BH}} \\propto |p-p_{*}|^{\\gamma}$. Comparing our derived relation with this definition, we can identify the exponent $\\gamma$:\n    $$M_{\\mathrm{BH}} \\propto |p-p_{*}|^{1/\\kappa}$$\n    Therefore, the scaling exponent is:\n    $$\\gamma = \\frac{1}{\\kappa}$$\n\nThis result, known as Choptuik's scaling law, directly relates the critical exponent $\\gamma$ of the black hole mass to the Lyapunov exponent $\\kappa$ of the single unstable mode of the critical solution.", "answer": "$$\\boxed{\\frac{1}{\\kappa}}$$", "id": "3471159"}, {"introduction": "The theoretical prediction of scaling laws motivates a direct numerical investigation, but before we can measure any exponents, we must first precisely locate the critical parameter $p^*$ that separates the two possible outcomes of collapse. This \"criticality search\" is a quintessential root-finding problem, but one with a practical complication: the outcome of any single numerical evolution can be misclassified due to finite grid resolution. This practice [@problem_id:3471223] challenges you to design a robust and efficient bisection algorithm that systematically narrows the hunt for $p^*$ while managing the statistical uncertainty of the outcome, a core task in any experimental study of critical collapse.", "problem": "Consider a one-parameter family of spherically symmetric initial data for the Einstein field equations of General Relativity (GR), parameterized by a dimensionless amplitude $p \\in \\mathbb{R}$. There exists a critical parameter $p^\\ast$ such that evolutions with $p < p^\\ast$ disperse without forming a black hole, while evolutions with $p > p^\\ast$ undergo gravitational collapse and form a black hole. In numerical relativity, black hole formation can be determined using an Apparent Horizon (AH) finder, which returns a boolean $1$ if an apparent horizon is detected and $0$ otherwise. Due to finite spatial and temporal resolution, each AH classification may be misclassified with independent probability $q \\in [0,1)$.\n\nYou will implement a bracketing scheme based on bisection on $p$ to locate $p^\\ast$ to a specified relative accuracy. The scheme assumes an initial bracketing interval $[p_{\\mathrm{lo}}, p_{\\mathrm{hi}}]$ with $p_{\\mathrm{lo}} < p^\\ast < p_{\\mathrm{hi}}$ and employs the following decision mechanism at each bisection step: at a chosen midpoint $p_{\\mathrm{mid}}$, run the evolution and AH finder repeatedly $m$ times at the same $p_{\\mathrm{mid}}$ and classify the outcome by majority vote. Let the per-decision tolerance be $\\rho \\in (0,1)$, meaning that the chosen $m$ must be the smallest odd integer such that the probability that the majority vote is incorrect is at most $\\rho$. All repeated AH classifications at the same $p_{\\mathrm{mid}}$ are independent and identically distributed with misclassification probability $q$. One “evolution” is defined as a single forward time-integration of the Einstein field equations for the given initial data up to the point where the AH decision is made; thus running $m$ repeats at one $p_{\\mathrm{mid}}$ consumes $m$ evolutions.\n\nDefine the relative accuracy requirement as follows: after $K$ bisection steps, the bracket $[p_{\\mathrm{lo}}^{(K)}, p_{\\mathrm{hi}}^{(K)}]$ must satisfy $p_{\\mathrm{hi}}^{(K)} - p_{\\mathrm{lo}}^{(K)} \\le 2 \\,\\epsilon \\, p^\\ast$ for a given target $\\epsilon \\in (0,1)$, where $p^\\ast$ is the true threshold. Assume $p^\\ast$ is known for the purpose of benchmarking and test-scenario construction only. Under these assumptions:\n\n- Determine the minimal number of bisection steps $K$ needed to meet the relative accuracy requirement, starting from the initial bracket $[p_{\\mathrm{lo}}, p_{\\mathrm{hi}}]$.\n- Determine the minimal odd number $m$ of repeated evolutions per bisection decision such that the majority-vote misclassification probability is at most $\\rho$ when the single-evolution misclassification probability is $q$.\n- Estimate the total number of evolutions required as $E = m \\times K$.\n\nYou must implement a program that, for each test case below, computes and outputs $E$ as an integer. No physical units are involved; all quantities are dimensionless.\n\nTest suite:\n- Case A (general case): $p_{\\mathrm{lo}} = 0.4$, $p_{\\mathrm{hi}} = 0.6$, $p^\\ast = 0.515$, $\\epsilon = 10^{-3}$, $q = 0.1$, $\\rho = 10^{-4}$.\n- Case B (stringent accuracy, low $q$): $p_{\\mathrm{lo}} = 0.49$, $p_{\\mathrm{hi}} = 0.51$, $p^\\ast = 0.5$, $\\epsilon = 10^{-6}$, $q = 0.01$, $\\rho = 10^{-6}$.\n- Case C (high misclassification risk): $p_{\\mathrm{lo}} = 0.9$, $p_{\\mathrm{hi}} = 1.1$, $p^\\ast = 1.0$, $\\epsilon = 10^{-2}$, $q = 0.45$, $\\rho = 10^{-3}$.\n- Case D (initial bracket already sufficiently accurate): $p_{\\mathrm{lo}} = 0.995$, $p_{\\mathrm{hi}} = 1.005$, $p^\\ast = 1.0$, $\\epsilon = 10^{-2}$, $q = 0.2$, $\\rho = 10^{-5}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[resultA,resultB,resultC,resultD]\"), where each entry is the integer $E$ for the corresponding test case in the order A, B, C, D.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. It is a valid formalization of a common challenge in computational science: balancing numerical accuracy with computational cost in the presence of stochastic errors. We will proceed to derive the solution.\n\nThe objective is to compute the total number of evolutions, $E$, required to locate the critical parameter $p^\\ast$ within a specified relative accuracy. This total cost is the product of two independently determined quantities: the number of bisection steps, $K$, needed to achieve the desired interval tolerance, and the number of repeated evolutions, $m$, required per bisection step to ensure a reliable decision. Thus, $E = m \\times K$.\n\n**Determination of the Number of Bisection Steps ($K$)**\n\nThe bisection method is an iterative root-finding algorithm that repeatedly halves an interval known to contain the root. In this context, the \"root\" is the critical parameter $p^\\ast$.\n\nWe begin with an initial interval $[p_{\\mathrm{lo}}^{(0)}, p_{\\mathrm{hi}}^{(0)}] = [p_{\\mathrm{lo}}, p_{\\mathrm{hi}}]$ of width $W_0 = p_{\\mathrm{hi}} - p_{\\mathrm{lo}}$. By construction, $p_{\\mathrm{lo}} < p^\\ast < p_{\\mathrm{hi}}$. After one bisection step, the width of the new interval is $W_1 = W_0 / 2$. After $K$ steps, the interval width is reduced to:\n$$ W_K = p_{\\mathrm{hi}}^{(K)} - p_{\\mathrm{lo}}^{(K)} = \\frac{p_{\\mathrm{hi}} - p_{\\mathrm{lo}}}{2^K} $$\nThe process terminates when the interval width satisfies the relative accuracy requirement:\n$$ W_K \\le 2 \\epsilon p^\\ast $$\nwhere $\\epsilon$ is the target relative accuracy and $p^\\ast$ is the true critical parameter.\n\nSubstituting the expression for $W_K$ into the inequality, we obtain the condition on $K$:\n$$ \\frac{p_{\\mathrm{hi}} - p_{\\mathrm{lo}}}{2^K} \\le 2 \\epsilon p^\\ast $$\nTo find the minimal number of steps, we solve for $K$:\n$$ 2^K \\ge \\frac{p_{\\mathrm{hi}} - p_{\\mathrm{lo}}}{2 \\epsilon p^\\ast} $$\nTaking the logarithm base $2$ of both sides yields:\n$$ K \\ge \\log_2\\left(\\frac{p_{\\mathrm{hi}} - p_{\\mathrm{lo}}}{2 \\epsilon p^\\ast}\\right) $$\nSince $K$ must be an integer representing the number of steps, we must take the ceiling of the right-hand side. However, a special case arises if the initial interval already satisfies the accuracy condition, i.e., if $p_{\\mathrm{hi}} - p_{\\mathrm{lo}} \\le 2 \\epsilon p^\\ast$. In this scenario, $0$ bisection steps are required. The expression inside the logarithm would be less than or equal to $1$, yielding a non-positive result. The number of steps cannot be negative. Therefore, the minimal number of bisection steps $K$ is given by:\n$$ K = \\max\\left(0, \\left\\lceil \\log_2\\left(\\frac{p_{\\mathrm{hi}} - p_{\\mathrm{lo}}}{2 \\epsilon p^\\ast}\\right) \\right\\rceil\\right) $$\n\n**Determination of Repetitions per Bisection Step ($m$)**\n\nAt each bisection step, a decision must be made about the character of the evolution at the midpoint $p_{\\mathrm{mid}}$. This decision is subject to misclassification. To mitigate this, $m$ independent evolutions are performed, and the outcome is decided by majority vote. We must find the smallest odd integer $m$ such that the probability of an incorrect majority vote is at most $\\rho$.\n\nLet $q$ be the probability of a single evolution being misclassified. The outcome of each of the $m$ evolutions is a Bernoulli trial. Let $N_{\\mathrm{incorrect}}$ be the number of incorrect outcomes among the $m$ trials. $N_{\\mathrm{incorrect}}$ follows a binomial distribution, denoted $\\mathcal{B}(m, q)$, with probability mass function:\n$$ P(N_{\\mathrm{incorrect}}=k) = \\binom{m}{k} q^k (1-q)^{m-k} $$\nfor $k \\in \\{0, 1, \\dots, m\\}$.\n\nThe problem specifies that $m$ must be an odd integer. A majority vote is incorrect if the number of incorrect outcomes exceeds half the total votes, i.e., $N_{\\mathrm{incorrect}} > m/2$. Since $m$ is odd, this is equivalent to $N_{\\mathrm{incorrect}} \\ge (m+1)/2$.\n\nThe probability of an incorrect majority vote, $P_{\\mathrm{error}}$, is the sum of probabilities of all such outcomes:\n$$ P_{\\mathrm{error}}(m, q) = P(N_{\\mathrm{incorrect}} \\ge (m+1)/2) = \\sum_{k=(m+1)/2}^{m} \\binom{m}{k} q^k (1-q)^{m-k} $$\nThis expression is the tail probability of the binomial distribution. We are tasked with finding the smallest odd integer $m$ that satisfies the per-decision tolerance $\\rho$:\n$$ P_{\\mathrm{error}}(m, q) \\le \\rho $$\nThis inequality does not typically have a simple closed-form solution for $m$. Instead, $m$ must be found numerically. We can start with $m=1$ and iteratively increment by $2$ (to maintain oddness), calculating $P_{\\mathrm{error}}$ at each step, until the condition is met. The summation can be efficiently computed using the survival function of the binomial distribution, which is often available in scientific computing libraries and is related to the regularized incomplete beta function.\n\n**Total Computational Cost ($E$)**\n\nThe total number of evolutions, $E$, is the product of the number of bisection decisions to be made and the computational cost per decision. With $K$ bisection steps and $m$ evolutions per step, the total cost is:\n$$ E = m \\times K $$\nIf $K=0$, no bisection steps are performed, and thus no evolutions are required to refine the bracket. In this case, $E=0$ irrespective of the value of $m$.\n\nThe following program implements this logic to calculate $E$ for each of the specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\nfrom scipy.stats import binom\n\ndef solve():\n    \"\"\"\n    Computes the total number of evolutions E for a series of test cases\n    related to finding a critical parameter in gravitational collapse.\n    \"\"\"\n    \n    # Define the test cases from the problem statement as a list of tuples.\n    # Each tuple: (p_lo, p_hi, p_star, epsilon, q, rho)\n    test_cases = [\n        # Case A: general case\n        (0.4, 0.6, 0.515, 1e-3, 0.1, 1e-4),\n        # Case B: stringent accuracy, low q\n        (0.49, 0.51, 0.5, 1e-6, 0.01, 1e-6),\n        # Case C: high misclassification risk\n        (0.9, 1.1, 1.0, 1e-2, 0.45, 1e-3),\n        # Case D: initial bracket already sufficiently accurate\n        (0.995, 1.005, 1.0, 1e-2, 0.2, 1e-5),\n    ]\n\n    def calculate_k(p_lo, p_hi, p_star, epsilon):\n        \"\"\"\n        Calculates the minimal number of bisection steps K.\n        \"\"\"\n        # The required final interval width\n        target_width = 2 * epsilon * p_star\n        initial_width = p_hi - p_lo\n\n        # If the initial interval is already accurate enough, K=0.\n        if initial_width <= target_width:\n            return 0\n        \n        # Calculate the ratio of initial width to target width.\n        # This determines how many times we need to halve the interval.\n        ratio = initial_width / target_width\n        \n        # The number of steps K is the ceiling of log base 2 of the ratio.\n        k = math.ceil(math.log2(ratio))\n        return int(k)\n\n    def calculate_m(q, rho):\n        \"\"\"\n        Calculates the minimal odd number of repetitions m per bisection.\n        \"\"\"\n        # Start with the smallest odd integer.\n        m = 1\n        while True:\n            # For majority vote with odd m, error occurs if incorrect votes k > m/2,\n            # which is equivalent to k >= (m+1)/2.\n            # The survival function sf(k, n, p) computes P(X > k).\n            # We need P(X >= (m+1)/2) = P(X > (m+1)/2 - 1) = P(X > (m-1)/2).\n            # k_threshold is the argument to sf().\n            k_threshold = (m - 1) // 2\n            \n            error_prob = binom.sf(k_threshold, n=m, p=q)\n            \n            # Check if majority vote error probability is within tolerance.\n            if error_prob <= rho:\n                return m\n            \n            # Increment to the next odd integer.\n            m += 2\n\n    results = []\n    for case in test_cases:\n        p_lo, p_hi, p_star, epsilon, q, rho = case\n        \n        # Calculate the number of bisection steps.\n        k = calculate_k(p_lo, p_hi, p_star, epsilon)\n        \n        # If K is 0, no bisections are needed, so total evolutions E is 0.\n        if k == 0:\n            e = 0\n        else:\n            # Otherwise, calculate m and the total cost E = m * K.\n            m = calculate_m(q, rho)\n            e = m * k\n        \n        results.append(e)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3471223"}, {"introduction": "After locating the critical threshold and measuring the resulting black hole masses, a computational physicist must confront a final, crucial question: how much confidence can we have in our results? Numerical solutions to the Einstein field equations are subject to truncation errors that depend on the simulation's resolution. This advanced exercise [@problem_id:3471252] explores how these numerical artifacts can contaminate the physical scaling law, sometimes even mimicking the effect of an additional physical mode and leading to an incorrect scaling exponent. You will implement a rigorous convergence analysis using Richardson-type extrapolation to distinguish numerical error from physical reality, a vital skill for validating results in computational science.", "problem": "A critical collapse in General Relativity admits a threshold between dispersion and black hole formation governed by the Einstein field equations, and for a spherically symmetric one-parameter family of initial data, there exists a threshold parameter $p^\\star$ such that for $p > p^\\star$ a black hole forms. Near threshold, the black hole mass generated, denoted $M$, exhibits a scaling relation commonly observed in numerical relativity experiments: leading-order dynamics are controlled by a single relevant perturbation mode around the critical solution. Finite difference discretizations of the Einstein equations at resolution $h$ introduce truncation errors that can modify observed scaling in computations, potentially mimicking additional relevant modes and biasing the measurement of the physical scaling exponent. In a computational experiment, one seeks to recover the physical scaling exponent and diagnose whether truncation error acts as an apparent additional relevant mode.\n\nStarting from the following accepted bases:\n- The Einstein field equations imply scale-free critical phenomena near threshold for suitable matter models with one dominant relevant mode.\n- A discretization with resolution $h$ has truncation error that scales as a known formal order $r$, producing an error term proportional to $h^r$.\n\nYou must construct a program to separate physical scaling from numerical artifacts via controlled convergence tests across multiple resolutions. For synthetic datasets generated in dimensionless units, assume the following model for the measured black hole masses at resolution $h$ as a function of supercriticality $\\epsilon = p - p^\\star$:\n$$\nM_h(\\epsilon) = C\\, \\epsilon^{\\gamma} + D\\, h^r \\epsilon^{\\beta} + \\eta(\\epsilon,h),\n$$\nwhere $C > 0$ is a dimensionless amplitude, $\\gamma > 0$ is the physical scaling exponent to be recovered, $D$ is a dimensionless contamination amplitude, $r$ is the convergence order of the discretization, $\\beta \\ge 0$ parameterizes how the truncation-induced artifact scales with $\\epsilon$ and can mimic an additional relevant mode if $\\beta < \\gamma$, and $\\eta$ is small stochastic noise. All quantities are dimensionless.\n\nDesign diagnostics that use data from at least $3$ resolutions $h_1, h_2, h_3$ and multiple $\\epsilon$ values to:\n- Estimate the convergence order $r$ directly from the data, without prior knowledge of $r$.\n- Eliminate the leading truncation term $D h^r \\epsilon^{\\beta}$ by Richardson-type extrapolation to recover an approximation to the continuum mass $M(\\epsilon) \\approx C\\, \\epsilon^{\\gamma}$.\n- Fit the physical scaling exponent $\\gamma$ from the extrapolated continuum mass across $\\epsilon$.\n- Compare the apparent slopes obtained from single-resolution fits to the multi-resolution extrapolated fit to diagnose whether truncation error mimics an additional relevant mode that biases single-resolution measurements.\n\nYou must implement the following computational steps:\n- Given $3$ resolutions $h_1 < h_2 < h_3$, and arrays of $\\epsilon_k$ values with $k = 1,\\dots,K$, compute the naive apparent exponent $\\hat{\\gamma}_j$ at each resolution $h_j$ by linear least-squares fitting of $\\log M_{h_j}(\\epsilon_k)$ versus $\\log \\epsilon_k$.\n- Estimate $r$ by minimizing over $r$ the discrepancy in the resolution-difference ratios across $\\epsilon_k$:\n$$\nS(r) = \\sum_{k} \\left[ \\frac{M_{h_1}(\\epsilon_k) - M_{h_2}(\\epsilon_k)}{M_{h_2}(\\epsilon_k) - M_{h_3}(\\epsilon_k)} - \\frac{h_1^r - h_2^r}{h_2^r - h_3^r} \\right]^2,\n$$\nexcluding terms where the denominator is too small in magnitude to avoid numerical instability.\n- With the estimated $r$, eliminate the leading truncation term via two-point Richardson extrapolation at each $\\epsilon_k$:\n$$\n\\widehat{M}(\\epsilon_k) = \\frac{h_2^r M_{h_1}(\\epsilon_k) - h_1^r M_{h_2}(\\epsilon_k)}{h_2^r - h_1^r},\n$$\nand also\n$$\n\\widehat{M}'(\\epsilon_k) = \\frac{h_3^r M_{h_2}(\\epsilon_k) - h_2^r M_{h_3}(\\epsilon_k)}{h_3^r - h_2^r}.\n$$\nAverage these two extrapolants to obtain a robust continuum estimate\n$$\n\\widehat{M}_{\\text{avg}}(\\epsilon_k) = \\frac{1}{2}\\left[\\widehat{M}(\\epsilon_k) + \\widehat{M}'(\\epsilon_k)\\right].\n$$\n- Fit $\\gamma$ from $\\log \\widehat{M}_{\\text{avg}}(\\epsilon_k)$ versus $\\log \\epsilon_k$ by linear least squares to obtain $\\hat{\\gamma}_{\\text{extrap}}$.\n- Declare that truncation error mimics an additional relevant mode if the maximum discrepancy between single-resolution apparent slopes and the extrapolated slope exceeds a threshold, namely if\n$$\n\\max_{j \\in \\{1,2,3\\}} \\left| \\hat{\\gamma}_j - \\hat{\\gamma}_{\\text{extrap}} \\right| > \\tau,\n$$\nwith threshold $\\tau = 0.02$ in dimensionless units.\n\nYour program must generate synthetic datasets for the following test suite of parameter sets, then apply the above diagnostics and produce the specified output. All quantities are dimensionless, and all logs are natural logarithms.\n\nTest Suite:\n- Case $1$ (happy path, moderate contamination):\n  - $C = 1.0$, $\\gamma = 0.374$, $D = 0.5$, $\\beta = 0.20$, $r = 2$, $h_1 = 0.4$, $h_2 = 0.2$, $h_3 = 0.1$.\n  - $\\epsilon_k$ are $20$ values logarithmically spaced from $10^{-6}$ to $10^{-2}$.\n  - Additive Gaussian noise $\\eta$ with zero mean and standard deviation $\\sigma = 10^{-8}$ times $C\\,\\epsilon^{\\gamma}$.\n- Case $2$ (edge case, strong contamination at coarse resolutions):\n  - $C = 1.0$, $\\gamma = 0.374$, $D = 5.0$, $\\beta = 0.20$, $r = 2$, $h_1 = 0.8$, $h_2 = 0.4$, $h_3 = 0.2$.\n  - $\\epsilon_k$ are $20$ values logarithmically spaced from $5\\times 10^{-6}$ to $10^{-3}$.\n  - Additive Gaussian noise $\\eta$ with zero mean and standard deviation $\\sigma = 5\\times 10^{-8}$ times $C\\,\\epsilon^{\\gamma}$.\n- Case $3$ (boundary case, higher-order scheme with weak contamination):\n  - $C = 1.0$, $\\gamma = 0.374$, $D = 0.3$, $\\beta = 0.05$, $r = 4$, $h_1 = 0.5$, $h_2 = 0.25$, $h_3 = 0.125$.\n  - $\\epsilon_k$ are $20$ values logarithmically spaced from $10^{-6}$ to $10^{-2}$.\n  - Additive Gaussian noise $\\eta$ with zero mean and standard deviation $\\sigma = 10^{-8}$ times $C\\,\\epsilon^{\\gamma}$.\n\nImplementation requirements:\n- In all fits, use ordinary least squares on the pairs $\\left(\\log \\epsilon_k, \\log(\\cdot)\\right)$.\n- For the $r$-estimation functional $S(r)$, ignore any $k$ with $\\left|M_{h_2}(\\epsilon_k) - M_{h_3}(\\epsilon_k)\\right| < 10^{-14}$ to avoid division by near-zero.\n- Use a deterministic pseudorandom generator (set a fixed seed) for $\\eta$ so results are reproducible.\n\nFinal output specification:\n- For each case, return a list $[\\hat{\\gamma}_{\\text{extrap}}, \\hat{r}, \\text{diagnosis}]$, where $\\hat{\\gamma}_{\\text{extrap}}$ is rounded to $6$ decimals, $\\hat{r}$ is rounded to $3$ decimals, and $\\text{diagnosis}$ is a boolean returned as either $\\text{True}$ or $\\text{False}$ indicating whether truncation error mimics an additional relevant mode as defined above.\n- Your program should produce a single line of output containing the results as a comma-separated list of these three-element lists, enclosed in square brackets, for the three cases in order: for example, $[[\\dots],[\\dots],[\\dots]]$.\n\nYour program must be entirely self-contained, require no input, and use only the specified libraries. All outputs are dimensionless. Angles are not used. Percentages are not used.", "solution": "The problem presented is a well-posed and scientifically grounded exercise in numerical analysis, situated within the context of critical phenomena in gravitational collapse, a topic of significant interest in numerical relativity. The objective is to diagnose and correct for numerical artifacts in simulated data that purports to follow a physical scaling law. This requires a systematic application of convergence testing and extrapolation techniques, which are fundamental tools in computational science for validating simulation results and estimating the accuracy of numerical solutions. We will proceed with a detailed, principle-based solution.\n\nThe theoretical model for the measured black hole mass $M_h(\\epsilon)$ at a numerical resolution $h$ and for a supercriticality parameter $\\epsilon = p - p^\\star$ is given as:\n$$\nM_h(\\epsilon) = C\\, \\epsilon^{\\gamma} + D\\, h^r \\epsilon^{\\beta} + \\eta(\\epsilon,h)\n$$\nThis model is composed of three distinct terms. The first term, $C\\, \\epsilon^{\\gamma}$, represents the physical scaling law predicted by the theory of critical phenomena in General Relativity, where $\\gamma$ is the universal critical exponent and $C$ is a family-dependent amplitude. The second term, $D\\, h^r \\epsilon^{\\beta}$, models the leading-order truncation error of the numerical method used to solve the Einstein field equations. Here, $h$ is a measure of the grid spacing, $r$ is the formal order of convergence of the numerical scheme, and $D$ and $\\beta$ are parameters characterizing how this error term depends on the simulation parameters. The third term, $\\eta(\\epsilon,h)$, represents stochastic noise, which is unavoidable in floating-point computations and can also model small, unresolved physical or numerical effects.\n\nOur primary goal is to determine the physical exponent $\\gamma$ from a set of simulated data points $\\{M_{h_j}(\\epsilon_k)\\}$ generated at three different resolutions $h_1 > h_2 > h_3$ and across multiple values of $\\epsilon_k$.\n\nA standard technique for extracting a scaling exponent is to perform a linear fit on a log-log plot. Taking the natural logarithm of the mass model, we have:\n$$\n\\log M_h(\\epsilon) = \\log \\left( C\\, \\epsilon^{\\gamma} + D\\, h^r \\epsilon^{\\beta} \\right) + \\log\\left(1 + \\frac{\\eta}{C\\epsilon^\\gamma + Dh^r\\epsilon^\\beta}\\right) \\approx \\log \\left( C\\, \\epsilon^{\\gamma} + D\\, h^r \\epsilon^{\\beta} \\right)\n$$\nIf the numerical error term were absent ($D=0$), this would simplify to $\\log M(\\epsilon) \\approx \\gamma \\log \\epsilon + \\log C$. In this ideal case, the slope of a linear fit of $\\log M$ versus $\\log \\epsilon$ would directly yield $\\gamma$. However, the presence of the truncation error term contaminates this relationship. The slope of the log-log plot becomes dependent on $\\epsilon$, and a simple linear fit over a range of $\\epsilon$ values will produce an \"apparent\" exponent $\\hat{\\gamma}_j$ at each resolution $h_j$ that is a biased estimate of the true exponent $\\gamma$. If $\\beta < \\gamma$, the error term $D h^r \\epsilon^\\beta$ becomes increasingly dominant relative to the physical term $C \\epsilon^\\gamma$ as $\\epsilon \\to 0$. This can severely corrupt the estimate of $\\gamma$, a phenomenon the problem describes as the truncation error \"mimicking an additional relevant mode.\"\n\nTo overcome this, we must first characterize and then eliminate the leading-order error term.\n\n**Step 1: Estimation of the Convergence Order, $r$**\n\nWe can estimate the convergence order $r$ without prior knowledge by assuming the model holds and that higher-order error terms are subdominant. Consider the difference in mass measurements between two resolutions, say $h_1$ and $h_2$, at a fixed $\\epsilon$:\n$$\nM_{h_1}(\\epsilon) - M_{h_2}(\\epsilon) \\approx \\left( C\\epsilon^\\gamma + D h_1^r \\epsilon^\\beta \\right) - \\left( C\\epsilon^\\gamma + D h_2^r \\epsilon^\\beta \\right) = D \\epsilon^\\beta (h_1^r - h_2^r)\n$$\nSimilarly, for resolutions $h_2$ and $h_3$:\n$$\nM_{h_2}(\\epsilon) - M_{h_3}(\\epsilon) \\approx D \\epsilon^\\beta (h_2^r - h_3^r)\n$$\nTaking the ratio of these differences eliminates the unknown factors $D$ and $\\epsilon^\\beta$:\n$$\n\\frac{M_{h_1}(\\epsilon) - M_{h_2}(\\epsilon)}{M_{h_2}(\\epsilon) - M_{h_3}(\\epsilon)} \\approx \\frac{h_1^r - h_2^r}{h_2^r - h_3^r}\n$$\nThis \"ratio of differences\" should be approximately constant across all values of $\\epsilon_k$. We can therefore find the best-fit value of $r$ by minimizing the sum of squared residuals between the left-hand side (calculated from data) and the right-hand side (a function of $r$). This leads to the specified cost functional to be minimized over $r$:\n$$\nS(r) = \\sum_{k} \\left[ \\frac{M_{h_1}(\\epsilon_k) - M_{h_2}(\\epsilon_k)}{M_{h_2}(\\epsilon_k) - M_{h_3}(\\epsilon_k)} - \\frac{h_1^r - h_2^r}{h_2^r - h_3^r} \\right]^2\n$$\nMinimizing $S(r)$ yields our data-driven estimate, $\\hat{r}$.\n\n**Step 2: Richardson Extrapolation**\n\nWith an estimate $\\hat{r}$ for the convergence order, we can eliminate the leading-order error term and obtain a more accurate estimate of the continuum mass, $M(\\epsilon) = C\\epsilon^\\gamma$. This procedure is known as Richardson extrapolation. We have two approximate equations for the true mass $M(\\epsilon_k)$ at any given $\\epsilon_k$:\n$$\nM_{h_1}(\\epsilon_k) \\approx M(\\epsilon_k) + K_k h_1^{\\hat{r}}\n$$\n$$\nM_{h_2}(\\epsilon_k) \\approx M(\\epsilon_k) + K_k h_2^{\\hat{r}}\n$$\nwhere $K_k = D \\epsilon_k^\\beta$. This is a system of two linear equations for the two unknowns $M(\\epsilon_k)$ and $K_k$. Solving for $M(\\epsilon_k)$ gives the extrapolated value:\n$$\n\\widehat{M}(\\epsilon_k) = \\frac{h_2^{\\hat{r}} M_{h_1}(\\epsilon_k) - h_1^{\\hat{r}} M_{h_2}(\\epsilon_k)}{h_2^{\\hat{r}} - h_1^{\\hat{r}}}\n$$\nThis new estimate $\\widehat{M}(\\epsilon_k)$ has an error of order higher than $\\hat{r}$, meaning it is a significantly better approximation of the true continuum mass $M(\\epsilon_k)$. We apply this method to two pairs of resolutions, $(h_1, h_2)$ and $(h_2, h_3)$, to generate two extrapolated series, $\\widehat{M}(\\epsilon_k)$ and $\\widehat{M}'(\\epsilon_k)$. Averaging them provides a more robust estimate, $\\widehat{M}_{\\text{avg}}(\\epsilon_k)$, which reduces the influence of noise and higher-order errors.\n\n**Step 3: Final Fit and Diagnosis**\n\nWe now perform a linear least-squares fit on $\\log \\widehat{M}_{\\text{avg}}(\\epsilon_k)$ versus $\\log \\epsilon_k$. The slope of this fit, $\\hat{\\gamma}_{\\text{extrap}}$, is our best estimate of the true physical scaling exponent $\\gamma$, as it is derived from data where the dominant numerical artifact has been systematically removed.\n\nFinally, the diagnostic test compares this robust, extrapolated exponent with the naive exponents, $\\hat{\\gamma}_j$, obtained from single-resolution fits. The criterion\n$$\n\\max_{j \\in \\{1,2,3\\}} \\left| \\hat{\\gamma}_j - \\hat{\\gamma}_{\\text{extrap}} \\right| > \\tau\n$$\nwith a given threshold $\\tau=0.02$, provides a quantitative measure of the impact of truncation error. If this inequality holds, we conclude that the single-resolution analyses were significantly biased and that the truncation error did indeed mimic an additional relevant mode, distorting the observed scaling behavior. This procedure exemplifies the rigor required in computational physics to distinguish physical reality from numerical artifacts.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the results.\n    \"\"\"\n    # Set a fixed seed for the pseudorandom generator for reproducibility.\n    np.random.seed(42)\n\n    # Test Suite Definition\n    test_cases = [\n        # Case 1: happy path, moderate contamination\n        {\n            \"params\": {\"C\": 1.0, \"gamma\": 0.374, \"D\": 0.5, \"beta\": 0.20, \"r_true\": 2},\n            \"h\": [0.4, 0.2, 0.1],\n            \"eps_range\": (1e-6, 1e-2),\n            \"eps_num\": 20,\n            \"noise_sigma_factor\": 1e-8,\n            \"tau\": 0.02\n        },\n        # Case 2: edge case, strong contamination\n        {\n            \"params\": {\"C\": 1.0, \"gamma\": 0.374, \"D\": 5.0, \"beta\": 0.20, \"r_true\": 2},\n            \"h\": [0.8, 0.4, 0.2],\n            \"eps_range\": (5e-6, 1e-3),\n            \"eps_num\": 20,\n            \"noise_sigma_factor\": 5e-8,\n            \"tau\": 0.02\n        },\n        # Case 3: boundary case, higher-order scheme\n        {\n            \"params\": {\"C\": 1.0, \"gamma\": 0.374, \"D\": 0.3, \"beta\": 0.05, \"r_true\": 4},\n            \"h\": [0.5, 0.25, 0.125],\n            \"eps_range\": (1e-6, 1e-2),\n            \"eps_num\": 20,\n            \"noise_sigma_factor\": 1e-8,\n            \"tau\": 0.02\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # str(list) produces a string representation like '[...]', so joining them\n    # with ',' and wrapping with '[]' gives the desired format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef generate_data(params, h_vals, eps_range, eps_num, noise_sigma_factor):\n    \"\"\"\n    Generates synthetic mass data based on the provided model.\n    \"\"\"\n    C, gamma, D, beta, r_true = params[\"C\"], params[\"gamma\"], params[\"D\"], params[\"beta\"], params[\"r_true\"]\n    eps = np.logspace(np.log10(eps_range[0]), np.log10(eps_range[1]), eps_num)\n    \n    mass_data = []\n    for h in h_vals:\n        M_physical = C * eps**gamma\n        M_error = D * h**r_true * eps**beta\n        noise_sigma = noise_sigma_factor * M_physical\n        noise = np.random.normal(0, 1, size=eps.shape) * noise_sigma\n        \n        M_h = M_physical + M_error + noise\n        mass_data.append(M_h)\n        \n    return eps, mass_data\n\ndef fit_gamma(eps, mass):\n    \"\"\"\n    Performs an ordinary least-squares fit on log-log data to find the scaling exponent.\n    y = m*x + c where y=log(mass), x=log(eps), m=gamma\n    \"\"\"\n    log_eps = np.log(eps)\n    log_mass = np.log(mass)\n    \n    # Design matrix for linear regression (slope and intercept)\n    A = np.vstack([log_eps, np.ones(len(log_eps))]).T\n    \n    # Solve for [slope, intercept]\n    slope, _ = np.linalg.lstsq(A, log_mass, rcond=None)[0]\n    return slope\n\ndef estimate_r(h, mass_data, eps):\n    \"\"\"\n    Estimates the convergence order r by minimizing the functional S(r).\n    \"\"\"\n    h1, h2, h3 = h\n    M1, M2, M3 = mass_data\n    \n    def S(r):\n        numerator_M = M1 - M2\n        denominator_M = M2 - M3\n        \n        # Ratio of mass differences from data\n        valid_indices = np.abs(denominator_M) >= 1e-14\n        ratio_M = numerator_M[valid_indices] / denominator_M[valid_indices]\n        \n        # Ratio of resolution differences\n        numerator_h = h1**r - h2**r\n        denominator_h = h2**r - h3**r\n        if abs(denominator_h) < 1e-16: # Avoid division by zero for r=0\n            return np.inf\n        ratio_h = numerator_h / denominator_h\n        \n        # Sum of squared differences\n        return np.sum((ratio_M - ratio_h)**2)\n\n    # Minimize S(r) using a bounded scalar optimizer\n    res = minimize_scalar(S, bounds=(0.5, 8.0), method='bounded')\n    return res.x\n\ndef process_case(case_data):\n    \"\"\"\n    Executes the full analysis pipeline for a single test case.\n    \"\"\"\n    eps, mass_data = generate_data(\n        case_data[\"params\"], \n        case_data[\"h\"], \n        case_data[\"eps_range\"], \n        case_data[\"eps_num\"], \n        case_data[\"noise_sigma_factor\"]\n    )\n    \n    h1, h2, h3 = case_data[\"h\"]\n    M1, M2, M3 = mass_data\n\n    # 1. Compute naive apparent exponents\n    gamma_hat_1 = fit_gamma(eps, M1)\n    gamma_hat_2 = fit_gamma(eps, M2)\n    gamma_hat_3 = fit_gamma(eps, M3)\n    gamma_hats_naive = [gamma_hat_1, gamma_hat_2, gamma_hat_3]\n\n    # 2. Estimate convergence order r\n    r_hat = estimate_r(case_data[\"h\"], mass_data, eps)\n\n    # 3. Richardson extrapolation\n    hr2, hr1 = h2**r_hat, h1**r_hat\n    M_hat = (hr2 * M1 - hr1 * M2) / (hr2 - hr1)\n    \n    hr3, hr2 = h3**r_hat, h2**r_hat\n    M_hat_prime = (hr3 * M2 - hr2 * M3) / (hr3 - hr2)\n    \n    M_hat_avg = 0.5 * (M_hat + M_hat_prime)\n\n    # 4. Fit gamma from extrapolated data\n    gamma_hat_extrap = fit_gamma(eps, M_hat_avg)\n    \n    # 5. Diagnosis\n    max_discrepancy = max(abs(gamma_naive - gamma_hat_extrap) for gamma_naive in gamma_hats_naive)\n    diagnosis = max_discrepancy > case_data[\"tau\"]\n    \n    return [\n        round(gamma_hat_extrap, 6),\n        round(r_hat, 3),\n        diagnosis\n    ]\n\n# Execute the solution process.\nsolve()\n\n```", "id": "3471252"}]}