{"hands_on_practices": [{"introduction": "The first step in performance analysis is to translate an algorithm's structure into a mathematical cost model by carefully counting its fundamental operations. This foundational skill allows us to predict how runtime will scale with problem size long before writing or benchmarking code. This exercise [@problem_id:3503805] guides you through this process for a direct N-body simulation, teaching you to derive the asymptotic complexity and distinguish between the cost of searching for particle pairs and the cost of calculating their interactions.", "problem": "Consider $N$ point particles with masses $\\{m_i\\}_{i=1}^{N}$ distributed uniformly and independently in a cubic box of edge length $L$ (so volume $V = L^{3}$). The pairwise Newtonian gravitational potential energy between particles $i$ and $j$ separated by distance $r_{ij}$ is $-G m_i m_j / r_{ij}$, where $G$ is the gravitational constant. You implement the naive explicit loop-nest algorithm (no neighbor lists, no spatial indexing) to compute the total potential energy with a spherical cut-off radius $r_c$ such that only pairs with $r_{ij} \\le r_c$ contribute. The algorithm uses the following steps for each distinct pair $(i,j)$ with $i < j$:\n\n- Compute coordinate differences $\\Delta x = x_j - x_i$, $\\Delta y = y_j - y_i$, $\\Delta z = z_j - z_i$.\n- Form $r_{ij}^{2} = \\Delta x^{2} + \\Delta y^{2} + \\Delta z^{2}$.\n- If $r_{ij}^{2} \\le r_c^{2}$, then compute $r_{ij} = \\sqrt{r_{ij}^{2}}$, compute $1/r_{ij}$, and accumulate the contribution $-G m_i m_j / r_{ij}$ into the running total of the energy.\n\nAssume the following counting model for Floating-Point Operations (FLOPs): each subtraction, addition, multiplication, division, and square root counts as one FLOP; comparisons, branching, and memory accesses are not counted. Precomputation of $r_c^{2}$ outside the loops is not counted. Assume $r_c \\ll L$ so that boundary effects can be neglected, and assume all positions are independent and uniformly distributed.\n\nLet the number density be $\\eta = N/V$, and assume $\\eta$ is held fixed as $N$ varies (i.e., $L = (N/\\eta)^{1/3}$).\n\nStarting from this loop structure and the fundamental definitions above, derive a closed-form expression for the expected total FLOP count as a function of $N$, $r_c$, and $\\eta$, and then simplify it to its leading-order asymptotic form in $N$ and $r_c$ under the fixed-$\\eta$ assumption. Express your final answer as a single analytical expression in $N$, $r_c$, and $\\eta$. No numerical evaluation is required.", "solution": "The problem asks for the expected total number of Floating-Point Operations (FLOPs) required to compute the total gravitational potential energy of an $N$-particle system using a naive loop-nest algorithm with a cut-off radius $r_c$. The analysis proceeds in a series of logical steps.\n\nFirst, we identify the structure of the algorithm. It iterates through all distinct pairs of particles $(i,j)$ with $i<j$. The total number of such pairs is given by the binomial coefficient $\\binom{N}{2}$, which is equal to $\\frac{N(N-1)}{2}$.\n\nSecond, we determine the number of FLOPs executed for each pair. The operations are divided into two parts: an unconditional part executed for every pair, and a conditional part executed only for pairs satisfying the distance criterion $r_{ij} \\le r_c$.\n\nThe unconditional FLOPs, $F_{\\text{uncond}}$, are for calculating the squared distance $r_{ij}^2$:\n1.  Compute coordinate differences: $\\Delta x = x_j - x_i$, $\\Delta y = y_j - y_i$, $\\Delta z = z_j - z_i$. This requires $3$ subtractions.\n2.  Compute the squares of the differences: $\\Delta x^2$, $\\Delta y^2$, $\\Delta z^2$. This requires $3$ multiplications.\n3.  Sum the squares: $r_{ij}^2 = \\Delta x^2 + \\Delta y^2 + \\Delta z^2$. This requires $2$ additions.\nThe total unconditional FLOP count per pair is $F_{\\text{uncond}} = 3 + 3 + 2 = 8$.\n\nThe conditional FLOPs, $F_{\\text{cond}}$, are executed only if $r_{ij}^2 \\le r_c^2$:\n1.  Compute the distance: $r_{ij} = \\sqrt{r_{ij}^2}$. This is $1$ FLOP (square root).\n2.  Compute the reciprocal distance: $1/r_{ij}$. This is $1$ FLOP (division).\n3.  Compute the energy term $-G m_i m_j / r_{ij}$. This can be calculated as $(-G \\cdot m_i \\cdot m_j) \\cdot (1/r_{ij})$. This involves $3$ multiplications (assuming negation is a free operation, or can be folded into a multiplication).\n4.  Accumulate the energy into the running total. This is $1$ FLOP (addition).\nThe total conditional FLOP count per qualifying pair is $F_{\\text{cond}} = 1 + 1 + 3 + 1 = 6$.\n\nThird, we determine the expected number of pairs that satisfy the condition $r_{ij} \\le r_c$. Let $P(r_{ij} \\le r_c)$ be the probability that two randomly chosen particles $i$ and $j$ are separated by a distance less than or equal to $r_c$.\nThe particles are distributed uniformly and independently in a cubic box of volume $V=L^3$. The assumption $r_c \\ll L$ allows us to neglect boundary effects. Therefore, for any given particle $i$, the probability of another particle $j$ being located within a sphere of radius $r_c$ centered at particle $i$ is the ratio of the sphere's volume to the total volume of the box. The volume of this sphere is $V_{\\text{sphere}} = \\frac{4}{3}\\pi r_c^3$.\nThe probability is thus:\n$$ P(r_{ij} \\le r_c) = \\frac{V_{\\text{sphere}}}{V} = \\frac{\\frac{4}{3}\\pi r_c^3}{V} $$\nBy linearity of expectation, the expected number of pairs satisfying the condition, $\\mathbb{E}[N_{\\text{pairs}, r_c}]$, is the total number of pairs multiplied by this probability:\n$$ \\mathbb{E}[N_{\\text{pairs}, r_c}] = \\binom{N}{2} P(r_{ij} \\le r_c) = \\frac{N(N-1)}{2} \\frac{4\\pi r_c^3}{3V} $$\n\nFourth, we assemble the expression for the expected total FLOP count, $\\mathbb{E}[F_{\\text{total}}]$. It is the sum of the total unconditional FLOPs and the expected total conditional FLOPs.\n$$ \\mathbb{E}[F_{\\text{total}}] = \\binom{N}{2} F_{\\text{uncond}} + \\mathbb{E}[N_{\\text{pairs}, r_c}] F_{\\text{cond}} $$\nSubstituting the expressions derived above:\n$$ \\mathbb{E}[F_{\\text{total}}] = \\frac{N(N-1)}{2} \\cdot 8 + \\left( \\frac{N(N-1)}{2} \\frac{4\\pi r_c^3}{3V} \\right) \\cdot 6 $$\n$$ \\mathbb{E}[F_{\\text{total}}] = 4N(N-1) + \\frac{N(N-1)}{2} \\frac{24\\pi r_c^3}{3V} $$\n$$ \\mathbb{E}[F_{\\text{total}}] = 4N(N-1) + N(N-1) \\frac{4\\pi r_c^3}{V} $$\n\nFifth, we use the given constraint that the number density $\\eta = N/V$ is held fixed. This implies $V = N/\\eta$. Substituting this into the expression for $\\mathbb{E}[F_{\\text{total}}]$:\n$$ \\mathbb{E}[F_{\\text{total}}] = 4N(N-1) + N(N-1) \\frac{4\\pi r_c^3}{N/\\eta} $$\n$$ \\mathbb{E}[F_{\\text{total}}] = 4N(N-1) + (N-1) 4\\pi r_c^3 \\eta $$\nThis is the exact closed-form expression for the expected total FLOP count.\n\nFinally, we simplify this expression to its leading-order asymptotic form for large $N$. For large $N$, we can make the approximations $N-1 \\approx N$ and $N(N-1) = N^2 - N \\approx N^2$. The expression has two main parts: the pair-search cost and the interaction-calculation cost. We find the leading-order term for each part separately.\nThe unconditional part becomes:\n$$ 4N(N-1) \\approx 4N^2 $$\nThe conditional part becomes:\n$$ (N-1) 4\\pi r_c^3 \\eta \\approx N \\cdot 4\\pi r_c^3 \\eta $$\nCombining these gives the leading-order asymptotic form of the total expected FLOP count. This form retains the dependence on all specified parameters ($N$, $r_c$, $\\eta$) and correctly captures the scaling behavior of both components of the algorithm.\n$$ \\mathbb{E}[F_{\\text{total}}]_{\\text{asymptotic}} = 4N^2 + 4\\pi \\eta r_c^3 N $$\nThis expression clearly shows that the overall cost is dominated by the $\\mathcal{O}(N^2)$ term for searching all pairs, while the cost of computing the interactions within the cutoff radius scales as $\\mathcal{O}(N)$.", "answer": "$$\\boxed{4N^{2} + 4\\pi\\eta r_{c}^{3}N}$$", "id": "3503805"}, {"introduction": "An algorithm's theoretical complexity in floating-point operations (FLOPs) does not tell the whole story; on modern hardware, performance is often dictated by the cost of moving data between memory and the processor. The roofline model provides a powerful framework for this analysis by relating computational throughput to memory bandwidth via a kernel's arithmetic intensity. This practice [@problem_id:3503871] challenges you to apply this model to a common finite-difference stencil, revealing how implementation choices like cache-blocking can dramatically shift a kernel from being memory-bound to compute-bound.", "problem": "A three-dimensional uniform Cartesian grid of size $n^3$ is used to update a scalar field $\\phi$ representing a gravitational potential surrogate in a computational astrophysics code. The numerical update uses an anisotropic seven-point finite-difference stencil with distinct directional weights due to non-uniform effective coupling in each principal direction. For each interior grid point $(i,j,k)$, the update is computed out-of-place as\n$$\n\\phi_{\\text{new}}(i,j,k) = w_{0}\\,\\phi(i,j,k) + w_{x^{+}}\\,\\phi(i+1,j,k) + w_{x^{-}}\\,\\phi(i-1,j,k) + w_{y^{+}}\\,\\phi(i,j+1,k) + w_{y^{-}}\\,\\phi(i,j-1,k) + w_{z^{+}}\\,\\phi(i,j,k+1) + w_{z^{-}}\\,\\phi(i,j,k-1).\n$$\nAll weights $w_{\\alpha}$ are constants across the grid but can be distinct. Arithmetic is performed in double precision.\n\nConsider two implementations of a single sweep (one update at every interior point):\n\n- Naive implementation: a straightforward triple-nested loop with poor spatial and temporal locality. For each interior update, assume the code issues seven loads from Dynamic Random-Access Memory (DRAM) for the seven input values and one store for the output. The store uses a write-allocate policy, so the cache line is allocated from DRAM before the write, incurring one double-precision read for the destination followed by one double-precision write.\n\n- Blocked implementation: a cache-blocked version that tiles the grid into cubic tiles of size $t^3$ that fit into the last-level cache. Within a tile, input values are reused from cache so that each input element is fetched from DRAM only once for its contribution to its own update. The output is written using non-temporal (streaming) stores, avoiding write-allocate. Ignore the contribution of halo exchanges between tiles by assuming $n \\gg t$ and focus on the interior points of each tile.\n\nAssume the following machine characteristics for double precision on a single Graphics Processing Unit (GPU): peak floating-point performance $P_{\\text{peak}} = 150 \\times 10^{9}$ floating-point operations per second and sustained main-memory bandwidth $B_{\\text{w}} = 250 \\times 10^{9}$ bytes per second.\n\nStarting from first principles of operation counting and memory traffic, determine:\n\n1. The arithmetic intensity $I_{\\text{naive}}$ and $I_{\\text{blocked}}$ in floating-point operations per byte for one interior update in the naive and blocked implementations, respectively.\n\n2. The attainable performance for each case under the roofline reasoning, expressed as gigaflops per second, and the corresponding performance regime (memory-bound versus compute-bound) implied by the comparison between arithmetic intensity and the ratio $P_{\\text{peak}}/B_{\\text{w}}$.\n\nProvide your final numerical values for $I_{\\text{naive}}$, $I_{\\text{blocked}}$, the attainable performance for the naive implementation, and the attainable performance for the blocked implementation, in that order. Round your answers to four significant figures. Express the attainable performance values in gigaflops per second (GFLOP/s).", "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, and complete. It describes a standard computational science scenario involving finite-difference methods and performance analysis using the roofline model, which are established concepts in high-performance computing. All necessary parameters and assumptions are provided. I will now proceed with a formal solution.\n\nThe core of this problem lies in applying the roofline model, which provides an upper bound on the attainable performance of a computational kernel. The performance, $P$, measured in floating-point operations per second (FLOP/s), is limited by both the peak floating-point performance of the hardware, $P_{\\text{peak}}$, and the sustained main-memory bandwidth, $B_{\\text{w}}$. The relationship is given by:\n$$\nP_{\\text{attainable}} = \\min(P_{\\text{peak}}, I \\times B_{\\text{w}})\n$$\nwhere $I$ is the arithmetic intensity of the algorithm, defined as the ratio of floating-point operations performed to the total bytes of data moved between main memory and the processor's caches.\n$$\nI = \\frac{\\text{Floating-Point Operations (FLOPs)}}{\\text{Memory Traffic (Bytes)}}\n$$\nA data value of type double precision occupies $8$ bytes.\n\nThe machine characteristics are given as:\n- Peak performance, $P_{\\text{peak}} = 150 \\times 10^{9}$ FLOP/s.\n- Memory bandwidth, $B_{\\text{w}} = 250 \\times 10^{9}$ bytes/s.\n\nFrom these values, we can calculate the machine balance, or critical intensity, $I_{\\text{critical}}$, which is the threshold that separates memory-bound from compute-bound regimes:\n$$\nI_{\\text{critical}} = \\frac{P_{\\text{peak}}}{B_{\\text{w}}} = \\frac{150 \\times 10^{9} \\, \\text{FLOP/s}}{250 \\times 10^{9} \\, \\text{bytes/s}} = \\frac{150}{250} \\frac{\\text{FLOP}}{\\text{byte}} = 0.6 \\, \\text{FLOP/byte}\n$$\nIf an algorithm's arithmetic intensity $I  I_{\\text{critical}}$, its performance is limited by memory bandwidth ($P = I \\times B_{\\text{w}}$). If $I > I_{\\text{critical}}$, its performance is limited by the peak computational rate ($P = P_{\\text{peak}}$).\n\nWe will now analyze each implementation for a single interior point update.\n\n**1. Analysis of the Naive Implementation**\n\nFirst, we determine the number of floating-point operations (FLOPs) for one update. The update rule is:\n$$\n\\phi_{\\text{new}}(i,j,k) = w_{0}\\,\\phi(i,j,k) + w_{x^{+}}\\,\\phi(i+1,j,k) + w_{x^{-}}\\,\\phi(i-1,j,k) + w_{y^{+}}\\,\\phi(i,j+1,k) + w_{y^{-}}\\,\\phi(i,j-1,k) + w_{z^{+}}\\,\\phi(i,j,k+1) + w_{z^{-}}\\,\\phi(i,j,k-1)\n$$\nThis computation involves $7$ multiplications and $6$ additions. Therefore, the total number of FLOPs per update is $7 + 6 = 13$ FLOPs.\n\nNext, we determine the memory traffic. For each interior point update:\n- There are $7$ loads from DRAM for the input values of $\\phi$. Since each is a double-precision value, the load traffic is $7 \\times 8 = 56$ bytes.\n- There is $1$ store for the output value $\\phi_{\\text{new}}$. The store uses a write-allocate policy, which means the cache line for the destination is first read from DRAM before being modified and written. This incurs $1$ read of a double-precision value and $1$ write of a double-precision value. The store traffic is therefore $(1+1) \\times 8 = 16$ bytes.\n- The total memory traffic, $M_{\\text{naive}}$, is the sum of the load and store traffic: $M_{\\text{naive}} = 56 + 16 = 72$ bytes.\n\nThe arithmetic intensity for the naive implementation, $I_{\\text{naive}}$, is:\n$$\nI_{\\text{naive}} = \\frac{13 \\, \\text{FLOPs}}{72 \\, \\text{bytes}} \\approx 0.180556 \\, \\text{FLOP/byte}\n$$\nComparing this to the critical intensity, we find $I_{\\text{naive}} \\approx 0.1806  I_{\\text{critical}} = 0.6$. The naive implementation is therefore **memory-bound**.\n\nThe attainable performance, $P_{\\text{naive}}$, is limited by the memory bandwidth:\n$$\nP_{\\text{naive}} = I_{\\text{naive}} \\times B_{\\text{w}} = \\frac{13}{72} \\times (250 \\times 10^{9}) \\, \\text{FLOP/s} = \\frac{3250}{72} \\times 10^{9} \\, \\text{FLOP/s} \\approx 45.1389 \\times 10^{9} \\, \\text{FLOP/s}\n$$\nExpressed in GFLOP/s and rounded to four significant figures, this is $45.14$ GFLOP/s.\n\n**2. Analysis of the Blocked Implementation**\n\nThe number of FLOPs per update remains $13$.\n\nFor the blocked implementation, we analyze the memory traffic under the assumption of perfect data reuse within a cache-resident tile. This is an amortized analysis over the points in a large tile, as specified by $n \\gg t$.\n- Each input element $\\phi(i,j,k)$ is loaded from DRAM once. This contributes to the update of its own location and its $6$ neighbors. After being used for all relevant updates within the tile, it is not needed again. Amortized per update, this corresponds to $1$ read from DRAM.\n- The output value $\\phi_{\\text{new}}(i,j,k)$ is written to DRAM once using non-temporal (streaming) stores, which bypass the cache and avoid the read-on-write penalty of write-allocate. Amortized per update, this corresponds to $1$ write to DRAM.\n- The total memory traffic, $M_{\\text{blocked}}$, is the sum of $1$ amortized read and $1$ amortized write. Each is a double-precision value.\n- Total memory traffic: $M_{\\text{blocked}} = (1 \\times 8) + (1 \\times 8) = 16$ bytes.\n\nThe arithmetic intensity for the blocked implementation, $I_{\\text{blocked}}$, is:\n$$\nI_{\\text{blocked}} = \\frac{13 \\, \\text{FLOPs}}{16 \\, \\text{bytes}} = 0.8125 \\, \\text{FLOP/byte}\n$$\nComparing this to the critical intensity, we find $I_{\\text{blocked}} = 0.8125 > I_{\\text{critical}} = 0.6$. The blocked implementation is therefore **compute-bound**.\n\nThe attainable performance, $P_{\\text{blocked}}$, is limited by the peak floating-point performance of the hardware:\n$$\nP_{\\text{blocked}} = P_{\\text{peak}} = 150 \\times 10^{9} \\, \\text{FLOP/s} = 150 \\, \\text{GFLOP/s}\n$$\n\n**Summary of Results**\n\n- $I_{\\text{naive}} = \\frac{13}{72} \\approx 0.1806$ FLOP/byte.\n- $I_{\\text{blocked}} = \\frac{13}{16} = 0.8125$ FLOP/byte.\n- $P_{\\text{naive}} \\approx 45.14$ GFLOP/s (memory-bound).\n- $P_{\\text{blocked}} = 150.0$ GFLOP/s (compute-bound, written to four significant figures).\n\nThe final numerical answers are requested in the order: $I_{\\text{naive}}$, $I_{\\text{blocked}}$, $P_{\\text{naive}}$ (in GFLOP/s), and $P_{\\text{blocked}}$ (in GFLOP/s), with values rounded to four significant figures.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.1806  0.8125  45.14  150.0 \\end{pmatrix}}\n$$", "id": "3503871"}, {"introduction": "When parallelizing code across multiple processors, the resulting speedup is rarely perfect due to portions of the algorithm that must remain serial. Amdahl's Law and Gustafson's Law offer two essential, and starkly contrasting, perspectives for predicting the efficiency and potential of parallel algorithms. This final exercise [@problem_id:3503847] asks you to apply both models to a parallel Barnes-Hut N-body code, highlighting the crucial difference between scaling a problem of fixed size versus scaling the problem size to fit a fixed time budget.", "problem": "A Barnes–Hut $N$-body time step on a uniform particle distribution is profiled as follows: a parallelizable tree-walk accounts for fraction $f_{p} = 0.95$ of the total single-node runtime, while serial time-integration and overheads account for fraction $s = 0.05$. Consider a parallel system with $p = 64$ identical nodes and ideal load balance on the tree-walk. Use the core definitions of speedup and efficiency, namely $S_{p} = T_{1}/T_{p}$ and $E_{p} = S_{p}/p$, without invoking any pre-stated scaling formulas. Starting from these definitions and the assumptions appropriate to each model, derive the model-predicted strong-scaling efficiency under two interpretations:\n- Fixed total work (Amdahl’s fixed-size viewpoint).\n- Scaled total work at fixed wall-clock budget (Gustafson–Barsis scaled-speedup viewpoint).\nCompute both efficiencies for $s = 0.05$ and $p = 64$. Round both values to four significant figures and express them as decimal fractions. Report your final numerical pair as a row matrix in the order $(\\text{Amdahl}, \\text{Gustafson})$. In one or two sentences, reconcile why the two predictions differ given the same $s$ and $p$.", "solution": "The problem is validated as self-contained, scientifically grounded in the principles of parallel computing, and well-posed. All necessary information and definitions are provided.\n\nLet $T_{1}$ be the total execution time of the $N$-body time step on a single processor. The problem states that a fraction $s$ of this time is purely serial and a fraction $f_{p}$ is perfectly parallelizable. We are given $s = 0.05$ and $f_{p} = 0.95$. Note that $s + f_{p} = 0.05 + 0.95 = 1$. The time spent on the serial part is $s \\cdot T_{1}$ and the time spent on the parallelizable part is $f_{p} \\cdot T_{1}$. The number of processors is $p=64$. The definitions for speedup and efficiency are given as $S_{p} = T_{1}/T_{p}$ and $E_{p} = S_{p}/p$, where $T_{p}$ is the execution time on $p$ processors.\n\nWe derive the efficiency under two distinct models: Amdahl's fixed-size model and Gustafson-Barsis's scaled-size model.\n\n**1. Amdahl's Law (Fixed Total Work)**\n\nThis model assumes that the total amount of work to be done is fixed, regardless of the number of processors used. The serial portion of the work cannot be parallelized, while the parallelizable portion is divided equally among the $p$ processors, assuming ideal load balance.\n\nThe time to execute the program on $p$ processors, $T_{p}$, is the sum of the time for the serial part and the time for the parallelized part.\n$$ T_{p} = (\\text{serial time}) + (\\text{parallel time on } p \\text{ processors}) $$\n$$ T_{p} = s \\cdot T_{1} + \\frac{f_{p} \\cdot T_{1}}{p} $$\nFactoring out $T_{1}$, we get:\n$$ T_{p} = T_{1} \\left( s + \\frac{f_{p}}{p} \\right) $$\nNow, we use the definition of speedup, $S_{p} = T_{1}/T_{p}$:\n$$ S_{p} = \\frac{T_{1}}{T_{1} \\left( s + \\frac{f_{p}}{p} \\right)} = \\frac{1}{s + \\frac{f_{p}}{p}} $$\nUsing $f_{p} = 1 - s$, this is the classic Amdahl's Law:\n$$ S_{p} = \\frac{1}{s + \\frac{1-s}{p}} $$\nThe efficiency, $E_{p}$, is defined as $S_{p}/p$:\n$$ E_{p, \\text{Amdahl}} = \\frac{S_{p}}{p} = \\frac{1}{p \\left( s + \\frac{1-s}{p} \\right)} = \\frac{1}{p \\cdot s + (1-s)} $$\nSubstituting the given values $s = 0.05$ and $p = 64$:\n$$ E_{p, \\text{Amdahl}} = \\frac{1}{64 \\cdot 0.05 + (1-0.05)} = \\frac{1}{3.2 + 0.95} = \\frac{1}{4.15} $$\n$$ E_{p, \\text{Amdahl}} \\approx 0.240963855... $$\nRounding to four significant figures, we get $0.2410$.\n\n**2. Gustafson–Barsis Law (Scaled Total Work)**\n\nThis model assumes that the total amount of work to be done scales with the number of processors, such that the wall-clock time remains fixed. Specifically, the parallelizable portion of the workload is scaled by a factor of $p$.\n\nLet the runtime on a parallel system with $p$ processors be normalized to $1$ unit of time. This time is composed of the serial fraction $s$ and the parallel fraction $f_p$:\n$$ T_{p} = s + f_{p} = 1 $$\nIn this model, the total workload is scaled. The serial work remains $s$, but the parallel work is increased by a factor of $p$. The total work for this scaled problem, if it were to be run on a single processor, would take time $T_{1}'$.\n$$ T_{1}' = (\\text{serial work}) + (\\text{scaled parallel work}) = s + p \\cdot f_{p} $$\nThe scaled speedup, according to the Gustafson-Barsis viewpoint, is the ratio of the single-processor time for the scaled problem, $T_{1}'$, to the multi-processor time for that same problem, $T_{p}$.\n$$ S_{p, \\text{Gustafson}} = \\frac{T_{1}'}{T_{p}} = \\frac{s + p \\cdot f_{p}}{s + f_{p}} = \\frac{s + p \\cdot f_{p}}{1} = s + p \\cdot f_{p} $$\nUsing $f_{p} = 1 - s$, we can also write this as $s + p(1-s) = p - s(p-1)$.\nThe efficiency is again $E_{p} = S_{p}/p$:\n$$ E_{p, \\text{Gustafson}} = \\frac{S_{p, \\text{Gustafson}}}{p} = \\frac{s + p \\cdot f_{p}}{p} = \\frac{s}{p} + f_{p} $$\nSubstituting the given values $s = 0.05$, $f_{p} = 0.95$ and $p = 64$:\n$$ E_{p, \\text{Gustafson}} = \\frac{0.05}{64} + 0.95 $$\n$$ E_{p, \\text{Gustafson}} = 0.00078125 + 0.95 = 0.95078125 $$\nRounding to four significant figures, we get $0.9508$.\n\n**Reconciliation**\n\nThe two predictions differ because Amdahl's law models a fixed-size problem where the fixed-time serial fraction becomes an increasingly dominant bottleneck as processors are added. Gustafson's law, conversely, models a scaled-size problem where the parallel workload grows, diminishing the relative impact of the fixed-time serial portion and thus maintaining high efficiency.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.2410  0.9508\n\\end{pmatrix}\n}\n$$", "id": "3503847"}]}