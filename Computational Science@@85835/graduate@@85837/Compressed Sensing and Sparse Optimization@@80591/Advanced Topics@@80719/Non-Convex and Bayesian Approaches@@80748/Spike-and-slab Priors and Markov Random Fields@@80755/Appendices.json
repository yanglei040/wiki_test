{"hands_on_practices": [{"introduction": "A core task in structured signal processing is to find the most probable underlying configuration given noisy observations. When the model's structure is a simple chain, as is common in time-series or genomic data, we can solve this Maximum A Posteriori (MAP) estimation problem exactly and efficiently using dynamic programming. This first exercise [@problem_id:3480135] will guide you through this fundamental algorithm, demonstrating how it optimally balances local evidence with the structural constraints imposed by the Markov random field prior. By comparing the result to a simpler, greedy approach, you will gain a concrete understanding of how the model leverages pairwise interactions to enforce continuity and overcome local noise.", "problem": "Consider a compressed sensing model with a Spike-and-Slab prior over coefficients. Let $y \\in \\mathbb{R}^{n}$ be measurements obeying $y = A x + \\epsilon$, where $A \\in \\mathbb{R}^{n \\times p}$ is a known sensing matrix, $x \\in \\mathbb{R}^{p}$ are unknown coefficients, and $\\epsilon \\sim \\mathcal{N}(0, \\sigma^{2} I)$ is Gaussian noise. Introduce binary latent variables $z_{i} \\in \\{0,1\\}$ encoding sparsity via the Spike-and-Slab prior $p(x_{i} \\mid z_{i}) = (1 - z_{i}) \\delta(x_{i}) + z_{i} \\,\\mathcal{N}(x_{i} \\mid 0, \\tau^{2})$. Assume a one-dimensional chain Markov Random Field (MRF) prior on $z_{1},\\dots,z_{p}$ with pairwise potential encouraging contiguity,\n$$\n\\phi(z_{i}, z_{i+1}) \\triangleq w \\, |z_{i} - z_{i+1}|,\\quad w > 0,\n$$\nand unary potentials $\\psi_{i}(z_{i})$ derived from the local evidence of $y$ under the likelihood and prior. Up to an additive constant independent of $z$, the posterior energy takes the form\n$$\nE(z_{1},\\dots,z_{p}) \\triangleq \\sum_{i=1}^{p} \\theta_{i} z_{i} \\;+\\; w \\sum_{i=1}^{p-1} |z_{i} - z_{i+1}|,\n$$\nwhere $\\theta_{i} \\in \\mathbb{R}$ are fixed unary costs summarizing local evidence (e.g., negative log Bayes factors favoring $z_{i}=1$ over $z_{i}=0$).\n\nFor a one-dimensional chain of length $p=5$ with homogeneous pairwise weight $w=1$ and unary costs\n$$\n\\theta_{1} = -0.6,\\quad \\theta_{2} = -0.4,\\quad \\theta_{3} = 0.2,\\quad \\theta_{4} = -0.5,\\quad \\theta_{5} = -0.3,\n$$\nperform the following:\n\n1. Derive, from first principles of dynamic programming (DP) on a chain Markov Random Field (MRF), a recurrence that computes the minimum energy to index $i$ when $z_{i}$ is fixed to $0$ or $1$. Use this to compute the Maximum A Posteriori (MAP) configuration $z^{\\star} \\in \\{0,1\\}^{5}$ that minimizes $E(z)$, and show the backtracking steps that recover $z^{\\star}$.\n\n2. Define independent thresholding that ignores pairwise interactions by minimizing $\\sum_{i=1}^{5} \\theta_{i} z_{i}$ site-wise. Compute the thresholded configuration $z^{\\mathrm{thr}}$ and compare its energy $E(z^{\\mathrm{thr}})$ to $E(z^{\\star})$, explaining any differences in terms of the contiguity penalty.\n\nExpress your final MAP configuration as a binary row matrix with five entries using LaTeX. No rounding is needed.", "solution": "The problem is valid. It presents a well-posed MAP inference task on a chain Markov Random Field, a standard problem in statistical signal processing and machine learning. All parameters are provided, and the objective is clearly defined. We will proceed with the solution in two parts as requested.\n\nThe energy function to be minimized is given by:\n$$\nE(z_{1},\\dots,z_{p}) = \\sum_{i=1}^{p} \\theta_{i} z_{i} + w \\sum_{i=1}^{p-1} |z_{i} - z_{i+1}|\n$$\nWith the given parameters $p=5$, $w=1$, and $\\theta = (-0.6, -0.4, 0.2, -0.5, -0.3)$, the energy for a configuration $z = (z_1, z_2, z_3, z_4, z_5)$ is:\n$$\nE(z) = \\sum_{i=1}^{5} \\theta_{i} z_{i} + \\sum_{i=1}^{4} |z_{i} - z_{i+1}|\n$$\nThe pairwise cost $|z_i - z_{i+1}|$ is $1$ if $z_i \\neq z_{i+1}$ (a change in state) and $0$ if $z_i = z_{i+1}$ (state remains the same).\n\n### Part 1: Dynamic Programming for MAP Inference\n\nWe seek the Maximum A Posteriori (MAP) configuration $z^{\\star} = \\arg\\min_{z \\in \\{0,1\\}^5} E(z)$. Since the underlying graphical model is a chain, this can be solved efficiently using dynamic programming (specifically, the min-sum or Viterbi algorithm).\n\nLet $M_i(k)$ be the minimum energy of a partial configuration $(z_1, \\dots, z_i)$ ending with $z_i = k$, where $k \\in \\{0, 1\\}$. The total energy is additive, so we can establish a recurrence. The energy up to site $i$ with $z_i=k$ is the unary cost at $i$ ($\\theta_i k$) plus the minimum possible energy of a path ending at $i-1$ plus the transition cost from $z_{i-1}$ to $z_i$.\n\nThe recurrence relation is:\n$$\nM_i(k) = \\theta_i k + \\min_{j \\in \\{0,1\\}} \\left( M_{i-1}(j) + w|k-j| \\right)\n$$\nSince $w=1$, we can write the recurrences for $k=0$ and $k=1$ explicitly:\n$$\nM_i(0) = \\theta_i \\cdot 0 + \\min \\left( M_{i-1}(0) + |0-0|, M_{i-1}(1) + |0-1| \\right) = \\min(M_{i-1}(0), M_{i-1}(1) + 1)\n$$\n$$\nM_i(1) = \\theta_i \\cdot 1 + \\min \\left( M_{i-1}(0) + |1-0|, M_{i-1}(1) + |1-1| \\right) = \\theta_i + \\min(M_{i-1}(0) + 1, M_{i-1}(1))\n$$\nTo recover the optimal path, we also store backpointers $\\psi_i(k)$ which record the state $j \\in \\{0,1\\}$ at step $i-1$ that yielded the minimum cost for $M_i(k)$:\n$$\n\\psi_i(k) = \\arg\\min_{j \\in \\{0,1\\}} \\left( M_{i-1}(j) + w|k-j| \\right)\n$$\n\n**Forward Pass:**\n\n**Initialization ($i=1$):**\nThe energy of a path of length $1$ is just the unary cost.\n- $M_1(0) = \\theta_1 \\cdot 0 = 0$\n- $M_1(1) = \\theta_1 \\cdot 1 = -0.6$\n\n**Step $i=2$ ($\\theta_2 = -0.4$):**\n- $M_2(0) = \\min(M_1(0), M_1(1) + 1) = \\min(0, -0.6 + 1) = \\min(0, 0.4) = 0$. The minimum came from $M_1(0)$, so $\\psi_2(0) = 0$.\n- $M_2(1) = \\theta_2 + \\min(M_1(0) + 1, M_1(1)) = -0.4 + \\min(0+1, -0.6) = -0.4 - 0.6 = -1.0$. The minimum came from $M_1(1)$, so $\\psi_2(1) = 1$.\n\n**Step $i=3$ ($\\theta_3 = 0.2$):**\n- $M_3(0) = \\min(M_2(0), M_2(1) + 1) = \\min(0, -1.0 + 1) = \\min(0, 0) = 0$. The minimum is achieved by both paths. We can choose $\\psi_3(0) = 0$ by convention.\n- $M_3(1) = \\theta_3 + \\min(M_2(0) + 1, M_2(1)) = 0.2 + \\min(0+1, -1.0) = 0.2 - 1.0 = -0.8$. The minimum came from $M_2(1)$, so $\\psi_3(1) = 1$.\n\n**Step $i=4$ ($\\theta_4 = -0.5$):**\n- $M_4(0) = \\min(M_3(0), M_3(1) + 1) = \\min(0, -0.8 + 1) = \\min(0, 0.2) = 0$. The minimum came from $M_3(0)$, so $\\psi_4(0) = 0$.\n- $M_4(1) = \\theta_4 + \\min(M_3(0) + 1, M_3(1)) = -0.5 + \\min(0+1, -0.8) = -0.5 - 0.8 = -1.3$. The minimum came from $M_3(1)$, so $\\psi_4(1) = 1$.\n\n**Step $i=5$ ($\\theta_5 = -0.3$):**\n- $M_5(0) = \\min(M_4(0), M_4(1) + 1) = \\min(0, -1.3 + 1) = \\min(0, -0.3) = -0.3$. The minimum came from $M_4(1)$, so $\\psi_5(0) = 1$.\n- $M_5(1) = \\theta_5 + \\min(M_4(0) + 1, M_4(1)) = -0.3 + \\min(0+1, -1.3) = -0.3 - 1.3 = -1.6$. The minimum came from $M_4(1)$, so $\\psi_5(1) = 1$.\n\n**Backtracking:**\n\nThe minimum total energy for the entire chain is $E(z^{\\star}) = \\min(M_5(0), M_5(1)) = \\min(-0.3, -1.6) = -1.6$.\nThe optimal state at the final position is $z^{\\star}_5 = \\arg\\min_{k \\in \\{0,1\\}} M_5(k) = 1$.\n\nNow we trace back using the stored pointers $\\psi$ to find the optimal path:\n- $z^{\\star}_5 = 1$\n- $z^{\\star}_4 = \\psi_5(z^{\\star}_5) = \\psi_5(1) = 1$\n- $z^{\\star}_3 = \\psi_4(z^{\\star}_4) = \\psi_4(1) = 1$\n- $z^{\\star}_2 = \\psi_3(z^{\\star}_3) = \\psi_3(1) = 1$\n- $z^{\\star}_1 = \\psi_2(z^{\\star}_2) = \\psi_2(1) = 1$\n\nThus, the MAP configuration is $z^{\\star} = (1, 1, 1, 1, 1)$.\n\n### Part 2: Independent Thresholding and Comparison\n\nIndependent thresholding ignores the pairwise interaction term $w \\sum |z_i - z_{i+1}|$ and minimizes only the sum of unary terms, $E_{\\mathrm{thr}}(z) = \\sum_{i=1}^5 \\theta_i z_i$. This sum is an uncoupled optimization, so we can minimize each term $\\theta_i z_i$ independently. For each site $i$, the optimal choice $z^{\\mathrm{thr}}_i$ is:\n$$\nz^{\\mathrm{thr}}_i =\n\\begin{cases}\n1 & \\text{if } \\theta_i < 0 \\\\\n0 & \\text{if } \\theta_i \\ge 0\n\\end{cases}\n$$\nApplying this rule to the given unary costs:\n- $\\theta_1 = -0.6 < 0 \\implies z^{\\mathrm{thr}}_1 = 1$\n- $\\theta_2 = -0.4 < 0 \\implies z^{\\mathrm{thr}}_2 = 1$\n- $\\theta_3 = 0.2 > 0 \\implies z^{\\mathrm{thr}}_3 = 0$\n- $\\theta_4 = -0.5 < 0 \\implies z^{\\mathrm{thr}}_4 = 1$\n- $\\theta_5 = -0.3 < 0 \\implies z^{\\mathrm{thr}}_5 = 1$\n\nThe thresholded configuration is $z^{\\mathrm{thr}} = (1, 1, 0, 1, 1)$.\n\nNow, we compute the energy of this configuration using the *full* energy function $E(z)$:\n$$\nE(z^{\\mathrm{thr}}) = \\sum_{i=1}^{5} \\theta_{i} z^{\\mathrm{thr}}_{i} + \\sum_{i=1}^{4} |z^{\\mathrm{thr}}_{i} - z^{\\mathrm{thr}}_{i+1}|\n$$\nThe unary term contribution is:\n$$\n\\sum \\theta_i z^{\\mathrm{thr}}_i = (-0.6)\\cdot 1 + (-0.4)\\cdot 1 + (0.2)\\cdot 0 + (-0.5)\\cdot 1 + (-0.3)\\cdot 1 = -0.6 - 0.4 - 0.5 - 0.3 = -1.8\n$$\nThe pairwise term contribution (contiguity penalty) is:\n$$\n\\sum |z^{\\mathrm{thr}}_i - z^{\\mathrm{thr}}_{i+1}| = |1-1| + |1-0| + |0-1| + |1-1| = 0 + 1 + 1 + 0 = 2\n$$\nThe total energy is $E(z^{\\mathrm{thr}}) = -1.8 + 2 = 0.2$.\n\n**Comparison and Explanation:**\n- MAP energy: $E(z^{\\star}) = E(1,1,1,1,1) = (-0.6 - 0.4 + 0.2 - 0.5 - 0.3) + 0 = -1.6$.\n- Thresholding energy: $E(z^{\\mathrm{thr}}) = E(1,1,0,1,1) = 0.2$.\n\nAs expected, $E(z^{\\star})  E(z^{\\mathrm{thr}})$ since $z^{\\star}$ is the global minimizer. The independent thresholding solution $z^{\\mathrm{thr}}$ achieves a lower unary energy ($-1.8$) compared to the MAP solution's unary energy ($-1.6$). However, $z^{\\mathrm{thr}}$ pays a significant penalty ($2$) from the pairwise term for breaking contiguity at index $i=3$. The MAP solution $z^{\\star}$ pays a small price in the unary term (by choosing $z_3=1$ despite $\\theta_3=0.2 > 0$) in order to achieve a zero pairwise penalty. The trade-off is clear at $i=3$: setting $z_3=0$ (with neighbors being $1$) adds $(\\theta_3\\cdot 0) + |z_2-z_3| + |z_3-z_4| = 0 + |1-0|+|0-1| = 2$ to the energy, while setting $z_3=1$ adds $(\\theta_3\\cdot 1) + |z_2-z_3|+|z_3-z_4| = 0.2 + |1-1|+|1-1| = 0.2$. Since $0.2  2$, the MAP solution correctly chooses $z_3=1$, demonstrating how the pairwise potential enforces structural preferences (contiguity) over local, greedy decisions.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1  1  1  1  1 \\end{pmatrix}}\n$$", "id": "3480135"}, {"introduction": "While dynamic programming provides an elegant solution for chain-structured problems, most real-world scenarios involve more complex, loopy graphs where finding the exact MAP estimate is computationally intractable. In these cases, we often turn to local search algorithms like Iterated Conditional Modes (ICM), but they risk getting trapped in suboptimal local minima. This practice [@problem_id:3480144] introduces a powerful continuation method, also known as homotopy, which guides the local search from a simpler problem to the complex target, significantly improving the chances of finding the global optimum. Through this implementation, you will explore the stability of this optimization path and analyze the 'ruggedness' of the posterior landscape.", "problem": "Consider the linear observation model in compressed sensing with a spike-and-slab prior and a Markov Random Field (MRF) on the support indicators. Let $y \\in \\mathbb{R}^{m}$ be generated as $y = A x + \\varepsilon$, where $A \\in \\mathbb{R}^{m \\times n}$ is a known design matrix, $x \\in \\mathbb{R}^{n}$ is the unknown sparse signal, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$ is Gaussian noise with variance parameter $\\sigma^2  0$. The spike-and-slab prior introduces binary support indicators $z \\in \\{0,1\\}^{n}$ and continuous slab weights $w \\in \\mathbb{R}^{n}$ such that $x = z \\odot w$, where $\\odot$ denotes element-wise multiplication. Given $z$, the slab weights follow a Gaussian prior $w \\sim \\mathcal{N}(0, \\tau^2 I_n)$ with slab variance $\\tau^2  0$. The support vector $z$ follows an Ising-type Markov Random Field (MRF) prior on a chain graph with pairwise interactions:\n$$\np(z) \\propto \\exp\\left( \\beta \\sum_{i=1}^{n-1} z_i z_{i+1} + h \\sum_{i=1}^{n} z_i \\right),\n$$\nwhere $\\beta \\in \\mathbb{R}$ is the coupling strength and $h \\in \\mathbb{R}$ is the unary field that biases sparsity.\n\nFor a fixed parameter tuple $(\\sigma^2, \\tau^2, \\beta, h)$, the Maximum A Posteriori (MAP) estimate solves\n$$\n\\max_{z \\in \\{0,1\\}^{n},\\, w \\in \\mathbb{R}^{n}} \\left\\{ -\\frac{1}{2\\sigma^2}\\|y - A(z \\odot w)\\|_2^2 - \\frac{1}{2\\tau^2}\\|w\\|_2^2 + \\beta \\sum_{i=1}^{n-1} z_i z_{i+1} + h \\sum_{i=1}^{n} z_i \\right\\}.\n$$\nFor a fixed $z$, the optimal $w$ is the unique minimizer of a strictly convex quadratic, and thus can be obtained in closed form by solving a ridge regression restricted to the active support.\n\nYou must propose and implement a continuation (homotopy) strategy that starts from weak MRF coupling and large slab variance, and anneals to target values. Specifically, define an annealing schedule of $K$ steps that linearly interpolates\n$$\n\\beta_k = \\beta_{\\mathrm{init}} + \\frac{k}{K}(\\beta_{\\mathrm{tgt}} - \\beta_{\\mathrm{init}}), \\quad \\tau_k^2 = \\tau_{\\mathrm{init}}^2 - \\frac{k}{K}(\\tau_{\\mathrm{init}}^2 - \\tau_{\\mathrm{tgt}}^2),\n$$\nfor $k \\in \\{0,1,\\dots,K\\}$, with initial parameters $\\beta_{\\mathrm{init}} = 0$ and $\\tau_{\\mathrm{init}}^2 = 10$. At each step, compute:\n- The global MAP support by exact enumeration over all $z \\in \\{0,1\\}^{n}$, optimizing $w$ in closed form for each $z$.\n- A locally tracked MAP support using Iterated Conditional Modes (ICM), initialized from the previous stepâ€™s locally tracked support, where ICM alternates discrete updates of individual $z_i$ with exact re-optimization of $w$ for the updated support.\n\nHomotopy path stability is defined as the event that, for all steps $k \\in \\{1,\\dots,K\\}$, the locally tracked MAP support equals the exact global MAP support at that step. The basin of attraction at the target parameters is quantified as the fraction of random initial supports that converge under ICM to the exact global MAP support at the target parameters.\n\nStarting from first principles, your program must:\n1. Construct $A \\in \\mathbb{R}^{m \\times n}$ by drawing independent standard normal entries using a fixed pseudorandom seed $0$, and then normalizing each column to unit Euclidean norm.\n2. Fix $n = 8$ and $m = 5$.\n3. Define a deterministic ground-truth sparse signal $x^{\\star} \\in \\mathbb{R}^{n}$ with active indices at positions $2$, $4$, and $7$ (one-based indexing), i.e., $x^{\\star}_{2} = 1.5$, $x^{\\star}_{4} = -1.0$, $x^{\\star}_{7} = 0.8$, and all other entries equal to $0$.\n4. Generate the observation $y = A x^{\\star} + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{gen}}^2 I_m)$ using a fixed pseudorandom seed $1$ and $\\sigma_{\\mathrm{gen}}^2 = 0.01$.\n5. Use the chain graph MRF prior $p(z)$ with edges $(i, i+1)$ for $i \\in \\{1,\\dots,n-1\\}$.\n\nDefine the Iterated Conditional Modes (ICM) local optimization as follows: given $(\\sigma^2, \\tau^2, \\beta, h)$ and an initialization $z^{(0)}$, repeat coordinate ascent sweeps over $i \\in \\{1,\\dots,n\\}$; at each coordinate, evaluate the posterior objective for $z_i = 0$ and $z_i = 1$ while keeping other $z_j$ fixed, each time re-optimizing $w$ in closed form for the resulting support; update $z_i$ to the choice yielding the higher posterior objective. Stop when a full sweep makes no changes or after a maximum of $50$ sweeps.\n\nYour program must implement the above and compute, for each test case, two outputs:\n- A stability flag, equal to $1$ if the locally tracked path matches the exact global path at all steps, and $0$ otherwise.\n- The basin fraction at the target parameters, defined as the fraction (rounded to three decimal places) of $R$ random initial supports (each sampled i.i.d. Bernoulli with parameter $0.5$ using a fixed pseudorandom seed $2$) that converge under ICM to the exact global MAP support at the target parameters.\n\nTest Suite. Use three test cases with parameters:\n- Case $1$: $\\beta_{\\mathrm{tgt}} = 0.4$, $\\tau_{\\mathrm{tgt}}^2 = 1.0$, $h = -0.3$, $\\sigma^2 = 0.01$, $K = 6$, $R = 25$.\n- Case $2$: $\\beta_{\\mathrm{tgt}} = 0.0$, $\\tau_{\\mathrm{tgt}}^2 = 1.0$, $h = -0.3$, $\\sigma^2 = 0.01$, $K = 6$, $R = 25$.\n- Case $3$: $\\beta_{\\mathrm{tgt}} = 1.2$, $\\tau_{\\mathrm{tgt}}^2 = 0.3$, $h = -0.15$, $\\sigma^2 = 0.015$, $K = 6$, $R = 25$.\n\nAlgorithmic Requirements:\n- The exact global MAP support must be obtained at each step by enumerating all $z \\in \\{0,1\\}^{n}$, and solving the ridge regression for $w$ restricted to the active support:\n$$\nw_S = \\left( \\frac{1}{\\sigma^2} A_S^{\\top} A_S + \\frac{1}{\\tau^2} I_{|S|} \\right)^{-1} \\left( \\frac{1}{\\sigma^2} A_S^{\\top} y \\right),\n$$\nwhere $S = \\{ i \\mid z_i = 1 \\}$ and $A_S$ is the submatrix of $A$ with columns in $S$.\n- The posterior objective to be maximized for a given $z$ is\n$$\n\\mathcal{L}(z) = -\\frac{1}{2\\sigma^2}\\left\\| y - A(z \\odot w_S) \\right\\|_2^2 - \\frac{1}{2\\tau^2}\\left\\| w_S \\right\\|_2^2 + \\beta \\sum_{i=1}^{n-1} z_i z_{i+1} + h \\sum_{i=1}^{n} z_i,\n$$\nwith $w_S$ as above and $w_{i} = 0$ for $i \\notin S$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated Python-style list of three inner lists, each inner list containing two entries: the stability flag (an integer $0$ or $1$) and the basin fraction (a float rounded to three decimals). For example, the output should look like $[[1,0.920],[0,0.480],[1,0.840]]$, but with the actual values computed by your program.\n\nNo physical units or angles are involved in this problem. All numeric values must be treated as dimensionless. Ensure deterministic behavior by using the prescribed pseudorandom seeds where specified, and do not read any external input.", "solution": "The problem requires an analysis of Maximum A Posteriori (MAP) estimation for a sparse signal in a Bayesian framework. This involves a spike-and-slab prior on the signal combined with a Markov Random Field (MRF) prior on its support. The analysis will compare a global optimization method (exact enumeration) with a local one (Iterated Conditional Modes, ICM) within a homotopy or continuation scheme.\n\n### Principle-Based Design\n\n#### 1. Bayesian Model Formulation\nThe problem begins with a standard linear observation model from compressed sensing:\n$$\ny = A x + \\varepsilon\n$$\nwhere $y \\in \\mathbb{R}^{m}$ is the measurement vector, $A \\in \\mathbb{R}^{m \\times n}$ is the design matrix, $x \\in \\mathbb{R}^{n}$ is the sparse signal to be recovered, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$ is additive Gaussian noise with variance $\\sigma^2$.\n\nThe sparsity of $x$ is modeled using a spike-and-slab prior. This prior introduces a binary support vector $z \\in \\{0, 1\\}^n$, where $z_i=1$ indicates that the $i$-th component of the signal, $x_i$, is non-zero, and $z_i=0$ indicates it is zero. The signal is then represented as $x = z \\odot w$, where $\\odot$ is the element-wise product and $w \\in \\mathbb{R}^n$ are the \"slab\" coefficients. The prior distributions are:\n-   For the slab weights $w$, given the support $z$, a Gaussian prior is assumed for the active coefficients: $p(w_i|z_i=1) \\sim \\mathcal{N}(0, \\tau^2)$. This is equivalent to a prior on the full vector $w \\sim \\mathcal{N}(0, \\tau^2 I_n)$, as the posterior for $w_i$ with $z_i=0$ will be irrelevant since $x_i = 0$ anyway.\n-   For the support vector $z$, an Ising-type Markov Random Field (MRF) prior is defined on a chain graph. This prior encourages structured sparsity, where neighboring coefficients in the signal are more likely to be simultaneously active or inactive. The probability mass function is:\n    $$\n    p(z|\\beta, h) \\propto \\exp\\left( \\beta \\sum_{i=1}^{n-1} z_i z_{i+1} + h \\sum_{i=1}^{n} z_i \\right)\n    $$\n    The parameter $\\beta$ controls the coupling strength between adjacent support variables, and $h$ is a global parameter controlling the overall sparsity level.\n\n#### 2. MAP Estimation and Profiled Objective\nThe goal is to find the MAP estimate of both the support $z$ and the weights $w$ by maximizing the posterior distribution $p(z, w | y)$. This is equivalent to maximizing the log-posterior, which, by Bayes' theorem, is proportional to the sum of the log-likelihood and the log-priors:\n$$\n\\log p(z, w | y) \\propto \\log p(y | w, z) + \\log p(w | z) + \\log p(z)\n$$\nSubstituting the Gaussian likelihood and the specified priors, the objective function to maximize is:\n$$\n\\mathcal{J}(z, w) = -\\frac{1}{2\\sigma^2}\\|y - A(z \\odot w)\\|_2^2 - \\frac{1}{2\\tau^2}\\|w\\|_2^2 + \\beta \\sum_{i=1}^{n-1} z_i z_{i+1} + h \\sum_{i=1}^{n} z_i\n$$\nA key insight is that for a fixed support vector $z$, the optimization problem for $w$ is quadratic and strictly convex. Let $S = \\{i | z_i=1\\}$ be the set of active indices and $|S|$ its cardinality. Let $w_S$ be the vector of weights corresponding to these indices and $A_S$ be the submatrix of $A$ with columns indexed by $S$. The objective for $w_S$ is:\n$$\n\\mathcal{J}(w_S | z) = -\\frac{1}{2\\sigma^2}\\|y - A_S w_S\\|_2^2 - \\frac{1}{2\\tau^2}\\|w_S\\|_2^2\n$$\nThe unique maximizer $w_S^*$ is found by setting the gradient to zero, which yields the solution to a ridge regression problem:\n$$\nw_S^*(z) = \\left( A_S^{\\top} A_S + \\frac{\\sigma^2}{\\tau^2} I_{|S|} \\right)^{-1} A_S^{\\top} y\n$$\nBy substituting this optimal $w_S^*(z)$ back into the full objective function, we obtain a profiled objective $\\mathcal{L}(z)$ that depends only on the discrete support vector $z$:\n$$\n\\mathcal{L}(z) = \\mathcal{J}(z, w^*(z)) = -\\frac{1}{2\\sigma^2}\\|y - A_S w_S^*(z)\\|_2^2 - \\frac{1}{2\\tau^2}\\|w_S^*(z)\\|_2^2 + \\beta \\sum_{i=1}^{n-1} z_i z_{i+1} + h \\sum_{i=1}^{n} z_i\n$$\nThe MAP estimation problem is now reduced to a combinatorial optimization over $z \\in \\{0, 1\\}^n$:\n$$\nz_{\\text{MAP}} = \\arg\\max_{z \\in \\{0,1\\}^n} \\mathcal{L}(z)\n$$\n\n#### 3. Optimization and Analysis Strategy\nThe problem requires comparing two approaches to find $z_{\\text{MAP}}$ within a continuation framework.\n\n-   **Global Optimization**: Since the signal dimension $n=8$ is small, the search space for $z$ has a manageable size of $2^8 = 256$. We can find the true global MAP support $z_{\\text{global}}$ by computing $\\mathcal{L}(z)$ for every possible $z$ and selecting the one that yields the maximum value. This provides a gold standard for comparison.\n\n-   **Local Optimization (ICM)**: For larger $n$, enumeration is infeasible. Iterated Conditional Modes (ICM) is a greedy local search algorithm. It iteratively optimizes the objective one coordinate at a time. For each $z_i$, holding all other $z_j$ ($j \\neq i$) fixed, ICM computes $\\mathcal{L}(z)$ for the two cases $z_i=0$ and $z_i=1$ and updates $z_i$ to the value that yields a higher objective. A full sweep consists of iterating through all $i \\in \\{1, \\dots, n\\}$. The algorithm terminates when a sweep produces no changes, guaranteeing convergence to a local maximum of $\\mathcal{L}(z)$.\n\n-   **Continuation (Homotopy) Method**: Local search methods like ICM are susceptible to getting trapped in poor local optima. A continuation strategy can mitigate this. The idea is to start with a \"simpler\" version of the problem and gradually anneal the parameters towards their target values. Here, we start with weak MRF coupling ($\\beta_{\\mathrm{init}}=0$) and a large slab variance ($\\tau_{\\mathrm{init}}^2=10$, which down-weights the prior on $w$). The parameters are linearly interpolated over $K$ steps to their target values $(\\beta_{\\mathrm{tgt}}, \\tau_{\\mathrm{tgt}}^2)$. The locally-optimal solution from step $k-1$ is used as the starting point for the ICM search at step $k$. This path-following procedure helps the local search stay near the basin of attraction of the global optimum.\n\n#### 4. Evaluation Metrics\n\n-   **Homotopy Path Stability**: This metric assesses the effectiveness of the continuation strategy. Stability is declared if the locally-tracked ICM solution matches the globally optimal solution at every step of the annealing schedule. A stability flag of $1$ indicates success, while $0$ indicates failure.\n\n-   **Basin of Attraction**: This quantifies the \"niceness\" of the final optimization landscape at the target parameters. It is estimated by the fraction of random initial supports that converge to the true global MAP support when ICM is run. A larger basin fraction suggests that the global optimum is easier to find from a random starting point.\n\nThe implementation will proceed by first generating the fixed synthetic data ($A, y$). Then, for each test case, it will execute the homotopy analysis to determine path stability and subsequently compute the basin fraction at the final parameters. All random processes are governed by fixed seeds to ensure reproducibility.", "answer": "```python\nimport numpy as np\nimport itertools\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the three test cases.\n    It orchestrates data generation, and for each case, runs the homotopy\n    analysis and basin of attraction calculation.\n    \"\"\"\n    \n    # Problem constants\n    n, m = 8, 5\n    sigma_gen_sq = 0.01\n\n    # Generate synthetic data A and y (fixed for all test cases)\n    rng_A = np.random.default_rng(0)\n    A = rng_A.standard_normal((m, n))\n    A /= np.linalg.norm(A, axis=0)\n\n    x_star = np.zeros(n)\n    # One-based indexing in problem: 2, 4, 7\n    # Zero-based indexing in python: 1, 3, 6\n    x_star[1] = 1.5\n    x_star[3] = -1.0\n    x_star[6] = 0.8\n\n    rng_eps = np.random.default_rng(1)\n    epsilon = rng_eps.normal(0, np.sqrt(sigma_gen_sq), m)\n    y = A @ x_star + epsilon\n\n    # Test cases\n    test_cases = [\n        # (beta_tgt, tau_tgt^2, h, sigma^2, K, R)\n        (0.4, 1.0, -0.3, 0.01, 6, 25),\n        (0.0, 1.0, -0.3, 0.01, 6, 25),\n        (1.2, 0.3, -0.15, 0.015, 6, 25),\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result = process_case(params, A, y, n)\n        all_results.append(result)\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\n\ndef compute_log_posterior(z, sigma2, tau2, beta, h, y, A):\n    \"\"\"\n    Computes the profiled log-posterior objective L(z) for a given support z.\n    \"\"\"\n    z = np.array(z)\n    active_set = np.where(z == 1)[0]\n    k = len(active_set)\n    \n    mrf_term = beta * np.sum(z[:-1] * z[1:]) + h * np.sum(z)\n\n    if k == 0:\n        log_posterior = -0.5 / sigma2 * np.dot(y, y)\n        return log_posterior + mrf_term\n\n    A_S = A[:, active_set]\n    \n    lambda_reg = sigma2 / tau2\n    \n    try:\n        # Ridge regression solution for w_S\n        mat_inv = np.linalg.inv(A_S.T @ A_S + lambda_reg * np.eye(k))\n        w_S = mat_inv @ (A_S.T @ y)\n\n        residual = y - A_S @ w_S\n        log_lik = -0.5 / sigma2 * np.dot(residual, residual)\n        w_prior = -0.5 / tau2 * np.dot(w_S, w_S)\n        \n        log_posterior = log_lik + w_prior\n        return log_posterior + mrf_term\n    except np.linalg.LinAlgError:\n        return -np.inf\n\ndef find_global_map_support(params, y, A, n):\n    \"\"\"\n    Finds the global MAP support by enumerating all 2^n possibilities.\n    \"\"\"\n    sigma2, tau2, beta, h = params\n    best_z = None\n    max_log_post = -np.inf\n\n    for z_tuple in itertools.product([0, 1], repeat=n):\n        z = np.array(z_tuple)\n        log_post = compute_log_posterior(z, sigma2, tau2, beta, h, y, A)\n        if log_post > max_log_post:\n            max_log_post = log_post\n            best_z = z\n            \n    return best_z\n\ndef run_icm(z_init, params, y, A, n, max_sweeps=50):\n    \"\"\"\n    Performs Iterated Conditional Modes (ICM) local search.\n    \"\"\"\n    sigma2, tau2, beta, h = params\n    z_current = np.array(z_init, dtype=int)\n\n    for _ in range(max_sweeps):\n        changed = False\n        for i in range(n):\n            z_off = z_current.copy()\n            z_off[i] = 0\n            log_post_off = compute_log_posterior(z_off, sigma2, tau2, beta, h, y, A)\n\n            z_on = z_current.copy()\n            z_on[i] = 1\n            log_post_on = compute_log_posterior(z_on, sigma2, tau2, beta, h, y, A)\n\n            current_val = z_current[i]\n            if log_post_on > log_post_off:\n                if current_val == 0:\n                    z_current[i] = 1\n                    changed = True\n            else:\n                if current_val == 1:\n                    z_current[i] = 0\n                    changed = True\n        \n        if not changed:\n            break\n            \n    return z_current\n\ndef process_case(case_params, A, y, n):\n    \"\"\"\n    Executes the full analysis for a single test case.\n    \"\"\"\n    beta_tgt, tau2_tgt, h, sigma2, K, R = case_params\n    \n    # Homotopy parameters\n    beta_init = 0.0\n    tau2_init = 10.0\n    \n    betas = np.linspace(beta_init, beta_tgt, K + 1)\n    # The problem formula is equivalent to linspace.\n    # tau2_k = tau2_init - (k/K) * (tau2_init - tau2_tgt)\n    # This is (1 - k/K) * tau2_init + (k/K) * tau2_tgt\n    tau2s = np.linspace(tau2_init, tau2_tgt, K + 1)\n\n    # --- Homotopy Path Stability Analysis ---\n    stability_flag = 1\n    \n    # Step k=0\n    params_k0 = (sigma2, tau2s[0], betas[0], h)\n    z_global_k0 = find_global_map_support(params_k0, y, A, n)\n    z_local_prev = z_global_k0\n    z_global_target = None # Will store the final global MAP\n\n    for k in range(1, K + 1):\n        params_k = (sigma2, tau2s[k], betas[k], h)\n        z_global_k = find_global_map_support(params_k, y, A, n)\n        \n        # Track local MAP from previous step's solution\n        z_local_k = run_icm(z_local_prev, params_k, y, A, n)\n        \n        if not np.array_equal(z_local_k, z_global_k):\n            stability_flag = 0\n            \n        z_local_prev = z_local_k\n        if k == K:\n            z_global_target = z_global_k\n            \n    if K == 0: # Handle edge case where no annealing steps\n        z_global_target = z_global_k0\n\n    # --- Basin of Attraction Analysis ---\n    target_params = (sigma2, tau2s[K], betas[K], h)\n    convergence_count = 0\n    rng_basin = np.random.default_rng(2)\n\n    for _ in range(R):\n        z_init = rng_basin.integers(0, 2, size=n, dtype=int)\n        z_converged = run_icm(z_init, target_params, y, A, n)\n        if np.array_equal(z_converged, z_global_target):\n            convergence_count += 1\n            \n    basin_fraction = round(convergence_count / R, 3)\n\n    return [stability_flag, basin_fraction]\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3480144"}, {"introduction": "Beyond estimating the parameters of a given model, a crucial task in scientific inquiry is to compare competing hypotheses, or models, in light of the available data. In a Bayesian framework, this is accomplished by comparing the marginal likelihood, or 'evidence,' for each model. This exercise [@problem_id:3480136] delves into this core concept by guiding you to compute the Bayes factor, which weighs the evidence for a model with a structured Ising MRF prior against one with a simpler, independent Bernoulli prior. By working through the exact calculation for a small-scale problem, you will learn how the data itself can justify the use of a more complex prior that captures dependencies between variables.", "problem": "Consider the linear observation model $y = A x + \\varepsilon$ with $p = 4$, $n = 4$, $A = I_{4}$, and Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{4})$, where $y \\in \\mathbb{R}^{4}$ is observed. Impose a spike-and-slab prior on $x$ coupled to a binary support vector $z \\in \\{0,1\\}^{4}$: conditional on $z$, the coordinates are independent and for each $i \\in \\{1,2,3,4\\}$, $x_{i} \\mid z_{i}$ has distribution $(1 - z_{i}) \\, \\delta_{0} + z_{i} \\, \\mathcal{N}(0, \\tau^{2})$, where $\\delta_{0}$ denotes a point mass at $0$ and $\\tau^{2}  0$ is the slab variance. Assume two alternative priors for the support $z$:\n- Independent Bernoulli prior with parameter $\\pi \\in (0,1)$, i.e., $p(z) = \\prod_{i=1}^{4} \\pi^{z_{i}} (1 - \\pi)^{1 - z_{i}}$.\n- An Ising Markov random field (MRF) prior on the $4$-node chain graph with homogeneous field $h \\in \\mathbb{R}$ and ferromagnetic coupling $J  0$, i.e., $p(z) \\propto \\exp\\!\\left(h \\sum_{i=1}^{4} z_{i} + J \\sum_{i=1}^{3} z_{i} z_{i+1}\\right)$.\n\nStarting from fundamental probabilistic definitions and the given model, compute the exact marginal likelihood $p(y)$ under each prior by summing over all $z \\in \\{0,1\\}^{4}$ and analytically integrating out $x$. Then, using these exact results, form the Bayes factor that compares the Ising MRF prior to the independent Bernoulli prior,\n$$\nB(y; \\sigma^{2}, \\tau^{2}, \\pi, h, J) = \\frac{p(y \\mid \\text{Ising MRF}(h,J))}{p(y \\mid \\text{Bernoulli}(\\pi))}.\n$$\nExpress your final answer as a single closed-form analytical expression in terms of $y$, $\\sigma^{2}$, $\\tau^{2}$, $\\pi$, $h$, and $J$. No numerical evaluation or rounding is required. Clearly state the expression for the Bayes factor. Based on the sign of $\\ln B(y; \\sigma^{2}, \\tau^{2}, \\pi, h, J)$, determine in your solution which prior is favored by the observed data in the sense of evidence (Ising MRF favored if $\\ln B  0$, Bernoulli favored if $\\ln B  0$, and neither decisively favored if $\\ln B = 0$).", "solution": "The problem requires the computation of a Bayes factor to compare two different priors on the binary support vector $z$ in a spike-and-slab regression model. The Bayes factor $B$ is the ratio of the marginal likelihoods of the data $y$ under the two competing models.\n$$\nB(y; \\sigma^{2}, \\tau^{2}, \\pi, h, J) = \\frac{p(y \\mid \\text{Ising MRF}(h,J))}{p(y \\mid \\text{Bernoulli}(\\pi))}\n$$\nThe marginal likelihood under a given model for $z$ is obtained by marginalizing over both the continuous variable $x$ and the discrete variable $z$:\n$$\np(y) = \\sum_{z \\in \\{0,1\\}^4} \\int_{\\mathbb{R}^4} p(y|x) p(x|z) p(z) \\, dx = \\sum_{z \\in \\{0,1\\}^4} p(y|z) p(z)\n$$\nwhere $p(y|z) = \\int p(y|x)p(x|z)dx$ is the marginal likelihood for a fixed support vector $z$. The calculation proceeds in three main steps:\n1.  Compute the conditional marginal likelihood $p(y|z)$.\n2.  Compute the full marginal likelihood $p(y)$ for each of the two priors on $z$.\n3.  Form the Bayes factor by taking the ratio of the marginal likelihoods.\n\nStep 1: Compute the conditional marginal likelihood $p(y|z)$\n\nThe observation model is $y = Ax + \\varepsilon$ with $A = I_4$, so $y = x + \\varepsilon$. The noise is $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_4)$. This implies the likelihood of $y$ given $x$ is $p(y|x) = \\mathcal{N}(y; x, \\sigma^2 I_4)$. Since the covariance matrix is diagonal, the model decomposes by coordinate: $p(y|x) = \\prod_{i=1}^4 p(y_i|x_i)$, where $p(y_i|x_i) = \\mathcal{N}(y_i; x_i, \\sigma^2)$.\n\nThe spike-and-slab prior on $x$ conditional on $z$ is also independent across coordinates: $p(x|z) = \\prod_{i=1}^4 p(x_i|z_i)$, where $p(x_i|z_i) = (1-z_i)\\delta_0(x_i) + z_i \\mathcal{N}(x_i; 0, \\tau^2)$.\n\nThus, the marginal likelihood for a fixed $z$ also decomposes: $p(y|z) = \\prod_{i=1}^4 p(y_i|z_i)$. We compute $p(y_i|z_i)$ for each of the two cases for $z_i \\in \\{0,1\\}$.\n\nCase $z_i=0$: The prior is $p(x_i|z_i=0) = \\delta_0(x_i)$, a point mass at $0$.\n$$\np(y_i|z_i=0) = \\int_{-\\infty}^{\\infty} p(y_i|x_i) p(x_i|z_i=0) \\, dx_i = \\int_{-\\infty}^{\\infty} \\mathcal{N}(y_i; x_i, \\sigma^2) \\delta_0(x_i) \\, dx_i = \\mathcal{N}(y_i; 0, \\sigma^2)\n$$\nThis is the density of a Gaussian with mean $0$ and variance $\\sigma^2$ evaluated at $y_i$.\n\nCase $z_i=1$: The prior is $p(x_i|z_i=1) = \\mathcal{N}(x_i; 0, \\tau^2)$. The model for $y_i$ is $y_i = x_i + \\varepsilon_i$, where $x_i \\sim \\mathcal{N}(0, \\tau^2)$ and $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ are independent. The sum of two independent Gaussian random variables is also Gaussian. The mean is $E[y_i] = E[x_i] + E[\\varepsilon_i] = 0$. The variance is $Var(y_i) = Var(x_i) + Var(\\varepsilon_i) = \\tau^2 + \\sigma^2$.\nTherefore, $y_i|z_i=1 \\sim \\mathcal{N}(0, \\sigma^2+\\tau^2)$, and its density is\n$$\np(y_i|z_i=1) = \\mathcal{N}(y_i; 0, \\sigma^2+\\tau^2)\n$$\n\nTo simplify notation, let's define the ratio of these two likelihoods for each coordinate $i$:\n$$\n\\lambda_i = \\frac{p(y_i|z_i=1)}{p(y_i|z_i=0)} = \\frac{\\frac{1}{\\sqrt{2\\pi(\\sigma^2+\\tau^2)}}\\exp\\left(-\\frac{y_i^2}{2(\\sigma^2+\\tau^2)}\\right)}{\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{y_i^2}{2\\sigma^2}\\right)} = \\sqrt{\\frac{\\sigma^2}{\\sigma^2+\\tau^2}} \\exp\\left(\\frac{y_i^2}{2}\\left(\\frac{1}{\\sigma^2}-\\frac{1}{\\sigma^2+\\tau^2}\\right)\\right)\n$$\n$$\n\\lambda_i = \\sqrt{\\frac{\\sigma^2}{\\sigma^2+\\tau^2}} \\exp\\left(\\frac{y_i^2 \\tau^2}{2\\sigma^2(\\sigma^2+\\tau^2)}\\right)\n$$\nUsing this ratio, we can write $p(y_i|z_i) = p(y_i|z_i=0) \\cdot \\lambda_i^{z_i}$. Then,\n$$\np(y|z) = \\prod_{i=1}^4 p(y_i|z_i) = \\left(\\prod_{i=1}^4 p(y_i|z_i=0)\\right) \\left(\\prod_{i=1}^4 \\lambda_i^{z_i}\\right)\n$$\nThe first term is a constant with respect to $z$. Let's call it $C_y = \\prod_{i=1}^4 \\mathcal{N}(y_i; 0, \\sigma^2) = (2\\pi\\sigma^2)^{-2}\\exp(-\\|y\\|^2_2/(2\\sigma^2))$.\n\nStep 2: Compute the marginal likelihood under each prior\n\nFor the independent Bernoulli prior, $p(z|\\text{Bernoulli}) = \\prod_{i=1}^4 \\pi^{z_i}(1-\\pi)^{1-z_i}$. The marginal likelihood is:\n$$\np(y|\\text{Bernoulli}) = \\sum_{z \\in \\{0,1\\}^4} p(y|z)p(z|\\text{Bernoulli}) = \\sum_{z \\in \\{0,1\\}^4} C_y \\left(\\prod_{i=1}^4 \\lambda_i^{z_i}\\right) \\left(\\prod_{j=1}^4 \\pi^{z_j}(1-\\pi)^{1-z_j}\\right)\n$$\n$$\np(y|\\text{Bernoulli}) = C_y \\sum_{z \\in \\{0,1\\}^4} \\prod_{i=1}^4 \\left(\\lambda_i^ {z_i} \\pi^{z_i} (1-\\pi)^{1-z_i}\\right) = C_y \\prod_{i=1}^4 \\sum_{z_i \\in \\{0,1\\}} (\\lambda_i \\pi)^{z_i} (1-\\pi)^{1-z_i}\n$$\nThe sum for each $i$ is $(1-\\pi) + \\lambda_i \\pi$. Therefore,\n$$\np(y|\\text{Bernoulli}) = C_y \\prod_{i=1}^4 (1-\\pi + \\lambda_i \\pi)\n$$\n\nFor the Ising MRF prior, $p(z|\\text{Ising MRF}) = \\frac{1}{Z_{\\text{MRF}}} \\exp\\left(h\\sum_{i=1}^4 z_i + J\\sum_{i=1}^3 z_i z_{i+1}\\right)$, where $Z_{\\text{MRF}}$ is the partition function.\n$$\np(y|\\text{Ising MRF}) = \\sum_{z \\in \\{0,1\\}^4} C_y \\left(\\prod_{i=1}^4 \\lambda_i^{z_i}\\right) \\frac{1}{Z_{\\text{MRF}}} \\exp\\left(h\\sum_{j=1}^4 z_j + J\\sum_{j=1}^3 z_j z_{j+1}\\right)\n$$\nUsing $\\prod \\lambda_i^{z_i} = \\exp(\\sum z_i \\ln\\lambda_i)$, we combine exponents:\n$$\np(y|\\text{Ising MRF}) = \\frac{C_y}{Z_{\\text{MRF}}} \\sum_{z \\in \\{0,1\\}^4} \\exp\\left(\\sum_{i=1}^4 (h+\\ln\\lambda_i)z_i + J\\sum_{i=1}^3 z_i z_{i+1}\\right)\n$$\nThe sum is the partition function $Z'$ of an inhomogeneous 1D Ising model on a chain with fields $h_i' = h+\\ln\\lambda_i$ and coupling $J$. This can be computed using the transfer matrix method. The partition function for the prior, $Z_{\\text{MRF}}$, is the same calculation with $\\lambda_i=1$ for all $i$ (i.e., $h_i'=h$).\nSo, $p(y|\\text{Ising MRF}) = C_y \\frac{Z'}{Z_{\\text{MRF}}}$.\n\nStep 3: Form the Bayes Factor\n\nThe Bayes factor $B$ is the ratio of the two marginal likelihoods. The term $C_y$ cancels.\n$$\nB = \\frac{p(y|\\text{Ising MRF})}{p(y|\\text{Bernoulli})} = \\frac{Z' / Z_{\\text{MRF}}}{\\prod_{j=1}^4 (1-\\pi + \\lambda_j \\pi)}\n$$\nThe numerator's partition function ratio can be expressed explicitly using transfer matrices. Let $M_i(\\lambda_k) = \\begin{pmatrix} 1  \\lambda_i \\exp(h) \\\\ 1  \\lambda_i \\exp(h)\\exp(J) \\end{pmatrix}$ be the transfer matrix with data dependence. Let $M = M_i(1)$ be the transfer matrix for the prior. The numerator is:\n$$\n\\frac{Z'}{Z_{\\text{MRF}}} = \\frac{\\begin{pmatrix} 1  \\lambda_1 \\exp(h) \\end{pmatrix} M_2(\\lambda_2) M_3(\\lambda_3) M_4(\\lambda_4) \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}}{\\begin{pmatrix} 1  \\exp(h) \\end{pmatrix} M^3 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}}\n$$\nThis gives the final analytical expression for the Bayes factor.\n\nThe sign of $\\ln B$ determines which model is favored by the data.\n- If $\\ln B  0$ ($B  1$), the evidence for the Ising MRF model is stronger. This would typically occur if the observed data $y$ suggests a structure in the non-zero coefficients of $x$ that is well-modeled by the ferromagnetic coupling $J0$ (e.g., adjacent coefficients being simultaneously non-zero).\n- If $\\ln B  0$ ($B  1$), the evidence favors the independent Bernoulli prior. This might happen if the non-zero coefficients appear to be scattered randomly, without any dependency structure.\n- If $\\ln B = 0$ ($B = 1$), both models are equally supported by the data.", "answer": "$$\n\\boxed{\\frac{\\begin{pmatrix} 1  \\lambda_1 \\exp(h) \\end{pmatrix} \\left( \\prod_{i=2}^{4} \\begin{pmatrix} 1  \\lambda_i \\exp(h) \\\\ 1  \\lambda_i \\exp(h)\\exp(J) \\end{pmatrix} \\right) \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}}{\\left(\\begin{pmatrix} 1  \\exp(h) \\end{pmatrix} \\begin{pmatrix} 1  \\exp(h) \\\\ 1  \\exp(h)\\exp(J) \\end{pmatrix}^{3} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\right) \\prod_{j=1}^{4} (1-\\pi+\\lambda_j\\pi)}}\n$$", "id": "3480136"}]}