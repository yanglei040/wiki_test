{"hands_on_practices": [{"introduction": "The formal bridge between the microcanonical and canonical ensembles is the Laplace transform, which converts the microcanonical density of states $\\Omega(E)$ into the canonical partition function $Z(\\beta)$. This exercise provides a direct computational experience of this fundamental link [@problem_id:3410951]. By numerically integrating a given microcanonical entropy, you will reconstruct canonical thermodynamic properties and test the accuracy of the saddle-point approximation, which lies at the heart of ensemble equivalence, particularly in scenarios designed to challenge its validity.", "problem": "You are given microcanonical density of states functions $\\Omega(E)$ defined on discrete energy grids, as would be obtained from Wang–Landau (WL) sampling. Your task is to reconstruct canonical thermodynamics via the Laplace transform and to assess both numerical stability of the computation and the accuracy of a saddle-point approximation that represents ensemble equivalence in the thermodynamic limit. All quantities are in dimensionless units with Boltzmann constant $k_{\\mathrm{B}}=1$, so energy $E$, inverse temperature $\\beta$, and entropy $S(E)=\\ln \\Omega(E)$ are dimensionless. No other physical units are used or required.\n\nFundamental definitions to use:\n- The canonical partition function is defined by the Laplace transform $Z(\\beta)=\\int_{0}^{\\infty}\\Omega(E)\\,\\mathrm{e}^{-\\beta E}\\,\\mathrm{d}E$.\n- The canonical mean energy is $U(\\beta)=\\int_{0}^{\\infty} E \\, \\pi_{\\beta}(E) \\,\\mathrm{d}E$, where $\\pi_{\\beta}(E)=\\Omega(E)\\,\\mathrm{e}^{-\\beta E}/Z(\\beta)$ is the normalized canonical energy distribution.\n- The microcanonical entropy is $S(E)=\\ln \\Omega(E)$, and the saddle-point approximation for the dominant energy solves the stationarity condition for the Legendre transform, which on a discrete grid you should compute as the maximizer of $F_{\\beta}(E)=S(E)-\\beta E$.\n\nDiscretization requirements:\n- Each input $\\Omega(E)$ is provided implicitly by a parametric form on a uniform grid $E_i=E_{\\min}+i\\,\\Delta E$ for $i=0,1,\\dots,N-1$ with constant spacing $\\Delta E=(E_{\\max}-E_{\\min})/(N-1)$.\n- You must approximate the Laplace integrals by the trapezoidal rule. To ensure numerical stability, you must implement a computation of $\\ln Z(\\beta)$ using a log-sum-exp transformation. Specifically, if $w_i$ are trapezoidal weights ($w_0=w_{N-1}=\\tfrac{1}{2}$ and $w_i=1$ otherwise), then define $f_i=\\ln \\Omega(E_i)-\\beta E_i+\\ln w_i+\\ln \\Delta E$, and compute $\\ln Z(\\beta)=\\ln \\sum_i \\exp(f_i)$ using a max-subtraction, i.e., $\\ln Z(\\beta)=m+\\ln \\sum_i \\exp(f_i-m)$ with $m=\\max_i f_i$.\n- The canonical mean energy must be computed from the same stabilized weights as $U(\\beta)=\\sum_i E_i \\, \\exp(f_i-\\ln Z(\\beta))$.\n- The saddle-point energy $E_{\\ast}(\\beta)$ must be obtained by maximizing $F_{\\beta}(E_i)=S(E_i)-\\beta E_i$ over the grid.\n\nNumerical stability assessment:\n- In addition to the stabilized computation, also compute a naive trapezoidal evaluation using direct exponentials $g_i=\\exp(\\ln \\Omega(E_i)-\\beta E_i)$ without max-subtraction. Declare the naive evaluation \"unstable\" if any $g_i$ is not finite, or if the naive partition sum is non-finite or non-positive. If it is finite and positive, compute the naive canonical mean energy $U_{\\mathrm{naive}}(\\beta)$ and compare to the stabilized $U(\\beta)$. If the relative difference exceeds a tolerance $\\tau=10^{-6}$, declare it unstable; otherwise declare it stable.\n- For each test case, report two results: (i) the relative absolute error of the saddle-point approximation for the mean energy, $\\varepsilon=\\lvert E_{\\ast}(\\beta)-U(\\beta)\\rvert/\\max(U(\\beta),\\epsilon)$ with $\\epsilon=10^{-300}$ to avoid division by zero, and (ii) a stability indicator $s$ that equals $1.0$ if the naive evaluation is unstable by the above criteria, and $0.0$ otherwise.\n\nTest suite:\nImplement the following three cases, each fully specified by a parametric $\\ln \\Omega(E)$ and by $(E_{\\min},E_{\\max},N,\\beta)$:\n\n- Case $1$ (unimodal, happy path):\n  - $\\ln \\Omega(E)=(a-1)\\ln E$ with $a=50$. This corresponds to an idealized many-degree-of-freedom kinetic density of states.\n  - Domain: $E_{\\min}=10^{-9}$, $E_{\\max}=500$, $N=20001$.\n  - Inverse temperature: $\\beta=0.5$.\n\n- Case $2$ (bimodal, ensemble nonequivalence regime):\n  - $\\ln \\Omega(E)=\\ln\\left(\\exp\\left(A_1+s_1 E-\\dfrac{(E-m_1)^2}{2 w_1^2}\\right)+\\exp\\left(A_2+s_2 E-\\dfrac{(E-m_2)^2}{2 w_2^2}\\right)\\right)$ with parameters $A_1=0$, $s_1=0.06$, $m_1=40$, $w_1=8$, $A_2=2.0$, $s_2=0.015$, $m_2=140$, $w_2=8$.\n  - Domain: $E_{\\min}=0$, $E_{\\max}=220$, $N=40001$.\n  - Inverse temperature: $\\beta=0.03$.\n\n- Case $3$ (extreme-scale, numerical stability stress test):\n  - $\\ln \\Omega(E)=(a-1)\\ln E$ with $a=200$.\n  - Domain: $E_{\\min}=10^{-9}$, $E_{\\max}=50000$, $N=80001$.\n  - Inverse temperature: $\\beta=0.01$.\n\nComputational and reporting requirements:\n- Use natural logarithms throughout.\n- Angles are not used; no angle units are required.\n- For each case, compute and return the pair $(\\varepsilon,s)$ as specified above, as floating-point numbers.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[\\varepsilon_1,s_1,\\varepsilon_2,s_2,\\varepsilon_3,s_3]$.", "solution": "The problem requires the computation of canonical thermodynamic properties from a given microcanonical density of states, $\\Omega(E)$, defined on a discrete energy grid. This involves evaluating Laplace transforms numerically. A key part of the task is to assess the numerical stability of this procedure and to quantify the accuracy of the saddle-point approximation, which is a manifestation of ensemble equivalence in the thermodynamic limit.\n\nThe fundamental relationship between the microcanonical entropy $S(E) = \\ln \\Omega(E)$ and the canonical partition function $Z(\\beta)$ at an inverse temperature $\\beta$ is given by the Laplace transform:\n$$\nZ(\\beta) = \\int_{0}^{\\infty} \\Omega(E) e^{-\\beta E} \\,\\mathrm{d}E = \\int_{0}^{\\infty} e^{S(E) - \\beta E} \\,\\mathrm{d}E\n$$\nThe canonical average energy $U(\\beta)$ is the first moment of the energy distribution $\\pi_{\\beta}(E) = e^{S(E)-\\beta E}/Z(\\beta)$:\n$$\nU(\\beta) = \\langle E \\rangle_{\\beta} = \\frac{\\int_{0}^{\\infty} E e^{S(E) - \\beta E} \\,\\mathrm{d}E}{\\int_{0}^{\\infty} e^{S(E) - \\beta E} \\,\\mathrm{d}E}\n$$\nIn the thermodynamic limit (for large systems), the integrand $e^{S(E)-\\beta E}$ becomes sharply peaked around its maximum. The energy $E_{\\ast}(\\beta)$ that maximizes the exponent, $S(E)-\\beta E$, is called the saddle-point energy. This condition is equivalent to finding the energy $E$ where the microcanonical temperature, defined as $T(E) = (\\partial S/\\partial E)^{-1}$, equals the canonical temperature $T = 1/\\beta$. For large systems, ensemble equivalence implies that the canonical average energy $U(\\beta)$ should converge to this saddle-point energy $E_{\\ast}(\\beta)$.\n\nThe defined task is to implement these calculations numerically for three specific test cases and report on (i) the relative error $\\varepsilon = \\lvert E_{\\ast}(\\beta)-U(\\beta)\\rvert/\\max(U(\\beta),\\epsilon)$ of the saddle-point approximation and (ii) a flag $s$ indicating the numerical stability of a naive computational approach compared to a stabilized one.\n\nThe algorithmic procedure is as follows:\n\n1.  **Grid Discretization**: For each case, a uniform energy grid $E_i = E_{\\min} + i \\Delta E$ is defined for $i=0, \\dots, N-1$, with spacing $\\Delta E = (E_{\\max}-E_{\\min})/(N-1)$. The microcanonical entropy $S(E_i) = \\ln \\Omega(E_i)$ is evaluated on this grid.\n\n2.  **Saddle-Point Energy Calculation**: The saddle-point energy $E_{\\ast}(\\beta)$ is found by identifying the grid point $E_i$ that maximizes the function $F_{\\beta}(E_i) = S(E_i) - \\beta E_i$.\n    $$\n    E_{\\ast}(\\beta) = \\underset{E_i}{\\mathrm{argmax}} \\left( S(E_i) - \\beta E_i \\right)\n    $$\n\n3.  **Stabilized Canonical Calculation**: The integrals for $Z(\\beta)$ and $U(\\beta)$ are approximated using the trapezoidal rule. To handle the large range of values in the exponential term, which can lead to numerical overflow or underflow, a log-sum-exp stabilization technique is mandated.\n    The partition function integral is approximated as $Z(\\beta) \\approx \\sum_{i=0}^{N-1} w_i \\Delta E \\, e^{S(E_i) - \\beta E_i}$, where $w_i$ are the trapezoidal weights ($w_0=w_{N-1}=\\frac{1}{2}$, and $w_i=1$ otherwise).\n    To compute its logarithm, we define terms $f_i = S(E_i) - \\beta E_i + \\ln(w_i) + \\ln(\\Delta E)$, such that $Z(\\beta) \\approx \\sum_i e^{f_i}$.\n    The stabilized computation of $\\ln Z(\\beta)$ is then:\n    $$\n    m = \\max_i(f_i) \\\\\n    \\ln Z(\\beta) = m + \\ln\\left(\\sum_{i=0}^{N-1} e^{f_i - m}\\right)\n    $$\n    The canonical mean energy $U(\\beta)$ is computed using weights derived from this stabilized calculation:\n    $$\n    U(\\beta) = \\frac{\\sum_i E_i w_i \\Delta E e^{S(E_i) - \\beta E_i}}{\\sum_i w_i \\Delta E e^{S(E_i) - \\beta E_i}} = \\frac{\\sum_i E_i e^{f_i}}{\\sum_i e^{f_i}} = \\sum_{i=0}^{N-1} E_i e^{f_i - \\ln Z(\\beta)}\n    $$\n\n4.  **Numerical Stability Assessment**: A naive computation is also performed. Let $g_i = e^{S(E_i) - \\beta E_i}$.\n    The naive partition function is $Z_{\\mathrm{naive}} = \\sum_i w_i g_i \\Delta E$.\n    The calculation is deemed unstable (and the stability flag $s$ is set to $1.0$) if any $g_i$ is not finite, if $Z_{\\mathrm{naive}}$ is not finite or not positive, or if the naive mean energy $U_{\\mathrm{naive}} = (\\sum_i E_i w_i g_i \\Delta E)/Z_{\\mathrm{naive}}$ has a relative difference from the stabilized $U(\\beta)$ exceeding $\\tau=10^{-6}$. Otherwise, the calculation is stable ($s=0.0$).\n\n5.  **Reporting**: For each test case, the pair $(\\varepsilon, s)$ is computed and reported. The three cases are designed to test different physical and numerical regimes: a standard unimodal system (Case 1), a bimodal system exhibiting ensemble non-equivalence (Case 2), and a large-scale system designed to cause numerical overflow in naive implementations (Case 3). For the bimodal case, the function $\\ln \\Omega(E)$ is itself a log-sum-of-exponentials, requiring a nested stable evaluation.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for microcanonical to canonical ensemble calculations.\n    \"\"\"\n\n    def log_omega_power_law(E, a):\n        \"\"\"Computes ln(Omega(E)) for a power-law density of states.\"\"\"\n        # This handles vector input E, taking care of log(0) if E_min=0, although here E_min > 0.\n        return (a - 1) * np.log(E)\n\n    def log_omega_bimodal(E, **params):\n        \"\"\"Computes ln(Omega(E)) for a bimodal density of states using log-sum-exp.\"\"\"\n        p = params\n        term1 = p['A1'] + p['s1'] * E - ((E - p['m1'])**2) / (2 * p['w1']**2)\n        term2 = p['A2'] + p['s2'] * E - ((E - p['m2'])**2) / (2 * p['w2']**2)\n        \n        # Use log-sum-exp for numerical stability\n        max_term = np.maximum(term1, term2)\n        return max_term + np.log(np.exp(term1 - max_term) + np.exp(term2 - max_term))\n\n    def process_case(log_omega_func, E_min, E_max, N, beta, func_params):\n        \"\"\"\n        Processes a single test case to compute the saddle-point error and stability indicator.\n        \n        Returns:\n            A tuple (epsilon, s) containing the relative error and stability flag.\n        \"\"\"\n        # --- 1. Grid and Function Setup ---\n        E = np.linspace(E_min, E_max, N)\n        delta_E = (E_max - E_min) / (N - 1)\n        \n        # Handle E=0 in log for power law case (though not strictly necessary for problems as given)\n        if E_min == 0 and log_omega_func == log_omega_power_law:\n            log_omega_E = np.full_like(E, -np.inf)\n            log_omega_E[1:] = log_omega_func(E[1:], **func_params)\n        else:\n            log_omega_E = log_omega_func(E, **func_params)\n\n        # --- 2. Saddle-Point Energy Calculation ---\n        F_beta = log_omega_E - beta * E\n        E_star = E[np.argmax(F_beta)]\n\n        # --- 3. Stabilized Canonical Calculation ---\n        trapezoidal_weights = np.ones(N)\n        trapezoidal_weights[0] = 0.5\n        trapezoidal_weights[-1] = 0.5\n        \n        f = log_omega_E - beta * E + np.log(trapezoidal_weights) + np.log(delta_E)\n\n        # Log-sum-exp for ln(Z)\n        m = np.max(f[np.isfinite(f)]) # Avoid -inf from log(0) if any\n        log_Z = m + np.log(np.sum(np.exp(f - m)))\n        \n        # Stabilized mean energy U\n        U = np.sum(E * np.exp(f - log_Z))\n\n        # --- 4. Numerical Stability Assessment ---\n        s = 0.0\n        \n        # Naive calculation\n        g = np.exp(log_omega_E - beta * E)\n        \n        if not np.all(np.isfinite(g)):\n            s = 1.0\n        else:\n            Z_naive = np.sum(trapezoidal_weights * g * delta_E)\n            if not np.isfinite(Z_naive) or Z_naive <= 0:\n                s = 1.0\n            else:\n                U_naive = np.sum(trapezoidal_weights * E * g * delta_E) / Z_naive\n                \n                # Check relative difference\n                tau = 1e-6\n                if U == 0:\n                    if abs(U_naive) > tau: # If U is zero, U_naive should also be zero\n                        s = 1.0\n                elif np.abs(U_naive - U) / U > tau:\n                    s = 1.0\n\n        # --- 5. Error Calculation ---\n        epsilon_small = 1e-300\n        epsilon_rel_error = np.abs(E_star - U) / np.max([U, epsilon_small])\n        \n        return epsilon_rel_error, s\n\n    # Define test cases\n    test_cases = [\n        {\n            \"log_omega_func\": log_omega_power_law,\n            \"E_min\": 1e-9, \"E_max\": 500, \"N\": 20001, \"beta\": 0.5,\n            \"func_params\": {\"a\": 50}\n        },\n        {\n            \"log_omega_func\": log_omega_bimodal,\n            \"E_min\": 0, \"E_max\": 220, \"N\": 40001, \"beta\": 0.03,\n            \"func_params\": {\n                'A1': 0, 's1': 0.06, 'm1': 40, 'w1': 8,\n                'A2': 2.0, 's2': 0.015, 'm2': 140, 'w2': 8\n            }\n        },\n        {\n            \"log_omega_func\": log_omega_power_law,\n            \"E_min\": 1e-9, \"E_max\": 50000, \"N\": 80001, \"beta\": 0.01,\n            \"func_params\": {\"a\": 200}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        eps, s = process_case(**case)\n        results.extend([eps, s])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{r:.7e}' for r in results)}]\")\n\nsolve()\n```", "id": "3410951"}, {"introduction": "While theory provides exact statistical distributions for ideal ensembles, verifying that a complex molecular dynamics simulation correctly generates these statistics is a critical validation step. This practice introduces a robust method for testing a key signature of the canonical ($NVT$) ensemble: the distribution of kinetic energy [@problem_id:3410952]. You will implement statistical tests to diagnose common issues, such as incorrect accounting of degrees of freedom or non-canonical thermostats, thereby learning how to ensure the fidelity of simulation results.", "problem": "You are to write a complete, runnable program that cross-validates kinetic energy distributions from three hypothetical Molecular Dynamics (MD) packages sampling the constant Number, Volume, and Temperature (NVT) ensemble. The validation is based on fundamental statistical mechanical principles that, for a system of $N$ identical particles at temperature $T$ in the canonical ensemble (NVT), the instantaneous kinetic energy $K$ has a distribution implied by the Maxwell-Boltzmann distribution for momenta. Specifically, when the center-of-mass (COM) momentum is removed, the number of translational degrees of freedom (DOF) is $f = 3N - 3$, and the scaled kinetic energy $Y = 2K/(k_{\\mathrm{B}} T)$ follows a chi-square distribution with $f$ degrees of freedom. Your task is to implement a cross-validation procedure that tests whether the synthetic kinetic energy samples from each package are consistent with the expected chi-square distribution under correct DOF accounting, and to detect deviations indicative of non-canonical sampling or incorrect DOF handling.\n\nStart from the fundamental basis that the canonical phase space distribution has probability density proportional to $\\exp(-\\beta H)$ with $\\beta = 1/(k_{\\mathrm{B}} T)$ and Hamiltonian $H = K + U$, and that the momentum components are independent Gaussians under this ensemble for unconstrained degrees of freedom. From this basis, derive the distribution of $K$ and the scaled variable $Y$ as described, and design a test accordingly.\n\nYou must generate synthetic kinetic energy samples for three hypothetical MD packages for a set of test cases. Use reduced units with Boltzmann constant $k_{\\mathrm{B}} = 1$, so energies and temperatures are dimensionless. No physical unit conversion is required. For each test case, define $N$, $T$, and the sample count $n_s$, and generate the following three datasets:\n- Package A (canonical, correct DOF): Draw $Y \\sim \\chi^2_f$ with $f = 3N - 3$, and set $K = (k_{\\mathrm{B}} T/2) Y$.\n- Package B (incorrect DOF accounting): Draw $Y \\sim \\chi^2_{3N}$ (i.e., not removing COM motion), and set $K = (k_{\\mathrm{B}} T/2) Y$; this will be tested against the expected $f = 3N - 3$.\n- Package C (non-canonical sampling with suppressed variance): Draw $Y \\sim \\chi^2_f$ as in Package A, then construct $Y' = f + \\alpha_v (Y - f)$ with variance shrink factor $\\alpha_v = 0.6$, and set $K = (k_{\\mathrm{B}} T/2) Y'$.\n\nUse fixed random seeds for reproducibility. For test case index $i$ (starting at $i = 1$), use seeds $s_A = 12345 + i$ for Package A, $s_B = 22345 + i$ for Package B, and $s_C = 32345 + i$ for Package C.\n\nFor each dataset, implement the following validation steps:\n1. Compute $f = 3N - 3$. If $f \\le 0$, return a failure (boolean $False$) for all three packages for that test case, because the canonical distribution of $Y$ is degenerate and the test is ill-posed.\n2. Compute the scaled kinetic energies $Y_i = 2K_i/(k_{\\mathrm{B}} T)$ for $i = 1, \\ldots, n_s$.\n3. Perform a Kolmogorov-Smirnov (KS) goodness-of-fit test of $Y$ to the chi-square distribution with $f$ degrees of freedom, obtaining a $p$-value. Use significance level $\\alpha = 0.01$. The KS test passes if $p \\ge \\alpha$.\n4. Independently check the consistency of the sample mean with the expected mean of the chi-square distribution. The expected mean of $Y$ is $f$, and its variance is $2f$. The variance of the sample mean of $Y$ over $n_s$ samples is $2f/n_s$. Compute a $z$-score $z = | \\overline{Y} - f | / \\sqrt{2f/n_s}$, and define the critical value $z_{\\mathrm{crit}}$ from the standard normal distribution by $z_{\\mathrm{crit}} = \\Phi^{-1}(1 - \\alpha/2)$, where $\\Phi^{-1}$ is the inverse cumulative distribution function. The mean-consistency test passes if $z \\le z_{\\mathrm{crit}}$.\n5. A dataset is deemed consistent (boolean $True$) only if both the KS test and the mean-consistency test pass.\n\nImplement the above for the following test suite:\n- Test case $1$: $N = 64$, $T = 2.0$, $n_s = 20000$.\n- Test case $2$: $N = 8$, $T = 1.5$, $n_s = 20000$.\n- Test case $3$: $N = 1$, $T = 1.0$, $n_s = 10000$.\n- Test case $4$: $N = 256$, $T = 0.7$, $n_s = 30000$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with booleans ordered as $[A_1,B_1,C_1,A_2,B_2,C_2,A_3,B_3,C_3,A_4,B_4,C_4]$, where $A_i$ corresponds to Package A for test case $i$, $B_i$ to Package B, and $C_i$ to Package C.\n\nBe precise and rigorous in your implementation. Angles are not involved. Energies and temperatures are in reduced, dimensionless units with $k_{\\mathrm{B}} = 1$. No external input or files are permitted; all data must be internally generated according to the rules above.", "solution": "The objective is to develop a computational procedure to validate kinetic energy distributions from simulated molecular dynamics trajectories against the predictions of canonical ensemble theory. This involves deriving the theoretical distribution of kinetic energy and implementing statistical tests to check for consistency.\n\nThe problem is founded on the principles of statistical mechanics for the canonical ensemble, which describes a system with a fixed number of particles $N$, volume $V$, and temperature $T$. The probability of finding the system in a particular microstate with phase space coordinates $(\\mathbf{q}, \\mathbf{p})$ and Hamiltonian $H(\\mathbf{q}, \\mathbf{p})$ is given by the probability density function $p(\\mathbf{q}, \\mathbf{p}) \\propto \\exp(-\\beta H(\\mathbf{q}, \\mathbf{p}))$, where $\\beta = 1/(k_{\\mathrm{B}} T)$ and $k_{\\mathrm{B}}$ is the Boltzmann constant. The Hamiltonian is the sum of kinetic energy $K(\\mathbf{p})$ and potential energy $U(\\mathbf{q})$, so $H = K + U$.\n\nThe distribution of kinetic energy can be found by integrating the full phase space probability density over all position coordinates $\\mathbf{q}$. As $K$ depends only on momenta $\\mathbf{p}$, the resulting momentum distribution is $p(\\mathbf{p}) \\propto \\exp(-\\beta K(\\mathbf{p}))$. For a system of $N$ identical particles of mass $m$, the kinetic energy is the sum of contributions from all $3N$ Cartesian momentum components $p_j$:\n$$K = \\sum_{j=1}^{3N} \\frac{p_j^2}{2m}$$\nThis implies that each momentum component $p_j$ is an independent random variable drawn from a Gaussian distribution with mean $0$ and variance $m/\\beta = m k_{\\mathrm{B}} T$. Consequently, the scaled momentum component $p_j / \\sqrt{m k_{\\mathrm{B}} T}$ is a standard normal variable, $p_j / \\sqrt{m k_{\\mathrm{B}} T} \\sim \\mathcal{N}(0, 1)$.\n\nThe scaled instantaneous kinetic energy, denoted as $Y$, is defined as:\n$$Y = \\frac{2K}{k_{\\mathrm{B}} T} = \\frac{2}{k_{\\mathrm{B}} T} \\sum_{j=1}^{3N} \\frac{p_j^2}{2m} = \\sum_{j=1}^{3N} \\left(\\frac{p_j}{\\sqrt{m k_{\\mathrm{B}} T}}\\right)^2$$\nBy definition, a chi-square ($\\chi^2$) distribution with $f$ degrees of freedom is the distribution of a sum of the squares of $f$ independent standard normal random variables. If all $3N$ momentum components were independent, $Y$ would follow a chi-square distribution with $3N$ degrees of freedom, $Y \\sim \\chi^2_{3N}$.\n\nHowever, in typical molecular dynamics simulations of isolated systems in the NVT ensemble, the total linear momentum of the system is conserved and often fixed to zero to keep the center of mass stationary. This imposes $3$ constraints on the momentum components:\n$$\\sum_{i=1}^{N} \\mathbf{p}_i = \\mathbf{0} \\quad \\implies \\quad \\sum_{i=1}^{N} p_{ix} = 0, \\quad \\sum_{i=1}^{N} p_{iy} = 0, \\quad \\sum_{i=1}^{N} p_{iz} = 0$$\nThese constraints reduce the number of independent quadratic terms in the kinetic-energy sum by $3$. Therefore, the correct number of translational degrees of freedom is $f = 3N - 3$. The scaled kinetic energy $Y$ for such a system is correctly described by a chi-square distribution with $f = 3N-3$ degrees of freedom, $Y \\sim \\chi^2_{f}$. This distribution has an expected mean of $\\mathbb{E}[Y] = f$ and a variance of $\\mathrm{Var}[Y] = 2f$.\n\nThe validation procedure is designed to test if the kinetic energy samples $\\{K_i\\}$ from a given simulation conform to this theoretical expectation. For each test case defined by $N$, $T$, and sample size $n_s$, and for each of the three hypothetical software packages, the following steps are performed:\n\n1.  **DOF Calculation and Sanity Check**: The number of degrees of freedom $f = 3N - 3$ is computed. If $N \\le 1$, then $f \\le 0$. The $\\chi^2_0$ distribution is a degenerate point mass at $0$, for which standard statistical tests are ill-defined. As per the problem specification, such cases are automatically deemed invalid, and the validation for all three packages returns a failure (boolean $False$).\n\n2.  **Data Generation and Scaling**: Synthetic kinetic energy samples are generated for each package.\n    - **Package A**: Simulates correct canonical sampling by drawing a scaled energy $Y$ from $\\chi^2_f$ with $f = 3N-3$.\n    - **Package B**: Simulates incorrect degrees of freedom accounting by drawing $Y$ from $\\chi^2_{3N}$.\n    - **Package C**: Simulates non-canonical sampling with suppressed energy fluctuations. It generates a sample $Y$ from $\\chi^2_f$ and then transforms it via $Y' = f + \\alpha_v (Y - f)$ with a variance shrink factor $\\alpha_v = 0.6$.\n    For each package, the kinetic energy samples are calculated as $K = (k_{\\mathrm{B}} T/2) Y$ (or $Y'$), with the Boltzmann constant $k_{\\mathrm{B}}=1$ as per the problem's use of reduced units. For subsequent testing, these $K$ samples are scaled back to $Y_i = 2K_i/T$.\n\n3.  **Kolmogorov-Smirnov (KS) Test**: This is a non-parametric goodness-of-fit test that compares the cumulative distribution function (CDF) of the sample data $\\{Y_i\\}$ with the theoretical CDF of the target distribution, which is $\\chi^2_f$. The test yields a $p$-value, which is the probability of observing a sample at least as extreme as the one measured, under the null hypothesis that the data are drawn from the target distribution. The test is considered passed if the $p$-value is greater than or equal to the chosen significance level $\\alpha = 0.01$. A low $p$-value ($p < \\alpha$) indicates a significant deviation in the distribution's shape.\n\n4.  **Mean Consistency Test**: This test checks if the sample mean of the scaled kinetic energy is consistent with its theoretical expectation. The sample mean $\\overline{Y} = \\frac{1}{n_s}\\sum_{i=1}^{n_s} Y_i$ is calculated. According to the Central Limit Theorem, for a large number of samples $n_s$, $\\overline{Y}$ is approximately normally distributed with mean $\\mathbb{E}[\\overline{Y}] = f$ and variance $\\mathrm{Var}[\\overline{Y}] = \\mathrm{Var}[Y]/n_s = 2f/n_s$. A $z$-score is computed to measure the deviation of the sample mean from the expected mean in units of standard error:\n    $$z = \\frac{|\\overline{Y} - f|}{\\sqrt{2f/n_s}}$$\n    This $z$-score is compared against a critical value $z_{\\mathrm{crit}}$ from the standard normal distribution for a two-tailed test at significance level $\\alpha$. The critical value is $z_{\\mathrm{crit}} = \\Phi^{-1}(1 - \\alpha/2)$, where $\\Phi^{-1}$ is the inverse CDF of the standard normal distribution. For $\\alpha = 0.01$, $z_{\\mathrm{crit}} \\approx 2.576$. The test passes if $z \\le z_{\\mathrm{crit}}$.\n\n5.  **Final Verdict**: A dataset is considered consistent with the canonical ensemble theory only if it passes both the KS test and the mean consistency test.\n\nThis rigorous two-part validation is robust. The mean test is sensitive to systematic errors in the average energy (e.g., incorrect DOF), while the KS test is sensitive to the overall shape of the distribution, including its variance and higher moments, making it capable of detecting more subtle deviations from canonical sampling, such as the suppressed fluctuations in Package C. Package A should pass, while Packages B (wrong mean) and C (wrong variance) are expected to fail.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import kstest, chi2, norm\n\ndef solve():\n    \"\"\"\n    Main function to run the cross-validation for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (N, T, n_s)\n        (64, 2.0, 20000),\n        (8, 1.5, 20000),\n        (1, 1.0, 10000),\n        (256, 0.7, 30000),\n    ]\n    \n    # Constants from the problem statement\n    k_B = 1.0\n    alpha_v = 0.6\n    alpha_sig = 0.01\n\n    results = []\n    \n    for i, (N, T, n_s) in enumerate(test_cases, 1):\n        # Step 1: Compute DOF and handle the degenerate case\n        f = 3 * N - 3\n        if f <= 0:\n            results.extend([False, False, False])\n            continue\n            \n        # Define random seeds for reproducibility\n        seed_A = 12345 + i\n        seed_B = 22345 + i\n        seed_C = 32345 + i\n        \n        # --- Data Generation ---\n\n        # Package A: Correct canonical sampling\n        rng_A = np.random.default_rng(seed_A)\n        Y_samples_A = rng_A.chisquare(f, n_s)\n        K_samples_A = (k_B * T / 2.0) * Y_samples_A\n        \n        # Package B: Incorrect DOF accounting\n        rng_B = np.random.default_rng(seed_B)\n        Y_samples_B = rng_B.chisquare(3 * N, n_s)\n        K_samples_B = (k_B * T / 2.0) * Y_samples_B\n        \n        # Package C: Non-canonical sampling with suppressed variance\n        rng_C = np.random.default_rng(seed_C)\n        Y_base_samples_C = rng_C.chisquare(f, n_s)\n        Y_samples_C = f + alpha_v * (Y_base_samples_C - f)\n        K_samples_C = (k_B * T / 2.0) * Y_samples_C\n        \n        # --- Validation ---\n        \n        result_A = validate_dataset(K_samples_A, N, T, n_s, alpha_sig)\n        result_B = validate_dataset(K_samples_B, N, T, n_s, alpha_sig)\n        result_C = validate_dataset(K_samples_C, N, T, n_s, alpha_sig)\n        \n        results.extend([result_A, result_B, result_C])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef validate_dataset(K_samples: np.ndarray, N: int, T: float, n_s: int, alpha: float) -> bool:\n    \"\"\"\n    Performs the validation procedure for a single dataset.\n    \n    Args:\n        K_samples: Array of kinetic energy samples.\n        N: Number of particles.\n        T: Temperature.\n        n_s: Number of samples.\n        alpha: Significance level.\n\n    Returns:\n        True if the dataset passes both validation tests, False otherwise.\n    \"\"\"\n    f = 3 * N - 3\n    k_B = 1.0\n\n    # Step 2: Compute scaled kinetic energies\n    Y_samples = 2 * K_samples / (k_B * T)\n    \n    # Step 3: Kolmogorov-Smirnov goodness-of-fit test\n    # The 'args' parameter provides the degrees of freedom for the chi2 distribution.\n    ks_statistic, p_value = kstest(Y_samples, 'chi2', args=(f,))\n    ks_pass = p_value >= alpha\n\n    # Step 4: Mean consistency test\n    sample_mean_Y = np.mean(Y_samples)\n    expected_mean_Y = f\n    \n    # Variance of the theoretical chi-square distribution with f DOFs.\n    expected_var_Y = 2 * f\n    \n    # The standard error is the standard deviation of the sample mean's distribution.\n    # The denominator can't be zero due to the f > 0 check in the main loop.\n    std_error_of_mean = np.sqrt(expected_var_Y / n_s)\n    \n    z_score = np.abs(sample_mean_Y - expected_mean_Y) / std_error_of_mean\n    \n    # Critical value for a two-tailed test.\n    z_crit = norm.ppf(1 - alpha / 2.0)\n    \n    mean_pass = z_score <= z_crit\n    \n    # Step 5: A dataset is consistent only if both tests pass.\n    return ks_pass and mean_pass\n\n# Execute the main function\nsolve()\n```", "id": "3410952"}, {"introduction": "Ensemble equivalence is a powerful concept, but it is not universally guaranteed and can break down, most notably in systems undergoing phase transitions. This practice explores this breakdown using the Curie-Weiss mean-field Ising model, a cornerstone for understanding collective phenomena [@problem_id:3410966]. By deriving and comparing the microcanonical entropy and the canonical free energy, you will pinpoint the critical temperature where the underlying entropy function becomes non-concave, giving rise to a phase transition that is perceived differently by the two ensembles.", "problem": "Consider the Curie–Weiss mean-field Ising model with $N$ spins $\\sigma_{i}\\in\\{-1,+1\\}$, interaction strength $J>0$, and external magnetic field $h\\in\\mathbb{R}$. The Hamiltonian is\n$$\nH_{N}(\\sigma) \\;=\\; -\\,\\frac{J}{2N}\\,\\Big(\\sum_{i=1}^{N}\\sigma_{i}\\Big)^{2}\\;-\\;h\\sum_{i=1}^{N}\\sigma_{i}.\n$$\nLet the magnetization per spin be $m=\\frac{1}{N}\\sum_{i=1}^{N}\\sigma_{i}$ and the energy per spin be $e=\\frac{1}{N}H_{N}(\\sigma)$. Work in the thermodynamic limit $N\\to\\infty$, and use standard combinatorial counting and large-deviation (Laplace) principles as needed.\n\n(a) Starting only from the definitions of the microcanonical entropy and the Hamiltonian, derive the microcanonical entropy density $s(e,m)$, defined as the leading-order exponential growth rate of the number of configurations with energy density $e$ and magnetization $m$. Clearly state the domain of $(e,m)$ on which $s(e,m)$ is finite and give its explicit expression on that domain.\n\n(b) Using the canonical ensemble at inverse temperature $\\beta>0$, express the canonical free-energy density $f(\\beta,h)$ in the thermodynamic limit in terms of a variational problem over $m$. Your derivation must start from the canonical partition function and proceed by reducing the sum over microstates to a sum over magnetizations weighted by their degeneracies and Boltzmann factors. Give the final closed-form variational expression for $f(\\beta,h)$.\n\n(c) Specialize to zero field $h=0$. Analyze the curvature of the objective in the variational expression for $f(\\beta,0)$ with respect to $m$ at $m=0$ and determine when it ceases to be concave there, thereby indicating a loss of global concavity and the onset of $m$-constrained ensemble inequivalence driven by nonconcavity in the entropy contribution. As your final answer, provide the analytic expression for the critical inverse temperature $\\beta_{c}$ in terms of $J$ that separates the concave and nonconcave regimes. No rounding is required; report $\\beta_{c}$ as a single closed-form expression with no units.", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard problem in statistical mechanics concerning the Curie-Weiss model, and all terms and requested derivations are conventional within the field. We proceed with the solution.\n\n(a) Derivation of the microcanonical entropy density $s(e,m)$.\n\nThe state of the system is defined by a configuration of $N$ spins $\\sigma = \\{\\sigma_1, \\dots, \\sigma_N\\}$ where $\\sigma_i \\in \\{-1, +1\\}$. The magnetization per spin is given by $m = \\frac{1}{N}\\sum_{i=1}^{N}\\sigma_{i}$. Let $N_+$ be the number of spins with value $+1$ and $N_-$ be the number of spins with value $-1$. We have the relations:\n$N = N_+ + N_-$\n$Nm = N_+ - N_-$\nSolving for $N_+$ and $N_-$ in terms of $N$ and $m$ yields:\n$$N_+ = N\\frac{1+m}{2} \\quad \\text{and} \\quad N_- = N\\frac{1-m}{2}$$\nFor $N_+$ and $N_-$ to be integers between $0$ and $N$, the allowed values of $m$ are discrete, ranging from $-1$ to $1$ in steps of $2/N$.\n\nThe Hamiltonian of the system is $H_{N}(\\sigma) = -\\frac{J}{2N}\\left(\\sum_{i=1}^{N}\\sigma_{i}\\right)^{2} - h\\sum_{i=1}^{N}\\sigma_{i}$. It depends only on the total magnetization $\\sum_{i=1}^{N}\\sigma_{i}=Nm$. Thus, for a given magnetization $m$, all microstates with that magnetization have the same energy:\n$$H_N(m) = -\\frac{J}{2N}(Nm)^2 - h(Nm) = -N\\left(\\frac{J}{2}m^2 + hm\\right)$$\nThe energy per spin, $e = H_N/N$, is therefore a deterministic function of the magnetization $m$:\n$$e(m) = -\\frac{J}{2}m^2 - hm$$\nThis relationship implies that the macroscopic variables $e$ and $m$ are not independent for this model. A configuration with magnetization $m$ necessarily has energy density $e(m)$.\n\nThe microcanonical entropy density $s(e,m)$ is defined as the leading-order exponential growth rate of the number of configurations, $\\Omega(N,e,m)$, with given energy density $e$ and magnetization $m$.\n$$s(e,m) = \\lim_{N\\to\\infty} \\frac{1}{N}\\ln \\Omega(N, e, m)$$\nDue to the constraint $e=e(m)$, the number of configurations is non-zero only on the curve defined by this relation. If $e \\neq -\\frac{J}{2}m^2 - hm$, then $\\Omega(N,e,m) = 0$, and consequently $s(e,m) = -\\infty$.\nIf $e = -\\frac{J}{2}m^2 - hm$, then the set of configurations is simply the set of all configurations with magnetization $m$. The number of such configurations is given by the binomial coefficient for arranging $N_+$ up-spins among $N$ sites:\n$$\\Omega(N, e(m), m) = \\binom{N}{N_+} = \\binom{N}{N\\frac{1+m}{2}}$$\nWe now compute the entropy density using Stirling's approximation for the logarithm of the binomial coefficient, $\\ln \\binom{N}{k} \\approx -N [p\\ln p + (1-p)\\ln(1-p)]$ for $p=k/N$ and large $N$. Setting $p = \\frac{1+m}{2}$, we get:\n$$s(m) = \\lim_{N\\to\\infty} \\frac{1}{N}\\ln\\binom{N}{N\\frac{1+m}{2}} = -\\left(\\frac{1+m}{2}\\right)\\ln\\left(\\frac{1+m}{2}\\right) - \\left(\\frac{1-m}{2}\\right)\\ln\\left(\\frac{1-m}{2}\\right)$$\nThe domain on which $s(e,m)$ is finite requires $m \\in [-1, 1]$ for the arguments of the logarithms to be non-negative and the spin counts to be valid. The entropy is zero at $m=\\pm 1$ and positive for $m\\in(-1,1)$.\nThus, the microcanonical entropy density $s(e,m)$ is given by:\n$$s(e,m) = \\begin{cases} -\\left(\\frac{1+m}{2}\\right)\\ln\\left(\\frac{1+m}{2}\\right) - \\left(\\frac{1-m}{2}\\right)\\ln\\left(\\frac{1-m}{2}\\right) & \\text{if } e = -\\frac{J}{2}m^2 - hm \\\\ -\\infty & \\text{otherwise} \\end{cases}$$\nThe domain where $s(e,m)$ is finite is the one-dimensional curve in the $(e,m)$-plane defined by $e = -\\frac{J}{2}m^2 - hm$ for $m \\in [-1, 1]$.\n\n(b) Variational expression for the canonical free-energy density $f(\\beta,h)$.\n\nThe canonical partition function at inverse temperature $\\beta$ is $Z_N(\\beta, h) = \\sum_{\\sigma} \\exp(-\\beta H_N(\\sigma))$. Since the Hamiltonian $H_N$ depends only on the magnetization $m$, we can rewrite the sum over all $2^N$ spin configurations as a sum over all possible values of magnetization, weighted by the number of configurations $\\Omega(N,m)$ for each $m$:\n$$Z_N(\\beta, h) = \\sum_{m} \\Omega(N,m) \\exp(-\\beta H_N(m))$$\nSubstituting the expressions for $\\Omega(N,m)$ (in the large $N$ limit) and $H_N(m)$:\n$$Z_N(\\beta, h) \\approx \\sum_{m} \\exp(N s(m)) \\exp\\left(-\\beta N\\left(-\\frac{J}{2}m^2 - hm\\right)\\right) = \\sum_{m} \\exp\\left(N\\left[s(m) + \\beta\\frac{J}{2}m^2 + \\beta hm\\right]\\right)$$\nIn the thermodynamic limit $N\\to\\infty$, the sum is dominated by the term with the maximum exponent. This is an application of Laplace's method. The sum can be approximated by its largest term (or an integral that is evaluated by the maximum of the integrand's exponent).\n$$\\lim_{N\\to\\infty} \\frac{1}{N}\\ln Z_N(\\beta, h) = \\max_{m \\in [-1,1]} \\left\\{ s(m) + \\beta\\frac{J}{2}m^2 + \\beta hm \\right\\}$$\nThe canonical free-energy density is defined as $f(\\beta, h) = \\lim_{N\\to\\infty} -\\frac{1}{N\\beta}\\ln Z_N(\\beta,h)$. Therefore:\n$$f(\\beta, h) = -\\frac{1}{\\beta} \\max_{m \\in [-1,1]} \\left\\{ s(m) + \\beta\\frac{J}{2}m^2 + \\beta hm \\right\\}$$\nThis can be rewritten as a minimization problem:\n$$f(\\beta, h) = \\min_{m \\in [-1,1]} \\left\\{ -\\frac{1}{\\beta}s(m) - \\frac{J}{2}m^2 - hm \\right\\}$$\nSubstituting the explicit formula for $s(m)$ from part (a), we obtain the final variational expression for the free-energy density:\n$$f(\\beta, h) = \\min_{m \\in [-1,1]} \\left\\{ \\frac{1}{\\beta}\\left[\\left(\\frac{1+m}{2}\\right)\\ln\\left(\\frac{1+m}{2}\\right) + \\left(\\frac{1-m}{2}\\right)\\ln\\left(\\frac{1-m}{2}\\right)\\right] - \\frac{J}{2}m^2 - hm \\right\\}$$\nThis expresses $f(\\beta,h)$ as the minimum value of a function of a single variational parameter $m$.\n\n(c) Analysis of curvature and determination of the critical inverse temperature $\\beta_c$.\n\nWe specialize to the case of zero external field, $h=0$. The objective function in the variational problem for $f(\\beta, 0)$ is the Ginzburg-Landau-like free energy functional:\n$$\\phi(m; \\beta) = \\frac{1}{\\beta}\\left[\\left(\\frac{1+m}{2}\\right)\\ln\\left(\\frac{1+m}{2}\\right) + \\left(\\frac{1-m}{2}\\right)\\ln\\left(\\frac{1-m}{2}\\right)\\right] - \\frac{J}{2}m^2$$\nThe problem asks for an analysis of the curvature of this objective function at $m=0$. The curvature is given by the second derivative, $\\frac{d^2\\phi}{dm^2}$. We first compute the derivatives of the entropic part. Let $g(m) = \\left(\\frac{1+m}{2}\\right)\\ln\\left(\\frac{1+m}{2}\\right) + \\left(\\frac{1-m}{2}\\right)\\ln\\left(\\frac{1-m}{2}\\right)$.\nThe first derivative with respect to $m$ is:\n$$\\frac{dg}{dm} = \\frac{1}{2}\\left[\\ln\\left(\\frac{1+m}{2}\\right)+1\\right] - \\frac{1}{2}\\left[\\ln\\left(\\frac{1-m}{2}\\right)+1\\right] = \\frac{1}{2}\\ln\\left(\\frac{1+m}{1-m}\\right) = \\text{arctanh}(m)$$\nThe second derivative is:\n$$\\frac{d^2g}{dm^2} = \\frac{d}{dm} \\text{arctanh}(m) = \\frac{1}{1-m^2}$$\nNow, we compute the second derivative of the full objective function $\\phi(m; \\beta)$:\n$$\\frac{d^2\\phi}{dm^2} = \\frac{1}{\\beta}\\frac{d^2g}{dm^2} - J = \\frac{1}{\\beta(1-m^2)} - J$$\nWe evaluate this curvature at the point $m=0$:\n$$\\frac{d^2\\phi}{dm^2}\\bigg|_{m=0} = \\frac{1}{\\beta} - J$$\nThe problem asks when the objective function $\\phi(m;\\beta)$ \"ceases to be concave\" at $m=0$. A function is locally concave at a point if its second derivative is less than or equal to zero. Thus, $\\phi(m;\\beta)$ is concave at $m=0$ if:\n$$\\frac{1}{\\beta} - J \\leq 0 \\quad \\implies \\quad 1 \\leq \\beta J \\quad \\implies \\quad \\beta \\geq \\frac{1}{J}$$\nThis corresponds to the low-temperature phase, where $m=0$ is an unstable point (a local maximum), and ferromagnetic order appears. The function \"ceases to be concave\" at $m=0$ when this condition is violated, i.e., when the curvature becomes positive. The transition occurs at the critical point where the curvature is exactly zero.\n$$\\frac{1}{\\beta_c} - J = 0$$\nSolving for the critical inverse temperature $\\beta_c$ gives:\n$$\\beta_c = \\frac{1}{J}$$\nFor $\\beta < \\beta_c$ (high temperatures), the curvature at $m=0$ is positive, making the objective function locally convex and yielding a stable paramagnetic state at $m=0$. For $\\beta > \\beta_c$ (low temperatures), the curvature is negative, the objective is locally concave, and the state at $m=0$ becomes unstable, leading to a phase transition to a ferromagnetic state with $m \\neq 0$.", "answer": "$$\\boxed{\\frac{1}{J}}$$", "id": "3410966"}]}