{"hands_on_practices": [{"introduction": "The dot product is a fundamental building block in countless numerical algorithms. This exercise [@problem_id:3533794] provides a foundational look into backward error analysis by comparing three different strategies for its computation. By deriving explicit backward error bounds for naive, pairwise, and compensated summation from first principles, you will develop a concrete understanding of how algorithmic design choices directly impact the stability of the result.", "problem": "Consider the dot product $s = x^{\\top} y$ for the given vectors\n$$\nx = \\begin{pmatrix}\n2^{10} & -3.7 & 5.1 & -8.2 & 1.3 & -0.75 & 9.9 & -4.0\n\\end{pmatrix}^{\\top}, \\quad\ny = \\begin{pmatrix}\n1.0 & 2.5 & -3.1 & 4.2 & -5.3 & 6.4 & -7.5 & 8.6\n\\end{pmatrix}^{\\top}.\n$$\nAssume arithmetic is performed in the Institute of Electrical and Electronics Engineers (IEEE) 754 binary64 (double precision) model with rounding to nearest, so that the unit roundoff is $u = 2^{-53}$. Use the standard floating-point model $\\mathrm{fl}(a \\ \\mathrm{op}\\ b) = (a \\ \\mathrm{op}\\ b)(1 + \\delta)$ with $|\\delta| \\le u$ for each basic operation $\\mathrm{op} \\in \\{+, -, \\times\\}$.\n\nDefine three algorithms for computing an approximation $\\widehat{s}$ to $s$:\n- Algorithm A (naive): compute $s_i = \\mathrm{fl}(x_i \\times y_i)$ for $i = 1,\\dots,n$, then accumulate $\\widehat{s} = \\mathrm{fl}(\\cdots \\mathrm{fl}(\\mathrm{fl}(s_1 + s_2) + s_3) + \\cdots + s_n)$ from left to right.\n- Algorithm B (pairwise): compute $s_i = \\mathrm{fl}(x_i \\times y_i)$ for $i = 1,\\dots,n$, then sum $\\{s_i\\}$ pairwise in a balanced binary tree using $\\mathrm{fl}(\\cdot + \\cdot)$ at each internal node.\n- Algorithm C (compensated/Kahan): compute $s_i = \\mathrm{fl}(x_i \\times y_i)$ for $i = 1,\\dots,n$, then apply the compensated summation loop\n  $\n  c \\leftarrow 0,\\; \\widehat{s} \\leftarrow 0,\\; \\text{for } i=1{:}n:\\; y' \\leftarrow \\mathrm{fl}(s_i - c),\\; t \\leftarrow \\mathrm{fl}(\\widehat{s} + y'),\\; c \\leftarrow \\mathrm{fl}(\\mathrm{fl}(t - \\widehat{s}) - y'),\\; \\widehat{s} \\leftarrow t.\n  $\n\nLet $n$ denote the vector length, here $n = 8$. For each algorithm, derive from first principles of the floating-point model a componentwise backward error bound of the form: there exists a perturbation $\\Delta x$ such that\n$$\n\\widehat{s} = (x + \\Delta x)^{\\top} y, \\quad |\\Delta x| \\le \\theta_{\\mathrm{alg}} |x|\n$$\n(componentwise inequality), and obtain an explicit computable upper bound for $\\theta_{\\mathrm{alg}}$ as a function of $u$ and $n$ (and the algorithm-specific structure). Then evaluate these bounds numerically for $u = 2^{-53}$ and $n = 8$.\n\nReport your final result as a row matrix containing the three numerical values\n$$\n\\theta_{\\mathrm{naive}},\\; \\theta_{\\mathrm{pairwise}},\\; \\theta_{\\mathrm{comp}}\n$$\nin that order, rounded to four significant figures. No units are required.", "solution": "The problem as stated is a valid exercise in the backward error analysis of floating-point algorithms. It is scientifically grounded in the standard model of floating-point arithmetic, is well-posed with all necessary data provided, and is expressed objectively. Therefore, a solution will be provided.\n\nThe objective is to find a bound $\\theta_{\\mathrm{alg}}$ for three different dot product algorithms, such that the computed result $\\widehat{s}$ is the exact dot product of a perturbed vector $x+\\Delta x$ and the original vector $y$. That is,\n$$ \\widehat{s} = (x + \\Delta x)^{\\top} y $$\nwith the componentwise backward error satisfying $|\\Delta x| \\le \\theta_{\\mathrm{alg}} |x|$. This is equivalent to finding a bound $\\theta_{\\mathrm{alg}}$ such that for each component $i=1, \\dots, n$:\n$$ |\\Delta x_i| \\le \\theta_{\\mathrm{alg}} |x_i| $$\nThe computed sum $\\widehat{s}$ can be generally expressed as a sum of the exact products $x_i y_i$ with some multiplicative error, $\\widehat{s} = \\sum_{i=1}^n x_i y_i (1+\\zeta_i)$. Equating this with the desired backward error form gives:\n$$ \\sum_{i=1}^n (x_i + \\Delta x_i) y_i = \\sum_{i=1}^n x_i y_i (1 + \\zeta_i) $$\n$$ \\sum_{i=1}^n \\Delta x_i y_i = \\sum_{i=1}^n x_i y_i \\zeta_i $$\nThis equality must hold for any vector $y$. This implies that we must have $\\Delta x_i y_i = x_i y_i \\zeta_i$ for each $i$, which simplifies to $\\Delta x_i = x_i \\zeta_i$. The condition $|\\Delta x| \\le \\theta_{\\mathrm{alg}} |x|$ thus becomes $|x_i \\zeta_i| \\le \\theta_{\\mathrm{alg}} |x_i|$, which means we need to find a uniform bound $\\theta_{\\mathrm{alg}} \\ge |\\zeta_i|$ for all $i=1, \\dots, n$.\n\nWe use the standard floating-point model where for any basic operation $\\mathrm{op}$, $\\mathrm{fl}(a \\ \\mathrm{op} \\ b) = (a \\ \\mathrm{op} \\ b)(1+\\delta)$ with $|\\delta| \\le u$. A product of $k$ such factors satisfies $\\prod_{j=1}^k (1+\\delta_j) = 1+\\eta_k$ where $|\\eta_k| \\le \\gamma_k = \\frac{ku}{1-ku}$, assuming $ku < 1$.\n\nFor all three algorithms, the first step is to compute the products $s_i = \\mathrm{fl}(x_i y_i)$.\n$$ s_i = x_i y_i (1+\\delta'_i), \\quad |\\delta'_i| \\le u $$\nThe remaining analysis concerns how these products $\\{s_i\\}$ are summed.\n\n**Algorithm A: Naive Summation**\nIn the naive left-to-right summation, the computed sum $\\widehat{s}$ takes the form\n$$ \\widehat{s} = \\mathrm{fl}(\\cdots \\mathrm{fl}(\\mathrm{fl}(s_1 + s_2) + s_3) + \\cdots + s_n) $$\nA detailed analysis of the propagation of rounding errors shows that this can be written as:\n$$ \\widehat{s} = s_1 \\prod_{k=2}^n(1+\\delta_k) + s_2 \\prod_{k=2}^n(1+\\delta_k) + s_3 \\prod_{k=3}^n(1+\\delta_k) + \\dots + s_n(1+\\delta_n) $$\nwhere each $|\\delta_k| \\le u$.\nSubstituting $s_i = x_i y_i (1+\\delta'_i)$, the term for $x_i y_i$ in the total sum is $x_i y_i (1+\\zeta_i)$, where $1+\\zeta_i$ is a product of terms $(1+\\delta)$.\nFor $i=1$: $1+\\zeta_1 = (1+\\delta'_1) \\prod_{k=2}^n(1+\\delta_k)$. This is a product of $1+(n-1)=n$ perturbation terms. Thus, $|\\zeta_1| \\le \\gamma_n$.\nFor $i=2$: $1+\\zeta_2 = (1+\\delta'_2) \\prod_{k=2}^n(1+\\delta_k)$. This is also a product of $n$ perturbation terms. Thus, $|\\zeta_2| \\le \\gamma_n$.\nFor $i > 2$: $1+\\zeta_i = (1+\\delta'_i) \\prod_{k=i}^n(1+\\delta_k)$. This is a product of $1+(n-i+1) = n-i+2$ terms. Thus, $|\\zeta_i| \\le \\gamma_{n-i+2}$.\nThe bound must hold for all $i$. The worst case is for $i=1$ and $i=2$, which gives the uniform bound $|\\zeta_i| \\le \\gamma_n$. Therefore,\n$$ \\theta_{\\mathrm{naive}} = \\gamma_n = \\frac{nu}{1-nu} $$\n\n**Algorithm B: Pairwise Summation**\nIn pairwise summation, for $n=8$ (a power of $2$), the summation proceeds in a balanced binary tree. The number of summation steps for any single term $s_i$ to be included in the final sum is $\\log_2(n)$.\nThe computed value $\\widehat{s}$ can be written as $\\sum_{i=1}^n x_i y_i (1+\\zeta_i)$. The term $1+\\zeta_i$ corresponds to the accumulated multiplicative errors affecting $x_i y_i$. This consists of one error from the multiplication $x_i y_i$ and $\\log_2(n)$ errors from the summation tree. Each term $x_i y_i$ is affected by a total of $1+\\log_2(n)$ floating-point operations. The accumulated error factor is therefore bounded by $\\gamma_{1+\\log_2(n)}$. This bound is uniform for all $i$.\n$$ \\theta_{\\mathrm{pairwise}} = \\gamma_{1+\\log_2 n} = \\frac{(1+\\log_2 n)u}{1-(1+\\log_2 n)u} $$\n\n**Algorithm C: Compensated (Kahan) Summation**\nCompensated summation uses a correction term $c$ to track the roundoff error from the main summation. Let $\\widehat{s}$ be the result of a compensated sum of terms $\\{s_i\\}$. A standard result from detailed error analysis (e.g., Higham, 1993, \"The Accuracy of Floating Point Summation\") shows that $\\widehat{s} = \\sum_{i=1}^n s_i(1+\\mu_i)$, where $|\\mu_i| \\le 2u + O(nu^2)$. This bound reflects that error accumulation is largely prevented.\nCombining this with the error from the initial multiplications $s_i = x_i y_i(1+\\delta'_i)$, we have:\n$$ \\widehat{s} = \\sum_{i=1}^n x_i y_i (1+\\delta'_i)(1+\\mu_i) $$\nSo, $\\zeta_i = (1+\\delta'_i)(1+\\mu_i) - 1 = \\delta'_i + \\mu_i + \\delta'_i\\mu_i$.\nTo obtain a computable bound, we use $|\\delta'_i| \\le u$ and a more explicit form for the bound on $|\\mu_i|$ from the same literature, $|\\mu_i| \\le 2u + 2(n-i+1)u^2$. For a uniform bound on $|\\zeta_i|$, we take the worst case over $i$, which is $i=1$: $|\\mu_i| \\le 2u + 2nu^2$.\n$$ |\\zeta_i| \\le (1+u)(1 + 2u + 2nu^2) - 1 = 1+2u+2nu^2+u+2u^2+2nu^3 - 1 = 3u + 2(n+1)u^2 + 2nu^3 $$\nThis gives our bound for the compensated algorithm:\n$$ \\theta_{\\mathrm{comp}} = 3u + 2(n+1)u^2 + 2nu^3 $$\nFor small $u$, this bound is approximately $3u$, demonstrating a massive improvement in accuracy that is largely independent of $n$.\n\n**Numerical Evaluation**\nWe are given $u = 2^{-53}$ and $n = 8$.\n\nFor Algorithm A (Naive):\n$$ \\theta_{\\mathrm{naive}} = \\frac{8u}{1-8u} = \\frac{8 \\times 2^{-53}}{1-8 \\times 2^{-53}} = \\frac{2^{-50}}{1-2^{-50}} \\approx 8.881784197 \\times 10^{-16} $$\nRounded to four significant figures, this is $8.882 \\times 10^{-16}$.\n\nFor Algorithm B (Pairwise):\nWith $n=8$, $\\log_2(n) = 3$.\n$$ \\theta_{\\mathrm{pairwise}} = \\frac{(1+3)u}{1-(1+3)u} = \\frac{4u}{1-4u} = \\frac{4 \\times 2^{-53}}{1-4 \\times 2^{-53}} = \\frac{2^{-51}}{1-2^{-51}} \\approx 4.440892099 \\times 10^{-16} $$\nRounded to four significant figures, this is $4.441 \\times 10^{-16}$.\n\nFor Algorithm C (Compensated):\n$$ \\theta_{\\mathrm{comp}} = 3u + 18u^2 + 16u^3 $$\n$$ \\theta_{\\mathrm{comp}} = 3 \\times 2^{-53} + 18 \\times (2^{-53})^2 + 16 \\times (2^{-53})^3 = 3 \\times 2^{-53} + 18 \\times 2^{-106} + 16 \\times 2^{-159} $$\nThe terms in $u^2$ and $u^3$ are orders of magnitude smaller than the term in $u$.\nFirst term: $3 \\times 2^{-53} \\approx 3.330669074 \\times 10^{-16}$.\nSecond term: $18 \\times 2^{-106} \\approx 2.21859... \\times 10^{-31}$.\nThe higher-order terms do not affect the first four significant figures.\n$$ \\theta_{\\mathrm{comp}} \\approx 3u \\approx 3.330669074 \\times 10^{-16} $$\nRounded to four significant figures, this is $3.331 \\times 10^{-16}$.\n\nThe final reported values are the three numerical bounds for $\\theta_{\\mathrm{naive}}$, $\\theta_{\\mathrm{pairwise}}$, and $\\theta_{\\mathrm{comp}}$.", "answer": "$$ \\boxed{ \\begin{pmatrix} 8.882 \\times 10^{-16} & 4.441 \\times 10^{-16} & 3.331 \\times 10^{-16} \\end{pmatrix} } $$", "id": "3533794"}, {"introduction": "Gaussian elimination with partial pivoting (GEPP) is a cornerstone algorithm for solving linear systems, widely used due to its general backward stability. This stability, however, is not guaranteed to be ideal in all cases. This practice [@problem_id:3533819] delves into the crucial concept of the pivot growth factor, $\\rho$, by having you analyze a matrix specifically constructed to exhibit large growth. This exercise will demonstrate how the backward error bound for GEPP is moderated by this factor and solidifies the understanding that stability analysis often involves more than just the algorithm itself.", "problem": "Consider Gaussian elimination with partial pivoting (GEPP), which factors a nonsingular matrix $A \\in \\mathbb{R}^{n \\times n}$ into a unit lower triangular factor $L$ and an upper triangular factor $U$ after a sequence of row interchanges. The pivot growth factor is defined elementwise by\n$$\\rho \\;=\\; \\frac{\\max_{i,j} |u_{i j}|}{\\max_{i,j} |a_{i j}|},$$\nwhere $U$ is the exact upper triangular factor that would be produced by GEPP in exact arithmetic on the given $A$. Work in the floating-point model in which each elementary arithmetic operation $x \\circ y$ ($\\circ \\in \\{+,-,\\times,\\div\\}$) is replaced by $\\operatorname{fl}(x \\circ y) = (x \\circ y)(1+\\delta)$ with $|\\delta| \\leq u$, where $u$ is the unit roundoff. Assume the GEPP tie-breaking rule that, when multiple candidates have equal maximum magnitude in a pivot column, the pivot chosen is the one with smallest row index among those candidates.\n\nConstruct a concrete matrix by considering the explicit $6 \\times 6$ matrix $A$ defined by\n- $a_{i i} = 1$ for $i=1,2,3,4,5,6$,\n- $a_{i+1,i} = -1$ for $i=1,2,3,4,5$,\n- $a_{i,6} = 1$ for $i=1,2,3,4,5,6$,\n- all other entries of $A$ are $0$.\n\nTasks:\n1. Apply the GEPP process symbolically to $A$ (respecting the stated tie-breaking rule), and identify the exact upper triangular factor $U$ that would be produced in exact arithmetic.\n2. Compute the exact pivot growth factor $\\rho$ for this $A$.\n3. Using the standard floating-point error model, derive a normwise backward error bound of the form $\\|\\Delta A\\|_{\\infty} / \\|A\\|_{\\infty}$ expressed symbolically in terms of $u$ and quantities you compute from $A$, $L$, and $U$. Base your derivation on first principles: the floating-point model, the definition of backward error (the existence of $\\Delta A$ such that $(A+\\Delta A)\\hat{x}=\\hat{b}$ for the computed solution $\\hat{x}$), and norm submultiplicativity; do not assume any prepackaged stability theorems.\n4. From that backward error bound, deduce a forward error implication for solving the linear system $A x = b$ in terms of the infinity-norm condition number $\\kappa_{\\infty}(A) = \\|A\\|_{\\infty}\\,\\|A^{-1}\\|_{\\infty}$ and the unit roundoff $u$; make your inference explicit for this specific $A$ by computing $\\|A\\|_{\\infty}$ and $\\|A^{-1}\\|_{\\infty}$.\n\nYour final reported result must be the exact value of the pivot growth factor $\\rho$ for this matrix $A$. No rounding is required. Express the final result as a single real number.", "solution": "The user has provided a problem in numerical linear algebra. I will first validate the problem statement according to the specified criteria and then, if it is valid, proceed with a full solution.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Algorithm**: Gaussian elimination with partial pivoting (GEPP).\n- **Matrix**: $A \\in \\mathbb{R}^{n \\times n}$ where $n=6$.\n- **Matrix Definition**:\n    - $a_{i i} = 1$ for $i=1,2,3,4,5,6$.\n    - $a_{i+1,i} = -1$ for $i=1,2,3,4,5$.\n    - $a_{i,6} = 1$ for $i=1,2,3,4,5,6$.\n    - All other entries of $A$ are $0$.\n- **Pivot Growth Factor**: $\\rho = \\frac{\\max_{i,j} |u_{i j}|}{\\max_{i,j} |a_{i j}|}$, where $U$ is the exact upper triangular factor from GEPP.\n- **Floating-Point Model**: $\\operatorname{fl}(x \\circ y) = (x \\circ y)(1+\\delta)$ with $|\\delta| \\leq u$.\n- **GEPP Tie-Breaking Rule**: When multiple pivot candidates have equal maximum magnitude, choose the one with the smallest row index.\n- **Tasks**:\n    1. Apply GEPP to $A$ to find the exact factor $U$.\n    2. Compute the exact pivot growth factor $\\rho$.\n    3. Derive a normwise backward error bound $\\|\\Delta A\\|_{\\infty} / \\|A\\|_{\\infty}$.\n    4. Deduce a forward error implication for solving $Ax=b$ by computing $\\kappa_{\\infty}(A)$.\n- **Final Answer Requirement**: The exact value of $\\rho$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, objective, and self-contained. It is a standard exercise in numerical linear algebra, designed to explore the concepts of pivot growth and error analysis for a specific, well-behaved (though illustrative) matrix. The matrix definition is unambiguous. The GEPP algorithm with the specified tie-breaking rule is deterministic. The tasks are clearly defined and lead to a unique, meaningful answer. There are no contradictions, missing pieces of information, or violations of scientific principles.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. I will proceed with the solution.\n\n### Solution\n\nThe given matrix $A$ is a $6 \\times 6$ matrix defined as:\n$$\nA = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & 1 \\\\\n-1 & 1 & 0 & 0 & 0 & 1 \\\\\n0 & -1 & 1 & 0 & 0 & 1 \\\\\n0 & 0 & -1 & 1 & 0 & 1 \\\\\n0 & 0 & 0 & -1 & 1 & 1 \\\\\n0 & 0 & 0 & 0 & -1 & 1\n\\end{pmatrix}\n$$\nThe maximum absolute value of any element in $A$ is $\\max_{i,j} |a_{ij}| = 1$.\n\n**Task 1: Apply GEPP to find U**\n\nWe perform Gaussian elimination with partial pivoting. Let $A^{(1)} = A$.\n\n**Step 1 ($k=1$):**\nThe first column is $(1, -1, 0, 0, 0, 0)^T$. The maximum absolute value is $1$, which occurs in rows $1$ and $2$. According to the tie-breaking rule, we choose the pivot from the row with the smallest index, so the pivot is $a_{11}^{(1)} = 1$. No row interchange is needed, so $P_1 = I$. The only multiplier is $l_{21} = a_{21}^{(1)} / a_{11}^{(1)} = -1 / 1 = -1$. We update row $2$: $R_2 \\leftarrow R_2 - l_{21} R_1 = R_2 + R_1$.\n$R_2: (-1, 1, 0, 0, 0, 1) + (1, 0, 0, 0, 0, 1) = (0, 1, 0, 0, 0, 2)$.\nThe resulting matrix is:\n$$\nA^{(2)} = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 0 & 2 \\\\\n0 & -1 & 1 & 0 & 0 & 1 \\\\\n0 & 0 & -1 & 1 & 0 & 1 \\\\\n0 & 0 & 0 & -1 & 1 & 1 \\\\\n0 & 0 & 0 & 0 & -1 & 1\n\\end{pmatrix}\n$$\n\n**Step 2 ($k=2$):**\nThe pivot column (from row $2$ downwards) is $(1, -1, 0, 0, 0)^T$. The maximum absolute value is $1$, occurring in rows $2$ and $3$. We choose the pivot $a_{22}^{(2)} = 1$. No row interchange. The multiplier is $l_{32} = a_{32}^{(2)} / a_{22}^{(2)} = -1 / 1 = -1$. Update row $3$: $R_3 \\leftarrow R_3 - l_{32} R_2 = R_3 + R_2$.\n$R_3: (0, -1, 1, 0, 0, 1) + (0, 1, 0, 0, 0, 2) = (0, 0, 1, 0, 0, 3)$.\n$$\nA^{(3)} = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 0 & 2 \\\\\n0 & 0 & 1 & 0 & 0 & 3 \\\\\n0 & 0 & -1 & 1 & 0 & 1 \\\\\n0 & 0 & 0 & -1 & 1 & 1 \\\\\n0 & 0 & 0 & 0 & -1 & 1\n\\end{pmatrix}\n$$\n\n**Step 3 ($k=3$):**\nPivot column: $(1, -1, 0, 0)^T$. Pivot is $a_{33}^{(3)} = 1$. Multiplier is $l_{43} = -1$. Update row $4$: $R_4 \\leftarrow R_4 + R_3$.\n$R_4: (0, 0, -1, 1, 0, 1) + (0, 0, 1, 0, 0, 3) = (0, 0, 0, 1, 0, 4)$.\n$$\nA^{(4)} = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 0 & 2 \\\\\n0 & 0 & 1 & 0 & 0 & 3 \\\\\n0 & 0 & 0 & 1 & 0 & 4 \\\\\n0 & 0 & 0 & -1 & 1 & 1 \\\\\n0 & 0 & 0 & 0 & -1 & 1\n\\end{pmatrix}\n$$\n\n**Step 4 ($k=4$):**\nPivot column: $(1, -1, 0)^T$. Pivot is $a_{44}^{(4)} = 1$. Multiplier is $l_{54} = -1$. Update row $5$: $R_5 \\leftarrow R_5 + R_4$.\n$R_5: (0, 0, 0, -1, 1, 1) + (0, 0, 0, 1, 0, 4) = (0, 0, 0, 0, 1, 5)$.\n$$\nA^{(5)} = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 0 & 2 \\\\\n0 & 0 & 1 & 0 & 0 & 3 \\\\\n0 & 0 & 0 & 1 & 0 & 4 \\\\\n0 & 0 & 0 & 0 & 1 & 5 \\\\\n0 & 0 & 0 & 0 & -1 & 1\n\\end{pmatrix}\n$$\n\n**Step 5 ($k=5$):**\nPivot column: $(1, -1)^T$. Pivot is $a_{55}^{(5)} = 1$. Multiplier is $l_{65} = -1$. Update row $6$: $R_6 \\leftarrow R_6 + R_5$.\n$R_6: (0, 0, 0, 0, -1, 1) + (0, 0, 0, 0, 1, 5) = (0, 0, 0, 0, 0, 6)$.\nThe final upper triangular matrix $U$ is:\n$$\nU = A^{(6)} = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 0 & 2 \\\\\n0 & 0 & 1 & 0 & 0 & 3 \\\\\n0 & 0 & 0 & 1 & 0 & 4 \\\\\n0 & 0 & 0 & 0 & 1 & 5 \\\\\n0 & 0 & 0 & 0 & 0 & 6\n\\end{pmatrix}\n$$\nNo row interchanges were performed, so the permutation matrix is the identity, $P=I$. The lower triangular matrix $L$ contains the multipliers:\n$$\nL = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n-1 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & -1 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & -1 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & -1 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & -1 & 1\n\\end{pmatrix}\n$$\n\n**Task 2: Compute the pivot growth factor $\\rho$**\n\nThe pivot growth factor is defined as $\\rho = \\frac{\\max_{i,j} |u_{ij}|}{\\max_{i,j} |a_{ij}|}$.\nFrom matrix $A$, we have $\\max_{i,j} |a_{ij}| = 1$.\nFrom matrix $U$, the entries are all non-negative. The maximum value is clearly $u_{66} = 6$. So, $\\max_{i,j} |u_{ij}| = 6$.\nTherefore, the pivot growth factor is:\n$$ \\rho = \\frac{6}{1} = 6 $$\n\n**Task 3: Derive a normwise backward error bound**\n\nWhen solving $Ax=b$ using GEPP, the computed solution $\\hat{x}$ is the exact solution of a perturbed system $(A+\\Delta A)\\hat{x}=b$. A standard result from the analysis of floating-point errors in LU factorization followed by forward and backward substitution gives a bound on the perturbation $\\Delta A$. Ignoring terms of order $u^2$ and higher, this bound is:\n$$ |\\Delta A| \\le 3 \\gamma_n |P^T||\\hat{L}||\\hat{U}| $$\nwhere $\\gamma_n = \\frac{nu}{1-nu} \\approx nu$ for small $u$, and $|\\cdot|$ denotes the matrix of absolute values. For our problem, $P=I$ and the computed factors $\\hat{L}$ and $\\hat{U}$ are close to the exact factors $L$ and $U$. Thus, we can approximate the bound by:\n$$ |\\Delta A| \\le 3nu |L||U| $$\nTaking the infinity norm and using norm submultiplicativity:\n$$ \\|\\Delta A\\|_{\\infty} \\le 3nu \\left\\| |L||U| \\right\\|_{\\infty} $$\nWe have $n=6$. The matrices $|L|$ and $|U|=U$ are:\n$$\n|L| = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & 1\n\\end{pmatrix}, \\quad |U| = U = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & 1 \\\\\n0 & 1 & 0 & 0 & 0 & 2 \\\\\n0 & 0 & 1 & 0 & 0 & 3 \\\\\n0 & 0 & 0 & 1 & 0 & 4 \\\\\n0 & 0 & 0 & 0 & 1 & 5 \\\\\n0 & 0 & 0 & 0 & 0 & 6\n\\end{pmatrix}\n$$\nWe compute the product $|L||U|$:\n$$\n|L||U| = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & 1 \\\\\n1 & 1 & 0 & 0 & 0 & 3 \\\\\n0 & 1 & 1 & 0 & 0 & 5 \\\\\n0 & 0 & 1 & 1 & 0 & 7 \\\\\n0 & 0 & 0 & 1 & 1 & 9 \\\\\n0 & 0 & 0 & 0 & 1 & 11\n\\end{pmatrix}\n$$\nThe infinity norm $\\|B\\|_{\\infty} = \\max_i \\sum_j |b_{ij}|$ is the maximum absolute row sum.\nRow sums of $|L||U|$: $1+1=2$, $1+1+3=5$, $1+1+5=7$, $1+1+7=9$, $1+1+9=11$, $1+11=12$.\nSo, $\\left\\| |L||U| \\right\\|_{\\infty} = 12$.\nThe infinity norm of the original matrix $A$ is:\n$\\|A\\|_{\\infty} = \\max(|1|+|1|, |-1|+|1|+|1|, |-1|+|1|+|1|, \\dots, |-1|+|1|) = \\max(2,3,3,3,3,2) = 3$.\nThe relative backward error bound is:\n$$ \\frac{\\|\\Delta A\\|_{\\infty}}{\\|A\\|_{\\infty}} \\le \\frac{3(6)u (12)}{3} = 72u $$\n\n**Task 4: Deduce a forward error implication**\n\nThe relative forward error in the solution is bounded by:\n$$ \\frac{\\|x-\\hat{x}\\|_{\\infty}}{\\|x\\|_{\\infty}} \\le \\kappa_{\\infty}(A) \\frac{\\|\\Delta A\\|_{\\infty}}{\\|A\\|_{\\infty}} $$\nwhere $\\kappa_{\\infty}(A) = \\|A\\|_{\\infty} \\|A^{-1}\\|_{\\infty}$ is the condition number. We need to compute $A^{-1} = (LU)^{-1} = U^{-1}L^{-1}$.\nFirst, we find the inverses of $L$ and $U$.\nSolving $Lx=y$ gives $x_1=y_1$, $x_2=y_2+x_1$, $x_3=y_3+x_2=y_3+y_2+y_1$, etc. Thus $L^{-1}$ is the lower triangular matrix of all ones:\n$$\nL^{-1} = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 0 & 0 & 0 \\\\\n1 & 1 & 1 & 0 & 0 & 0 \\\\\n1 & 1 & 1 & 1 & 0 & 0 \\\\\n1 & 1 & 1 & 1 & 1 & 0 \\\\\n1 & 1 & 1 & 1 & 1 & 1\n\\end{pmatrix}\n$$\nSolving $Ux=y$ gives $x_6 = y_6/6$, and $x_i = y_i - u_{i,6} x_6 = y_i - i \\cdot (y_6/6)$ for $i<6$.\n$$\nU^{-1} = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & -1/6 \\\\\n0 & 1 & 0 & 0 & 0 & -2/6 \\\\\n0 & 0 & 1 & 0 & 0 & -3/6 \\\\\n0 & 0 & 0 & 1 & 0 & -4/6 \\\\\n0 & 0 & 0 & 0 & 1 & -5/6 \\\\\n0 & 0 & 0 & 0 & 0 & 1/6\n\\end{pmatrix}\n$$\nNow, $A^{-1} = U^{-1}L^{-1}$:\n$$\nA^{-1} = \\begin{pmatrix}\n1 & 0 & \\dots & -1/6 \\\\\n0 & 1 & \\dots & -2/6 \\\\\n\\vdots &   & \\ddots & \\vdots \\\\\n0 & \\dots & 0 & 1/6\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 & \\dots & 0 \\\\\n1 & 1 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & 1 & \\dots & 1\n\\end{pmatrix}\n$$\nThe $(i,j)$ entry of $A^{-1}$ for $j<6$ is $(A^{-1})_{ij} = (L^{-1})_{ij}$. For $j=6$, the entry is $(A^{-1})_{i6} = \\sum_{k=1}^6 (U^{-1})_{ik}(L^{-1})_{k6} = \\sum_{k=1}^6 (U^{-1})_{ik}(1) = (U^{-1})_{ii} + (U^{-1})_{i6} = 1 - i/6$ for $i<6$, and $(A^{-1})_{66} = 1/6$.\n$$\nA^{-1} = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & 5/6 \\\\\n1 & 1 & 0 & 0 & 0 & 4/6 \\\\\n1 & 1 & 1 & 0 & 0 & 3/6 \\\\\n1 & 1 & 1 & 1 & 0 & 2/6 \\\\\n1 & 1 & 1 & 1 & 1 & 1/6 \\\\\n1 & 1 & 1 & 1 & 1 & 1/6\n\\end{pmatrix}\n$$\nThe infinity norm of $A^{-1}$ is the maximum absolute row sum:\nRow 1: $1 + 5/6 = 11/6$\nRow 2: $1 + 1 + 4/6 = 2 + 2/3 = 8/3 = 16/6$\nRow 3: $1+1+1+3/6 = 3+1/2 = 7/2 = 21/6$\nRow 4: $1+1+1+1+2/6 = 4+1/3 = 13/3 = 26/6$\nRow 5: $1+1+1+1+1+1/6 = 5+1/6 = 31/6$\nRow 6: $1+1+1+1+1+1/6 = 5+1/6 = 31/6$\nThe maximum is $31/6$. So, $\\|A^{-1}\\|_{\\infty} = 31/6$.\nThe condition number is $\\kappa_{\\infty}(A) = \\|A\\|_{\\infty} \\|A^{-1}\\|_{\\infty} = 3 \\cdot \\frac{31}{6} = \\frac{31}{2} = 15.5$.\nThe forward error implication is:\n$$ \\frac{\\|x-\\hat{x}\\|_{\\infty}}{\\|x\\|_{\\infty}} \\le \\kappa_{\\infty}(A) \\frac{\\|\\Delta A\\|_{\\infty}}{\\|A\\|_{\\infty}} \\le (15.5)(72u) = 1116u $$\nThis shows that for this specific matrix, the solution is expected to be computed with a forward error of about $1116$ times the unit roundoff $u$.\n\nThe problem asks for the single value of the pivot growth factor $\\rho$.\nFinal calculation summary for $\\rho$:\n$\\max_{i,j} |a_{ij}| = 1$.\n$\\max_{i,j} |u_{ij}| = 6$.\n$\\rho = 6/1 = 6$.", "answer": "$$\\boxed{6}$$", "id": "3533819"}, {"introduction": "While normwise backward stability provides a valuable guarantee, many applications demand high relative accuracy in every component of the solution, which requires a small componentwise backward error. This goal can be elusive for ill-scaled systems where entries span many orders of magnitude. This hands-on coding practice [@problem_id:3533816] investigates diagonal equilibration, a preconditioning technique designed to mitigate these scaling issues. By implementing the equilibration process and measuring the resulting improvement, you will gain practical insight into enhancing the componentwise stability of linear system solvers.", "problem": "Consider the linear system $A x = b$ with $A \\in \\mathbb{R}^{n \\times n}$ and $b \\in \\mathbb{R}^{n}$ solved in floating-point arithmetic under the Institute of Electrical and Electronics Engineers (IEEE) $754$ model with unit roundoff $u$. The componentwise backward error of an approximate solution $\\hat{x}$ is defined by the quantity\n$$\n\\eta(A,b,\\hat{x}) = \\max_{1 \\leq i \\leq n} \\frac{|r_i|}{\\left(|A| \\cdot |\\hat{x}| + |b|\\right)_i},\n$$\nwhere $r = b - A \\hat{x}$ is the residual, $|\\cdot|$ denotes componentwise absolute value, and the denominator is interpreted componentwise with the conventions $0/0 = 0$ and $\\alpha/0 = \\infty$ for $\\alpha > 0$. A backward stable algorithm aims to produce $\\hat{x}$ for which $\\eta(A,b,\\hat{x})$ is on the order of $u$ multiplied by modest growth factors. When entries of $A$ span several orders of magnitude, componentwise guarantees can degrade unless the system is equilibrated by positive diagonal row and column scalings.\n\nDiagonal equilibration constructs positive diagonal matrices $D_r$ and $D_c$ such that the scaled system $D_r A D_c y = D_r b$ has better-balanced magnitudes; the solution to the original system is then recovered by $\\hat{x}_{\\mathrm{eq}} = D_c \\hat{y}$. A practical scheme is to choose $D_r$ so that each row of $D_r A$ has unit maximum absolute entry and then choose $D_c$ so that each column of $D_r A D_c$ has unit maximum absolute entry, with the convention that rows or columns that are identically zero receive a diagonal scaling of $1$.\n\nStarting from the floating-point model and the definition of componentwise backward error, design and implement a program that:\n- Constructs matrices $A$ whose entries span prescribed orders of magnitude by combining a diagonal matrix $D$ with entries $10^{e_i}$ and a diagonally dominant perturbation $B = I + \\varepsilon R$, where $R$ has entries uniformly distributed in $[-1,1]$ and $I$ is the identity matrix.\n- Forms $b = A x_{\\mathrm{ref}}$ for a prescribed reference vector $x_{\\mathrm{ref}}$.\n- Computes $\\hat{x}_{\\mathrm{uns}}$ by solving $A x = b$ directly and evaluates $\\eta_{\\mathrm{uns}} = \\eta(A,b,\\hat{x}_{\\mathrm{uns}})$.\n- Performs diagonal equilibration to obtain $A_{\\mathrm{eq}} = D_r A D_c$, $b_{\\mathrm{eq}} = D_r b$, solves $A_{\\mathrm{eq}} y = b_{\\mathrm{eq}}$ for $\\hat{y}$, forms $\\hat{x}_{\\mathrm{eq}} = D_c \\hat{y}$, and evaluates $\\eta_{\\mathrm{eq}} = \\eta(A,b,\\hat{x}_{\\mathrm{eq}})$.\n- Reports the improvement factor $\\rho = \\eta_{\\mathrm{uns}} / \\eta_{\\mathrm{eq}}$ as a floating-point number for each test case.\n\nUse the following test suite, each specified by $(\\text{name}, n, \\text{exponents}, \\varepsilon, \\text{seed}_R, \\text{seed}_x)$:\n- Case $1$ (happy path): $(\\text{\"happy\\_path\"}, 10, \\text{linearly spaced exponents }[-8,\\ldots,8]\\text{ of length }10, 10^{-2}, 123, 321)$.\n- Case $2$ (balanced): $(\\text{\"balanced\"}, 10, \\text{all zeros of length }10, 10^{-2}, 456, 654)$.\n- Case $3$ (extreme): $(\\text{\"extreme\"}, 12, \\text{linearly spaced exponents }[-12,\\ldots,12]\\text{ of length }12, 10^{-3}, 789, 987)$.\n- Case $4$ (alternating): $(\\text{\"alternating\"}, 12, \\text{alternating exponents }[-9,9,-9,9,\\ldots]\\text{ of length }12, 5 \\cdot 10^{-3}, 135, 531)$.\n\nMatrix construction details for each case:\n- Form $D = \\mathrm{diag}(10^{e_1}, \\ldots, 10^{e_n})$ with specified exponents.\n- Draw $R \\in \\mathbb{R}^{n \\times n}$ with independent entries uniformly distributed in $[-1,1]$ using $\\text{seed}_R$.\n- Set $B = I + \\varepsilon R$ and define $A = D B$.\n- Draw $x_{\\mathrm{ref}} \\in \\mathbb{R}^{n}$ with entries uniformly distributed in $[0.1,1]$, then multiply by random signs from a standard normal distribution thresholded at zero, using $\\text{seed}_x$; form $b = A x_{\\mathrm{ref}}$.\n\nEquilibration details:\n- For $D_r = \\mathrm{diag}(d^{(r)}_1,\\ldots,d^{(r)}_n)$, set $d^{(r)}_i = 1 / \\max_j |a_{ij}|$ if the maximum is nonzero, otherwise $d^{(r)}_i = 1$.\n- For $D_c = \\mathrm{diag}(d^{(c)}_1,\\ldots,d^{(c)}_n)$, applied to $A_1 = D_r A$, set $d^{(c)}_j = 1 / \\max_i |(A_1)_{ij}|$ if the maximum is nonzero, otherwise $d^{(c)}_j = 1$.\n\nNumerical conventions:\n- Evaluate $\\eta(A,b,\\hat{x})$ with $0/0 = 0$ and $\\alpha/0 = \\infty$ for $\\alpha > 0$ on a per-component basis.\n- If $\\eta_{\\mathrm{eq}} = 0$, report $\\rho = \\infty$ for that case.\n\nYour program should produce a single line of output containing the improvement factors for the four test cases as a comma-separated list enclosed in square brackets (e.g., $[\\rho_1,\\rho_2,\\rho_3,\\rho_4]$). The outputs must be floating-point numbers. No user input is allowed, and all random draws must use the specified seeds to ensure reproducibility.", "solution": "The problem at hand requires an analysis of the componentwise backward stability of solving a linear system $A x = b$ and the effect of diagonal equilibration. We are tasked with quantifying the improvement in backward stability afforded by a specific equilibration scheme for a set of programmatically generated ill-scaled matrices.\n\nThe bedrock of this analysis is the concept of backward error. For a computed solution $\\hat{x}$ to the system $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ and $b \\in \\mathbb{R}^{n}$, the backward error measures the size of the smallest perturbation $(\\Delta A, \\Delta b)$ such that $\\hat{x}$ is the exact solution to the perturbed system $(A + \\Delta A) \\hat{x} = b + \\Delta b$. A backward stable algorithm guarantees that the computed solution is the exact solution of a nearby problem, implying the backward error is small. The problem specifies a componentwise relative backward error, defined as:\n$$\n\\eta(A,b,\\hat{x}) = \\max_{1 \\leq i \\leq n} \\frac{|r_i|}{\\left(|A| \\cdot |\\hat{x}| + |b|\\right)_i}\n$$\nwhere $r = b - A \\hat{x}$ is the residual vector, and $|\\cdot|$ denotes the componentwise absolute value. A value of $\\eta(A,b,\\hat{x}) \\approx u$, where $u$ is the unit roundoff of the floating-point arithmetic (for IEEE $754$ double precision, $u = 2^{-53} \\approx 1.11 \\times 10^{-16}$), indicates excellent backward stability. This definition implies the existence of perturbations $\\Delta A$ and $\\Delta b$ such that $(A + \\Delta A) \\hat{x} = b + \\Delta b$ with $|\\Delta A| \\le \\eta(A,b,\\hat{x}) |A|$ and $|\\Delta b| \\le \\eta(A,b,\\hat{x}) |b|$.\n\nThe problem requires us to construct test matrices designed to be ill-scaled. This is achieved by forming $A = D B$, where $D = \\mathrm{diag}(10^{e_1}, \\ldots, 10^{e_n})$ is a diagonal matrix introducing large variations in the magnitudes of the rows. The matrix $B$ is a small perturbation of the identity matrix, $B = I + \\varepsilon R$, where $R$ has random entries from $U[-1,1]$ and $\\varepsilon$ is a small parameter. For sufficiently small $\\varepsilon$, $B$ is strongly diagonally dominant and thus well-conditioned and non-singular, ensuring $A$ is also non-singular. The right-hand side is constructed as $b = A x_{\\mathrm{ref}}$ for a known reference solution $x_{\\mathrm{ref}}$. This setup allows us to attribute any numerical difficulties primarily to the poor scaling of $A$ rather than to ill-conditioning of the underlying problem.\n\nThe core of the task is to compare two approaches for solving $A x = b$:\n\n1.  **Direct (Unscaled) Solution**: We solve $A x = b$ directly using a standard numerical solver. This yields a solution $\\hat{x}_{\\mathrm{uns}}$. We then compute its componentwise backward error $\\eta_{\\mathrm{uns}} = \\eta(A, b, \\hat{x}_{\\mathrm{uns}})$. For ill-scaled matrices, we expect standard algorithms like Gaussian elimination with partial pivoting (as implemented in `numpy.linalg.solve`) to potentially produce a solution with a large backward error.\n\n2.  **Equilibrated Solution**: We first apply a two-sided diagonal scaling to the system. The goal of equilibration is to produce a new system $A_{\\mathrm{eq}} y = b_{\\mathrm{eq}}$ where the entries of $A_{\\mathrm{eq}}$ are more uniform in magnitude. The specific scheme is:\n    a.  **Row Scaling**: Find a diagonal matrix $D_r = \\mathrm{diag}(d_1^{(r)}, \\dots, d_n^{(r)})$ such that each row of $A_1 = D_r A$ has a maximum absolute value of $1$. The scaling factors are $d_i^{(r)} = 1 / \\max_j |a_{ij}|$, with $d_i^{(r)} = 1$ if the $i$-th row is all zeros.\n    b.  **Column Scaling**: Find a diagonal matrix $D_c = \\mathrm{diag}(d_1^{(c)}, \\dots, d_n^{(c)})$ such that each column of $A_{\\mathrm{eq}} = A_1 D_c = D_r A D_c$ has a maximum absolute value of $1$. The scaling factors are $d_j^{(c)} = 1 / \\max_i |(A_1)_{ij}|$, with $d_j^{(c)} = 1$ if the $j$-th column of $A_1$ is all zeros.\n    \n    The original system $A x = b$ is transformed into $(D_r A D_c) (D_c^{-1} x) = D_r b$, or $A_{\\mathrm{eq}} y = b_{\\mathrm{eq}}$, where $y = D_c^{-1} x$ and $b_{\\mathrm{eq}} = D_r b$. We solve this better-scaled system for $\\hat{y}$ and then recover the solution to the original problem via $\\hat{x}_{\\mathrm{eq}} = D_c \\hat{y}$. The backward error for this approach is evaluated with respect to the original problem: $\\eta_{\\mathrm{eq}} = \\eta(A, b, \\hat{x}_{\\mathrm{eq}})$.\n\nFinally, we compute the improvement factor $\\rho = \\eta_{\\mathrm{uns}} / \\eta_{\\mathrm{eq}}$ for each test case. A value of $\\rho \\gg 1$ demonstrates the effectiveness of equilibration. The implementation uses `numpy` for all numerical computations, with random number generators seeded as specified to ensure reproducibility. The componentwise backward error function $\\eta$ is implemented carefully to handle the conventions $0/0=0$ and $\\alpha/0=\\infty$ for $\\alpha > 0$. If $\\eta_{\\mathrm{eq}} = 0$, indicating a numerically exact solution was found, the improvement factor $\\rho$ is taken to be infinite.\n\nThe procedural steps for each test case are as follows:\n1.  Construct the matrix $A$ and vector $b$ according to the case parameters ($n$, exponents, $\\varepsilon$, seeds).\n2.  Solve $A x = b$ to find $\\hat{x}_{\\mathrm{uns}}$ and compute $\\eta_{\\mathrm{uns}}$.\n3.  Compute scaling matrices $D_r$ and $D_c$.\n4.  Form the equilibrated system $A_{\\mathrm{eq}} = D_r A D_c$ and $b_{\\mathrm{eq}} = D_r b$.\n5.  Solve $A_{\\mathrm{eq}} y = b_{\\mathrm{eq}}$ for $\\hat{y}$.\n6.  Compute the equilibrated solution $\\hat{x}_{\\mathrm{eq}} = D_c \\hat{y}$.\n7.  Compute the corresponding backward error $\\eta_{\\mathrm{eq}}$ for the original system.\n8.  Calculate the improvement factor $\\rho = \\eta_{\\mathrm{uns}} / \\eta_{\\mathrm{eq}}$.\n\nThis systematic comparison will highlight the practical benefits of equilibration in numerical linear algebra.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef calculate_eta(A, b, x_hat):\n    \"\"\"\n    Computes the componentwise backward error eta(A, b, x_hat).\n    \"\"\"\n    if x_hat is None:\n        return np.inf\n\n    n = A.shape[0]\n    r = b - A @ x_hat\n    abs_r = np.abs(r)\n    \n    # Denominator term: (|A|*|x_hat| + |b|)_i\n    denominator = np.abs(A) @ np.abs(x_hat) + np.abs(b)\n    \n    # Initialize ratios to 0, which handles the 0/0 case.\n    ratios = np.zeros(n, dtype=float)\n    \n    # Case 1: Denominator is non-zero\n    denom_nz_mask = denominator != 0\n    ratios[denom_nz_mask] = abs_r[denom_nz_mask] / denominator[denom_nz_mask]\n    \n    # Case 2: Denominator is zero, but numerator is non-zero (alpha/0 -> inf)\n    num_nz_denom_z_mask = (denominator == 0)  (abs_r != 0)\n    ratios[num_nz_denom_z_mask] = np.inf\n    \n    return np.max(ratios)\n\ndef process_case(n, exponents, epsilon, seed_R, seed_x):\n    \"\"\"\n    Processes a single test case for the equilibration problem.\n    \"\"\"\n    # 1. Setup: Construct A, xref, and b\n    rng_R = np.random.default_rng(seed_R)\n    rng_x = np.random.default_rng(seed_x)\n    \n    # Construct D\n    D = np.diag(np.power(10.0, exponents))\n    \n    # Construct B = I + epsilon * R\n    R = rng_R.uniform(-1.0, 1.0, size=(n, n))\n    B = np.eye(n) + epsilon * R\n    \n    # Construct A = D * B\n    A = D @ B\n    \n    # Construct x_ref\n    x_ref_mag = rng_x.uniform(0.1, 1.0, size=n)\n    x_ref_signs = np.sign(rng_x.standard_normal(size=n))\n    x_ref_signs[x_ref_signs == 0] = 1.0  # Convention for sign(0)\n    x_ref = x_ref_mag * x_ref_signs\n    \n    # Construct b = A * x_ref\n    b = A @ x_ref\n    \n    # 2. Unscaled Solution\n    try:\n        x_hat_uns = np.linalg.solve(A, b)\n        eta_uns = calculate_eta(A, b, x_hat_uns)\n    except np.linalg.LinAlgError:\n        eta_uns = np.inf\n\n    # 3. Equilibrated Solution\n    # Row scaling\n    row_maxes = np.max(np.abs(A), axis=1)\n    dr_diag = np.ones(n)\n    nonzero_rows = row_maxes != 0\n    dr_diag[nonzero_rows] = 1.0 / row_maxes[nonzero_rows]\n    Dr = np.diag(dr_diag)\n    \n    A1 = Dr @ A\n    \n    # Column scaling\n    col_maxes = np.max(np.abs(A1), axis=0)\n    dc_diag = np.ones(n)\n    nonzero_cols = col_maxes != 0\n    dc_diag[nonzero_cols] = 1.0 / col_maxes[nonzero_cols]\n    Dc = np.diag(dc_diag)\n    \n    A_eq = A1 @ Dc\n    b_eq = Dr @ b\n    \n    try:\n        y_hat = np.linalg.solve(A_eq, b_eq)\n        x_hat_eq = Dc @ y_hat\n        eta_eq = calculate_eta(A, b, x_hat_eq)\n    except np.linalg.LinAlgError:\n         eta_eq = np.inf\n\n    # 4. Improvement Factor\n    if eta_eq == 0:\n        if eta_uns == 0:\n            rho = 1.0 # Or undefined, but 1.0 is reasonable\n        else:\n            rho = np.inf\n    else:\n        rho = eta_uns / eta_eq\n        \n    return rho\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path)\n        (10, np.linspace(-8, 8, 10), 1e-2, 123, 321),\n        # Case 2 (balanced)\n        (10, np.zeros(10), 1e-2, 456, 654),\n        # Case 3 (extreme)\n        (12, np.linspace(-12, 12, 12), 1e-3, 789, 987),\n        # Case 4 (alternating)\n        (12, np.array([-9, 9] * 6, dtype=float), 5e-3, 135, 531)\n    ]\n\n    results = []\n    for n, exponents, epsilon, seed_R, seed_x in test_cases:\n        result = process_case(n, exponents, epsilon, seed_R, seed_x)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3533816"}]}