{"hands_on_practices": [{"introduction": "To build a solid foundation, we first dissect the core mechanism of a Givens rotation. Theoretical knowledge of numerical methods comes to life when applied, and this first practice challenges you to derive the rotation parameters, typically denoted $c$ and $s$, directly from the defining properties of orthogonal transformations. By working from first principles to annihilate a specific matrix element, you will translate abstract mathematical requirements into concrete formulas, a fundamental skill in computational science. [@problem_id:3548499]", "problem": "Consider the dense matrix\n$$\nA \\;=\\; \\begin{pmatrix}\n4 & -3 & 8 \\\\\n7 & 5 & -2 \\\\\n1 & -9 & 6\n\\end{pmatrix}.\n$$\nA Givens rotation is a planar orthogonal transformation that acts nontrivially on a selected pair of coordinates and preserves the Euclidean $2$-norm. Using only the fundamental definition of orthogonal matrices as norm-preserving linear isometries in $\\mathbb{R}^{n}$ and the characterization of planar rotations in $\\mathbb{R}^{2}$, derive the constraints on the two real parameters of a Givens rotation that acts on rows $1$ and $2$ by left multiplication and annihilates the entry below the diagonal in column $1$ of $A$, that is, the element $a_{21}$. Starting from these first principles, determine an analytic expression for these parameters in terms of $a_{11}$ and $a_{21}$, and then evaluate them for the specific matrix $A$ given above.\n\nReport as your final answer the cosine parameter $c$ of the Givens rotation that annihilates $a_{21}$ for the given matrix $A$. Express your final answer in exact closed form; no rounding is required.", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard problem in numerical linear algebra that is self-contained and free of contradictions.\n\nThe task is to derive the parameters of a Givens rotation that annihilates a specific element of a matrix and then to compute one of these parameters for the given matrix $A$. The derivation must be based on first principles. A Givens rotation is an orthogonal transformation that acts on a two-dimensional subspace.\n\nLet the given matrix be $A \\in \\mathbb{R}^{3 \\times 3}$, with entries denoted by $a_{ij}$.\n$$\nA \\;=\\; \\begin{pmatrix}\n4 & -3 & 8 \\\\\n7 & 5 & -2 \\\\\n1 & -9 & 6\n\\end{pmatrix}\n$$\nWe seek a Givens rotation matrix $G$ that, when applied via left multiplication to $A$, annihilates the element $a_{21}=7$. This rotation must act on rows $1$ and $2$. The Givens rotation matrix $G$ is an orthogonal matrix that is identical to the identity matrix except for a $2 \\times 2$ submatrix that performs a planar rotation. For an action on rows $1$ and $2$, the matrix $G$ has the general form:\n$$\nG \\;=\\; \\begin{pmatrix}\nc & s & 0 \\\\\n-s & c & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n$$\nwhere $c$ and $s$ are real parameters.\n\nThe first principle stated is that $G$ must be an orthogonal matrix. An orthogonal matrix $Q$ satisfies the condition $Q^T Q = I$, where $I$ is the identity matrix. This property ensures that the transformation is a linear isometry, i.e., it preserves the Euclidean $2$-norm. Applying this condition to $G$:\n$$\nG^T G = \\begin{pmatrix}\nc & -s & 0 \\\\\ns & c & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nc & s & 0 \\\\\n-s & c & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix} = \\begin{pmatrix}\nc^2 + s^2 & cs - sc & 0 \\\\\nsc - cs & s^2 + c^2 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n$$\nFrom the top-left entry of the resulting identity matrix, we derive the first constraint on the parameters $c$ and $s$:\n$$\nc^2 + s^2 = 1\n$$\nThis constraint confirms that $c$ and $s$ can be interpreted as the cosine and sine of some rotation angle $\\theta$, i.e., $c = \\cos(\\theta)$ and $s = \\sin(\\theta)$, for the rotation matrix convention used here (a clockwise rotation by $\\theta$).\n\nNext, we apply the transformation to the matrix $A$ to form a new matrix $A' = G A$.\n$$\nA' = \\begin{pmatrix}\nc & s & 0 \\\\\n-s & c & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{pmatrix}\n$$\nThe problem requires that the element in the second row and first column of $A'$, denoted $a'_{21}$, be annihilated (i.e., set to $0$). The expression for $a'_{21}$ is obtained by computing the dot product of the second row of $G$ and the first column of $A$:\n$$\na'_{21} = (-s) \\cdot a_{11} + c \\cdot a_{21} + 0 \\cdot a_{31} = -s a_{11} + c a_{21}\n$$\nSetting $a'_{21} = 0$ gives us the second constraint:\n$$\n-s a_{11} + c a_{21} = 0 \\quad \\implies \\quad c a_{21} = s a_{11}\n$$\nWe now have a system of two equations for the two unknowns $c$ and $s$:\n1. $c^2 + s^2 = 1$\n2. $c a_{21} = s a_{11}$\n\nAssuming $a_{11} \\ne 0$, we can write $s = c \\frac{a_{21}}{a_{11}}$. Substituting this into the first equation:\n$$\nc^2 + \\left(c \\frac{a_{21}}{a_{11}}\\right)^2 = 1\n$$\n$$\nc^2 \\left(1 + \\frac{a_{21}^2}{a_{11}^2}\\right) = 1\n$$\n$$\nc^2 \\left(\\frac{a_{11}^2 + a_{21}^2}{a_{11}^2}\\right) = 1\n$$\n$$\nc^2 = \\frac{a_{11}^2}{a_{11}^2 + a_{21}^2}\n$$\nTaking the square root, we get $c = \\pm \\frac{|a_{11}|}{\\sqrt{a_{11}^2 + a_{21}^2}}$. A standard convention, which also ensures the new diagonal element $a'_{11}$ is positive, is to choose the positive root and let $c$ have the same sign as $a_{11}$. This leads to the general analytic expression for $c$:\n$$\nc = \\frac{a_{11}}{\\sqrt{a_{11}^2 + a_{21}^2}}\n$$\nSubstituting this back into $c a_{21} = s a_{11}$, we solve for $s$:\n$$\n\\left(\\frac{a_{11}}{\\sqrt{a_{11}^2 + a_{21}^2}}\\right) a_{21} = s a_{11}\n$$\nAssuming $a_{11} \\ne 0$, we find the expression for $s$:\n$$\ns = \\frac{a_{21}}{\\sqrt{a_{11}^2 + a_{21}^2}}\n$$\nIf $a_{11}=0$, the annihilation equation becomes $c a_{21} = 0$. If $a_{21} \\ne 0$, then $c=0$. From $c^2+s^2=1$, we get $s^2=1$, so $s=\\pm 1$. Our formulas yield $c=0$ and $s=a_{21}/|a_{21}| = \\pm 1$, which is consistent. If both $a_{11}=0$ and $a_{21}=0$, the element is already zero and no rotation is needed ($c=1, s=0$). Our formulas are thus robust.\n\nNow, we evaluate these parameters for the given matrix $A$. The relevant entries are $a_{11} = 4$ and $a_{21} = 7$.\nFirst, we compute the denominator, which corresponds to the Euclidean norm of the vector $(a_{11}, a_{21})^T$. Let this be $r$.\n$$\nr = \\sqrt{a_{11}^2 + a_{21}^2} = \\sqrt{4^2 + 7^2} = \\sqrt{16 + 49} = \\sqrt{65}\n$$\nUsing the derived analytic expression, the cosine parameter $c$ is:\n$$\nc = \\frac{a_{11}}{r} = \\frac{4}{\\sqrt{65}}\n$$\nFor completeness, the sine parameter $s$ is:\n$$\ns = \\frac{a_{21}}{r} = \\frac{7}{\\sqrt{65}}\n$$\nThe problem requests the value of the cosine parameter $c$, expressed in exact closed form.\nThe final answer is $c = \\frac{4}{\\sqrt{65}}$.", "answer": "$$\n\\boxed{\\frac{4}{\\sqrt{65}}}\n$$", "id": "3548499"}, {"introduction": "With the mechanics of a single rotation established, we now analyze the efficiency of the complete algorithm. This practice guides you through a detailed analysis of the total computational cost (flop count) for a full $QR$ factorization using Givens rotations, comparing it with the widely used Householder method. Understanding this trade-off is crucial for making informed decisions about which algorithm to deploy in practical, performance-sensitive scenarios. [@problem_id:3548480]", "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ be a dense real matrix with $m \\ge n$. Consider two orthogonal methods to produce the upper triangular factor $R$ of a QR factorization: successive Givens rotations and successive Householder reflections. Use the following flop model: each floating-point addition, subtraction, multiplication, division, and square root counts as one flop.\n\nStarting only from the definitions of a Givens rotation and a Householder reflection, and the requirement that orthogonal transformations preserve the Euclidean norm and effect annihilation of subdiagonal entries, perform the following:\n\n1. Derive, by explicit operation counting over all steps and all affected entries, the total flop count $F_{\\mathrm{G}}(m,n)$ required to form $R$ from $A$ using Givens rotations in the dense case (annihilating one subdiagonal element at a time and applying each rotation across the remaining trailing columns of the active submatrix). Count both the flops to compute each rotation’s parameters and the flops to apply the rotation to all entries updated by it.\n\n2. Derive, by explicit operation counting over all steps and all affected entries, the total flop count $F_{\\mathrm{H}}(m,n)$ required to form $R$ from $A$ using Householder reflections (constructing each reflector on the active column and applying it to the trailing submatrix). Your derivation should account for the cost of forming each reflector and the cost of its application to all trailing columns, using only the structure of a rank-one update by a Householder reflection.\n\nFinally, define the aspect ratio $\\beta = n/m \\in (0,1]$ and consider the leading-order asymptotics as $m \\to \\infty$ with $\\beta$ fixed. Compute the asymptotic ratio of the leading-order flop counts,\n$$\n\\rho(\\beta) \\equiv \\lim_{m \\to \\infty} \\frac{F_{\\mathrm{G}}(m,n)}{F_{\\mathrm{H}}(m,n)},\n$$\nand express your final answer as a single simplified analytical expression. No rounding is required, and no units are involved.", "solution": "The user has requested a derivation of the flop counts for QR factorization of a dense $m \\times n$ matrix $A$ (with $m \\ge n$) using Givens rotations and Householder reflections, followed by an asymptotic analysis of their ratio. The flop model defines addition, subtraction, multiplication, division, and square root each as one flop.\n\n### Step 1: Validation of the Problem Statement\nThe provided problem is a standard, well-defined exercise in numerical linear algebra.\n- **Givens**: The matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$, the goal of producing the upper triangular factor $R$ of a QR factorization, the specified methods (Givens rotations and Householder reflections), and the explicit flop model constitute a complete set of givens.\n- **Scientific Soundness**: The problem is grounded in the fundamental principles of numerical linear algebra. The algorithms and their analysis are canonical topics.\n- **Well-Posedness and Objectivity**: The problem is clearly stated, with unambiguous terminology and a precise, objective goal. The required output is an analytical expression derived from first principles.\nThe problem is deemed **valid**. We may proceed to the solution.\n\n### Part 1: Flop Count for Givens Rotations, $F_{\\mathrm{G}}(m,n)$\n\nA Givens rotation is an orthogonal transformation that zeros out a single element of a vector. To zero out an element $y$ using an element $x$, we apply the rotation:\n$$\n\\begin{pmatrix} c & s \\\\ -s & c \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} r \\\\ 0 \\end{pmatrix}\n$$\nwhere $r = \\sqrt{x^2 + y^2}$, $c = x/r$, and $s = y/r$.\n\n**1. Flop cost per rotation:**\n- **Computing parameters $(c,s)$**: To find $c$ and $s$ from $x$ and $y$, we compute $r = \\sqrt{x^2 + y^2}$. This requires $2$ multiplications, $1$ addition, and $1$ square root, for a total of $4$ flops. Then, we compute $c = x/r$ and $s = y/r$, costing $2$ divisions. The total cost to compute the parameters of one Givens rotation is $4+2=6$ flops.\n- **Applying the rotation**: Applying this rotation to another column vector $\\begin{pmatrix} u \\\\ v \\end{pmatrix}$ requires computing $u' = cu + sv$ and $v' = -su + cv$. Each new element costs $2$ multiplications and $1$ addition. The total cost to update the two entries in one column is $3+3=6$ flops.\n\n**2. Total flop count:**\nThe algorithm proceeds column by column, from $j=1$ to $n$. For each column $j$, we annihilate the subdiagonal elements $A_{k,j}$ for $k=j+1, \\dots, m$. This requires $m-j$ Givens rotations. For each such rotation that zeros out $A_{k,j}$ (using, for example, $A_{j,j}$), the transformation must be applied to the remaining columns from $j+1$ to $n$.\n\nLet's analyze the work for annihilating one element $A_{k,j}$ where $k > j$:\n- Cost to compute the rotation parameters $(c,s)$ from $A_{j,j}$ and $A_{k,j}$: $6$ flops. This operation also yields the updated value $A'_{j,j} = r$ and sets $A'_{k,j} = 0$.\n- Cost to apply the rotation to the trailing columns $l=j+1, \\dots, n$: There are $n-j$ such columns. Each application costs $6$ flops. Total application cost is $6(n-j)$ flops.\nThe total cost for a single Givens rotation to zero out one element is $6 + 6(n-j)$ flops.\n\nTo form $R$, we must zero out all $m-j$ subdiagonal elements in each column $j=1, \\ldots, n$. The total flop count is the sum over all such rotations:\n$$\nF_{\\mathrm{G}}(m,n) = \\sum_{j=1}^{n} \\sum_{k=j+1}^{m} \\left(6 + 6(n-j)\\right)\n$$\nThe inner sum is over $k$ and has $m-j$ identical terms.\n$$\nF_{\\mathrm{G}}(m,n) = \\sum_{j=1}^{n} (m-j)\\left(6 + 6(n-j)\\right) = 6 \\sum_{j=1}^{n} (m-j)(1+n-j)\n$$\nTo find the leading-order terms for large $m$ and $n$, we can approximate the sum. The dominant part of the cost for step $j$ is when $n-j$ is large. We can approximate $(1+n-j) \\approx n-j$.\n$$\nF_{\\mathrm{G}}(m,n) \\approx 6 \\sum_{j=1}^{n} (m-j)(n-j)\n$$\nApproximating the sum with an integral for large $n$:\n$$\n\\sum_{j=1}^{n} (m-j)(n-j) \\approx \\int_{0}^{n} (m-x)(n-x) dx = \\int_{0}^{n} (mn - (m+n)x + x^2) dx\n$$\n$$\n= \\left[mnx - (m+n)\\frac{x^2}{2} + \\frac{x^3}{3}\\right]_0^n = mn^2 - (m+n)\\frac{n^2}{2} + \\frac{n^3}{3}\n$$\n$$\n= mn^2 - \\frac{mn^2}{2} - \\frac{n^3}{2} + \\frac{n^3}{3} = \\frac{1}{2}mn^2 - \\frac{1}{6}n^3\n$$\nThus, the leading-order flop count is:\n$$\nF_{\\mathrm{G}}(m,n) \\approx 6 \\left(\\frac{1}{2}mn^2 - \\frac{1}{6}n^3\\right) = 3mn^2 - n^3\n$$\n\n### Part 2: Flop Count for Householder Reflections, $F_{\\mathrm{H}}(m,n)$\n\nA Householder reflection is defined by a vector $v$ as $P = I - \\beta v v^T$. It reflects a vector $x$ across the hyperplane orthogonal to $v$.\n\n**1. Flop cost per reflection:**\nAt step $j$ of the QR factorization, we construct a reflector $P_j$ to zero out the subdiagonal elements of the $j$-th column. Let the active part of the column be $x = A(j:m, j)$, which is a vector of length $k=m-j+1$.\n- **Forming the reflector**: This involves constructing the Householder vector $v \\in \\mathbb{R}^k$ and the scalar $\\beta = 2/(v^T v)$.\n    - Compute $\\|x\\|_2$: requires $k$ multiplications, $k-1$ additions, and $1$ square root, totaling $\\approx 2k$ flops.\n    - Construct $v = x \\pm \\|x\\|_2 e_1$: $1$ addition.\n    - Compute $\\beta$: this efficiently costs a few more flops, independent of $k$ to leading order. For example, using $\\|v\\|_2^2=2\\|x\\|_2(\\|x\\|_2\\pm x_1)$ costs $1$ multiplication, $1$ addition. Then $\\beta=2/\\|v\\|_2^2$ costs $1$ division.\n    The total formation cost is dominated by $2k$ flops. So, forming the reflector for a vector of length $k$ costs $\\approx 2k$ flops.\n- **Applying the reflector**: The reflector $P_j$ is applied to the trailing submatrix $C = A(j:m, j+1:n)$, which has dimensions $k \\times (n-j)$. The update is $C \\leftarrow PC = C - \\beta v (v^T C)$. For each of the $n-j$ columns $c$ of $C$:\n    - Compute the scalar dot product $s = v^T c$: $k$ multiplications, $k-1$ additions ($\\approx 2k$ flops).\n    - Update the column $c \\leftarrow c - (\\beta s) v$: $1$ multiplication for $\\beta s$, then $k$ multiplications and $k$ additions for the scaled vector subtraction ($\\approx 2k$ flops).\n    Total cost to apply the reflector to one column of length $k$ is $\\approx 4k$ flops.\n\n**2. Total flop count:**\nAt step $j=1, \\ldots, n$, the vector length is $k=m-j+1$. There are $n-j$ columns in the trailing matrix.\n- Cost to form the reflector: $\\approx 2k = 2(m-j+1)$.\n- Cost to apply the reflector to the $n-j$ trailing columns: $\\approx (n-j) \\times 4k = 4(m-j+1)(n-j)$.\nThe total cost for step $j$ is the sum of formation and application costs:\n$$\n\\text{Cost}(j) \\approx 2(m-j+1) + 4(m-j+1)(n-j)\n$$\nThe total flop count is the sum over all steps $j=1, \\ldots, n$:\n$$\nF_{\\mathrm{H}}(m,n) = \\sum_{j=1}^{n} \\left( 2(m-j+1) + 4(m-j+1)(n-j) \\right)\n$$\nFor the asymptotic analysis, we focus on the highest order terms in $m$ and $n$. The dominant term in the sum is $4(m-j+1)(n-j) \\approx 4(m-j)(n-j)$. The reflector formation cost is of a lower order.\n$$\nF_{\\mathrm{H}}(m,n) \\approx \\sum_{j=1}^{n} 4(m-j)(n-j) = 4 \\sum_{j=1}^{n} (m-j)(n-j)\n$$\nUsing our integral approximation from Part 1:\n$$\nF_{\\mathrm{H}}(m,n) \\approx 4 \\left( \\frac{1}{2}mn^2 - \\frac{1}{6}n^3 \\right) = 2mn^2 - \\frac{2}{3}n^3\n$$\n\n### Part 3: Asymptotic Ratio of Flop Counts\n\nWe are asked to compute the limit of the ratio of the leading-order flop counts as $m \\to \\infty$ with the aspect ratio $\\beta = n/m \\in (0,1]$ held fixed.\n\nThe leading-order flop counts are:\n$$\nF_{\\mathrm{G}}(m,n) \\approx 3mn^2 - n^3\n$$\n$$\nF_{\\mathrm{H}}(m,n) \\approx 2mn^2 - \\frac{2}{3}n^3\n$$\nWe compute the ratio $\\rho(\\beta) = \\lim_{m \\to \\infty} \\frac{F_{\\mathrm{G}}(m,n)}{F_{\\mathrm{H}}(m,n)}$. Substitute $n = \\beta m$:\n$$\n\\rho(\\beta) = \\lim_{m \\to \\infty} \\frac{3m(\\beta m)^2 - (\\beta m)^3}{2m(\\beta m)^2 - \\frac{2}{3}(\\beta m)^3}\n= \\lim_{m \\to \\infty} \\frac{3\\beta^2 m^3 - \\beta^3 m^3}{2\\beta^2 m^3 - \\frac{2}{3}\\beta^3 m^3}\n$$\nThe expressions are homogeneous polynomials of degree $3$ in $m$. We can factor out $m^3$:\n$$\n\\rho(\\beta) = \\frac{m^3(3\\beta^2 - \\beta^3)}{m^3(2\\beta^2 - \\frac{2}{3}\\beta^3)} = \\frac{3\\beta^2 - \\beta^3}{2\\beta^2 - \\frac{2}{3}\\beta^3}\n$$\nSince $\\beta \\in (0,1]$, $\\beta \\neq 0$, so we can factor out $\\beta^2$ from the numerator and denominator:\n$$\n\\rho(\\beta) = \\frac{\\beta^2(3 - \\beta)}{\\beta^2(2 - \\frac{2}{3}\\beta)} = \\frac{3 - \\beta}{2 - \\frac{2}{3}\\beta}\n$$\nWe can simplify the denominator by factoring out $2/3$:\n$$\n2 - \\frac{2}{3}\\beta = \\frac{2}{3}(3 - \\beta)\n$$\nSubstituting this back into the expression for $\\rho(\\beta)$:\n$$\n\\rho(\\beta) = \\frac{3 - \\beta}{\\frac{2}{3}(3 - \\beta)}\n$$\nSince $\\beta \\le 1$, the term $3 - \\beta$ is non-zero and can be cancelled.\n$$\n\\rho(\\beta) = \\frac{1}{2/3} = \\frac{3}{2}\n$$\nThe asymptotic ratio of the flop counts is a constant, independent of the aspect ratio $\\beta$.", "answer": "$$\\boxed{\\frac{3}{2}}$$", "id": "3548480"}, {"introduction": "This final practice bridges the gap between algorithm design and robust, verifiable software. You are tasked with implementing $QR$ factorization routines using both Givens and Householder methods and then developing a principled verification suite to ensure their correctness. A key challenge is to confront the non-uniqueness of the $QR$ factorization by implementing a sign-alignment convention, a critical step in producing consistent and comparable numerical results in a real-world library. [@problem_id:3548524]", "problem": "Design and implement a complete program that, for a given finite set of real matrices, computes two orthogonal-triangular factorization (QR) decompositions: one via Householder reflectors and one via Givens rotations. The goal is to construct principled cross-checks to verify orthogonality, upper-trapezoidal structure of the triangular factor, reconstruction accuracy, and to propose and implement a mathematically sound method to align signs so that the two triangular factors are comparable under a consistent sign convention.\n\nYour derivation and implementation must start from the following fundamental base:\n- The definition of an orthogonal matrix: a real matrix $\\mathbf{Q} \\in \\mathbb{R}^{m \\times m}$ is orthogonal if $\\mathbf{Q}^{\\top} \\mathbf{Q} = \\mathbf{I}_m$.\n- The Euclidean norm induced by the inner product, and the Frobenius norm $\\|\\cdot\\|_F$ induced by the Euclidean inner product on matrices.\n- A Householder reflector is an orthogonal transformation of the form $\\mathbf{H} = \\mathbf{I} - 2 \\mathbf{u} \\mathbf{u}^{\\top}$ where $\\mathbf{u} \\in \\mathbb{R}^{k}$ with $\\|\\mathbf{u}\\|_2 = 1$, which reflects vectors across the hyperplane orthogonal to $\\mathbf{u}$.\n- A Givens rotation is an orthogonal transformation acting on a coordinate plane spanned by two standard basis vectors, parameterized by scalars $c$ and $s$ satisfying $c^2 + s^2 = 1$, used to annihilate selected entries.\n\nYou must:\n- Construct $\\mathbf{Q}_{\\mathrm{H}}$ and $\\mathbf{R}_{\\mathrm{H}}$ using Householder reflectors so that $\\mathbf{A} = \\mathbf{Q}_{\\mathrm{H}} \\mathbf{R}_{\\mathrm{H}}$ with $\\mathbf{Q}_{\\mathrm{H}}$ orthogonal and $\\mathbf{R}_{\\mathrm{H}}$ upper-trapezoidal.\n- Construct $\\mathbf{Q}_{\\mathrm{G}}$ and $\\mathbf{R}_{\\mathrm{G}}$ using Givens rotations so that $\\mathbf{A} = \\mathbf{Q}_{\\mathrm{G}} \\mathbf{R}_{\\mathrm{G}}$ with $\\mathbf{Q}_{\\mathrm{G}}$ orthogonal and $\\mathbf{R}_{\\mathrm{G}}$ upper-trapezoidal.\n- Propose and implement a sign-alignment method to make the diagonals of $\\mathbf{R}_{\\mathrm{H}}$ and $\\mathbf{R}_{\\mathrm{G}}$ comparable under a single sign convention. The method must preserve the equality $\\mathbf{A} = \\mathbf{Q} \\mathbf{R}$ for each factorization. A valid approach is to enforce a nonnegative diagonal in $\\mathbf{R}$ by multiplying row $k$ of $\\mathbf{R}$ and column $k$ of $\\mathbf{Q}$ by $-1$ whenever the diagonal element $\\mathbf{R}_{k k}$ is negative, for $k = 1, \\dots, \\min(m,n)$; when $\\mathbf{R}_{k k}$ is numerically zero, leave it unchanged.\n\nFor each matrix, design cross-checks that verify:\n- Upper-trapezoidal structure: the entries strictly below the main diagonal of $\\mathbf{R}_{\\mathrm{H}}$ and $\\mathbf{R}_{\\mathrm{G}}$ have small magnitude relative to the scale of $\\mathbf{A}$.\n- Orthogonality: $\\|\\mathbf{Q}^{\\top}\\mathbf{Q} - \\mathbf{I}\\|_F$ is small for both $\\mathbf{Q}_{\\mathrm{H}}$ and $\\mathbf{Q}_{\\mathrm{G}}$.\n- Reconstruction: $\\|\\mathbf{A} - \\mathbf{Q}\\mathbf{R}\\|_F$ is small for both decompositions.\n- Aligned triangular comparison: after applying the sign-alignment convention to both $\\mathbf{R}_{\\mathrm{H}}$ and $\\mathbf{R}_{\\mathrm{G}}$, the two triangular factors are close in Frobenius norm.\n\nAll comparisons must be performed using a numerically reasonable tolerance based on the Frobenius norm $\\|\\mathbf{A}\\|_F$ of the input and a scale-invariant threshold. Angles are implicit in the use of cosines and sines of Givens rotations and do not need explicit units. No physical units are involved.\n\nImplement the algorithms directly from the above principles; do not call any built-in orthogonal-triangular factorization (QR) routines. You may, however, use basic linear algebra operations.\n\nUse the following test suite of real matrices, given explicitly as arrays:\n- Test case $1$ (tall, full column rank):\n$$\n\\mathbf{A}_1 =\n\\begin{bmatrix}\n3 & -2 & 5 \\\\\n7 & 1 & -1 \\\\\n2 & 4 & 0 \\\\\n-3 & 5 & 2 \\\\\n1 & -6 & 3\n\\end{bmatrix}.\n$$\n- Test case $2$ (square, full rank):\n$$\n\\mathbf{A}_2 =\n\\begin{bmatrix}\n4 & 2 & -2 \\\\\n1 & 3 & 0 \\\\\n0 & -1 & 5\n\\end{bmatrix}.\n$$\n- Test case $3$ (square, rank deficient):\n$$\n\\mathbf{A}_3 =\n\\begin{bmatrix}\n1 & 0 & 1 & 2 \\\\\n0 & 1 & 2 & -1 \\\\\n2 & -3 & -4 & 1 \\\\\n-1 & 4 & 7 & 0\n\\end{bmatrix}.\n$$\n- Test case $4$ (wide, full row rank):\n$$\n\\mathbf{A}_4 =\n\\begin{bmatrix}\n1 & 0 & 2 & -1 & 3 \\\\\n0 & -2 & 1 & 4 & -1 \\\\\n3 & 5 & -2 & 0 & 1\n\\end{bmatrix}.\n$$\n\nFor each $\\mathbf{A}_i$, aggregate all cross-checks into a single boolean result $b_i$ that is $ \\mathrm{True} $ if and only if all of the following hold simultaneously:\n- $\\mathbf{R}_{\\mathrm{H}}$ is upper-trapezoidal within tolerance.\n- $\\mathbf{R}_{\\mathrm{G}}$ is upper-trapezoidal within tolerance.\n- $\\mathbf{Q}_{\\mathrm{H}}$ is orthogonal within tolerance.\n- $\\mathbf{Q}_{\\mathrm{G}}$ is orthogonal within tolerance.\n- $\\mathbf{A}_i \\approx \\mathbf{Q}_{\\mathrm{H}} \\mathbf{R}_{\\mathrm{H}}$ and $\\mathbf{A}_i \\approx \\mathbf{Q}_{\\mathrm{G}} \\mathbf{R}_{\\mathrm{G}}$ within tolerance.\n- After sign alignment to enforce a nonnegative diagonal in both $\\mathbf{R}_{\\mathrm{H}}$ and $\\mathbf{R}_{\\mathrm{G}}$, the two triangular factors approximately match within tolerance.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the boolean results as a comma-separated list enclosed in square brackets, in the order $[b_1,b_2,b_3,b_4]$. For example, an output could look like $[\\mathrm{True},\\mathrm{True},\\mathrm{True},\\mathrm{True}]$ if all test cases pass all cross-checks.", "solution": "The problem is well-posed, scientifically sound, and contains all necessary information for its resolution. It requests the implementation and verification of two fundamental numerical linear algebra algorithms—QR factorization via Householder reflectors and Givens rotations—and a subsequent comparison. This constitutes a standard and valuable exercise in computational science.\n\nThe core task is to find the QR factorization of a given real matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, which is a decomposition of the form $\\mathbf{A} = \\mathbf{Q}\\mathbf{R}$, where $\\mathbf{Q} \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix ($\\mathbf{Q}^{\\top}\\mathbf{Q} = \\mathbf{I}_m$) and $\\mathbf{R} \\in \\mathbb{R}^{m \\times n}$ is an upper-trapezoidal matrix. An upper-trapezoidal matrix has zero entries below its main diagonal, i.e., $\\mathbf{R}_{ij} = 0$ for $i > j$.\n\nWe will construct two such factorizations, $(\\mathbf{Q}_{\\mathrm{H}}, \\mathbf{R}_{\\mathrm{H}})$ and $(\\mathbf{Q}_{\\mathrm{G}}, \\mathbf{R}_{\\mathrm{G}})$, using Householder reflectors and Givens rotations, respectively.\n\n### 1. QR Factorization via Householder Reflectors\n\nThe strategy is to apply a sequence of orthogonal transformations to the matrix $\\mathbf{A}$ to progressively introduce zeros below the main diagonal. Each transformation zeroes out the sub-diagonal elements of a single column. The Householder reflector is the tool for this task.\n\nFor each column $k$ (from $1$ to $\\min(m-1, n)$), we aim to transform the current matrix, which we will call $\\mathbf{A}^{(k-1)}$ (with $\\mathbf{A}^{(0)} = \\mathbf{A}$), into $\\mathbf{A}^{(k)} = \\mathbf{H}_k \\mathbf{A}^{(k-1)}$, where $\\mathbf{H}_k$ is a Householder reflector chosen to zero out the entries in column $k$ below the diagonal element $\\mathbf{A}^{(k)}_{kk}$.\n\nThe reflector $\\mathbf{H}_k$ acts on the vector $\\mathbf{x} \\in \\mathbb{R}^{m-k+1}$ which is the sub-column $\\mathbf{A}^{(k-1)}_{k:m,k}$. We require a reflector $\\hat{\\mathbf{H}}_k$ such that $\\hat{\\mathbf{H}}_k \\mathbf{x} = \\alpha \\mathbf{e}_1$, where $\\mathbf{e}_1 = [1, 0, \\dots, 0]^{\\top}$ and $\\alpha$ is a scalar. Since orthogonal transformations preserve the Euclidean norm, $|\\alpha| = \\|\\mathbf{x}\\|_2$. The reflector is defined by a vector $\\mathbf{v}$ normal to the reflection hyperplane. This vector is given by $\\mathbf{v} = \\mathbf{x} - \\alpha \\mathbf{e}_1$. To avoid numerical cancellation, a robust choice for $\\alpha$ is $\\alpha = -\\mathrm{sgn}(\\mathbf{x}_1) \\|\\mathbf{x}\\|_2$, where $\\mathbf{x}_1$ is the first component of $\\mathbf{x}$.\n\nThe Householder reflector is then given by $\\hat{\\mathbf{H}}_k = \\mathbf{I} - 2\\frac{\\mathbf{v}\\mathbf{v}^{\\top}}{\\mathbf{v}^{\\top}\\mathbf{v}}$. In practice, to avoid forming the matrix explicitly, we compute $\\beta = 2/(\\mathbf{v}^{\\top}\\mathbf{v})$ and apply the transformation to a matrix $\\mathbf{C}$ as $\\hat{\\mathbf{H}}_k\\mathbf{C} = \\mathbf{C} - \\beta \\mathbf{v}(\\mathbf{v}^{\\top}\\mathbf{C})$.\n\nThe full transformation matrix at step $k$ is $$\n\\mathbf{H}_k = \\begin{pmatrix} \\mathbf{I}_{k-1} & \\mathbf{0} \\\\ \\mathbf{0} & \\hat{\\mathbf{H}}_k \\end{pmatrix}.\n$$ After $\\min(m-1, n)$ steps, we have $\\mathbf{H}_{\\min(m-1,n)} \\dots \\mathbf{H}_2 \\mathbf{H}_1 \\mathbf{A} = \\mathbf{R}_{\\mathrm{H}}$.\nSince each $\\mathbf{H}_k$ is orthogonal, their product is also orthogonal. Let $\\mathbf{Q}_{\\mathrm{H}}^{\\top} = \\mathbf{H}_{\\min(m-1,n)} \\dots \\mathbf{H}_1$. Then $\\mathbf{A} = \\mathbf{Q}_{\\mathrm{H}}\\mathbf{R}_{\\mathrm{H}}$, with $\\mathbf{Q}_{\\mathrm{H}} = \\mathbf{H}_1^{\\top} \\dots \\mathbf{H}_{\\min(m-1,n)}^{\\top} = \\mathbf{H}_1 \\dots \\mathbf{H}_{\\min(m-1,n)}$.\nThe matrix $\\mathbf{Q}_{\\mathrm{H}}$ is formed by starting with $\\mathbf{Q} = \\mathbf{I}_m$ and successively applying the reflectors: $\\mathbf{Q} \\leftarrow \\mathbf{Q} \\mathbf{H}_k$.\n\n### 2. QR Factorization via Givens Rotations\n\nGivens rotations offer a more fine-grained approach, introducing zeros one element at a time. A Givens rotation acts on a 2D plane spanned by two coordinate axes. To zero out the element at position $(i, j)$ (with $i > j$) using the pivot element at $(j, j)$, we apply a rotation in the $(j, i)$-plane.\n\nLet the values of the two elements be $a = \\mathbf{R}_{jj}$ and $b = \\mathbf{R}_{ij}$. We seek a rotation matrix that transforms the vector $[a, b]^{\\top}$ into $[r, 0]^{\\top}$. The rotation matrix is\n$$\n\\mathbf{G}_{ji}^{\\top} = \\begin{pmatrix} c & s \\\\ -s & c \\end{pmatrix}\n$$\nwhere $c^2 + s^2 = 1$. The transformation yields $$\n\\begin{pmatrix} c & s \\\\ -s & c \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = \\begin{pmatrix} ca + sb \\\\ -sa + cb \\end{pmatrix}.\n$$ To make the second component zero, we choose $c = a/r$ and $s = b/r$, where $r = \\sqrt{a^2 + b^2}$. This results in the new vector $[r, 0]^{\\top}$.\n\nThe overall algorithm proceeds by iterating through columns $j$ from $1$ to $n$, and for each column, iterating through rows $i$ from $j+1$ to $m$. At each step $(i, j)$, a Givens rotation $\\mathbf{G}_{ji}^{\\top}$ is applied to the left of the current matrix to zero out the element $\\mathbf{R}_{ij}$.\nThe final upper-trapezoidal matrix is $\\mathbf{R}_{\\mathrm{G}} = (\\dots \\mathbf{G}_{m,n-1}^{\\top} \\dots \\mathbf{G}_{j+1,j}^{\\top} \\dots) \\mathbf{A}$.\nThe orthogonal matrix $\\mathbf{Q}_{\\mathrm{G}}$ is the product of all the Givens matrices: $\\mathbf{Q}_{\\mathrm{G}} = (\\dots \\mathbf{G}_{j+1,j} \\dots \\mathbf{G}_{m,n-1} \\dots)$. This is constructed by initializing $\\mathbf{Q} = \\mathbf{I}_m$ and successively post-multiplying by each Givens matrix $\\mathbf{G}_{ji}$. The update rule for two columns $\\mathbf{q}_j$ and $\\mathbf{q}_i$ of $\\mathbf{Q}$ becomes:\n$$\n(\\mathbf{q}'_j, \\mathbf{q}'_i) = (\\mathbf{q}_j, \\mathbf{q}_i) \\begin{pmatrix} c & -s \\\\ s & c \\end{pmatrix}.\n$$\n\n### 3. Sign Alignment and Uniqueness\n\nThe QR factorization of a full-rank matrix is unique up to the signs of the columns of $\\mathbf{Q}$ and rows of $\\mathbf{R}$. Specifically, if $\\mathbf{A} = \\mathbf{Q}\\mathbf{R}$, then for any diagonal matrix $\\mathbf{D}$ with $\\mathbf{D}_{kk} = \\pm 1$, the factorization $\\mathbf{A} = (\\mathbf{Q}\\mathbf{D})(\\mathbf{D}\\mathbf{R})$ is also a valid QR factorization. To enable a meaningful comparison between $\\mathbf{R}_{\\mathrm{H}}$ and $\\mathbf{R}_{\\mathrm{G}}$, we must enforce a consistent sign convention.\n\nThe problem specifies a canonical representation where all diagonal elements of $\\mathbf{R}$ are non-negative. For each $k \\in \\{1, \\dots, \\min(m,n)\\}$, if $\\mathbf{R}_{kk} < 0$, we multiply the $k$-th row of $\\mathbf{R}$ and the $k$-th column of $\\mathbf{Q}$ by $-1$. This corresponds to setting $\\mathbf{D}_{kk} = \\mathrm{sgn}(\\mathbf{R}_{kk})$ (with $\\mathrm{sgn}(0)$ defined as $1$). This operation preserves the equality $\\mathbf{A}=\\mathbf{Q}\\mathbf{R}$ and the orthogonality of $\\mathbf{Q}$.\n\n### 4. Verification and Cross-Checks\n\nTo validate the correctness and precision of the implemented algorithms, we perform four types of checks for each input matrix $\\mathbf{A}$, using a dynamically scaled tolerance $\\tau = \\|\\mathbf{A}\\|_F \\times \\epsilon$, where $\\epsilon$ is a small factor like $10^{-12}$.\n\n1.  **Upper-Trapezoidal Structure**: The Frobenius norm of the strictly lower-triangular part of $\\mathbf{R}$ must be close to zero. We check $\\|\\mathrm{tril}(\\mathbf{R}, -1)\\|_F < \\tau$.\n2.  **Orthogonality of Q**: The matrix $\\mathbf{Q}$ must be orthogonal. We verify this by checking if its deviation from orthogonality is small: $\\|\\mathbf{Q}^{\\top}\\mathbf{Q} - \\mathbf{I}_m\\|_F < \\tau$.\n3.  **Reconstruction Accuracy**: The product $\\mathbf{Q}\\mathbf{R}$ must reconstruct the original matrix $\\mathbf{A}$. We check the reconstruction error: $\\|\\mathbf{A} - \\mathbf{Q}\\mathbf{R}\\|_F < \\tau$.\n4.  **Aligned Triangular Comparison**: After applying the sign-alignment convention to both $(\\mathbf{Q}_{\\mathrm{H}}, \\mathbf{R}_{\\mathrm{H}})$ and $(\\mathbf{Q}_{\\mathrm{G}}, \\mathbf{R}_{\\mathrm{G}})$ to obtain $(\\mathbf{Q}_{\\mathrm{H}}', \\mathbf{R}_{\\mathrm{H}}')$ and $(\\mathbf{Q}_{\\mathrm{G}}', \\mathbf{R}_{\\mathrm{G}}')$, the resulting triangular factors should be identical (within tolerance). We check $\\|\\mathbf{R}_{\\mathrm{H}}' - \\mathbf{R}_{\\mathrm{G}}'\\|_F < \\tau$.\n\nA test case is considered successful only if all these checks pass for both the Householder and Givens-based factorizations.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef qr_householder(A):\n    \"\"\"\n    Computes QR factorization of a matrix A using Householder reflectors.\n    Returns Q, R where A = QR, Q is orthogonal, and R is upper-trapezoidal.\n    \"\"\"\n    m, n = A.shape\n    R = A.copy().astype(float)\n    Q = np.identity(m, dtype=float)\n\n    for k in range(min(m - 1, n)):\n        # Extract the k-th column vector from the k-th row\n        x = R[k:, k]\n        # Skip if the sub-column is already zero\n        x_norm = np.linalg.norm(x)\n        if np.isclose(x_norm, 0.0):\n            continue\n\n        # Compute the Householder vector v\n        v = x.copy()\n        # Use copysign for robustness to avoid cancellation\n        v[0] += np.copysign(x_norm, x[0])\n        \n        v_norm = np.linalg.norm(v)\n        if np.isclose(v_norm, 0.0):\n            continue\n\n        v = v / v_norm  # Normalize v to get u\n        # Efficiently v is prepared for beta computation\n        # beta = 2.0\n        # H = I - beta * u * u.T\n        \n        # Apply the reflector to the submatrix of R\n        # R_sub = R_sub - beta * u * (u.T @ R_sub)\n        R_sub = R[k:, k:]\n        R[k:, k:] -= 2.0 * np.outer(v, v.T @ R_sub)\n\n        # Apply the reflector to the submatrix of Q\n        # Q_sub = Q_sub @ H = Q_sub @ (I - beta * u * u.T) = Q_sub - beta * (Q_sub @ u) * u.T\n        Q_sub = Q[:, k:]\n        Q[:, k:] -= 2.0 * np.outer(Q_sub @ v, v)\n\n    return Q, R\n\n\ndef qr_givens(A):\n    \"\"\"\n    Computes QR factorization of a matrix A using Givens rotations.\n    Returns Q, R where A = QR, Q is orthogonal, and R is upper-trapezoidal.\n    \"\"\"\n    m, n = A.shape\n    R = A.copy().astype(float)\n    Q = np.identity(m, dtype=float)\n\n    for j in range(n):\n        for i in range(j + 1, m):\n            # Element to be zeroed out is R[i, j]\n            a = R[j, j]\n            b = R[i, j]\n\n            if np.isclose(b, 0.0):\n                continue\n            \n            # Compute c and s for the Givens rotation\n            r = np.hypot(a, b)\n            c = a / r\n            s = b / r\n\n            # Rotation matrix for rows: [[c, s], [-s, c]]\n            # Apply G.T to R (affecting rows j and i)\n            R_j_row = R[j, j:].copy()\n            R_i_row = R[i, j:].copy()\n            R[j, j:] = c * R_j_row + s * R_i_row\n            R[i, j:] = -s * R_j_row + c * R_i_row\n\n            # Apply G to Q (affecting columns j and i)\n            # Rotation matrix for columns: [[c, -s], [s, c]]\n            Q_j_col = Q[:, j].copy()\n            Q_i_col = Q[:, i].copy()\n            Q[:, j] = c * Q_j_col + s * Q_i_col\n            Q[:, i] = -s * Q_j_col + c * Q_i_col\n            \n    return Q, R\n\n\ndef align_signs(Q, R):\n    \"\"\"\n    Enforces a sign convention on QR factorization.\n    The convention is that the diagonal elements of R must be non-negative.\n    \"\"\"\n    m, n = R.shape\n    Q_aligned = Q.copy()\n    R_aligned = R.copy()\n\n    for k in range(min(m, n)):\n        if R_aligned[k, k] < 0:\n            R_aligned[k, :] *= -1\n            Q_aligned[:, k] *= -1\n    \n    return Q_aligned, R_aligned\n\n\ndef run_checks_for_matrix(A):\n    \"\"\"\n    Performs all computations and cross-checks for a single matrix A.\n    Returns True if all checks pass, False otherwise.\n    \"\"\"\n    m, n = A.shape\n    \n    # Use a relative tolerance, with a small absolute floor\n    norm_A = np.linalg.norm(A, 'fro')\n    if np.isclose(norm_A, 0.0):\n        tol = 1e-10\n    else:\n        tol = norm_A * 1e-12\n\n    # 1. Compute QR factorizations\n    Q_H, R_H = qr_householder(A)\n    Q_G, R_G = qr_givens(A)\n\n    # 2. Perform cross-checks\n    checks = []\n\n    # Check 1: Upper-trapezoidal structure\n    checks.append(np.linalg.norm(np.tril(R_H, -1), 'fro') < tol)\n    checks.append(np.linalg.norm(np.tril(R_G, -1), 'fro') < tol)\n\n    # Check 2: Orthogonality of Q\n    I_m = np.identity(m)\n    checks.append(np.linalg.norm(Q_H.T @ Q_H - I_m, 'fro') < tol)\n    checks.append(np.linalg.norm(Q_G.T @ Q_G - I_m, 'fro') < tol)\n\n    # Check 3: Reconstruction accuracy\n    checks.append(np.linalg.norm(A - Q_H @ R_H, 'fro') < tol)\n    checks.append(np.linalg.norm(A - Q_G @ R_G, 'fro') < tol)\n\n    # Check 4: Aligned triangular comparison\n    Q_H_a, R_H_a = align_signs(Q_H, R_H)\n    Q_G_a, R_G_a = align_signs(Q_G, R_G)\n\n    # Sanity check: ensure alignment preserved factorization\n    checks.append(np.linalg.norm(A - Q_H_a @ R_H_a, 'fro') < tol)\n    checks.append(np.linalg.norm(A - Q_G_a @ R_G_a, 'fro') < tol)\n    \n    # Main comparison of aligned R factors\n    checks.append(np.linalg.norm(R_H_a - R_G_a, 'fro') < tol)\n\n    return all(checks)\n\n\ndef solve():\n    \"\"\"\n    Main function to run the process for the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (tall, full column rank)\n        np.array([\n            [3, -2, 5],\n            [7, 1, -1],\n            [2, 4, 0],\n            [-3, 5, 2],\n            [1, -6, 3]\n        ]),\n        # Test case 2 (square, full rank)\n        np.array([\n            [4, 2, -2],\n            [1, 3, 0],\n            [0, -1, 5]\n        ]),\n        # Test case 3 (square, rank deficient)\n        np.array([\n            [1, 0, 1, 2],\n            [0, 1, 2, -1],\n            [2, -3, -4, 1],\n            [-1, 4, 7, 0]\n        ]),\n        # Test case 4 (wide, full row rank)\n        np.array([\n            [1, 0, 2, -1, 3],\n            [0, -2, 1, 4, -1],\n            [3, 5, -2, 0, 1]\n        ]),\n    ]\n    \n    results = []\n    for A in test_cases:\n        result = run_checks_for_matrix(A)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3548524"}]}