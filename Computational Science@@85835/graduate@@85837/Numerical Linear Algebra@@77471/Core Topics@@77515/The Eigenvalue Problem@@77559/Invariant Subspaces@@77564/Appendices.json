{"hands_on_practices": [{"introduction": "This first exercise is designed to solidify your understanding of the fundamental definitions of invariant and reducing subspaces. By working with a small, non-diagonalizable matrix, you will manually derive its complete lattice of invariant subspaces, offering a concrete look at their structure beyond simple eigenspaces. This foundational practice is crucial for building intuition before moving on to the algorithmic applications that rely on these concepts [@problem_id:3551519].", "problem": "Let $A \\in \\mathbb{R}^{3 \\times 3}$ act on $\\mathbb{R}^{3}$ equipped with the standard Euclidean inner product, where\n$$\nA=\\begin{bmatrix}2&1&0\\\\0&2&0\\\\0&0&3\\end{bmatrix}.\n$$\nA subspace $W \\subseteq \\mathbb{R}^{3}$ is called $A$-invariant if $AW \\subseteq W$. A subspace $W$ is called a reducing subspace for $A$ if both $W$ and its orthogonal complement $W^{\\perp}$ are $A$-invariant; equivalently, $W$ is invariant for both $A$ and the adjoint $A^{\\ast}$ with respect to the given inner product.\n\nStarting only from these definitions and the structure of $A$, derive all $A$-invariant subspaces of $\\mathbb{R}^{3}$ and determine which of these are reducing subspaces. Then compute the ordered pair $(N_{\\mathrm{inv}}, N_{\\mathrm{red}})$, where $N_{\\mathrm{inv}}$ is the total number of $A$-invariant subspaces and $N_{\\mathrmed}$ is the total number of reducing subspaces, counting also the trivial subspaces $\\{0\\}$ and $\\mathbb{R}^{3}$. Provide your final answer as this ordered pair. No rounding is required and no units are to be used.", "solution": "The problem asks for the determination and enumeration of all $A$-invariant and reducing subspaces for a given matrix $A \\in \\mathbb{R}^{3 \\times 3}$. Let the standard basis for $\\mathbb{R}^3$ be $\\{e_1, e_2, e_3\\}$, where $e_1 = (1, 0, 0)^T$, $e_2 = (0, 1, 0)^T$, and $e_3 = (0, 0, 1)^T$.\n\nFirst, we analyze the structure of the matrix $A$:\n$$\nA=\\begin{bmatrix}2&1&0\\\\0&2&0\\\\0&0&3\\end{bmatrix}\n$$\nThe characteristic polynomial is $\\det(A - \\lambda I) = (2-\\lambda)^2(3-\\lambda)$. The eigenvalues are $\\lambda_1 = 2$ with algebraic multiplicity $m_1 = 2$, and $\\lambda_2 = 3$ with algebraic multiplicity $m_2 = 1$.\n\nAn invariant subspace for $A$ is a subspace $W \\subseteq \\mathbb{R}^3$ such that $Aw \\in W$ for all $w \\in W$. The theory of linear operators states that any invariant subspace $W$ of $A$ can be decomposed as a direct sum of its intersections with the generalized eigenspaces of $A$. Let $G_{\\lambda}(A) = \\ker((A - \\lambda I)^m)$ be the generalized eigenspace for an eigenvalue $\\lambda$ with algebraic multiplicity $m$. Then $W = (W \\cap G_{2}(A)) \\oplus (W \\cap G_{3}(A))$.\n\nLet's determine the generalized eigenspaces for $A$.\nFor $\\lambda_1 = 2$:\n$A - 2I = \\begin{bmatrix}0&1&0\\\\0&0&0\\\\0&0&1\\end{bmatrix}$. The eigenspace is $E_2 = \\ker(A-2I) = \\text{span}\\{e_1\\}$.\nThe generalized eigenspace is $G_2(A) = \\ker((A-2I)^2)$.\n$$\n(A-2I)^2 = \\begin{bmatrix}0&1&0\\\\0&0&0\\\\0&0&1\\end{bmatrix}\\begin{bmatrix}0&1&0\\\\0&0&0\\\\0&0&1\\end{bmatrix} = \\begin{bmatrix}0&0&0\\\\0&0&0\\\\0&0&1\\end{bmatrix}\n$$\nThe null space of $(A-2I)^2$ is the set of vectors $(x, y, z)^T$ where $z=0$. Thus, $G_2(A) = \\text{span}\\{e_1, e_2\\}$.\n\nFor $\\lambda_2 = 3$:\nThe algebraic multiplicity is $1$, so the generalized eigenspace is the same as the eigenspace.\n$A - 3I = \\begin{bmatrix}-1&1&0\\\\0&-1&0\\\\0&0&0\\end{bmatrix}$.\nThe null space is given by $-x+y=0$ and $-y=0$, which implies $x=y=0$. Thus, $G_3(A) = E_3 = \\ker(A-3I) = \\text{span}\\{e_3\\}$.\n\nNow, we find the invariant subspaces of the restrictions of $A$ to its generalized eigenspaces.\n1. Invariant subspaces of $A$ contained in $G_3(A) = \\text{span}\\{e_3\\}$: Since this is a $1$-dimensional eigenspace, the only invariant subspaces are $\\{0\\}$ and $G_3(A)$ itself. There are $2$ such subspaces.\n\n2. Invariant subspaces of $A$ contained in $G_2(A) = \\text{span}\\{e_1, e_2\\}$: The restriction of $A$ to $G_2(A)$ has the matrix $A_2 = \\begin{bmatrix}2&1\\\\0&2\\end{bmatrix}$ in the basis $\\{e_1, e_2\\}$. The invariant subspaces of this operator are:\n   - The $0$-dimensional subspace: $\\{0\\}$.\n   - Any $1$-dimensional invariant subspace must be an eigenspace. The only eigenvalue is $2$, and the corresponding eigenspace is $\\ker(A_2-2I) = \\ker(\\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}) = \\text{span}\\{e_1\\}$.\n   - The $2$-dimensional subspace: $G_2(A)$ itself.\n   Thus, there are $3$ invariant subspaces contained in $G_2(A)$.\n\nAn arbitrary $A$-invariant subspace $W$ is a direct sum of an invariant subspace from $G_2(A)$ and one from $G_3(A)$. The total number of $A$-invariant subspaces is the product of the number of such subspaces in each generalized eigenspace.\n$N_{\\mathrm{inv}} = 3 \\times 2 = 6$.\nThe $A$-invariant subspaces are:\n\\begin{enumerate}\n    \\item $\\{0\\} \\oplus \\{0\\} = \\{0\\}$\n    \\item $\\{0\\} \\oplus \\text{span}\\{e_3\\} = \\text{span}\\{e_3\\}$\n    \\item $\\text{span}\\{e_1\\} \\oplus \\{0\\} = \\text{span}\\{e_1\\}$\n    \\item $\\text{span}\\{e_1\\} \\oplus \\text{span}\\{e_3\\} = \\text{span}\\{e_1, e_3\\}$\n    \\item $\\text{span}\\{e_1, e_2\\} \\oplus \\{0\\} = \\text{span}\\{e_1, e_2\\}$\n    \\item $\\text{span}\\{e_1, e_2\\} \\oplus \\text{span}\\{e_3\\} = \\mathbb{R}^3$\n\\end{enumerate}\n\nNext, we identify the reducing subspaces. A subspace $W$ is reducing for $A$ if it is invariant under both $A$ and its adjoint $A^*$. Since we are using the standard Euclidean inner product, $A^* = A^T$.\n$$\nA^T = \\begin{bmatrix}2&0&0\\\\1&2&0\\\\0&0&3\\end{bmatrix}\n$$\nA subspace is reducing if and only if it is in the intersection of the set of $A$-invariant subspaces and the set of $A^T$-invariant subspaces. We proceed to find the invariant subspaces for $A^T$. The eigenvalues of $A^T$ are the same as for $A$: $\\lambda_1 = 2$ and $\\lambda_2 = 3$.\n\nThe generalized eigenspaces for $A^T$ are:\nFor $\\lambda_1=2$: $A^T - 2I = \\begin{bmatrix}0&0&0\\\\1&0&0\\\\0&0&1\\end{bmatrix}$. The eigenspace is $E'_2 = \\ker(A^T-2I) = \\text{span}\\{e_2\\}$.\nThe generalized eigenspace $G_2(A^T) = \\ker((A^T-2I)^2)$.\n$$\n(A^T-2I)^2 = \\begin{bmatrix}0&0&0\\\\0&0&0\\\\0&0&1\\end{bmatrix}\n$$\nThe null space is where $z=0$, so $G_2(A^T) = \\text{span}\\{e_1, e_2\\}$.\n\nFor $\\lambda_2=3$: $G_3(A^T) = \\ker(A^T-3I) = \\text{span}\\{e_3\\}$.\n\nThe invariant subspaces of $A^T$ are constructed similarly.\n1. Invariant subspaces within $G_3(A^T) = \\text{span}\\{e_3\\}$ are $\\{0\\}$ and $\\text{span}\\{e_3\\}$. ($2$ subspaces)\n2. Invariant subspaces within $G_2(A^T) = \\text{span}\\{e_1, e_2\\}$: The restriction of $A^T$ to this space has matrix $A^T_2 = \\begin{bmatrix}2&0\\\\1&2\\end{bmatrix}$.\n   - $\\{0\\}$\n   - The $1$-dimensional invariant subspace (eigenspace) is $\\ker(A^T_2-2I) = \\ker(\\begin{bmatrix}0&0\\\\1&0\\end{bmatrix}) = \\text{span}\\{e_2\\}$.\n   - $G_2(A^T)$ itself.\n   There are $3$ such subspaces.\n\nThe $A^T$-invariant subspaces are:\n\\begin{enumerate}\n    \\item $\\{0\\}$\n    \\item $\\text{span}\\{e_3\\}$\n    \\item $\\text{span}\\{e_2\\}$\n    \\item $\\text{span}\\{e_2, e_3\\}$\n    \\item $\\text{span}\\{e_1, e_2\\}$\n    \\item $\\mathbb{R}^3$\n\\end{enumerate}\n\nThe reducing subspaces for $A$ are those common to both lists of invariant subspaces.\nComparing the two lists:\n- $\\{0\\}$ is common.\n- $\\text{span}\\{e_3\\}$ is common.\n- $\\text{span}\\{e_1\\}$ is only $A$-invariant.\n- $\\text{span}\\{e_1, e_3\\}$ is only $A$-invariant.\n- $\\text{span}\\{e_1, e_2\\}$ is common.\n- $\\mathbb{R}^3$ is common.\n- $\\text{span}\\{e_2\\}$ is only $A^T$-invariant.\n- $\\text{span}\\{e_2, e_3\\}$ is only $A^T$-invariant.\n\nThe reducing subspaces are $\\{0\\}$, $\\text{span}\\{e_3\\}$, $\\text{span}\\{e_1, e_2\\}$, and $\\mathbb{R}^3$.\nThere are $N_{\\mathrm{red}} = 4$ reducing subspaces.\n\nAn equivalent definition of a reducing subspace $W$ is that both $W$ and its orthogonal complement $W^\\perp$ are $A$-invariant.\n- For $W = \\{0\\}$, $W^\\perp = \\mathbb{R}^3$. Both are $A$-invariant.\n- For $W = \\mathbb{R}^3$, $W^\\perp = \\{0\\}$. Both are $A$-invariant.\n- For $W = \\text{span}\\{e_3\\}$, $W^\\perp = \\text{span}\\{e_1, e_2\\}$. Both are in the list of $A$-invariant subspaces.\n- For $W = \\text{span}\\{e_1, e_2\\}$, $W^\\perp = \\text{span}\\{e_3\\}$. Both are in the list of $A$-invariant subspaces.\nThis confirms the four reducing subspaces.\n\nThe total number of $A$-invariant subspaces is $N_{\\mathrm{inv}} = 6$. The total number of reducing subspaces is $N_{\\mathrm{red}} = 4$.\nThe required ordered pair is $(N_{\\mathrm{inv}}, N_{\\mathrm{red}})$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n6 & 4\n\\end{pmatrix}\n}\n$$", "id": "3551519"}, {"introduction": "The primary reason invariant subspaces are central to numerical linear algebra is their ability to simplify matrix structure through similarity transformations. This practice demonstrates this crucial principle by showing how a known invariant subspace leads directly to a block-triangular form of a matrix, a concept that underpins the Schur decomposition and many eigenvalue algorithms. You will bridge theory and practice by using the QR factorization to construct the necessary unitary transformation [@problem_id:3551527].", "problem": "Let $A \\in \\mathbb{C}^{5 \\times 5}$ and $V \\in \\mathbb{C}^{5 \\times 2}$ be given by\n$$\nA \\;=\\;\n\\begin{pmatrix}\n2 & 1 & -1 & 0 & 0 \\\\\n0 & 3 & 0 & 4 & 0 \\\\\n5 & -2 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & -1 & 0 \\\\\n0 & 7 & 0 & 0 & 2\n\\end{pmatrix},\n\\qquad\nV \\;=\\;\n\\begin{pmatrix}\n1 & 1 \\\\\n0 & 0 \\\\\n1 & -1 \\\\\n0 & 0 \\\\\n0 & 0\n\\end{pmatrix}.\n$$\nSuppose the columns of $V$ form a basis for a $2$-dimensional $A$-invariant subspace, so that there exists $T \\in \\mathbb{C}^{2 \\times 2}$ with $A V = V T$. Using only first principles from numerical linear algebra, perform the following steps:\n\n1. Verify that $A V$ lies in the range of $V$ by determining the matrix $T$ such that $A V = V T$.\n\n2. Compute the thin orthonormal-triangular (QR) factorization of $V$, that is $V = Q R$ with $Q \\in \\mathbb{C}^{5 \\times 2}$ having orthonormal columns and $R \\in \\mathbb{C}^{2 \\times 2}$ upper triangular.\n\n3. Extend $Q$ to a unitary basis $\\widehat{Q} \\in \\mathbb{C}^{5 \\times 5}$ by adjoining three orthonormal columns $Q_{\\perp} \\in \\mathbb{C}^{5 \\times 3}$ orthogonal to $\\mathrm{range}(Q)$, for example by choosing the canonical basis vectors $e_2, e_4, e_5$ as the columns of $Q_{\\perp}$. Form the similarity transform $\\widehat{Q}^{*} A \\widehat{Q}$, where $(\\cdot)^{*}$ denotes the conjugate transpose.\n\n4. Using the invariance and the computed factors, show that $\\widehat{Q}^{*} A \\widehat{Q}$ is block upper triangular and compute its leading $2 \\times 2$ block explicitly.\n\nReport as your final answer the determinant of the leading $2 \\times 2$ block of $\\widehat{Q}^{*} A \\widehat{Q}$. Express your final answer exactly (no rounding).", "solution": "The user wants to analyze an invariant subspace problem.\n\nThe problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\nThe given quantities are:\n- The matrix $A \\in \\mathbb{C}^{5 \\times 5}$:\n$$ A = \\begin{pmatrix} 2 & 1 & -1 & 0 & 0 \\\\ 0 & 3 & 0 & 4 & 0 \\\\ 5 & -2 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & -1 & 0 \\\\ 0 & 7 & 0 & 0 & 2 \\end{pmatrix} $$\n- The matrix $V \\in \\mathbb{C}^{5 \\times 2}$:\n$$ V = \\begin{pmatrix} 1 & 1 \\\\ 0 & 0 \\\\ 1 & -1 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix} $$\n- The columns of $V$ form a basis for a $2$-dimensional $A$-invariant subspace. This is expressed by the relation $A V = V T$ for some matrix $T \\in \\mathbb{C}^{2 \\times 2}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem deals with fundamental concepts of numerical linear algebra: invariant subspaces, QR factorization, and similarity transformations (Schur decomposition). These are well-established and scientifically sound.\n- **Well-Posedness**: The problem is fully specified. The matrices are given, and the tasks are laid out in a clear sequence. The premise of invariance is testable and, as shown below, holds true. The construction of the extended basis $\\widehat{Q}$ is made unambiguous by a specific suggestion. Thus, a unique solution exists.\n- **Objectivity**: The language is purely mathematical and devoid of any subjective elements.\n\nThe crucial premise is that $\\mathrm{range}(V)$ is an $A$-invariant subspace. We verify this as the first part of the solution process. If this premise were false, the problem would be invalid due to a contradictory setup. As will be shown in Step 1 of the solution, the premise is correct. The problem is therefore deemed **valid**.\n\n**Solution**\n\n**1. Verification of Invariance and Determination of T**\n\nLet the columns of $V$ be denoted by $v_1$ and $v_2$:\n$$ v_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad v_2 = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThe statement that the column space of $V$, denoted $\\mathrm{range}(V)$, is an $A$-invariant subspace means that for any vector $x \\in \\mathrm{range}(V)$, the vector $Ax$ is also in $\\mathrm{range}(V)$. It is sufficient to check this for the basis vectors $v_1$ and $v_2$.\n\nFirst, we compute the product $AV$:\n$$ A v_1 = \\begin{pmatrix} 2 & 1 & -1 & 0 & 0 \\\\ 0 & 3 & 0 & 4 & 0 \\\\ 5 & -2 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & -1 & 0 \\\\ 0 & 7 & 0 & 0 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2-1 \\\\ 0 \\\\ 5 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 5 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n$$ A v_2 = \\begin{pmatrix} 2 & 1 & -1 & 0 & 0 \\\\ 0 & 3 & 0 & 4 & 0 \\\\ 5 & -2 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & -1 & 0 \\\\ 0 & 7 & 0 & 0 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2+1 \\\\ 0 \\\\ 5 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 0 \\\\ 5 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nSo, the matrix $AV$ is:\n$$ AV = \\begin{pmatrix} 1 & 3 \\\\ 0 & 0 \\\\ 5 & 5 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix} $$\nFor $\\mathrm{range}(V)$ to be $A$-invariant, the columns of $AV$ ($Av_1$ and $Av_2$) must be linear combinations of the columns of $V$ ($v_1$ and $v_2$). The coefficients of these linear combinations form the matrix $T = \\begin{pmatrix} t_{11} & t_{12} \\\\ t_{21} & t_{22} \\end{pmatrix}$ such that $Av_1 = t_{11}v_1 + t_{21}v_2$ and $Av_2 = t_{12}v_1 + t_{22}v_2$.\n\nFor $Av_1$:\n$$ \\begin{pmatrix} 1 \\\\ 0 \\\\ 5 \\\\ 0 \\\\ 0 \\end{pmatrix} = t_{11} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + t_{21} \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} t_{11}+t_{21} \\\\ 0 \\\\ t_{11}-t_{21} \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThis gives the system of equations: $t_{11}+t_{21} = 1$ and $t_{11}-t_{21} = 5$. Adding them gives $2t_{11}=6 \\implies t_{11}=3$. Subtracting gives $2t_{21}=-4 \\implies t_{21}=-2$.\n\nFor $Av_2$:\n$$ \\begin{pmatrix} 3 \\\\ 0 \\\\ 5 \\\\ 0 \\\\ 0 \\end{pmatrix} = t_{12} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + t_{22} \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} t_{12}+t_{22} \\\\ 0 \\\\ t_{12}-t_{22} \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThis gives the system: $t_{12}+t_{22} = 3$ and $t_{12}-t_{22} = 5$. Adding gives $2t_{12}=8 \\implies t_{12}=4$. Subtracting gives $2t_{22}=-2 \\implies t_{22}=-1$.\n\nThus, $AV$ lies in the range of $V$, and the matrix $T$ is:\n$$ T = \\begin{pmatrix} 3 & 4 \\\\ -2 & -1 \\end{pmatrix} $$\n\n**2. Thin QR Factorization of V**\n\nWe apply the Gram-Schmidt process to the columns of $V=[v_1, v_2]$ to find $Q=[q_1, q_2]$ and $R$.\nFirst column $q_1$:\n$$ r_{11} = \\|v_1\\|_2 = \\sqrt{1^2+0^2+1^2+0^2+0^2} = \\sqrt{2} $$\n$$ q_1 = \\frac{v_1}{r_{11}} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nSecond column $q_2$:\nThe off-diagonal element $r_{12}$ is:\n$$ r_{12} = q_1^* v_2 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & 0 & 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\frac{1}{\\sqrt{2}}(1-1) = 0 $$\nThe columns $v_1$ and $v_2$ are orthogonal. We compute the next orthogonal vector for the basis:\n$$ w_2 = v_2 - r_{12} q_1 = v_2 - 0 \\cdot q_1 = v_2 $$\nThe diagonal element $r_{22}$ is:\n$$ r_{22} = \\|w_2\\|_2 = \\|v_2\\|_2 = \\sqrt{1^2+0^2+(-1)^2+0^2+0^2} = \\sqrt{2} $$\n$$ q_2 = \\frac{w_2}{r_{22}} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThe thin QR factorization $V = QR$ is given by:\n$$ Q = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & 1 \\\\ 0 & 0 \\\\ 1 & -1 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix}, \\quad R = \\begin{pmatrix} \\sqrt{2} & 0 \\\\ 0 & \\sqrt{2} \\end{pmatrix} $$\n\n**3. Extension to a Unitary Basis**\n\nThe matrix $Q \\in \\mathbb{C}^{5 \\times 2}$ has two orthonormal columns that span the same invariant subspace as $V$. We extend this basis to a full unitary basis $\\widehat{Q} \\in \\mathbb{C}^{5 \\times 5}$ of $\\mathbb{C}^5$. The problem suggests adjoining the canonical basis vectors $e_2, e_4, e_5$.\nLet $Q_{\\perp} = [e_2, e_4, e_5] = \\begin{pmatrix} 0 & 0 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$.\nThe columns of $Q$ have non-zero entries only in the first and third components. The columns of $Q_\\perp$ have non-zero entries in the second, fourth, and fifth components. Therefore, $Q^* Q_{\\perp} = 0$, meaning the columns of $Q_\\perp$ are orthogonal to the range of $Q$. Since the columns of $Q_\\perp$ are also orthonormal, $\\widehat{Q} = [Q, Q_{\\perp}]$ is a unitary matrix.\n$$ \\widehat{Q} = \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1\n\\end{pmatrix} $$\nThe similarity transform is $\\widehat{Q}^{*} A \\widehat{Q}$, which can be written in block form as:\n$$ \\widehat{Q}^{*} A \\widehat{Q} = \\begin{pmatrix} Q^* \\\\ Q_{\\perp}^* \\end{pmatrix} A \\begin{pmatrix} Q & Q_{\\perp} \\end{pmatrix} = \\begin{pmatrix} Q^* A Q & Q^* A Q_{\\perp} \\\\ Q_{\\perp}^* A Q & Q_{\\perp}^* A Q_{\\perp} \\end{pmatrix} $$\n\n**4. Block Structure and Leading Block Computation**\n\nThe space $\\mathrm{range}(Q) = \\mathrm{range}(V)$ is an $A$-invariant subspace. This means that for any column $q_j$ of $Q$, $A q_j$ is in $\\mathrm{range}(Q)$. The space $\\mathrm{range}(Q_{\\perp})$ is the orthogonal complement of $\\mathrm{range}(Q)$. Therefore, any vector in $\\mathrm{range}(Q)$, including $A q_j$, is orthogonal to any column of $Q_{\\perp}$. This implies that $Q_{\\perp}^* (A q_j) = 0$ for each $j$. Consequently, the entire block $Q_{\\perp}^* A Q$ is a zero matrix.\n$$ Q_{\\perp}^* A Q = 0 $$\nThe transformed matrix is block upper triangular:\n$$ \\widehat{Q}^{*} A \\widehat{Q} = \\begin{pmatrix} Q^* A Q & Q^* A Q_{\\perp} \\\\ 0 & Q_{\\perp}^* A Q_{\\perp} \\end{pmatrix} $$\nThe leading $2 \\times 2$ block is $Q^* A Q$. We can compute this block using the relations established in the previous steps. We start with the invariance property $AV = VT$ and substitute the QR factorization $V = QR$:\n$$ A(QR) = (QR)T $$\nMultiplying from the left by $Q^*$ and using the property that $Q^*Q=I_2$ (since $Q$ has orthonormal columns), we get:\n$$ Q^* A Q R = Q^* Q R T = I_2 R T = RT $$\nSo, $(Q^* A Q) R = RT$. Since $R = \\sqrt{2} I_2$ is invertible, we can multiply from the right by $R^{-1}$:\n$$ Q^* A Q = R T R^{-1} $$\nSubstituting the computed matrices for $R$ and $T$:\n$$ R = \\sqrt{2} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad R^{-1} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} $$\n$$ Q^* A Q = \\left( \\sqrt{2} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\right) \\begin{pmatrix} 3 & 4 \\\\ -2 & -1 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\right) = \\begin{pmatrix} 3 & 4 \\\\ -2 & -1 \\end{pmatrix} = T $$\nThe leading $2 \\times 2$ block of $\\widehat{Q}^{*} A \\widehatQ$ is precisely the matrix $T$.\n\nThe final step is to compute the determinant of this block:\n$$ \\det(Q^* A Q) = \\det(T) = \\det \\begin{pmatrix} 3 & 4 \\\\ -2 & -1 \\end{pmatrix} = (3)(-1) - (4)(-2) = -3 + 8 = 5 $$", "answer": "$$\\boxed{5}$$", "id": "3551527"}, {"introduction": "While the previous exercises assumed knowledge of an invariant subspace, this final practice tackles the essential question of how to find one. You will implement the Arnoldi iteration, a cornerstone algorithm for large-scale matrix problems, to construct an invariant Krylov subspace from a given starting vector. This computational exercise highlights the moment an expanding Krylov subspace \"stabilizes\" into an invariant one, providing a direct, hands-on look at the mechanics of a powerful iterative method [@problem_id:3551503].", "problem": "You are given square real matrices $A \\in \\mathbb{R}^{n \\times n}$ and nonzero vectors $b \\in \\mathbb{R}^n$. Let the Krylov subspace of order $m$ be defined by\n$$\n\\mathcal{K}_m(A,b) \\equiv \\operatorname{span}\\{b, Ab, A^2 b, \\dots, A^{m-1} b\\}.\n$$\nA subspace $\\mathcal{S} \\subseteq \\mathbb{R}^n$ is called invariant under $A$ if $A \\mathcal{S} \\subseteq \\mathcal{S}$. Your task is to compute, for each given pair $(A,b)$, the smallest positive integer $m$ for which $\\mathcal{K}_m(A,b)$ is invariant under $A$, and to verify this invariance numerically via the residual of the Arnoldi relation.\n\nBase your reasoning only on the following fundamental definitions and well-tested facts:\n- The Krylov subspace $\\mathcal{K}_m(A,b)$ is built by repeatedly applying $A$ to $b$, and its dimension is at most $m$ and at most $n$.\n- The Arnoldi process constructs an orthonormal basis $V_m = [v_1,\\dots,v_m]$ for $\\mathcal{K}_m(A,b)$ (with $v_1 = b/\\lVert b\\rVert_2$) and an upper Hessenberg matrix $H_m \\in \\mathbb{R}^{m \\times m}$ satisfying the Arnoldi relation\n$$\nA V_m = V_m H_m + h_{m+1,m} v_{m+1} e_m^\\top,\n$$\nwhere $e_m \\in \\mathbb{R}^m$ is the $m$-th standard basis vector and $h_{m+1,m} \\ge 0$. In exact arithmetic, $h_{m+1,m} = 0$ if and only if $\\mathcal{K}_m(A,b)$ is invariant under $A$.\n- Numerical computations in finite precision can only establish the equality $h_{m+1,m} = 0$ up to a tolerance.\n\nDesign an algorithm that, given $(A,b)$ and a tolerance $\\tau > 0$, computes the smallest $m$ such that the Arnoldi process satisfies $h_{m+1,m} \\le \\tau$. Interpret this as numerical detection of invariance. Then, verify the invariance by computing the Frobenius norm of the Arnoldi residual\n$$\nR_m \\equiv A V_m - V_m H_m,\n$$\nwhich should be numerically small when $h_{m+1,m}$ is numerically zero. Use Classical Gram-Schmidt with a second reorthogonalization pass (CGS2) to improve numerical stability. Use the Euclidean norm for vector normalization and the Frobenius norm for matrices.\n\nImplement your algorithm as a complete program that solves the following test suite. For each case, use a tolerance $\\tau = 10^{-12}$:\n\n- Case 1 (Diagonalizable with multiple active eigenvalues): Let\n$$\nA_1 = \\operatorname{diag}(1,2,4,7,11), \\quad b_1 = [1, \\, 1, \\, 0, \\, 1, \\, 0]^\\top.\n$$\n- Case 2 (Single Jordan block, non-diagonalizable): Let $A_2 \\in \\mathbb{R}^{4 \\times 4}$ be the Jordan block with eigenvalue $2$, that is, $(A_2)_{ii} = 2$ and $(A_2)_{i,i+1} = 1$ for $i=1,2,3$, and zeros elsewhere. Let\n$$\nb_2 = [1, \\, 0, \\, 0, \\, 0]^\\top.\n$$\n- Case 3 (Eigenvector input, immediate invariance): Let\n$$\nA_3 = \\begin{bmatrix}\n3 & 1 & 0 & 0 \\\\\n0 & 5 & 2 & 0 \\\\\n0 & 0 & 7 & 0 \\\\\n0 & 0 & 0 & 9\n\\end{bmatrix}, \\quad b_3 = [0, \\, 0, \\, 1, \\, 0]^\\top.\n$$\n- Case 4 (Block diagonal with a nilpotent block): Let\n$$\nA_4 = \\begin{bmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 5 & 0 \\\\\n0 & 0 & 0 & 6\n\\end{bmatrix}, \\quad b_4 = [1, \\, 0, \\, 0, \\, 0]^\\top.\n$$\n\nFor each case, your program must:\n- Run Arnoldi until the first index $m$ such that $h_{m+1,m} \\le \\tau$ is encountered (this is the smallest $m$ declaring numerical invariance).\n- Compute the Frobenius norm $\\lVert R_m \\rVert_F$ to verify the Arnoldi relation and thus the invariance.\n- Return the triple consisting of the integer $m$, the scalar $h_{m+1,m}$, and the scalar $\\lVert R_m \\rVert_F$. Round the two scalar outputs to $12$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a three-element list $[m, h, r]$ corresponding to one test case in the order given above. For example, an output with four cases should look like\n$$\n[[m_1,h_1,r_1],[m_2,h_2,r_2],[m_3,h_3,r_3],[m_4,h_4,r_4]].\n$$\nNo additional text should be printed.", "solution": "The problem requires us to determine the smallest positive integer $m$ for which the Krylov subspace $\\mathcal{K}_m(A,b) = \\operatorname{span}\\{b, Ab, \\dots, A^{m-1} b\\}$ is an invariant subspace under a given real matrix $A \\in \\mathbb{R}^{n \\times n}$ for a given non-zero vector $b \\in \\mathbb{R}^n$. A subspace $\\mathcal{S}$ is invariant under $A$ if for any vector $s \\in \\mathcal{S}$, the vector $As$ is also in $\\mathcal{S}$, which is denoted as $A\\mathcal{S} \\subseteq \\mathcal{S}$.\n\nThe Arnoldi iteration is a direct method for constructing an orthonormal basis for $\\mathcal{K}_m(A,b)$ and provides a criterion for detecting invariance. The process generates a sequence of orthonormal vectors $\\{v_1, v_2, \\dots, v_m\\}$ that form a basis for $\\mathcal{K}_m(A,b)$, where $v_1 = b / \\lVert b \\rVert_2$. These vectors are stored as columns of a matrix $V_m = [v_1, v_2, \\dots, v_m] \\in \\mathbb{R}^{n \\times m}$. The algorithm also produces an upper Hessenberg matrix $H_m \\in \\mathbb{R}^{m \\times m}$. These matrices are related by the Arnoldi relation:\n$$\nA V_m = V_m H_m + h_{m+1,m} v_{m+1} e_m^\\top\n$$\nwhere $v_{m+1}$ is a unit vector orthogonal to the columns of $V_m$, $e_m \\in \\mathbb{R}^m$ is the $m$-th standard basis vector, and $h_{m+1,m} = \\lVert (I - V_m V_m^\\top) A v_m \\rVert_2 \\ge 0$.\n\nThe subspace $\\mathcal{K}_m(A,b)$ is invariant under $A$ if and only if $A v_j \\in \\mathcal{K}_m(A,b)$ for all $j \\in \\{1, \\dots, m\\}$. Due to the structure of the Arnoldi process, this condition simplifies to checking if $A v_m \\in \\mathcal{K}_m(A,b)$. The vector $(I - V_m V_m^\\top) A v_m$ represents the component of $A v_m$ that is orthogonal to $\\mathcal{K}_m(A,b)$. Therefore, $A v_m \\in \\mathcal{K}_m(A,b)$ if and only if this orthogonal component is the zero vector. The norm of this vector is precisely $h_{m+1,m}$. Consequently, in exact arithmetic, the Krylov subspace $\\mathcal{K}_m(A,b)$ is invariant under $A$ if and only if $h_{m+1,m}=0$. This marks a \"breakdown\" of the Arnoldi process, as a new basis vector $v_{m+1}$ cannot be generated. The smallest such $m$ corresponds to the degree of the minimal polynomial of the vector $b$ with respect to the matrix $A$.\n\nIn finite-precision arithmetic, we check for numerical invariance by testing if $h_{m+1,m}$ is smaller than a given tolerance $\\tau > 0$. The task is to find the smallest positive integer $m$ satisfying $h_{m+1,m} \\le \\tau$.\n\nThe algorithm to be implemented is the Arnoldi iteration with Classical Gram-Schmidt and a second reorthogonalization pass (CGS2) for numerical stability.\n\nThe algorithm proceeds as follows:\n1.  Initialize: Given $A \\in \\mathbb{R}^{n \\times n}$, $b \\in \\mathbb{R}^n$, and tolerance $\\tau > 0$. Let $n$ be the dimension of the space. Pre-allocate matrices $V \\in \\mathbb{R}^{n \\times (n+1)}$ and $H \\in \\mathbb{R}^{(n+1) \\times n}$.\n2.  Start: Compute the first basis vector $v_1 = b / \\lVert b \\rVert_2$. Store it as the first column of $V$.\n3.  Iterate for $m = 1, 2, \\dots, n$:\n    a. Let $v_m$ be the $m$-th basis vector (the $m$-th column of $V$). Compute the new vector $w = A v_m$.\n    b. Orthogonalize $w$ against the existing basis $\\{v_1, \\dots, v_m\\}$. This is the first pass of CGS. Compute the projection coefficients $h_{j,m} = v_j^\\top w$ for $j=1,\\dots,m$, and subtract the projection sum from $w$.\n    c. Reorthogonalize for numerical stability. This is the second pass. Compute correction terms and update both the vector $w$ and the Hessenberg entries $h_{j,m}$.\n    d. Compute the norm of the resulting vector $w$, which is $h_{m+1,m} = \\lVert w \\rVert_2$. Store this in $H$.\n    e. Check for termination: If $h_{m+1,m} \\le \\tau$, the process terminates. The smallest dimension for which the subspace is numerically invariant is $m$. Break the loop.\n    f. If the process has not terminated, normalize to get the next basis vector: $v_{m+1} = w / h_{m+1,m}$. Store it as the $(m+1)$-th column of $V$.\n\nUpon termination at step $m$, we have the orthonormal basis $V_m = [v_1, \\dots, v_m]$ and the upper Hessenberg matrix $H_m \\in \\mathbb{R}^{m \\times m}$. The residual of the Arnoldi relation is the matrix $R_m = A V_m - V_m H_m$. From the relation, $R_m = h_{m+1,m} v_{m+1} e_m^\\top$. To verify the numerical invariance and correctness of the implementation, we compute the Frobenius norm $\\lVert R_m \\rVert_F$. Theoretically, we have $\\lVert R_m \\rVert_F = \\lVert h_{m+1,m} v_{m+1} e_m^\\top \\rVert_F = h_{m+1,m} \\lVert v_{m+1} e_m^\\top \\rVert_F = h_{m+1,m}$. Computing $\\lVert A V_m - V_m H_m \\rVert_F$ directly serves as a robust numerical check.\n\nThe final output for each test case will be the triplet $(m, h_{m+1,m}, \\lVert R_m \\rVert_F)$, where $m$ is the dimension of the invariant subspace, $h_{m+1,m}$ is the value that triggered termination, and $\\lVert R_m \\rVert_F$ is the computed Frobenius norm of the residual.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef arnoldi_invariance(A, b, tol=1e-12):\n    \"\"\"\n    Computes the smallest m for which the Krylov subspace K_m(A,b) is invariant.\n\n    Args:\n        A (np.ndarray): The square matrix of size n x n.\n        b (np.ndarray): The starting vector of size n.\n        tol (float): The tolerance for h_{m+1,m} to be considered zero.\n\n    Returns:\n        tuple: A triplet (m, h, r) where:\n            m (int): The smallest dimension of the invariant Krylov subspace.\n            h (float): The value of h_{m+1,m} at termination.\n            r (float): The Frobenius norm of the residual matrix R_m = A V_m - V_m H_m.\n    \"\"\"\n    n = A.shape[0]\n    V = np.zeros((n, n + 1), dtype=float)\n    H = np.zeros((n + 1, n), dtype=float)\n\n    # Step 1: Initialize\n    V[:, 0] = b / np.linalg.norm(b)\n\n    for m in range(n):\n        # Step 2: Generate new vector\n        v_m = V[:, m]\n        w = A @ v_m\n\n        # Step 3: Classical Gram-Schmidt with reorthogonalization (CGS2)\n        # First pass\n        h_col = V[:, :m+1].T @ w\n        w = w - V[:, :m+1] @ h_col\n        H[:m+1, m] = h_col\n\n        # Second reorthogonalization pass for stability\n        h_corr = V[:, :m+1].T @ w\n        w = w - V[:, :m+1] @ h_corr\n        H[:m+1, m] += h_corr\n        \n        # Step 4: Compute h_{m+1,m} and check for invariance\n        h_next = np.linalg.norm(w)\n        H[m+1, m] = h_next\n\n        # Termination condition\n        if h_next <= tol:\n            m_final = m + 1\n            Vm = V[:, :m_final]\n            Hm = H[:m_final, :m_final]\n            \n            # Verification: Compute residual norm\n            Rm = A @ Vm - Vm @ Hm\n            res_norm = np.linalg.norm(Rm, 'fro')\n            \n            return m_final, h_next, res_norm\n\n        # Step 5: Normalize to get next basis vector\n        V[:, m+1] = w / h_next\n\n    # If loop completes, the space is the full R^n\n    m_final = n\n    Vm = V[:, :n]\n    Hm = H[:n, :n]\n    Rm = A @ Vm - Vm @ Hm\n    res_norm = np.linalg.norm(Rm, 'fro')\n    h_final = H[n, n-1]\n\n    return m_final, h_final, res_norm\n\n\ndef solve():\n    \"\"\"\n    Defines and solves the test suite of problems.\n    \"\"\"\n    tol = 1e-12\n\n    # Case 1\n    A1 = np.diag([1, 2, 4, 7, 11])\n    b1 = np.array([1, 1, 0, 1, 0], dtype=float)\n\n    # Case 2\n    A2 = np.diag([2.0]*4) + np.diag([1.0]*3, k=1)\n    b2 = np.array([1, 0, 0, 0], dtype=float)\n\n    # Case 3\n    A3 = np.array([\n        [3, 1, 0, 0],\n        [0, 5, 2, 0],\n        [0, 0, 7, 0],\n        [0, 0, 0, 9]\n    ], dtype=float)\n    b3 = np.array([0, 0, 1, 0], dtype=float)\n    \n    # Case 4\n    A4 = np.array([\n        [0, 1, 0, 0],\n        [0, 0, 0, 0],\n        [0, 0, 5, 0],\n        [0, 0, 0, 6]\n    ], dtype=float)\n    b4 = np.array([1, 0, 0, 0], dtype=float)\n    \n    test_cases = [\n        (A1, b1),\n        (A2, b2),\n        (A3, b3),\n        (A4, b4),\n    ]\n\n    results = []\n    for A, b in test_cases:\n        m, h, r = arnoldi_invariance(A, b, tol)\n        # Format h and r to 12 decimal places as strings, m is an integer\n        h_str = f\"{h:.12f}\"\n        r_str = f\"{r:.12f}\"\n        results.append(f\"[{m},{h_str},{r_str}]\")\n\n    # Final print statement in the exact required format\n    print(f\"[[{','.join(results)}]]\")\n\nsolve()\n```", "id": "3551503"}]}