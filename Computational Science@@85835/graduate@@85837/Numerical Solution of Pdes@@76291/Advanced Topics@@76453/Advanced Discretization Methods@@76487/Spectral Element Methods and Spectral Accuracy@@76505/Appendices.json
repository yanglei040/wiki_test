{"hands_on_practices": [{"introduction": "This first exercise is about rolling up your sleeves and building a fundamental component of the spectral element method—the mass matrix. By starting from the basic definitions of Legendre-Gauss-Lobatto (LGL) nodes and weights, this practice demystifies how the discrete inner product is formed [@problem_id:3446203]. You will see firsthand how the unique properties of the nodal Lagrange basis, where $\\ell_i(x_j) = \\delta_{ij}$, combined with the LGL quadrature rule, lead to a remarkably simple and efficient diagonal mass matrix—a property often called \"mass lumping.\"", "problem": "Consider the reference element $[-1,1]$ and the degree-$N$ spectral element with nodal interpolation at the Legendre–Gauss–Lobatto (LGL) nodes, where $N=4$. Let $\\{x_i\\}_{i=0}^{N}$ denote the LGL nodes and $\\{w_i\\}_{i=0}^{N}$ the associated LGL quadrature weights. Let $\\{\\ell_i(x)\\}_{i=0}^{N}$ be the nodal Lagrange basis on $[-1,1]$ satisfying $\\ell_i(x_j)=\\delta_{ij}$. The LGL quadrature defines the quadrature-based mass matrix $M \\in \\mathbb{R}^{(N+1)\\times(N+1)}$ by\n$$\nM_{ij} \\equiv \\sum_{k=0}^{N} w_k\\, \\ell_i(x_k)\\,\\ell_j(x_k).\n$$\nTasks:\n- Using only fundamental definitions of Legendre polynomials and the LGL construction, determine the LGL nodes $\\{x_i\\}_{i=0}^{4}$ and LGL weights $\\{w_i\\}_{i=0}^{4}$ for $N=4$, and report their numerical values rounded to at least four significant figures.\n- Assemble the matrix $M$ using the above quadrature and the nodal Lagrange basis. Prove from first principles why all off-diagonal entries $M_{ij}$ with $i\\neq j$ vanish under LGL quadrature for nodal bases.\n- From your assembled $M$, compute its determinant exactly.\n\nProvide as your final answer the exact value of $\\det(M)$ as a single simplified analytic expression (no units, no decimal approximation).", "solution": "The problem requires us to determine the Legendre–Gauss–Lobatto (LGL) nodes and weights for a degree $N=4$ spectral element, analyze the structure of the quadrature-based mass matrix $M$, and compute its exact determinant. We will address each part of the problem systematically.\n\nFirst, we determine the LGL nodes and weights for $N=4$. The LGL nodes $\\{x_i\\}_{i=0}^{N}$ on the interval $[-1,1]$ are the roots of the equation $(1-x^2)P_N'(x) = 0$, where $P_N(x)$ is the Legendre polynomial of degree $N$. For $N=4$, the nodes are the roots of $(1-x^2)P_4'(x)=0$. The endpoints $x_0=-1$ and $x_4=1$ are immediately identified as nodes. The interior nodes are the roots of $P_4'(x)=0$.\n\nWe find $P_4(x)$ using the Legendre polynomial recurrence relation:\n$(n+1)P_{n+1}(x) = (2n+1)xP_n(x) - nP_{n-1}(x)$, with base cases $P_0(x)=1$ and $P_1(x)=x$.\nFor $n=1$: $2P_2(x) = 3xP_1(x) - 1P_0(x) = 3x^2 - 1 \\implies P_2(x) = \\frac{1}{2}(3x^2-1)$.\nFor $n=2$: $3P_3(x) = 5xP_2(x) - 2P_1(x) = \\frac{5}{2}x(3x^2-1) - 2x = \\frac{1}{2}(15x^3-9x) \\implies P_3(x) = \\frac{1}{2}(5x^3-3x)$.\nFor $n=3$: $4P_4(x) = 7xP_3(x) - 3P_2(x) = \\frac{7}{2}x(5x^3-3x) - \\frac{3}{2}(3x^2-1) = \\frac{1}{2}(35x^4 - 21x^2 - 9x^2 + 3) = \\frac{1}{2}(35x^4 - 30x^2 + 3)$.\nThus, $P_4(x) = \\frac{1}{8}(35x^4 - 30x^2 + 3)$.\n\nThe derivative is $P_4'(x) = \\frac{1}{8}(140x^3 - 60x) = \\frac{5}{2}(7x^3 - 3x) = \\frac{5}{2}x(7x^2 - 3)$.\nThe roots of $P_4'(x)=0$ are found by setting $x(7x^2 - 3)=0$, which yields $x=0$ and $x=\\pm\\sqrt{\\frac{3}{7}}$.\n\nThe LGL nodes for $N=4$, ordered from smallest to largest, are:\n$x_0 = -1$\n$x_1 = -\\sqrt{\\frac{3}{7}}$\n$x_2 = 0$\n$x_3 = \\sqrt{\\frac{3}{7}}$\n$x_4 = 1$\n\nNumerically, these nodes are approximately:\n$x_0 = -1.000$\n$x_1 \\approx -0.6547$\n$x_2 = 0.0000$\n$x_3 \\approx 0.6547$\n$x_4 = 1.0000$\n\nThe LGL quadrature weights $\\{w_i\\}_{i=0}^{N}$ are given by the formula $w_i = \\frac{2}{N(N+1)[P_N(x_i)]^2}$. For $N=4$, this becomes $w_i = \\frac{2}{4(5)[P_4(x_i)]^2} = \\frac{1}{10[P_4(x_i)]^2}$.\n\n- For the endpoints $x_0=-1$ and $x_4=1$, we use $P_4(-1) = (-1)^4 = 1$ and $P_4(1) = 1$.\n$w_0 = w_4 = \\frac{1}{10[P_4(1)]^2} = \\frac{1}{10(1)^2} = \\frac{1}{10}$.\n\n- For the node $x_2=0$, we evaluate $P_4(0) = \\frac{1}{8}(3) = \\frac{3}{8}$.\n$w_2 = \\frac{1}{10[P_4(0)]^2} = \\frac{1}{10(\\frac{3}{8})^2} = \\frac{1}{10 \\cdot \\frac{9}{64}} = \\frac{64}{90} = \\frac{32}{45}$.\n\n- For the interior nodes $x_1=-\\sqrt{\\frac{3}{7}}$ and $x_3=\\sqrt{\\frac{3}{7}}$, we evaluate $P_4(x)$ at $x^2 = \\frac{3}{7}$.\n$P_4(\\pm\\sqrt{\\frac{3}{7}}) = \\frac{1}{8}\\left(35\\left(\\frac{3}{7}\\right)^2 - 30\\left(\\frac{3}{7}\\right) + 3\\right) = \\frac{1}{8}\\left(35\\frac{9}{49} - \\frac{90}{7} + 3\\right) = \\frac{1}{8}\\left(\\frac{45}{7} - \\frac{90}{7} + \\frac{21}{7}\\right) = \\frac{1}{8}\\left(\\frac{-24}{7}\\right) = -\\frac{3}{7}$.\n$w_1 = w_3 = \\frac{1}{10[P_4(\\sqrt{3/7})]^2} = \\frac{1}{10(-\\frac{3}{7})^2} = \\frac{1}{10 \\cdot \\frac{9}{49}} = \\frac{49}{90}$.\n\nThe LGL weights for $N=4$ are:\n$w_0 = \\frac{1}{10}$ (approx. $0.1000$)\n$w_1 = \\frac{49}{90}$ (approx. $0.5444$)\n$w_2 = \\frac{32}{45}$ (approx. $0.7111$)\n$w_3 = \\frac{49}{90}$ (approx. $0.5444$)\n$w_4 = \\frac{1}{10}$ (approx. $0.1000$)\n\nNext, we address the structure of the mass matrix $M$. The problem defines the quadrature-based mass matrix elements as $M_{ij} = \\sum_{k=0}^{N} w_k\\, \\ell_i(x_k)\\,\\ell_j(x_k)$. The basis functions $\\{\\ell_i(x)\\}_{i=0}^{N}$ are the nodal Lagrange basis polynomials, defined by the property $\\ell_i(x_j) = \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\n\nApplying this property to the definition of $M_{ij}$:\nThe term $\\ell_i(x_k)$ is $1$ if $k=i$ and $0$ otherwise. Thus, we can write $\\ell_i(x_k) = \\delta_{ik}$.\nSimilarly, $\\ell_j(x_k) = \\delta_{jk}$.\nSubstituting these into the sum:\n$M_{ij} = \\sum_{k=0}^{N} w_k \\delta_{ik} \\delta_{jk}$.\n\nTo prove that off-diagonal entries vanish, consider $i \\neq j$. For any term in the sum (indexed by $k$), the product $\\delta_{ik}\\delta_{jk}$ is non-zero only if both $\\delta_{ik}=1$ and $\\delta_{jk}=1$. This would require $k=i$ and $k=j$ simultaneously. Since we assumed $i \\neq j$, this is impossible. Therefore, for any $k \\in \\{0, 1, \\dots, N\\}$, the product $\\delta_{ik}\\delta_{jk}$ is always $0$ when $i \\neq j$.\nThis leads to:\n$M_{ij} = \\sum_{k=0}^{N} w_k \\cdot 0 = 0$ for $i \\neq j$.\nThis proves from first principles that all off-diagonal entries of $M$ are zero.\n\nFor the diagonal entries, consider $i = j$. The expression becomes:\n$M_{ii} = \\sum_{k=0}^{N} w_k \\delta_{ik} \\delta_{ik} = \\sum_{k=0}^{N} w_k (\\delta_{ik})^2$.\nSince $\\delta_{ik}$ is either $0$ or $1$, $(\\delta_{ik})^2 = \\delta_{ik}$.\n$M_{ii} = \\sum_{k=0}^{N} w_k \\delta_{ik}$.\nThis sum contains only one non-zero term, which occurs when $k=i$. The value of that term is $w_i \\cdot \\delta_{ii} = w_i \\cdot 1 = w_i$.\nSo, $M_{ii} = w_i$.\n\nThe matrix $M$ is a diagonal matrix whose diagonal entries are the LGL quadrature weights:\n$M = \\text{diag}(w_0, w_1, w_2, w_3, w_4)$.\n\nAssembling the matrix $M$ with the calculated weights for $N=4$:\n$$\nM = \\begin{pmatrix} \\frac{1}{10} & 0 & 0 & 0 & 0 \\\\ 0 & \\frac{49}{90} & 0 & 0 & 0 \\\\ 0 & 0 & \\frac{32}{45} & 0 & 0 \\\\ 0 & 0 & 0 & \\frac{49}{90} & 0 \\\\ 0 & 0 & 0 & 0 & \\frac{1}{10} \\end{pmatrix}\n$$\n\nFinally, we compute the determinant of $M$. The determinant of a diagonal matrix is the product of its diagonal elements.\n$\\det(M) = w_0 \\cdot w_1 \\cdot w_2 \\cdot w_3 \\cdot w_4$.\nSubstituting the exact fractional values of the weights:\n$\\det(M) = \\left(\\frac{1}{10}\\right) \\cdot \\left(\\frac{49}{90}\\right) \\cdot \\left(\\frac{32}{45}\\right) \\cdot \\left(\\frac{49}{90}\\right) \\cdot \\left(\\frac{1}{10}\\right)$.\nWe simplify this product using prime factorization.\n$10 = 2 \\cdot 5$\n$49 = 7^2$\n$90 = 9 \\cdot 10 = 3^2 \\cdot 2 \\cdot 5$\n$32 = 2^5$\n$45 = 9 \\cdot 5 = 3^2 \\cdot 5$\n\n$\\det(M) = \\frac{1}{2 \\cdot 5} \\cdot \\frac{7^2}{3^2 \\cdot 2 \\cdot 5} \\cdot \\frac{2^5}{3^2 \\cdot 5} \\cdot \\frac{7^2}{3^2 \\cdot 2 \\cdot 5} \\cdot \\frac{1}{2 \\cdot 5}$\n\nCombining the terms in the numerator and denominator:\nNumerator: $1 \\cdot 7^2 \\cdot 2^5 \\cdot 7^2 \\cdot 1 = 2^5 \\cdot 7^4$.\nDenominator: $(2\\cdot5) \\cdot (3^2\\cdot2\\cdot5) \\cdot (3^2\\cdot5) \\cdot (3^2\\cdot2\\cdot5) \\cdot (2\\cdot5) = 2^{1+1+1+1} \\cdot 3^{2+2+2} \\cdot 5^{1+1+1+1+1} = 2^4 \\cdot 3^6 \\cdot 5^5$.\n\n$\\det(M) = \\frac{2^5 \\cdot 7^4}{2^4 \\cdot 3^6 \\cdot 5^5} = \\frac{2 \\cdot 7^4}{3^6 \\cdot 5^5}$.\nThis is the exact simplified analytical expression for the determinant. Let's calculate the integer values for presentation: $7^4=2401$, $3^6=729$, $5^5=3125$.\n$\\det(M) = \\frac{2 \\cdot 2401}{729 \\cdot 3125} = \\frac{4802}{2278125}$. The form with prime powers is more fundamental.\nThe final answer is $\\frac{2 \\cdot 7^4}{3^6 \\cdot 5^5}$.", "answer": "$$\n\\boxed{\\frac{2 \\cdot 7^4}{3^6 \\cdot 5^5}}\n$$", "id": "3446203"}, {"introduction": "While spectral methods offer exceptional accuracy for linear problems, their application to nonlinear PDEs introduces a subtle but critical pitfall: aliasing. This exercise moves beyond simple cases to investigate what happens when a nonlinear term, such as $u(x)^3$, is evaluated using a standard quadrature rule that lacks sufficient precision for the resulting high-degree polynomial [@problem_id:3446167]. By explicitly calculating the aliasing error, you will gain a concrete understanding of why this error does not simply vanish as the polynomial degree $N$ increases, and why advanced techniques like over-integration are essential for stable and accurate nonlinear simulations.", "problem": "Consider a single spectral element on the reference interval $[-1,1]$ in a spectral element method. Let the polynomial approximation be $u(x)=\\sum_{k=0}^{N} c_k x^k$ of degree $N$ with real coefficients $c_k$. To evaluate the nonlinear modal energy transfer, one often computes the integral $\\int_{-1}^{1} u(x)^{3}\\,dx$ using Legendre–Gauss–Lobatto (LGL) quadrature. Define the aliasing error as the difference between the LGL quadrature applied to $u(x)^{3}$ and the exact integral,\n$$E_{N}(c_0,\\ldots,c_N) \\equiv \\sum_{j=0}^{N} w_{j}\\,u(x_{j})^{3} \\;-\\; \\int_{-1}^{1} u(x)^{3}\\,dx,$$\nwhere $\\{x_{j}\\}_{j=0}^{N}$ and $\\{w_{j}\\}_{j=0}^{N}$ are the LGL nodes and weights for degree-$N$ interpolation on $[-1,1]$.\n\nTask 1. Starting from the facts that (i) LGL quadrature integrates polynomials exactly up to a finite degree tied to $N$, and (ii) the polynomial $u(x)^{3}$ has degree $3N$, give a principled argument that $E_{N}(c_0,\\ldots,c_N)$ is of order $O(1)$ in $N$ under the assumption that the coefficients $\\{c_k\\}$ are independent of $N$ and bounded. Your argument should begin from these foundational facts and explain, without resorting to any shortcut formulas, why underintegration of $u(x)^{3}$ generally leads to an aliasing error that does not grow with $N$.\n\nTask 2. Compute the aliasing error $E_{3}(c_0,c_1,c_2,c_3)$ explicitly for $N=3$. For this computation, use the $N+1=4$-point LGL rule on $[-1,1]$ with nodes $x_{0}=-1$, $x_{1}=-1/\\sqrt{5}$, $x_{2}=1/\\sqrt{5}$, $x_{3}=1$ and weights $w_{0}=w_{3}=\\frac{1}{6}$, $w_{1}=w_{2}=\\frac{5}{6}$. Your final expression must be given in closed form as a function of $c_0,c_1,c_2,c_3$ only.\n\nAnswer format: Provide a single closed-form analytic expression for $E_{3}(c_0,c_1,c_2,c_3)$. No rounding is required, and no physical units apply here.", "solution": "The problem is divided into two parts. The first part requires a principled argument regarding the order of the aliasing error. The second part requires a direct computation of this error for a specific case.\n\n**Part 1: Argument for Aliasing Error Scaling**\n\nThe aliasing error is defined as\n$$E_{N}(c_0,\\ldots,c_N) \\equiv \\sum_{j=0}^{N} w_{j}\\,u(x_{j})^{3} \\;-\\; \\int_{-1}^{1} u(x)^{3}\\,dx$$\nwhere $u(x) = \\sum_{k=0}^{N} c_k x^k$ is a polynomial of degree $N$, and the quadrature is the $(N+1)$-point Legendre-Gauss-Lobatto (LGL) rule.\n\nThe principled argument proceeds as follows:\n1.  We are given that an $(N+1)$-point LGL quadrature rule exactly integrates any polynomial of degree up to $2N-1$.\n2.  The integrand in our problem is $f(x) = u(x)^3$. Since $u(x)$ is a polynomial of degree $N$, $f(x)$ is a polynomial of degree $3N$. For any $N \\ge 2$, the degree of the integrand, $3N$, is greater than the degree of exactness of the quadrature rule, $2N-1$. Therefore, the LGL quadrature of $u(x)^3$ is not exact, and the error $E_N$ is generally non-zero. This error is a direct result of aliasing, where high-frequency components of the integrand are misinterpreted as low-frequency components by the discrete set of quadrature points.\n3.  We can decompose the integrand $u(x)^3$ into a low-degree part that is integrated exactly and a high-degree part that is not. Let $u(x)^3 = P_{\\le 2N-1}(x) + R(x)$, where $P_{\\le 2N-1}(x)$ is the polynomial containing all terms of $u(x)^3$ with degree up to $2N-1$, and $R(x)$ contains all terms with degrees from $2N$ to $3N$.\n4.  By linearity of the integral and the quadrature sum, the error is:\n    $$E_N = \\left( \\sum_{j=0}^{N} w_j P_{\\le 2N-1}(x_j) + \\sum_{j=0}^{N} w_j R(x_j) \\right) - \\left( \\int_{-1}^{1} P_{\\le 2N-1}(x) dx + \\int_{-1}^{1} R(x) dx \\right)$$\n5.  Due to the exactness property of the LGL rule, the terms involving $P_{\\le 2N-1}(x)$ cancel out: $\\sum_{j=0}^{N} w_j P_{\\le 2N-1}(x_j) = \\int_{-1}^{1} P_{\\le 2N-1}(x) dx$.\n6.  The aliasing error thus simplifies to the error incurred from integrating the high-degree remainder polynomial $R(x)$:\n    $$E_N = \\sum_{j=0}^{N} w_j R(x_j) - \\int_{-1}^{1} R(x) dx$$\n7.  The problem assumes that the coefficients $\\{c_k\\}$ of $u(x)$ are bounded and independent of $N$. This implies that as $N$ increases, we are adding higher-degree terms to $u(x)$ whose coefficients do not grow. The high-degree remainder polynomial $R(x)$ is constructed from products of these coefficients. For example, the highest-degree term in $u(x)^3$ is $c_N^3 x^{3N}$, and the coefficient $c_N^3$ is bounded if $c_N$ is bounded. More generally, the coefficients of $R(x)$ are polynomials in the coefficients $\\{c_k\\}$, and under the given assumption, they remain bounded, not growing with $N$.\n8.  The aliasing error $E_N$ is the difference between the true integral of $R(x)$ and its aliased representation from the quadrature sum. The aliasing phenomenon maps high-frequency modes (like those in $R(x)$) back onto the low-frequency modes resolvable by the grid. The magnitude of this aliased contribution is determined by the magnitude of the high-frequency content itself (i.e., the coefficients of $R(x)$).\n9.  Since the coefficients of $R(x)$ are bounded (not functions of N that grow), the magnitude of $R(x)$ and its integral, as well as the magnitude of its aliased counterpart given by the quadrature sum, are also controlled. They do not contain a factor that systematically grows with $N$. Consequently, their difference, the aliasing error $E_N$, is expected to remain bounded, or be of order $O(1)$, as $N$ varies. It is a structural error inherent to the choice of quadrature rule, not an error that diminishes or explodes with refinement for a fixed class of polynomials as described.\n\n**Part 2: Explicit Computation for $N=3$**\n\nFor $N=3$, the polynomial is $u(x) = c_0 + c_1 x + c_2 x^2 + c_3 x^3$. The integrand $u(x)^3$ is a polynomial of degree $9$. The $N=3$ LGL quadrature rule, which uses $N+1=4$ points, is exact for polynomials of degree up to $2N-1 = 2(3)-1 = 5$.\n\nThe aliasing error $E_3$ is given by:\n$$E_3 = \\sum_{j=0}^{3} w_{j}\\,u(x_{j})^{3} - \\int_{-1}^{1} u(x)^{3}\\,dx$$\nAs established in Part 1, this error arises only from the terms in $u(x)^3$ with degree greater than $5$. Let $R(x)$ be the part of $u(x)^3$ with degree from $6$ to $9$.\n$$E_3 = \\left(\\sum_{j=0}^{3} w_j R(x_j)\\right) - \\left(\\int_{-1}^{1} R(x) dx\\right)$$\nWe first determine the polynomial $R(x)$ by expanding $(c_0 + c_1 x + c_2 x^2 + c_3 x^3)^3$ and collecting terms of degree $6$ and higher.\nThe coefficient of $x^k$ in the expansion of $(\\sum_{i=0}^3 c_i x^i)^3$ is $\\sum_{i+j+l=k} c_i c_j c_l$. For terms of degree $\\ge 6$:\n-   Degree $9$: $x^9$ arises from $(c_3 x^3)^3$. Coefficient is $c_3^3$.\n-   Degree $8$: $x^8$ arises from $3(c_3 x^3)^2(c_2 x^2)$. Coefficient is $3c_2 c_3^2$.\n-   Degree $7$: $x^7$ arises from $3(c_3 x^3)^2(c_1 x)$ and $3(c_2 x^2)^2(c_3 x^3)$. Coefficient is $(3c_1 c_3^2 + 3c_2^2 c_3)$.\n-   Degree $6$: $x^6$ arises from $3(c_3 x^3)^2(c_0)$, $6(c_3 x^3)(c_2 x^2)(c_1 x)$, and $(c_2 x^2)^3$. Coefficient is $(3c_0 c_3^2 + 6c_1 c_2 c_3 + c_2^3)$.\n\nSo, $R(x) = (3c_0 c_3^2 + 6c_1 c_2 c_3 + c_2^3)x^6 + (3c_1 c_3^2 + 3c_2^2 c_3)x^7 + (3c_2 c_3^2)x^8 + c_3^3 x^9$.\n\nLet's denote the coefficients as $R(x) = A x^6 + B x^7 + C x^8 + D x^9$. By linearity of the error functional $E_3(\\cdot)$, we have $E_3 = A \\cdot E_3(x^6) + B \\cdot E_3(x^7) + C \\cdot E_3(x^8) + D \\cdot E_3(x^9)$.\n\nThe LGL grid $\\{x_j, w_j\\}$ is symmetric. The integral of any odd function over $[-1,1]$ is zero, and the LGL quadrature of any odd function over a symmetric grid is also zero. Therefore, $E_3(x^7)=0$ and $E_3(x^9)=0$. We only need to compute the errors for $x^6$ and $x^8$.\n\nFor $x^6$:\n-   Exact integral: $\\int_{-1}^1 x^6 dx = \\frac{x^7}{7}\\Big|_{-1}^1 = \\frac{2}{7}$.\n-   Quadrature: $\\sum_{j=0}^3 w_j x_j^6 = 2w_0(1)^6 + 2w_1(1/\\sqrt{5})^6 = 2(\\frac{1}{6}) + 2(\\frac{5}{6})(\\frac{1}{125}) = \\frac{1}{3} + \\frac{5}{375} = \\frac{1}{3} + \\frac{1}{75} = \\frac{25+1}{75} = \\frac{26}{75}$.\n-   Error $E_3(x^6) = \\frac{26}{75} - \\frac{2}{7} = \\frac{182 - 150}{525} = \\frac{32}{525}$.\n\nFor $x^8$:\n-   Exact integral: $\\int_{-1}^1 x^8 dx = \\frac{x^9}{9}\\Big|_{-1}^1 = \\frac{2}{9}$.\n-   Quadrature: $\\sum_{j=0}^3 w_j x_j^8 = 2w_0(1)^8 + 2w_1(1/\\sqrt{5})^8 = \\frac{1}{3} + \\frac{5}{3}(\\frac{1}{625}) = \\frac{1}{3} + \\frac{1}{375} = \\frac{125+1}{375} = \\frac{126}{375} = \\frac{42}{125}$.\n-   Error $E_3(x^8) = \\frac{42}{125} - \\frac{2}{9} = \\frac{378 - 250}{1125} = \\frac{128}{1125}$.\n\nNow, we combine these results:\n$E_3 = A \\cdot E_3(x^6) + C \\cdot E_3(x^8) = A \\left(\\frac{32}{525}\\right) + C \\left(\\frac{128}{1125}\\right)$.\n\nSubstituting the expressions for $A$ and $C$:\n$A = 3c_0 c_3^2 + 6c_1 c_2 c_3 + c_2^3$\n$C = 3c_2 c_3^2$\n\n$E_3 = (3c_0 c_3^2 + 6c_1 c_2 c_3 + c_2^3)\\left(\\frac{32}{525}\\right) + (3c_2 c_3^2)\\left(\\frac{128}{1125}\\right)$.\n\nDistributing the terms:\n$E_3 = \\frac{32 \\cdot 3}{525}c_0 c_3^2 + \\frac{32 \\cdot 6}{525}c_1 c_2 c_3 + \\frac{32}{525}c_2^3 + \\frac{128 \\cdot 3}{1125}c_2 c_3^2$.\n\nSimplifying the numerical coefficients:\n-   $\\frac{32 \\cdot 3}{525} = \\frac{32}{175}$.\n-   $\\frac{32 \\cdot 6}{525} = \\frac{64}{175}$.\n-   $\\frac{128 \\cdot 3}{1125} = \\frac{128}{375}$.\n\nThe final expression for the aliasing error is:\n$$E_3(c_0, c_1, c_2, c_3) = \\frac{32}{175}c_0 c_3^2 + \\frac{64}{175}c_1 c_2 c_3 + \\frac{32}{525}c_2^3 + \\frac{128}{375}c_2 c_3^2$$", "answer": "$$\\boxed{\\frac{32}{175} c_{0} c_{3}^{2} + \\frac{64}{175} c_{1} c_{2} c_{3} + \\frac{32}{525} c_{2}^{3} + \\frac{128}{375} c_{2} c_{3}^{2}}$$", "id": "3446167"}, {"introduction": "This final practice transitions from pen-and-paper analysis to a practical coding implementation, tackling a common and challenging problem in computational science. You will build a spectral collocation solver for a convection-diffusion equation, which is known for producing sharp boundary layers that are difficult to resolve with standard numerical methods [@problem_id:3446182]. The exercise powerfully demonstrates the \"spectral accuracy\" concept by showing how an intelligent, problem-adapted choice of basis—clustering nodes in the boundary layer using Jacobi polynomials—can yield dramatically better accuracy than a generic basis, illustrating a core principle of high-performance scientific computing.", "problem": "Consider the one-dimensional boundary value problem for a convection–diffusion operator on the interval $[0,1]$,\n$$\n-\\varepsilon\\,u_{xx}(x) + b\\,u_{x}(x) = 0,\\quad x\\in(0,1),\n$$\nwith Dirichlet boundary conditions $u(0)=0$ and $u(1)=1$, where $\\varepsilon>0$ is the diffusion coefficient and $b\\in\\mathbb{R}$ is the convection coefficient. When $0<\\varepsilon\\ll 1$ and $b \\neq 0$, the solution develops an exponentially thin boundary layer on the inflow boundary: for $b>0$ a layer at $x=1$; for $b<0$ a layer at $x=0$.\n\nThe Spectral Element Method (SEM) in one spatial dimension on a single element reduces to a global high-order spectral discretization, typically implemented with Lagrange polynomials at Gauss–Lobatto nodes. The choice of nodal distribution is crucial for resolving thin layers. One family of node sets is induced by Jacobi polynomials $P_n^{(\\alpha,\\beta)}(\\xi)$ on the reference interval $\\xi\\in[-1,1]$, with parameters $\\alpha>-1$ and $\\beta>-1$, and the associated Gauss–Lobatto–Jacobi nodes. By tuning $(\\alpha,\\beta)$, one can cluster nodes near $\\xi=-1$ or $\\xi=+1$, and thus near $x=0$ or $x=1$ after the affine mapping $x=(\\xi+1)/2$.\n\nYour task is to write a complete program that:\n- Implements a spectral collocation SEM on a single element using the Lagrange basis at Gauss–Lobatto–Jacobi nodes for polynomial degree $p$, with parameters $(\\alpha,\\beta)$.\n- Uses the barycentric Lagrange differentiation matrix to approximate $u_x$ and $u_{xx}$ at the collocation nodes.\n- Enforces the Dirichlet boundary conditions strongly at the endpoints and satisfies the differential equation at interior nodes.\n- Computes the numerical solution and its error against the exact solution in the supremum norm on the collocation nodes, for multiple values of $p$.\n\nFundamental base to use:\n- The differential equation and boundary conditions as stated.\n- The affine mapping $x=(\\xi+1)/2$ from the reference interval to the physical interval and the chain rule $u_x=2u_\\xi$ and $u_{xx}=4u_{\\xi\\xi}$.\n- The definition of Jacobi polynomials $P_n^{(\\alpha,\\beta)}(\\xi)$ and the fact that Gauss–Lobatto–Jacobi interior nodes are the zeros of $P_{p-1}^{(\\alpha+1,\\beta+1)}(\\xi)$, with endpoints included.\n- The barycentric Lagrange interpolation and its differentiation matrix constructed from the nodes and barycentric weights.\n\nExact solution for verification:\n- The exact solution for $b \\neq 0$ is\n$$\nu(x)=\\frac{e^{(b/\\varepsilon)x}-1}{e^{b/\\varepsilon}-1}.\n$$\n\nYou must study the impact of the Jacobi parameters on boundary layer resolution by comparing a baseline choice $(\\alpha,\\beta)=(0,0)$, corresponding to Legendre Gauss–Lobatto nodes, against a tuned choice that clusters nodes at the boundary where the layer resides:\n- If $b>0$ (layer at $x=1$), use a tuned choice $(\\alpha,\\beta)=(0,3)$.\n- If $b<0$ (layer at $x=0$), use a tuned choice $(\\alpha,\\beta)=(3,0)$.\n\nFor each test case, compute the maximum absolute error at the collocation nodes for a list of polynomial degrees $p$. Report, for each test case, a single float equal to the ratio of the baseline error to the tuned error at the largest degree in the list. A ratio greater than $1$ indicates improvement by tuning.\n\nTest Suite:\n- Case 1 (happy path, moderate layer): $\\varepsilon=0.1$, $b=+1$, degrees $p\\in\\{4,8,12,16\\}$.\n- Case 2 (thin layer, challenging resolution at right boundary): $\\varepsilon=0.01$, $b=+1$, degrees $p\\in\\{8,16,24,32\\}$.\n- Case 3 (thin layer, challenging resolution at left boundary): $\\varepsilon=0.01$, $b=-1$, degrees $p\\in\\{8,16,24,32\\}$.\n\nOutput specification:\n- Your program should produce a single line of output containing the three resulting ratios for the test cases, as a comma-separated list enclosed in square brackets, for example, \"[ratio_case1,ratio_case2,ratio_case3]\".\n- All numbers must be printed as plain decimals. No units are involved.\n- No user input is required; all parameters are fixed as above and embedded in the program.\n\nScientific realism and constraints:\n- Ensure $\\alpha>-1$ and $\\beta>-1$ for all Jacobi parameters used.\n- The algorithm must be self-contained and numerically consistent for the specified parameter ranges.\n- Do not use any external data files or networks.", "solution": "The problem describes how to solve a one-dimensional steady convection-diffusion equation using a spectral collocation method and analyze the results.\n\n**1. Mathematical Formulation and Transformation**\n\nThe governing equation is the one-dimensional steady convection-diffusion equation on the physical domain $x \\in [0, 1]$:\n$$\n-\\varepsilon\\,u_{xx}(x) + b\\,u_{x}(x) = 0\n$$\nsubject to Dirichlet boundary conditions $u(0)=0$ and $u(1)=1$. Here, $\\varepsilon > 0$ is the diffusion coefficient and $b \\in \\mathbb{R}$ is the convection speed.\n\nThe spectral method is most conveniently formulated on a canonical reference interval $\\xi \\in [-1, 1]$. We use the affine mapping $x(\\xi) = (\\xi+1)/2$ to transform the physical domain to the reference domain. The derivatives transform according to the chain rule:\n$$\n\\frac{d}{dx} = \\frac{d\\xi}{dx} \\frac{d}{d\\xi} = 2 \\frac{d}{d\\xi}\n$$\n$$\n\\frac{d^2}{dx^2} = \\frac{d}{dx}\\left(2 \\frac{d}{d\\xi}\\right) = 2 \\frac{d\\xi}{dx} \\frac{d}{d\\xi}\\left(\\frac{d}{d\\xi}\\right) = 4 \\frac{d^2}{d\\xi^2}\n$$\nSubstituting these into the original equation yields the transformed PDE on the reference interval:\n$$\n-4\\varepsilon\\,u_{\\xi\\xi}(\\xi) + 2b\\,u_{\\xi}(\\xi) = 0, \\quad \\xi \\in (-1, 1)\n$$\nThe boundary conditions are transformed as $u(\\xi=-1) = u(x=0) = 0$ and $u(\\xi=1) = u(x=1) = 1$.\n\n**2. Discretization and Collocation Nodes**\n\nThe spectral collocation method seeks an approximate solution $U(\\xi)$ as a polynomial of degree $p$, which is represented by its values at a set of $p+1$ distinct collocation nodes $\\{\\xi_j\\}_{j=0}^p$. The choice of these nodes is crucial for the accuracy and stability of the method. We use the Gauss-Lobatto-Jacobi (GLJ) nodes, which include the endpoints $\\xi_0 = -1$ and $\\xi_p = 1$. The $p-1$ interior nodes are the zeros of the derivative of the Jacobi polynomial $P_p^{(\\alpha,\\beta)}(\\xi)$, which are equivalent to the zeros of $P_{p-1}^{(\\alpha+1,\\beta+1)}(\\xi)$. The parameters $\\alpha, \\beta > -1$ control the distribution of nodes. The choice $(\\alpha,\\beta) = (0,0)$ gives the standard Gauss-Lobatto-Legendre nodes, which are roughly evenly spaced in a certain sense. By choosing $\\alpha > \\beta$, nodes cluster near $\\xi = -1$ (i.e., $x=0$), and by choosing $\\beta > \\alpha$, nodes cluster near $\\xi = 1$ (i.e., $x=1$). This property is exploited to resolve sharp boundary layers present in the solution when $\\varepsilon \\ll |b|$.\n\n**3. Derivative Approximation via Barycentric Differentiation Matrix**\n\nThe derivatives of the solution polynomial $U(\\xi)$ at the collocation nodes are computed via matrix-vector products, $U' = DU$ and $U'' = D^2U$, where $D$ is the $(p+1) \\times (p+1)$ spectral differentiation matrix. We construct $D$ using the barycentric Lagrange interpolation formula, which is robust for any set of distinct nodes. The entries of the differentiation matrix $D$ are given by:\n$$\nD_{ij} = \\begin{cases}\n\\frac{w_j/w_i}{\\xi_i - \\xi_j} & \\text{if } i \\neq j \\\\\n-\\sum_{k \\neq i} D_{ik} & \\text{if } i = j\n\\end{cases}\n$$\nwhere the barycentric weights $w_j$ are defined as:\n$$\nw_j = \\frac{1}{\\prod_{k=0, k \\neq j}^p (\\xi_j - \\xi_k)}\n$$\nThe diagonal entries are computed this way to enforce the property that the derivative of a constant is zero, which enhances numerical stability.\n\n**4. Assembly and Solution of the Linear System**\n\nWe enforce the transformed differential equation at the $p-1$ interior collocation nodes $(\\xi_1, \\dots, \\xi_{p-1})$. This yields a system of linear equations. Let $U$ be the vector of unknown solution values $[U_0, U_1, \\dots, U_p]^T$ at the nodes. The discretized system is:\n$$\n(-4\\varepsilon D^2 + 2b D) U = \\mathbf{0}\n$$\nThe boundary conditions are enforced strongly by setting $U_0 = 0$ and $U_p = 1$. The system of equations for the $p-1$ interior unknowns $\\tilde{U} = [U_1, \\dots, U_{p-1}]^T$ can be written as:\n$$\n\\sum_{k=1}^{p-1} L_{jk} U_k = -L_{j0}U_0 - L_{jp}U_p, \\quad \\text{for } j=1, \\dots, p-1\n$$\nwhere $L = -4\\varepsilon D^2 + 2b D$. Substituting the known boundary values:\n$$\n\\sum_{k=1}^{p-1} L_{jk} U_k = -L_{jp}, \\quad \\text{for } j=1, \\dots, p-1\n$$\nThis $(p-1) \\times (p-1)$ linear system is then solved for the interior nodal values $\\tilde{U}$, for instance, using LU decomposition.\n\n**5. Error Analysis and Parameter Tuning**\n\nOnce the full vector of numerical solution values $U$ is assembled, the error is measured against the exact analytical solution. The exact solution to the BVP is:\n$$\nu(x) = \\frac{e^{(b/\\varepsilon)x} - 1}{e^{b/\\varepsilon} - 1}\n$$\nThe error is computed in the maximum norm over the set of collocation nodes:\n$$\nE = \\max_{j=0, \\dots, p} |U_j - u(x_j)|\n$$\nTo demonstrate the effectiveness of node clustering, we compare the error from the standard Legendre nodes $(\\alpha, \\beta) = (0,0)$ with that from a tuned set of Jacobi nodes. For a boundary layer at $x=1$ ($b>0$), we use $(\\alpha,\\beta) = (0,3)$ to cluster nodes near $\\xi=1$. For a layer at $x=0$ ($b<0$), we use $(\\alpha,\\beta) = (3,0)$ to cluster nodes near $\\xi=-1$. The ratio of the error from the baseline case to the error from the tuned case quantifies the improvement afforded by this problem-specific tuning of the spectral method.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import roots_jacobi\n\ndef barycentric_diff_matrix(nodes):\n    \"\"\"\n    Computes the barycentric differentiation matrix for a given set of nodes.\n    \n    Args:\n        nodes (np.ndarray): A 1D array of collocation nodes.\n        \n    Returns:\n        np.ndarray: The differentiation matrix.\n    \"\"\"\n    p = len(nodes) - 1\n    dtype = np.float64\n    nodes = np.array(nodes, dtype=dtype)\n    weights = np.ones(p + 1, dtype=dtype)\n\n    # Calculate barycentric weights using the direct product formula.\n    for j in range(p + 1):\n        prod = 1.0\n        for k in range(p + 1):\n            if k != j:\n                prod *= (nodes[j] - nodes[k])\n        if prod == 0:\n            raise ValueError(\"Product in weight calculation is zero, likely due to duplicate nodes.\")\n        weights[j] = 1.0 / prod\n    \n    D = np.zeros((p + 1, p + 1), dtype=dtype)\n    \n    # Off-diagonal entries\n    for i in range(p + 1):\n        for j in range(p + 1):\n            if i != j:\n                D[i, j] = (weights[j] / weights[i]) / (nodes[i] - nodes[j])\n\n    # Diagonal entries are computed to ensure the sum of each row is zero.\n    for i in range(p + 1):\n        D[i, i] = -np.sum(D[i, :])\n        \n    return D\n\ndef solve_bvp(p, epsilon, b, alpha, beta):\n    \"\"\"\n    Solves the 1D convection-diffusion BVP using spectral collocation.\n    \n    Args:\n        p (int): Polynomial degree.\n        epsilon (float): Diffusion coefficient.\n        b (float): Convection coefficient.\n        alpha (float): Jacobi polynomial parameter.\n        beta (float): Jacobi polynomial parameter.\n        \n    Returns:\n        float: Maximum absolute error at collocation nodes.\n    \"\"\"\n    # 1. Compute Gauss-Lobatto-Jacobi nodes on the reference interval [-1, 1].\n    # Interior nodes are the roots of P_{p-1}^{alpha+1, beta+1}.\n    if p > 1:\n        interior_nodes, _ = roots_jacobi(p - 1, alpha + 1, beta + 1)\n    else: # For p=1, there are no interior nodes.\n        interior_nodes = np.array([])\n    \n    # The full set of nodes includes the endpoints.\n    xi_nodes = np.concatenate(([-1.0], np.sort(interior_nodes), [1.0]))\n    \n    # Map nodes to the physical domain [0, 1].\n    x_nodes = (xi_nodes + 1.0) / 2.0\n\n    # 2. Compute differentiation matrices on the reference interval.\n    D_xi = barycentric_diff_matrix(xi_nodes)\n    D2_xi = np.dot(D_xi, D_xi)\n\n    # 3. Assemble and solve the linear system for interior nodes.\n    # The ODE in xi is: -4*epsilon*u_xixi + 2*b*u_xi = 0.\n    L = -4.0 * epsilon * D2_xi + 2.0 * b * D_xi\n\n    # Enforce Dirichlet BCs u(0)=0 (at xi=-1) and u(1)=1 (at xi=1) strongly.\n    # We solve for the p-1 unknown values at the interior nodes.\n    if p > 1:\n        interior_indices = np.arange(1, p)\n        # Construct the (p-1)x(p-1) submatrix for the interior.\n        L_interior = L[np.ix_(interior_indices, interior_indices)]\n        \n        # The right-hand side comes from the known boundary values U_0=0 and U_p=1.\n        # sum_{k=1}^{p-1} L_jk U_k = -L_{j,0}*U_0 - L_{j,p}*U_p = -L_{j,p}\n        rhs = -L[interior_indices, p]\n        \n        U_interior = np.linalg.solve(L_interior, rhs)\n    else:\n        U_interior = np.array([])\n        \n    # Assemble the full solution vector.\n    U = np.concatenate(([0.0], U_interior, [1.0]))\n\n    # 4. Compute error against the exact solution.\n    # The exact solution is u(x) = (exp(b*x/eps) - 1) / (exp(b/eps) - 1).\n    term = b / epsilon\n    u_exact_vals = (np.exp(x_nodes * term) - 1.0) / (np.exp(term) - 1.0)\n    \n    error = np.max(np.abs(U - u_exact_vals))\n    \n    return error\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'epsilon': 0.1, 'b': 1, 'p_list': [4, 8, 12, 16]},\n        {'epsilon': 0.01, 'b': 1, 'p_list': [8, 16, 24, 32]},\n        {'epsilon': 0.01, 'b': -1, 'p_list': [8, 16, 24, 32]},\n    ]\n\n    results = []\n    for case in test_cases:\n        epsilon = case['epsilon']\n        b = case['b']\n        p_max = max(case['p_list'])\n\n        # Baseline case: Gauss-Lobatto-Legendre nodes\n        alpha_base, beta_base = 0.0, 0.0\n        error_base = solve_bvp(p_max, epsilon, b, alpha_base, beta_base)\n\n        # Tuned case: Gauss-Lobatto-Jacobi nodes clustered at the boundary layer\n        if b > 0: # Layer at x=1 (xi=1), so increase beta\n            alpha_tuned, beta_tuned = 0.0, 3.0\n        else: # b  0, layer at x=0 (xi=-1), so increase alpha\n            alpha_tuned, beta_tuned = 3.0, 0.0\n        \n        error_tuned = solve_bvp(p_max, epsilon, b, alpha_tuned, beta_tuned)\n\n        # Compute the ratio of errors; a ratio > 1 indicates improvement.\n        if error_tuned > 0:\n            ratio = error_base / error_tuned\n        else:\n            # Handle case where tuned error is zero (or near-zero) to avoid division by zero\n            # If base error is also zero, ratio is 1. Otherwise, improvement is effectively infinite.\n            ratio = 1.0 if error_base == 0 else np.inf\n        results.append(ratio)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3446182"}]}