{"hands_on_practices": [{"introduction": "Gradient-based optimization algorithms iteratively refine a solution by choosing a descent direction and a step size. While an exact line search provides a theoretically optimal step size, it is often computationally prohibitive. This exercise introduces the strong Wolfe conditions, a set of practical and robust criteria that ensure both sufficient decrease in the objective function and adequate progress along the search direction, forming the backbone of modern quasi-Newton and conjugate gradient methods. Mastering these conditions is essential for understanding how real-world optimization solvers guarantee stable and efficient convergence [@problem_id:3471643].", "problem": "In atomistic energy minimization for crystalline solids, small displacements around a mechanically stable configuration can be approximated by a quadratic energy in the harmonic regime. Consider the harmonic elastic energy model $$f(x)=\\frac{1}{2}x^{\\top}Ax,$$ where $x\\in\\mathbb{R}^{2}$ represents a displacement vector of two coupled atomic degrees of freedom and $A\\in\\mathbb{R}^{2\\times 2}$ is a symmetric positive definite stiffness matrix given by $$A=\\begin{pmatrix}4 & 1 \\\\ 1 & 2\\end{pmatrix}.$$ We wish to perform one steepest descent step from the initial configuration $$x_{0}=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$$ along the steepest descent direction $$p=-\\nabla f(x_{0}).$$ In numerical optimization for energy minimization, the line search step size $\\alpha>0$ is typically chosen to satisfy the strong Wolfe conditions, which for a continuously differentiable objective $f$ and search direction $p$ are defined in terms of the univariate function $$\\phi(\\alpha)=f(x_{0}+\\alpha p)$$ as\n- the sufficient decrease condition: $$\\phi(\\alpha)\\leq \\phi(0)+c_{1}\\,\\alpha\\,\\phi'(0),$$\n- the strong curvature condition: $$|\\phi'(\\alpha)|\\leq c_{2}\\,|\\phi'(0)|,$$\nwith constants $c_{1}$ and $c_{2}$ satisfying $0<c_{1}<c_{2}<1$. Use $c_{1}=10^{-4}$ and $c_{2}=0.9$.\n\nStarting from the fundamental definitions of the gradient and the strong Wolfe conditions, derive and compute a step size $\\alpha$ that satisfies these strong Wolfe conditions for the given $f$, $A$, $x_{0}$, and $p$. Express your final answer as an exact rational number. No rounding is required. The step size is dimensionless.", "solution": "The problem is to find a step size $\\alpha > 0$ that satisfies the strong Wolfe conditions for a steepest descent step from an initial point $x_0$ for a quadratic objective function $f(x)$.\n\nFirst, we validate the problem statement.\nThe givens are:\n- The objective function is $f(x) = \\frac{1}{2}x^{\\top}Ax$, where $x \\in \\mathbb{R}^2$.\n- The stiffness matrix is $A = \\begin{pmatrix} 4 & 1 \\\\ 1 & 2 \\end{pmatrix}$. This matrix is symmetric. Its eigenvalues are $\\lambda_{1,2} = 3 \\pm \\sqrt{2}$, which are both positive, so $A$ is positive definite. This guarantees that $f(x)$ is a strictly convex function with a unique minimum at $x=0$.\n- The initial configuration is $x_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n- The search direction is the steepest descent direction, $p = -\\nabla f(x_0)$.\n- The strong Wolfe conditions for the univariate function $\\phi(\\alpha) = f(x_0 + \\alpha p)$ are:\n  1. $\\phi(\\alpha) \\leq \\phi(0) + c_1 \\alpha \\phi'(0)$ (Sufficient Decrease)\n  2. $|\\phi'(\\alpha)| \\leq c_2 |\\phi'(0)|$ (Strong Curvature)\n- The constants are $c_1 = 10^{-4}$ and $c_2 = 0.9$, which satisfy the requirement $0 < c_1 < c_2 < 1$.\nThe problem is scientifically grounded in numerical optimization and computational physics, is well-posed, objective, and contains all necessary information. It is therefore valid.\n\nWe proceed to the solution.\n\nThe gradient of the objective function $f(x) = \\frac{1}{2}x^{\\top}Ax$ is given by $\\nabla f(x) = Ax$.\nWe compute the gradient at the initial point $x_0$:\n$$\n\\nabla f(x_0) = Ax_0 = \\begin{pmatrix} 4 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 4(1) + 1(0) \\\\ 1(1) + 2(0) \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix}.\n$$\nThe steepest descent direction $p$ is the negative of the gradient at $x_0$:\n$$\np = -\\nabla f(x_0) = -\\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -4 \\\\ -1 \\end{pmatrix}.\n$$\nNext, we define the univariate function $\\phi(\\alpha) = f(x_0 + \\alpha p)$. Its derivative with respect to $\\alpha$ is given by the chain rule:\n$$\n\\phi'(\\alpha) = \\nabla f(x_0 + \\alpha p)^{\\top} p.\n$$\nFor our quadratic function, $\\nabla f(x_0 + \\alpha p) = A(x_0 + \\alpha p) = Ax_0 + \\alpha Ap$.\nThus, the derivative is:\n$$\n\\phi'(\\alpha) = (Ax_0 + \\alpha Ap)^{\\top} p = (Ax_0)^{\\top}p + \\alpha (Ap)^{\\top}p = (Ax_0)^{\\top}p + \\alpha p^{\\top}Ap,\n$$\nwhere we used the fact that for a scalar result $u^{\\top}v$, $(Av)^{\\top}u = v^{\\top}A^{\\top}u$.\nLet's evaluate the terms. At $\\alpha=0$, we have:\n$$\n\\phi'(0) = (Ax_0)^{\\top}p = \\begin{pmatrix} 4 & 1 \\end{pmatrix} \\begin{pmatrix} -4 \\\\ -1 \\end{pmatrix} = 4(-4) + 1(-1) = -17.\n$$\nNow we compute the term $p^{\\top}Ap$:\nFirst, $Ap = \\begin{pmatrix} 4 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} -4 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 4(-4) + 1(-1) \\\\ 1(-4) + 2(-1) \\end{pmatrix} = \\begin{pmatrix} -17 \\\\ -6 \\end{pmatrix}$.\nThen, $p^{\\top}Ap = \\begin{pmatrix} -4 & -1 \\end{pmatrix} \\begin{pmatrix} -17 \\\\ -6 \\end{pmatrix} = (-4)(-17) + (-1)(-6) = 68 + 6 = 74$.\nSo, the derivative $\\phi'(\\alpha)$ is a linear function of $\\alpha$:\n$$\n\\phi'(\\alpha) = -17 + 74\\alpha.\n$$\nThe function $\\phi(\\alpha)$ is quadratic in $\\alpha$. We can find the step size $\\alpha^*$ that exactly minimizes the energy along the direction $p$ by setting $\\phi'(\\alpha^*) = 0$:\n$$\n-17 + 74\\alpha^* = 0 \\implies \\alpha^* = \\frac{17}{74}.\n$$\nThis exact minimizer is a natural candidate for a step size that satisfies the Wolfe conditions. We must verify this.\n\nWe check the two strong Wolfe conditions for $\\alpha = \\alpha^* = \\frac{17}{74}$.\n\n1.  Sufficient Decrease Condition: $\\phi(\\alpha) \\leq \\phi(0) + c_1 \\alpha \\phi'(0)$\n    For a quadratic objective, the function $\\phi(\\alpha)$ can be written as a Taylor expansion around $\\alpha=0$: $\\phi(\\alpha) = \\phi(0) + \\alpha\\phi'(0) + \\frac{1}{2}\\alpha^2\\phi''(0)$. The exact minimizer is $\\alpha^* = -\\phi'(0)/\\phi''(0)$.\n    Substituting $\\alpha^*$ into the expansion gives $\\phi(\\alpha^*) = \\phi(0) + \\alpha^*\\phi'(0) + \\frac{1}{2}(\\alpha^*)^2\\phi''(0) = \\phi(0) + \\alpha^*\\phi'(0) - \\frac{1}{2}\\alpha^*\\phi'(0) = \\phi(0) + \\frac{1}{2}\\alpha^*\\phi'(0)$.\n    The sufficient decrease condition for $\\alpha^*$ becomes:\n    $$\n    \\phi(0) + \\frac{1}{2}\\alpha^*\\phi'(0) \\leq \\phi(0) + c_1 \\alpha^* \\phi'(0).\n    $$\n    Subtracting $\\phi(0)$ from both sides gives $\\frac{1}{2}\\alpha^*\\phi'(0) \\leq c_1 \\alpha^* \\phi'(0)$.\n    Since we are taking a descent step, $\\phi'(0) = (Ax_0)^{\\top}(-Ax_0) = -\\|Ax_0\\|^2 = -17 < 0$. Also, $\\alpha^* = \\frac{17}{74} > 0$.\n    Dividing by the negative quantity $\\alpha^*\\phi'(0)$ reverses the inequality sign:\n    $$\n    \\frac{1}{2} \\geq c_1.\n    $$\n    With $c_1=10^{-4}$, we have $0.5 \\geq 0.0001$, which is true. Thus, the sufficient decrease condition is satisfied.\n\n2.  Strong Curvature Condition: $|\\phi'(\\alpha)| \\leq c_2 |\\phi'(0)|$\n    For the exact minimizer $\\alpha^* = \\frac{17}{74}$, the derivative is zero by definition:\n    $$\n    \\phi'(\\alpha^*) = -17 + 74\\left(\\frac{17}{74}\\right) = -17 + 17 = 0.\n    $$\n    The condition becomes:\n    $$\n    |0| \\leq c_2 |\\phi'(0)|.\n    $$\n    We have $c_2 = 0.9 > 0$ and $|\\phi'(0)| = |-17| = 17 > 0$. So, $0 \\leq 0.9 \\times 17 = 15.3$. This inequality is true.\n\nBoth strong Wolfe conditions are satisfied for the exact line search step size $\\alpha^* = \\frac{17}{74}$. This value is an exact rational number as requested.", "answer": "$$\n\\boxed{\\frac{17}{74}}\n$$", "id": "3471643"}, {"introduction": "Many real-world problems in materials design involve optimizing an objective, such as energy or cost, subject to physical constraints like mass conservation, phase stability, or geometric limitations. This moves us from unconstrained to constrained optimization, a far richer and more applicable domain. This practice introduces the Karush-Kuhn-Tucker (KKT) conditions, which generalize the method of Lagrange multipliers to problems with both equality and inequality constraints, providing a necessary and, for convex problems, sufficient set of conditions for optimality [@problem_id:3471693].", "problem": "In computational materials design, consider a binary alloy screening step where a composition vector $x \\in \\mathbb{R}^{2}$ collects two normalized, dimensionless design variables (e.g., regularized phase fractions or defect occupation variables) subject to mass conservation and non-negativity of one component. A convex surrogate of the stored elastic energy is modeled by the quadratic functional $f(x) = \\tfrac{1}{2}\\|x\\|_{2}^{2}$, where $\\|x\\|_{2}^{2} = x_{1}^{2} + x_{2}^{2}$. The feasible set is defined by the equality constraint $h(x) = x_{1} + x_{2} - 1 = 0$ and the inequality constraint $g(x) = -x_{1} \\le 0$.\n\nStarting from the fundamental definitions of convex optimization and the Karush-Kuhn-Tucker (KKT) conditions, derive the necessary and sufficient optimality conditions for this problem. Explicitly construct the Lagrangian and the KKT system (stationarity, primal feasibility, dual feasibility, and complementary slackness), justify the constraint qualification, and solve the system to obtain the optimizer $x^{\\star}$ and the associated Lagrange multipliers $\\lambda^{\\star}$ for the equality constraint and $\\mu^{\\star}$ for the inequality constraint.\n\nExpress your final answer as a single row matrix containing $x_{1}^{\\star}$, $x_{2}^{\\star}$, $\\lambda^{\\star}$, and $\\mu^{\\star}$, in that order, using exact values. No rounding is required and all quantities are dimensionless.", "solution": "The problem is to find the minimum of a convex function subject to linear equality and inequality constraints. This is a canonical problem in convex optimization. The solution is found by applying the Karush-Kuhn-Tucker (KKT) conditions.\n\nFirst, we formally state the optimization problem.\nLet $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathbb{R}^2$. The problem is:\nMinimize $f(x) = \\frac{1}{2}(x_1^2 + x_2^2)$\nsubject to:\n$h(x) = x_1 + x_2 - 1 = 0$\n$g(x) = -x_1 \\le 0$\n\nThis is a convex optimization problem. The objective function $f(x)$ is strictly convex, as its Hessian matrix, $\\nabla^2 f(x) = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$, is the identity matrix, which is positive definite for all $x$. The equality constraint function $h(x)$ is affine, and the inequality constraint function $g(x)$ is also affine (and therefore convex). The feasible set is thus convex. Since we are minimizing a strictly convex function over a non-empty, closed convex set, a unique global minimum exists.\n\nTo find this minimum, we use the Karush-Kuhn-Tucker (KKT) conditions. The applicability of the KKT conditions as necessary conditions for optimality is guaranteed by a constraint qualification. The constraints are affine, so for instance, the Linear Independence Constraint Qualification (LICQ) can be checked. The gradients of the constraint functions are:\n$\\nabla h(x) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\n$\\nabla g(x) = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$\nThese two vectors are linearly independent. Thus, LICQ holds at all feasible points, which guarantees that the KKT conditions are necessary for a local minimum. Because the problem is convex, these conditions are also sufficient for a global minimum.\n\nWe construct the Lagrangian function, $\\mathcal{L}(x, \\lambda, \\mu)$, where $\\lambda$ is the Lagrange multiplier for the equality constraint $h(x)=0$ and $\\mu$ is the Lagrange multiplier for the inequality constraint $g(x) \\le 0$.\n$$ \\mathcal{L}(x_1, x_2, \\lambda, \\mu) = f(x) + \\lambda h(x) + \\mu g(x) $$\n$$ \\mathcal{L}(x_1, x_2, \\lambda, \\mu) = \\frac{1}{2}(x_1^2 + x_2^2) + \\lambda(x_1 + x_2 - 1) + \\mu(-x_1) $$\n\nThe KKT conditions are:\n1.  **Stationarity**: $\\nabla_x \\mathcal{L}(x, \\lambda, \\mu) = 0$.\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial x_1} = x_1 + \\lambda - \\mu = 0 $$\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial x_2} = x_2 + \\lambda = 0 $$\n\n2.  **Primal Feasibility**: The original constraints must hold.\n    $$ x_1 + x_2 - 1 = 0 $$\n    $$ -x_1 \\le 0 \\quad (\\text{or } x_1 \\ge 0) $$\n\n3.  **Dual Feasibility**: The multiplier for the inequality constraint must be non-negative.\n    $$ \\mu \\ge 0 $$\n\n4.  **Complementary Slackness**: The product of the inequality constraint and its multiplier must be zero.\n    $$ \\mu(-x_1) = 0 \\quad (\\text{or } \\mu x_1 = 0) $$\n\nWe now solve this system of equations and inequalities.\nFrom the stationarity conditions, we can express $x_1$ and $x_2$ in terms of the multipliers:\n$$ x_1 = \\mu - \\lambda \\quad (1) $$\n$$ x_2 = -\\lambda \\quad (2) $$\nSubstitute these expressions into the primal feasibility equality constraint:\n$$ (\\mu - \\lambda) + (-\\lambda) - 1 = 0 $$\n$$ \\mu - 2\\lambda - 1 = 0 \\implies \\mu = 1 + 2\\lambda \\quad (3) $$\n\nNext, we use the complementary slackness condition, $\\mu x_1 = 0$, which implies that either $\\mu=0$ or $x_1=0$. We analyze these two cases.\n\n**Case 1: Assume $\\mu = 0$.**\nIf $\\mu=0$, we substitute this into equation $(3)$:\n$$ 0 = 1 + 2\\lambda \\implies \\lambda = -\\frac{1}{2} $$\nNow we can find $x_1$ and $x_2$ using equations $(1)$ and $(2)$:\n$$ x_1 = \\mu - \\lambda = 0 - \\left(-\\frac{1}{2}\\right) = \\frac{1}{2} $$\n$$ x_2 = -\\lambda = -\\left(-\\frac{1}{2}\\right) = \\frac{1}{2} $$\nWe must verify that this solution, $(x_1, x_2, \\lambda, \\mu) = (\\frac{1}{2}, \\frac{1}{2}, -\\frac{1}{2}, 0)$, satisfies all KKT conditions.\n- Stationarity: $x_1+\\lambda-\\mu = \\frac{1}{2} - \\frac{1}{2} - 0 = 0$. $x_2+\\lambda = \\frac{1}{2} - \\frac{1}{2} = 0$. (Satisfied)\n- Primal Feasibility: $x_1+x_2-1 = \\frac{1}{2} + \\frac{1}{2} - 1 = 0$. $x_1 = \\frac{1}{2} \\ge 0$. (Satisfied)\n- Dual Feasibility: $\\mu = 0 \\ge 0$. (Satisfied)\n- Complementary Slackness: $\\mu x_1 = 0 \\cdot \\frac{1}{2} = 0$. (Satisfied)\nAll conditions are satisfied. This is a valid KKT point.\n\n**Case 2: Assume $x_1 = 0$.**\nIf $x_1=0$, we use the primal feasibility equality constraint to find $x_2$:\n$$ 0 + x_2 - 1 = 0 \\implies x_2 = 1 $$\nThis gives the candidate point $x=(0, 1)$. Now we find the corresponding multipliers.\nFrom equation $(2)$:\n$$ x_2 = -\\lambda \\implies 1 = -\\lambda \\implies \\lambda = -1 $$\nFrom equation $(1)$:\n$$ x_1 = \\mu - \\lambda \\implies 0 = \\mu - (-1) \\implies \\mu = -1 $$\nLet's check the KKT conditions for $(x_1, x_2, \\lambda, \\mu) = (0, 1, -1, -1)$. The dual feasibility condition is $\\mu \\ge 0$. However, we found $\\mu = -1$, which violates this condition. Therefore, this case does not lead to a valid KKT point.\n\nThe only solution satisfying all KKT conditions is the one found in Case 1. Since the problem is convex, this point corresponds to the unique global minimum.\nThe optimizer is $x^{\\star} = \\begin{pmatrix} x_1^{\\star} \\\\ x_2^{\\star} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix}$.\nThe associated Lagrange multipliers are $\\lambda^{\\star} = -\\frac{1}{2}$ and $\\mu^{\\star} = 0$.\n\nThe final answer is the ordered set of values for $x_1^{\\star}$, $x_2^{\\star}$, $\\lambda^{\\star}$, and $\\mu^{\\star}$.\n$x_1^{\\star} = \\frac{1}{2}$\n$x_2^{\\star} = \\frac{1}{2}$\n$\\lambda^{\\star} = -\\frac{1}{2}$\n$\\mu^{\\star} = 0$", "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} & -\\frac{1}{2} & 0 \\end{pmatrix}} $$", "id": "3471693"}, {"introduction": "Advanced optimization techniques can offer more than just a numerical answer; they can provide deep physical insight into a system's behavior. This practice explores Lagrangian duality, a powerful framework that recasts a constrained primal problem into an alternative dual problem. Solving the dual problem not only helps find the solution but also reveals the values of the Lagrange multipliers, which are not mere mathematical artifacts but often represent profound physical quantities like generalized stresses or chemical potentials, directly linking the mathematics of optimization to the physics of the material [@problem_id:3471718].", "problem": "Consider a minimal-dissipation identification of slip magnitudes in a Face-Centered Cubic (FCC) single crystal within a crystal plasticity surrogate model. Let the vector of slip magnitudes across three independent crystallographic slip systems be $x \\in \\mathbb{R}^{3}$, and suppose the macroscopic plastic strain increments measured along two independent strain modes are $b \\in \\mathbb{R}^{2}$. A linear kinematic mapping $A \\in \\mathbb{R}^{2 \\times 3}$ relates slip magnitudes to macroscopic strain increments. Under a quadratic dissipation potential, the identification problem is posed as a convex program with equality constraints:\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2} \\|x\\|_{2}^{2} \\quad \\text{subject to} \\quad A x = b.\n$$\nYou are asked to derive and solve the dual problem, verify zero duality gap, and interpret the dual variables physically in the materials context. Proceed as follows.\n\n1. Using first principles of convex analysis and Lagrangian duality, construct the Lagrangian for the primal problem, derive the dual function, and formulate the dual optimization problem for general $A$ and $b$. State and justify any assumptions necessary to ensure existence of optimal solutions and validity of strong duality, using only foundational facts (convexity of quadratic forms, properties of affine constraints, and conditions ensuring feasibility).\n\n2. Specialize to the concrete, scientifically plausible instance\n$$\nA = \\begin{pmatrix}\n2 & -1 & 0 \\\\\n1 & 1 & 1\n\\end{pmatrix}, \\qquad\nb = \\begin{pmatrix}\n1 \\\\\n2\n\\end{pmatrix}.\n$$\nSolve the primal and dual problems explicitly. Compute the primal optimal solution $x^{\\star}$ and the dual optimal variables $y^{\\star} \\in \\mathbb{R}^{2}$, verify the Karush–Kuhn–Tucker (KKT) conditions, and calculate both the primal optimal value $\\frac{1}{2}\\|x^{\\star}\\|_{2}^{2}$ and the dual optimal value. Demonstrate that the duality gap is zero.\n\n3. Interpret the dual variables $y^{\\star}$ physically in the computational materials science setting. Explain what the dual variables represent in terms of generalized stresses or reaction forces that enforce the macroscopic strain constraints $A x = b$, and discuss how the relation $x^{\\star} = -A^{\\top} y^{\\star}$ links these generalized stresses to the identified slip magnitudes.\n\nExpress the final reported quantity as the optimal dissipation value $\\frac{1}{2}\\|x^{\\star}\\|_{2}^{2}$ for the given $A$ and $b$. Treat this value as dimensionless. Express your final answer as an exact fraction; do not round.", "solution": "The user has provided a constrained optimization problem from the field of computational materials science and has requested a three-part solution involving Lagrangian duality, explicit calculation for a specific case, and physical interpretation.\n\n### **Problem Validation**\n\n**Step 1: Extract Givens**\n\n- Primal problem: $\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2} \\|x\\|_{2}^{2}$ subject to $A x = b$.\n- $x \\in \\mathbb{R}^{3}$: Vector of slip magnitudes.\n- $b \\in \\mathbb{R}^{2}$: Vector of macroscopic plastic strain increments.\n- $A \\in \\mathbb{R}^{2 \\times 3}$: Linear kinematic mapping.\n- Objective function to minimize: $\\frac{1}{2} \\|x\\|_{2}^{2}$, representing a quadratic dissipation potential.\n- Equality constraint: $A x = b$.\n- Specific instance for Part 2: $A = \\begin{pmatrix} 2 & -1 & 0 \\\\ 1 & 1 & 1 \\end{pmatrix}$, $b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n- Required tasks:\n  1.  Construct the Lagrangian, derive the dual problem, and justify strong duality.\n  2.  Solve the specific primal and dual problems, verify KKT conditions, and show the duality gap is zero.\n  3.  Provide a physical interpretation of the dual variables.\n- Final output quantity: The optimal dissipation value $\\frac{1}{2}\\|x^{\\star}\\|_{2}^{2}$.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded**: The problem is a well-established formulation in continuum mechanics and materials science, specifically in the context of crystal plasticity. It represents a minimal-work or minimal-dissipation principle for identifying internal state variables (slip magnitudes $x$) that are consistent with macroscopic observations (strain increments $b$). This is a standard approach for creating surrogate or reduced-order models. The formulation is scientifically sound.\n- **Well-Posed**: The objective function, $f(x) = \\frac{1}{2} \\|x\\|_{2}^{2}$, is a strictly convex quadratic function. The constraints, $h(x) = Ax - b = 0$, are affine. A minimization problem with a strictly convex objective function and convex (in this case, affine) constraints has at most one optimal solution. The existence of a solution is guaranteed if the feasible set is non-empty. The constraint matrix $A$ has rank $2$, meaning it is a surjective map from $\\mathbb{R}^{3}$ to $\\mathbb{R}^{2}$. Therefore, for any $b \\in \\mathbb{R}^{2}$, a solution $x$ to $Ax=b$ exists. The problem is thus well-posed, admitting a unique, stable, and meaningful solution.\n- **Objective**: The problem is stated in precise, unambiguous mathematical language.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. It is scientifically grounded, well-posed, and objectively stated. I will proceed with the solution.\n\n---\n\n### **Part 1: Dual Problem Formulation**\n\nThe primal optimization problem is:\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2} \\|x\\|_{2}^{2} \\quad \\text{subject to} \\quad A x = b.\n$$\nThis is a convex optimization problem, as the objective function $f(x) = \\frac{1}{2} x^{\\top}x$ is convex (its Hessian is the identity matrix, which is positive definite) and the equality constraints are affine.\n\nWe introduce a vector of Lagrange multipliers (or dual variables) $y \\in \\mathbb{R}^{2}$ associated with the equality constraints $A x - b = 0$. The Lagrangian function is defined as:\n$$\n\\mathcal{L}(x, y) = \\frac{1}{2} \\|x\\|_{2}^{2} + y^{\\top}(Ax - b)\n$$\nThe dual function, $g(y)$, is the infimum of the Lagrangian over the primal variable $x$:\n$$\ng(y) = \\inf_{x \\in \\mathbb{R}^{3}} \\mathcal{L}(x, y) = \\inf_{x \\in \\mathbb{R}^{3}} \\left( \\frac{1}{2} x^{\\top}x + y^{\\top}Ax - y^{\\top}b \\right)\n$$\nSince $\\mathcal{L}(x, y)$ is a convex and differentiable function of $x$, its infimum is found where its gradient with respect to $x$ is zero:\n$$\n\\nabla_{x} \\mathcal{L}(x, y) = x + A^{\\top}y = 0\n$$\nThis condition gives the unique value of $x$ that minimizes $\\mathcal{L}$ for a fixed $y$:\n$$\nx_{\\text{min}}(y) = -A^{\\top}y\n$$\nSubstituting this expression for $x$ back into the Lagrangian, we obtain the dual function $g(y)$:\n$$\ng(y) = \\frac{1}{2} (-A^{\\top}y)^{\\top}(-A^{\\top}y) + y^{\\top}A(-A^{\\top}y) - y^{\\top}b\n$$\n$$\ng(y) = \\frac{1}{2} y^{\\top}A A^{\\top}y - y^{\\top}A A^{\\top}y - y^{\\top}b\n$$\n$$\ng(y) = -\\frac{1}{2} y^{\\top}A A^{\\top}y - y^{\\top}b\n$$\nThe dual problem is to maximize the dual function $g(y)$ with respect to $y$:\n$$\n\\max_{y \\in \\mathbb{R}^{2}} \\ g(y) \\quad \\Longleftrightarrow \\quad \\max_{y \\in \\mathbb{R}^{2}} \\left( -\\frac{1}{2} y^{\\top}(A A^{\\top})y - b^{\\top}y \\right)\n$$\nThis is an unconstrained quadratic maximization problem in $y$.\n\nFor strong duality to hold (i.e., for the duality gap to be zero), we can invoke Slater's condition. The primal problem is convex. The constraints $Ax=b$ are affine. Slater's condition requires that there exists a strictly feasible point. For affine constraints, this reduces to the requirement that the feasible set is non-empty. As established in the validation, the matrix $A$ is full-rank and maps $\\mathbb{R}^{3}$ onto $\\mathbb{R}^{2}$, so a feasible solution $x$ exists for any $b \\in \\mathbb{R}^{2}$. Therefore, strong duality holds, and the Karush–Kuhn–Tucker (KKT) conditions are necessary and sufficient for optimality.\n\n### **Part 2: Solution for the Specific Instance**\n\nWe are given:\n$$\nA = \\begin{pmatrix}\n2 & -1 & 0 \\\\\n1 & 1 & 1\n\\end{pmatrix}, \\qquad\nb = \\begin{pmatrix}\n1 \\\\\n2\n\\end{pmatrix}\n$$\n\nFirst, we solve the dual problem. We need the matrix $A A^{\\top}$:\n$$\nA A^{\\top} = \\begin{pmatrix}\n2 & -1 & 0 \\\\\n1 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n2 & 1 \\\\\n-1 & 1 \\\\\n0 & 1\n\\end{pmatrix} =\n\\begin{pmatrix}\n(2)(2) + (-1)(-1) + (0)(0) & (2)(1) + (-1)(1) + (0)(1) \\\\\n(1)(2) + (1)(-1) + (1)(0) & (1)(1) + (1)(1) + (1)(1)\n\\end{pmatrix}\n= \\begin{pmatrix}\n5 & 1 \\\\\n1 & 3\n\\end{pmatrix}\n$$\nThe dual problem is to maximize $g(y) = -\\frac{1}{2} y^{\\top} \\begin{pmatrix} 5 & 1 \\\\ 1 & 3 \\end{pmatrix} y - \\begin{pmatrix} 1 & 2 \\end{pmatrix} y$.\nTo find the optimal dual variables $y^{\\star}$, we set the gradient of $g(y)$ to zero:\n$$\n\\nabla_y g(y) = -(A A^{\\top})y - b = 0 \\quad \\implies \\quad (A A^{\\top})y^{\\star} = -b\n$$\n$$\n\\begin{pmatrix}\n5 & 1 \\\\\n1 & 3\n\\end{pmatrix}\n\\begin{pmatrix}\ny_1 \\\\\ny_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-1 \\\\\n-2\n\\end{pmatrix}\n$$\nThis is a system of linear equations:\n1. $5y_1 + y_2 = -1$\n2. $y_1 + 3y_2 = -2$\n\nFrom equation (2), $y_1 = -2 - 3y_2$. Substituting into equation (1):\n$5(-2 - 3y_2) + y_2 = -1$\n$-10 - 15y_2 + y_2 = -1$\n$-14y_2 = 9 \\implies y_2^{\\star} = -\\frac{9}{14}$\nNow, we find $y_1^{\\star}$:\n$y_1^{\\star} = -2 - 3(-\\frac{9}{14}) = -2 + \\frac{27}{14} = \\frac{-28+27}{14} = -\\frac{1}{14}$\nThe optimal dual solution is $y^{\\star} = \\begin{pmatrix} -1/14 \\\\ -9/14 \\end{pmatrix}$.\n\nThe dual optimal value, $d^{\\star}$, is $g(y^{\\star})$. We can use the simplified expression $d^{\\star} = -\\frac{1}{2} (y^{\\star})^{\\top} b$ which arises from substituting $(A A^{\\top})y^{\\star} = -b$ into the expression for $g(y^{\\star})$:\n$d^{\\star} = g(y^{\\star}) = \\frac{1}{2} (y^{\\star})^{\\top} (-b) - (y^{\\star})^{\\top}b = -\\frac{1}{2} (y^{\\star})^{\\top} b$.\n$$\nd^{\\star} = -\\frac{1}{2} \\begin{pmatrix} -\\frac{1}{14} & -\\frac{9}{14} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = -\\frac{1}{2} \\left( -\\frac{1}{14} - \\frac{18}{14} \\right) = -\\frac{1}{2} \\left( -\\frac{19}{14} \\right) = \\frac{19}{28}\n$$\nNow, we find the primal optimal solution $x^{\\star}$ using the stationarity condition $x^{\\star} = -A^{\\top}y^{\\star}$:\n$$\nx^{\\star} = -\\begin{pmatrix}\n2 & 1 \\\\\n-1 & 1 \\\\\n0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n-1/14 \\\\\n-9/14\n\\end{pmatrix}\n= -\\frac{1}{14}\n\\begin{pmatrix}\n(2)(-1) + (1)(-9) \\\\\n(-1)(-1) + (1)(-9) \\\\\n(0)(-1) + (1)(-9)\n\\end{pmatrix}\n= -\\frac{1}{14}\n\\begin{pmatrix}\n-11 \\\\\n-8 \\\\\n-9\n\\end{pmatrix}\n= \\frac{1}{14}\n\\begin{pmatrix}\n11 \\\\\n8 \\\\\n9\n\\end{pmatrix}\n$$\nSo, $x^{\\star} = \\begin{pmatrix} 11/14 \\\\ 8/14 \\\\ 9/14 \\end{pmatrix} = \\begin{pmatrix} 11/14 \\\\ 4/7 \\\\ 9/14 \\end{pmatrix}$.\n\nWe verify the KKT conditions for $(x^{\\star}, y^{\\star})$:\n1.  **Primal Feasibility ($Ax^{\\star} = b$)**:\n    $$\n    A x^{\\star} = \\frac{1}{14} \\begin{pmatrix} 2 & -1 & 0 \\\\ 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 11 \\\\ 8 \\\\ 9 \\end{pmatrix} = \\frac{1}{14} \\begin{pmatrix} 22-8 \\\\ 11+8+9 \\end{pmatrix} = \\frac{1}{14} \\begin{pmatrix} 14 \\\\ 28 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = b.\n    $$\n    The condition is satisfied.\n2.  **Stationarity ($\\nabla_x \\mathcal{L}(x^{\\star}, y^{\\star}) = x^{\\star} + A^{\\top}y^{\\star} = 0$)**: This condition was used to derive $x^{\\star}$, so it is satisfied by construction.\n\nNow we calculate the primal optimal value, $p^{\\star} = \\frac{1}{2}\\|x^{\\star}\\|_{2}^{2}$:\n$$\np^{\\star} = \\frac{1}{2} \\left( \\left( \\frac{11}{14} \\right)^2 + \\left( \\frac{8}{14} \\right)^2 + \\left( \\frac{9}{14} \\right)^2 \\right) = \\frac{1}{2} \\frac{11^2 + 8^2 + 9^2}{14^2} = \\frac{1}{2} \\frac{121 + 64 + 81}{196} = \\frac{1}{2} \\frac{266}{196} = \\frac{133}{196}\n$$\nTo simplify the fraction, we note that $133 = 7 \\times 19$ and $196 = 4 \\times 49 = 4 \\times 7^2$.\n$$\np^{\\star} = \\frac{7 \\times 19}{4 \\times 7^2} = \\frac{19}{4 \\times 7} = \\frac{19}{28}\n$$\nThe primal optimal value is $p^{\\star} = \\frac{19}{28}$.\n\nFinally, we verify the duality gap: $p^{\\star} - d^{\\star} = \\frac{19}{28} - \\frac{19}{28} = 0$. The duality gap is zero, as expected from strong duality.\n\n### **Part 3: Physical Interpretation of Dual Variables**\n\nThe dual variables $y^{\\star}$ are the Lagrange multipliers for the kinematic constraints $Ax=b$. In the context of mechanics and materials science, Lagrange multipliers associated with kinematic constraints are interpreted as generalized forces or stresses.\n\n1.  **$y^{\\star}$ as Generalized Stresses**: The primal problem minimizes a dissipation potential (proportional to total slip squared) while enforcing a macroscopic deformation. The dual variables $y^{\\star}$ represent the \"price\" or \"cost\" of enforcing this deformation. Specifically, they can be interpreted as the components of a generalized macroscopic stress tensor that are energetically conjugate to the macroscopic strain increments $b$. The value of $y_j^{\\star}$ quantifies the marginal increase in the minimum dissipation for a unit increase in the $j$-th strain constraint, $b_j$. They are the \"reaction stresses\" that the material must generate to satisfy the externally imposed strain.\n\n2.  **The Role of $A^{\\top}$**: The matrix $A$ maps microscopic slips $x$ to macroscopic strains $b$. Its transpose, $A^{\\top}$, performs the reverse mapping in the energetic sense: it maps macroscopic generalized stresses (like $y$) to the work-conjugate microscopic \"forces\" on the slip systems. In crystal plasticity, this microscopic force is the resolved shear stress, $\\tau$. Thus, the vector $A^{\\top}y$ represents the set of resolved shear stresses on the three slip systems induced by the macroscopic stress state $y$.\n\n3.  **The Stationarity Condition as a Flow Rule**: The stationarity condition from the KKT analysis, $x^{\\star} + A^{\\top}y^{\\star} = 0$, can be written as $x^{\\star} = -A^{\\top}y^{\\star}$. This equation is a form of a constitutive law, or \"flow rule\". It states that the optimal slips $x^{\\star}$ (the plastic flow) are directly proportional to the resolved shear stresses, $-A^{\\top}y^{\\star}$, that are induced by the macroscopic stress state $y^{\\star}$. The negative sign is a convention arising from the standard Lagrangian formulation; the physical principle is one of proportionality. This means the system achieves minimal dissipation by activating slip systems in direct proportion to the effective \"driving force\" (resolved shear stress) they experience. The vector $y^{\\star}$ is precisely the macroscopic stress state that results in a set of resolved shear stresses that, via this flow rule, produce slips $x^{\\star}$ that in turn satisfy the macroscopic strain constraint $A x^{\\star} = b$.\n\nIn summary, $y^{\\star}$ is the macroscopic stress required to enforce the strain $b$, and the relation $x^{\\star} = -A^{\\top}y^{\\star}$ links this stress to the plastic flow via the concept of resolved shear stress, embodying the principle of minimal dissipation.", "answer": "$$\n\\boxed{\\frac{19}{28}}\n$$", "id": "3471718"}]}