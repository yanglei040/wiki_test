{"hands_on_practices": [{"introduction": "In the foundational linear-Gaussian setting, the posterior distribution is exactly Gaussian, meaning the Laplace approximation is not an approximation at all but an exact representation. This exercise [@problem_id:3395961] grounds the theory in practice by tasking you with computing the key operator in this setting: the Hessian of the negative log-posterior, which is precisely the inverse posterior covariance. By implementing core computational tasks such as Hessian-vector products, linear solves, and log-determinant calculations, you will gain hands-on experience with the linear algebra that underpins nearly all Gaussian-based inference methods.", "problem": "Consider the Bayesian linear inverse problem in which an unknown parameter vector $u \\in \\mathbb{R}^n$ is inferred from noisy observations $y \\in \\mathbb{R}^m$ under a Gaussian prior and Gaussian observational noise. The forward map is linear, so the data model is $y = A u + \\eta$, where $A \\in \\mathbb{R}^{m \\times n}$ is a known matrix and $\\eta \\sim \\mathcal{N}(0, \\Gamma_{\\text{noise}})$ is Gaussian noise with known covariance matrix $\\Gamma_{\\text{noise}}$. The prior distribution is $u \\sim \\mathcal{N}(m_0, \\Gamma_{\\text{prior}})$ with prior mean $m_0 \\in \\mathbb{R}^n$ and prior covariance matrix $\\Gamma_{\\text{prior}}$. In the Laplace approximation to the posterior, computational realizations require efficient evaluation of Hessian actions, linear solves with the Hessian, and log-determinants of the Hessian.\n\nStarting from the fundamental definitions of the Gaussian data likelihood and Gaussian prior, and the resulting negative log-posterior objective, derive the Hessian with respect to $u$ of the negative log-posterior for the linear forward model, and use it to:\n- compute the action of the Hessian on a given vector $v$ (that is, compute $Hv$),\n- solve the linear system $Hz = b$ for a given right-hand side vector $b$,\n- compute the natural logarithm of the determinant of $H$, $\\log \\det(H)$.\n\nAll derivations must begin from the fundamental base: the data likelihood definition for $y$ given $u$, the prior definition for $u$, and the negative log of the posterior density as the sum of the negative log-likelihood and the negative log-prior (up to an irrelevant additive constant). Do not use shortcut results; derive the expressions needed for the Hessian from these definitions.\n\nAssume that $\\Gamma_{\\text{noise}}$ and $\\Gamma_{\\text{prior}}$ are diagonal and positive definite. You may exploit any structural properties that follow from these assumptions in your computational implementation. There are no physical units involved in this problem.\n\nFor each test case below, the program must:\n- construct the Hessian $H$ implied by the model and covariances,\n- compute the Hessian action $Hv$ for the specified $v$,\n- solve $Hz = b$ for the specified $b$,\n- compute $\\log \\det(H)$.\n\nUse the following test suite. Each test case specifies $(m, n)$, the matrix $A$, the diagonal entries of $\\Gamma_{\\text{noise}}$, the diagonal entries of $\\Gamma_{\\text{prior}}$, and the vectors $v$ and $b$.\n\nTest case $1$ (happy path, moderately conditioned):\n- $m = 4$, $n = 3$,\n- $$A = \\begin{bmatrix}\n1.0 & 0.5 & 0.0 \\\\\n0.0 & 1.0 & 0.5 \\\\\n0.5 & 0.0 & 1.0 \\\\\n1.0 & -0.5 & 0.5\n\\end{bmatrix},$$\n- $$\\operatorname{diag}(\\Gamma_{\\text{noise}}) = \\begin{bmatrix} 0.1 \\\\ 0.2 \\\\ 0.15 \\\\ 0.3 \\end{bmatrix}, \\quad \\operatorname{diag}(\\Gamma_{\\text{prior}}) = \\begin{bmatrix} 1.0 \\\\ 0.5 \\\\ 2.0 \\end{bmatrix},$$\n- $$v = \\begin{bmatrix} 1.0 \\\\ -1.0 \\\\ 0.5 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 0.2 \\\\ -0.1 \\\\ 0.3 \\end{bmatrix}.$$\n\nTest case $2$ (weak prior, near-collinearity in columns of $A$):\n- $m = 6$, $n = 5$,\n- $$A = \\begin{bmatrix}\n1.0 & 0.9 & 0.0 & 0.1 & -0.2 \\\\\n0.8 & 0.7 & 0.1 & 0.0 & 0.2 \\\\\n0.6 & 0.59 & -0.1 & 0.1 & 0.0 \\\\\n0.4 & 0.41 & 0.2 & -0.2 & 0.1 \\\\\n0.2 & 0.21 & -0.2 & 0.2 & -0.1 \\\\\n0.1 & 0.09 & 0.0 & -0.1 & 0.0\n\\end{bmatrix},$$\n- $$\\operatorname{diag}(\\Gamma_{\\text{noise}}) = \\begin{bmatrix} 0.05 \\\\ 0.05 \\\\ 0.1 \\\\ 0.2 \\\\ 0.3 \\\\ 0.4 \\end{bmatrix}, \\quad \\operatorname{diag}(\\Gamma_{\\text{prior}}) = \\begin{bmatrix} 10.0 \\\\ 10.0 \\\\ 10.0 \\\\ 10.0 \\\\ 10.0 \\end{bmatrix},$$\n- $$v = \\begin{bmatrix} 0.5 \\\\ 0.0 \\\\ -0.5 \\\\ 1.0 \\\\ -1.0 \\end{bmatrix}, \\quad b = \\begin{bmatrix} -0.1 \\\\ 0.2 \\\\ 0.0 \\\\ 0.3 \\\\ -0.2 \\end{bmatrix}.$$\n\nTest case $3$ (strong prior, ill-conditioned $A$ but stabilized by the prior):\n- $m = 4$, $n = 4$,\n- $$A = \\begin{bmatrix}\n1.0 & 0.99 & 0.98 & 0.97 \\\\\n0.5 & 0.495 & 0.49 & 0.485 \\\\\n0.2 & 0.198 & 0.196 & 0.194 \\\\\n-0.1 & -0.099 & -0.098 & -0.097\n\\end{bmatrix},$$\n- $$\\operatorname{diag}(\\Gamma_{\\text{noise}}) = \\begin{bmatrix} 0.2 \\\\ 0.25 \\\\ 0.3 \\\\ 0.35 \\end{bmatrix}, \\quad \\operatorname{diag}(\\Gamma_{\\text{prior}}) = \\begin{bmatrix} 0.01 \\\\ 0.02 \\\\ 0.015 \\\\ 0.03 \\end{bmatrix},$$\n- $$v = \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 1.0 \\\\ 1.0 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 0.5 \\\\ -0.5 \\end{bmatrix}.$$\n\nTest case $4$ (boundary case, scalar parameter):\n- $m = 2$, $n = 1$,\n- $$A = \\begin{bmatrix}\n2.0 \\\\\n-1.0\n\\end{bmatrix},$$\n- $$\\operatorname{diag}(\\Gamma_{\\text{noise}}) = \\begin{bmatrix} 0.5 \\\\ 0.25 \\end{bmatrix}, \\quad \\operatorname{diag}(\\Gamma_{\\text{prior}}) = \\begin{bmatrix} 0.1 \\end{bmatrix},$$\n- $$v = \\begin{bmatrix} 1.0 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 0.3 \\end{bmatrix}.$$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output the triple $[Hv, z, \\log \\det(H)]$, where $Hv$ and $z$ are lists of floats (the vector entries) and $\\log \\det(H)$ is a float. The final output must be a single list aggregating the four test case triples in order, for example:\n$$[ [Hv_1, z_1, \\log\\det(H_1)], [Hv_2, z_2, \\log\\det(H_2)], [Hv_3, z_3, \\log\\det(H_3)], [Hv_4, z_4, \\log\\det(H_4)] ].$$", "solution": "The problem requires the derivation of the Hessian of the negative log-posterior for a Bayesian linear inverse problem with Gaussian prior and noise, followed by its application to specific computational tasks. The derivation must proceed from fundamental principles.\n\nLet the unknown parameter vector be $u \\in \\mathbb{R}^n$ and the observation vector be $y \\in \\mathbb{R}^m$. The relationship between them is given by the linear forward model:\n$$\ny = A u + \\eta\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ is the forward operator and $\\eta \\in \\mathbb{R}^m$ is the observation noise.\n\nThe problem specifies a Gaussian model for both the prior on $u$ and the noise $\\eta$.\nThe prior distribution for $u$ is given as $u \\sim \\mathcal{N}(m_0, \\Gamma_{\\text{prior}})$, where $m_0 \\in \\mathbb{R}^n$ is the prior mean and $\\Gamma_{\\text{prior}} \\in \\mathbb{R}^{n \\times n}$ is the prior covariance matrix. The probability density function (PDF) for the prior is:\n$$\np(u) \\propto \\exp\\left(-\\frac{1}{2} (u - m_0)^T \\Gamma_{\\text{prior}}^{-1} (u - m_0)\\right)\n$$\nThe observational noise is $\\eta \\sim \\mathcal{N}(0, \\Gamma_{\\text{noise}})$, where $\\Gamma_{\\text{noise}} \\in \\mathbb{R}^{m \\times m}$ is the noise covariance matrix. This implies that for a given $u$, the distribution of the observation $y$ is $y|u \\sim \\mathcal{N}(Au, \\Gamma_{\\text{noise}})$. The corresponding data likelihood function is:\n$$\np(y|u) \\propto \\exp\\left(-\\frac{1}{2} (y - Au)^T \\Gamma_{\\text{noise}}^{-1} (y - Au)\\right)\n$$\nAccording to Bayes' theorem, the posterior distribution of $u$ given the data $y$ is proportional to the product of the likelihood and the prior:\n$$\np(u|y) \\propto p(y|u) p(u)\n$$\nSubstituting the expressions for the Gaussian likelihood and prior, we get:\n$$\np(u|y) \\propto \\exp\\left(-\\frac{1}{2} (y - Au)^T \\Gamma_{\\text{noise}}^{-1} (y - Au)\\right) \\exp\\left(-\\frac{1}{2} (u - m_0)^T \\Gamma_{\\text{prior}}^{-1} (u - m_0)\\right)\n$$\n$$\np(u|y) \\propto \\exp\\left(-\\frac{1}{2} \\left[ (y - Au)^T \\Gamma_{\\text{noise}}^{-1} (y - Au) + (u - m_0)^T \\Gamma_{\\text{prior}}^{-1} (u - m_0) \\right]\\right)\n$$\nThe Laplace approximation involves approximating the posterior distribution with a Gaussian centered at the posterior mode. The mode is found by maximizing $p(u|y)$, which is equivalent to minimizing the negative logarithm of the posterior density. Let $\\Phi(u)$ denote the negative log-posterior, up to an additive constant:\n$$\n\\Phi(u) = -\\log p(u|y) = \\frac{1}{2} (y - Au)^T \\Gamma_{\\text{noise}}^{-1} (y - Au) + \\frac{1}{2} (u - m_0)^T \\Gamma_{\\text{prior}}^{-1} (u - m_0) + C\n$$\nwhere $C$ is a constant that does not depend on $u$.\n\nThe Hessian of the negative log-posterior, $H$, is the matrix of second partial derivatives of $\\Phi(u)$ with respect to the components of $u$, i.e., $H = \\nabla_u^2 \\Phi(u)$. To find $H$, we first compute the gradient, $\\nabla_u \\Phi(u)$. Let's expand the terms in $\\Phi(u)$:\n$$\n\\Phi(u) = \\frac{1}{2} (y^T - u^T A^T) \\Gamma_{\\text{noise}}^{-1} (y - Au) + \\frac{1}{2} (u^T - m_0^T) \\Gamma_{\\text{prior}}^{-1} (u - m_0) + C\n$$\nThe first term expands to:\n$$\n\\frac{1}{2} (y^T \\Gamma_{\\text{noise}}^{-1} y - y^T \\Gamma_{\\text{noise}}^{-1} Au - u^T A^T \\Gamma_{\\text{noise}}^{-1} y + u^T A^T \\Gamma_{\\text{noise}}^{-1} Au)\n$$\nSince $\\Gamma_{\\text{noise}}^{-1}$ is symmetric, the two cross-terms are equal scalar quantities: $u^T A^T \\Gamma_{\\text{noise}}^{-1} y = (y^T \\Gamma_{\\text{noise}}^{-1} Au)^T = y^T \\Gamma_{\\text{noise}}^{-1} Au$. So, this becomes:\n$$\n\\frac{1}{2} (y^T \\Gamma_{\\text{noise}}^{-1} y - 2 u^T A^T \\Gamma_{\\text{noise}}^{-1} y + u^T A^T \\Gamma_{\\text{noise}}^{-1} Au)\n$$\nSimilarly, the second term in $\\Phi(u)$ expands to:\n$$\n\\frac{1}{2} (u^T \\Gamma_{\\text{prior}}^{-1} u - 2 u^T \\Gamma_{\\text{prior}}^{-1} m_0 + m_0^T \\Gamma_{\\text{prior}}^{-1} m_0)\n$$\nNow we differentiate $\\Phi(u)$ with respect to $u$. Using the matrix calculus identities $\\nabla_x (c^T x) = c$ and $\\nabla_x (x^T B x) = 2Bx$ for symmetric $B$:\n$$\n\\nabla_u \\Phi(u) = \\frac{1}{2} (-2 A^T \\Gamma_{\\text{noise}}^{-1} y + 2 A^T \\Gamma_{\\text{noise}}^{-1} A u) + \\frac{1}{2} (2 \\Gamma_{\\text{prior}}^{-1} u - 2 \\Gamma_{\\text{prior}}^{-1} m_0)\n$$\n$$\n\\nabla_u \\Phi(u) = A^T \\Gamma_{\\text{noise}}^{-1} (Au - y) + \\Gamma_{\\text{prior}}^{-1} (u - m_0)\n$$\nThe Hessian $H$ is the derivative of the gradient with respect to $u^T$. Differentiating $\\nabla_u \\Phi(u)$ with respect to $u$:\n$$\nH = \\nabla_u (\\nabla_u \\Phi(u)) = \\nabla_u (A^T \\Gamma_{\\text{noise}}^{-1} A u - A^T \\Gamma_{\\text{noise}}^{-1} y + \\Gamma_{\\text{prior}}^{-1} u - \\Gamma_{\\text{prior}}^{-1} m_0)\n$$\nTerms not involving $u$ have a zero derivative. We obtain:\n$$\nH = A^T \\Gamma_{\\text{noise}}^{-1} A + \\Gamma_{\\text{prior}}^{-1}\n$$\nThis is the Hessian of the negative log-posterior. Notably, for this linear-Gaussian model, the Hessian is a constant matrix, independent of the parameters $u$, the data $y$, and the prior mean $m_0$.\n\nThe problem states that $\\Gamma_{\\text{noise}}$ and $\\Gamma_{\\text{prior}}$ are positive definite. Their inverses, $\\Gamma_{\\text{noise}}^{-1}$ and $\\Gamma_{\\text{prior}}^{-1}$, are also positive definite. The matrix $A^T \\Gamma_{\\text{noise}}^{-1} A$ is positive semi-definite. The sum of a positive definite matrix ($\\Gamma_{\\text{prior}}^{-1}$) and a positive semi-definite matrix ($A^T \\Gamma_{\\text{noise}}^{-1} A$) results in a positive definite matrix. Therefore, the Hessian $H$ is symmetric and positive definite (SPD). This property guarantees that the linear system $Hz=b$ has a unique solution and that $\\det(H) > 0$.\n\nThe covariance matrices $\\Gamma_{\\text{noise}}$ and $\\Gamma_{\\text{prior}}$ are also specified to be diagonal. This simplifies computation, as their inverses are also diagonal matrices whose diagonal entries are the reciprocals of the original diagonals. Let $d_{\\text{noise}}$ and $d_{\\text{prior}}$ be the vectors of diagonal entries of $\\Gamma_{\\text{noise}}$ and $\\Gamma_{\\text{prior}}$ respectively. Then $\\Gamma_{\\text{noise}}^{-1} = \\operatorname{diag}(1/d_{\\text{noise}})$ and $\\Gamma_{\\text{prior}}^{-1} = \\operatorname{diag}(1/d_{\\text{prior}})$.\n\nThe three required computational tasks can be performed as follows:\n1.  **Compute the Hessian action $H v$**: Using the derived expression for $H$, the action on a vector $v \\in \\mathbb{R}^n$ is:\n    $$\n    H v = (A^T \\Gamma_{\\text{noise}}^{-1} A + \\Gamma_{\\text{prior}}^{-1}) v = A^T \\Gamma_{\\text{noise}}^{-1} (A v) + \\Gamma_{\\text{prior}}^{-1} v\n    $$\n    Computationally, this is performed by first forming the matrix $H = A^T \\operatorname{diag}(1/d_{\\text{noise}}) A + \\operatorname{diag}(1/d_{\\text{prior}})$ and then computing the matrix-vector product $H v$.\n\n2.  **Solve the linear system $H z = b$**: Since $H$ is an SPD matrix, this system can be efficiently and stably solved. Standard methods like Cholesky decomposition or LU decomposition (which simplifies to Cholesky for SPD matrices) are applicable. We will form the matrix $H$ and then use a numerical linear algebra solver to find $z$.\n\n3.  **Compute the natural logarithm of the determinant of $H$, $\\log \\det(H)$**: A numerically stable way to compute this is via the Cholesky decomposition of $H$. If $H = LL^T$, where $L$ is a lower triangular matrix (the Cholesky factor), then $\\det(H) = \\det(L L^T) = \\det(L) \\det(L^T) = (\\det(L))^2$. The determinant of a triangular matrix is the product of its diagonal entries, so $\\det(L) = \\prod_{i=1}^n L_{ii}$. Therefore,\n    $$\n    \\log \\det(H) = \\log\\left(\\left(\\prod_{i=1}^n L_{ii}\\right)^2\\right) = 2 \\sum_{i=1}^n \\log(L_{ii})\n    $$\n    This avoids computing the determinant directly, which can lead to overflow or underflow for large matrices. Specialized functions like `numpy.linalg.slogdet` can also be used, which directly and robustly compute the sign and log of the determinant. Since $H$ is positive definite, the sign will be $+1$.\n\nThe implementation will follow these steps for each test case provided.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases as specified.\n    - Constructs the Hessian of the negative log-posterior.\n    - Computes Hessian action, solves a linear system, and finds the log-determinant.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"m\": 4, \"n\": 3,\n            \"A\": np.array([\n                [1.0, 0.5, 0.0],\n                [0.0, 1.0, 0.5],\n                [0.5, 0.0, 1.0],\n                [1.0, -0.5, 0.5]\n            ]),\n            \"diag_gamma_noise\": np.array([0.1, 0.2, 0.15, 0.3]),\n            \"diag_gamma_prior\": np.array([1.0, 0.5, 2.0]),\n            \"v\": np.array([1.0, -1.0, 0.5]),\n            \"b\": np.array([0.2, -0.1, 0.3])\n        },\n        {\n            \"m\": 6, \"n\": 5,\n            \"A\": np.array([\n                [1.0, 0.9, 0.0, 0.1, -0.2],\n                [0.8, 0.7, 0.1, 0.0, 0.2],\n                [0.6, 0.59, -0.1, 0.1, 0.0],\n                [0.4, 0.41, 0.2, -0.2, 0.1],\n                [0.2, 0.21, -0.2, 0.2, -0.1],\n                [0.1, 0.09, 0.0, -0.1, 0.0]\n            ]),\n            \"diag_gamma_noise\": np.array([0.05, 0.05, 0.1, 0.2, 0.3, 0.4]),\n            \"diag_gamma_prior\": np.array([10.0, 10.0, 10.0, 10.0, 10.0]),\n            \"v\": np.array([0.5, 0.0, -0.5, 1.0, -1.0]),\n            \"b\": np.array([-0.1, 0.2, 0.0, 0.3, -0.2])\n        },\n        {\n            \"m\": 4, \"n\": 4,\n            \"A\": np.array([\n                [1.0, 0.99, 0.98, 0.97],\n                [0.5, 0.495, 0.49, 0.485],\n                [0.2, 0.198, 0.196, 0.194],\n                [-0.1, -0.099, -0.098, -0.097]\n            ]),\n            \"diag_gamma_noise\": np.array([0.2, 0.25, 0.3, 0.35]),\n            \"diag_gamma_prior\": np.array([0.01, 0.02, 0.015, 0.03]),\n            \"v\": np.array([1.0, 1.0, 1.0, 1.0]),\n            \"b\": np.array([0.5, -0.5, 0.5, -0.5])\n        },\n        {\n            \"m\": 2, \"n\": 1,\n            \"A\": np.array([\n                [2.0],\n                [-1.0]\n            ]),\n            \"diag_gamma_noise\": np.array([0.5, 0.25]),\n            \"diag_gamma_prior\": np.array([0.1]),\n            \"v\": np.array([1.0]),\n            \"b\": np.array([0.3])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A = case[\"A\"]\n        diag_gamma_noise = case[\"diag_gamma_noise\"]\n        diag_gamma_prior = case[\"diag_gamma_prior\"]\n        v = case[\"v\"]\n        b = case[\"b\"]\n        \n        # Invert diagonal covariance matrices\n        # Gamma_noise_inv is m x m, Gamma_prior_inv is n x n\n        gamma_noise_inv_diag = 1.0 / diag_gamma_noise\n        gamma_prior_inv_diag = 1.0 / diag_gamma_prior\n        \n        # Construct the Hessian H = A^T * Gamma_noise_inv * A + Gamma_prior_inv\n        # Using numpy's broadcasting for efficiency with diagonal Gamma_noise_inv\n        # (A.T * diag_vec) is equivalent to A.T @ diag(diag_vec)\n        term1 = (A.T * gamma_noise_inv_diag) @ A\n        term2 = np.diag(gamma_prior_inv_diag)\n        H = term1 + term2\n        \n        # 1. Compute Hessian action Hv\n        Hv = H @ v\n        \n        # 2. Solve Hz = b for z\n        # H is symmetric positive definite, so np.linalg.solve is stable.\n        z = np.linalg.solve(H, b)\n        \n        # 3. Compute log(det(H))\n        # np.linalg.slogdet returns (sign, logdet).\n        # Since H is positive definite, sign is 1.\n        sign, log_det_H = np.linalg.slogdet(H)\n        \n        # Format results for output: vectors as lists of floats, logdet as float\n        results.append([Hv.tolist(), z.tolist(), log_det_H])\n\n    # Final print statement must match the required format exactly.\n    # The default str() representation of lists is used.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3395961"}, {"introduction": "When the model is nonlinear, the posterior distribution is typically non-Gaussian, and the Laplace approximation's accuracy must be critically assessed. This practice [@problem_id:3137250] explores this issue within a Poisson-Gamma model, a framework where the exact posterior is known to be a Gamma distribution. You will discover firsthand how the Laplace approximation, being centered on the posterior mode, can misestimate key statistical quantities like the posterior mean and expected loss, especially when the true posterior is skewed. This exercise is crucial for developing the judgment needed to know when a Gaussian approximation is sufficient and when more advanced methods are required.", "problem": "Consider the following Bayesian one-parameter model, where independent observations are drawn from a Poisson distribution and the prior is Gamma. Let $\\{y_i\\}_{i=1}^n$ be independent draws from $\\text{Poisson}(\\lambda)$ with unknown rate $\\lambda > 0$, and use a Gamma prior $\\lambda \\sim \\text{Gamma}(a,b)$ with shape $a > 0$ and rate $b > 0$. Use Bayes' theorem together with the definitions of the Poisson likelihood and Gamma prior as the fundamental base. The Maximum A Posteriori (MAP) estimator is the posterior mode. The Laplace approximation approximates the posterior density near the MAP by a Gaussian distribution centered at the mode, with variance equal to the inverse of the negative second derivative (the negative Hessian for scalar) of the log-posterior at the mode.\n\nYour tasks:\n- Derive the posterior density up to a normalizing constant and express it in a recognized parametric form.\n- Derive the posterior mean and the posterior mode by differentiating the log-posterior.\n- Using the Laplace approximation centered at the posterior mode, derive the scalar Gaussian variance from the second derivative of the log-posterior at the mode.\n- For the convex risk given by the squared loss $L(\\lambda; t) = (\\lambda - t)^2$, compute the expected loss under the exact posterior and under the Laplace approximation. Use only the definitions of expectation for these distributions and quantities you derived.\n- Implement a program that, for each test case in the test suite below, computes the absolute difference between the exact expected loss and the Laplace-approximated expected loss.\n\nTest suite:\n- Case $1$: Prior $(a,b) = (2.5, 1.0)$, data $y = (0,1,2)$, loss target $t$ equal to the posterior mode of this case.\n- Case $2$: Prior $(a,b) = (0.3, 1.0)$, data $y = (1)$, loss target $t$ equal to the sample mean of the data for this case.\n- Case $3$: Prior $(a,b) = (2.0, 1.0)$, data $y = (10,12,9,11,8)$, loss target $t = 10.0$.\n- Case $4$: Prior $(a,b) = (3.0, 0.5)$, data $y = (0,0,0,1)$, loss target $t = 0.8$.\n\nProgram requirements:\n- Compute, for each case, the posterior shape $\\alpha$ and rate $\\beta$, the posterior mean and mode, the Laplace variance at the mode, and the expected squared loss under the exact posterior and under the Laplace approximation.\n- For each case, output the absolute difference between the exact expected loss and the Laplace-approximated expected loss as a floating-point number.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each number rounded to six decimal places (e.g., $[0.123456,0.000001,2.718282,1.000000]$).\n\nAll mathematical symbols, variables, functions, operators, and numbers in this problem statement are written using LaTeX. There are no physical units, angles, or percentages involved in this problem; all numerical outputs must be real numbers without units.", "solution": "The problem requires a thorough analysis of a Bayesian one-parameter model involving a Poisson likelihood and a Gamma prior. This framework is a classic example of conjugate families in Bayesian statistics. We will proceed by first validating the problem statement, which is found to be sound, and then systematically deriving the required quantities.\n\nThe model is specified as follows:\n- The data $\\{y_i\\}_{i=1}^n$ are independent and identically distributed draws from a Poisson distribution with parameter $\\lambda$: $y_i \\mid \\lambda \\sim \\text{Poisson}(\\lambda)$.\n- The prior distribution for the unknown parameter $\\lambda$ is a Gamma distribution with shape parameter $a$ and rate parameter $b$: $\\lambda \\sim \\text{Gamma}(a, b)$.\n\nThe probability mass function (PMF) for a Poisson distribution is $P(y \\mid \\lambda) = \\frac{e^{-\\lambda}\\lambda^y}{y!}$ for $y \\in \\{0, 1, 2, \\dots\\}$. The probability density function (PDF) for a Gamma distribution is $p(\\lambda \\mid a, b) = \\frac{b^a}{\\Gamma(a)}\\lambda^{a-1}e^{-b\\lambda}$ for $\\lambda > 0$.\n\nFirst, we derive the posterior density of $\\lambda$. By Bayes' theorem, the posterior density is proportional to the product of the likelihood and the prior density:\n$$ p(\\lambda \\mid y_1, \\dots, y_n) \\propto P(y_1, \\dots, y_n \\mid \\lambda) \\, p(\\lambda) $$\nGiven that the observations are independent, the likelihood is the product of the individual PMFs:\n$$ P(y_1, \\dots, y_n \\mid \\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!} = \\frac{e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n y_i}}{\\prod_{i=1}^n y_i!} $$\nAs a function of $\\lambda$, the likelihood is proportional to $e^{-n\\lambda} \\lambda^{\\sum y_i}$. The prior density is proportional to $\\lambda^{a-1}e^{-b\\lambda}$.\nCombining these, the posterior density is:\n$$ p(\\lambda \\mid \\mathbf{y}) \\propto \\left(e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n y_i}\\right) \\left(\\lambda^{a-1}e^{-b\\lambda}\\right) $$\n$$ p(\\lambda \\mid \\mathbf{y}) \\propto \\lambda^{(\\sum y_i + a) - 1} e^{-(n+b)\\lambda} $$\nThis expression is the kernel of a Gamma distribution. Thus, the posterior distribution is also a Gamma distribution, which demonstrates the conjugacy of the Gamma prior for the Poisson likelihood. The posterior distribution is $\\lambda \\mid \\mathbf{y} \\sim \\text{Gamma}(\\alpha, \\beta)$, with updated parameters:\n- Posterior shape: $\\alpha = \\sum_{i=1}^n y_i + a$\n- Posterior rate: $\\beta = n + b$\n\nSecond, we derive the posterior mean and posterior mode. For a Gamma distribution $\\text{Gamma}(\\alpha, \\beta)$, the mean and mode are well-known quantities.\n- The posterior mean is $E[\\lambda \\mid \\mathbf{y}] = \\frac{\\alpha}{\\beta} = \\frac{\\sum y_i + a}{n + b}$.\n- The posterior mode (Maximum A Posteriori estimate, $\\lambda_{\\text{MAP}}$) is found by maximizing the posterior density. For a Gamma distribution with shape $\\alpha > 1$, the mode is $\\lambda_{\\text{MAP}} = \\frac{\\alpha - 1}{\\beta} = \\frac{\\sum y_i + a - 1}{n + b}$. All test cases provided in the problem satisfy the condition $\\alpha > 1$.\n\nThird, we derive the variance for the Laplace approximation. The Laplace approximation approximates the posterior density with a Gaussian (Normal) distribution centered at the posterior mode, $\\lambda_{\\text{MAP}}$. The variance, which we denote $\\sigma_L^2$, is the inverse of the negative of the second derivative of the log-posterior density evaluated at the mode.\nThe log-posterior, up to an additive constant, is:\n$$ \\log p(\\lambda \\mid \\mathbf{y}) = (\\alpha-1)\\log\\lambda - \\beta\\lambda + C $$\nThe first derivative with respect to $\\lambda$ is:\n$$ \\frac{d}{d\\lambda} \\log p(\\lambda \\mid \\mathbf{y}) = \\frac{\\alpha-1}{\\lambda} - \\beta $$\nThe second derivative is:\n$$ \\frac{d^2}{d\\lambda^2} \\log p(\\lambda \\mid \\mathbf{y}) = -\\frac{\\alpha-1}{\\lambda^2} $$\nEvaluating the negative of the second derivative (the observed Fisher information) at the mode $\\lambda_{\\text{MAP}} = \\frac{\\alpha - 1}{\\beta}$:\n$$ J(\\lambda_{\\text{MAP}}) = -\\left(-\\frac{\\alpha-1}{(\\frac{\\alpha-1}{\\beta})^2}\\right) = \\frac{\\alpha-1}{(\\alpha-1)^2 / \\beta^2} = \\frac{\\beta^2}{\\alpha-1} $$\nThe variance of the Laplace approximation is the inverse of this quantity:\n$$ \\sigma_L^2 = [J(\\lambda_{\\text{MAP}})]^{-1} = \\frac{\\alpha-1}{\\beta^2} $$\nThus, the Laplace approximation to the posterior is $\\lambda_{\\text{approx}} \\sim N(\\mu_L, \\sigma_L^2)$, where the mean is $\\mu_L = \\lambda_{\\text{MAP}} = \\frac{\\alpha-1}{\\beta}$ and the variance is $\\sigma_L^2 = \\frac{\\alpha-1}{\\beta^2}$.\n\nFourth, we compute the expected squared loss $L(\\lambda; t) = (\\lambda - t)^2$. The expected loss under a probability distribution for $\\lambda$ is given by $E[(\\lambda-t)^2]$. This can be expanded using the definition of variance, $\\text{Var}(\\lambda) = E[\\lambda^2] - (E[\\lambda])^2$:\n$$ E[(\\lambda-t)^2] = E[\\lambda^2 - 2t\\lambda + t^2] = E[\\lambda^2] - 2tE[\\lambda] + t^2 $$\n$$ E[(\\lambda-t)^2] = (\\text{Var}(\\lambda) + (E[\\lambda])^2) - 2tE[\\lambda] + t^2 = \\text{Var}(\\lambda) + (E[\\lambda] - t)^2 $$\nThis formula relates the expected loss to the variance and the squared bias of the distribution's mean relative to the target $t$.\n\nWe apply this formula to both the exact posterior and the Laplace approximation.\n- For the exact posterior, $\\lambda \\mid \\mathbf{y} \\sim \\text{Gamma}(\\alpha, \\beta)$:\n  - Mean: $E_{\\text{post}}[\\lambda] = \\frac{\\alpha}{\\beta}$\n  - Variance: $\\text{Var}_{\\text{post}}(\\lambda) = \\frac{\\alpha}{\\beta^2}$\n  - Expected loss: $E_{\\text{exact}} = \\text{Var}_{\\text{post}}(\\lambda) + (E_{\\text{post}}[\\lambda] - t)^2 = \\frac{\\alpha}{\\beta^2} + \\left(\\frac{\\alpha}{\\beta} - t\\right)^2$.\n- For the Laplace approximation, $\\lambda_{\\text{approx}} \\sim N(\\mu_L, \\sigma_L^2)$:\n  - Mean: $E_{\\text{Laplace}}[\\lambda] = \\mu_L = \\frac{\\alpha-1}{\\beta}$\n  - Variance: $\\text{Var}_{\\text{Laplace}}(\\lambda) = \\sigma_L^2 = \\frac{\\alpha-1}{\\beta^2}$\n  - Expected loss: $E_{\\text{Laplace}} = \\text{Var}_{\\text{Laplace}}(\\lambda) + (E_{\\text{Laplace}}[\\lambda] - t)^2 = \\frac{\\alpha-1}{\\beta^2} + \\left(\\frac{\\alpha-1}{\\beta} - t\\right)^2$.\n\nThe program will implement these final formulas to compute the absolute difference $|E_{\\text{exact}} - E_{\\text{Laplace}}|$ for each test case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite by calculating the absolute difference\n    between the expected squared loss under the exact posterior and under the Laplace approximation.\n    \"\"\"\n    \n    # Test suite definition: (prior_a, prior_b, data_y, t_config)\n    # t_config is a tuple (type, value) where type is 'mode', 'mean', or 'value'.\n    test_cases = [\n        (2.5, 1.0, [0, 1, 2], ('mode', None)),\n        (0.3, 1.0, [1], ('mean', None)),\n        (2.0, 1.0, [10, 12, 9, 11, 8], ('value', 10.0)),\n        (3.0, 0.5, [0, 0, 0, 1], ('value', 0.8)),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        a_prior, b_prior, y, t_config = case\n        \n        # Convert y to a numpy array for easier calculations\n        y = np.array(y)\n        \n        # Calculate sufficient statistics from data\n        n = len(y)\n        sum_y = np.sum(y)\n        \n        # Calculate posterior parameters\n        # Posterior is Gamma(alpha, beta)\n        alpha_post = sum_y + a_prior\n        beta_post = float(n + b_prior)\n\n        # Ensure posterior mode is well-defined (alpha > 1)\n        if alpha_post <= 1:\n            # This case is not expected based on problem validation\n            # but good practice to handle.\n            results.append(np.nan)\n            continue\n\n        # Determine the loss target t based on the configuration\n        t_type, t_val = t_config\n        t = 0.0\n        if t_type == 'value':\n            t = t_val\n        elif t_type == 'mode':\n            # Posterior mode (MAP)\n            t = (alpha_post - 1) / beta_post\n        elif t_type == 'mean':\n            # Sample mean of the data\n            t = np.mean(y)\n\n        # === Calculations for the exact posterior: Gamma(alpha_post, beta_post) ===\n        \n        # Mean of the exact posterior\n        mean_exact = alpha_post / beta_post\n        # Variance of the exact posterior\n        var_exact = alpha_post / (beta_post**2)\n        # Expected squared loss for the exact posterior\n        expected_loss_exact = var_exact + (mean_exact - t)**2\n\n        # === Calculations for the Laplace approximation: Normal(mu_L, sigma_L^2) ===\n        \n        # Mean of the Laplace approximation is the posterior mode\n        mean_laplace = (alpha_post - 1) / beta_post\n        # Variance of the Laplace approximation\n        var_laplace = (alpha_post - 1) / (beta_post**2)\n        # Expected squared loss for the Laplace approximation\n        expected_loss_laplace = var_laplace + (mean_laplace - t)**2\n\n        # Calculate the absolute difference between the two expected losses\n        abs_diff = abs(expected_loss_exact - expected_loss_laplace)\n        results.append(abs_diff)\n    \n    # Format the output as a comma-separated list of strings with 6 decimal places\n    output_str = \",\".join([f\"{res:.6f}\" for res in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3137250"}, {"introduction": "The true power of Bayesian inversion is often realized in function-space settings, where the unknown is a continuous field rather than a discrete vector. This exercise [@problem_id:3395972] elevates the Laplace approximation from finite dimensions to this infinite-dimensional context, which is the natural home for many problems in physics and engineering. You will work with a simple nonlinear operator to derive the posterior covariance as an operator and analyze its structure by finding its eigenvalues in the Fourier basis. This practice develops the essential skill of interpreting posterior uncertainty not just as a number, but as a spatially correlated structure that reveals how information is constrained by data at different scales.", "problem": "Consider an inverse problem on the one-dimensional periodic domain $D = [0, 2\\pi]$ with periodic boundary conditions. The unknown field is $u \\in L^{2}(D)$, and the forward map is the nonlinear operator $G(u): L^{2}(D) \\to L^{2}(D)$ given by $G(u)(x) = u(x)^{2}$. Observations are modeled by $y = G(u_{\\mathrm{true}}) + \\eta$, where $\\eta$ is a mean-zero Gaussian noise field with covariance operator $\\Gamma = \\sigma^{2} I$, meaning the data misfit is measured in the weighted $L^{2}$ norm induced by $\\Gamma^{-1} = \\sigma^{-2} I$. The prior on $u$ is Gaussian $\\mathcal{N}(0, \\mathcal{C}_{0})$ with precision operator $\\mathcal{C}_{0}^{-1} = \\alpha (I - \\ell^{2} \\Delta)$, where $\\alpha > 0$, $\\ell > 0$, and $\\Delta$ is the Laplacian on $D$ with periodic boundary conditions.\n\nLet the observed data be spatially constant, $y(x) = y_{0}$ for all $x \\in D$, with $y_{0} > \\frac{\\sigma^{2}}{2} \\alpha$. Work at the level of the Maximum A Posteriori (MAP) estimate and the Laplace approximation to the posterior. Starting from the negative log posterior functional\n$$\n\\Phi(u) = \\frac{1}{2\\sigma^{2}} \\int_{D} \\big(y(x) - u(x)^{2}\\big)^{2} \\, dx + \\frac{1}{2} \\langle u, \\mathcal{C}_{0}^{-1} u \\rangle_{L^{2}(D)},\n$$\nperform the following:\n\n- Derive the first-order optimality condition and solve for the spatially constant MAP solution $u^{*}$ under the ansatz $u(x) \\equiv c$, expressing $c$ in terms of $\\alpha$, $\\sigma^{2}$, and $y_{0}$.\n\n- Compute the Hessian operator $H = D^{2} \\Phi(u^{*})$ at the MAP, carefully accounting for contributions from both the prior and the data misfit.\n\n- Carry out the spectral analysis of $H$ in the Fourier basis $\\{\\phi_{k}(x)\\}_{k \\in \\mathbb{Z}}$ with $\\phi_{k}(x) = (2\\pi)^{-1/2} \\exp(i k x)$, and deduce the Laplace-approximate posterior covariance operator $\\mathcal{C}_{\\mathrm{post}} \\approx H^{-1}$.\n\nReport, as your final answer, a single closed-form analytic expression for the eigenvalues $c_{k}$ of $\\mathcal{C}_{\\mathrm{post}}$ as a function of the Fourier mode $k \\in \\mathbb{Z}$. No rounding is required. Express the answer in terms of $\\alpha$, $\\ell$, $\\sigma^{2}$, $y_{0}$, and $k$.", "solution": "### Derive and Solve for the MAP Estimate $u^*$\n\nThe Maximum A Posteriori (MAP) estimate $u^*$ is the function $u$ that minimizes the negative log-posterior functional $\\Phi(u)$. We find this by setting the Fréchet derivative of $\\Phi(u)$ with respect to $u$ to zero. The Gateaux derivative of $\\Phi(u)$ in a direction $v \\in L^2(D)$ is given by:\n$$\n\\langle D\\Phi(u), v \\rangle_{L^2} = \\frac{d}{d\\epsilon}\\bigg|_{\\epsilon=0} \\Phi(u+\\epsilon v)\n$$\nFor the data misfit term:\n$$\n\\frac{d}{d\\epsilon}\\bigg|_{\\epsilon=0} \\frac{1}{2\\sigma^{2}} \\int_{D} \\left(y(x) - (u(x)+\\epsilon v(x))^{2}\\right)^{2} \\, dx = \\frac{1}{2\\sigma^{2}} \\int_{D} 2(y-u^2)(-2uv) \\, dx = \\left\\langle -\\frac{2u(y-u^2)}{\\sigma^2}, v \\right\\rangle_{L^2}\n$$\nFor the prior term, which is a quadratic form:\n$$\n\\frac{d}{d\\epsilon}\\bigg|_{\\epsilon=0} \\frac{1}{2} \\langle u+\\epsilon v, \\mathcal{C}_{0}^{-1}(u+\\epsilon v) \\rangle_{L^2} = \\langle \\mathcal{C}_{0}^{-1} u, v \\rangle_{L^2}\n$$\nsince $\\mathcal{C}_{0}^{-1}$ is self-adjoint. The first-order optimality condition $D\\Phi(u)=0$ is therefore:\n$$\n\\mathcal{C}_{0}^{-1} u - \\frac{2u(y-u^2)}{\\sigma^2} = 0\n$$\nWe look for a spatially constant solution $u^*(x) = c$ using the given constant data $y(x) = y_0$. For a constant function $c$, the Laplacian $\\Delta c = 0$. The optimality condition becomes an algebraic equation for the constant $c$:\n$$\n\\alpha(I - \\ell^2 \\Delta) c - \\frac{2c(y_0 - c^2)}{\\sigma^2} = 0 \\implies \\alpha c - \\frac{2c(y_0 - c^2)}{\\sigma^2} = 0\n$$\nFactoring out $c$:\n$$\nc \\left( \\alpha - \\frac{2(y_0 - c^2)}{\\sigma^2} \\right) = 0\n$$\nThis equation has solutions $c=0$ and $\\alpha - \\frac{2(y_0 - c^2)}{\\sigma^2} = 0$. The second case gives:\n$$\n\\alpha \\sigma^2 = 2(y_0 - c^2) \\implies c^2 = y_0 - \\frac{\\alpha \\sigma^2}{2}\n$$\nThe problem states that $y_0 > \\frac{\\alpha \\sigma^2}{2}$, which ensures that $y_0 - \\frac{\\alpha \\sigma^2}{2} > 0$. Therefore, real solutions for $c$ exist: $c = \\pm \\sqrt{y_0 - \\frac{\\alpha \\sigma^2}{2}}$. A stability analysis (performed below by inspecting the Hessian) confirms that these non-zero solutions correspond to local minima of $\\Phi(u)$, whereas $c = 0$ does not. We select one of these as our MAP point, $u^*$. The specific choice of sign does not affect the Hessian. Let's denote $(u^*)^2 = y_0 - \\frac{\\alpha \\sigma^2}{2}$.\n\n### Compute the Hessian Operator $H$\n\nThe Hessian $H = D^2\\Phi(u^*)$ is the second Fréchet derivative of $\\Phi$ evaluated at $u^*$. We differentiate the expression for $D\\Phi(u)$ with respect to $u$. The derivative of the prior term $\\mathcal{C}_{0}^{-1}u$ is simply the operator $\\mathcal{C}_{0}^{-1}$. The derivative of the data misfit term $-\\frac{2}{\\sigma^2}(yu-u^3)$ with respect to $u$ is multiplication by the function $-\\frac{2}{\\sigma^2}(y-3u^2)$.\nThus, the Hessian operator $H(u)$ acting on a function $h$ is:\n$$\nH(u)h = \\mathcal{C}_{0}^{-1} h - \\frac{2(y - 3u^2)}{\\sigma^2} h = \\left( \\mathcal{C}_{0}^{-1} - \\frac{2(y - 3u^2)}{\\sigma^2}I \\right)h\n$$\nWe evaluate this at the constant MAP solution $u^*$ and constant data $y_0$:\n$$\nH = \\mathcal{C}_{0}^{-1} - \\frac{2(y_0 - 3(u^*)^2)}{\\sigma^2}I\n$$\nSubstitute $(u^*)^2 = y_0 - \\frac{\\alpha \\sigma^2}{2}$ into the expression:\n$$\ny_0 - 3(u^*)^2 = y_0 - 3\\left(y_0 - \\frac{\\alpha \\sigma^2}{2}\\right) = -2y_0 + \\frac{3\\alpha \\sigma^2}{2}\n$$\nNow substitute this back into the expression for $H$, along with the definition $\\mathcal{C}_{0}^{-1} = \\alpha(I - \\ell^2 \\Delta)$:\n$$\nH = \\alpha(I - \\ell^2 \\Delta) - \\frac{2}{\\sigma^2}\\left(-2y_0 + \\frac{3\\alpha \\sigma^2}{2}\\right)I\n$$\n$$\nH = \\alpha I - \\alpha \\ell^2 \\Delta - \\left(-\\frac{4y_0}{\\sigma^2} + 3\\alpha\\right)I = (\\alpha + \\frac{4y_0}{\\sigma^2} - 3\\alpha)I - \\alpha \\ell^2 \\Delta\n$$\n$$\nH = \\left(\\frac{4y_0}{\\sigma^2} - 2\\alpha\\right)I - \\alpha \\ell^2 \\Delta\n$$\n\n### Spectral Analysis and Posterior Covariance\n\nThe Laplace approximation to the posterior distribution is a Gaussian $\\mathcal{N}(u^*, \\mathcal{C}_{\\mathrm{post}})$ where the posterior covariance is $\\mathcal{C}_{\\mathrm{post}} \\approx H^{-1}$. We are asked for the eigenvalues of $\\mathcal{C}_{\\mathrm{post}}$, which are the reciprocals of the eigenvalues of $H$.\nWe find the eigenvalues of $H$ by acting on the Fourier basis functions $\\phi_k(x) = (2\\pi)^{-1/2} \\exp(i k x)$. These are eigenfunctions of the Laplacian: $\\Delta \\phi_k(x) = -k^2 \\phi_k(x)$.\nSince $H$ is a linear differential operator with constant coefficients, it is diagonal in the Fourier basis. Let $\\lambda_k$ be the eigenvalue of $H$ corresponding to the eigenfunction $\\phi_k$:\n$$\nH \\phi_k = \\left[\\left(\\frac{4y_0}{\\sigma^2} - 2\\alpha\\right)I - \\alpha \\ell^2 \\Delta\\right] \\phi_k\n$$\n$$\nH \\phi_k = \\left(\\frac{4y_0}{\\sigma^2} - 2\\alpha\\right)\\phi_k - \\alpha \\ell^2 (-k^2 \\phi_k)\n$$\n$$\nH \\phi_k = \\left(\\frac{4y_0}{\\sigma^2} - 2\\alpha + \\alpha \\ell^2 k^2\\right)\\phi_k\n$$\nThe eigenvalues of $H$ are therefore:\n$$\n\\lambda_k = \\frac{4y_0}{\\sigma^2} - 2\\alpha + \\alpha \\ell^2 k^2, \\quad \\text{for } k \\in \\mathbb{Z}\n$$\nThe condition $y_0 > \\frac{\\sigma^2}{2} \\alpha$ implies $\\frac{4y_0}{\\sigma^2} - 2\\alpha > 2\\alpha - 2\\alpha = 0$, which ensures that $\\lambda_0 > 0$. Since $\\alpha > 0$ and $\\ell > 0$, the term $\\alpha \\ell^2 k^2 \\ge 0$, so all eigenvalues $\\lambda_k$ are positive. This confirms that $H$ is positive definite and $u^*$ is a local minimum.\nThe eigenvalues $c_k$ of the Laplace-approximate posterior covariance operator $\\mathcal{C}_{\\mathrm{post}} \\approx H^{-1}$ are the reciprocals of the eigenvalues $\\lambda_k$:\n$$\nc_k = (\\lambda_k)^{-1} = \\frac{1}{\\frac{4y_0}{\\sigma^2} - 2\\alpha + \\alpha \\ell^2 k^2}\n$$\nThis is the final closed-form expression for the eigenvalues of the posterior covariance.", "answer": "$$\n\\boxed{\\left( \\frac{4y_{0}}{\\sigma^{2}} - 2\\alpha + \\alpha \\ell^{2} k^{2} \\right)^{-1}}\n$$", "id": "3395972"}]}