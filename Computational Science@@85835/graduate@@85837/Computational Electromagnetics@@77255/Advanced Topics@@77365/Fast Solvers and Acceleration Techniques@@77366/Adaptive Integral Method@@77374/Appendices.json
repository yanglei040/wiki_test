{"hands_on_practices": [{"introduction": "The power of the Adaptive Integral Method (AIM) hinges on its separation of computations into near and far zones. This exercise provides a hands-on approach to determining the near-zone radius not by a fixed rule, but by controlling the global error introduced by the far-zone approximation [@problem_id:3288285]. You will implement an algorithm that adapts the radius to satisfy a prescribed error tolerance $\\tau$, connecting the theoretical accuracy of the method to a practical, tunable parameter.", "problem": "Consider the Electric Field Integral Equation (EFIE) for perfectly electrically conducting scatterers, where the scattered electric field is represented via the free-space scalar Green's function of the Helmholtz equation. In the Method of Moments (MoM) discretization with Rao–Wilton–Glisson basis functions, the impedance matrix entry $Z_{ij}$ involves the kernel $G(\\mathbf{r},\\mathbf{r}') = \\frac{e^{\\mathrm{i}k\\|\\mathbf{r}-\\mathbf{r}'\\|}}{4\\pi\\|\\mathbf{r}-\\mathbf{r}'\\|}$ integrated against basis functions over panels. The Adaptive Integral Method (AIM) splits the impedance matrix into a near-zone part and a far-zone part, denoted $Z_{\\text{near}}$ and $Z_{\\text{fft}}$, respectively, such that entries corresponding to geometrically near interactions are treated directly, while far interactions are accelerated via a convolution on a regular grid. This splitting introduces a near-zone classification radius $r_{\\text{near}}$, which determines which interactions are computed exactly versus via grid-based convolution.\n\nThe quantity of interest is the deviation in the near-zone contribution caused by misclassification or truncation, denoted $\\Delta\\mathbf{Z}_{\\text{near}}$, defined as the matrix whose $(i,j)$ entry is the difference between the direct near-zone evaluation and its accelerated approximation over interactions that are classified as far when they should be near. We seek to adapt the near classification radius $r_{\\text{near}}$ so that the induced error satisfies a global operator norm constraint $\\|\\|\\Delta\\mathbf{Z}_{\\text{near}}\\|\\|_2 \\le \\tau$, for a prescribed tolerance $\\tau$.\n\nStarting from the fundamental representation of the EFIE and the Newtonian singular behavior of the free-space Green's function, use the following construction to drive the adaptation:\n\n- The local singularity metric between two basis functions $i$ and $j$ with characteristic sizes $a_i$ and $a_j$ and centroid separation $R_{ij} = \\|\\mathbf{r}_i - \\mathbf{r}_j\\|$ is defined as $m_{ij} = \\frac{a_i a_j}{R_{ij}^2}$, reflecting the first-order sensitivity to the singular kernel $G$ via a Lipschitz-type bound on its gradient.\n\n- There exists a constant $\\beta$ depending on the free-space kernel scale such that a conservative entrywise bound satisfies $|\\Delta Z_{ij}| \\le \\beta\\, m_{ij}$ for interactions treated via the grid-based approximation. For the scalar Green's function magnitude envelope, use $\\beta = \\frac{1}{4\\pi}$.\n\n- Using the Frobenius norm bound $\\|\\|\\Delta\\mathbf{Z}_{\\text{near}}\\|\\|_2 \\le \\|\\|\\Delta\\mathbf{Z}_{\\text{near}}\\|\\|_F \\le \\sqrt{\\sum_{(i,j)\\in\\mathcal{F}(r)} b_{ij}^2}$, where $b_{ij} = \\beta \\frac{a_i a_j}{R_{ij}^2}$ and $\\mathcal{F}(r)$ denotes the set of far-classified pairs with $R_{ij} \\ge r$, and the inequality $\\sqrt{\\sum_{(i,j)\\in\\mathcal{F}(r)} b_{ij}^2} \\le \\sqrt{M(r)} \\max_{(i,j)\\in\\mathcal{F}(r)} b_{ij}$ with $M(r) = |\\mathcal{F}(r)|$, define the monotone decreasing function\n$$\nf(r) = \\sqrt{M(r)}\\; \\max_{(i,j)\\in\\mathcal{F}(r)} \\left( \\beta \\frac{a_i a_j}{R_{ij}^2} \\right),\n$$\nand choose $r_{\\text{near}}$ as the smallest radius $r \\ge h_{\\text{grid}}$ such that $f(r) \\le \\tau$, where $h_{\\text{grid}}$ is the AIM grid spacing below which far-zone convolution assumptions do not hold.\n\nImplement an algorithm that, given the basis centroids $\\{\\mathbf{r}_i\\}$, sizes $\\{a_i\\}$, and a grid spacing $h_{\\text{grid}}$, computes the minimal admissible near classification radius $r_{\\text{near}}$ (in meters) such that $f(r_{\\text{near}}) \\le \\tau$. Use a robust monotonic bisection over $r \\in [h_{\\text{grid}}, r_{\\max}]$, where $r_{\\max}$ is the maximum centroid separation in the set, and select $r_{\\text{near}}$ as the lower minimal radius satisfying the inequality to within a numerical tolerance. In the computation, treat only distinct pairs $(i,j)$ with $i \\neq j$.\n\nAssume the free-space wavenumber $k$ is finite and strictly positive, but for the purposes of the conservative singularity-driven bound, you will use $\\beta = \\frac{1}{4\\pi}$ exclusively; no additional dependence on $k$ is needed. You must:\n\n- Derive the bound structure starting from Maxwell's equations and the EFIE kernel singularity, and justify the form of $m_{ij}$ and the Frobenius-to-spectral norm reduction that leads to $f(r)$.\n\n- Implement the adaptation algorithm described above.\n\n- Apply your implementation to the following test suite. All geometric quantities are in meters, and your program must output $r_{\\text{near}}$ in meters to six decimal places.\n\nTest Suite (each case provides $(\\{\\mathbf{r}_i\\}, \\{a_i\\}, h_{\\text{grid}}, \\tau)$):\n\n1. Case A (moderate tolerance, dense patch):\n   - Positions: $\\{\\mathbf{r}_i\\}_{i=1}^9$ are the centroids of a $3 \\times 3$ grid on the plane $z=0$ with spacing $0.1$, i.e., $(0.0,0.0,0.0)$, $(0.1,0.0,0.0)$, $(0.2,0.0,0.0)$, $(0.0,0.1,0.0)$, $(0.1,0.1,0.0)$, $(0.2,0.1,0.0)$, $(0.0,0.2,0.0)$, $(0.1,0.2,0.0)$, $(0.2,0.2,0.0)$.\n   - Sizes: $a_i = 0.1$ for all $i$.\n   - Grid spacing: $h_{\\text{grid}} = 0.12$.\n   - Tolerance: $\\tau = 0.3$.\n\n2. Case B (tight tolerance, same patch):\n   - Positions: identical to Case A.\n   - Sizes: identical to Case A.\n   - Grid spacing: $h_{\\text{grid}} = 0.12$.\n   - Tolerance: $\\tau = 0.05$.\n\n3. Case C (sparse, large separation):\n   - Positions: $\\{\\mathbf{r}_i\\}_{i=1}^4$ are $(0.0,0.0,0.0)$, $(2.0,0.0,0.0)$, $(0.0,2.0,0.0)$, $(2.0,2.0,0.0)$.\n   - Sizes: $a_i = 0.05$ for all $i$.\n   - Grid spacing: $h_{\\text{grid}} = 0.2$.\n   - Tolerance: $\\tau = 0.1$.\n\nYour program should produce a single line of output containing the adapted radii as a comma-separated list enclosed in square brackets, with each entry formatted to six decimal places and expressed in meters, e.g., $[\\text{r\\_A},\\text{r\\_B},\\text{r\\_C}]$, where $\\text{r\\_A}$, $\\text{r\\_B}$, and $\\text{r\\_C}$ are the computed $r_{\\text{near}}$ for Cases A, B, and C, respectively.", "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded in the principles of computational electromagnetics, specifically the Method of Moments (MoM) and the Adaptive Integral Method (AIM). The problem is well-posed, objective, and provides all necessary information to formulate and solve for the adaptive near-zone classification radius. The described method, while based on a series of conservative bounds, represents a plausible and algorithmically tractable approach to error control in a fast multipole or AIM context.\n\nWe will now proceed with a complete, reasoned solution, beginning with the theoretical derivation followed by the algorithmic implementation.\n\n### Theoretical Foundation and Derivation\n\nThe problem is rooted in the numerical solution of the Electric Field Integral Equation (EFIE) for scattering from a perfect electric conductor (PEC). The EFIE relates the unknown surface electric current $\\mathbf{J}(\\mathbf{r}')$ induced on the scatterer's surface $S$ to a known incident electric field $\\mathbf{E}^{\\text{inc}}(\\mathbf{r})$:\n$$\n\\mathbf{\\hat{n}} \\times \\mathbf{E}^{\\text{inc}}(\\mathbf{r}) = -\\mathbf{\\hat{n}} \\times \\left[ -j\\omega\\mu_0 \\int_S G(\\mathbf{r}, \\mathbf{r}') \\mathbf{J}(\\mathbf{r}') dS' - \\frac{1}{j\\omega\\epsilon_0} \\nabla \\int_S G(\\mathbf{r}, \\mathbf{r}') (\\nabla' \\cdot \\mathbf{J}(\\mathbf{r}')) dS' \\right], \\quad \\mathbf{r} \\in S\n$$\nThe kernel of this integral equation is the free-space scalar Green's function for the Helmholtz equation, given by:\n$$\nG(\\mathbf{r}, \\mathbf{r}') = \\frac{e^{ik\\|\\mathbf{r}-\\mathbf{r}'\\|}}{4\\pi\\|\\mathbf{r}-\\mathbf{r}'\\|}\n$$\nwhere $k$ is the free-space wavenumber. A key feature of this kernel is its singularity as $\\|\\mathbf{r}-\\mathbf{r}'\\| \\to 0$, where it behaves as $1/\\|\\mathbf{r}-\\mathbf{r}'\\|$. This is the so-called Newtonian singularity. The EFIE operator is more singular, involving gradients of the Green's function, which exhibit a $1/\\|\\mathbf{r}-\\mathbf{r}'\\|^2$ singular behavior.\n\nIn the Method of Moments (MoM), the unknown current $\\mathbf{J}$ is expanded using a set of $N$ basis functions $\\mathbf{f}_j(\\mathbf{r})$, typically Rao-Wilton-Glisson (RWG) functions for triangular meshes: $\\mathbf{J}(\\mathbf{r}) = \\sum_{j=1}^N I_j \\mathbf{f}_j(\\mathbf{r})$. This discretization transforms the integral equation into a dense linear system of equations $\\mathbf{Z}\\mathbf{I} = \\mathbf{V}$, where $\\mathbf{I}$ is the vector of unknown current coefficients. The impedance matrix entry $Z_{ij}$ represents the interaction between basis function $i$ and basis function $j$.\n\nThe Adaptive Integral Method (AIM) accelerates the matrix-vector products required to solve this system by splitting the impedance matrix $\\mathbf{Z}$ into a sparse near-zone part and a structured far-zone part: $\\mathbf{Z} = \\mathbf{Z}_{\\text{near}} + \\mathbf{Z}_{\\text{fft}}$. Interactions between geometrically \"near\" basis functions are computed directly and stored in $\\mathbf{Z}_{\\text{near}}$, while \"far\" interactions are approximated via a convolution on a regular grid, computed efficiently using the Fast Fourier Transform (FFT).\n\nThe criterion for classifying pairs of basis functions as \"near\" or \"far\" is a separation radius, $r_{\\text{near}}$. A pair $(i, j)$ is near if their centroid separation $R_{ij} = \\|\\mathbf{r}_i - \\mathbf{r}_j\\|$ is less than $r_{\\text{near}}$; otherwise, it is far. The FFT-based approximation for far interactions introduces an error. The problem defines a matrix $\\Delta\\mathbf{Z}_{\\text{near}}$ representing the error incurred for interactions treated via the grid approximation. The problem aims to find a suitable $r_{\\text{near}}$ to control this error.\n\nThe problem provides a conservative, entrywise bound for the error magnitude, $|\\Delta Z_{ij}|$, for any pair $(i, j)$ treated as far:\n$$\n|\\Delta Z_{ij}| \\le \\beta \\, m_{ij} = \\beta \\frac{a_i a_j}{R_{ij}^2}\n$$\nHere, $a_i$ and $a_j$ are characteristic sizes of the basis functions, and $R_{ij}$ is their separation. This bound is motivated by the fact that the leading error term in far-field approximations (like multipole expansions used in AIM) is proportional to the strongest singularity in the operator, which in the EFIE scales as $1/R_{ij}^2$. The term $a_i a_j$ represents the product of the \"source strengths\" or areas of the basis functions. The constant $\\beta = 1/(4\\pi)$ is a scale factor derived from the magnitude envelope of the Green's function itself.\n\nTo control the global error, a constraint is placed on the spectral norm of the error matrix: $\\|\\|\\Delta\\mathbf{Z}_{\\text{near}}\\|\\|_2 \\le \\tau$, for a given tolerance $\\tau$. Computing the spectral norm is expensive, so a standard, more tractable upper bound is used via the Frobenius norm:\n$$\n\\|\\|\\Delta\\mathbf{Z}_{\\text{near}}\\|\\|_2 \\le \\|\\|\\Delta\\mathbf{Z}_{\\text{near}}\\|\\|_F = \\sqrt{\\sum_{i=1}^N \\sum_{j=1}^N |\\Delta Z_{ij}|^2}\n$$\nThe error $\\Delta Z_{ij}$ is non-zero only for pairs $(i, j)$ that are classified as far. Let $\\mathcal{F}(r)$ be the set of ordered pairs $(i,j)$, $i\\neq j$, with $R_{ij} \\ge r$. If we choose $r_{\\text{near}} = r$, these are the pairs treated by the FFT. Substituting the entrywise error bound, we get:\n$$\n\\|\\|\\Delta\\mathbf{Z}_{\\text{near}}\\|\\|_F^2 \\le \\sum_{(i,j)\\in\\mathcal{F}(r)} \\left(\\beta \\frac{a_i a_j}{R_{ij}^2}\\right)^2\n$$\nTo simplify this sum, a further, cruder bound is introduced: The sum of squares is less than or equal to the number of terms multiplied by the maximum squared term.\n$$\n\\sum_{(i,j)\\in\\mathcal{F}(r)} b_{ij}^2 \\le M(r) \\left( \\max_{(i,j)\\in\\mathcal{F}(r)} b_{ij} \\right)^2\n$$\nwhere $b_{ij} = \\beta \\frac{a_i a_j}{R_{ij}^2}$ and $M(r) = |\\mathcal{F}(r)|$ is the number of ordered pairs in the far-zone set.\n\nCombining these inequalities, the original constraint $\\|\\|\\Delta\\mathbf{Z}_{\\text{near}}\\|\\|_2 \\le \\tau$ is satisfied if:\n$$\n\\sqrt{M(r)} \\max_{(i,j)\\in\\mathcal{F}(r)} \\left( \\beta \\frac{a_i a_j}{R_{ij}^2} \\right) \\le \\tau\n$$\nThis defines the function to be controlled:\n$$\nf(r) = \\sqrt{M(r)} \\max_{(i,j)\\in\\mathcal{F}(r)} \\left( \\beta \\frac{a_i a_j}{R_{ij}^2} \\right)\n$$\nThe problem is to find the smallest radius $r_{\\text{near}} = r$ such that $r \\ge h_{\\text{grid}}$ and $f(r) \\le \\tau$. The AIM grid spacing $h_{\\text{grid}}$ serves as a hard lower bound on $r_{\\text{near}}$, below which the convolution model is invalid.\n\n### Algorithmic Solution via Bisection\n\nThe function $f(r)$ is monotonically non-increasing. As $r$ increases, the set of far pairs $\\mathcal{F}(r)$ shrinks or stays the same, so both $M(r)$ and the maximum term are non-increasing. This monotonicity allows for an efficient search for the optimal $r_{\\text{near}}$ using a bisection algorithm.\n\nThe algorithm proceeds as follows:\n1.  **Initialization**: Given basis centroids $\\{\\mathbf{r}_i\\}_{i=1}^N$, sizes $\\{a_i\\}_{i=1}^N$, grid spacing $h_{\\text{grid}}$, and tolerance $\\tau$. The constant $\\beta = 1/(4\\pi)$ is used.\n2.  **Preprocessing**: Compute all unique pairwise distances $R_{ij} = \\|\\mathbf{r}_i - \\mathbf{r}_j\\|$ and corresponding singularity metrics $m_{ij} = a_i a_j / R_{ij}^2$ for distinct pairs $\\{i,j\\}$ with $i \\neq j$. Store these in arrays for efficient lookup.\n3.  **Search Interval**: The search for $r_{\\text{near}}$ is conducted over the interval $[r_{\\text{low}}, r_{\\text{high}}]$. The initial lower bound is $r_{\\text{low}} = h_{\\text{grid}}$. The initial upper bound $r_{\\text{high}}$ is set to the maximum observed pairwise separation, $r_{\\max} = \\max_{i \\neq j} R_{ij}$. If $f(h_{\\text{grid}}) \\le \\tau$, the minimal admissible radius is $h_{\\text{grid}}$ itself, and no search is needed. Otherwise, the bisection proceeds.\n4.  **Bisection**: The algorithm iteratively halves the search interval:\n    a.  Calculate the midpoint $r_{\\text{mid}} = (r_{\\text{low}} + r_{\\text{high}}) / 2$.\n    b.  Evaluate $f(r_{\\text{mid}})$. This involves:\n        i.  Identifying all pairs $\\{i,j\\}$ with $R_{ij} \\ge r_{\\text{mid}}$.\n        ii. Counting the number of such unordered pairs, say $P$. The total number of ordered pairs is $M(r_{\\text{mid}}) = 2P$.\n        iii. Finding the maximum value of $b_{ij} = \\beta m_{ij}$ over this set of pairs.\n        iv. Computing $f(r_{\\text{mid}}) = \\sqrt{M(r_{\\text{mid}})} \\times \\max b_{ij}$. If no pairs satisfy $R_{ij} \\ge r_{\\text{mid}}$, then $M=0$ and $f(r_{\\text{mid}})=0$.\n    c.  Update the interval based on the result:\n        - If $f(r_{\\text{mid}}) \\le \\tau$: $r_{\\text{mid}}$ is a valid (admissible) radius. A smaller radius might also be valid, so we update the upper bound: $r_{\\text{high}} = r_{\\text{mid}}$.\n        - If $f(r_{\\text{mid}}) > \\tau$: $r_{\\text{mid}}$ is too small. The radius must be larger, so we update the lower bound: $r_{\\text{low}} = r_{\\text{mid}}$.\n5.  **Termination**: The loop continues for a fixed number of iterations or until the interval $(r_{\\text{high}} - r_{\\text{low}})$ is smaller than a specified numerical precision. The final result, $r_{\\text{near}}$, is the converged value of $r_{\\text{high}}$, which represents the smallest radius satisfying the condition found by the search.\n\nThis procedure robustly and efficiently computes the required minimal near-zone radius according to the specified error control model.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the adaptive near classification radius for multiple test cases.\n    \"\"\"\n    \n    # Test cases defined in the problem statement\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"positions\": np.array([\n                (0.0, 0.0, 0.0), (0.1, 0.0, 0.0), (0.2, 0.0, 0.0),\n                (0.0, 0.1, 0.0), (0.1, 0.1, 0.0), (0.2, 0.1, 0.0),\n                (0.0, 0.2, 0.0), (0.1, 0.2, 0.0), (0.2, 0.2, 0.0)\n            ]),\n            \"sizes\": np.full(9, 0.1),\n            \"h_grid\": 0.12,\n            \"tau\": 0.3\n        },\n        {\n            \"name\": \"Case B\",\n            \"positions\": np.array([\n                (0.0, 0.0, 0.0), (0.1, 0.0, 0.0), (0.2, 0.0, 0.0),\n                (0.0, 0.1, 0.0), (0.1, 0.1, 0.0), (0.2, 0.1, 0.0),\n                (0.0, 0.2, 0.0), (0.1, 0.2, 0.0), (0.2, 0.2, 0.0)\n            ]),\n            \"sizes\": np.full(9, 0.1),\n            \"h_grid\": 0.12,\n            \"tau\": 0.05\n        },\n        {\n            \"name\": \"Case C\",\n            \"positions\": np.array([\n                (0.0, 0.0, 0.0), (2.0, 0.0, 0.0),\n                (0.0, 2.0, 0.0), (2.0, 2.0, 0.0)\n            ]),\n            \"sizes\": np.full(4, 0.05),\n            \"h_grid\": 0.2,\n            \"tau\": 0.1\n        }\n    ]\n\n    results = []\n    \n    beta = 1.0 / (4.0 * np.pi)\n\n    for case in test_cases:\n        positions = case[\"positions\"]\n        sizes = case[\"sizes\"]\n        h_grid = case[\"h_grid\"]\n        tau = case[\"tau\"]\n        \n        N = positions.shape[0]\n        \n        # Pre-compute pairwise distances and singularity metrics for i != j\n        # We only need to consider each pair {i, j} once.\n        if N < 2:\n            results.append(f\"{h_grid:.6f}\")\n            continue\n\n        # Get indices for the upper triangle of the matrix to handle pairs {i,j} with i < j\n        i_upper, j_upper = np.triu_indices(N, k=1)\n        \n        # Calculate pairwise distances R_ij\n        pos_i = positions[i_upper]\n        pos_j = positions[j_upper]\n        R_pairs = np.linalg.norm(pos_i - pos_j, axis=1)\n        \n        # Calculate pairwise sizes a_i * a_j\n        sizes_i = sizes[i_upper]\n        sizes_j = sizes[j_upper]\n        a_i_aj_pairs = sizes_i * sizes_j\n        \n        # Calculate the singularity metric m_ij = (a_i*a_j) / R_ij^2\n        # R_pairs will be non-zero since i != j\n        m_pairs = a_i_aj_pairs / (R_pairs**2)\n        \n        # Define the function f(r) to be evaluated\n        def calculate_f(r, R_p, m_p):\n            \"\"\"Calculates f(r) based on pre-computed pair data.\"\"\"\n            # Identify pairs classified as far\n            is_far_mask = R_p >= r\n            \n            # Count the number of unordered far pairs\n            num_unordered_far_pairs = np.sum(is_far_mask)\n            \n            if num_unordered_far_pairs == 0:\n                return 0.0\n            \n            # M(r) is the number of ordered far pairs (i,j), i!=j\n            M_r = 2 * num_unordered_far_pairs\n            \n            # b_ij = beta * m_ij\n            b_pairs = beta * m_p\n            \n            # Find the maximum b_ij among the far pairs\n            max_b_far = np.max(b_pairs[is_far_mask])\n            \n            return np.sqrt(M_r) * max_b_far\n\n        # The minimal admissible radius is at least h_grid\n        # Check if h_grid itself already satisfies the condition\n        if calculate_f(h_grid, R_pairs, m_pairs) <= tau:\n            r_near = h_grid\n        else:\n            # Perform bisection search\n            r_low = h_grid\n            r_high = np.max(R_pairs)\n            \n            # Use a fixed number of iterations for guaranteed convergence precision\n            for _ in range(100):\n                r_mid = r_low + (r_high - r_low) / 2.0\n                if r_mid == r_low or r_mid == r_high: # Convergence reached\n                    break\n                \n                f_val = calculate_f(r_mid, R_pairs, m_pairs)\n                \n                if f_val <= tau:\n                    # r_mid is a potential solution, try for a smaller one\n                    r_high = r_mid\n                else:\n                    # r_mid is too small, must be larger\n                    r_low = r_mid\n            \n            r_near = r_high\n            \n        results.append(f\"{r_near:.6f}\")\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3288285"}, {"introduction": "A robust algorithm must be practical to implement, and for large-scale electromagnetic simulations, memory is often the most critical resource. This practice guides you through a quantitative analysis of the memory footprint of a complete AIM implementation [@problem_id:3288298]. By dissecting the algorithm into its components—the FFT grid, projection stencils, and near-field matrix—you will learn to predict its memory requirements and understand its asymptotic scaling behavior, which is essential for tackling large and complex problems.", "problem": "Consider a three-dimensional Fast Fourier Transform (FFT)-accelerated Adaptive Integral Method (AIM) implementation for the Electric Field Integral Equation (EFIE) with a Method of Moments (MoM) discretization using $N$ Rao–Wilton–Glisson basis functions. The unknowns are supported within a rectangular bounding box of dimensions $L_{x}$, $L_{y}$, and $L_{z}$, excited at frequency $f$ in free space. The AIM employs a uniform Cartesian FFT grid chosen adaptively by enforcing the Nyquist–Shannon sampling condition on the wavelength $\\lambda$, with anisotropic oversampling factors $s_{x}$, $s_{y}$, and $s_{z}$ in the three coordinate directions. Specifically, the grid spacings are $h_{i} = \\lambda / (2 s_{i})$ for $i \\in \\{x,y,z\\}$, and the grid size in each direction is padded to mitigate circular convolution as $N_{i} = \\gamma_{i} \\,\\lceil L_{i} / h_{i} \\rceil$ with padding factors $\\gamma_{i} \\geq 1$. The AIM stores the spectral-domain dyadic Green’s function $\\hat{G}(\\boldsymbol{\\kappa})$ at all FFT grid wavenumbers, with $d_{G}$ unique components per grid point, each component stored as a complex double-precision number.\n\nIn addition to $\\hat{G}$, AIM stores projection stencils that map basis functions to the grid and grid fields back to testing functions. Assume trilinear projection with $n_{s}$ stencil points per basis function per projection, with each stencil point storing one weight (a real double-precision number) and three integer grid indices. Both source-to-grid and grid-to-test stencils are stored separately. Finally, AIM stores near-field interactions as a unique sparse list of basis function pairs that are within a prescribed geometric proximity; assume each unknown has on average $n_{\\mathrm{nf}}$ near neighbors, and the storage retains one complex coupling value per unique pair along with the two integer indices identifying the pair.\n\nUse the following physically consistent parameters:\n- Speed of light $c = 3.0 \\times 10^{8} \\,\\text{m/s}$, operating frequency $f = 3.0 \\times 10^{9} \\,\\text{Hz}$, hence wavelength $\\lambda = c/f$.\n- Bounding box dimensions $L_{x} = 0.47 \\,\\text{m}$, $L_{y} = 0.36 \\,\\text{m}$, $L_{z} = 0.22 \\,\\text{m}$.\n- Oversampling factors $s_{x} = 1.4$, $s_{y} = 1.25$, $s_{z} = 1.05$, and padding factors $\\gamma_{x} = \\gamma_{y} = \\gamma_{z} = 2$.\n- Dyadic Green’s function storage: $d_{G} = 6$ unique components per grid point, each stored as one complex double ($16$ bytes).\n- Projection stencils: $n_{s} = 8$ points per basis function per projection; each stencil point stores one real double ($8$ bytes) and three $4$-byte integers (total $12$ bytes) for the grid indices; both source-to-grid and grid-to-test stencils are stored.\n- Near-field: average near neighbors per unknown $n_{\\mathrm{nf}} = 28$; storage per unique pair consists of one complex double ($16$ bytes) and two $4$-byte integers ($8$ bytes total).\n- Number of unknowns $N = 450{,}000$.\n\nStarting from the sampling requirement for $h_{i}$ and the definition of padded grid sizes $N_{i}$, derive the asymptotic dependence of each memory component on $N$ and the adaptive grid parameters, and then calculate the practical total memory footprint\n$$\nM_{\\text{total}} = M_{\\hat{G}} + M_{\\text{stencils}} + M_{\\text{near-field}}\n$$\nin megabytes, where one megabyte is defined as $10^{6}$ bytes. Round your final numerical answer to four significant figures and express it in megabytes. Provide only the single numerical value for $M_{\\text{total}}$ as your final answer. State your reasoning and intermediate steps clearly, justifying the impact of the adaptive grid choice on both asymptotic and practical memory use.", "solution": "The problem statement is evaluated to be scientifically grounded, well-posed, objective, and internally consistent. It provides all necessary parameters and definitions to compute the total memory footprint of the specified Adaptive Integral Method (AIM) implementation. The parameters are physically realistic, and the described methodology aligns with standard practices in computational electromagnetics. Therefore, the problem is valid, and we proceed with the solution.\n\nThe total memory footprint, $M_{\\text{total}}$, is the sum of the memory required for the spectral-domain Green's function, $M_{\\hat{G}}$, the projection stencils, $M_{\\text{stencils}}$, and the near-field impedance matrix, $M_{\\text{near-field}}$.\n$$\nM_{\\text{total}} = M_{\\hat{G}} + M_{\\text{stencils}} + M_{\\text{near-field}}\n$$\n\nFirst, we analyze the asymptotic dependence of each memory component on the number of unknowns, $N$. For a surface mesh, the number of unknowns $N$ is proportional to the surface area divided by the square of the average element size. If the object's characteristic size is $L_{\\text{obj}}$, then its surface area scales as $L_{\\text{obj}}^2$. This implies that the linear dimensions of the bounding box, $L_i$, scale as $L_{\\text{obj}}$, which in turn scales as $\\sqrt{N}$. The grid spacing $h_i$ is proportional to the wavelength $\\lambda$, which is a constant for a fixed frequency.\nTherefore, the number of FFT grid points in each dimension, $N_i$, scales as:\n$N_i = \\gamma_i \\lceil L_i / h_i \\rceil \\propto L_i \\propto \\sqrt{N}$\nThe total number of grid points in the FFT grid is $N_g = N_x N_y N_z$. Its scaling is:\n$N_g \\propto (\\sqrt{N})^3 = N^{3/2}$\n\nThe memory for each component can now be analyzed asymptotically:\n1.  $M_{\\hat{G}}$: The memory for the Green's function is proportional to the total number of grid points, $N_g$. Thus, the asymptotic scaling is $M_{\\hat{G}} \\propto O(N^{3/2})$.\n2.  $M_{\\text{stencils}}$: The memory for the projection stencils is directly proportional to the number of basis functions, $N$. Thus, the asymptotic scaling is $M_{\\text{stencils}} \\propto O(N)$.\n3.  $M_{\\text{near-field}}$: The memory for the near-field interactions is also proportional to the number of basis functions, $N$, assuming the average number of near neighbors per unknown, $n_{\\mathrm{nf}}$, is constant. Thus, the asymptotic scaling is $M_{\\text{near-field}} \\propto O(N)$.\n\nFor large $N$, the total memory $M_{\\text{total}}$ is dominated by the $O(N^{3/2})$ term from the Green's function storage, a characteristic of FFT-accelerated methods.\n\nNext, we proceed with the practical calculation of each memory component using the given parameters.\n\nFirst, we calculate the wavelength $\\lambda$:\n$$\n\\lambda = \\frac{c}{f} = \\frac{3.0 \\times 10^{8} \\,\\text{m/s}}{3.0 \\times 10^{9} \\,\\text{Hz}} = 0.1 \\,\\text{m}\n$$\n\nNext, we determine the AIM grid spacings $h_i$ using the oversampling factors $s_i$:\n$$\nh_x = \\frac{\\lambda}{2 s_x} = \\frac{0.1}{2 \\times 1.4} = \\frac{0.1}{2.8} \\,\\text{m} \\\\\nh_y = \\frac{\\lambda}{2 s_y} = \\frac{0.1}{2 \\times 1.25} = \\frac{0.1}{2.5} = 0.04 \\,\\text{m} \\\\\nh_z = \\frac{\\lambda}{2 s_z} = \\frac{0.1}{2 \\times 1.05} = \\frac{0.1}{2.1} \\,\\text{m}\n$$\n\nNow, we calculate the padded FFT grid dimensions $N_i$ using the bounding box dimensions $L_i$ and padding factors $\\gamma_i = 2$:\n$$\nN_x = \\gamma_x \\left\\lceil \\frac{L_x}{h_x} \\right\\rceil = 2 \\left\\lceil \\frac{0.47}{0.1 / 2.8} \\right\\rceil = 2 \\lceil 0.47 \\times 28 \\rceil = 2 \\lceil 13.16 \\rceil = 2 \\times 14 = 28 \\\\\nN_y = \\gamma_y \\left\\lceil \\frac{L_y}{h_y} \\right\\rceil = 2 \\left\\lceil \\frac{0.36}{0.04} \\right\\rceil = 2 \\lceil 9 \\rceil = 2 \\times 9 = 18 \\\\\nN_z = \\gamma_z \\left\\lceil \\frac{L_z}{h_z} \\right\\rceil = 2 \\left\\lceil \\frac{0.22}{0.1 / 2.1} \\right\\rceil = 2 \\lceil 0.22 \\times 21 \\rceil = 2 \\lceil 4.62 \\rceil = 2 \\times 5 = 10\n$$\nThe total number of points in the FFT grid is:\n$$\nN_g = N_x \\times N_y \\times N_z = 28 \\times 18 \\times 10 = 5040\n$$\n\nWith these intermediate values, we can calculate the memory for each component.\n\n1.  Memory for the spectral-domain Green's function, $M_{\\hat{G}}$:\nThe storage consists of $d_G = 6$ unique complex double-precision components per grid point. Each complex double is $16$ bytes.\n$$\nM_{\\hat{G}} = N_g \\times d_G \\times (16 \\,\\text{bytes/component}) = 5040 \\times 6 \\times 16 = 483,840 \\,\\text{bytes}\n$$\n\n2.  Memory for the projection stencils, $M_{\\text{stencils}}$:\nBoth source-to-grid and grid-to-test stencils are stored for all $N = 450,000$ basis functions. Each stencil has $n_s = 8$ points. Each point stores one real double ($8$ bytes) and three $4$-byte integers ($3 \\times 4 = 12$ bytes), for a total of $8+12=20$ bytes per stencil point.\n$$\nM_{\\text{stencils}} = 2 \\times N \\times n_s \\times (20 \\,\\text{bytes/point}) = 2 \\times 450,000 \\times 8 \\times 20 = 144,000,000 \\,\\text{bytes}\n$$\n\n3.  Memory for the near-field interactions, $M_{\\text{near-field}}$:\nEach of the $N=450,000$ unknowns has an average of $n_{\\mathrm{nf}}=28$ near neighbors. The interactions are stored as unique pairs. The number of unique pairs is $\\frac{N \\times n_{\\mathrm{nf}}}{2}$. Each pair's storage consists of one complex double ($16$ bytes) and two $4$-byte integers for indices ($2 \\times 4 = 8$ bytes), for a total of $16+8=24$ bytes per pair.\n$$\nM_{\\text{near-field}} = \\frac{N \\times n_{\\mathrm{nf}}}{2} \\times (24 \\,\\text{bytes/pair}) = \\frac{450,000 \\times 28}{2} \\times 24 = 6,300,000 \\times 24 = 151,200,000 \\,\\text{bytes}\n$$\n\nFinally, the total memory footprint in bytes is the sum of these components:\n$$\nM_{\\text{total}} = M_{\\hat{G}} + M_{\\text{stencils}} + M_{\\text{near-field}} = 483,840 + 144,000,000 + 151,200,000 = 295,683,840 \\,\\text{bytes}\n$$\n\nConverting to megabytes, using the definition $1 \\,\\text{MB} = 10^6 \\,\\text{bytes}$:\n$$\nM_{\\text{total}} = \\frac{295,683,840}{10^6} \\,\\text{MB} = 295.68384 \\,\\text{MB}\n$$\nRounding the result to four significant figures gives $295.7 \\,\\text{MB}$.", "answer": "$$ \\boxed{295.7} $$", "id": "3288298"}, {"introduction": "Beyond algorithmic correctness and memory management lies the crucial domain of execution speed. This advanced practice introduces the roofline model, a cornerstone of high-performance computing, to analyze and predict the throughput of the AIM algorithm on modern processors like CPUs and GPUs [@problem_id:3288252]. You will model how performance is limited by both arithmetic speed and memory bandwidth, and explore how to orchestrate computations on heterogeneous hardware to achieve maximum efficiency.", "problem": "You are asked to design and implement a quantitative performance model for the Adaptive Integral Method (AIM) used in computational electromagnetics to accelerate boundary integral equation solvers. In AIM, the impedance matrix is decomposed into a near-zone component and a far-zone component. The near-zone component, denoted by $\\mathbf{Z}_{\\text{near}}$, is applied as a sparse matrix-vector product, and the far-zone component is computed via convolution on a regular grid using the Fast Fourier Transform (FFT). Your task is to derive, implement, and evaluate a model that couples FFT arithmetic intensity and Central Processing Unit (CPU) or Graphics Processing Unit (GPU) memory bandwidth to predict end-to-end AIM throughput. You must also propose and model overlap strategies for $\\mathbf{Z}_{\\text{near}}$ application and FFT processing in heterogeneous settings (CPU plus GPU) and quantify the resulting throughput improvement.\n\nBegin from the following foundational definitions and widely-accepted facts:\n- A $3$-dimensional complex-to-complex FFT of size $n_x \\times n_y \\times n_z$ can be implemented as sequences of one-dimensional FFTs; the total number of real floating-point operations for one forward transform is approximately $5 n_g \\log_2 n_g$, where $n_g = n_x n_y n_z$. The same count applies to the inverse transform. A pointwise complex multiplication on the spectral grid costs $6$ real floating-point operations per grid point.\n- Arithmetic intensity is defined as $I = F/T_b$, where $F$ is the number of floating-point operations and $T_b$ is the number of bytes moved to or from main memory.\n- The roofline model states that the achieved floating-point rate $R$ for a kernel on a device with peak floating-point rate $P$ and sustainable memory bandwidth $B$ is $R = \\min(P, I B)$.\n- A complex double-precision number occupies $s_c = 16$ bytes.\n- For the sparse near-zone application with $N$ unknowns and average near neighbors $\\nu$, model the floating-point operations as $F_{\\text{near}} = 10 \\nu N$ (accounting for complex multiply-adds), and model the memory traffic as $T_{b,\\text{near}} = s_c N (2 \\nu + 1)$ (reading $\\nu$ complex matrix entries and $\\nu$ complex vector entries per unknown, and writing one complex result per unknown).\n- For the far-zone FFT step consisting of one forward FFT, one pointwise spectral-domain multiplication, and one inverse FFT, model the floating-point operations as $F_{\\text{fft}} = 10 n_g \\log_2 n_g + 6 n_g$ and the memory traffic as $T_{b,\\text{fft}} = 14 s_c n_g$, assuming each of the two FFTs performs three streamed passes (one per axis) with read and write per pass, and the spectral multiply performs one read and one write in-place.\n\nYou must compute the following per test case:\n- The non-overlapped iteration time $t_{\\text{no}}$ in seconds, defined as the sum of component times for the near-zone, the far-zone FFT, and any host-device transfer time if the near-zone and far-zone are executed on different devices.\n- The overlapped iteration time $t_{\\text{ov}}$ in seconds, under an idealized double-buffering strategy in which CPU-side $\\mathbf{Z}_{\\text{near}}$ application, GPU-side FFT, and host-device transfer proceed concurrently when they reside on different devices, modeled as $t_{\\text{ov}} = \\max(t_{\\text{near}}, t_{\\text{fft}}, t_{\\text{hd}})$. If both near and far tasks execute on the same device, assume no overlap so that $t_{\\text{ov}} = t_{\\text{no}}$.\n- The non-overlapped throughput $\\theta_{\\text{no}}$ in iterations per second, defined as $\\theta_{\\text{no}} = 1/t_{\\text{no}}$.\n- The overlapped throughput $\\theta_{\\text{ov}}$ in iterations per second, defined as $\\theta_{\\text{ov}} = 1/t_{\\text{ov}}$.\n- For the FFT kernel in each test case, report the arithmetic intensity $I_{\\text{fft}}$.\n\nModel details:\n- For a computation on a given device, compute arithmetic intensity $I$ and achieved rate $R = \\min(P, I B)$, then time $t = F / R$.\n- Use $t_{\\text{hd}} = T_{b,\\text{transfer}}/B_{\\text{pcie}}$ when near and far execute on different devices, with $T_{b,\\text{transfer}} = 2 s_c N$ per iteration to account for transferring the complex vector of length $N$ to and from the accelerator. If both stages execute on the same device, set $t_{\\text{hd}} = 0$.\n\nUnits and output requirements:\n- Express all times in seconds and throughput in iterations per second. When printing the results, round each floating-point number to $6$ decimal places.\n- Angles are not involved; do not use angle units.\n- Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case contributes a sublist of four floats and one additional float for $I_{\\text{fft}}$, ordered as $[t_{\\text{no}}, t_{\\text{ov}}, \\theta_{\\text{no}}, \\theta_{\\text{ov}}, I_{\\text{fft}}]$. The final line must be a list of these sublists, for all test cases, with no spaces.\n\nTest suite:\n- Case $1$ (CPU-only baseline):\n  - Grid: $n_x = 128$, $n_y = 128$, $n_z = 128$.\n  - Unknowns: $N = 2{,}097{,}152$.\n  - Near neighbors: $\\nu = 12$.\n  - Complex size: $s_c = 16$ bytes.\n  - CPU peak: $P_{\\text{cpu}} = 5 \\times 10^{11}$ flops/s.\n  - CPU bandwidth: $B_{\\text{cpu}} = 1 \\times 10^{11}$ bytes/s.\n  - Execution: far-zone on CPU, near-zone on CPU, so $t_{\\text{hd}} = 0$.\n- Case $2$ (heterogeneous overlap: GPU far, CPU near with host-device transfer):\n  - Grid: $n_x = 256$, $n_y = 256$, $n_z = 256$.\n  - Unknowns: $N = 8{,}000{,}000$.\n  - Near neighbors: $\\nu = 8$.\n  - Complex size: $s_c = 16$ bytes.\n  - GPU peak: $P_{\\text{gpu}} = 1.5 \\times 10^{13}$ flops/s.\n  - GPU bandwidth: $B_{\\text{gpu}} = 8.0 \\times 10^{11}$ bytes/s.\n  - CPU peak: $P_{\\text{cpu}} = 5 \\times 10^{11}$ flops/s.\n  - CPU bandwidth: $B_{\\text{cpu}} = 1 \\times 10^{11}$ bytes/s.\n  - Host-device bandwidth: $B_{\\text{pcie}} = 1.6 \\times 10^{10}$ bytes/s.\n  - Execution: far-zone on GPU, near-zone on CPU, so include $t_{\\text{hd}}$ and model overlap $t_{\\text{ov}} = \\max(t_{\\text{near}}, t_{\\text{fft}}, t_{\\text{hd}})$.\n- Case $3$ (GPU-only, near heavier than far):\n  - Grid: $n_x = 512$, $n_y = 512$, $n_z = 512$.\n  - Unknowns: $N = 30{,}000{,}000$.\n  - Near neighbors: $\\nu = 30$.\n  - Complex size: $s_c = 16$ bytes.\n  - GPU peak: $P_{\\text{gpu}} = 1.5 \\times 10^{13}$ flops/s.\n  - GPU bandwidth: $B_{\\text{gpu}} = 8.0 \\times 10^{11}$ bytes/s.\n  - Execution: far-zone on GPU, near-zone on GPU, so $t_{\\text{hd}} = 0$ and no overlap.\n- Case $4$ (small-grid boundary case, CPU-only):\n  - Grid: $n_x = 16$, $n_y = 16$, $n_z = 16$.\n  - Unknowns: $N = 10{,}000$.\n  - Near neighbors: $\\nu = 4$.\n  - Complex size: $s_c = 16$ bytes.\n  - CPU peak: $P_{\\text{cpu}} = 2 \\times 10^{11}$ flops/s.\n  - CPU bandwidth: $B_{\\text{cpu}} = 5.0 \\times 10^{10}$ bytes/s.\n  - Execution: far-zone on CPU, near-zone on CPU, so $t_{\\text{hd}} = 0$.\n\nYour program must implement the above model and produce a single line of output of the form $[[t_{\\text{no},1},t_{\\text{ov},1},\\theta_{\\text{no},1},\\theta_{\\text{ov},1},I_{\\text{fft},1}],\\dots,[t_{\\text{no},4},t_{\\text{ov},4},\\theta_{\\text{no},4},\\theta_{\\text{ov},4},I_{\\text{fft},4}]]$, with each float rounded to $6$ decimal places and no spaces anywhere in the line.", "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, and complete. All necessary parameters, models, and definitions are provided to construct a quantitative performance model for the Adaptive Integral Method (AIM).\n\nThe solution involves synthesizing several established models: the algorithmic structure of AIM, cost models for its computational kernels (sparse matrix-vector multiplication and Fast Fourier Transform), and the roofline performance model for predicting execution time on a specific hardware architecture. The problem requires the analysis of four distinct test cases, covering CPU-only, GPU-only, and heterogeneous CPU-GPU execution scenarios.\n\nThe core of the AIM performance model is the decomposition of the total iteration time. An iteration consists of a near-zone calculation and a far-zone calculation.\n\nFirst, we define the parameters provided in the problem statement:\n- The size of the FFT grid is $n_x \\times n_y \\times n_z$, with the total number of grid points being $n_g = n_x n_y n_z$.\n- The number of unknowns in the boundary element method is $N$.\n- The average number of near-neighbors for each unknown is $\\nu$.\n- The size of a complex double-precision number is $s_c = 16$ bytes.\n- The peak floating-point rate of a device is $P$ (in flops/s).\n- The sustainable memory bandwidth of a device is $B$ (in bytes/s).\n- The host-to-device interconnect bandwidth is $B_{\\text{pcie}}$.\n\nThe performance of each computational component is determined using the roofline model. The achieved floating-point rate, $R$, is given by $R = \\min(P, I \\times B)$, where $I$ is the arithmetic intensity of the kernel. The arithmetic intensity is the ratio of floating-point operations performed ($F$) to the number of bytes moved from/to main memory ($T_b$), i.e., $I = F/T_b$. The execution time $t$ of a kernel is then $t = F/R$.\n\n**Near-Zone Component ($t_{\\text{near}}$)**\nThe near-zone calculation is modeled as a sparse matrix-vector product.\n1.  The number of floating-point operations, $F_{\\text{near}}$, is modeled as $F_{\\text{near}} = 10 \\nu N$. This accounts for a complex multiply-add operation (e.g., $4$ multiplications and $2$ additions for complex multiplication, plus $2$ additions for complex addition, totaling $8$ real operations, but the model specifies $10$ as a reasonable approximation).\n2.  The memory traffic, $T_{b,\\text{near}}$, is modeled as $T_{b,\\text{near}} = s_c N (2 \\nu + 1)$. This assumes reading $\\nu$ complex matrix entries, $\\nu$ complex vector entries, and writing one complex result entry for each of the $N$ unknowns.\n3.  The arithmetic intensity is:\n$$I_{\\text{near}} = \\frac{F_{\\text{near}}}{T_{b,\\text{near}}} = \\frac{10 \\nu N}{s_c N (2 \\nu + 1)} = \\frac{10 \\nu}{s_c (2 \\nu + 1)}$$\n4.  Given the device parameters ($P_{\\text{near_dev}}$, $B_{\\text{near_dev}}$), the achieved rate is $R_{\\text{near}} = \\min(P_{\\text{near_dev}}, I_{\\text{near}} \\times B_{\\text{near_dev}})$.\n5.  The execution time for the near-zone component is $t_{\\text{near}} = F_{\\text{near}} / R_{\\text{near}}$.\n\n**Far-Zone Component ($t_{\\text{fft}}$)**\nThe far-zone calculation is dominated by three steps: a forward $3$D FFT, a pointwise multiplication in the spectral domain, and an inverse $3$D FFT.\n1.  The total floating-point operations, $F_{\\text{fft}}$, are given by the sum of operations for two FFTs and the pointwise multiplication:\n$$F_{\\text{fft}} = 2 \\times (5 n_g \\log_2 n_g) + 6 n_g = 10 n_g \\log_2 n_g + 6 n_g$$\n2.  The memory traffic, $T_{b,\\text{fft}}$, is specified as $T_{b,\\text{fft}} = 14 s_c n_g$. This is based on a specific implementation model where each FFT involves three passes (one per dimension) each requiring a read and a write of the entire grid ($2 \\times 3 \\times s_c n_g$ per FFT), and the spectral multiplication involves one read and one write pass ($2 s_c n_g$), totaling $(2 \\times 6 + 2)s_c n_g = 14 s_c n_g$ bytes.\n3.  The arithmetic intensity, $I_{\\text{fft}}$, which is one of the required outputs, is:\n$$I_{\\text{fft}} = \\frac{F_{\\text{fft}}}{T_{b,\\text{fft}}} = \\frac{10 n_g \\log_2 n_g + 6 n_g}{14 s_c n_g} = \\frac{10 \\log_2 n_g + 6}{14 s_c}$$\n4.  Given the device parameters ($P_{\\text{fft_dev}}$, $B_{\\text{fft_dev}}$), the achieved rate is $R_{\\text{fft}} = \\min(P_{\\text{fft_dev}}, I_{\\text{fft}} \\times B_{\\text{fft_dev}})$.\n5.  The execution time for the far-zone component is $t_{\\text{fft}} = F_{\\text{fft}} / R_{\\text{fft}}$.\n\n**Host-Device Transfer ($t_{\\text{hd}}$)**\nData transfer between CPU and GPU is necessary only in heterogeneous execution scenarios.\n1.  The transfer time $t_{\\text{hd}}$ is zero if both near-zone and far-zone computations are performed on the same device.\n2.  If they are on different devices, a complex vector of length $N$ must be transferred to the accelerator and the result transferred back. The total data moved is $T_{b,\\text{transfer}} = 2 s_c N$.\n3.  The time taken is $t_{\\text{hd}} = T_{b,\\text{transfer}} / B_{\\text{pcie}}$.\n\n**Total Time and Throughput**\nThe total iteration time and throughput are calculated for two scenarios: non-overlapped and overlapped.\n1.  **Non-Overlapped**: The total time, $t_{\\text{no}}$, is the sum of the individual component times: $t_{\\text{no}} = t_{\\text{near}} + t_{\\text{fft}} + t_{\\text{hd}}$. The corresponding throughput is $\\theta_{\\text{no}} = 1/t_{\\text{no}}$.\n2.  **Overlapped**: Overlap is only possible in the heterogeneous case, where the near-zone (on CPU), far-zone (on GPU), and data transfers can be pipelined. The iteration time is limited by the slowest stage: $t_{\\text{ov}} = \\max(t_{\\text{near}}, t_{\\text{fft}}, t_{\\text{hd}})$. If both components execute on the same device, no overlap is possible, so $t_{\\text{ov}} = t_{\\text{no}}$. The corresponding throughput is $\\theta_{\\text{ov}} = 1/t_{\\text{ov}}$.\n\nThese principles are implemented for each of the four test cases specified in the problem statement to compute the five required output values: $[t_{\\text{no}}, t_{\\text{ov}}, \\theta_{\\text{no}}, \\theta_{\\text{ov}}, I_{\\text{fft}}]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the AIM performance modeling problem for all test cases.\n    \"\"\"\n\n    def calculate_performance(case):\n        \"\"\"\n        Calculates the performance metrics for a single test case.\n        \"\"\"\n        nx, ny, nz = case[\"grid\"]\n        N = case[\"N\"]\n        nu = case[\"nu\"]\n        s_c = case[\"s_c\"]\n        \n        P_cpu = case.get(\"P_cpu\")\n        B_cpu = case.get(\"B_cpu\")\n        P_gpu = case.get(\"P_gpu\")\n        B_gpu = case.get(\"B_gpu\")\n        B_pcie = case.get(\"B_pcie\")\n\n        near_exec_dev = case.get(\"near_exec\", \"CPU\")\n        far_exec_dev = case.get(\"far_exec\", \"CPU\")\n\n        # Determine device parameters for each part\n        if near_exec_dev == 'CPU':\n            P_near, B_near = P_cpu, B_cpu\n        else:\n            P_near, B_near = P_gpu, B_gpu\n\n        if far_exec_dev == 'CPU':\n            P_far, B_far = P_cpu, B_cpu\n        else:\n            P_far, B_far = P_gpu, B_gpu\n\n        is_heterogeneous = near_exec_dev != far_exec_dev\n\n        # Grid size\n        ng = nx * ny * nz\n\n        # ---- Near-zone calculations ----\n        F_near = 10 * nu * N\n        Tb_near = s_c * N * (2 * nu + 1)\n        if Tb_near > 0:\n            I_near = F_near / Tb_near\n        else: # Avoid division by zero\n            I_near = 0\n        \n        R_near = min(P_near, I_near * B_near)\n        if R_near > 0:\n            t_near = F_near / R_near\n        else: # Avoid division by zero\n            t_near = 0\n\n        # ---- Far-zone calculations ----\n        log2_ng = np.log2(ng)\n        F_far = 10 * ng * log2_ng + 6 * ng\n        Tb_far = 14 * s_c * ng\n        \n        # This is one of the required outputs\n        I_fft = (10 * log2_ng + 6) / (14 * s_c)\n        \n        R_far = min(P_far, I_fft * B_far)\n        if R_far > 0:\n            t_far = F_far / R_far\n        else: # Avoid division by zero\n            t_far = 0\n\n\n        # ---- Host-device transfer time ----\n        if is_heterogeneous:\n            Tb_transfer = 2 * s_c * N\n            t_hd = Tb_transfer / B_pcie\n        else:\n            t_hd = 0\n\n        # ---- Total time and throughputs ----\n        t_no = t_near + t_far + t_hd\n        \n        if is_heterogeneous:\n            t_ov = max(t_near, t_far, t_hd)\n        else:\n            t_ov = t_no\n\n        if t_no > 0:\n            theta_no = 1 / t_no\n        else:\n            theta_no = 0\n\n        if t_ov > 0:\n            theta_ov = 1 / t_ov\n        else:\n            theta_ov = 0\n\n        return [t_no, t_ov, theta_no, theta_ov, I_fft]\n\n    test_cases = [\n        # Case 1 (CPU-only baseline)\n        {\n            \"grid\": (128, 128, 128), \"N\": 2097152, \"nu\": 12, \"s_c\": 16,\n            \"P_cpu\": 5e11, \"B_cpu\": 1e11,\n            \"near_exec\": \"CPU\", \"far_exec\": \"CPU\"\n        },\n        # Case 2 (heterogeneous overlap: GPU far, CPU near)\n        {\n            \"grid\": (256, 256, 256), \"N\": 8000000, \"nu\": 8, \"s_c\": 16,\n            \"P_gpu\": 1.5e13, \"B_gpu\": 8.0e11,\n            \"P_cpu\": 5e11, \"B_cpu\": 1e11,\n            \"B_pcie\": 1.6e10,\n            \"near_exec\": \"CPU\", \"far_exec\": \"GPU\"\n        },\n        # Case 3 (GPU-only, near heavier than far)\n        {\n            \"grid\": (512, 512, 512), \"N\": 30000000, \"nu\": 30, \"s_c\": 16,\n            \"P_gpu\": 1.5e13, \"B_gpu\": 8.0e11,\n            \"near_exec\": \"GPU\", \"far_exec\": \"GPU\"\n        },\n        # Case 4 (small-grid boundary case, CPU-only)\n        {\n            \"grid\": (16, 16, 16), \"N\": 10000, \"nu\": 4, \"s_c\": 16,\n            \"P_cpu\": 2e11, \"B_cpu\": 5.0e10,\n            \"near_exec\": \"CPU\", \"far_exec\": \"CPU\"\n        }\n    ]\n\n    all_results_str = []\n    for case_data in test_cases:\n        results = calculate_performance(case_data)\n        # Format each number to 6 decimal places and create the sublist string\n        case_str_list = [f\"{val:.6f}\" for val in results]\n        sublist_str = f\"[{','.join(case_str_list)}]\"\n        all_results_str.append(sublist_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n\n```", "id": "3288252"}]}